{"open_issues": {"1": {"issue_url": "https://github.com/explosion/spaCy/issues/4958", "issue_id": "#4958", "issue_summary": "Slow performance with Span.ents", "issue_description": "jakepoz commented 8 hours ago\nHow to reproduce the behaviour\ndoc = self.nlp(sentence)\n\nfor sentence in doc.sents:\n  for ent in sentence.ents:\n    [process some stuff]\nThe code above ends up having n^2 runtime, because the Span's class .ents property causes document entities to be recalculated from scratch, and then searched linearly:\nspaCy/spacy/tokens/span.pyx\nLines 413 to 426 in d031440\n     def ents(self): \n         \"\"\"The named entities in the span. Returns a tuple of named entity \n         `Span` objects, if the entity recognizer has been applied. \n            RETURNS (tuple): Entities in the span, one `Span` per entity. \n            DOCS: https://spacy.io/api/span#ents \n         \"\"\" \n         ents = [] \n         for ent in self.doc.ents: \n             if ent.start >= self.start and ent.end <= self.end: \n                 ents.append(ent) \n         return ents \n   We worked around this by adding a cache around doc.ents and then binary search to get the start and end entities.\nimport bisect\n\nfrom typing import List, Tuple\nfrom spacy.tokens.span import Span\nfrom spacy.tokens.doc import Doc\n\nfrom functools import lru_cache\n\n\n@lru_cache()\ndef __get_doc_ents(doc: Doc) -> Tuple[List[Span], List[int], List[int]]:\n    \"\"\"\n    Returns a precached version of a document's entity list from spacy.\n    Includes returning ordered indexes of the span start and end positions for easy binary searching.\n\n    :param doc: Spacy document\n    :return: (list of entities, list of start indexes, list of end indexes)\n    \"\"\"\n    ents = doc.ents\n    return ents, [x.start for x in ents], [x.end for x in ents]\n\n\ndef get_cached_ents(span: Span) -> List[Span]:\n    \"\"\"\n    Returns the spacy ents for a span of text within a spacy document.\n\n    It does this really fast by caching the entity list, and using binary search.\n    :param span: the Span you want to get entities from\n    :return: List of Entities as Spacy Spans\n    \"\"\"\n    ents, starts, ends = __get_doc_ents(span.doc)\n\n    start_index = bisect.bisect_left(starts, span.start)\n    end_index = bisect.bisect_right(ends, span.end)\n\n    return ents[start_index: end_index]\nPerformance for parsing really large documents (3+MB of raw text), went from 30+ minutes to 3 minutes when we used the code above.\nI wanted to post this here for reference in case anyone else had the same problem, and to ask if such a change would be accepted into mainline spacy.", "issue_status": "Open", "issue_reporting_time": "2020-01-30T21:08:25Z"}, "2": {"issue_url": "https://github.com/explosion/spaCy/issues/4956", "issue_id": "#4956", "issue_summary": "Greek Lemmatizer issue with lookup() keyword argument", "issue_description": "polekspd commented 15 hours ago\nI copied and pasted the updated files to ...\\spacy\\lang\\el and I got this error. Does anyone have any idea why that happened ? Thank you in advance\nimport spacy\nfrom spacy.lang.el import Greek\nnlp = Greek()\ndoc = nlp('\u03a7\u03b8\u03b5\u03c2')\ndoc[0].lemma_\nTraceback(most recent call last):\nFile \"\", line 1, in\nFile \"token.pyx\",line 8,in spacy.tokens.token.Token.lemma_._ get _\nTypeError: lookup() got an unexpected keyword argument 'orth'\nOperating System: Windows-10\nPython Version Used: 3.7.4\nspaCy Version Used: 2.2.3", "issue_status": "Open", "issue_reporting_time": "2020-01-30T13:59:47Z"}, "3": {"issue_url": "https://github.com/explosion/spaCy/issues/4955", "issue_id": "#4955", "issue_summary": "Simple train dependency parser on sentence splitting", "issue_description": "pdimiel commented 15 hours ago\nHello everyone,\nI'm currently trying to train a blank language model based on an existing language.\nI want to train the tagger and the dependency parser.\nThe annotated corpus I have includes multi-sentence documents.\nWhen I use the method described in https://spacy.io/usage/training#example-train-parser , both modules are trained but converting to the desired input format drops sentence boundaries information and the trained model does not split sentences.\nIf I do not want to use a custom sentence spliter or sentenciser but want to depende on the parser sentence splitting, how should I format the following to be treated as a two sentence document in the training?\nTRAIN_DATA = [\n    (\n        \"They trade mortgage-backed securities.\",\n        {\n            \"heads\": [1, 1, 4, 4, 5, 1, 1],\n            \"deps\": [\"nsubj\", \"ROOT\", \"compound\", \"punct\", \"nmod\", \"dobj\", \"punct\"],\n        },\n    ),\n    (\n        \"I like London and Berlin.\",\n        {\n            \"heads\": [1, 1, 1, 2, 2, 1],\n            \"deps\": [\"nsubj\", \"ROOT\", \"dobj\", \"cc\", \"conj\", \"punct\"],\n        },\n    ),\n]\nThanks in advance", "issue_status": "Open", "issue_reporting_time": "2020-01-30T13:38:31Z"}, "4": {"issue_url": "https://github.com/explosion/spaCy/issues/4954", "issue_id": "#4954", "issue_summary": "Underscore name spacing problem and pipeline reset feature request.", "issue_description": "JohnStuartRutledge commented 22 hours ago \u2022\nedited\nContext\nI work at a language learning company and am using SpaCy to build out a large suite of digital learning tools. Many of these tools allow the users (teachers in our case) to select options which load different language models and toggle on/off dozens of custom extensions. Many of these extensions are language specific and are process/memory intensive (e.g. some make external API calls or are designed to work on documents under 1000 words or less). Within the span of 5min it is not uncommon for teachers to switch between language models and toggle on/off multiple custom extensions, many of which are incompatible with each other (e.g. potential named entity collisions, etc).\nBefore committing to costly architectural changes we would like to push the limit on the number of languages/users we can support for each machine sitting behind our load-balancer. I.e., we would like to be able to support dozens of teachers working with various language model/extension combos on the same box simultaneously.\nProblem\nThe problem concerns the global nature of the Underscore object and is demonstrated below.\nimport en_core_web_sm\nimport es_core_news_sm\n\nfrom spacy.tokens.underscore import Underscore\n\nfrom myapp.custom_pipes.en import EnglishOnlyExtension\nfrom myapp.custom_pipes.es import SpanishOnlyExtension\n\nen_nlp = en_core_web_sm.load()\nes_nlp = es_core_news_sm.load()\n\ncustom_en_ext = EnglishOnlyExtension(en_nlp)\ncustom_es_ext = SpanishOnlyExtension(es_nlp)\nen_nlp.add_pipe(custom_en_ext)\nes_nlp.add_pipe(custom_es_ext)\n\nen_doc = en_nlp('Hello friend.')\nes_doc = es_nlp('Hola amigo.')\n\n# the fact these 2 assertions fail cause us all kinds of problems.\nassert en_doc.has_extension('custom_es_ext') is False\nassert es_doc.has_extension('custom_en_ext') is False\nQuestion\nDo you have any suggestions or elegant workarounds for how to deal the problems caused by the lack of name-spacing on the Underscore object?\nFeature Request\nWhat do you think of adding some sort of nlp.reset() feature that sets the model and all its extensions back to the \"factory default\"? Currently In my application I have been storing the initial state of the pipeline on load in the nlp.meta['pipeline_default'] so that I can easily reset without having to reload the model.\ndef toy_example_of_reset_logic(nlp):\n    # remove all custom pipes added after the initial load event\n    for name in nlp.pipe_names:\n        if name not in nlp.meta['default_pipe_names']:\n            nlp.remove_pipe(name)\n\n    # clear out all custom extensions\n    Underscore.doc_extensions = {}\n    Underscore.span_extensions = {}\n    Underscore.token_extensions = {}\n    return nlp", "issue_status": "Open", "issue_reporting_time": "2020-01-30T07:11:17Z"}, "5": {"issue_url": "https://github.com/explosion/spaCy/issues/4949", "issue_id": "#4949", "issue_summary": "Tokenizer fails to finish", "issue_description": "vid-koci commented 2 days ago\nHow to reproduce the behaviour\nThe text below was taken from Wikipedia, Spacy tokenizer (2.0.12) seems to get stuck and fails to process it.\nimport spacy\ns = list(open(\"err_sent.txt\"))[0]\ns\n'River Somme The Somme is a river in Picardy, northern France. The name \"\"Somme\"\" comes from a Celtic word meaning \u201ctranquility\u201d. The department Somme was named after this river. The river is long, from its source in the high ground of the former Forest of Arrouaise at Fonsommes near Saint-Quentin, to the Bay of the Somme, in the English Channel. It lies in the geological syncline which also forms the Solent. This gives it a fairly constant and gentle gradient. Hydrology. The river is characterized by a very gentle gradient and a steady flow. The valley is more or less steep-sided but its bottom is flat with fens and pools. These characteristics of steady flow and flooded valley bottom arise from the river's being fed by the ground water in the chalk basin in which it lies. At earlier, colder times, from the G\u00fcnz to the W\u00fcrm (Beestonian or Nebraskan to Devensian or Wisconsinian) the river has cut down into the Cretaceous geology to a level below the modern water table. The valley bottom has now therefore, filled with water which, in turn, has filled with fen. This picture, of the source of the Somme in 1986, shows it when the water table had fallen below the surface of the chalk in which the aquifer lies. Here, the flow of water had been sufficient to keep fen from forming. This satellite photograph shows the fenny valley crossing the chalk to the sea on the left. The sinuous length at the centre of the picture lies downstream from P\u00e9ronne. One of the fens, the \"\"Marais de l'\u00cele\"\" is a nature reserve in the town of St.Quentin. The traditional market gardens of Amiens, the \"\"Hortillonages\"\" are on this sort of land but drained. Once exploited for peat cutting, the fen is now used for fishing and shooting The construction of the \"\"Canal de la Somme\"\" began in 1770 and reached completion in 1843. It is long, beginning at St.Simon and opening into the Bay of the Somme. From St.Simon to Froissy (near Bray sur Somme, south of Albert), the canal is alongside the river. Thence to the sea, the river is partly river and partly navigation. From Abbeville, it is diverted through the silted, former estuary, to Saint-Valery-sur-Somme, where the maritime canal, once called the canal du Duc d'Angoul\u00eame enters the English Channel. The St Quentin Canal, famous for the 1918 battle, links the Somme to northern France and Belgium and southward to the Oise. The Canal du Nord also links the Somme to the Oise, at Noyon, thence to Paris. In 2001, the Somme valley was affected by particularly high floods, which were in large part due to a rise in the water table of the surrounding land. Flow-rate data (external links). Monthly flow rates (mean over 43 years). Catchment area . Flow rates at Hangest-sur-Somme. Daily flow rates compared with mean rates for the time of year at Hangest-sur-Somme (m\u00b3/s). Catchment area . 1993.1994. 1995. 1996. 1997. 1998. 1999. 2000.2001.2002.2003.2004.2005. Flow rates at P\u00e9ronne. Mean flow rates monthly and daily at P\u00e9ronne (m\u00b3/s). Catchment area . 1986.1987.1988.1989.1990.1991.1992.1993.1994.1995.1996.1997.1998.1999.2000.2001.2002.2003.2004.2005.\\n'\nnlp=spacy.load(\"en_core_web_sm\")\na = nlp(s)\nThe above command never finishes. When interrupted, it will always display the following path:\nFile \"tokenizer.pyx\", line 103, in spacy.tokenizer.Tokenizer.call\nFile \"tokenizer.pyx\", line 155, in spacy.tokenizer.Tokenizer._tokenize\nFile \"tokenizer.pyx\", line 172, in spacy.tokenizer.Tokenizer._split_affixes\nKeyboardInterrupt\nI'm happy to share the file with the text if copying the above text doesn't reproduce the error due to changes in the encoding.\nYour Environment\nspaCy version: 2.0.12\nPlatform: Linux-4.15.0-65-generic-x86_64-with-debian-stretch-sid\nPython version: 3.7.6\nModels: en_core_web_sm", "issue_status": "Open", "issue_reporting_time": "2020-01-29T11:56:17Z"}, "6": {"issue_url": "https://github.com/explosion/spaCy/issues/4948", "issue_id": "#4948", "issue_summary": "Transferring a the transformer model fined tuned using GPU to CPU", "issue_description": "ehsanasgari commented 2 days ago \u2022\nedited\nHi there,\nFirst of all, thanks for the great work. I am trying to pickle the fine tuned model over GPU. For this I need to transfer it to cpu otherwise I would get the following error:\nTypeError: can not serialize 'cupy.core.core.ndarray' object\nMay I ask if you know how can I transfer the GPU model to CPU so that I can pickle it?\nHow to reproduce the behaviour\nwith open('temp.pickle', 'wb') as handle:\npickle.dump(best_model, handle)\nYour Environment\nUbuntu", "issue_status": "Open", "issue_reporting_time": "2020-01-29T10:56:59Z"}, "7": {"issue_url": "https://github.com/explosion/spaCy/issues/4946", "issue_id": "#4946", "issue_summary": "Can't train_test_split with test_size < 0.32", "issue_description": "BlueRoss715 commented 3 days ago\nHey everyone!\nI am fairly new to python and much more to machine learning. I am currently working with spacy on Pycharm and I want to train and test a blank model with a certain dataset. The problem is that for some reason that I cannot understand I cant train_test_split with a test_size < 0.32 since it will display zero for all the scores and returns a empty list for the ents_per_type like shown in the link below when TESTING_DATA is used to evaluate the model with scorer.score:\nAlthough if i split the data to a test_size equal to 0.32 ou bigger then that i get normal results. This doesnt make any sense to me since I cant understand what would be the difference from 0.32 to 0.31..\nThe way I do it is like this:\nTOTAL_DATA = []\n\nTOTAL_DATA = plac.call(main)\n\nTRAIN_DATA, TESTING_DATA = train_test_split(TOTAL_DATA, test_size=0.3, random_state=42)\nI have been searching for some answer and didnt find anything related to my problem so I started printing the TRAIN_DATA and the TESTING_DATA and they both seem to be equal, but one has more information than the other. Because of that I was sure it was something inside the Scorer class more specifically scorer.score method that would be giving me this results so I started printing random stuff to see where was it not going through and it seems to be in this part of the score method :\nif \"-\" not in [token[-1] for token in gold.orig_annot]:\n            print(\"YES\")\n            # Find all NER labels in gold and doc\n            ent_labels = set([x[0] for x in gold_ents] + [k.label_ for k in doc.ents])\n            # Set up all labels for per type scoring and prepare gold per type\n            gold_per_ents = {ent_label: set() for ent_label in ent_labels}\n            for ent_label in ent_labels:\n                if ent_label not in self.ner_per_ents:\n                    self.ner_per_ents[ent_label] = PRFScore()\n                gold_per_ents[ent_label].update(\n                    [x for x in gold_ents if x[0] == ent_label]\n                )\nSo with my testing set being 0.32 or more it goes through it and prints \"YES\" but if i set the test_size to be less than 0.32 like for example 0.30 it will not go through which I think will result in empty results.\nSo if any of you might know whats happening I would greatly appreciate it!\nThanks in advance!\nPS:\nMy evaluation function is :\ndef evaluate(self, ner_model, examples):\n    scorer = Scorer()\n    for input_, annot in examples:\n        doc_gold_text = ner_model.make_doc(input_)\n        gold = GoldParse(doc_gold_text, entities=annot)\n        pred_value = ner_model(input_)\n        scorer.score(pred_value, gold)\n    return scorer.scores\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7.2\nspaCy Version Used: 2.2.1\nKeras = 2.3.1\nTensorflow: 2.0.0\nEnvironment Information: I am working on Pycharm", "issue_status": "Open", "issue_reporting_time": "2020-01-28T14:01:57Z"}, "8": {"issue_url": "https://github.com/explosion/spaCy/issues/4945", "issue_id": "#4945", "issue_summary": "nlp=spacy.load('en_core_web_sm') KeyError: 'PUNCTSIDE_FIN'", "issue_description": "ninulik82 commented 3 days ago\nCan someone please help?\nI installed spacy 2.2.3 using conda install -c conda-forge/label/gcc7 spacy and the en model 2.2.5 using conda install -c conda-forge spacy-model-en_core_web_sm on anaconda3. Installation was successful and the kernel was restarted.\nHowever, when I can import spacy - nlp=spacy.load('en_core_web_sm'), get the following KeyError: 'PUNCTSIDE_FIN'.\nI read all the blogs on this topic but could not find any fix for my issue.\noperating system - macOS\npython 3.7\nanaconda3 (anaconda navigator 1..9.7)\nspacy 2.2.3\nspacy model 2.2.5", "issue_status": "Open", "issue_reporting_time": "2020-01-27T21:52:04Z"}, "9": {"issue_url": "https://github.com/explosion/spaCy/issues/4944", "issue_id": "#4944", "issue_summary": "error: bad escape \\p at position 257", "issue_description": "utilizzatore commented 4 days ago\nI'm getting the error:\nError in py_call_impl(callable, dots$args, dots$keywords) : error: bad escape \\p at position 257\nrunning a python code with R through the use of \"reticulate\" package.\nThe issue comes up while executing the row:\nspacy.load('en_core_web_lg')\nI already checked with spacy validate that I have a version of spacy (v2.0.18) that agrees with the language model version (2.0.0).\nOf course the code runs fine if executed with python.", "issue_status": "Open", "issue_reporting_time": "2020-01-27T13:29:53Z"}, "10": {"issue_url": "https://github.com/explosion/spaCy/issues/4936", "issue_id": "#4936", "issue_summary": "Possible bug in spacy.gold.align", "issue_description": "Frojdholm commented 8 days ago\nThe b2a mapping is not one-to-one when the longer sequence is passed as tokens_a. Additionally the many-to-one mappings are only created when the longer sequence is passed as tokens_a.\nFrom my understanding similar to how a2b is explained in the documentation\nIf a2b[3] == 2, that means that tokens_a[3] aligns to tokens_b[2]. If there\u2019s no one-to-one alignment for a token, it has the value -1.\nthe same should be true for b2a. If b2a[2] == 3 then tokens_b[2] == tokens_a[3]\nHow to reproduce the behaviour\nThe following code shows the alignment using a2b.\nfrom spacy.gold import align\nimport numpy as np\n\nother_tokens = [\"obama\", \"'\", \"s\", \"podcast\"]\nspacy_tokens = [\"obama\", \"'s\", \"podcast\"]\n\ncost, a2b, b2a, a2b_multi, b2a_multi = align(other_tokens, spacy_tokens)\n\nfor a, b in enumerate(a2b):\n    if b != -1:\n        print(f\"other_tokens: {other_tokens[a]:>10}\\tspacy_tokens: {spacy_tokens[b]:>10}\")\n\n# Output:\n# other_tokens:      obama        spacy_tokens:      obama\n# other_tokens:    podcast        spacy_tokens:    podcast\nTesting b2a we get.\nfor b, a in enumerate(b2a):\n    if a != -1:\n        print(f\"other_tokens: {other_tokens[a]:>10}\\tspacy_tokens: {spacy_tokens[b]:>10}\")\n\n# Output:\n# other_tokens:      obama        spacy_tokens:      obama\n# other_tokens:          s        spacy_tokens:         's    <- tokens differ\n# other_tokens:    podcast        spacy_tokens:    podcast \nThe many-to-one mappings seem to be correct.\nprint(a2b_multi, b2a_multi)\n\n# Output:\n# {1: 1, 2: 1} {}\nHowever, if we switch other_tokens and spacy_tokens the one-to-one mappings are now correct, but the many-to-one mapping is empty.\n# switch other_tokens and spacy_tokens\ncost, a2b, b2a, a2b_multi, b2a_multi = align(spacy_tokens, other_tokens)\n\nfor a, b in enumerate(a2b):\n    if b != -1:\n        print(f\"other_tokens: {other_tokens[b]:>10}\\tspacy_tokens: {spacy_tokens[a]:>10}\")\n\n# Output:\n# other_tokens:      obama        spacy_tokens:      obama\n# other_tokens:    podcast        spacy_tokens:    podcast\nThis time, using b2a, the result is the same as using a2b.\nfor b, a in enumerate(b2a):\n    if a != -1:\n        print(f\"other_tokens: {other_tokens[b]:>10}\\tspacy_tokens: {spacy_tokens[a]:>10}\")\n\n# Output:\n# other_tokens:      obama        spacy_tokens:      obama\n# other_tokens:    podcast        spacy_tokens:    podcast\nMany-to-one mapping is now empty.\nprint(a2b_multi, b2a_multi)\n\n# Output:\n# {} {}\nYour Environment\nspaCy version: 2.2.3\nPlatform: Windows 10\nPython version: 3.7.6", "issue_status": "Open", "issue_reporting_time": "2020-01-22T22:30:01Z"}, "11": {"issue_url": "https://github.com/explosion/spaCy/issues/4934", "issue_id": "#4934", "issue_summary": "Changing conv_depth of pretrained model", "issue_description": "Contributor\nFallakAsad commented 9 days ago\nI am trying to change conv_depth of the pretrained model and then train it and save it. The training goes fine, however if I load the saved model, I see following error:\nAttributeError: 'Residual' object has no attribute 'G'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/src/Services/XXX/__init__.py\", line 483, in train_model\n    nlp = load_model(modelName)\n  File \"/src/Services/XXX/__init__.py\", line 593, in load_model\n    nlp = spacy.load(model_dir + \"/model\")\n  File \"/usr/lib64/python3.6/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 168, in load_model\n    return load_model_from_path(Path(name), **overrides)\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 211, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/usr/lib64/python3.6/site-packages/spacy/language.py\", line 878, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 670, in from_disk\n    reader(path / key)\n  File \"/usr/lib64/python3.6/site-packages/spacy/language.py\", line 873, in <lambda>\n    p, exclude=[\"vocab\"]\n  File \"nn_parser.pyx\", line 644, in spacy.syntax.nn_parser.Parser.from_disk\nValueError: [E149] Error deserializing model. Check that the config used to create the component matches the model being loaded.\nDo you think it is possible to change the conv_depth of pretrained model? Here is the part of code where I set conv_depth.\nnlp = spacy.load('de_core_news_md')\nner = nlp.get_pipe('ner')\noptimizer = nlp.begin_training(component_cfg={\"ner\": {\"conv_depth\": 15}})\n// Training code here\n// Saving model code here\nAfter saving the model cfg looks as follows:\n{\n  \"beam_width\":1,\n  \"beam_density\":0.0,\n  \"beam_update_prob\":1.0,\n  \"cnn_maxout_pieces\":3,\n  \"deprecation_fixes\":{\n    \"vectors_name\":\"de_core_news_md.vectors\"\n  },\n  \"nr_class\":106,\n  \"hidden_depth\":1,\n  \"token_vector_width\":96,\n  \"hidden_width\":64,\n  \"maxout_pieces\":2,\n  \"pretrained_vectors\":\"de_core_news_md.vectors\",\n  \"bilstm_depth\":0,\n  \"conv_depth\":15,\n  \"min_action_freq\":30\nYour Environment\nOperating System: openSUSE Leap 15.0\nPython Version Used: Python 3.6.9\nspaCy Version Used: 2.2.1\nEnvironment Information: not using GPU", "issue_status": "Open", "issue_reporting_time": "2020-01-22T09:21:53Z"}, "12": {"issue_url": "https://github.com/explosion/spaCy/issues/4923", "issue_id": "#4923", "issue_summary": "\"spacy train\" command - questions about some arguments", "issue_description": "CatarinaPC commented 11 days ago\nI have a few questions regarding the training of a model using the CLI.\n\n\nThere is an argument on the spacy train command named --n-iter which is described as the number of iterations.\nIs that the same as number of epochs?\nIs that the maximum number of iterations/epochs or there is only a maximum when the argument --n-early-stopping is set?\n\nThere is also the argument `--use-gpu` described as:\nWhether to use GPU. Can be either 0, 1 or -1.\nWhat does each value (0, 1 and -1) mean?\n\nWhat do the arguments --n_examples and --gold_preproc do?\n\n\nRegarding the hyperparameters for training how does the batch size work? Does it increase at each iteration? I don't understand how it fully works.\n\n\nAnd, are the steps described here performed (\"back-stage\" ) when the spacy train command is run?", "issue_status": "Open", "issue_reporting_time": "2020-01-20T02:56:27Z"}, "13": {"issue_url": "https://github.com/explosion/spaCy/issues/4922", "issue_id": "#4922", "issue_summary": "Loading a model under mod_wsgi hangs", "issue_description": "Edison-SK commented 13 days ago \u2022\nedited\nI am unable to use spacy integrated into a web service:\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nThe last statement hangs and apache returns timeout.\nMy environment is:\nOperating System: OpenSuSE Leap 15.1\nPython Version Used: 3.6.9\nspaCy Version Used: 2.2.3\nEnvironment Information:\napache, mod_wsgi, cherrypy,\napache configuration is set to 1 process and 1 thread\nI can use spacy in a traditional python script without any problems.", "issue_status": "Open", "issue_reporting_time": "2020-01-17T21:09:49Z"}, "14": {"issue_url": "https://github.com/explosion/spaCy/issues/4919", "issue_id": "#4919", "issue_summary": "Crashing when processing single word - built from 2.2.3 sources", "issue_description": "Lupotslboy commented 15 days ago\nJust as described in #3034: this is happening on spaCy built from 2.2.3 sources\nThe installation has been done running\n    pip install -r requirements.txt && \\\n    python3 setup.py build_ext --inplace && \\\n    pip install -e . \nThe author of #3034 eventually commented it was due to some gcc8 flag, but I'm not too sure what needs to change in the above install commands.\nHow to reproduce the behaviour\nimport spacy\nnlp = spacy.load('en')\nnlp(\"Word\")\nThis will result in\n/usr/include/c++/8/bits/stl_vector.h:932: std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::operator[](std::vector<_Tp, _Alloc>::size_type) [with _Tp = __pyx_t_5spacy_6syntax_6_state_StateC*; _Alloc = std::allocator<__pyx_t_5spacy_6syntax_6_state_StateC*>; std::vector<_Tp, _Alloc>::reference = __pyx_t_5spacy_6syntax_6_state_StateC*&; std::vector<_Tp, _Alloc>::size_type = long unsigned int]: Assertion '__builtin_expect(__n < this->size(), true)' failed.\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-4.4.0-166-generic-x86_64-with-redhat-8.1-Ootpa\nPython version: 3.6.8\nModels: es, en", "issue_status": "Open", "issue_reporting_time": "2020-01-16T16:44:38Z"}, "15": {"issue_url": "https://github.com/explosion/spaCy/issues/4918", "issue_id": "#4918", "issue_summary": "Documentation/specification of vocabulary file for spacy init-model", "issue_description": "emiguevara commented 15 days ago \u2022\nedited\nI am trying to make a custom model for spacy 2.2. This involves using the latest init-model, which requires a JSONL vocabulary file.\nI have done this in the past, using v. 2.0, and then all that was needed was a frequency list, clusters and embeddings model (deprecated options --freqs-loc and --clusters-loc).\nHow can I make my own vocabulary file? Is it possible to extract it from older models? Is there a dedicated script for this?\nAre the vocabulary files for the official models available?\nHow are the \"oov_prob\" and \"prob\" values defined?\nAre there any extensive specifications for the lexicon format? I am thinking about obligatory and optional fields, default values, etc.\nWhich page or section is this issue related to?\nhttps://spacy.io/api/cli#init-model\nhttps://spacy.io/api/annotation#vocab-jsonl\nhttps://spacy.io/usage/adding-languages#vocab-file", "issue_status": "Open", "issue_reporting_time": "2020-01-16T09:35:36Z"}, "16": {"issue_url": "https://github.com/explosion/spaCy/issues/4917", "issue_id": "#4917", "issue_summary": "TypeError: Unsupported type <class 'numpy.ndarray'>", "issue_description": "pyshahid commented 15 days ago \u2022\nedited\nHow to reproduce the behaviour\nhi i am using spacy2.2.3 with gpu enabled then i tried to use displacy.render function i got error\ncode that i used is like below\nspacy.require_gpu()\nimport pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nimport random\nfrom spacy.util import minibatch,compounding\nfrom spacy import displacy\n\nspacy.util.use_gpu(0)\ndf = pd.read_json(\"data_point_section_dataset.json\",lines=True)\nprint(len(df))\ndf = df[df[\"doc_type\"]==\"doc_type\"]\ndf = df[df[\"user_role\"]==\"special\"]\nprint(len(df))\nmodel_path = \"spacy_2_2_3\"\ndef populate_train_data(df):         \n    train_data = []         \n    for d_index, row in df.iterrows():             \n        content = row[\"annotations\"].replace(\"\\\\n\", \"\\n\").replace(\"\\n\", \" \")             \n        content = re.sub(r\"(?<=[:])(?=[^\\s])\", r\" \", content)             \n        # Finding tags and entities and store values in a entity list-----             \n        soup = BeautifulSoup(content, \"html.parser\")             \n        text = soup.get_text()             \n        entities = []             \n        for tag in soup.find_all():                 \n            if tag.string is None:                     \n                # failing silently for invalid tag                     \n                print(f'Tagging is invalid: {row[\"_id\"], tag.name}, skipping..')                     \n                continue                 \n\n            tag_index = content.split(str(tag))[0].count(tag.string)                 \n            try:                     \n                for index, match in enumerate(re.finditer(re.escape(tag.string), text)):                         \n                    if index == tag_index:                             \n                        entities.append((match.start(), match.end(), tag.name))                 \n            except Exception as e:                     \n                print(e)                     \n                continue             \n        if entities:                 \n            train_data.append((text, {\"entities\": entities}))\n\n    return train_data\n\ndef _train(train_data):         \n    nlp = spacy.load(\"en_core_web_sm\")         \n    if \"ner\" not in nlp.pipe_names:             \n        ner = nlp.create_pipe(\"ner\")             \n        nlp.add_pipe(ner, last=True)         \n    else:             \n        ner = nlp.get_pipe(\"ner\")         \n    for _, annotations in train_data:             \n        for ent in annotations.get(\"entities\"):                 \n            ner.add_label(ent[2])         \n    optimizer = nlp.begin_training()         \n    for i in range(20):             \n        random.shuffle(train_data)\n        correct = 1\n        batches = minibatch(train_data)             \n        for batch in batches:\n            texts, annotations = zip(*batch)                 \n            nlp.update(texts, annotations, sgd=optimizer)         \n    return nlp\n\ndef predict(text, expected_dps):         \n    try:             \n        nlp = spacy.load(model_path)         \n    except OSError:             \n        raise ModelNotFoundError(f\"Model not found. :{self.type}\")         \n    text = text.replace(\"\\n\", \" \")         \n    doc = nlp(text)         \n    entities = []         \n    for entity in doc.ents:             \n        if entity.label_ in expected_dps:                 \n            data = {                     \n                \"label\": entity.label_,                     \n                \"value\": entity.text,                     \n                \"start_index\": entity.start_char,                     \n                \"end_index\": entity.end_char,                 \n            }                 \n            entities.append(data)         \n    \n    return entities,doc\ndef train():\n    train_data = populate_train_data(df)\n    nlp = _train(train_data)\n    nlp.to_disk(model_path)\n    expected_dps = [\"claim_date_claim_form\",\"injury_date_claim_form\",\"start_injury_claim_form\",\"end_injury_claim_form\",\"injuries_claim_form\",\"app_address_claim_form\",\"injury_report_date_claim_form\"]\n    dps,dps_for_html = predict(text=\"the text i want to predict\",expected_dps)\n    print(dps)\n    return \"\"\n\n\ntrain()\n\n\nsoup = BeautifulSoup(text,\"html.parser\")\ntext = soup.get_text()\nexpected_dps = [\"claim_date_claim_form\",\"injury_date_claim_form\",\"start_injury_claim_form\",\"end_injury_claim_form\",\"injuries_claim_form\",\"app_address_claim_form\",\"injury_report_date_claim_form\"]\ndps,dps_for_html = predict(text=\"the text i want to predict\",expected_dps)\nprint(\"hello\")\nprint(dps)\nhtml = displacy.render(dps_for_html.sents,style=\"ent\")\nprint(html)```\n\nbut i am getting an error\n\n\nas follows\n\n```Traceback (most recent call last):\n  File \"train_test.py\", line 112, in <module>\n    html = displacy.render(dps_for_html.sents,style=\"ent\")\n  File \"/usr/local/lib/python3.6/site-packages/spacy/displacy/__init__.py\", line 46, in render\n    docs = [obj if not isinstance(obj, Span) else obj.as_doc() for obj in docs]\n  File \"/usr/local/lib/python3.6/site-packages/spacy/displacy/__init__.py\", line 46, in <listcomp>\n    docs = [obj if not isinstance(obj, Span) else obj.as_doc() for obj in docs]\n  File \"span.pyx\", line 232, in spacy.tokens.span.Span.as_doc\n  File \"span.pyx\", line 192, in __iter__\n  File \"cupy/core/core.pyx\", line 948, in cupy.core.core.ndarray.__add__\n  File \"cupy/core/_kernel.pyx\", line 886, in cupy.core._kernel.ufunc.__call__\n  File \"cupy/core/_kernel.pyx\", line 90, in cupy.core._kernel._preprocess_args\nTypeError: Unsupported type <class 'numpy.ndarray'>\nthis only happens with gpu enabled if remove require_gpu function dispalcy works well .\nYour Environment\nOperating System:ubuntu 18.04\nPython Version Used:3.6\nspaCy Version Used:2.2.3\nEnvironment Information:## Info about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.4.0-1100-aws-x86_64-with-debian-stretch-sid\nPython version: 3.6.0", "issue_status": "Open", "issue_reporting_time": "2020-01-16T07:04:50Z"}, "17": {"issue_url": "https://github.com/explosion/spaCy/issues/4916", "issue_id": "#4916", "issue_summary": "Spacy 2.2.3 gpu prediction gives less accurate result than spacy 2.0.18 cpu", "issue_description": "pyshahid commented 15 days ago\nHow to reproduce the behaviour\n1.Train a model for ner with spacy 2.0.18\n2.Check prediction\n3.Train a model for ner with spacy 2.2.3 with gpu\n4.Check prediction\nYour Environment\nOperating System:Ubuntu 18\nPython Version Used:3.6\nspaCy Version Used:2.2.3 and 2.0.18\nEnvironment Information:Docker\nPretrained Model used:en_core_web_sm", "issue_status": "Open", "issue_reporting_time": "2020-01-16T04:17:01Z"}, "18": {"issue_url": "https://github.com/explosion/spaCy/issues/4912", "issue_id": "#4912", "issue_summary": "How do I train sentence splitter without training DEP parser?", "issue_description": "buriy commented 16 days ago \u2022\nedited\nUD dep corpus is rather small, is made of single sentences per paragraph, and I'd like to train sentence splitter from DEP parser on arbitrary sentences, say, from other data sources containing sentence breaks.\nHow do I do it?\nWhich page or section is this issue related to?\nTraining? Training DEP?", "issue_status": "Open", "issue_reporting_time": "2020-01-15T12:00:09Z"}, "19": {"issue_url": "https://github.com/explosion/spaCy/issues/4907", "issue_id": "#4907", "issue_summary": "Any plan to make version >= 2.1.0 available on ppc64le?", "issue_description": "Xiaoming-Zhao commented 17 days ago\nHi, I am wondering whether there are some plans to make spacy>=2.1.0 available on ppc64le? Several popular packages, such as allennlp, require spacy>=2.1.0.\nI have searched several channels but could not find spacy>=2.1.0. May I have some suggestions about how to utilize spaCy on ppc64le? Thanks a lot!\nHow to reproduce the problem\nAny command to install spacy>=2.1.0\nYour Environment\nOperating System:\nNAME=\"Red Hat Enterprise Linux Server\"\nVERSION=\"7.6 (Maipo)\"\nID=\"rhel\"\nID_LIKE=\"fedora\"\nVARIANT=\"Server\"\nVARIANT_ID=\"server\"\nVERSION_ID=\"7.6\"\nPRETTY_NAME=\"Red Hat Enterprise Linux\"\nANSI_COLOR=\"0;31\"\nCPE_NAME=\"cpe:/o:redhat:enterprise_linux:7.6:GA:server\"\nHOME_URL=\"https://www.redhat.com/\"\nBUG_REPORT_URL=\"https://bugzilla.redhat.com/\"\n\nREDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 7\"\nREDHAT_BUGZILLA_PRODUCT_VERSION=7.6\nREDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7.6\"\nPython Version Used: have tried with Python 3.6 and 3.7\nspaCy Version Used: wish to install spacy>=2.1.0", "issue_status": "Open", "issue_reporting_time": "2020-01-13T22:54:34Z"}, "20": {"issue_url": "https://github.com/explosion/spaCy/issues/4906", "issue_id": "#4906", "issue_summary": "Noun Phrase Chunking - Improvements in recall.", "issue_description": "naveenjafer commented 17 days ago\nFeature description\nNoun Phrase chunking Precision and Recall.\nCould the feature be a custom component or spaCy plugin?\nIt would be a simple modification to existing noun phrase chunker https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py#L7\n@adrianeboyd had pointed me to https://www.clips.uantwerpen.be/conll2000/chunking/ to experiment with Noun Phrase chunking in spaCy. Going by the test.txt file in the CoNLL 2000 dataset as the ground truth for NP chunking, spaCy's NP chunker had the below performance.\nEvaluation Metric Value (%)\nPrecision 92.60\nRecall 80.83\nF Score 86.32\nAfter Simple modifications to the spaCy NP iterator after analysis of the cases that were responsbile for the low recall, the results are as follows.\nEvaluation Metric Value (%)\nPrecision 91.46\nRecall 88.64\nF Score 90.03\nSome of the phrases that the chunker could'nt handle are as below\nA company spokesman said yesterday that Coca-Cola Enterprises sticks by its 1989 forecast.\n\"yesterday\" should have been a valid noun phrase that was missed out on. Spacy misses out occurences of Nouns with npadvmod dependency - Fixed by including it in the labels.\nNet loss : $ 1.7 million vs. net income : $ 21.2 million ; or 12 cents a share\n12 cents is picked up as a valid NP but $ 1.7 million is not. Fixed by including NUM in POS that initiate the search for NP phrases.\nWhen bank financing for the buy-out collapsed last week, so did UAL's stock.\n\"last week\" is a valid NP that is not identified. Same as example 1 of npadvmod dependencies.\nMr. Wolf owns 75,000 UAL shares and has options to buy another 250,000 at $ 83.3125 each.\n\"another 250,000\" is missed by spaCy. The changes in example 2 fixes the issue here too.\nCase that needs a discussion\nGreat American said it increased its loan-loss reserves by $ 93 million after reviewing its loan portfolio, raising its total loan and real estate reserves to $ 217 million.\nThe ground truth has considered \"its total loan and real estate reserves\" as one single NP. spaCy splits them into 2. I am unsure as to which one of them is more desirable. This has got to do with a Coordinating conjunction in between 2 NP phrases.\nI can go ahead and raise a MR with the changes and the new test cases that will have to be included if someone can review these cases and let me know if this was an intended spaCy behavioral deviation from the CoNLL 2000 ground truth.", "issue_status": "Open", "issue_reporting_time": "2020-01-13T18:47:34Z"}, "21": {"issue_url": "https://github.com/explosion/spaCy/issues/4903", "issue_id": "#4903", "issue_summary": "Multiprocessing in pipe() not working with custom attributes", "issue_description": "Contributor\nBramVanroy commented 18 days ago \u2022\nedited\nI am using custom attributes and a custom pipeline for the first time, so it might very well be that there is a mistake on my part. However, the code works fine when not using the n_process argument.\nThe problem: using nlp.pipe(text, n_process=2) will throw an AttributeError complaining that I am assiging a value to an unregistered extension attribute. The error is not thrown without the n_process argument.\nTrace:\nProcess Process-1:\nTraceback (most recent call last):\n  File \"C:\\Python\\Python37\\Lib\\multiprocessing\\process.py\", line 297, in _bootstrap\n    self.run()\n  File \"C:\\Python\\Python37\\Lib\\multiprocessing\\process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\bmvroy\\.virtualenvs\\spacy_conll-PtwHJ_vN\\lib\\site-packages\\spacy\\language.py\", line 1124, in _apply_pipes\n    sender.send([doc.to_bytes() for doc in docs])\n  File \"C:\\Users\\bmvroy\\.virtualenvs\\spacy_conll-PtwHJ_vN\\lib\\site-packages\\spacy\\language.py\", line 1124, in <listcomp>\n    sender.send([doc.to_bytes() for doc in docs])\n  File \"nn_parser.pyx\", line 248, in pipe\n  File \"C:\\Users\\bmvroy\\.virtualenvs\\spacy_conll-PtwHJ_vN\\lib\\site-packages\\spacy\\util.py\", line 481, in minibatch\n    batch = list(itertools.islice(items, int(batch_size)))\n  File \"C:\\Users\\bmvroy\\.virtualenvs\\spacy_conll-PtwHJ_vN\\lib\\site-packages\\spacy\\language.py\", line 1106, in _pipe\n    doc = proc(doc, **kwargs)\n  File \"C:\\Users\\bmvroy\\.PyCharm2019.2\\config\\scratches\\scratch_33.py\", line 15, in __call__\n    sent._.set('my_ext', sent_ext)\n  File \"C:\\Users\\bmvroy\\.virtualenvs\\spacy_conll-PtwHJ_vN\\lib\\site-packages\\spacy\\tokens\\underscore.py\", line 71, in set\n    return self.__setattr__(name, value)\n  File \"C:\\Users\\bmvroy\\.virtualenvs\\spacy_conll-PtwHJ_vN\\lib\\site-packages\\spacy\\tokens\\underscore.py\", line 63, in __setattr__\n    raise AttributeError(Errors.E047.format(name=name))\nAttributeError: [E047] Can't assign a value to unregistered extension attribute 'my_ext'. Did you forget to call the `set_extension` method?\nMy guess would be that n_process creates new processes that recreate the nlp instance, but does not reinitialize its pipes - but that's just a guess.\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.tokens import Span, Doc\n\nclass CustomPipe:\n    name = 'my_pipe'\n\n    def __init__(self):\n        Span.set_extension('my_ext', getter=self._get_my_ext)\n        Doc.set_extension('my_ext', default=None)\n\n    def __call__(self, doc):\n        gathered_ext = []\n        for sent in doc.sents:\n            sent_ext = self._get_my_ext(sent)\n            sent._.set('my_ext', sent_ext)\n            gathered_ext.append(sent_ext)\n\n        doc._.set('my_ext', '\\n'.join(gathered_ext))\n\n        return doc\n\n    @staticmethod\n    def _get_my_ext(span):\n        return str(span.end)\n\n\nif __name__ == '__main__':\n    nlp = spacy.load('en_core_web_sm')\n    custom_component = CustomPipe()\n    nlp.add_pipe(custom_component, after='parser')\n\n    text = ['I like bananas.', 'Do you like them?', 'No, I prefer wasabi.']\n    # works without 'n_process' \n    for doc in nlp.pipe(text, n_process=2):\n        print(doc)\nYour Environment\nspaCy version: 2.2.3\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.3\nModels: en", "issue_status": "Open", "issue_reporting_time": "2020-01-13T15:00:29Z"}, "22": {"issue_url": "https://github.com/explosion/spaCy/issues/4897", "issue_id": "#4897", "issue_summary": "error while using spacy on docker", "issue_description": "RajatChaudhari commented 21 days ago\nHello,\nI am facing issues while running my docker image built with python 3.7 and spacy 2.2.1,\ngetting the below error:\nTraceback (most recent call last):\nFile \"./app.py\", line 2, in\nimport spacy\nFile \"C:\\Python\\lib\\site-packages\\spacy_init_.py\", line 12, in\nfrom .cli.info import info as cli_info\nFile \"C:\\Python\\lib\\site-packages\\spacy\\cli_init_.py\", line 6, in\nfrom .train import train # noqa: F401\nFile \"C:\\Python\\lib\\site-packages\\spacy\\cli\\train.py\", line 17, in\nfrom ..gold import GoldCorpus\nFile \"strings.pxd\", line 23, in init spacy.gold\nImportError: DLL load failed: The specified module could not be found.\nmy docker file is -\nFROM python:3.7-windowsservercore-ltsc2016\nCOPY ./* /app/\nWORKDIR /app\nRUN pip install --upgrade setuptools\nRUN pip install -r ./requirements.txt\nCMD [ \"python\", \"./app.py\" ]\nthe build is successful but I get error when I do docker run", "issue_status": "Open", "issue_reporting_time": "2020-01-10T05:13:01Z"}, "23": {"issue_url": "https://github.com/explosion/spaCy/issues/4896", "issue_id": "#4896", "issue_summary": "doc similarity is different between GPU version and CPU version", "issue_description": "omri374 commented 22 days ago\nHow to reproduce the behaviour\nIntroducing GPU makes doc.similarity return <class 'cupy.core.core.ndarray'> of size 1 instead of a scalar. On CPU the same call returns a scalar.\nReproduce:\nimport spacy\nnlp1 = spacy.load(\"en_vectors_web_lg\")\ndoc1 = nlp1(\"Hey there how are you?\")\ndoc2 = nlp1(\"I'm good and you?\")\ndoc1.similarity(doc2)\n0.9182046417319748\nThen, when requiring GPU:\nspacy.require_gpu()\nTrue\nnlp2 = spacy.load(\"en_vectors_web_lg\")\ndoc1 = nlp2(\"Hey there how are you?\")\ndoc2 = nlp2(\"I'm good and you?\")\ndoc1.similarity(doc2)\narray(0.9182048, dtype=float32)\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-4.15.0-1064-azure-x86_64-with-debian-stretch-sid\nPython version: 3.7.5", "issue_status": "Open", "issue_reporting_time": "2020-01-09T16:16:27Z"}, "24": {"issue_url": "https://github.com/explosion/spaCy/issues/4886", "issue_id": "#4886", "issue_summary": "Questions about training a model", "issue_description": "CatarinaPC commented 24 days ago\nI had a couple questions I hope someone can answer (also hope I'm posting this at the right place or with the right tag):\nShould one use cli or python code to train a model (more specifically a NER model)? Using python code seems more complicated. Are there any advantages to it?\nI read that hyper-parameters can be changed when using cli, is it possible to do the same but in python? Or is that one of the disadvantages regarding the first question?\nIs there any way to perform hyper-parameter tuning/selection? I couldn't find anything regarding this. if not, is it possible to use another library combined with SpaCy to achieve this?\nWhat is the purpose of the dev data used with the train command on the cli? Or, what are the changes that are made at each iteration in which the dev data is used to evaluate?\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-5.0.0-37-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8", "issue_status": "Open", "issue_reporting_time": "2020-01-06T20:59:11Z"}, "25": {"issue_url": "https://github.com/explosion/spaCy/issues/4871", "issue_id": "#4871", "issue_summary": "AttributeError: 'FunctionLayer' object has no attribute 'W'", "issue_description": "danielvasic commented 28 days ago \u2022\nedited\nHello, I have a problem using pretrained vectors with command line API. Steps to reproduce:\npython -m spacy pretrain fulltext.jsonl vectors/hr_vectors_web_md models/hr/language --use-vectors --use-char --dropout 0.3 --n-iter 60\nAfter that trying to train NER tagger with this command:\npython -m spacy train hr models/ner train-ner.json dev-ner.json -v vectors/hr_vectors_web_md --init-tok2vec models/hr/language/model59.bin -p ner\nProduces:\nTraceback (most recent call last):\nFile \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\nexec(code, run_globals)\nFile \"/usr/local/lib/python3.6/dist-packages/spacy/main.py\", line 33, in\nplac.call(commands[command], sys.argv[1:])\nFile \"/usr/local/lib/python3.6/dist-packages/plac_core.py\", line 328, in call\ncmd, result = parser.consume(arglist)\nFile \"/usr/local/lib/python3.6/dist-packages/plac_core.py\", line 207, in consume\nreturn cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"/usr/local/lib/python3.6/dist-packages/spacy/cli/train.py\", line 244, in train\ncomponents = _load_pretrained_tok2vec(nlp, init_tok2vec)\nFile \"/usr/local/lib/python3.6/dist-packages/spacy/cli/train.py\", line 551, in _load_pretrained_tok2vec\ncomponent.tok2vec.from_bytes(weights_data)\nFile \"/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\", line 375, in from_bytes\ndest = getattr(layer, name)\nAttributeError: 'FunctionLayer' object has no attribute 'W'\nI'm using Google Colaboratory enviroment.\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.9", "issue_status": "Open", "issue_reporting_time": "2020-01-03T15:04:56Z"}, "26": {"issue_url": "https://github.com/explosion/spaCy/issues/4868", "issue_id": "#4868", "issue_summary": "Include pxd in source tarball", "issue_description": "bollwyvl commented 28 days ago\nFeature description\nIt would be lovely to have the pxd files included in some official distribution, so that other packages can confidently build against spacy. I'm looking to make a conda package for neuralcoref, and my approach of merging the pypi tarball (preferred) and the github tarball (which has the pxd, but probably isn't tested) gives me the willies .\nCould the feature be a custom component or spaCy plugin?\nPretty much would have to be on the mainline. Luckily, I'd imagine it's a one-liner to MANIFEST.in, but I don't know enough about the build chain to know if that's A Bad Idea for some reason... other than size, perhaps? If there's no obvious red flags, I'll PR said line!", "issue_status": "Open", "issue_reporting_time": "2020-01-03T02:19:16Z"}, "27": {"issue_url": "https://github.com/explosion/spaCy/issues/4867", "issue_id": "#4867", "issue_summary": "Distilling NER from transformer based models into spacy", "issue_description": "mejobhoot commented 28 days ago \u2022\nedited\nI am trying to distil NER knowledge from transformer based models into Spacy (original, non-transformer offering V2.0+). I am doing this to improve accuracy keeping Spacy's speed. The target is to use the distilled model in production applications. There are 2 changes to make ..\nInstead of one-hot labels, need to pass high-T softmax outputs for all labels for each token from an external model. I guess I will have to change get_batch_loss method in nn_parser.pyx?\nChange the softmax on the 'upper' (affine) model to use high-T softmax. This change will probably be made in thinc/neural/_classes/affine.py?\nI dont see anything in the internet that tells me this was ever done before, I am wondering why! Are the above the right locations to make changes and has anybody tried to see if this works?\nYour Environment\nOperating System: Windows\nPython Version Used: 3.6.9\nspaCy Version Used: 2.2.3\nEnvironment Information:\nRegards.", "issue_status": "Open", "issue_reporting_time": "2020-01-02T18:52:11Z"}, "28": {"issue_url": "https://github.com/explosion/spaCy/issues/4866", "issue_id": "#4866", "issue_summary": "Displacy cannot show or colour non uppercase labels.", "issue_description": "johann-petrak commented 28 days ago\nOperating System: Linux-4.15.0.-70 x86_64 debian\nPython Version Used: 3.6.9\nspaCy Version Used: 2.2.1\nI create my own entities but I use CamelCase entity labels like \"ThisEntity\" \"AnotherOne\".\nDisplacy renders those as uppercase and does not match against options that map the original label to a colour but again requires a map where the labels are converted to uppercase.\nI do not see why displacy forces the use of uppercase onto the user: this is very limiting and makes it impossible to use better readable labels. On the other hand if spacy just uses whatever the label is without converting to uppercase, apps and implementations are still free to choose uppercase.", "issue_status": "Open", "issue_reporting_time": "2020-01-02T18:08:45Z"}, "29": {"issue_url": "https://github.com/explosion/spaCy/issues/4864", "issue_id": "#4864", "issue_summary": "Change the way spacy works - Custom properties for training and prediction", "issue_description": "SandeepBhutani commented 29 days ago\nHi,\nI originally posted this question on stackexchange but haven't got any answer yet.\nchange-the-way-spacy-works-custom-properties-for-training-and-prediction\nCan someone please suggest on this:\nSpacy detects the entities using its predefined algorithm. It parses tokens in text considering position of tokens with respect to tokens surrounding it. It also takes into consideration the POS tagging for these tokens.\nHowever, I believe it misses the position of tokens above and below (For example in a tabular data) or it also misses few properties of text like if it is underline etc. This statement is based on my understanding. Please correct me if I am wrong in these.\nNow the question is, can such properties be taken into consideration while doing training and prediction of entities? I have seen Extension Attributes, but these do not play role during training and prediction but work as meta data.", "issue_status": "Open", "issue_reporting_time": "2020-01-02T14:13:30Z"}, "30": {"issue_url": "https://github.com/explosion/spaCy/issues/4858", "issue_id": "#4858", "issue_summary": "dynamically adjust batch_size or provide batch_size in bytes in nlp.pipe", "issue_description": "Contributor\nAlJohri commented on 1 Jan 2020\nFeature description\nThe goal is optimally use resources when using nlp.pipe in a multiprocessing setting. Currently, in order to adjust batch_size it requires multiple calls to nlp.pipe with manually partitioned documents. This has the overhead of forking processes or re-loading models in each process (not sure how its currently implemented) for every invocation of nlp.pipe.\nI would like a way to dynamically adjust the batch_size based on the document length. If I could provide the batch_size as bytes this would achieve the same affect.\nI'm currently having a lot of trouble with this as I need to manually adjust both batch_size AND n_processes given the diverse document lengths in my corpus. Some documents require using 1/3 or 1/4 of the maximum concurrency at a batch_size of 1 while others can use max concurrency with a batch_size of 15-20. It would be great to have a way to more easily handle this.\nP.S. I am using neuralcoref in case that impacts the memory consumption calculations.\nCould the feature be a custom component or spaCy plugin?\nI don't think so?", "issue_status": "Open", "issue_reporting_time": "2019-12-31T20:54:46Z"}, "31": {"issue_url": "https://github.com/explosion/spaCy/issues/4854", "issue_id": "#4854", "issue_summary": "displaCy dependency tree labels backwards (and upside down) in RTL languages in certain browsers", "issue_description": "Contributor\nerip commented on 31 Dec 2019\nHow to reproduce the behaviour\nReproducing this is difficult without a depencency parse available for an RTL language, but I've included a link to HTML with the errant RTL. The link is here.\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Darwin-19.2.0-x86_64-i386-64bit\nPython version: 3.6.7", "issue_status": "Open", "issue_reporting_time": "2019-12-30T19:14:29Z"}, "32": {"issue_url": "https://github.com/explosion/spaCy/issues/4829", "issue_id": "#4829", "issue_summary": "please add the build dependencies back to pyproject.toml", "issue_description": "Contributor\nAlJohri commented on 22 Dec 2019\nPlease add the build dependencies back to pyproject.toml or remove the file altogether. I need to install the package from source and I want to do it in a single step using pip install git+https://....\nAs you can see below, I forked the repo and added the build dependencies back in which fixed the installation via pip.\nHow to reproduce the problem\nFAILS:\n.venv/bin/pip install git+https://github.com/explosion/spaCy\nWORKS:\n.venv/bin/pip install git+https://github.com/AlJohri/spaCy\nMy fork has a single commit which adds the build dependencies back to pyproject.toml: AlJohri@8156172\nYour Environment\n$ .venv/bin/python -m spacy info --markdown\nspaCy version: 2.2.3\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.5\ncc @ines\n1", "issue_status": "Open", "issue_reporting_time": "2019-12-21T21:56:55Z"}, "33": {"issue_url": "https://github.com/explosion/spaCy/issues/4821", "issue_id": "#4821", "issue_summary": "spacy gives Segmentation fault (core dumped) when trying to use gpu", "issue_description": "jlealtru commented on 19 Dec 2019\nSpacy 2.2.3 with GPU not working\nWhen I tried to use the most recent version of spacy (2.2.3) with the gpu from python on the terminal I get the error Segmentation fault (core dumped). When I try to run the same command from Jupyter notebook it crashes the kernel. Below the code:\nimport spacy\nspacy.prefer_gpu()\nI installed spacy with gpu support:\npip install -U spacy[cuda100]\nI know there is not a lot of info about this error but it may be related to #1757?\nAlso spacy seems to be working when not setting prefer_gpu\nYour Environment\nOperating System: Linux-4.15.0-70-generic-x86_64-with-Ubuntu-18.04-bionic\nPython Version Used: 3.6.9\nspaCy Version Used: 2.2.3\nEnvironment Information: conda\nModel: en, en_core_web_md, en_core_web_lg, es_core_news_md", "issue_status": "Open", "issue_reporting_time": "2019-12-19T05:27:45Z"}, "34": {"issue_url": "https://github.com/explosion/spaCy/issues/4819", "issue_id": "#4819", "issue_summary": "spacy pretrain CLI fails with custom parameters", "issue_description": "kormilitzin commented on 18 Dec 2019\nI am using spacy pretrain CLI command to train -t2v to further use for spacy train, howeer, if I change the parameters (network size) in spacy pretrain, I can't then use the pretrained weights with spacy train`.\nSpecifically:\nThis combination work OK:\n$ python -m spacy pretrain /data en_vectors_web_lg /spacy_pretrain_vectors_01 --use-vectors -i 1\n$ python -m spacy train en ./cli_models/tmp_01 /data_train /data_dev -t2v spacy_pretrain_vectors_01/model0.bin -p ner -g 0 \nIf I create a large embeddings, it is still OK:\n$ python -m spacy pretrain /data en_vectors_web_lg /spacy_pretrain_vectors_02 --use-vectors -i 1 -cw 128\n$ token_vector_width=128 python -m spacy train en ./cli_models/tmp_02 /data_train /data_dev -t2v spacy_pretrain_vectors_02/model0.bin -p ner -g 0\nHowever, if I change the depth, then it fails:\n$ python -m spacy pretrain /data en_vectors_web_lg /spacy_pretrain_vectors_03 --use-vectors -i 1 -cd 8\n$ python -m spacy train en ./cli_models/tmp_03 /data_train /data_dev -t2v spacy_pretrain_vectors_03/model0.bin -p ner -g 0\nwith the following error:\nAttributeError: 'HashEmbed' object has no attribute 'G'\nif I include LSTM in pretraining then it also fails:\n$ python -m spacy pretrain /data en_vectors_web_lg /spacy_pretrain_vectors_04 --use-vectors -i 1 -lstm 2\n$ python -m spacy train en ./cli_models/tmp_04 /data_train /data_dev -t2v spacy_pretrain_vectors_04/model0.bin -p ner -g 0\nwith the following error:\nTypeError: byte indices must be integers or slices, not bytes\nI guess I am missing some parameters that should be included in spacy training to account for new parameters values.\nThanks!\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.15.0-70-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nModels: en", "issue_status": "Open", "issue_reporting_time": "2019-12-18T17:18:36Z"}, "35": {"issue_url": "https://github.com/explosion/spaCy/issues/4818", "issue_id": "#4818", "issue_summary": "NotImplementedError: cannot instantiate 'WindowsPath' on your system", "issue_description": "ValentinCalomme commented on 18 Dec 2019\nI am training a SpaCy text categorizer on my Windows machine as part of an ML pipeline. Once training is over, the whole pipeline is pickled and then persisted. When the pipeline needs to be loaded, it is unpickled. This works perfectly fine on my machine, the nlp object is pickled and unpickled without any issues.\nHowever, when I try to unpickle the same pickle file within an Alpine Docker container, I get the following error when trying to unpickle my ML pipeline:\n     return pickle.loads(state)\n   File \"/usr/local/lib/python3.7/pathlib.py\", line 1013, in __new__\n     % (cls.__name__,))\n NotImplementedError: cannot instantiate 'WindowsPath' on your system\nI am guessing that this is because SpaCy must be serializing path information in a platform specific fashion?\nI don't recall having this issue when using from_disk and to_disk. Why am I not using these functions? I'd like to have my entire ML pipeline within a single binary file.\nIs this a bug? Are there workarounds? Or am I just not using SpaCy correctly?\nYour Environment\nspaCy version: 2.2.3\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.5", "issue_status": "Open", "issue_reporting_time": "2019-12-18T09:21:33Z"}, "36": {"issue_url": "https://github.com/explosion/spaCy/issues/4815", "issue_id": "#4815", "issue_summary": "One more call for stemming.", "issue_description": "buriy commented on 17 Dec 2019 \u2022\nedited\nFirst of all, there's a problem (in Russian):\nAfter training, you should have\nPOS('\u043a\u043e\u0434\u0438\u043b\u0430') -> VERB (translation: [she] coded)\nPOS('\u043a\u0440\u043e\u043a\u043e\u0434\u0438\u043b\u0430') -> NOUN (translation: [of] crocodile)\nBut looking at prefix (first char) and suffix (last 3 chars), if you haven't met these exact words in the training corpus... they are the same for the model! Both are VERBs or both are NOUNs.\nLooking at examples in which crocodiles are VERBS makes me laugh... Have you crocodiled somebody yesterday? :)\nWhy you might not meet the words in training? Because we have:\nlen(russian_dictionary) ~ 5,000,000\nlen(russian_lemmas) ~ 500,000\nlen(russian_stems) ~ 550,000\n24 words/lemma for ADJs,\n10-12 words/lemma for NOUNs,\n15-20 words/lemma for VERBs\nBut stemming would work well.\nNow, remembering the Zipf law definition, given that we have HashEmbeds of\n( orth | lower | norm | prefix | suffix | shape ) + G(vectors) + H(tok2vec),\nyou still have no chance to meet every word even once(!) in training, and to recognize them correctly in production then. Pretraining vectors and then sharing same vectors for all words for the same stem... well, you'll still have at least 1-2% of OOV words even with 100 GBs of text. (And two tables of 5M links +500K vectors to load...)\nHowever, you could often get POS from the word stem, and that should improve POS accuracy from 93% as we now have in https://github.com/buriy/spacy-ru , to more typical 97-98%. Syntax and NER quality will be improved too.\nFor one experiment, I've got 65% quality with TextCat for news category guessing, but quality increased to 72% when I just replaced words with their first sense lemmas from pymorphy2. We typically get the same result with stems, which are just words without their suffixes for Russian Snowball algorithm.\nWith hash-storage, no need to worry about that stem won't perfectly match the lemma, we only need to have much less stems than words so that combination of [prefix + stem + word + lower + shape] would be more powerful for rare words than the same without stems.\nSo at the moment I'm considering replacing .lower or .norm with stems using a custom tokenizer or some other dirty hack... but I'd like to include Russian fast and near-SOTA models to Spacy, and with that goal in mind, it's not a good option, right?\nAnother workaround is to replace randomly one name with another name, one loc with another loc -- that can be done for NERs while training, but it's complicated and won't work for less frequent regular words.\nSo, I think we need this for morphology-rich languages that have a lot of unpredictable word POS from only prefix and suffix: Russian, Ukrainian, Bulgarian, a lot of other slavic languages, then I guess German, Turkish.\nEven French and English should get OOV improvements, but I think not more than 0.1%, based on much lower probabilities of OOV when one has fewer words in the vocabulary.\nYou might also see improvements with Prodigy active-learning even without vectors / tok2vec, it would be interesting to try that.\nRelated issues: #327 #1825\nAlternative solution is proper support for fastText vectors: #2154 #3978\n2", "issue_status": "Open", "issue_reporting_time": "2019-12-16T20:55:05Z"}, "37": {"issue_url": "https://github.com/explosion/spaCy/issues/4813", "issue_id": "#4813", "issue_summary": "AttributeError: 'bool' object has no attribute 'use_params' when training using a base model", "issue_description": "Contributor\nerip commented on 16 Dec 2019 \u2022\nedited\nHow to reproduce the behaviour\nI have trained an NER model with which I am happy and I'd like to use it as the base model for a core model. Below shows my script to kick off training and the resulting error.\n(farsi-redo)\n~/farsi_full/Farsi_models_for_SpaCy on \ue0a0 master! \u231a 9:45:31\n$ cat ./train_model.bash\n#!/bin/bash\n\npython -m spacy train fa depedency_model2 corpus/spacy/fa_seraji-ud-train.json corpus/spacy/fa_seraji-ud-dev.json -v fa_model -b ./ner_model/model-best --pipeline='tagger,parser'\n(farsi-redo)\n~/farsi_full/Farsi_models_for_SpaCy on \ue0a0 master! \u231a 9:45:34\n$ ./train_model.bash\nTraining pipeline: ['tagger', 'parser']\nStarting with base model './ner_model/model-best'\nLoading vector from model 'fa_model'\nCounting training words (limit=0)\n\nItn  Tag Loss    Tag %    Dep Loss    UAS     LAS    Token %  CPU WPS\n---  ---------  --------  ---------  ------  ------  -------  -------\nTraceback (most recent call last):\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/site-packages/spacy/cli/train.py\", line 368, in train\n    losses=losses,\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/site-packages/spacy/language.py\", line 515, in update\n    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n  File \"nn_parser.pyx\", line 423, in spacy.syntax.nn_parser.Parser.update\n  File \"nn_parser.pyx\", line 262, in spacy.syntax.nn_parser.Parser.require_model\nValueError: [E109] Model for component 'parser' not initialized. Did you forget to load a model, or forget to call begin_training()?\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/site-packages/spacy/__main__.py\", line 33, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/site-packages/spacy/cli/train.py\", line 492, in train\n    with nlp.use_params(optimizer.averages):\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/contextlib.py\", line 81, in __enter__\n    return next(self.gen)\n  File \"/Users/erippeth/miniconda3/envs/farsi-redo/lib/python3.6/site-packages/spacy/language.py\", line 716, in use_params\n    next(context)\n  File \"pipes.pyx\", line 577, in use_params\nAttributeError: 'bool' object has no attribute 'use_params'\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Darwin-19.2.0-x86_64-i386-64bit\nPython version: 3.6.7", "issue_status": "Open", "issue_reporting_time": "2019-12-16T14:49:07Z"}, "38": {"issue_url": "https://github.com/explosion/spaCy/issues/4792", "issue_id": "#4792", "issue_summary": "newline \\n is captured as part of named entities?", "issue_description": "karzmei commented on 11 Dec 2019 \u2022\nedited\nHello there :)\nI ran the following code:\nimport spacy\ntext = \"\"\"\"This is about Alice\nwho visited Wonderland. Alice?\"\"\"\n\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"]) #  want NER only\ndoc = nlp(text)\nNE = set()\nfor ent in doc.ents:\n print(ent.text, ent.label_)\n NE.add(ent.text)\nprint(\"The named entities are:\", NE)\nThe result is:\nAlice\n PERSON\nWonderland GPE\nAlice PERSON\nThe named entities are: {'Alice\\n', 'Wonderland', 'Alice'}\nAs you see, it includes \"\\n\" as part of an entity found, and as a result treats \"Alice\" before line brake as different from a regular \"Alice\".\nThis bug seems to me similar to #2870, which was claimed to be solved. So was it?\nYour Environment\nOperating System: Windows 7\nPython Version Used: 3.7.3\nspaCy Version Used: 2.2.3\nEnvironment Information: ?", "issue_status": "Open", "issue_reporting_time": "2019-12-11T09:24:53Z"}, "39": {"issue_url": "https://github.com/explosion/spaCy/issues/4791", "issue_id": "#4791", "issue_summary": "Getting Error: gold.pyx in spacy.gold.GoldParse.__init__() IndexError: list index out of range", "issue_description": "SachinGarg10 commented on 11 Dec 2019\nHello,\nI'm training spacy english blank model with entities, tags, and parser in the normal training data format(not json).\nFollowing is the code that I'm using for training.\nfor i in tqdm(range(300)): \n    random.shuffle(Train_data) \n    losses = {} \n#     for text, annotations in Train_data:\n#         nlp.update([text], [{'words': annotations['words'], 'entities': annotations['entities'], 'heads': annotations['heads'], 'deps': annotations['deps'], 'tags': annotations['tags']}], sgd=optimizer, losses=losses, drop=0.5)\n#         nlp.update([text], [annotations], sgd=optimizer, losses=losses, drop=0.5)\n\n    batches = minibatch(Train_data, size=compounding(4.0, 32.0, 1.001)) \n    for batch in batches: \n        texts, annotations = zip(*batch) \n        nlp.update(texts, annotations, sgd=optimizer, losses=losses, drop=0.5) \n        \n    print(\"Losses: \", losses) \nWhile training, if I use USD symbol('$') in training data, everything works fine but as I replace the dollar symbol with Indian Rupee symbol('\u20b9') in the training data, I get following error:\nIndexError   Traceback (most recent call last)\n<ipython-input-12-4dc4a5e9998c> in <module>\n     12     for batch in batches:\n     13         texts, annotations = zip(*batch)\n---> 14         nlp.update(texts, annotations, sgd=optimizer, losses=losses, drop=0.5)\n     15 \n     16     print(\"Losses: \", losses)\n\n~/Documents/venv/spacy3.6/lib/python3.6/site-packages/spacy/language.py in update(self, docs, golds, drop, sgd, losses, component_cfg)\n    494             sgd = self._optimizer\n    495         # Allow dict of args to GoldParse, instead of GoldParse objects.\n--> 496         docs, golds = self._format_docs_and_golds(docs, golds)\n    497         grads = {}\n    498 \n\n~/Documents/venv/spacy3.6/lib/python3.6/site-packages/spacy/language.py in _format_docs_and_golds(self, docs, golds)\n    466                     err = Errors.E151.format(unexp=unexpected, exp=expected_keys)\n    467                     raise ValueError(err)\n--> 468                 gold = GoldParse(doc, **gold)\n    469             doc_objs.append(doc)\n    470             gold_objs.append(gold)\n\ngold.pyx in spacy.gold.GoldParse.__init__()\n\nIndexError: list index out of range\nPlease help and thanks in advance\nSystem Environment\nOperating System: 16.04 Ubuntu\nPython Version: 3.6.9\nSpaCy Version: 2.2.2", "issue_status": "Open", "issue_reporting_time": "2019-12-11T06:42:51Z"}, "40": {"issue_url": "https://github.com/explosion/spaCy/issues/4790", "issue_id": "#4790", "issue_summary": "token.ent_iob should be writable", "issue_description": "Contributor\ntamuhey commented on 10 Dec 2019 \u2022\nedited\nI think token.ent_iob should be writable because token.ent_type is writable:\nspaCy/spacy/tokens/token.pyx\nLine 733 in 38e1bc1\n def __set__(self, ent_type): \ntoken.ent_iob is not writable now, it can be only changed in cython:\nspaCy/spacy/tokens/token.pyx\nLine 751 in 38e1bc1\n return self.c.ent_iob ", "issue_status": "Open", "issue_reporting_time": "2019-12-10T17:37:05Z"}, "41": {"issue_url": "https://github.com/explosion/spaCy/issues/4787", "issue_id": "#4787", "issue_summary": "Clarification for behavior of `en_core_web_lg` with `doc.tensor`", "issue_description": "dennlinger commented on 10 Dec 2019\nUsing the en_core_web_lg model, I was comparing the output from doc.tensor, and doc.vector.\nUntil recently, I was expecting doc.tensor to be a matrix representation of the stacked word vectors (i.e., [token.vector for token in doc]).\nAs it turns out, doc.tensor is in fact similar in the dimension to the doc.tensor produced by en_core_web_sm or en_core_web_md (although the matrix entries do not match), which is not the dimension of the distinct token.vectors, and doc.vector.\nI was wondering whether this definition should be clarified in the Doc documentation page. I have not spotted any comment on the differences (or similarities) of the two properties. This is also especially relevant when considering the calculation of .similarity(), where it is not immediately clear from the docs whether this is based on doc.tensor or doc.vector.\nFurthermore, I could not find any comment on the specific computation of the tensors, so maybe a paragraph on this could also help clarify this issue?\nWhich page or section is this issue related to?\nhttps://github.com/explosion/spaCy/blob/master/website/docs/api/doc.md", "issue_status": "Open", "issue_reporting_time": "2019-12-10T14:08:07Z"}, "42": {"issue_url": "https://github.com/explosion/spaCy/issues/4781", "issue_id": "#4781", "issue_summary": "Setting the token.tag_ causes token.lemma_ to reset", "issue_description": "udnaan commented on 8 Dec 2019\nHow to reproduce the behavior\n    nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp('Expert kebab maker making kebabs expertly while wearing a funny hat.')\n\n    terms = [[doc[1:3], \"kebab baker\"]]\n    with doc.retokenize() as retokenizer:\n        for term in terms:\n            retokenizer.merge(term[0], attrs={\"LEMMA\": term[1]})\n    print(\"Before:\", [(token.text, token.lemma_) for token in doc])\n    for tok in doc:\n        tag = tok.tag_\n        tok.tag_ = tag\n    print(\"After:\", [(token.text, token.lemma_) for token in doc])\n\n\nBefore: [('Expert', 'expert'), ('kebab maker', 'kebab baker'), ('making', 'make'), ('kebabs', 'kebab'), ('expertly', 'expertly'), ('while', 'while'), ('wearing', 'wear'), ('a', 'a'), ('funny', 'funny'), ('hat', 'hat'), ('.', '.')]\nAfter: [('Expert', 'expert'), ('kebab maker', 'kebab maker'), ('making', 'make'), ('kebabs', 'kebab'), ('expertly', 'expertly'), ('while', 'while'), ('wearing', 'wear'), ('a', 'a'), ('funny', 'funny'), ('hat', 'hat'), ('.', '.')]\nYour Environment\nOS: Linux (Ubuntu 18.04)\nPython: 3.7.4\nSpacy: 2.2\nExpected behavior\nChanging the tag_ should not affect the custom lemma\nWorkaround\nSaving lemma and restoring it after setting the tok.tag_ is a workaround.", "issue_status": "Open", "issue_reporting_time": "2019-12-08T04:25:50Z"}, "43": {"issue_url": "https://github.com/explosion/spaCy/issues/4775", "issue_id": "#4775", "issue_summary": "Handle sentence boundaries from multiple components", "issue_description": "Collaborator\nadrianeboyd commented on 6 Dec 2019\nFeature description\nDecide how to handle is_sentenced and sentence boundaries that may come from multiple components (Sentencizer, SentenceRecognizer, Parser).\nSome ideas:\nhave an is_sentenced property more like is_parsed that can be set by components\nhave a way to set finalized sentence boundaries (all 0 to -1):\nhave an extra option for each component\nhave an extra pipeline component (e.g., finalize_sentences?) that can be inserted at the right point in the pipeline\nalso have a component that resets all sentence boundaries?\nmodify Sentencizer to only set sentence starts, not all tokens?\nCheck that no spacy components clobber sentence boundaries and that is_sentenced works consistently when sentence boundaries come from multiple sources. If a component after the parser changes sentence boundaries, make sure the required tree recalculations are done (a related issue: #4497).\nPotentially add warnings when non-zero sent_start is changed by any component?\nI think the default behavior could be that any pipeline component can add sentence boundaries but that components won't remove any sentence boundaries. The idea would be that the Sentencizer or SentenceRecognizer add punctuation-based boundaries (typically high precision, although the Sentencizer less so) and the Parser can add phrase-based boundaries (improving recall). I don't know if this works as cleanly as envisioned in practice, especially with the Sentencizer. Most likely people using the Sentencizer aren't using other components so it's less of an issue, but I could imagine SentenceRecognizer + Parser as a common combination.", "issue_status": "Open", "issue_reporting_time": "2019-12-05T20:26:07Z"}, "44": {"issue_url": "https://github.com/explosion/spaCy/issues/4769", "issue_id": "#4769", "issue_summary": "Add vocab-related warnings/errors to add_pipe()", "issue_description": "Collaborator\nadrianeboyd commented on 5 Dec 2019\nFeature description\nAdd warnings or errors to add_pipe() if you try to add components with a different vocab.", "issue_status": "Open", "issue_reporting_time": "2019-12-05T13:40:47Z"}, "45": {"issue_url": "https://github.com/explosion/spaCy/issues/4760", "issue_id": "#4760", "issue_summary": "Boosting tagger and lemmatizer using additional data", "issue_description": "Branislava commented on 4 Dec 2019\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Linux-4.15.0-70-generic-x86_64-with-debian-buster-sid\nPython version: 3.6.5\nDear spaCy developers,\nI have two questions regarding the inclusion of the external resources in the tagger training procedure.\nI saw that a tagger model can be improved using the word vectors. But:\nIs it possible to somehow \"plug-in\" a morphological dictionary to the tagger training procedure?\nand\nIs it possible to train a lemmatizer that is not only based on lookup tables, but relies on information that contains part-of-speech, as well?\nPut precisely, we would like to include external resources, besides our tagger training corpora, that contain information about a word, its lemma, universal part of speech tag and grammatical categories.\nMany thanks", "issue_status": "Open", "issue_reporting_time": "2019-12-04T11:31:15Z"}, "46": {"issue_url": "https://github.com/explosion/spaCy/issues/4755", "issue_id": "#4755", "issue_summary": "is_plural/is_singular are really missing", "issue_description": "hzitoun commented on 3 Dec 2019 \u2022\nedited\nFeature description\nThat would be nice to have an attribute is_plural associated to Token class.", "issue_status": "Open", "issue_reporting_time": "2019-12-03T15:52:52Z"}, "47": {"issue_url": "https://github.com/explosion/spaCy/issues/4748", "issue_id": "#4748", "issue_summary": "Memory usage of `debug-data` with a huge training set", "issue_description": "sfragis commented on 3 Dec 2019 \u2022\nedited\nHi, I'm using Spacy 2.2.2 to train new tagger and parser models for the Italian language.\nMy training data set is quite big (about 2.3 GB for the train and 580 MB for the dev) and is saved in two JSONL files.\nI'm experiencing an unexpected memory usage when running the debug-data command: memory usage starts low and then grows up to consuming my 32GB of RAM as well as the whole swap (about the same size).\nBefore upgrading my RAM to 128 GB (which I suspect might be useless), I'm interested in your opinion about:\nhints about data set structure: for instance, comments in issue #4700 suggested to reduce the sentence length on average, but I've no clue about what values might be optimal; is there any rule of thumb to properly dimension the data set?\npossible optimizations to the source code to reduce memory footprint (for instance by improving the lazy loading of the data set); I'm willing to contribute to Spacy if anyone would kindly point me to the problematic parts (if any, of course)\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Linux-4.4.0-112-generic-x86_64-with-debian-stretch-sid\nPython version: 3.7.4", "issue_status": "Open", "issue_reporting_time": "2019-12-03T10:37:23Z"}, "48": {"issue_url": "https://github.com/explosion/spaCy/issues/4746", "issue_id": "#4746", "issue_summary": "[suggestion] add information about which parts of the pipeline add an attribute", "issue_description": "Contributor\nBramVanroy commented on 3 Dec 2019\nAs a suggestion, I think it'd be useful if the API mentions for each attribute of Doc, Token, Span, Lexeme where the attribute is created. That way it is easy for users to read the documentation, see which attributes they need, and disable all the other pipeline parts. Now it is not very clear.\nI know there is a non-exhaustive overview on the usage part of the documentation, but a clear overview seems to be missing at the API side. An ideal place would seem in the attribute overview of the tables themselves (e.g. here) by adding a column titled \"created by\" or similar with possible values tokenizer, tagger, parser, ner, textcat. I can imagine that if you've worked with this (or created it) it all seems very clear-cut, but looking at it from the outside it might be quite difficult to make the distinction.\nIf this is a feature that you can agree on, I'll be willing to put some time into this. (But my changes need to be reviewed carefully, then.)", "issue_status": "Open", "issue_reporting_time": "2019-12-03T08:41:35Z"}, "49": {"issue_url": "https://github.com/explosion/spaCy/issues/4745", "issue_id": "#4745", "issue_summary": "thinc error on parallel training of bots using spacy with using & in curl commands to send requests to rasa server.", "issue_description": "shayan09 commented on 3 Dec 2019 \u2022\nedited\nThe following is the error:\n[2019-12-02 18:06:03,375] ERROR in app: Exception on /train [POST]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 2446, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1951, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1820, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python3.6/dist-packages/flask/_compat.py\", line 39, in reraise\n    raise value\n  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1949, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1935, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"predictor_app.py\", line 73, in train_route\n    interpreter_dict[model_name] = Interpreter.load(model_path[model_name])\n  File \"/usr/local/lib/python3.6/dist-packages/rasa_nlu/model.py\", line 304, in load\n    skip_validation)\n  File \"/usr/local/lib/python3.6/dist-packages/rasa_nlu/model.py\", line 331, in create\n    model_metadata, **context)\n  File \"/usr/local/lib/python3.6/dist-packages/rasa_nlu/components.py\", line 425, in load_component\n    cached_component, **context)\n  File \"/usr/local/lib/python3.6/dist-packages/rasa_nlu/registry.py\", line 172, in load_component_by_meta\n    cached_component, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/rasa_nlu/utils/spacy_utils.py\", line 114, in load\n    nlp = spacy.load(model_name, disable=['parser'])\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 129, in load_model\n    return load_model_from_link(name, **overrides)\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 146, in load_model_from_link\n    return cls.load(**overrides)\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/data/en/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 190, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 173, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/language.py\", line 791, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 630, in from_disk\n    reader(path / key)\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/language.py\", line 787, in <lambda>\n    deserializers[name] = lambda p, proc=proc: proc.from_disk(p, exclude=[\"vocab\"])\n  File \"pipes.pyx\", line 617, in spacy.pipeline.pipes.Tagger.from_disk\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 630, in from_disk\n    reader(path / key)\n  File \"pipes.pyx\", line 599, in spacy.pipeline.pipes.Tagger.from_disk.load_model\n  File \"pipes.pyx\", line 512, in spacy.pipeline.pipes.Tagger.Model\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/_ml.py\", line 513, in build_tagger_model\n    pretrained_vectors=pretrained_vectors,\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/_ml.py\", line 351, in Tok2Vec\n    (norm | prefix | suffix | shape)\n  File \"/usr/local/lib/python3.6/dist-packages/thinc/check.py\", line 129, in checker\n    raise UndefinedOperatorError(op, instance, args[0], instance._operators)\nthinc.exceptions.UndefinedOperatorError: \n\n  Undefined operator: |\n  Called by (<thinc.neural._classes.hash_embed.HashEmbed object at 0x7fdbbc858cf8>, <thinc.neural._classes.hash_embed.HashEmbed object at 0x7fdbbc865e48>)\n  Available: >>, +, |, *, \n\n  Traceback:\n  |__ <lambda> [787] in /usr/local/lib/python3.6/dist-packages/spacy/language.py\n  |____ from_disk [630] in /usr/local/lib/python3.6/dist-packages/spacy/util.py\n  |_____ build_tagger_model [513] in /usr/local/lib/python3.6/dist-packages/spacy/_ml.py\n         >>> pretrained_vectors=pretrained_vectors,\n\n127.0.0.1 - - [02/Dec/2019 18:06:03] \"POST /train HTTP/1.1\" 500 -\nEnvironment\nOperating System: Ubuntu 18.04\nPython Version Used: 3.6.9\nspaCy Version Used: 2.1.4\nEnvironment Information: Using it with rasa nlu\nI have sent the following curl commands to the Flask server:\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_1\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Thank\\\"}, {\\\"intent\\\": \\\"Affirm\\\"}, {\\\"intent\\\": \\\"Deny\\\"}, {\\\"intent\\\": \\\"Bye\\\"}, {\\\"intent\\\": \\\"Colors\\\"}, {\\\"intent\\\": \\\"Positive_feeling\\\"}, {\\\"intent\\\": \\\"Negative_feeling\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done1\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_2\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Bye\\\"}, {\\\"intent\\\": \\\"Colors\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done2\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_3\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Positive_feeling\\\"}, {\\\"intent\\\": \\\"Negative_feeling\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done3\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_4\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Thank\\\"}, {\\\"intent\\\": \\\"Affirm\\\"}, {\\\"intent\\\": \\\"Deny\\\"}, {\\\"intent\\\": \\\"Bye\\\"}, {\\\"intent\\\": \\\"Colors\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done4\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_5\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Positive_feeling\\\"}, {\\\"intent\\\": \\\"Negative_feeling\\\"}, {\\\"intent\\\": \\\"Thank\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done5\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_6\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Positive_feeling\\\"}, {\\\"intent\\\": \\\"Negative_feeling\\\"}, {\\\"intent\\\": \\\"Colors\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done6\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_7\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Thank\\\"}, {\\\"intent\\\": \\\"Affirm\\\"}, {\\\"intent\\\": \\\"Deny\\\"}, {\\\"intent\\\": \\\"Positive_feeling\\\"},{\\\"intent\\\": \\\"Negative_feeling\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done7\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_8\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Positive_feeling\\\"}, {\\\"intent\\\": \\\"Negative_feeling\\\"}, {\\\"intent\\\": \\\"Affirm\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done8\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_9\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Positive_feeling\\\"}, {\\\"intent\\\": \\\"Negative_feeling\\\"}, {\\\"intent\\\": \\\"Bye\\\"}], \\\"threshold\\\": 0.65}\" localhost:5556/train && echo \"done9\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_10\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Positive_feeling\\\"}, {\\\"intent\\\": \\\"Negative_feeling\\\"}, {\\\"intent\\\": \\\"Affirm\\\"}, {\\\"intent\\\": \\\"Deny\\\"}], \\\"threshold\\\": 0.7}\" localhost:5556/train && echo \"done10\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_11\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Positive_feeling\\\"}, {\\\"intent\\\": \\\"Negative_feeling\\\"}, {\\\"intent\\\": \\\"Bye\\\"}, {\\\"intent\\\": \\\"Colors\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done11\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_12\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Bye\\\"}, {\\\"intent\\\": \\\"Thank\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done12\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_13\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Negative_feeling\\\"}, {\\\"intent\\\": \\\"Affirm\\\"}, {\\\"intent\\\": \\\"Deny\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done13\" &\ncurl -X POST -H 'Content-Type: application/json' -d \"{\\\"bot_id\\\": \\\"mybot_1\\\", \\\"message_id\\\": \\\"message_14\\\", \\\"choices\\\": [{\\\"intent\\\": \\\"Thank\\\"}, {\\\"intent\\\": \\\"Affirm\\\"}, {\\\"intent\\\": \\\"Colors\\\"}], \\\"threshold\\\": 0.0}\" localhost:5556/train && echo \"done14\"\nPS: I tried training 14 bots simultaneously out of which 11 were successful and the others returned the error. They work if the curl commands are sent sequentially, but not concurrently. Any idea why this would happen?", "issue_status": "Open", "issue_reporting_time": "2019-12-02T23:12:01Z"}, "50": {"issue_url": "https://github.com/explosion/spaCy/issues/4743", "issue_id": "#4743", "issue_summary": "Disabled pipes interim save to disc", "issue_description": "pythonBerg commented on 2 Dec 2019\nThis is a feature suggestion that is perhaps only requested due to my ignorance...\nWhen running training for either textcat or ner, other pipes are disabled. The sample code suggests using \"with\" clause so that they are enabled before save.\nWhen running long training program, saves are applied frequently. These saves appear to change meta.json to reflect only the pipe being trained since others have been disabled. This seems to be cleaned up at the end with the save outside the \"with\" clause.\nShould a long program fail or be canceled within the \"with\" clause, it is possible that other trained pipes could be lost if manual edit to meta.json is not completed before next training run...\nDoes this make sense? Users have the ability to both disable and remove pipes. Shouldn't a save always reflect pipes that exist, even if they've been disabled? What am I missing?", "issue_status": "Open", "issue_reporting_time": "2019-12-02T12:25:28Z"}, "51": {"issue_url": "https://github.com/explosion/spaCy/issues/4741", "issue_id": "#4741", "issue_summary": "Top n similar tokens APIs", "issue_description": "alexcoca commented on 2 Dec 2019\nFeature description\nHi Guys,\nA few issues have discussed the problem of retrieving synonyms (e.g., #276, #1561, #1018). The solution presented in #276:\n>>> def most_similar(word):\n...   queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n...   by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n...   return by_similarity[:10]\nis quite a bit slower than I'd like for my use case.\nAn alternative that was proposed (in #276) is to retrieve the vectors using something like:\ntvec = nlp(\"king\")[0].vector\nms = nlp.vocab.vectors.most_similar(tvec.reshape(1,tvec.shape[0]))\nHowever, this seems very limited as it only returns the most similar word by cosine similarity. Ideally I would be able to specify something like\nms = nlp.vocab.vectors.most_similar(tvec.reshape(1,tvec.shape[0]), n=500)\nand receive a list of, ideally POS tag + surface form of the word. The reason for this is that I need to select the most similar top_n words with the same POS tag - I need the actual word, not the vector representation. Is there an elegant (and most importantly efficient) way to achieve this in spaCy or am I stuck with the basic version outlined at the start of this post?\nMany thanks.\n__", "issue_status": "Open", "issue_reporting_time": "2019-12-02T10:35:01Z"}, "52": {"issue_url": "https://github.com/explosion/spaCy/issues/4739", "issue_id": "#4739", "issue_summary": "2.2 and Data Augmentation", "issue_description": "caseybasichis commented on 1 Dec 2019 \u2022\nedited\nI can\u2019t find any documentation about the Data Augmentation pipeline mentioned on the spacy blog. It was unclear if it was an announcement or an already included feature.\nI\u2019m looking to add custom augmentation before fine tuning the spacy-transformers with a maskedLM task.", "issue_status": "Open", "issue_reporting_time": "2019-12-01T17:18:02Z"}, "53": {"issue_url": "https://github.com/explosion/spaCy/issues/4736", "issue_id": "#4736", "issue_summary": "Adding Romanian NER", "issue_description": "Contributor\navramandrei commented on 30 Nov 2019\nHello,\nI committed a pull request several months ago for adding a Romanian NER: #4151 (comment). I saw it was added in Spacy Universe: https://spacy.io/universe/project/ronec, but when I try to download the model with python -m spacy download ro_ner, it says that No compatible model found for 'ro_ner' (spaCy v2.2.3). We would like to know what is the progress in introducing the model and/or if you encountered any problems in training the model on the corpus (we would be happy to help you in this case).\nP.S. We also refactored the spacy training tutorial for the Romanian NER corpus since then: https://github.com/dumitrescustefan/ronec/tree/master/spacy/train-local-model, and also, we added an already trained model (as a demo) that should be working with spacy: https://github.com/dumitrescustefan/ronec/tree/master/spacy/online-model.\nThank you,\nAvram Andrei", "issue_status": "Open", "issue_reporting_time": "2019-11-30T15:39:42Z"}, "54": {"issue_url": "https://github.com/explosion/spaCy/issues/4735", "issue_id": "#4735", "issue_summary": "nb: lemmatization of copula AUX", "issue_description": "Contributor\njarib commented on 30 Nov 2019 \u2022\nedited\nIf you train an nb model on the latest NorNE/UD data, spacy gets the lemmatization of the copula er (\"is\") wrong, since the data now (correctly) tags it as AUX and not VERB.\nThe current nb_core_news_sm model does not have this problem since it was trained on older version of the UD data, before these verbs were since changed to use the AUX tag for copulas.\nHow to reproduce the behaviour\nExample sentence: Hun er statsminister (\"She is prime minister\")\nThe correct lemma for copula er (\"is\") would be v\u00e6re (\"to be\").\nThe current nb_core_news_sm model finds the correct lemma, since it is tagged as VERB and presumably it's then found in the lemma_exc lookup table:\n>>> nlp = spacy.load('nb_core_news_sm')\n>>> doc = nlp(\"Hun er statsminister\")\n>>> [(t.text, t.lemma_, t.pos_) for t in nlp(\"Hun er statsminister\")]\n[('Hun', '-PRON-', 'PRON'),\n ('er', 'v\u00e6re', 'VERB'),\n ('statsminister', 'statsminister', 'NOUN')]\n>>> nlp.vocab.morphology.lemmatizer('er', 'VERB')\n['v\u00e6re']\nUsing a model trained on the updated data, the lemma becomes incorrect.\n>>> nlp = spacy.load('./data/nb-sm-training-norne/model-best')\n>>> [(t.text, t.lemma_, t.pos_) for t in nlp(\"Hun er statsminister\")]\n[('Hun', 'hun', 'PRON'),\n ('er', 'er', 'AUX'),\n ('statsminister', 'statsminister', 'NOUN')]\nWe can also see that the lemmatizer still gets the lemma of the VERB correctly, but not AUX:\n>>> nlp.vocab.morphology.lemmatizer('er', 'VERB')\n['v\u00e6re']\n>>> nlp.vocab.morphology.lemmatizer('er', 'AUX')\n['er']\n>>>\nSolution?\nWhat is the best way to solve this? The English model seems to get it right for some reason, even though the lemmatizer has the same problem with AUX vs VERB:\n>>> nlp = spacy.load('en_core_web_sm')\n>>>[(t.text, t.lemma_, t.pos_) for t in nlp(\"She is prime minister\")]\n[('She', '-PRON-', 'PRON'),\n ('is', 'be', 'AUX'),\n ('prime', 'prime', 'PROPN'),\n ('minister', 'minister', 'PROPN')]\n>>> nlp.vocab.morphology.lemmatizer('is', 'AUX')\n['is']\n>>> nlp.vocab.morphology.lemmatizer('is', 'VERB')\n['be']\nA naive solution would be to simply treat AUX as a VERB in the lemmatizer, but it feels a bit invasive.\ndiff --git a/spacy/lemmatizer.py b/spacy/lemmatizer.py\nindex d70e4cfc4..d90aa0fb2 100644\n--- a/spacy/lemmatizer.py\n+++ b/spacy/lemmatizer.py\n@@ -3,7 +3,7 @@ from __future__ import unicode_literals\n \n from collections import OrderedDict\n \n-from .symbols import NOUN, VERB, ADJ, PUNCT, PROPN\n+from .symbols import NOUN, VERB, ADJ, PUNCT, PROPN, AUX\n from .errors import Errors\n from .lookups import Lookups\n \n@@ -45,7 +45,7 @@ class Lemmatizer(object):\n             return [lookup_table.get(string, string)]\n         if univ_pos in (NOUN, \"NOUN\", \"noun\"):\n             univ_pos = \"noun\"\n-        elif univ_pos in (VERB, \"VERB\", \"verb\"):\n+        elif univ_pos in (VERB, \"VERB\", \"verb\", AUX, \"AUX\", \"aux\"):\n             univ_pos = \"verb\"\n         elif univ_pos in (ADJ, \"ADJ\", \"adj\"):\n             univ_pos = \"adj\"\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.15.0-1054-aws-x86_64-with-debian-buster-sid\nPython version: 3.6.5", "issue_status": "Open", "issue_reporting_time": "2019-11-30T14:38:15Z"}, "55": {"issue_url": "https://github.com/explosion/spaCy/issues/4725", "issue_id": "#4725", "issue_summary": "GPU and multiprocessing leads to `TypeError: can not serialize 'cupy.core.core.ndarray' object`", "issue_description": "mathisc commented on 28 Nov 2019 \u2022\nedited\nI am using spacy with GPU and multiprocessing like this :\nif __name__ == '__main__':\n    multiprocessing.set_start_method('spawn')\n    spacy.require_gpu()\n    nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'textcat'])\n    for _ in nlp.pipe(docs, batch_size=16, n_process=2):\n        pass \nand I am getting this as an error :\n  File \"benchmark.py\", line 117, in <module>\n    for _ in nlp.pipe(docs, batch_size=16, n_process=2):\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/language.py\", line 816, in pipe\n    for doc in docs:\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/language.py\", line 859, in _multiprocessing_pipe\n    proc.start()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 105, in start\n    self._popen = self._Popen(self)\n  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/usr/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/usr/lib/python3.6/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\n    reduction.dump(process_obj, fp)\n  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 60, in dump\n    ForkingPickler(file, protocol).dump(obj)\n  File \"vectors.pyx\", line 129, in spacy.vectors.Vectors.__reduce__\n  File \"vectors.pyx\", line 464, in spacy.vectors.Vectors.to_bytes\n  File \"/usr/local/lib/python3.6/dist-packages/spacy/util.py\", line 625, in to_bytes\n    serialized[key] = getter()\n  File \"vectors.pyx\", line 458, in spacy.vectors.Vectors.to_bytes.serialize_weights\n  File \"/usr/local/lib/python3.6/dist-packages/srsly/_msgpack_api.py\", line 16, in msgpack_dumps\n    return msgpack.dumps(data, use_bin_type=True)\n  File \"/usr/local/lib/python3.6/dist-packages/srsly/msgpack/__init__.py\", line 40, in packb\n    return Packer(**kwargs).pack(o)\n  File \"_packer.pyx\", line 285, in srsly.msgpack._packer.Packer.pack\n  File \"_packer.pyx\", line 291, in srsly.msgpack._packer.Packer.pack\n  File \"_packer.pyx\", line 288, in srsly.msgpack._packer.Packer.pack\n  File \"_packer.pyx\", line 282, in srsly.msgpack._packer.Packer._pack\nTypeError: can not serialize 'cupy.core.core.ndarray' object\nNOTE : if I am using n_process=1 the code runs fine.\nIf I am not using the gpu the code also runs fine.\nIt is the combination of GPU and multiprocessing that leads to this error.\nINFO :\nPython 3.6.9\nCupy : cupy-cuda100==7.0.0rc1\nSpacy : spacy==2.2.3", "issue_status": "Open", "issue_reporting_time": "2019-11-27T21:27:48Z"}, "56": {"issue_url": "https://github.com/explosion/spaCy/issues/4719", "issue_id": "#4719", "issue_summary": "spacy train CLI skips training and saves the model", "issue_description": "kormilitzin commented on 27 Nov 2019\nPreamble:\nI am interested in training a spaCy NER model with 8 categories and I have collected a small annotated dataset with all 8 categories. However from my previous experiments, I have a substantial amount of annotated data, but only for 5 of 8 categories. Therefore I decided to:\nfirst train a small NER model with only 5 categories\nand then re-use it as a baseline model, for NER model with all 8 categories.\nThis is similar to spaCy examples on extending the list of entities by re-using a standard model and adding new categories.\nHowever, when I use spaCy CLI for training, the training step is skipped and the model is directly saved:\nYour Environment\nAny ideas?", "issue_status": "Open", "issue_reporting_time": "2019-11-27T00:19:01Z"}, "57": {"issue_url": "https://github.com/explosion/spaCy/issues/4704", "issue_id": "#4704", "issue_summary": "Creating vrt files for cqpweb", "issue_description": "tianshuo commented on 25 Nov 2019\nFeature description\nWe want to find process text into an annotated corpus and search on it using POS+regex. For example, if we want to search for \"look|looking|looked for\", we can use\n\"{look/V} for\"\nWe already have been using spacy's pattern matching function but it is a bit slow(because we don't index anything) and we found out cqpweb is a great tool for both indexing, searching (it has a special syntax that works for us) and has a web UI. So the next step is we need to export everything into the xml-based .vrt files used in Cqpweb. See the instructions here: http://cwb.sourceforge.net/files/CWB_Encoding_Tutorial.pdf\nAn example format:\n<?xml version=\"1.0\" encoding=\"ISO-8859-1\" standalone=\"yes\" ?>\n<!-- A Thrilling Experience -->\n<story num=\"4\" title=\"A Thrilling Experience\">\n<p>\n<s>\nTick NN tick\n. SENT .\n</s>\n<s>\nA DT a\nclock NN clock\n. SENT .\n</s>\n<s>\nTick VB tick\n, , ,\ntick VB tick\n. SENT .\n</s>\n</p>\n...\n</story>\nCould the feature be a custom component or spaCy plugin?\nYes! This could be a spaCy plugin, where we can export our corpus as the .vrt file. Or we could even create a tool for importing managing, browsing and searching text in our corpus.", "issue_status": "Open", "issue_reporting_time": "2019-11-25T06:50:17Z"}, "58": {"issue_url": "https://github.com/explosion/spaCy/issues/4695", "issue_id": "#4695", "issue_summary": "support for zh models", "issue_description": "Contributor\nphiedulxp commented on 22 Nov 2019\nFeature description\nAdd zh models trained on OntoNotes v5 Chinese.\nCould the feature be a custom component or spaCy plugin?\nI will released the converted dataset for spacy.", "issue_status": "Open", "issue_reporting_time": "2019-11-22T07:13:07Z"}, "59": {"issue_url": "https://github.com/explosion/spaCy/issues/4665", "issue_id": "#4665", "issue_summary": "conll2json: Underscore not allowed in head column", "issue_description": "HendricButz commented on 17 Nov 2019\nI got a conllU file, from my university, where the head column is filled with .\nProcessing such file with the cli.convert method will result in a int cast error in\nhttps://github.com/explosion/spaCy/blob/master/spacy/cli/converters/conllu2json.py line 73\nin the read_conllx method (head = (int(head) - 1) if head != \"0\" else id).\nIn the format documentation on https://universaldependencies.org/format.html\nthere is stated that: the allowed values for head are the same as for id (so _ should not be allowed)\n\"7. HEAD: Head of the current word, which is either a value of ID or zero (0).\"\nBut some lines later, underscore \"\" is only forbidden for the id column.\n\"Underscore () is used to denote unspecified values in all fields except ID. Note that no format-level distinction is made for the rare cases where the FORM or LEMMA is the literal underscore \u2013 processing in such cases is application-dependent. Further, in UD treebanks the UPOS, HEAD, and DEPREL columns are not allowed to be left unspecified.\"\nSo, the university guys who gave me the problematic conll file insits on beeing right in allowing _ in the head column and asked me to write a bug report here.", "issue_status": "Open", "issue_reporting_time": "2019-11-17T14:07:59Z"}, "60": {"issue_url": "https://github.com/explosion/spaCy/issues/4661", "issue_id": "#4661", "issue_summary": "Tag problem after update", "issue_description": "gbochikubo commented on 16 Nov 2019 \u2022\nedited\nHi, i'm having a problem trying to update an existing spacy model. First I am just reading a file where the phrases I will use for training are stored and allocating the phrases in TRAIN_DATA [], then I take the tags contained in the test_texte phrase just for testing and the feedback is:\nApesar ADV advmod\nde ADP mark\nestar VERB cop\ndoente ADJ advcl\n, PUNCT punct\nMaria PROPN nsubj\nfoi VERB ROOT\npara ADP case\na DET det\nescola NOUN obl\nhoje ADV advmod\nAfter that I just perform the model update. The problem is when I try to get the tags after the model update most of the tags are coming with an X and this was not supposed to happen, this is the return after the update with the same phrase above:\nApesar ADV advmod\nde ADP mark\nestar X cop\ndoente X advcl\n, X punct\nMaria PROPN nsubj\nfoi X ROOT\npara ADP case\na X det\nescola NOUN obl\nhoje ADV advmod\nI'm not understanding why this is happening, this is my code:\nfrom __future__ import unicode_literals, print_function\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\nimport csv\nfrom spacy.symbols import ADJ, ADV, INTJ, NOUN, PROPN, VERB, ADP, AUX, CCONJ,DET, NUM, PRON, SCONJ, PUNCT, SYM, X \n\nTRAIN_DATA = []\nnlp = spacy.load('pt_core_news_sm')\n\nwith open('../Docs/Sentencas.csv', 'r',encoding='utf8') as data_file:\n    lines = data_file.readlines()\n    i = 0\n    while i < (len(lines)-1):\n        tokens = lines[i].split(\"***\")\n        i += 1\n        tags = lines[i].split(\"***\")\n        i += 1\n        TRAIN_DATA.append((\"{}\".format(' '.join(tokens[:-1])), {\"tags\": tags[:-1]}))\n\noptimizer = nlp.begin_training()\ntest_text = \"Apesar de estar doente, Maria foi para a escola hoje\"\ndoc3 = nlp(test_text)\nfor token in doc3:\n    print(token.text, token.pos_, token.dep_)  \n\nfor i in range(20):\n    print(\"Epoch \", i)\n    random.shuffle(TRAIN_DATA)\n    for text, annotations in TRAIN_DATA:\n        try:\n            nlp.update([text], [annotations], sgd=optimizer)\n        except:\n            print(\"Error on \", text, annotations)\n\ndoc3 = nlp(test_text)\nfor token in doc3:\n    print(token.text, token.pos_, token.dep_)  \nYour Environment\nOperating System: Windows\nPython Version Used: 3.7.5\nspaCy Version Used: 2.1.9\nEnvironment Information:", "issue_status": "Open", "issue_reporting_time": "2019-11-16T18:20:20Z"}, "61": {"issue_url": "https://github.com/explosion/spaCy/issues/4655", "issue_id": "#4655", "issue_summary": "Support for attrs like Gender and Case in Greek in POS Tagging", "issue_description": "itsmegeorge commented on 15 Nov 2019\nHi! Is there any method that takes there of this? NLP Cube does it but it's not that reliable with Greek and SpaCy has been very good at predicting POS labels for me.", "issue_status": "Open", "issue_reporting_time": "2019-11-15T12:55:55Z"}, "62": {"issue_url": "https://github.com/explosion/spaCy/issues/4650", "issue_id": "#4650", "issue_summary": "AssertionError when running beam_parse", "issue_description": "lhyleo commented on 15 Nov 2019 \u2022\nedited\nIt might be related to https://github.com/explosion/spaCy/issues/4313, but I cannot figure out how to reopen an issue\nHow to reproduce the behaviour\nimport spacy\n\nnlp = spacy.load('en_core_web_lg')\ndoc = nlp(\"Our principal executive office is located at 333 South Grand Avenue, 28th Floor, Los Angeles, CA 90071 and our telephone number is (213) 830-6300.\") \ndocs = [doc]\nner = nlp.get_pipe('ner')\nbeams = ner.beam_parse(docs, beam_width=5)\nI got the following traceback:\nAssertionError                            Traceback (most recent call last)\n<ipython-input-4-7f0219500ddb> in <module>\n      2 docs = [doc]\n      3 ner = nlp.get_pipe('ner')\n----> 4 beams = ner.beam_parse(docs, beam_width=5)\n\nnn_parser.pyx in spacy.syntax.nn_parser.Parser.beam_parse()\n\nnn_parser.pyx in spacy.syntax.nn_parser.Parser.transition_beams()\n\nsearch.pyx in thinc.extra.search.Beam.advance()\n\nAssertionError: \nYour Environment\nOperating System: macOS 14.10.6\nPython Version Used: 3.7.4\nspaCy Version Used: 2.1.9", "issue_status": "Open", "issue_reporting_time": "2019-11-14T18:33:37Z"}, "63": {"issue_url": "https://github.com/explosion/spaCy/issues/4649", "issue_id": "#4649", "issue_summary": "Saved NER model is different from the NER model before saved", "issue_description": "chunlei2 commented on 14 Nov 2019\nHow to reproduce the behaviour\nAdjust the example code in Spacy documentation which trains an additional entity type animal\nReplace LABEL = \"ANIMAL\" with LABEL = \"DATE\"\nReplace TRAIN_DATA with TRAIN_DATA = [ ( \"ADHD (2008)\", {\"entities\": [(6, 10, LABEL)]} ) ]\nReplace test_text = \"Do you like horses?\" with test_text = \"ADHD (2008)\"\nCall main function main(model = 'en_core_web_sm', output_dir='test')\nThe printed output will be Entities in 'ADHD (2008)' Saved model to test Loading from test VEGETABLE 2008)\nwhich means the trained model doesn't detect any entity, but the saved model detect VEGETABLE entity. And VEGETABLE shouldn't appear because training data doesn't contain the entity type.\nThanks in advance!\nYour Environment\nOperating System: Windows Server 2012 R2\nPython Version Used: 3.6.4\nspaCy Version Used: 2.1.8\nEnvironment Information:", "issue_status": "Open", "issue_reporting_time": "2019-11-14T18:20:21Z"}, "64": {"issue_url": "https://github.com/explosion/spaCy/issues/4642", "issue_id": "#4642", "issue_summary": "spaCy Token Matcher does not support group capture", "issue_description": "courtarro commented on 13 Nov 2019 \u2022\nedited\nFeature description\nWhen using the spaCy Token Matcher, the output is the overall sequence that is matched. While this is useful in many cases, one powerful feature is missing: group capture. At least two matchers in Stanford NLP support this explicitly. The JAPE mechanism in GATE supports this by including assignment features within the pattern.\nLooking over the code for the current Matcher, I do not see a mechanism for passing out the tokens that satisfy each part of the pattern. My experiments with the Dependency Matcher show that it returns the nodes that satisfy parts of the pattern, so it effectively enables group capture that way. However, its performance is such that it's not a practical replacement for the regular Matcher.\nI wrote a detailed question about this on Stack Overflow a while back and got no answer, and looking at the code, it seems that the answer is that it's not currently possible.", "issue_status": "Open", "issue_reporting_time": "2019-11-13T17:28:03Z"}, "65": {"issue_url": "https://github.com/explosion/spaCy/issues/4635", "issue_id": "#4635", "issue_summary": "ValueError: spacy.syntax.nn_parser.Parser size changed, may indicate binary incompatibility. Expected 72 from C header, got 64 from PyObject", "issue_description": "chrisnweb commented on 13 Nov 2019\nEnvironment Information: Jupyter Notebooks\nWhen do the following:\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nI received error as shown below\nValueError Traceback (most recent call last)\nin ()\n1 import spacy\n----> 2 nlp = spacy.load('en_core_web_lg')\n~\\Anaconda3\\lib\\site-packages\\spacy_init_.py in load(name, **overrides)\n13 if depr_path not in (True, False, None):\n14 deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 15 return util.load_model(name, **overrides)\n16\n17\n~\\Anaconda3\\lib\\site-packages\\spacy\\util.py in load_model(name, **overrides)\n112 return load_model_from_link(name, **overrides)\n113 if is_package(name): # installed as package\n--> 114 return load_model_from_package(name, **overrides)\n115 if Path(name).exists(): # path to model data directory\n116 return load_model_from_path(Path(name), **overrides)\n~\\Anaconda3\\lib\\site-packages\\spacy\\util.py in load_model_from_package(name, **overrides)\n133 \"\"\"Load a model from an installed package.\"\"\"\n134 cls = importlib.import_module(name)\n--> 135 return cls.load(**overrides)\n136\n137\n~\\Anaconda3\\lib\\site-packages\\en_core_web_lg_init_.py in load(**overrides)\n10\n11 def load(**overrides):\n---> 12 return load_model_from_init_py(file, **overrides)\n~\\Anaconda3\\lib\\site-packages\\spacy\\util.py in load_model_from_init_py(init_file, **overrides)\n171 if not model_path.exists():\n172 raise IOError(Errors.E052.format(path=path2str(data_path)))\n--> 173 return load_model_from_path(data_path, meta, **overrides)\n174\n175\n~\\Anaconda3\\lib\\site-packages\\spacy\\util.py in load_model_from_path(model_path, meta, **overrides)\n141 if not meta:\n142 meta = get_model_meta(model_path)\n--> 143 cls = get_lang_class(meta['lang'])\n144 nlp = cls(meta=meta, **overrides)\n145 pipeline = meta.get('pipeline', [])\n~\\Anaconda3\\lib\\site-packages\\spacy\\util.py in get_lang_class(lang)\n48 if lang not in LANGUAGES:\n49 try:\n---> 50 module = importlib.import_module('.lang.%s' % lang, 'spacy')\n51 except ImportError:\n52 raise ImportError(Errors.E048.format(lang=lang))\n~\\Anaconda3\\lib\\importlib_init_.py in import_module(name, package)\n124 break\n125 level += 1\n--> 126 return _bootstrap._gcd_import(name[level:], package, level)\n127\n128\n~\\Anaconda3\\lib\\importlib_bootstrap.py in _gcd_import(name, package, level)\n~\\Anaconda3\\lib\\importlib_bootstrap.py in find_and_load(name, import)\n~\\Anaconda3\\lib\\importlib_bootstrap.py in find_and_load_unlocked(name, import)\n~\\Anaconda3\\lib\\importlib_bootstrap.py in _load_unlocked(spec)\n~\\Anaconda3\\lib\\importlib_bootstrap_external.py in exec_module(self, module)\n~\\Anaconda3\\lib\\importlib_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\n~\\Anaconda3\\lib\\site-packages\\spacy\\lang\\en_init_.py in ()\n13 from ..tokenizer_exceptions import BASE_EXCEPTIONS\n14 from ..norm_exceptions import BASE_NORMS\n---> 15 from ...language import Language\n16 from ...attrs import LANG, NORM\n17 from ...util import update_exc, add_lookups\n~\\Anaconda3\\lib\\site-packages\\spacy\\language.py in ()\n16 from .vocab import Vocab\n17 from .lemmatizer import Lemmatizer\n---> 18 from .pipeline import DependencyParser, Tensorizer, Tagger, EntityRecognizer\n19 from .pipeline import SimilarityHook, TextCategorizer, SentenceSegmenter\n20 from .pipeline import merge_noun_chunks, merge_entities\n~\\Anaconda3\\lib\\site-packages\\spacy\\pipeline_init_.py in ()\n2 from future import unicode_literals\n3\n----> 4 from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker\n5 from .pipes import TextCategorizer, Tensorizer, Pipe, Sentencizer\n6 from .morphologizer import Morphologizer\npipes.pyx in init spacy.pipeline.pipes()", "issue_status": "Open", "issue_reporting_time": "2019-11-13T06:11:35Z"}, "66": {"issue_url": "https://github.com/explosion/spaCy/issues/4633", "issue_id": "#4633", "issue_summary": "Slow training speed for textcat pipeline", "issue_description": "Contributor\nZhuoruLin commented on 13 Nov 2019 \u2022\nedited\nHi!\nI am trying to train a textcat pipeline with over 6000 classes. The training data consists of around 300k documents. I tried to convert my training data to the correct jsonl format but that would result in a file size of over 100G. And the initialization of GoldCorpus would take forever in writing the message packs. Therefore I wrote the following TextcatGoldCorpus class:\nclass TextcatGoldCorpus(object):\n    _textcat_cols = ['text', 'topic_id']\n\n    def __init__(self, train, dev, labels):\n        self.train = pd.read_csv(train, names=self._textcat_cols, header=None)\n        self.dev = pd.read_csv(dev, names=self._textcat_cols, header=None)\n        self.labels = labels\n\n    def train_docs(self):\n        train_shuffled = self.train.sample(frac=1)\n        datasets = self.iter_textcat_df(train_shuffled, )\n        yield from datasets\n\n    def dev_docs(self):\n        datasets = self.iter_textcat_df(self.dev, )\n        yield from datasets\n\n    def iter_textcat_df(self, textcat_df):\n        for i, (text, topic_id) in textcat_df.iterrows():\n            yield (text, {'cats': self.textcat_annotate(topic_id)})\n\n    def textcat_annotate(self, gold_topic_id):\n        cats = dict([(topic_id, True) if topic_id == gold_topic_id else (topic_id, False) for topic_id in self.labels])\n        return cats\nThen I write a regular train loop to call nlp.update for batch size of 64. However, the training is so slow. I am using an Nvidia V100 GPU and the average update speed is around 2-3 documents/second. This would take around two days to train one epoch for my task. I also notice the GPU training does not gain any significant speedup from CPU.\nI used to train a Convolution model (with PyTorch) on the exact same task and each epoch takes around 3 to 4 hours. I also used to fine-tune Bert Base model on classification task and the entire training finished in around one day with 3 epochs.\nI have almost no idea about the potential cause of this slowdown. Please give me some suggestions. Thanks!", "issue_status": "Open", "issue_reporting_time": "2019-11-12T20:44:44Z"}, "67": {"issue_url": "https://github.com/explosion/spaCy/issues/4629", "issue_id": "#4629", "issue_summary": "Weighted NER Loss", "issue_description": "atakanokan commented on 12 Nov 2019\nFeature description\nUsing different weights for NER classes to enable learning towards more important classes. For example if detecting PERSONs is much more important to my task than detecting LOCATIONs, I would love to configure the loss function so that it PERSON's contribution is much higher.\nIt can be a custom component that users can pass a dictionary or two lists that reweighs the loss function based on the users' needs. custom component", "issue_status": "Open", "issue_reporting_time": "2019-11-12T15:09:14Z"}, "68": {"issue_url": "https://github.com/explosion/spaCy/issues/4625", "issue_id": "#4625", "issue_summary": "SpaCy should warn when pipelines aren't enabled", "issue_description": "Contributor\nerip commented on 12 Nov 2019\nHow to reproduce the behaviour\nimport random\n\nfrom spacy.gold import GoldParse\n\nrandom.seed(42)\n\n# Ensures that only models without a pipeline pass to the training logic\nassert len(list(nlp.pipeline)) == 0\n\noptimizer = nlp.begin_training()\nfor itn in range(100):\n    random.shuffle(train_data)\n    docs = []\n    golds = []\n    for raw_text, tokenizations, pos, ner in train_data:\n        doc = nlp.make_doc(raw_text)\n        gold = GoldParse(doc, words=tokenizations, tags=pos, entities=ner)\n        docs.append(doc)\n        golds.append(gold)\n    nlp.update(docs, golds, drop=0.5, sgd=optimizer)\nThis will train \"completely\", but won't yield a usable model because the tagging and entity recognizer pipelines haven't been added. It may be wise to log a warning if a kwarg corresponding to a non-existent component is passed to nlp.update via GoldParse.\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.7", "issue_status": "Open", "issue_reporting_time": "2019-11-11T21:30:36Z"}, "69": {"issue_url": "https://github.com/explosion/spaCy/issues/4622", "issue_id": "#4622", "issue_summary": "German adjectives ending on `-e` are not lemmatized", "issue_description": "SuzanaK commented on 11 Nov 2019\nHow to reproduce the behaviour\nimport spacy\nnlp = spacy.load('de')\ns1 = 'Der sch\u00f6ne Garten'                                                                                                                                                             \ndoc = nlp(s1)                                                                                                                                                                        \n[(t, t.lemma_) for t in doc]                                                                                                                                                         \n >> [(Der, 'der'), (sch\u00f6ne, 'sch\u00f6ne'), (Garten, 'Garten')]\n\ns2 = 'Ein sch\u00f6ner Garten'  \ndoc = nlp(s2)                                                                                                                                                                        \n[(t, t.lemma_) for t in doc]                                                                                                                                                         \n>> [(Ein, 'Ein'), (sch\u00f6ner, 'sch\u00f6n'), (Garten, 'Garten')]\nMy Environment\nspaCy version: 2.2.2\nPlatform: Linux-5.0.0-25-generic-x86_64-with-LinuxMint-19.2-tina\nPython version: 3.6.7\nModels: de\nReason\nAs far as I can see, all forms of German adjectives ending on e in spacy-lookups-data/spacy_lookups_data/data/de_lemma_lookup.json are capitalized, e.g.:\n\"Dekorative\": \"dekorativ\",\n\"Wei\u00dfe\": \"Wei\u00df\",\n\"Sch\u00f6ne\": \"Sch\u00f6nes\",", "issue_status": "Open", "issue_reporting_time": "2019-11-11T10:38:31Z"}, "70": {"issue_url": "https://github.com/explosion/spaCy/issues/4592", "issue_id": "#4592", "issue_summary": "Function to list installed models or to check if a model is installed", "issue_description": "DavidNemeskey commented on 5 Nov 2019\nFeature description\nIt would be great if spaCy had functions to list all installed models and to check if a particular model is installed.\nMy use-case is that if that when the user of my script specifies a model that spacy.load doesn't find, I try to install the model via the cli module. I am using multiprocessing, and since I do not want all processes to download the model, I run this code before I fire up the pool. However, as it is, now I load the model twice (once in the main process and once in the worker processes).\nIt would be much nicer if I could just check in the main process if the model is installed, download it if not, and only load the model in the worker processes. Alternatively, I would argue that just listing the models available is a useful feature.\nI know that the cli module has some functions that do something similar; validate(), for instance lists the models. However, it outputs the results to stdout instead of returning data structures we could work with in Python. I believe that separating the model and the view would make for a better user (programmer) experience.", "issue_status": "Open", "issue_reporting_time": "2019-11-05T16:05:16Z"}, "71": {"issue_url": "https://github.com/explosion/spaCy/issues/4576", "issue_id": "#4576", "issue_summary": "Matcher breaks with \"complex\" patterns ppc64le", "issue_description": "rjknight commented on 2 Nov 2019\nHello, I am seeing failures in the matcher unit tests, when complex patterns are involved.\nIt seems very similar to this older issue Matcher breaks with \"complex\" patterns #3328\nHow to reproduce the behavior\nrun the following sample code from issue 3328:\nimport spacy\nfrom spacy.tokens import Token, Doc, Span\nfrom spacy.matcher import Matcher, PhraseMatcher\n\ntext = \"Hello, how are you doing?\"\n\nnlp = spacy.load('en_core_web_md')\n\nmatcher = Matcher(nlp.vocab)\npatterns = [\n    [{'LOWER': {'IN': [\"hello\", \"how\"]}}],\n    [{'LOWER': {'IN': [\"you\", \"doing\"]}}],\n]\n\nmatcher.add('TEST', None, *patterns)\n\ndoc = nlp(text)\n\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = Span(doc, start, end, label=match_id)\n    print(span.text)\nExected results\nrunning in an x86 container I see the following\n(wmlce) pwrai@8d94e9e4cbc2:~$ python ./testme.py \nHello\nhow\nyou\ndoing\n(wmlce) pwrai@8d94e9e4cbc2:~$ python -m spacy info --markdown\n\n## Info about spaCy\n\n* **spaCy version:** 2.2.1\n* **Platform:** Linux-3.10.0-957.5.1.el7.x86_64-x86_64-with-debian-buster-sid\n* **Python version:** 3.7.3\nActual results\nwhen running the same sample code in a ppc64le container I see the following:\n(wmlce) pwrai@ed647b42dde1:~$ python testme.py \nHello\n,\nhow\nare\nyou\ndoing\n?\n(wmlce) pwrai@ed647b42dde1:~$ python -m spacy info --markdown\n\n## Info about spaCy\n\n* **spaCy version:** 2.2.1\n* **Platform:** Linux-4.14.0-115.10.1.el7a.ppc64le-ppc64le-with-debian-buster-sid\n* **Python version:** 3.7.4\n* **Models:** en", "issue_status": "Open", "issue_reporting_time": "2019-11-02T13:49:40Z"}, "72": {"issue_url": "https://github.com/explosion/spaCy/issues/4572", "issue_id": "#4572", "issue_summary": "Approximate Nearest Neighbors for Aliases in KB for Entity Linking", "issue_description": "Contributor\nkabirkhan commented on 2 Nov 2019\nFeature description\nCurrently, the EntityLinker is looking for exact match aliases. While this might be ideal for some entity types and even for the WikiData KB in general, there are some obvious cases where this won't work well.\nSpecifically for custom knowledge bases, I think it makes sense to compute an Approximate Nearest Neighbors index for all aliases in a knowledge base using character ngrams and tfidf.\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.\nThis could be a custom pipeline component but I think it makes more sense to have this functionality present in the EntityLinker. Open to discussion on this though.", "issue_status": "Open", "issue_reporting_time": "2019-11-02T00:22:32Z"}, "73": {"issue_url": "https://github.com/explosion/spaCy/issues/4570", "issue_id": "#4570", "issue_summary": "user_data or UnderScore in spacy.kb.Candidate", "issue_description": "Contributor\ntamuhey commented on 2 Nov 2019\nFeature description\nIf spacy.kb.Candidate can contain more information, it is very useful, e.g. it can represent knowledge graph node.\nLike Doc, I think it can be achiedved if Candidate have user_data or UnderScore.", "issue_status": "Open", "issue_reporting_time": "2019-11-01T18:48:02Z"}, "74": {"issue_url": "https://github.com/explosion/spaCy/issues/4562", "issue_id": "#4562", "issue_summary": "No entities retrieved by Custom Hindi NER model", "issue_description": "adridjs commented on 31 Oct 2019\nHello, I'm trying to detect entities with a custome NER spacy model. I trained it on wikipedia articles via CLI and it gave an F-Score of around 75%. Now I'm trying to test it with articles from the web, even with entities it has seen while training, but it is not detecting any, that is doc.ents is an empty tuple. I packaged it as the docs say and it loads without problem. I also trained a POS with a different dataset and it is working properly.\nI also checked that the tags are there (LOC, PER and ORG) and it is a blank new model too, so the case of \"catastrophic forgetting\" is not applying. What am I missing on the NER model?\nspaCy version: 2.2.0\nPlatform: Linux-4.4.0-165-generic-x86_64-with-Ubuntu-16.04-xenial\nPython version: 3.6.9", "issue_status": "Open", "issue_reporting_time": "2019-10-31T11:23:12Z"}, "75": {"issue_url": "https://github.com/explosion/spaCy/issues/4550", "issue_id": "#4550", "issue_summary": "CharacterEmbed broken in architecture refactor", "issue_description": "Member\nsvlandeg commented on 29 Oct 2019\nWith respect to current code on master branch.\nhttps://github.com/explosion/spaCy/blob/master/spacy/ml/tok2vec.py#L88 doesn't work because it needs to refer to https://github.com/explosion/spaCy/blob/master/spacy/_ml.py#L913 instead of to its own constructor (the one in _ml.py then probably needs to be moved or you'd get a circular dependency from _ml to ml.py and back?)\nFurther issues: https://github.com/explosion/spaCy/blob/master/spacy/ml/tok2vec.py#L20:\ntok2vec = chain(doc2feats, with_flatten(chain(embed, encode), pad=field_size))\nThis works when embed is MultiHashEmbed, but doesn't work if it's CharacterEmbed because the latter needs access to the original Doc, and thus perhaps doc2feats and embed need to be concatenate_lists()'d in this case?\nStumbled upon all this when running ud_train.py, which uses build_morphologizer_model() which sets\nchar_embed = cfg.get(\"char_embed\", True)\nSetting that to False, makes the script run again.\n@honnibal : considering this is new code and I assume you're still working on this, let me know if you'd rather fix this yourself or if you'd like me to have a stab at a PR.", "issue_status": "Open", "issue_reporting_time": "2019-10-29T11:00:25Z"}, "76": {"issue_url": "https://github.com/explosion/spaCy/issues/4523", "issue_id": "#4523", "issue_summary": "[conllu converter] Support raw_text field", "issue_description": "Contributor\ntamuhey commented on 25 Oct 2019\nspacy convert miss the information of raw text, it is often shown # text = comment line in conllu format. (e.g. https://github.com/UniversalDependencies/UD_English-EWT/blob/master/en_ewt-ud-dev.conllu)\nI found the unused variable raw_text in the below line, but why isn't it used?\nspaCy/spacy/cli/converters/conllu2json.py\nLine 25 in d2da117\n for i, (raw_text, tokens) in enumerate(conll_tuples): ", "issue_status": "Open", "issue_reporting_time": "2019-10-25T15:30:01Z"}, "77": {"issue_url": "https://github.com/explosion/spaCy/issues/4519", "issue_id": "#4519", "issue_summary": "Custom morphological analysis", "issue_description": "ryszardtuora commented on 24 Oct 2019 \u2022\nedited\nHey!\nI'm working on POS tagging for Polish for my model. It is a language with very rich morphology, and we would like to offer some information about this to the user. But becasue of the sheer number of tags, training the POS tagger to recognize all the fine-grained tags of the NKJP tagset that we work with is impossible (it has thousands of combinations of few dozens morphological features). For this reason we've cut down the tagset to 35 part of speech tags, and completely disregard morphology in the tagger.\nHowever we have a separate tool which is able to do morphological analysis very well, and we aim to integrate it. Ideally we would want to write the features directly to the token.morph, but this is prohibited. If I understand it correctly, an indirect assignment would have to go through the tag_map, but I'd not risk causing troubles to the POS tagger in this way.\nI was also thinking about writing this information directly to token.tag_, e.g. turning SUBST (corresponding to UD NOUN) into SUBST:SG:ACC:F, but I am worried thatthis would cause problems somewhere else.\nHow would you go about this task?", "issue_status": "Open", "issue_reporting_time": "2019-10-24T16:30:49Z"}, "78": {"issue_url": "https://github.com/explosion/spaCy/issues/4497", "issue_id": "#4497", "issue_summary": "token.left_edge/right_edge gets out of sync when manually modifying doc heads", "issue_description": "jessecoleman commented on 22 Oct 2019\nI'm doing some post processing on the output from the parse pipe. As part of this process, I generate multiple document roots by setting doc[i].head = doc[i]. I would expect this to recreate the dependency tree and recompute all the associated attributes. It works as expected when calling .subtree, .ancestors, .lefts, and .rights, but .left_edge and .right_edge use the old tree structure.\nHow to reproduce the behaviour\nimport spacy\nen = spacy.load('en_core_web_sm')\ndoc = en('apple pie')\nassert doc[1].left_edge == doc[0]\ndoc[0].head = doc[0]\nassert list(doc[1].subtree) == [doc[1]] # fine\nassert list(doc[1].lefts) == [] # fine\nassert list(doc[0].ancestors) == [] # fine\nassert doc[1].left_edge == doc[1] # AssertionError\nYour Environment\nOperating System: Ubuntu 16.04\nPython Version Used: 3.6.8\nspaCy Version Used: 2.2.1", "issue_status": "Open", "issue_reporting_time": "2019-10-21T21:18:31Z"}, "79": {"issue_url": "https://github.com/explosion/spaCy/issues/4441", "issue_id": "#4441", "issue_summary": "Extracting Verb Phrases (VP) using spaCy", "issue_description": "phosseini commented on 15 Oct 2019 \u2022\nedited\nFeature description\nIt would be nice if there be a verb chunking method for extracting Verb Phrases (VP) using spaCy. There's already a Noun Phrase (NP) chunker but I wonder why there's no VP chunker?\nAlso, one thing to think about when developing the VP chunker is resolving the overlapping VPs issue. For example, we can extract VPs using CoreNLP's tregex, but the problem is that we sometimes have VPs with overlaps. So it would be nice if the output of the VP chunker in spaCy have this issue already resolved.", "issue_status": "Open", "issue_reporting_time": "2019-10-14T21:38:23Z"}, "80": {"issue_url": "https://github.com/explosion/spaCy/issues/4440", "issue_id": "#4440", "issue_summary": "\"Copying\" a Doc with to_array/from_array does not yield the same SENT_START", "issue_description": "Wirg commented on 15 Oct 2019 \u2022\nedited\nContext\nI am playing around building new Docs in a pipeline to remove sentences (https://stackoverflow.com/questions/58368208/filtering-out-sentences-between-sentence-segmentation-and-other-spacy-components?noredirect=1#comment103103941_58368208).\nI noticed that I could not make SENT_START work with from_array. I expect to have the same sentences that what I had previously and end up with one sentence by token.\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy import attrs\nimport numpy as np\n\nATTRIBUTES_TO_RESTORE = [attrs.ORTH, attrs.LEMMA, attrs.SHAPE, attrs.IS_STOP, attrs.DEP, attrs.LOWER, attrs.POS, attrs.TAG, attrs.IS_ALPHA, attrs.SENT_START]\n\ndef copy_doc(doc: Doc) -> Doc:\n    return Doc(\n        doc.vocab,\n        words=list(map(str, doc)),\n        spaces=list(map(lambda token: token.whitespace_ != '', doc))\n    ).from_array(ATTRIBUTES_TO_RESTORE, doc.to_array(ATTRIBUTES_TO_RESTORE))\n\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(\"This is a short sentence. Too short. This is a really really long sentence with a lot of junk.\")\ndoc_copy = copy_doc(doc)\nfor attr in ATTRIBUTES_TO_RESTORE:\n    print(attrs.NAMES[attr])\n    # Expect no errors, raise for attrs.SENT_START\n    np.testing.assert_array_equal(doc.to_array([attr]), doc_copy.to_array([attr]))\nYour Environment\nspaCy version: 2.2.0\nPlatform: Linux-4.4.0-18362-Microsoft-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.7\nEDIT:\nAfter some fiddling, it seems like I should not be using attrs.SENT_START but attrs.HEAD in my case as I use dep.\nIt seems like I should probably have an error message :\nspaCy/spacy/tokens/doc.pyx\nLines 785 to 786 in f2d2247\n if SENT_START in attrs and HEAD in attrs: \n     raise ValueError(Errors.E032) \nSo :\nATTRIBUTES_TO_RESTORE = [attrs.SENT_START] works if I don't have HEAD and/or DEP\nATTRIBUTES_TO_RESTORE = [attrs.HEAD, attrs.DEP] works if I don't have SENT_START\nI guess that the previous snippet should be replaced by something like :\n        if SENT_START in attrs and (HEAD in attrs or DEP in attrs):\n            raise ValueError(Errors.E032)\nShould I do a PR ?\n1", "issue_status": "Open", "issue_reporting_time": "2019-10-14T21:06:39Z"}, "81": {"issue_url": "https://github.com/explosion/spaCy/issues/4439", "issue_id": "#4439", "issue_summary": "Doc.tensor into user_hooks", "issue_description": "Contributor\ntamuhey commented on 14 Oct 2019\nI want to customize Doc.tensor property by user_hooks like Doc.vector, because sometimes calculation of the tensor is expensive (e.g. tok2vec pipe in spacy-transformers)", "issue_status": "Open", "issue_reporting_time": "2019-10-14T14:52:05Z"}, "82": {"issue_url": "https://github.com/explosion/spaCy/issues/4433", "issue_id": "#4433", "issue_summary": "DependencyMatcher documentation", "issue_description": "fabio-reale commented on 11 Oct 2019\nI would like to use the DependencyMatcher, which I learned to exist reading this issue. There, I also learned there is no documentation for it.\nI figured I would learn it from the code and testing, which I might end up doing, but I also figured I could try and ask for some help. I'm having some difficulty understanding what all of the operators are supposed to do (these ones for example: \">>\", \".\", \"$+\").\nOnce I get it, I fully intend to help with this documentation the best I can. So, any help or advice about either the DependencyMatcher or how to contribute to documentation are welcome\nWhich page or section is this issue related to?\nI assume the correct place for this documentation would be rule based matching page\n2", "issue_status": "Open", "issue_reporting_time": "2019-10-11T17:22:28Z"}, "83": {"issue_url": "https://github.com/explosion/spaCy/issues/4417", "issue_id": "#4417", "issue_summary": "Support for Norwegian Nynorsk", "issue_description": "emilmuller commented on 9 Oct 2019\nIt would be great to also have a model for Norwegian Nynorsk (in addition to Bokm\u00e5l). If I'm not mistaken, the Bokm\u00e5l model is based on data from the University of Oslo. The University of Oslo usually releases Bokm\u00e5l data together with Nynorsk data, so I'm guessing the exact same databases can be obtained as was used in training the Bokm\u00e5l model.", "issue_status": "Open", "issue_reporting_time": "2019-10-09T14:10:20Z"}, "84": {"issue_url": "https://github.com/explosion/spaCy/issues/4416", "issue_id": "#4416", "issue_summary": "tok2vec missing", "issue_description": "davidbren commented on 9 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nI upgraded my local spaCy version from 2.0.18 to 2.1.18\nI built a custom model NER model on a unix box which also has spaCy 2.1.18\nIn order to visually debug it I copy the model over to my local environment\nWhen I run the following line\nner_model = spacy.load(modelDir)\nIt throws a file not found error on C:\\myModel\\ner\\tok2vec_model\nIn older version 2.0.8 when I had build models it did produce the above file but it seems this has changed & but the install has copied the necessary files when I upgraded\nError Stack is\nFile \"C:\\Python36\\lib\\site-packages\\spacy_init_.py\", line 21, in load\nreturn util.load_model(name, **overrides)\nFile \"C:\\Python36\\lib\\site-packages\\spacy\\util.py\", line 116, in load_model\nreturn load_model_from_path(Path(name), **overrides)\nFile \"C:\\Python36\\lib\\site-packages\\spacy\\util.py\", line 156, in load_model_from_path\nreturn nlp.from_disk(model_path)\nFile \"C:\\Python36\\lib\\site-packages\\spacy\\language.py\", line 647, in from_disk\nutil.from_disk(path, deserializers, exclude)\nFile \"C:\\Python36\\lib\\site-packages\\spacy\\util.py\", line 511, in from_disk\nreader(path / key)\nFile \"C:\\Python36\\lib\\site-packages\\spacy\\language.py\", line 643, in\ndeserializers[name] = lambda p, proc=proc: proc.from_disk(p, vocab=False)\nFile \"nn_parser.pyx\", line 912, in spacy.syntax.nn_parser.Parser.from_disk\nFile \"C:\\Python36\\lib\\pathlib.py\", line 1162, in open\nopener=self._opener)\nFile \"C:\\Python36\\lib\\pathlib.py\", line 1016, in _opener\nreturn self._accessor.open(self, flags, mode)\nFile \"C:\\Python36\\lib\\pathlib.py\", line 388, in wrapped\nreturn strfunc(str(pathobj), *args)\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\myModel\\ner\\tok2vec_model'\nYour Environment\nOperating System: Win 7\nPython Version Used: 3.6\nspaCy Version Used: 2.1.8\nEnvironment Information:", "issue_status": "Open", "issue_reporting_time": "2019-10-09T14:04:03Z"}, "85": {"issue_url": "https://github.com/explosion/spaCy/issues/4411", "issue_id": "#4411", "issue_summary": "Make it possible to merge Vocab/StringStore instances", "issue_description": "johann-petrak commented on 9 Oct 2019 \u2022\nedited\nFeature description\nHere is the background: in order to use a spacy document it is necessary to have the correct Vocab/StringStore. But when documents are created/processed in a distributed/multiprocessing way, different subsets of documents will get processed, linked to the vocab of the process's\nvocab (in nlp). In order to save a processed document efficiently, one uses \"to_disk\" to save it without the vocab.\nWhen resuming processing or just when one wants to load the document later, a vocab is needed that has all the entries from all the parallel processes combined in order to ensure the document can be deserialised.\nCould the feature be a custom component\nI do not think so.", "issue_status": "Open", "issue_reporting_time": "2019-10-09T11:06:03Z"}, "86": {"issue_url": "https://github.com/explosion/spaCy/issues/4410", "issue_id": "#4410", "issue_summary": "When will the Chinese model be supported?", "issue_description": "suozq1 commented on 9 Oct 2019\nWhen will the Chinese model be supported?", "issue_status": "Open", "issue_reporting_time": "2019-10-09T10:20:54Z"}, "87": {"issue_url": "https://github.com/explosion/spaCy/issues/4409", "issue_id": "#4409", "issue_summary": "Add checks (and converters?) for documents with multiple sentences in debug-data", "issue_description": "Collaborator\nadrianeboyd commented on 9 Oct 2019\nFeature description\nThe parser section of spacy debug-data should show a warning when there are no/few documents with multiple sentences in the training data.\nPotentially add a simple converter to spacy convert to group sentences, similar to -n with the IOB converters. A bit of variety in document lengths is probably a good idea here, too, rather than just -n N, but I don't know if it makes that much difference in the model performance.\n2", "issue_status": "Open", "issue_reporting_time": "2019-10-09T09:30:20Z"}, "88": {"issue_url": "https://github.com/explosion/spaCy/issues/4391", "issue_id": "#4391", "issue_summary": "Displacy renderer for token level heatmap", "issue_description": "galtay commented on 7 Oct 2019\nFeature description\nThe displacy visualization tool is great. The parser and named entity visualizations are very useful. Would it be possible to add a renderer for token level \"heatmap\" style visualizations? This could be used (for example) to show token level model activation or to show the strength of an attention like vector over a set of tokens. I imagine it could be very similar to the named entity renderer (with text and token offsets as input) but then also a way to specify a color (or maybe an index into an arbitrary color map) for each token.", "issue_status": "Open", "issue_reporting_time": "2019-10-07T15:18:33Z"}, "89": {"issue_url": "https://github.com/explosion/spaCy/issues/4390", "issue_id": "#4390", "issue_summary": "After to_bytes without vocab, and from_bytes, lang_ is None in Doc objects", "issue_description": "gthb commented on 7 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nThe following was written to not have to wait 15 seconds on each pytest invocation of our test suite, where we need to tokenize strings exactly as Spacy's language model does, but don't need the vocab:\ndef get_spacy_tokenizer(lang: str) -> Language:\n    \"\"\"Returns just the tokenizer part extracted from the given Spacy language model, cached on disk. Loads faster, so tests start up faster\"\"\"\n    tokenizer_filename = f\"{lang}_tokenizer.bytes\"\n    if not os.path.exists(tokenizer_filename):\n        lang_model = get_language_model(lang)\n        with open(tokenizer_filename, 'wb') as tokenizer_file:\n            tokenizer_file.write(lang_model.to_bytes(exclude=['vocab']))\n    with open(tokenizer_filename, 'rb') as tokenizer_file:\n        lang_model = Language().from_bytes(tokenizer_file.read())\n        lang_model.meta[\"lang\"] = lang\n        return lang_model\n(where get_language_model just looks up the model name we've configured for the language, and calls spacy.load)\nThat works great, speeds up test startup a lot ... but then it turns out that the Doc objects from this language model have doc.lang_ is None, despite the meta attribute being present on the language model (I'm poking it in there, trying unsuccessfully to work around this).\nThis is because doc.lang_ is a property proxying to doc.vocab.lang and of course doc.vocab is not loaded.\nBut the language is a property of the language model itself, not just of its vocabulary (though of course they ought to match). So I don't think doc.lang_ should break just because the vocabulary isn't loaded.\nInfo about spaCy\nspaCy version: 2.1.6\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.4", "issue_status": "Open", "issue_reporting_time": "2019-10-07T12:42:13Z"}, "90": {"issue_url": "https://github.com/explosion/spaCy/issues/4384", "issue_id": "#4384", "issue_summary": "Uncommon hyphen doesn't get recognized", "issue_description": "KristiyanVachev commented on 6 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nEnough said.\nThe hyphen in question is '\u2013' in '4\u201315 kg'. The hyphen that's recognized is the regular '-'.\nThe text comes from SQuAD v1 dataset.\nYour Environment\nOperating System: Windows-10-10.0.18362-SP0\nPython Version Used: Python 3.7.2\nspaCy Version Used: 2.1.8\nEdit: * spaCy Version 2.2.1 also includes the bug", "issue_status": "Open", "issue_reporting_time": "2019-10-06T12:18:26Z"}, "91": {"issue_url": "https://github.com/explosion/spaCy/issues/4369", "issue_id": "#4369", "issue_summary": "Error using pretrain when building a model with embedded word vectors", "issue_description": "Contributor\ngustavengstrom commented on 3 Oct 2019\nI am trying to train a Swedish model with tagging, parsing and embedded word vectors for similarity scoring. Training data comes from https://github.com/UniversalDependencies/UD_Swedish-Talbanken and word vectors are trained using gensim.\nTraining a small model with pretraining but without embedded vectors works from the command line. Likewise a large model with embedded vectors but without pretraining also works great. The problem arises when trying\ntrying to train a large model with embedded vectors and pretraining.\nAs I understand it \"spacy pretrain\" uses the command --use-vectors argument is used if you want the word model to include features from the word vectors.\nHowever, although pretrain works fine with the --use-vectors command the \"spacy train\" command fails.\nHow to reproduce the behaviour\npython -m spacy init-model sv ./init_models/word2vec -v ./vectors/word2vec.txt\npython -m spacy pretrain './corpus/sents_webbnyheter2013.jsonl' './init_models/word4vec' ./pretrained/ud_w2v' --use-vectors -i 100\npython -m spacy train sv models_temp ./corpus/ud_swedish_talbanken_json_sent10/sv_talbanken-ud-train.json ./corpus/ud_swedish_talbanken_json_sent10/sv_talbanken-ud-dev.json -p 'tagger,parser' -t2v ./pretrained/ud_w2v/model99.bin -g 0 -n 55\nProduces the following error\nTraining pipeline: ['tagger', 'parser']\nStarting with blank model 'sv'\nCounting training words (limit=0)\nLoaded pretrained tok2vec for: ['tagger', 'parser']\nItn Tag Loss Tag % Dep Loss UAS LAS Token % CPU WPS GPU WPS\n\u2714 Saved model to output directory\nmodels_temp/model-final\n\u2819 Creating best model...\nTraceback (most recent call last):\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/spacy/cli/train.py\", line 365, in train\nlosses=losses,\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/spacy/language.py\", line 516, in update\nproc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\nFile \"nn_parser.pyx\", line 424, in spacy.syntax.nn_parser.Parser.update\nFile \"_parser_model.pyx\", line 214, in spacy.syntax._parser_model.ParserModel.begin_update\nFile \"_parser_model.pyx\", line 262, in spacy.syntax._parser_model.ParserStepModel.init\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\nX, inc_layer_grad = layer.begin_update(X, drop=drop)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/thinc/api.py\", line 295, in begin_update\nX, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad), drop=drop)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\nX, inc_layer_grad = layer.begin_update(X, drop=drop)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/thinc/api.py\", line 379, in uniqued_fwd\nY_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\nX, inc_layer_grad = layer.begin_update(X, drop=drop)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/layernorm.py\", line 60, in begin_update\nX, backprop_child = self.child.begin_update(X, drop=0.0)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/maxout.py\", line 76, in begin_update\noutput__boc = self.ops.gemm(X__bi, W, trans2=True)\nFile \"ops.pyx\", line 860, in thinc.neural.ops.CupyOps.gemm\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/cupy/linalg/product.py\", line 35, in dot\nreturn a.dot(b, out)\nFile \"cupy/core/core.pyx\", line 1306, in cupy.core.core.ndarray.dot\nFile \"cupy/core/core.pyx\", line 1940, in cupy.core.core.dot\nValueError: Axis dimension mismatch\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \"/home/gustav/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"/home/gustav/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\nexec(code, run_globals)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/spacy/main.py\", line 35, in\nplac.call(commands[command], sys.argv[1:])\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/plac_core.py\", line 328, in call\ncmd, result = parser.consume(arglist)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/plac_core.py\", line 207, in consume\nreturn cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/spacy/cli/train.py\", line 486, in train\nbest_model_path = _collate_best_model(meta, output_path, nlp.pipe_names)\nFile \"/home/gustav/anaconda3/lib/python3.7/site-packages/spacy/cli/train.py\", line 554, in _collate_best_model\npath2str(best_component_src / component), path2str(best_dest / component)\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'str'\nYour Environment\nOperating System: Ubuntu 19.04\nPython Version Used: 3.7\nspaCy Version Used: 2.1", "issue_status": "Open", "issue_reporting_time": "2019-10-03T13:19:36Z"}, "92": {"issue_url": "https://github.com/explosion/spaCy/issues/4338", "issue_id": "#4338", "issue_summary": "\ud83e\udde9 Plugins and project ideas master thread", "issue_description": "Member\nines commented on 29 Sep 2019 \u2022\nedited\nI was going though the existing enhancement issues again and though it'd be nice to collect ideas for spaCy plugins and related projects. There are always people in the community who are looking for new things to build, so here's some inspiration For existing plugins and projects, check out the spaCy universe.\nIf you have questions about the projects I suggested, or the spaCy plugin system in general, I should also be able to help. And if you're looking for collaborators or there's a plugin you'd love to see built, feel free to comment here as well.\nVisual Studio Code extension (#2969)\nI started on a little spaCy snippets extension ages ago and never really quite finished it. But I always thought it'd be cool to have a spaCy extension with some helpers and maybe some deeper pipeline, data structures and model inspection tools. I haven't really worked with VSCode plugins (yet), but maybe someone from the community has an idea and/or experience? Would be cool to work on this together!\nPandas helpers and utilities (#3702)\nI think some helpers for pandas could be a nice spaCy plugin? We wouldn't want to ship anything that depends on pandas in the core library, but I can totally see a little helper library that depends on spaCy and pandas and includes useful functions to represent a spaCy Doc as a dataframe.\nSee: https://github.com/yash1994/dframcy\nWrappers for debugging pipeline components\nInspired by this Stack Overflow question: https://stackoverflow.com/a/57964354/6400719. Could be a helper that wraps the nlp object and logs processing time and other useful details. I also have a bunch of draft code I'm happy to share if someone wants to work on this. (Also see #3943 for related functionality we want to ship in spaCy.)\nProject starter as GitHub repo template\nGitHub now supports template repos, so it could be cool to have a \"spaCy project starter\" template that's set up as a Python package, includes some basic scaffolding around loading models and processing texts, and maybe exposes a small REST API using FastAPI.\nmicrosoft/cookiecutter-spacy-fastapi by @kabirkhan\nspaCy + Apache Beam\nThread with notebook and discussion: https://twitter.com/swartchris8/status/1194192895244480512 A package could, for instance, wrap the boilerplate code so all the user has to do is pass in an nlp object and config options (what should be extracted).\nTranslations of the spaCy course\nThe spaCy course is open-source and on GitHub and the content is released under a CC BY-NC license. Translating it to other languages could be really cool, to make it easier for people to get started\nChinese: in progress (@GoooIce)\nOther ideas\n#2466: Export spaCy models for use in Java environments\n#2264: Render dependency graph with graphviz\n#2625: visualising NER activations inside spaCy models\n3", "issue_status": "Open", "issue_reporting_time": "2019-09-29T16:34:21Z"}, "93": {"issue_url": "https://github.com/explosion/spaCy/issues/4281", "issue_id": "#4281", "issue_summary": "GPU support for Entity Linking pipe", "issue_description": "Member\nsvlandeg commented on 12 Sep 2019\nThe new Entity Linking code has not been tested/adapted to GPU yet, cf Matt's comment here.\n1", "issue_status": "Open", "issue_reporting_time": "2019-09-12T10:17:35Z"}, "94": {"issue_url": "https://github.com/explosion/spaCy/issues/4173", "issue_id": "#4173", "issue_summary": "Add evaluation, train CLI pointers to example training scripts", "issue_description": "Collaborator\nadrianeboyd commented on 22 Aug 2019\nFeature description\nIt's a problem that the example train scripts are a bit too simple but the step to the train CLI is either a bit too complicated due to format differences or not an obvious next step.\nThings to improve this:\nAdd a simple final evaluation to the example train scripts with sample EVAL_DATA\nAdd converters for the TRAIN_DATA format to the train CLI format somewhere (which could be set up in a general way so they seamlessly convert to the new format in future, maybe as a function in spacy.gold?)\nAdd pointers to the train CLI to the usage docs and as comments in the example scripts", "issue_status": "Open", "issue_reporting_time": "2019-08-22T08:42:12Z"}, "95": {"issue_url": "https://github.com/explosion/spaCy/issues/4168", "issue_id": "#4168", "issue_summary": "Adding exceptions to sentencizer", "issue_description": "Contributor\nmr-bjerre commented on 22 Aug 2019\nI am not sure if I haven't look thoroughly enough in the docs but I want to add abbreviation exceptions to the sentence tokenizer.\nE.g. Operating income incl. JV was SEK 2.1 b. with an operating margin of 4.0% is split into Operating income incl. and JV was SEK 2.1 b. with an operating margin of 4.0%.\nMy experience so far with spaCy tells me that there is probably a smart way to fix it?\nPosted as bug but it might be doc related or a feature request.\nfrom spacy.lang.en import English\n\nnlp = English()\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\ndoc = nlp('Operating income incl. JV was SEK 2.1 b. with an operating margin of 4.0%')\n\nassert len([s for s in doc.sents]) == 1\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Linux-5.0.0-25-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.7.3", "issue_status": "Open", "issue_reporting_time": "2019-08-21T21:44:02Z"}, "96": {"issue_url": "https://github.com/explosion/spaCy/issues/4160", "issue_id": "#4160", "issue_summary": "Make text context / per document features available to the pipeline processing steps", "issue_description": "johann-petrak commented on 21 Aug 2019\nCurrently, a pipeline is a function that maps a text to a (processed document) where the tokenizer (which is not explicitly part of the pipeline) first creates the document and where the processing steps of the pipeline then update that document.\nOften, NLP processing requires or may use additional information, not just text: meta information about each document like author, language, topic, classification labels etc.\nBoth the tokenizer and the processing steps in the pipeline may want to access those additional features on a document by document basis.\nThere is no clean and generic way at the moment to achieve this. The pipe as_tuples parameter can be used to pass on input contexts to the consumer AFTER the document has been processed, but it is not possible to access those contexts from either the tokenizer or the processing steps in a pipeline.\nThe maybe easiest way to do this would be to add a context=None keyword parameter to the tokenizer and processing step interfaces and to add a use_context=False keyword parameter to the pipe method. If use_context=True and as_tuples is True as well, the per tuple context would get passed to each __call__ or pipe that is invoked by the top pipe invocation.\nThat way the using existing tokenizers and processing steps would not break when as_tuples=True is used, but if use_context=True is specified as well, all the steps involved would have to already accept the new context=.. parameter.\nEach processing step could make use of the context however they see fit (eg just storing a feature with a document) or just ignore it.\nConecptually a pipeline would then be a function that maps a tuple (text, context) or (text, c1, c2, c3) to a document. (if a tuple of the form (text, c1, c2,..) is processed then the list [c1, c2, ..] could be passed on as the context)", "issue_status": "Open", "issue_reporting_time": "2019-08-21T12:07:58Z"}, "97": {"issue_url": "https://github.com/explosion/spaCy/issues/4142", "issue_id": "#4142", "issue_summary": "AttributeError: 'FeedForward' object has no attribute 'G'", "issue_description": "damianoporta commented on 18 Aug 2019 \u2022\nedited\nHi,\ni am training a new NER model ad i found a problem with conv_depth.\nI recently added another similar issue, this: #4058 that has been closed,\nBasically i only have added this:\n    ner = nlp.create_pipe(\"ner\", config={\"conv_depth\": 6})\n    nlp.add_pipe(ner, last=True)\nas @ines said.\nWhen i try to load the model again i get the following error:\n>>> nlp = spacy.load('/home/nlp/32')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py\", line 133, in load_model\n    return load_model_from_path(Path(name), **overrides)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py\", line 173, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/language.py\", line 791, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py\", line 630, in from_disk\n    reader(path / key)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/language.py\", line 787, in <lambda>\n    deserializers[name] = lambda p, proc=proc: proc.from_disk(p, exclude=[\"vocab\"])\n  File \"nn_parser.pyx\", line 634, in spacy.syntax.nn_parser.Parser.from_disk\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/thinc/neural/_classes/model.py\", line 371, in from_bytes\n    dest = getattr(layer, name)\nAttributeError: 'FeedForward' object has no attribute 'G'\nin this case the trick with os.environ['conv_depth'] = '6' does not solve the problem.\nI also have checked the ner/cfg and this is the content:\n{\n  \"conv_depth\":6,\n  \"beam_width\":1,\n  \"beam_density\":0.0,\n  \"beam_update_prob\":1.0,\n  \"cnn_maxout_pieces\":3,\n  \"nr_class\":81,\n  \"hidden_depth\":1,\n  \"token_vector_width\":96,\n  \"hidden_width\":64,\n  \"maxout_pieces\":2,\n  \"pretrained_vectors\":\"it_model.vectors\",\n  \"bilstm_depth\":0\n}\nThe model has been trained with the same code:\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    # reset and initialize the weights randomly \u2013 but only if we're\n    # training a new model\n    nlp.begin_training()\n\n    for itn in range(N_ITER):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(\n                texts,  # batch of texts\n                annotations,  # batch of annotations\n                drop=0.2,  # dropout - make it harder to memorise data\n                losses=losses,\n            )\nI did not pass the config in begin_training() with component_cfg={\"ner\": {\"conv_depth\": 6}}\nI thought it was useless because of the ner initialization:\nner = nlp.create_pipe(\"ner\", config={\"conv_depth\": 6})\nDo i also need it? Any workaround?\nThanks\nYour Environment\nspaCy version    2.1.4                         \nLocation         /home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy\nPlatform         Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version   3.6.7                         \nModels   \n1", "issue_status": "Open", "issue_reporting_time": "2019-08-18T09:44:23Z"}, "98": {"issue_url": "https://github.com/explosion/spaCy/issues/4131", "issue_id": "#4131", "issue_summary": "Negation detection as a Multi-task learning (MTL) layer", "issue_description": "Contributor\nnipunsadvilkar commented on 16 Aug 2019\nI have been going through this paper - Joint Entity Extraction and Assertion Detection for Clinical Text - which proposes an MTL approach to negation detection that leverages overlapping representation across sub-tasks i.e., jointly model named entity and negation\nin an end-to-end system.\nI already have NER model in place and was thinking how would I implement MTL within spacy. Through some research, I see @honnibal has already done some work and provided an example - ner_multitask_objective.py but I find it difficult mold above example into negation multitask.\n@honnibal @ines : Would love to know your take on how to go about it and would like to contribute to repo if it pans out well.", "issue_status": "Open", "issue_reporting_time": "2019-08-16T14:32:28Z"}, "99": {"issue_url": "https://github.com/explosion/spaCy/issues/4119", "issue_id": "#4119", "issue_summary": "Adding patterns to EntityRuler and deserializing EntityRuler very slow", "issue_description": "jdprusa commented on 14 Aug 2019\nAdding a large number of patterns to the EntityRuler, or loading a saved EntityRuler that has a large number of patterns is extremely slow.\nThis is easily reproduced with:\nnlp = spacy.load('en_core_web_sm')\nentityruler = EntityRuler(nlp)\npatterns = [{\"label\": \"TEST\", \"pattern\": str(i)} for i in range(100000)]\nentityruler .add_patterns(patterns)\nand\nnlp = spacy.load('en_core_web_sm')\npatterns = [nlp.make_doc(str(i)) for i in range(1000000)]\nphrasematcher= PhraseMatcher(nlp.vocab)\nphrasematcher.add(\"TEST\", None, *patterns)\nthe EntityRuler code takes around 10 minutes to execute on an m5.4xlarge AWS SageMaker Notebook instance, while the PhraseMatcher code takes 20 seconds. Changing nlp.make_doc(str(i)) to nlp(str(i)) slows the PhraseMatcher down to the speed of the EntityRuler.\nLooking through the code for the EntityRuler, it employs the PhraseMatcher and should be similar in speed, but is using nlp(pattern) with the full nlp pipeline (tagger, parser, ner) instead of using nlp.make_doc(pattern) as recommended for the PhraseMatcher in https://spacy.io/usage/rule-based-matching#phrasematcher\nSince EntityRuler uses add_patterns() when deserializing this also slows down from_bytes() and from_disk() by a considerable amount.\nThis is fortunately an easy fix, just change the offending line\nCurrent Line 187 in https://github.com/explosion/spaCy/blob/master/spacy/pipeline/entityruler.py:\nself.phrase_patterns[label].append(self.nlp(pattern))\nUpdated Line 187:\nself.phrase_patterns[label].append(self.nlp.make_doc(pattern))\nThis puts pattern adding and loading times in line with the PhraseMatcher as expected and doesn't appear to break anything.\nYour Environment\nspaCy version: 2.1.8\nPlatform: Amazon Linux AMI 2018.03\nPython version: 3.6.5", "issue_status": "Open", "issue_reporting_time": "2019-08-14T16:37:56Z"}, "100": {"issue_url": "https://github.com/explosion/spaCy/issues/4106", "issue_id": "#4106", "issue_summary": "Token pattern validation doesn't support lowercase attributes", "issue_description": "Collaborator\nadrianeboyd commented on 12 Aug 2019\nHow to reproduce the behaviour\nThe JSON token pattern schema/validator only supports uppercase attributes.\nimport spacy\nfrom spacy.matcher import Matcher, PhraseMatcher\nnlp = spacy.load('en_core_web_sm')\nmatcher = Matcher(nlp.vocab, validate=True)\nmatcher.add(\"a\", None, [{'orth': 'a'}])\nOutput:\nspacy.errors.MatchPatternError: Invalid token patterns for matcher rule 'a'\n\nPattern 0:\n- Additional properties are not allowed ('orth' was unexpected) [0]", "issue_status": "Open", "issue_reporting_time": "2019-08-12T11:53:27Z"}, "101": {"issue_url": "https://github.com/explosion/spaCy/issues/4009", "issue_id": "#4009", "issue_summary": "Can word vectors have an impact on Textcat?", "issue_description": "romlatron commented on 23 Jul 2019\nI have a model with NER and Textcat components, using custom word vectors.\nWhile the impact of the vectors is clear on the NER, there doesn't seem to be any difference on the Textcat whether it is loaded with or without the vocabulary (and thus the vectors).\nIs there a way to get the best of the word vectors to improve my textcat component?\nWhich page or section is this issue related to?\nhttps://spacy.io/api/textcategorizer", "issue_status": "Open", "issue_reporting_time": "2019-07-23T09:30:28Z"}, "102": {"issue_url": "https://github.com/explosion/spaCy/issues/3987", "issue_id": "#3987", "issue_summary": "Make Doc.to_json function include Span & Token custom attribute", "issue_description": "Mardelor commented on 18 Jul 2019 \u2022\nedited\nFeature description\nDoc.to_json function now takes only Doc custom attribute, and no Span or Token custom attribute. It could be more convenient and generic if there was Span.to_json and Token.to_json methods to render all attribute in Doc.to_json, including custom attribute.\nIs it possible ?", "issue_status": "Open", "issue_reporting_time": "2019-07-18T11:06:34Z"}, "103": {"issue_url": "https://github.com/explosion/spaCy/issues/3981", "issue_id": "#3981", "issue_summary": "Wrong tokenization of text with quotes separated only by coma", "issue_description": "serzh commented on 17 Jul 2019\nHow to reproduce the behaviour\nSpacy do wrong tokenization of the text that has quotes in it that are seprated only with coma, without the space. For example:\n   > list(map(print(nlp.tokenizer('\"high\",\"low\"'))))\n   \"\n   high\",\"low\n   \"\nWith space everything works as expected:\n   > list(map(print(nlp.tokenizer('\"high\", \"low\"'))))\n   \"\n   high\n   \"\n   ,\n   \"\n   low\n   \"\nThis whole is a little bit confusing, because documentation says that Tokenizer works \"recursively\", however in the code we see that infix_finditer is called only once at the end of suffix-prefix iterations.\nAs a fix I've added spacy.lang.char_classes.LIST_QUOTES to the lisft of Tokenizer infixes and overrided infix_finditer, but I think it is better to fix it in the spacy itself.\nYour Environment\nspaCy version: 2.1.6\nPlatform: Darwin-17.7.0-x86_64-i386-64bit\nPython version: 3.6.6", "issue_status": "Open", "issue_reporting_time": "2019-07-17T14:04:13Z"}, "104": {"issue_url": "https://github.com/explosion/spaCy/issues/3979", "issue_id": "#3979", "issue_summary": "Pretrain T2V - Width of CNN layers.", "issue_description": "agombert commented on 17 Jul 2019 \u2022\nedited\nHello,\nI tried to pretrain a model with the CNN architecture, but I would like to change the width of the CNN layer to get bigger vectors at the end (128 instead of 96).\nAnd so I get an error about broadcast ValueError: could not broadcast input array from shape (128) into shape (96). Which looks like to come from the pretraining change of the CNN parameters.\nHow to reproduce the behaviour\nI followed those steps:\n1st step - W2V init\nI trained on the same text corpus a W2V model, I wanted to use this W2V model as inputs to learn from it. /w2v_vectors.txt.gz came from gensim modeling.\npython -m spacy init-model es /path/to/my/W2V/ --vectors-loc /path/to/my/w2v_vectors.txt.gz\n2nd step - model from W2V\nI used the train doc to train my new model without any problem\npython -m spacy train es /path/to/my/model_with_w2v/  es_ancora-ud-train.json es_ancora-ud-dev.json --vectors /path/to/my/W2V/\n3rd step - pretrain\nI pretrained the model, as explained in the doc with the following command:\npython -m spacy pretrain /path/to/my/texts.jsonl /path/to/W2V/model /path/to/my/t2v/ -i 50 -cw 128\n4th step - train\nAnd well after I got my pretrain processed, I try to train from the new token2vec:\npython -m spacy train es /path/to/my/model_with_t2v/  es_ancora-ud-train.json es_ancora-ud-dev.json -t2v /path/to/my/t2v/model49.bin\nAnd well I get this error:\nTraceback (most recent call last):\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/runpy.py\", line 193, in _run_module_as_main \"__main__\", mod_spec)\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/runpy.py\", line 85, in _run_code exec( code, run_globals)\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module> plac.call(commands[command], sys.argv[1:])\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/site-packages/plac_core.py\", line 328, in call cmd, result = parser;consume(arglist)\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/site-packages/plac_core.py\", line 207, in consume return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/spacy/cli/train.py\", line 219, in train components = _load_pretrained_tok2vec(nlp, init_tok2vec)\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/spacy/cli/train.py\", line 417, in _load_pretrained_tok2vec component.tok2vec/from_bytes(weights_data)\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/think/neural/_classes/model.py\", line 372, in from_bytes copy_array(dest, param[b\"value\"])\n   File \"/home/jovyan/environments/word_emb/lib/python3.6/think/neural/util.py\", line 124, in  copy_array(dest, param[b\"value\"])\nValueError: could not broadcast input array from shape (128) into shape (96)\nOther information about the bug:\nWhen I use it without the -cw 128 everything works well.\nMoreover, I can perform the training if I put an alias in the training such as token_vector_width=128 alias. And when I'm doing so, it looks like it's training ok but I get this error when trying to load the new t2v model:\nValueError                                Traceback (most recent call last)\n<ipython-input-23-b9d787f3e370> in <module>\n----> 1 nlp = spacy.load('/home/jovyan/words-representation/data/external/20190717_es_iomed_128_alias/model0')\n      2 nlp1 = spacy.load('es_core_news_md')\n\n~/environments/word_emb/lib/python3.6/site-packages/spacy/__init__.py in load(name, **overrides)\n     25     if depr_path not in (True, False, None):\n     26         deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 27     return util.load_model(name, **overrides)\n     28 \n     29 \n\n~/environments/word_emb/lib/python3.6/site-packages/spacy/util.py in load_model(name, **overrides)\n    131             return load_model_from_package(name, **overrides)\n    132         if Path(name).exists():  # path to model data directory\n--> 133             return load_model_from_path(Path(name), **overrides)\n    134     elif hasattr(name, \"exists\"):  # Path or Path-like to model data\n    135         return load_model_from_path(name, **overrides)\n\n~/environments/word_emb/lib/python3.6/site-packages/spacy/util.py in load_model_from_path(model_path, meta, **overrides)\n    171             component = nlp.create_pipe(name, config=config)\n    172             nlp.add_pipe(component, name=name)\n--> 173     return nlp.from_disk(model_path)\n    174 \n    175 \n\n~/environments/word_emb/lib/python3.6/site-packages/spacy/language.py in from_disk(self, path, exclude, disable)\n    789             # Convert to list here in case exclude is (default) tuple\n    790             exclude = list(exclude) + [\"vocab\"]\n--> 791         util.from_disk(path, deserializers, exclude)\n    792         self._path = path\n    793         return self\n\n~/environments/word_emb/lib/python3.6/site-packages/spacy/util.py in from_disk(path, readers, exclude)\n    628         # Split to support file names like meta.json\n    629         if key.split(\".\")[0] not in exclude:\n--> 630             reader(path / key)\n    631     return path\n    632 \n\n~/environments/word_emb/lib/python3.6/site-packages/spacy/language.py in <lambda>(p, proc)\n    785             if not hasattr(proc, \"from_disk\"):\n    786                 continue\n--> 787             deserializers[name] = lambda p, proc=proc: proc.from_disk(p, exclude=[\"vocab\"])\n    788         if not (path / \"vocab\").exists() and \"vocab\" not in exclude:\n    789             # Convert to list here in case exclude is (default) tuple\n\npipes.pyx in spacy.pipeline.pipes.Tagger.from_disk()\n\n~/environments/word_emb/lib/python3.6/site-packages/spacy/util.py in from_disk(path, readers, exclude)\n    628         # Split to support file names like meta.json\n    629         if key.split(\".\")[0] not in exclude:\n--> 630             reader(path / key)\n    631     return path\n    632 \n\npipes.pyx in spacy.pipeline.pipes.Tagger.from_disk.load_model()\n\npipes.pyx in spacy.pipeline.pipes.Tagger.from_disk.load_model()\n\n~/environments/word_emb/lib/python3.6/site-packages/thinc/neural/_classes/model.py in from_bytes(self, bytes_data)\n    370                         name = name.decode(\"utf8\")\n    371                     dest = getattr(layer, name)\n--> 372                     copy_array(dest, param[b\"value\"])\n    373                 i += 1\n    374             if hasattr(layer, \"_layers\"):\n\n~/environments/word_emb/lib/python3.6/site-packages/thinc/neural/util.py in copy_array(dst, src, casting, where)\n    122 def copy_array(dst, src, casting=\"same_kind\", where=None):\n    123     if isinstance(dst, numpy.ndarray) and isinstance(src, numpy.ndarray):\n--> 124         dst[:] = src\n    125     elif is_cupy_array(dst):\n    126         src = cupy.array(src, copy=False)\n\nValueError: could not broadcast input array from shape (128) into shape (96)\nBesides, when I disable the tagger when loading the t2v model, vectors of size 0 came from the t2v model.\nYour Environment\nLinux-4.9.0-7-amd64-x86_64-with-debian-buster-sid\nPython 3.6.7 | packaged by conda-forge | (default, Feb 28 2019, 09:07:38)\n[GCC 7.3.0]\nspacy 2.1.4\ngensim 3.7.2\nthinc 7.0.8", "issue_status": "Open", "issue_reporting_time": "2019-07-17T11:22:04Z"}, "105": {"issue_url": "https://github.com/explosion/spaCy/issues/3961", "issue_id": "#3961", "issue_summary": "\ud83d\udcab Proposal: Non-NER span categorizer", "issue_description": "Member\nhonnibal commented on 13 Jul 2019 \u2022\nedited\nPeople often want to train spaCy's EntityRecognizer component on sequence labeling tasks that aren't \"traditional\" Named Entity Recognition (NER). A named entity actually has a fairly specific linguistic definition: they're either definite descriptors, or a numeric expression. Software designed for doing NER may not perform well on other sequence labelling tasks. This is especially true of spaCy's NER system, which makes fairly aggressive trade-offs. What we need is another system that's better suited for these non-NER sequence tasks.\nWIP can be found here: https://github.com/explosion/spaCy/blob/feature/span-categorizer/spacy/pipeline/spancat.py\nExamples of non-NER span categorization\n(This section needs to be filled out with actual examples. For now I've just noted some general categories of problem.)\nChatbot tutorials and software have taken to calling all intent arguments \"entities\". I wish they wouldn't, but they do --- so people want to recognize and label these phrases with an \"entity recognizer\". These phrases could be anything. They're whatever argument of the \"intent\" the author has defined, even if they don't really make good linguistic sense. The boundaries of these \"entities\" can also be quite inconsistently defined.\nIn aspect-oriented sentiment analysis, one often has a set of attributes to recognize, like \"price\", \"quality\", \"customer service\", etc. Sometimes these can be recognized as topics in a sentence, but often people want them localised to specific spans.\nIn information or relation extraction, it's often important to recognize predicate words or phrases. For verbs, this is often a single word, but sometimes you want nouns as well, which can be multi-word expressions.\nTasks such as disfluency detection, detection of spelling errors, detection of grammatical errors etc are all quite sensible to code as sequence tagging problems.\nProblems with IOB/BILUO\nNER systems typically make one decision per word. Usually that decision corresponds to a tag (such as an IOB or BILUO tag). spaCy does things only slightly differently: on each word we predict an action, where the actions correspond to the BILUO tags. Encoding the task as transition-based parsing has some subtle advantages I won't go into here.\nThe problem is, the BILUO-style approach implies a loss where a boundary error means the model's prediction is completely wrong. This is the right way to think about NER and probably chunking, but it doesn't match up well with what people often want on non-NER problems. Often people are pretty ambivalent about the exact boundaries of their target spans. They would much rather have the boundary off-by-one than miss the entity entirely.\nSuggested solution\nThe SpanCategorizer will consist of three components:\nUser-configurable functions to suggest candidate spans from a Doc\nA submodel to extract token vector features (e.g. the current CNN, a BiLSTM, etc)\nA submodel to classify the spans\nFor instance, we might set out get_spans function to get the span offsets for all unigrams, bigrams, noun chunks and named entities. No matter how many candidate spans we have, we only need to run the token-to-vector model once -- and that model should be able to do most of the \"heavy lifting\". For each span, we need to perform some sort of weighted sum, and then run a feed-forward network. Hopefully this can be quite cheap.\nThere's an additional trick we could play when encoding the problem, to reduce the number of spans. If we have relatively few classes (especially if we only have one class), we could have extra classes that denote span offsets. This is what the object detection systems like YOLO do. Instead of generating all the overlapping spans, we can generate non-overlapping spans, and have classes that tell us to adjust the border. This might make things easier for the model as well, if it means the spans are of more uniform length.\nIf we do have spans of non-uniform length, we should still be able to process them efficiently. We need to be able to handle cases where there are some long spans, and a lot of short ones, without blowing up the space by padding out into a huge square. Batching the spans by length internally should take care of this.\nI think it would be good to have a loss function that assigns partial credit to spans according to how many tokens of overlap they have with the gold-standard. For instance, if we extract a span with 90% overlap with a gold-standard phrase, we can make the loss small.\nCurrent progress\nSketch out pipeline component\nSketch out architecture for span-to-scores model\nWrite utility functions for component the tok2vec and span-to-scores models\nTest utility functions\nImplement dummy experiment 1 as test: replace some words with TARGET, try to detect them.\nImplement dummy experiment 2 as test: Try to recognize two-word sequences that start with the dummy word TARGET, or three word sequences with TARGET in the middle.\nImplement script for NER experiment\nImplement script for chunking experiment\nFind dataset for more motivated experiment, implement script.\nTweak the model to get better results on experiments.\nWrite example\nDocument model\n6\n2\n21\n12", "issue_status": "Open", "issue_reporting_time": "2019-07-13T11:07:49Z"}, "106": {"issue_url": "https://github.com/explosion/spaCy/issues/3943", "issue_id": "#3943", "issue_summary": "\ud83d\udcab Proposal: component decorator for pipeline metadata and static analysis", "issue_description": "Member\nines commented on 10 Jul 2019\nAs I mentioned in our talk at spaCy IRL, here's a rough proposal for enhanced pipeline component metadata.\nOverview\nspaCy lets you add your own components to the pipeline that take a Doc, modify it and return it. You can assemble your components however you like. Their order doesn't always matter, but it can: for instance, you could write a custom lemmatizer that runs after the part-of-speech tagger, uses the tokens and part-of-speech tags and sets the Token.lemma/Token.lemma_ attribute. However, your custom component will always be a black box, and there's currently no way to express that it requires the Token.pos and Token.tag attributes to be set and that it writes to Token.lemma. You have to know this, and make sure to add your components correctly and do your error handling in your components if necessary. It's also currently not really exposed to the user where many attributes are set and what they're based on. If pipeline components could define what they set and what they require, spaCy could output more helpful diagnostics.\nIn terms of the implementation, there's a challenge here: How do we add metadata to a component? Pipeline components need to be callables that take a Doc, modify it and return it. If a component is a class with a __call__ method, adding more attributes is easy \u2013 but we don't want to require pipeline components to be classes, because the \"just write a function\" thing is actually pretty great.\nAdvantages and use cases\nOutput more details on the attributes on the Doc and Token and where in the pipeline they're set. For example, you could see that the \"parser\" component sets Token.dep and Doc.sents.\nRaise better error messages for custom components. If your custom component depends on the POS tags and they're not set, spaCy will be able to tell you this before it even runs.\nGenerate pipeline overviews and visualizations of the Doc passing through the pipeline\nPseudocode example implementation\n@component(\n    assigns={\"token\": {\"lemma\": True, \"_\": {\"extended_lemma\": True}}},\n    requires={\"token\": {\"pos\": True}}\n)\ndef custom_lemmatizer(doc):\n    return doc\nName Type Description\nassigns dict Attributes set or overwritten by the component.\nrequires dict Attributes required by the component.\nname ? str Alternative default component name.\nThe above assigns definition means that the component will set two attributes: Token.lemma (and by proxy, Token.lemma_) and the custom extension attribute Token._.extended_lemma.\nThe requires definition means that it requires the Token.pos attribute to be set.\nSyntax\nThe above implementation was just an example to illustrate the idea. There are different ways we could structure the assigns and requires arguments.\n1. Dict keyed by object and string attributes with boolean values\nassigns = {\n    \"token\": {\n        \"lemma\": True,\n        \"_\": {\n            \"extended_lemma\": True\n        }\n    }\n}\nPros: Most obvious way to represent nested values.\nCons: Can get pretty verbose and boolean values are technically not necessary because there's no reason to use False.\n2. Dict kexed by object with list of string attributes\nassigns = {\n    \"token\": [\"lemma\"],\n    \"_\": {\n        \"token\": [\"extended_lemma\"]\n    }\n}\nassigns = {\n    \"token\": [\"lemma\"],\n    \"token_\": [\"extended_lemma\"]  # or: \"token._\", \"token_underscore\"\n}\nPros: Easier to read, less verbose, only includes what's needed.\nCons: The handling of the _ can easily get awkward and unintuitive.\n3. Dot notation\nassigns = [\"token.lemma\", \"token._.extended_lemma\"]\nPros: Very straightforward and closest to how a user thinks about these attributes, as they're written the same way they're accessed.\nCons: Requires an additional helper to parse into dict representation, e.g. something like this.\nImplementation notes and open questions\nWe do need to take the built-in and the user's annotation at face value. If a component claims that it assigns Token.pos, we assume that this is true. We don't actually want to go and check each token to see if the attribute is set.\nCan a component require something like Doc.is_parsed to be True? If so, how do we handle this? Requiring a flag to have a certain value is quite different from requiring attributes to be set.\nHow do we implement this efficiently on the Language class so that we can output warnings when calling nlp.add_pipe? For each component, we need to be able to retrieve its left dependencies (what's in assigns) and then match that up with the component's requires.\n1\n1", "issue_status": "Open", "issue_reporting_time": "2019-07-10T12:38:52Z"}, "107": {"issue_url": "https://github.com/explosion/spaCy/issues/3940", "issue_id": "#3940", "issue_summary": "ENT_IOB as a token pattern key", "issue_description": "Varun-Epi commented on 10 Jul 2019 \u2022\nedited\nFeature description\nAdding ENT_IOB attribute as one of the pattern key. Currently, it has not been integrated into entity ruler pipeline.\nExample\nimport spacy\nfrom spacy.pipeline import EntityRuler\n\nnlp = spacy.load('en')\nruler = EntityRuler(nlp, overwrite_ents=True)\npattern = [{'ENT_TYPE': 'GPE', 'OP': '+'}, {'ORTH': 'california'}]\nruler.add_patterns([{\"label\":'COMB', \"pattern\":pattern}])\nprint('Before Ruler')\nprint(nlp('Delhi New York california').ents)\nOutput: (Delhi, New York)\nprint('After Ruler')\nnlp.add_pipe(ruler)\nprint(nlp('Delhi New York california').ents)\n-- Output: (Delhi New York california,)\nCorrect Output should be (Delhi, New York california)\nLogical answer would be to annotate only New York california as new entity, but here since ENT_IOB option is not available, it annotates all the GPE present in the given pattern", "issue_status": "Open", "issue_reporting_time": "2019-07-10T07:20:36Z"}, "108": {"issue_url": "https://github.com/explosion/spaCy/issues/3913", "issue_id": "#3913", "issue_summary": "different results for NER of spcay when using in DJANGO and terminal", "issue_description": "sandeep-adireddi commented on 6 Jul 2019\ni am using spacy for date recognition in my project. when i am using spacy ner in terminal it gives me a different results from when i use it in my project python file i attached images so u can understand by problem better\nYour Environment\nOperating System: ubuntu 18.04\nPython Version Used: 3.6\nspaCy Version Used: 2.1.4\nEnvironment Information: django", "issue_status": "Open", "issue_reporting_time": "2019-07-06T12:28:22Z"}, "109": {"issue_url": "https://github.com/explosion/spaCy/issues/3890", "issue_id": "#3890", "issue_summary": "distinguish types of adjectives", "issue_description": "chaouiy commented on 1 Jul 2019 \u2022\nedited\nIt would be great if we can use spacy to distinguish which semantic type of adjective / adverb it is. This is a recurrent use, because we usually need to extract only descriptive adjectives and results are poluted with other types of adjectives. Here is an example list I use to do this manually :\n       \"quantitative\": [\"half\", \"some\", \"several\", \"much\", \"all\", \"many\", \"whole\", \"no\", \"enough\", \"little\", \"any\", \"sufficient\", \"none\", \"most\", \"few\"],\n        \"demonstrative\": [\"this\", \"that\", \"these\", \"those\"],\n        \"possessive\": [\"my\", \"his\", \"their\", \"your\", \"our\", \"mine\", \"his\", \"hers\", \"theirs\", \"yours\", \"ours\"],\n        \"interrogative\": [\"which\", \"what\", \"whose\"],\n        \"distributive\": [\"each\", \"every\", \"either\", \"neither\", \"any\"],\n        \"article\": [\"a\", \"an\", \"the\"],", "issue_status": "Open", "issue_reporting_time": "2019-06-30T19:37:01Z"}, "110": {"issue_url": "https://github.com/explosion/spaCy/issues/3875", "issue_id": "#3875", "issue_summary": "Custom sentence segmentation not serialized on some cases", "issue_description": "Contributor\nclippered commented on 24 Jun 2019\nHow to reproduce the behaviour\nThis is from https://spacy.io/usage/linguistic-features#sbd-custom with minor modifications noted below\nimport spacy \n\n# NOTE: add a space at the start\ntext = u\" this is a sentence...hello...and another sentence.\"\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(text)\nprint(\"Before:\", [sent.text for sent in doc.sents])\n\ndef set_custom_boundaries(doc):\n    # NOTE: set token1 to sent_start=True\n    doc[1].is_sent_start = True\n    for token in doc[:-1]:\n        if token.text == \"...\":\n            doc[token.i+1].is_sent_start = True\n    return doc\n\nnlp.add_pipe(set_custom_boundaries, before=\"parser\")\ndoc = nlp(text)\nprint(\"After 1:\", [sent.text for sent in doc.sents])\n\n# print serialized/deserialized doc\nnew_doc = spacy.tokens.Doc(nlp.vocab).from_bytes(doc.to_bytes(exclude=['tensor']), exclude=['tensor'])\nprint(\"After 2:\", [sent.text for sent in new_doc.sents])\nYour Environment\nOperating System: macOS Mojave 10.14.5\nPython Version Used: Python 3.7.1\nspaCy Version Used: spaCy 2.1.4\nThe output was:\nBefore: [' this is a sentence...', 'hello...', 'and another sentence.']\nAfter 1: [' ', 'this is a sentence...', 'hello...', 'and another sentence.']\nAfter 2: [' this is a sentence...', 'hello...', 'and another sentence.']\nI was expecting that After 1 and After 2 are the same.", "issue_status": "Open", "issue_reporting_time": "2019-06-24T08:01:34Z"}, "111": {"issue_url": "https://github.com/explosion/spaCy/issues/3861", "issue_id": "#3861", "issue_summary": "Failing to install spaCy 2.1 AND 2.0 on some architectures", "issue_description": "Impelon commented on 18 Jun 2019 \u2022\nedited\nCurrently it is impossible to install spaCy 2.1+ on some architectures due to it's dependency on blis. This is related to #3451, but goes beyond that.\nThis is because blis is not supposed to support the 32-bit operating systems discussed in #3451 (raspbian and ubuntu for the raspberry pi 3), but I have failed installing spaCy even with Arch Linux ARM's 64-bit OS for the Raspberry Pi 3, because blis cannot compile.\nI would like to use some of spaCy-2.1's new features, but was going to be happy with spaCy 2.0 for now. The problem is, I have other installation issues with spaCy 2.0.18.\nDo you see yourselves fixing this in the coming weeks/month?\nPlease tell me if there is any additional information or clarification I could provide to you, that would help you.\nHow to reproduce the problem\nI should mention that I use a virtualenv to install spacy and its dependencies.\nI tried both pip3 install spacy==2.1.4 and building blis from source (and planning to build spacy after). Both these methods failed, because blis 0.2.4 (as required by spaCy on pip) always assumes x86_64 architecture on linux environments (also does not support any other architecture on linux) and blis 0.3+ (which would not work with spacy anyways, according to pip) cannot be built from source, because of the problems mentioned in explosion/cython-blis#9, explosion/cython-blis#6 and similar. Setting BLIS_ARCH to generic does not help.\nThis is the error I get, building blis 0.2.4 (and therefore spaCy 2.1.4) with pip (aka. pip3 install blis==0.2.4).\n  [...]\n  copying blis/__init__.pxd -> build/lib.linux-aarch64-3.7/blis\n  running build_ext\n  unix\n  py_compiler gcc\n  {'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'HOSTTYPE': 'x86_64', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'LANG': 'C.UTF-8', 'OLDPWD': '/home/matt/repos/flame-blis', 'VIRTUAL_ENV': '/home/matt/repos/wheelwright/env3.6', 'USER': 'matt', 'PWD': '/home/matt/repos/cython-blis', 'HOME': '/home/matt', 'NAME': 'LAPTOP-OMKOB3VM', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SHELL': '/bin/bash', 'TERM': 'xterm-256color', 'SHLVL': '1', 'LOGNAME': 'matt', 'PATH': '/home/matt/repos/wheelwright/env3.6/bin:/tmp/google-cloud-sdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/Python37/Scripts:/mnt/c/Python37:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/WiFi/bin:/mnt/c/Program Files/Common Files/Intel/WirelessCommon:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/LLVM/bin:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/nodejs:/mnt/c/Users/matt/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/matt/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/matt/AppData/Roaming/npm:/snap/bin:/mnt/c/Program Files/Oracle/VirtualBox', 'PS1': '(env3.6) \\\\[\\\\e]0;\\\\u@\\\\h: \\\\w\\\\a\\\\]${debian_chroot:+($debian_chroot)}\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\]:\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ ', 'VAGRANT_HOME': '/home/matt/.vagrant.d/', 'OMP_NUM_THREADS': '1', 'LESSOPEN': '| /usr/bin/lesspipe %s', '_': '/home/matt/repos/wheelwright/env3.6/bin/python'}\n  gcc -c /tmp/pip-install-ucphfa1p/blis/blis/_src/config/generic/bli_cntx_init_generic.c -o /tmp/tmp6gss7sd1/bli_cntx_init_generic.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-ucphfa1p/blis/blis/_src/include/linux-x86_64\n  gcc -c /tmp/pip-install-ucphfa1p/blis/blis/_src/config/haswell/bli_cntx_init_haswell.c -o /tmp/tmp6gss7sd1/bli_cntx_init_haswell.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-ucphfa1p/blis/blis/_src/include/linux-x86_64\n  gcc -c /tmp/pip-install-ucphfa1p/blis/blis/_src/config/penryn/bli_cntx_init_penryn.c -o /tmp/tmp6gss7sd1/bli_cntx_init_penryn.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-ucphfa1p/blis/blis/_src/include/linux-x86_64\n  gcc -c /tmp/pip-install-ucphfa1p/blis/blis/_src/config/piledriver/bli_cntx_init_piledriver.c -o /tmp/tmp6gss7sd1/bli_cntx_init_piledriver.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-ucphfa1p/blis/blis/_src/include/linux-x86_64\n  gcc -c /tmp/pip-install-ucphfa1p/blis/blis/_src/config/sandybridge/bli_cntx_init_sandybridge.c -o /tmp/tmp6gss7sd1/bli_cntx_init_sandybridge.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-ucphfa1p/blis/blis/_src/include/linux-x86_64\n  gcc -c /tmp/pip-install-ucphfa1p/blis/blis/_src/config/steamroller/bli_cntx_init_steamroller.c -o /tmp/tmp6gss7sd1/bli_cntx_init_steamroller.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-ucphfa1p/blis/blis/_src/include/linux-x86_64\n  gcc -c /tmp/pip-install-ucphfa1p/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c -o /tmp/tmp6gss7sd1/bli_amaxv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-ucphfa1p/blis/blis/_src/include/linux-x86_64\n  gcc: error: unrecognized command line option '-mavx2'\n  gcc: error: unrecognized command line option '-mfma'\n  gcc: error: unrecognized command line option '-mfpmath=sse'\n  Traceback (most recent call last):\n    File \"<string>\", line 1, in <module>\n    File \"/tmp/pip-install-ucphfa1p/blis/setup.py\", line 266, in <module>\n      'Topic :: Scientific/Engineering'\n    File \"/home/pi/project-2/env/lib/python3.7/site-packages/setuptools/__init__.py\", line 145, in setup\n      return distutils.core.setup(**attrs)\n    File \"/usr/lib/python3.7/distutils/core.py\", line 148, in setup\n      dist.run_commands()\n    File \"/usr/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n      self.run_command(cmd)\n    File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n      cmd_obj.run()\n    File \"/home/pi/project-2/env/lib/python3.7/site-packages/wheel/bdist_wheel.py\", line 192, in run\n      self.run_command('build')\n    File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n      self.distribution.run_command(command)\n    File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n      cmd_obj.run()\n    File \"/usr/lib/python3.7/distutils/command/build.py\", line 135, in run\n      self.run_command(cmd_name)\n    File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n      self.distribution.run_command(command)\n    File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n      cmd_obj.run()\n    File \"/usr/lib/python3.7/distutils/command/build_ext.py\", line 340, in run\n      self.build_extensions()\n    File \"/tmp/pip-install-ucphfa1p/blis/setup.py\", line 105, in build_extensions\n      objects = self.compile_objects(compiler.split('-')[0], arch, OBJ_DIR)\n    File \"/tmp/pip-install-ucphfa1p/blis/setup.py\", line 184, in compile_objects\n      objects.append(self.build_object(env=env, **spec))\n    File \"/tmp/pip-install-ucphfa1p/blis/setup.py\", line 198, in build_object\n      subprocess.check_call(command, cwd=BLIS_DIR)\n    File \"/usr/lib/python3.7/subprocess.py\", line 347, in check_call\n      raise CalledProcessError(retcode, cmd)\n  subprocess.CalledProcessError: Command '['gcc', '-c', '/tmp/pip-install-ucphfa1p/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c', '-o', '/tmp/tmp6gss7sd1/bli_amaxv_zen_int.o', '-O3', '-mavx2', '-mfma', '-mfpmath=sse', '-march=core-avx2', '-fPIC', '-std=c99', '-D_POSIX_C_SOURCE=200112L', '-DBLIS_VERSION_STRING=\"0.5.0-6\"', '-DBLIS_IS_BUILDING_LIBRARY', '-Iinclude/linux-x86_64', '-I./frame/3/', '-I./frame/ind/ukernels/', '-I./frame/1m/', '-I./frame/1f/', '-I./frame/1/', '-I./frame/include', '-I/tmp/pip-install-ucphfa1p/blis/blis/_src/include/linux-x86_64']' returned non-zero exit status 1.\n  ----------------------------------------\n  ERROR: Failed building wheel for blis\nThe error one gets from building blis directly from the cloned master-branch from github is, as mentioned in explosion/cython-blis#9 (by using BLIS_ARCH=generic python -m setup.py):\n  [...]\n  gcc: error: blis/cy.c: No such file or directory\n  gcc: fatal error: no input files\n  [...]\nAs mentioned before I, afterwards I tried installing spacy 2.0.18.\n  [...]\n  building 'spacy.parts_of_speech' extension\n  creating build/temp.linux-aarch64-3.7\n  creating build/temp.linux-aarch64-3.7/spacy\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/spacy/include -I/usr/include/python3.7m -c spacy/parts_of_speech.cpp -o build/temp.linux-aarch64-3.7/spacy/parts_of_speech.o -O2 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/spacy/parts_of_speech.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/spacy/parts_of_speech.cpython-37m-aarch64-linux-gnu.so\n  building 'spacy.strings' extension\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/spacy/include -I/usr/include/python3.7m -c spacy/strings.cpp -o build/temp.linux-aarch64-3.7/spacy/strings.o -O2 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  spacy/strings.cpp: In function 'int __pyx_pf_5spacy_7strings_11StringStore_8__contains__(__pyx_obj_5spacy_7strings_StringStore*, PyObject*)':\n  spacy/strings.cpp:3682:29: warning: comparison of integer expressions of different signedness: '__pyx_t_5spacy_8typedefs_hash_t' {aka 'long unsigned int'} and 'Py_ssize_t' {aka 'long int'} [-Wsign-compare]\n     __pyx_t_1 = ((__pyx_v_key < __pyx_t_6) != 0);\n                   ~~~~~~~~~~~~^~~~~~~~~~~\n  spacy/strings.cpp: In function 'PyObject* __pyx_gb_5spacy_7strings_11StringStore_12generator(__pyx_CoroutineObject*, PyThreadState*, PyObject*)':\n  spacy/strings.cpp:3836:33: warning: comparison of integer expressions of different signedness: 'int' and 'std::vector<long unsigned int>::size_type' {aka 'long unsigned int'} [-Wsign-compare]\n     for (__pyx_t_2 = 0; __pyx_t_2 < __pyx_t_1; __pyx_t_2+=1) {\n                         ~~~~~~~~~~^~~~~~~~~~~\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/spacy/strings.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/spacy/strings.cpython-37m-aarch64-linux-gnu.so\n  building 'spacy.lexeme' extension\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/spacy/include -I/usr/include/python3.7m -c spacy/lexeme.cpp -o build/temp.linux-aarch64-3.7/spacy/lexeme.o -O2 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  In file included from /tmp/pip-install-lvepk9hx/spacy/include/numpy/ndarraytypes.h:1728,\n                   from /tmp/pip-install-lvepk9hx/spacy/include/numpy/ndarrayobject.h:17,\n                   from /tmp/pip-install-lvepk9hx/spacy/include/numpy/arrayobject.h:15,\n                   from spacy/lexeme.cpp:547:\n  /tmp/pip-install-lvepk9hx/spacy/include/numpy/npy_deprecated_api.h:11:2: warning: #warning \"Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n   #warning \"Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\"\n    ^~~~~~~\n  {standard input}: Assembler messages:\n  {standard input}:111820: Warning: end of file not at end of a line; newline inserted\n  {standard input}:112658: Error: unknown pseudo-op: `.lbb'\n  {standard input}: Error: open CFI at the end of file; missing .cfi_endproc directive\n  gcc: fatal error: Killed signal terminated program cc1plus\n  compilation terminated.\n  error: command 'gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for spacy\nA few moments later thinc also fails:\n  [...]\n  building 'thinc.linalg' extension\n  creating build/temp.linux-aarch64-3.7\n  creating build/temp.linux-aarch64-3.7/thinc\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/thinc/include -I/usr/include/python3.7m -c thinc/linalg.cpp -o build/temp.linux-aarch64-3.7/thinc/linalg.o -O3 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/thinc/linalg.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/thinc/linalg.cpython-37m-aarch64-linux-gnu.so\n  building 'thinc.structs' extension\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/thinc/include -I/usr/include/python3.7m -c thinc/structs.cpp -o build/temp.linux-aarch64-3.7/thinc/structs.o -O3 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/thinc/structs.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/thinc/structs.cpython-37m-aarch64-linux-gnu.so\n  building 'thinc.typedefs' extension\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/thinc/include -I/usr/include/python3.7m -c thinc/typedefs.cpp -o build/temp.linux-aarch64-3.7/thinc/typedefs.o -O3 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/thinc/typedefs.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/thinc/typedefs.cpython-37m-aarch64-linux-gnu.so\n  building 'thinc.linear.avgtron' extension\n  creating build/temp.linux-aarch64-3.7/thinc/linear\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/thinc/include -I/usr/include/python3.7m -c thinc/linear/avgtron.cpp -o build/temp.linux-aarch64-3.7/thinc/linear/avgtron.o -O3 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/thinc/linear/avgtron.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/thinc/linear/avgtron.cpython-37m-aarch64-linux-gnu.so\n  building 'thinc.linear.features' extension\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/thinc/include -I/usr/include/python3.7m -c thinc/linear/features.cpp -o build/temp.linux-aarch64-3.7/thinc/linear/features.o -O3 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/thinc/linear/features.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/thinc/linear/features.cpython-37m-aarch64-linux-gnu.so\n  building 'thinc.linear.serialize' extension\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/thinc/include -I/usr/include/python3.7m -c thinc/linear/serialize.cpp -o build/temp.linux-aarch64-3.7/thinc/linear/serialize.o -O3 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/thinc/linear/serialize.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/thinc/linear/serialize.cpython-37m-aarch64-linux-gnu.so\n  building 'thinc.linear.sparse' extension\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/thinc/include -I/usr/include/python3.7m -c thinc/linear/sparse.cpp -o build/temp.linux-aarch64-3.7/thinc/linear/sparse.o -O3 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  g++ -pthread -shared -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now build/temp.linux-aarch64-3.7/thinc/linear/sparse.o -L/usr/lib -lpython3.7m -o build/lib.linux-aarch64-3.7/thinc/linear/sparse.cpython-37m-aarch64-linux-gnu.so\n  building 'thinc.linear.linear' extension\n  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -march=armv8-a -O3 -pipe -fstack-protector-strong -fno-plt -fPIC -I/usr/include/python3.7m -I/tmp/pip-install-lvepk9hx/thinc/include -I/usr/include/python3.7m -c thinc/linear/linear.cpp -o build/temp.linux-aarch64-3.7/thinc/linear/linear.o -O3 -Wno-strict-prototypes -Wno-unused-function\n  cc1plus: warning: command line option '-Wno-strict-prototypes' is valid for C/ObjC but not for C++\n  In file included from /tmp/pip-install-lvepk9hx/thinc/include/numpy/ndarraytypes.h:1728,\n                   from /tmp/pip-install-lvepk9hx/thinc/include/numpy/ndarrayobject.h:17,\n                   from /tmp/pip-install-lvepk9hx/thinc/include/numpy/arrayobject.h:15,\n                   from thinc/linear/linear.cpp:628:\n  /tmp/pip-install-lvepk9hx/thinc/include/numpy/npy_deprecated_api.h:11:2: warning: #warning \"Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n   #warning \"Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\"\n    ^~~~~~~\n  {standard input}: Assembler messages:\n  {standard input}:296793: Warning: end of file not at end of a line; newline inserted\n  gcc: fatal error: Killed signal terminated program cc1plus\n  compilation terminated.\n  error: command 'gcc' failed with exit status 1\n  ----------------------------------------\n  ERROR: Failed building wheel for thinc\nI have also tried pip3 install spacy==2.0.15 with the same results.\nYour Environment\nI ran the code used in spacy info manually to get these results:\nOperating System: Linux-5.1.9-1-ARCH-aarch64-with-arch\nPython Version Used: 3.7.3", "issue_status": "Open", "issue_reporting_time": "2019-06-18T17:04:01Z"}, "112": {"issue_url": "https://github.com/explosion/spaCy/issues/3828", "issue_id": "#3828", "issue_summary": "Parallelize TextCategorizer training", "issue_description": "tsoernes commented on 6 Jun 2019\nIs there a way to do this? It's using only 1 CPU core (by design, it seems) but it's taking an awfully long long time; shame to have 15 other threads sitting idle.", "issue_status": "Open", "issue_reporting_time": "2019-06-06T14:25:30Z"}, "113": {"issue_url": "https://github.com/explosion/spaCy/issues/3823", "issue_id": "#3823", "issue_summary": "Getting vectors from BPE using user_hooks", "issue_description": "alejandrojcastaneira commented on 5 Jun 2019 \u2022\nedited\nFeature description\nFollowing #3761 I tried an script for getting the vector of an unknown word, as the average of all the subword units in it's byte-pair-encoding (BPE) decomposition, using SentencePiece. By defining a small vocabulary, each row from the model vocab will a contain a subword unit associated with a wordvector trained with glove, word2vec, etc. The method is better shown in BPEmb.\nThis could guaranteed that each oov word will always have an associated vector theoretically, according to the BPE tokenization while keeping the vocabulary and vectors table relative smalls. It is also very useful in languages such as German or Norwegian, where they could have infinite possibilities of word compositions.\nAt the moment I wrote an overwrite function for the doc.user_token_hooks[\"vector\"] property and a pipeline component that installs it:\nimport spacy\nimport sentencepiece as spm\nimport numpy as np\n\ndef spm_vectors(token):\n    if token.has_vector:\n        return nlp.vocab[token.text].vector\n    else:\n        return np.mean([nlp.vocab[sub_word_unit].vector for sub_word_unit in sp.EncodeAsPieces(token.text)], axis=0, dtype=np.float32)\n\ndef set_custom_vectors(doc):\n    doc.user_token_hooks[\"vector\"] = spm_vectors\n    return doc\n\nsp = spm.SentencePieceProcessor()\nsp.Load(\"de_bpe.model\")\n\nnlp = spacy.load(\"de_custom_bpe_model\")\nnlp.add_pipe(set_custom_vectors, first = True)\n\ndoc = nlp(\"Thanos sucht nach dem Unendlichkeitsstein\")\n\nprint(doc[4].vector)\n\n\n[-0.06138334 -0.075055    0.05727967 -0.22417586  0.05276467 -0.03159783 ... \nMaybe it's a little hacky, I would like to make sure If I'm using the user_hooks in the correct way so this vector representation for oov words could be used as input in the different processing pipelines like: \"tagger\", \"parser\",\"ner\",\"textcat\", etc.\nCould the feature be a custom component or spaCy plugin?\nBPE segmentation could be subject of PR in some cases as getting oov words, reduce models size, tokenization, applied before pretraining, as segmentation #828, etc.\nBest regards", "issue_status": "Open", "issue_reporting_time": "2019-06-05T13:14:46Z"}, "114": {"issue_url": "https://github.com/explosion/spaCy/issues/3820", "issue_id": "#3820", "issue_summary": "v2.1 launching multiple threads when numpy installed via conda", "issue_description": "Contributor\nf11r commented on 4 Jun 2019\nIt looks like spaCy v2.1 is launching multiple threads when using numpy installed via conda (and thus with mkl). This does not happen when installing numpy via pip. Is this expected behavior? From the v2.1 release blog post it sounded like this shouldn't happen.\nHow to reproduce the behaviour\nsingle-threaded:\ndocker run -t --entrypoint bash continuumio/miniconda3:4.6.14 -c \"pip install numpy==1.16.2 spacy==2.1.4 https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz && python -c 'import de_core_news_sm; nlp = de_core_news_sm.load(); print(\\\"start\\\"); docs = [nlp(100*\\\"This is a test text. \\\") for _ in range(10000)]'\"\nmulti-threaded:\ndocker run -t --entrypoint bash continuumio/miniconda3:4.6.14 -c \"conda install -y numpy==1.16.2 && pip install spacy==2.1.4 https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz && python -c 'import de_core_news_sm; nlp = de_core_news_sm.load(); print(\\\"start\\\"); docs = [nlp(100*\\\"This is a test text. \\\") for _ in range(10000)]'\"", "issue_status": "Open", "issue_reporting_time": "2019-06-04T12:27:20Z"}, "115": {"issue_url": "https://github.com/explosion/spaCy/issues/3801", "issue_id": "#3801", "issue_summary": "Exploiting available linguistic resources for the Italian language", "issue_description": "gtoffoli commented on 31 May 2019\nCurrently I have no resources to contribute to the spaCy project, but I think that it could be useful to point out some language resources available for the Italian language. The most interesting I am aware of is the free morphological lexicon morph-it, which was compiled based also on a large annotated corpus comprising many years of the national daily newspaper Repubblica; I had the opportunity of using both the lexicon and the corpus a few years ago, in learning to train some NLTK pos-taggers and chunkers.\nmorph-it contains about 500.000 word forms, annotated with pos-tags and other features, while the current Italian lemmatizer of spaCy contains about 333.000 word forms.\nSince a classical example of Italian ambiguous sentence, containing multiple ambiguous words, is \"La vecchia porta la sbarra\" (The old woman carries the bar / The old door bars it), I looked for the word form \"porta\" and found only one (improbable) entry in the spaCy lemmatizer map, versus 5 entries (4 lemmata) in morph-it. References:\nhttps://docs.sslmit.unibo.it/doku.php?id=resources:morph-it\nhttps://github.com/giodegas/morphit-lemmatizer/tree/master/master\nThe corpus annotation is good, although not perfect; it was done in both manual and automatic way. Years ago the corpus wasn't open, but I had access to it without difficulty telling that I needed it to train some algorithms. Moreover, I think that the corpus was annotated using an approach similar to that being used in the \"WaCky - The Web-As-Corpus\" multi-lingual project, which probably you already know; the products of this project are open. References:\nhttps://wacky.sslmit.unibo.it/doku.php\n1", "issue_status": "Open", "issue_reporting_time": "2019-05-31T16:13:08Z"}, "116": {"issue_url": "https://github.com/explosion/spaCy/issues/3768", "issue_id": "#3768", "issue_summary": "VERB has two nsubj", "issue_description": "hg-sun commented on 22 May 2019\nHi, I was trying to get dep, pos from below example and found out a root VERB has two nsubj. Linguistically, I think it is impossible to have two nsubj(phone with index 2 and version with index\n9) from same VERB (have with index 12)but is it possible or it is just a bug?\n>>>import spacy\n>>>nlp = spacy.load('en_core_web_sm')\n>>>test = \"An excellent phone to have but unfortunately the unlocked version doesn't have it's standard video calling\"\n>>>doc =  nlp(test)\n>>>for token in doc:\n>>>    print(token, token.pos_, token.dep_, token.head, token.head.i)\nAn DET det phone 2\nexcellent ADJ amod phone 2\nphone Noun nsubj have 12\nto PART aux have 4\nhave VERB relcl phone 2\nbut CCONJ cc phone 2\nunfortunately ADV advmod have 12\nthe DET det version 9\nunlocked ADJ amod version 9\nversion NOUN nsubj have 12\ndoes VERB aux have 12\n't ADV neg have 12\nhave VERB ROOT have 12\nit PRON subj 's 14\n's PART ccomp have 12\nstandard ADJ amod calling 17\ncalling NOUN attr 's 14\nYour Environment\nOperating System: Linux-4.4.0-130-generic-x86_64-with-debian-stretch-sid\nPython Version Used:3.6.5\nspaCy Version Used:2.1.3\nEnvironment Information:", "issue_status": "Open", "issue_reporting_time": "2019-05-22T06:25:30Z"}, "117": {"issue_url": "https://github.com/explosion/spaCy/issues/3756", "issue_id": "#3756", "issue_summary": "Japanese Model", "issue_description": "Contributor\npolm commented on 17 May 2019\nFeature description\nI'd like to add a Japanese model to spaCy. (Let me know if this should be discussed in #3056 instead - I thought it best to just tag it in for now.)\nThe Ginza project exists, but currently it's a repackaging of spaCy rather than a model to use with normal spaCy, and I think some of the resources it uses may be tricky to integrate from a licensing perspective.\nMy understanding is that the main parts of a model now are 1. the dependency model, 2. NER, and 3. word vectors. Notes on each of those:\nDependencies. For dependency info we can use UD Japanese GSD. UD BCCWJ is bigger but the corpus has licensing issues. GSD is rather small but probably enough to be usable (8k sentences). I have trained it with spaCy and there were no conversion issues.\nNER. I don't know of a good dataset for this; Christopher Manning mentioned the same problem two years ago. I guess I could make one based on Wikipedia - I think some other spaCy models use data produced by Nothman et al's method, which skipped Japanese to avoid dealing with segmentation, so that might be one approach. (A reasonable question here is: what do people use for NER in Japanese? Most tokenizer dictionaries, including Unidic, have entity-like information and make it easy to add your own entries, so that's probably the most common approach.)\nVectors. Using JA Wikipedia is no problem. I haven't worked with the Common Crawl before and I'm not sure I have the hardware for it, buf if I could get some help on it that's also an option.\nSo, how does that sound? If there's no issues with that I'll look into creating an NER dataset.\n7", "issue_status": "Open", "issue_reporting_time": "2019-05-17T09:48:52Z"}, "118": {"issue_url": "https://github.com/explosion/spaCy/issues/3698", "issue_id": "#3698", "issue_summary": "Show doc['id'] or at least some other infos for errors when training with CLI", "issue_description": "Arakkun commented on 8 May 2019\nI'm training a model, and obtain the\n\"ValueError: [E069] Invalid gold-standard parse tree. Found cycle between word IDs: ...\"\nKnowing which doc is causing the error or which phrase, or at least a bit more infos about the error would be really useful. Especially as the JSON input format for training has already an 'id' field meaning \"# ID of the document within the corpus\"\n2", "issue_status": "Open", "issue_reporting_time": "2019-05-07T20:32:33Z"}, "119": {"issue_url": "https://github.com/explosion/spaCy/issues/3695", "issue_id": "#3695", "issue_summary": "Pattern with match operator \"!\" does not work if text is shorter", "issue_description": "Contributor\nmr-bjerre commented on 7 May 2019\nSuppose I want to match all tokens that have match as lowercase if there is not a not preceding token.\nI'd expect all assertions here to pass but the last does not.\nimport spacy.lang.en\nimport spacy.matcher\n\nnlp = spacy.lang.en.English()\nmatcher = spacy.matcher.Matcher(nlp.vocab)\nmatcher.add('MATCH', None, [{'LOWER': 'not', 'OP': '!'}, {'LOWER': 'match'}])\nassert len(matcher(nlp('a match'))) == 1\nassert len(matcher(nlp('not match'))) == 0\nassert len(matcher(nlp('match'))) == 1\nIs there a workaround?\nInfo about spaCy\nspaCy version: 2.1.3\nPlatform: Linux-4.18.0-18-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.7.1", "issue_status": "Open", "issue_reporting_time": "2019-05-07T18:21:10Z"}, "120": {"issue_url": "https://github.com/explosion/spaCy/issues/3678", "issue_id": "#3678", "issue_summary": "Character-level features and ngrams for the TextCategorizer", "issue_description": "heyalistair commented on 6 May 2019\nFeature description\nThe TextCategorizer currently comes with three different architecture options (ensemble, simple-cnn, bow). Though these three already cover a range of use cases, they are all based on word-level features, and they are not ideal for extremely short documents (1 to 3 word documents). This feature request is for another architecture option that uses character-level ngrams (or something similar) at features for TextCategorizer.\nCould the feature be a custom component or spaCy plugin?\nPossibly, but it would integrate nicely as a general architecture option. However, maybe extremely short documents are outside the scope of what spaCy is trying to cover as they are borderline \"natural language\"?", "issue_status": "Open", "issue_reporting_time": "2019-05-06T09:24:42Z"}, "121": {"issue_url": "https://github.com/explosion/spaCy/issues/3668", "issue_id": "#3668", "issue_summary": "Unable to load textcat model with pretraining from disk", "issue_description": "nikhildes commented on 3 May 2019 \u2022\nedited\nHow to reproduce the behaviour\nI first performed language model pretraining on my own unlabeled data, and was able to successfully train a multi-class textcat model and save it to my output_dir using the train_textcat.py example code with the -t2v flag.\nWhen I try to load the saved model in using this code:\nprint(\"Loading from\", output_dir)\nnlp2 = spacy.load(output_dir)\nI get the following error message:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-48-4711bd0dfd51> in <module>\n----> 1 nlp2 = spacy.load('spacy_srk')\n\n~/anaconda2/envs/spacy/lib/python3.6/site-packages/spacy/__init__.py in load(name, **overrides)\n     25     if depr_path not in (True, False, None):\n     26         deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 27     return util.load_model(name, **overrides)\n     28 \n     29 \n\n~/anaconda2/envs/spacy/lib/python3.6/site-packages/spacy/util.py in load_model(name, **overrides)\n    131             return load_model_from_package(name, **overrides)\n    132         if Path(name).exists():  # path to model data directory\n--> 133             return load_model_from_path(Path(name), **overrides)\n    134     elif hasattr(name, \"exists\"):  # Path or Path-like to model data\n    135         return load_model_from_path(name, **overrides)\n\n~/anaconda2/envs/spacy/lib/python3.6/site-packages/spacy/util.py in load_model_from_path(model_path, meta, **overrides)\n    171             component = nlp.create_pipe(name, config=config)\n    172             nlp.add_pipe(component, name=name)\n--> 173     return nlp.from_disk(model_path)\n    174 \n    175 \n\n~/anaconda2/envs/spacy/lib/python3.6/site-packages/spacy/language.py in from_disk(self, path, exclude, disable)\n    784             # Convert to list here in case exclude is (default) tuple\n    785             exclude = list(exclude) + [\"vocab\"]\n--> 786         util.from_disk(path, deserializers, exclude)\n    787         self._path = path\n    788         return self\n\n~/anaconda2/envs/spacy/lib/python3.6/site-packages/spacy/util.py in from_disk(path, readers, exclude)\n    609         # Split to support file names like meta.json\n    610         if key.split(\".\")[0] not in exclude:\n--> 611             reader(path / key)\n    612     return path\n    613 \n\n~/anaconda2/envs/spacy/lib/python3.6/site-packages/spacy/language.py in <lambda>(p, proc)\n    780             if not hasattr(proc, \"from_disk\"):\n    781                 continue\n--> 782             deserializers[name] = lambda p, proc=proc: proc.from_disk(p, exclude=[\"vocab\"])\n    783         if not (path / \"vocab\").exists() and \"vocab\" not in exclude:\n    784             # Convert to list here in case exclude is (default) tuple\n\npipes.pyx in spacy.pipeline.pipes.Pipe.from_disk()\n\n~/anaconda2/envs/spacy/lib/python3.6/site-packages/spacy/util.py in from_disk(path, readers, exclude)\n    609         # Split to support file names like meta.json\n    610         if key.split(\".\")[0] not in exclude:\n--> 611             reader(path / key)\n    612     return path\n    613 \n\npipes.pyx in spacy.pipeline.pipes.Pipe.from_disk.load_model()\n\n~/anaconda2/envs/spacy/lib/python3.6/site-packages/thinc/neural/_classes/model.py in from_bytes(self, bytes_data)\n    369                     if isinstance(name, bytes):\n    370                         name = name.decode(\"utf8\")\n--> 371                     dest = getattr(layer, name)\n    372                     copy_array(dest, param[b\"value\"])\n    373                 i += 1\n\nAttributeError: 'FeedForward' object has no attribute 'W'\nDo I need to load in the the pretrained weights somehow for the new nlp pipeline? Or is this a bug?\nYour Environment\nspaCy version: 2.1.3\nPlatform: Linux-4.15.0-47-generic-x86_64-with-debian-stretch-sid\nPython version: 3.6.8\nModels: en", "issue_status": "Open", "issue_reporting_time": "2019-05-02T21:46:30Z"}, "122": {"issue_url": "https://github.com/explosion/spaCy/issues/3637", "issue_id": "#3637", "issue_summary": "Add is_sent_end to token attributes", "issue_description": "michelleful commented on 25 Apr 2019\nFeature description\nTokens already have an attribute called is_sent_start which can be used in the Matcher. Having an equivalent is_sent_end would be useful, so that we can say \"only match this token if it's at the end of a sentence\".\nCould the feature be a custom component or spaCy plugin?\nIt seems more at home in the SpaCy library itself.\n3", "issue_status": "Open", "issue_reporting_time": "2019-04-25T16:47:39Z"}, "123": {"issue_url": "https://github.com/explosion/spaCy/issues/3584", "issue_id": "#3584", "issue_summary": "Add a parameter for saving only the n latest epochs while using CLI Training", "issue_description": "Contributor\nBharat123rox commented on 12 Apr 2019 \u2022\nedited\nFeature description\nEnable saving models of the n_latest epochs alone to disk (say n_latest=5, save only the last 5 epochs to disk) in spacy train cli here.\nThis would ideally be a parameter, n_latest, specified by the user, to save only that many epochs to disk at the end of training, for example:-\nIf n_iter=9 and n_latest=5, then only model5 to model9 must be saved on disk, however to prevent losing information in the event of some crash/errors, a suggested implementation is model0 may be deleted only after model5 is saved, model1 may be deleted only after model6 is saved etc.\nThis will help users decide the appropriate tradeoffs for disk space vs ability to choose the \"best\" model.\nIn my case, I certainly require this feature as I'm running out of disk space.\n@ines @honnibal I'm ok to submit a PR for this, if this feature seems ok. (Even if it is a breaking change, I feel it's essential moving forward and some steps could be taken to make it a v2.1 feature)\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.\nNo, since it is a parameter specific to Spacy's CLI Training", "issue_status": "Open", "issue_reporting_time": "2019-04-12T06:45:36Z"}, "124": {"issue_url": "https://github.com/explosion/spaCy/issues/3581", "issue_id": "#3581", "issue_summary": "Hierarchical Attention Networks in Text Categorizer", "issue_description": "alejandrojcastaneira commented on 12 Apr 2019\nHello,\nI understand that the text categorizer architecture it's similar to (Yang et al., 2016). I believe would be good to included an attention mechanism at sentences levels, for cases of long documents classification like your example on thinc imdb_cnn: https://github.com/explosion/thinc/blob/master/examples/imdb_cnn.py. I believe it will be very nice to have this, besides of the currently available: \"ensemble\", \"simple_cnn\" and \"bow\".\nBest regards.", "issue_status": "Open", "issue_reporting_time": "2019-04-11T18:36:21Z"}, "125": {"issue_url": "https://github.com/explosion/spaCy/issues/3579", "issue_id": "#3579", "issue_summary": "Retokenizer and custom pipeline components", "issue_description": "thomwolf commented on 11 Apr 2019\nSome pipeline components (like NeuralCoref) may store Spans as results of their processes.\nWhen a Doc is retokenized, these Spans may not be pointing to the right tokens anymore.\nHere is a simple example using NeuralCoref (from huggingface/neuralcoref#110):\nimport spacy\nimport neuralcoref\nnlp = spacy.load('en_coref_sm')\nneuralcoref.add_to_pipe(nlp)\n\ntext = nlp(\"Michelle Obama is the wife of former U.S. President Barack Obama. Prior to her role as first lady, she was a lawyer.\")\n\nspans = list(text.noun_chunks)\nwith text.retokenize() as retokenizer:\n    for span in spans:\n        retokenizer.merge(span)\n\nfor word in text:\n    print(word)\n    if(word._.in_coref):\n        print(text._.coref_clusters)\nThis throws an error (IndexError: [E037] Error calculating span: Can't find a token ending at character offset 78).\nThe simplest fix is to re-run the specific pipeline component once the tokens have been merged.\nI'm just opening this issue to discuss whether there could be a (simple) way to have a more automated solution (I don't have anything specific in mind though).", "issue_status": "Open", "issue_reporting_time": "2019-04-11T13:20:30Z"}, "126": {"issue_url": "https://github.com/explosion/spaCy/issues/3571", "issue_id": "#3571", "issue_summary": "backend choice with joblib multiprocessing", "issue_description": "fcharras commented on 10 Apr 2019\nThe multi-processing example given is the repository or in the documentation seems to insist on using the multiprocessing backend.\nIt seems unclear as to why or what are the risks with the default loky backend, is this documented ? if not, could you elaborate on the amount of risks taken by using loky ?\nWhich page or section is this issue related to?\nhttps://github.com/explosion/spaCy/blob/master/examples/pipeline/multi_processing.py#L42", "issue_status": "Open", "issue_reporting_time": "2019-04-10T15:11:33Z"}, "127": {"issue_url": "https://github.com/explosion/spaCy/issues/3536", "issue_id": "#3536", "issue_summary": "Feature Request: Seamless installation of language models from PyPI", "issue_description": "ChristianSi commented on 3 Apr 2019\nI use spaCy as a dependency for my own Python package (in case anyone's interested: lytspel). So I've added 'spacy' to the install_requires section of my setup.py. This basically works, but there are two issues:\n(1) When I install my package in a venv (virtual environment), installation is somewhat slow and the venv takes up considerable space, mainly because spaCy itself is so big. lib/python*/site-packages/spacy has 402 MB, nearly all of which is in spacy/lang (386 MB). spacy/lang/en in turn has only 5 MB, and I don't need support for any other language.\n(2) After installing spaCy, it's still necessary to download a suitable language model: python3 -m spacy download en. I found out that it's also possible to call this from within a Python script:\nfrom spacy.cli import download\ndownload('en')\nBut I cannot do this during setup since the wheel format (now standard on PyPI) doesn't allow execution of arbitrary code during installation. So instead, in my script, I check whether spacy.load('en') fails, and if it does, I download the model. This works, but it's clumsy and in some cases there may be permission issues.\nSo my dream would be that I could configure my dependencies to automatically install spaCy for English, including the language model, but not any other languages. This could be accomplished if spaCy had multiple packages on PyPI:\nspacy-core: Everything that's currently part of the spacy package, except for the (huge) lang subdirectory.\nspacy-en: Depends on spacy-core and additionally installs what's necessary to use spaCy for English, that is (presumable) the code in lang/en.\nspacy-de, spacy-fr etc.: Likewise for other languages.\nspacy-en-model: Depends on spacy-en and additionally installs the language model (which currently ends up in en_core_web_sm), making a separate download step unnecessary.\nspacy-de-model, spacy-fr-model etc.: Likewise for other languages.\nThis would make spaCy a much more compact and elegant dependency.\n8\n2", "issue_status": "Open", "issue_reporting_time": "2019-04-03T17:12:53Z"}, "128": {"issue_url": "https://github.com/explosion/spaCy/issues/3350", "issue_id": "#3350", "issue_summary": "Parallelize Pretraining", "issue_description": "alejandrojcastaneira commented on 3 Mar 2019 \u2022\nedited\nHello, great feature!, currently I'm doing some experiments on my specific use cases.\nBut I notice that pretraining speed It's considerably slow, 1 epoch took almost two days in 1B corpus at an average of 4800 w/s.\nSo I check the uses of resources of the task, I'm training whit a configuration of dual 12 cores Xeon CPUs, (total 24 CPUs) a single machine, without GPU, and I noticed It's only using 1 core at a time.\nWill be possible to add the desired number of workers on this task, then we could use the maximum number of cores and parallelize the pretraining, it could accelerate the processing time?\nBest regards", "issue_status": "Open", "issue_reporting_time": "2019-03-03T14:26:16Z"}, "129": {"issue_url": "https://github.com/explosion/spaCy/issues/3333", "issue_id": "#3333", "issue_summary": "GloVe Integration", "issue_description": "ttymck commented on 26 Feb 2019 \u2022\nedited\nFeature description\nI am interested in integrating GloVe closer to the python ecosystem. There currently exists 2 public re-implementations of the GloVe algorithm in the Python runtime (pure python: https://github.com/hans/glove.py and cython: https://github.com/maciejkula/glove-python). I am interested in providing a thin Cython wrapper around the stanfordnlp distribution of glove (a C program).\nMy proposed implementation would refactor the extant source to accept files instead of stdin/stdout, and a call to my GloVe.train() class would return a Path object to the file-on-disk containing the word vectors. I think this would integrate nicely with the from_glove() method in spacy.\nI am wondering:\nif this would be seen as valuable to the spacy ecosystem\nor if this has been tried before\nif there are obvious barriers to making this performant that I am missing, as I am relatively new to Cython. (i.e. if the stanfordnlp implementation of the algorithm is not entirely efficient)\nif there are suggestions for the interface\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.\nI believe this would would make sense as a plugin, with an emphasis on compatibility with other scientific packages (numpy, pandas, nltk, etc)\n3", "issue_status": "Open", "issue_reporting_time": "2019-02-26T07:34:15Z"}, "130": {"issue_url": "https://github.com/explosion/spaCy/issues/3320", "issue_id": "#3320", "issue_summary": "\ud83d\udcab Runtime customization of component model architectures without subclassing", "issue_description": "Member\nhonnibal commented on 23 Feb 2019\nFor the parser, NER and POS tagger, it's been fairly okay to have just one model solution implemented. For other pipeline components, this is less true. The situation is especially bad for the TextCategorizer. There's a huge diversity of text categorization problems people might be applying this component to. There's no single best solution to text categorization, so we need to make it easier to define custom models.\nCurrently, users can define a custom text categorization model by creating a subclass of the TextCategorizer class and overwriting the Model classmethod. Here's an example:\nfrom spacy.pipeline import TextCategorizer\nfrom thinc.api import layerize\n\nclass StupidTextCategorizer(TextCategorizer):\n    name = 'stupid_textcat'\n    @classmethod\n    def Model(cls, nr_class, **cfg):\n        return create_dummy_model(nr_class, cfg.get('preferred_class', 0))\n\ndef create_dummy_model(nr_class, preferred_class):\n     \"\"\"Create a Thinc model that always predicts the same class.\"\"\"\n    def dummy_model(docs, drop=0.):\n        scores = model.ops.allocate((len(docs), nr_class)\n        scores[:, preferred_class] = 1.0\n        return scores\n    model = layerize(dummy_model)\n    return model\nTo use this class, you would then register it as a factory, so that when you call nlp.create_pipe('stupid_textcat'), the class is instantiated.\nI don't think this is so bad, but it's still a bit more bureaucracy than we'd like. Plus, it relies on subclassing, which means it can't compose well with other solutions. If you already need to subclass your components for some orthogonal reason, you'll have problems. We want subclassing to be the override of last resort.\nI think we should have a registry for functions that return models. Instead of hard-coding the function within the Model classmethod, we would expect a cfg argument to tell us the name of the function, and we then look it up in the registry.\nSince \"model\" is pretty overloaded already, I think we should introduce the new term architecture. Basically we'd have a little helper function like this:\nARCHITECTURES = {}\n\ndef get_architecture(name):\n    return ARCHITECTURES[name]\n\ndef register_architecture(name, function):\n    ARCHITECTURES[name] = function\n    return function\n\ndef architecture(name):\n    \"\"\"Decorator to register an architecture\"\"\"\n    return functools.partial(register_architecture, name)\nThen inside the TextCategorizer class (and other pipeline components) we have something like this:\nclass TextCategorizer(Pipe):\n    @classmethod\n    def Model(cls, **cfg):\n        architecture = get_architecture(cfg.get('architecture', 'default_textcat'))\n        return architecture(**cfg)\nLet's say we want to define a textcat model with PyTorch. We write a function that defines the model, and wraps it as a Thinc model, and register it as an architecture:\n@architecture('my_textcat_model')\ndef pytorch_textcat_model(**cfg):\n    pytorch_model = your_pytorch_model(**cfg)\n    return thinc.extra.wrappers.PyTorchWrapper(pytorch_model)\nNow we don't have to subclass: we can control the choice of architecture via configuration, like this:\ntextcat = nlp.create_pipe('textcat', architecture='my_textcat_model')\nOf course, we would also add entry-points for the architectures, so that if a third-party library defines some, they'll be available even if the library isn't imported.\n2\n1\n1", "issue_status": "Open", "issue_reporting_time": "2019-02-23T10:05:54Z"}, "131": {"issue_url": "https://github.com/explosion/spaCy/issues/3275", "issue_id": "#3275", "issue_summary": "Sub-pattern labeling for pattern matcher", "issue_description": "cyclecycle commented on 14 Feb 2019 \u2022\nedited\nMy problem is that I would like to match patterns based on linguistic annotations, and then label certain tokens within the match, depending on which token_specs those matched tokens correspond to. I therefore need to know which tokens in the match span correspond to which token_specs in the provided patterns. For patterns which don't use operators, this is easy, as the match will be the same length as the pattern, so a corresponding one-to-one list of labels would suffice. For token_specs with operators, it is more complicated, as the number of corresponding tokens in the match is variable.\nN.B. Which tokens get which labels is dependent upon their position in a given pattern, so the labeling is tightly coupled to the pattern matching in this context.\nI see two solutions:\n- Alter the code in matcher.pyx so that for each token added to each open match, also store the current token_spec.\n- Reproduce the pattern matching logic in a callback in order to map tokens in the match to token_specs in the pattern.\nAm I missing an easier solution? I prefer the former option, so I could fork and attempt to make a satisfactory change. I have no idea whether such a feature would merit its cost for the broader community.\nThanks,\nNick", "issue_status": "Open", "issue_reporting_time": "2019-02-14T10:01:32Z"}, "132": {"issue_url": "https://github.com/explosion/spaCy/issues/3056", "issue_id": "#3056", "issue_summary": "\ud83c\udf0d Adding models for new languages master thread", "issue_description": "Member\nines commented on 16 Dec 2018 \u2022\nedited\nThis thread bundles discussion around adding pre-trained models for new languages (and improving the existing language data). A lot of information and discussion has been spread over various different issues (usually specific to the language), which made it more difficult to get an overview.\nSee here for the available pre-trained models, and this page for all languages currently available in spaCy. Languages marked as \"alpha support\" usually only include tokenization rules and various other rules and language data.\nHow to go from alpha support to a pre-trained model\nThe process requires the following steps and components:\nLanguage data: shipped with spaCy, see here. The tokenization should be reliable, and there should be a tag map that maps the tags used in the training data to coarse-grained tags like NOUN and optional morphological features.\nTraining corpus: the model needs to be trained on a suitable corpus, e.g. an existing Universal Dependencies treebank. Commercial-friendly treebank licenses are always a plus. Data for tagging and parsing is usually easier to find than data for named entity recognition \u2013 in the long term, we want to do more data annotation ourselves using Prodigy, but that's obviously a much bigger project. In the meantime, we have to use other available resources (academic etc.).\nData conversion: spaCy comes with a range of built-in converters via the spacy convert command that take .conllu files and output spaCy's JSON format. See here for an example of a training pipeline with data conversion. Corpora can have very subtle formatting differences, so it's important to check that they can be converted correctly.\nTraining pipeline: if we have language data plus a suitable training corpus plus a conversion pipeline, we can run spacy train to train a new model.\nWith our new internal model training infrastructure, it's now much easier for us to integrate new pipelines and train new models.\nImportant note: In order to train and distribute \"official\" spaCy models, we need to be able to integrate and reproduce the full training pipeline whenever we release a new version of spaCy that requires new models (so we can't just upload a model trained by someone else).\nIdeas for how to get involved\nContributing to the models isn't always easy, because there are a lot of different things to consider, and a big part of it comes down to sourcing suitable data and running experiments. But here are a few ideas for things that can move us forward:\nDifficulty: good for beginners\nProofread and correct the existing language data for a language of your choice. There can always be typos or mistakes ported over from a different resource.\nWrite tokenizer tests with expected input / output. It's always really helpful to have examples of how things should work, to ensure we don't accidentally introduce regressions. Tests should be \"fair\" and representative of what's common in general-purpose texts. While edge cases and \"tricky\" examples can be nice, they shouldn't be the focus of the tests. Otherwise, we won't actually get a realistic picture of what works and what doesn't. See the English tests for examples.\nRelevant documentation: Adding languages, Tokenization, Test suite Readme\nDifficulty: advanced\nAdd a tag map for a language and its treebank (e.g. Universal Dependencies). The tag map is keyed by the fine-grained part-of-speech tag (token.tag_, e.g. \"NNS\"), mapped to the coarse-grained tag (token.pos_, e.g. \"NOUN\") and other morphological features. The tags in the tag map should be the tags used by the treebank.\nExperiment with training a model. Convert the training and development data using spacy convert and run spacy train to train the model. See here for an example. (Note that most corpora don't come with NER annotations, so you'll usually only be able to train the tagger and parser). It might work out-of-the-box straight away \u2013 or it might require some more formatting and pre-processing. Finding this out will be very helpful. You can share your results and the reproducible commands to use in this thread.\nPrepare a raw text corpus from the CommonCrawl or a similar resource for the language you want to work on. Raw unlabelled text can be used to train the word vectors, estimate the unigram probabilities and \u2013 coming in v2.1.0 \u2013 pre-training a language model similar to BERT/Elmo/ULMFiT etc. (see #2931). We only need the cleaned, raw text \u2013 for example as a .txt or .jsonl file:\n{\"text\": \"This is a paragraph of raw text in some language\"}\nWhen using other resources, make sure the data license is compatible with spaCy's MIT license and ideally allows commercial use (since many people use spaCy commercially). Examples of suitable licenses are CC, Apache, MIT. Examples of unsuitable licenses are CC BY-NC, CC BY-SA, (A)GPL.\nRelevant documentation: Adding languages, Training via the CLI\nIf you have questions, feel free to leave a comment here. We'll also be updating this post with more tasks and ideas as we go.\n2", "issue_status": "Open", "issue_reporting_time": "2018-12-16T15:57:42Z"}, "133": {"issue_url": "https://github.com/explosion/spaCy/issues/3052", "issue_id": "#3052", "issue_summary": "\ud83d\udcda Inaccurate pre-trained model predictions master thread", "issue_description": "Member\nines commented on 14 Dec 2018\nThis thread is a master thread for collecting problems and reports related to incorrect and/or problematic predictions of the pre-trained models.\nWhy a master thread instead of separate issues?\nGitHub now supports pinned issues, which lets us create master threads more easily without them getting buried.\nUsers often report issues that come down to incorrect predictions made by the pre-trained statistical models. Those are all good and valid, and can include very useful test cases. However, having a lot of open issues around minor incorrect predictions across various languages also makes it more difficult to keep track of the reports. Unlike bug reports, they're much more difficult to action on. Sometimes, mistakes a model makes can indicate deeper problems that occurred during training or when preprocessing the data. Sometimes they can give us ideas for how to use data augmentation to make the models less sensitive to very small variations like punctuation or capitalisation.\nOther times, it's just something we have to accept. A model that's 90% accurate will make a mistake on every 10th prediction. A model that's 99% accurate will be wrong once every 100 predictions.\nThe main reason we distribute pre-trained models is that it makes it easier for users to build their own systems by fine-tuning pre-trained models on their data. Of course, we want them to be as good as possible, and we're always optimising for the best compromise of speed, size and accuracy. But we won't be able to ship pre-trained models that are always correct on all data ever.\nFor many languages, we're also limited by the resources available, especially when it comes to data for named entity recognition. We've already made substantial investments into licensing training corpora, and we'll continue doing so (including running our own annotation projects with Prodigy ) \u2013 but this will take some time.\nReporting incorrect predictions in this thread\nIf you've come across suspicious predictions in the pre-trained models (tagger, parser, entity recognizer) or you want to contribute test cases for a given language, feel free to submit them here. (Test cases should be \"fair\" and useful for measuring the model's general accuracy, so single words, significant typos and very ambiguous parses aren't usually that helpful.)\nYou can check out our new models test suite for spaCy v2.1.0 to see the tests we're currently running.", "issue_status": "Open", "issue_reporting_time": "2018-12-14T11:11:47Z"}, "134": {"issue_url": "https://github.com/explosion/spaCy/issues/2953", "issue_id": "#2953", "issue_summary": "adding new labels to an existing text categorizer", "issue_description": "dxiao2003 commented on 20 Nov 2018\nHow to reproduce the behaviour\nWe need to generate a dummy textcat model, so copy and paste the main method of spaCy/examples/training/train_textcat.py, add an extra return nlp as the last line so we capture the trained model, then do\n>> nlp = main(n_texts=10)\n>> textcat = nlp.get_pipe(\"textcat\")\n>> textcat.add_label(\"test\")\nExpected behavior\nI would expect that this succeeds and now we have a text classifier that has an additional possible label. Even though this new label wouldn't be returned by the previously trained model, we should be able to train the new model with data with \"test\" labels.\nActual behavior\nInstead, it throws an error:\n---------------------------------------------------------------------------\nExpectedTypeError                         Traceback (most recent call last)\n<ipython-input-12-cccc5ec09c95> in <module>\n----> 1 textcat.add_label(\"test_2\")\n\npipeline.pyx in spacy.pipeline.TextCategorizer.add_label()\n\n/usr/local/lib/python3.6/site-packages/thinc/describe.py in __get__(self, obj, type)\n     39         else:\n     40             shape = self.get_shape(obj)\n---> 41             data = obj._mem.add(key, shape)\n     42             if self.init is not None:\n     43                 self.init(data, obj.ops)\n\n/usr/local/lib/python3.6/site-packages/thinc/check.py in checked_function(wrapped, instance, args, kwargs)\n    143                 if not isinstance(check, Callable):\n    144                     raise ExpectedTypeError(check, ['Callable'])\n--> 145                 check(arg_id, fix_args, kwargs)\n    146         return wrapped(*args, **kwargs)\n    147 \n\n/usr/local/lib/python3.6/site-packages/thinc/check.py in is_shape(arg_id, args, func_kwargs, **kwargs)\n     74     for value in arg:\n     75         if not isinstance(value, integer_types) or value < 0:\n---> 76             raise ExpectedTypeError(arg, ['valid shape (positive ints)'])\n     77 \n     78 \n\nExpectedTypeError: \n\n Expected type valid shape (positive ints), but got: (2, None) (<class 'tuple'>)\n\n Traceback:\n \u251c\u2500 run_ast_nodes [3189] in /usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\n \u251c\u2500\u2500\u2500 run_code [3265] in /usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\n \u2514\u2500\u2500\u2500\u2500\u2500 <module> [1] in <ipython-input-12-cccc5ec09c95>\n        >>> textcat.add_label(\"test_2\")\nDigging through the source it seems the problem is that in the TextCategorizer.add_label function it takes the last layer of the model self.model.layers[-1] and assumes it is assumes it has shape (<int>, <int>), but the last layer has shape (<int>, None) instead.\nYour Environment\nOperating System: Ubuntu 18\nPython Version Used: 3.6\nspaCy Version Used: 2.0\nEnvironment Information:\n1", "issue_status": "Open", "issue_reporting_time": "2018-11-20T16:47:19Z"}, "135": {"issue_url": "https://github.com/explosion/spaCy/issues/2928", "issue_id": "#2928", "issue_summary": "\ud83d\udcab Proposal: New JSON(L) format for training and improved training commands", "issue_description": "Member\nines commented on 15 Nov 2018 \u2022\nedited\nMotivation\nOne of the biggest invonveniences and sources of frustration is spaCy's current JSON format for training. It's weirdly specific, annoying to create outside of the built-in converters and difficult to read. Training the model with incomplete information is pretty unintuitive and inconvenient as well.\nTo finally fix this, here's my proposal for a new and simplfied training file format that is easier to read, generate and compose.\nExample\n{\n  \"text\": \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California. It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.\",\n  \"ents\": [\n    {\"start\": 0, \"end\": 10, \"label\": \"ORG\"},\n    {\"start\": 17, \"end\": 25, \"label\": \"NORP\"},\n    {\"start\": 76, \"end\": 85, \"label\": \"GPE\"},\n    {\"start\": 87, \"end\": 97, \"label\": \"GPE\"},\n    {\"start\": 117, \"end\": 127, \"label\": \"PERSON\"},\n    {\"start\": 129, \"end\": 142, \"label\": \"PERSON\"},\n    {\"start\": 148, \"end\": 160, \"label\": \"PERSON\"},\n    {\"start\": 164, \"end\": 174, \"label\": \"DATE\"}\n  ],\n  \"sents\": [\n    {\"start\": 0, \"end\": 98},\n    {\"start\": 99, \"end\": 175}\n  ],\n  \"cats\": {\n    \"TECHNOLOGY\": true,\n    \"FINANCE\": false,\n    \"LEGAL\": false\n  },\n  \"tokens\": [\n    {\"start\": 0, \"end\": 5, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"compound\", \"head\": 1},\n    {\"start\": 6, \"end\": 10, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"nsubj\", \"head\": 2},\n    {\"start\": 11, \"end\": 13, \"pos\": \"VERB\", \"tag\": \"VBZ\", \"dep\": \"ROOT\", \"head\": 2},\n    {\"start\": 14, \"end\": 16, \"pos\": \"DET\", \"tag\": \"DT\", \"dep\": \"det\", \"head\": 7},\n    {\"start\": 17, \"end\": 25, \"pos\": \"ADJ\", \"tag\": \"JJ\", \"dep\": \"amod\", \"head\": 7},\n    {\"start\": 26, \"end\": 39, \"pos\": \"ADJ\", \"tag\": \"JJ\", \"dep\": \"amod\", \"head\": 6},\n    {\"start\": 40, \"end\": 50, \"pos\": \"NOUN\", \"tag\": \"NN\", \"dep\": \"compound\", \"head\": 7},\n    {\"start\": 51, \"end\": 58, \"pos\": \"NOUN\", \"tag\": \"NN\", \"dep\": \"attr\", \"head\": 2},\n    {\"start\": 59, \"end\": 72, \"pos\": \"VERB\", \"tag\": \"VBN\", \"dep\": \"acl\", \"head\": 7},\n    {\"start\": 73, \"end\": 75, \"pos\": \"ADP\", \"tag\": \"IN\", \"dep\": \"prep\", \"head\": 8},\n    {\"start\": 76, \"end\": 85, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"pobj\", \"head\": 9},\n    {\"start\": 85, \"end\": 86, \"pos\": \"PUNCT\", \"tag\": \",\", \"dep\": \"punct\", \"head\": 10},\n    {\"start\": 87, \"end\": 97, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"appos\", \"head\": 10},\n    {\"start\": 97, \"end\": 98, \"pos\": \"PUNCT\", \"tag\": \".\", \"dep\": \"punct\", \"head\": 2},\n    {\"start\": 99, \"end\": 101, \"pos\": \"PRON\", \"tag\": \"PRP\", \"dep\": \"nsubjpass\", \"head\": 16},\n    {\"start\": 102, \"end\": 105, \"pos\": \"VERB\", \"tag\": \"VBD\", \"dep\": \"auxpass\", \"head\": 16},\n    {\"start\": 106, \"end\": 113, \"pos\": \"VERB\", \"tag\": \"VBN\", \"dep\": \"ROOT\", \"head\": 16},\n    {\"start\": 114, \"end\": 116, \"pos\": \"ADP\", \"tag\": \"IN\", \"dep\": \"agent\", \"head\": 16},\n    {\"start\": 117, \"end\": 122, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"compound\", \"head\": 19},\n    {\"start\": 123, \"end\": 127, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"pobj\", \"head\": 17},\n    {\"start\": 127, \"end\": 128, \"pos\": \"PUNCT\", \"tag\": \",\", \"dep\": \"punct\", \"head\": 19},\n    {\"start\": 129, \"end\": 134, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"compound\", \"head\": 22},\n    {\"start\": 135, \"end\": 142, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"conj\", \"head\": 19},\n    {\"start\": 142, \"end\": 143, \"pos\": \"PUNCT\", \"tag\": \",\", \"dep\": \"punct\", \"head\": 22},\n    {\"start\": 144, \"end\": 147, \"pos\": \"CCONJ\", \"tag\": \"CC\", \"dep\": \"cc\", \"head\": 22},\n    {\"start\": 148, \"end\": 154, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"compound\", \"head\": 26},\n    {\"start\": 155, \"end\": 160, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"conj\", \"head\": 22},\n    {\"start\": 161, \"end\": 163, \"pos\": \"ADP\", \"tag\": \"IN\", \"dep\": \"prep\", \"head\": 16},\n    {\"start\": 164, \"end\": 169, \"pos\": \"PROPN\", \"tag\": \"NNP\", \"dep\": \"pobj\", \"head\": 27},\n    {\"start\": 170, \"end\": 174, \"pos\": \"NUM\", \"tag\": \"CD\", \"dep\": \"nummod\", \"head\": 28},\n    {\"start\": 174, \"end\": 175, \"pos\": \"PUNCT\", \"tag\": \".\", \"dep\": \"punct\", \"head\": 16}\n  ]\n}\nNotes\nEach record contains a \"text\" and optional \"ents\" (named entity spans), \"sents\" (sentence spans), \"cats\" (text categories) and \"tokens\" (tokens with offsets into the text and optional attributes).\nOffsets into the text are standardised: \"start\" (start index) and \"end\" (end index). Other attributes match spaCy's API.\nThe \"tokens\" don't have to include all attributes. If an attribute isn't present (e.g. a part-of-speech tag or dependency label), it's treated as a missing value.\nThe token \"head\" is the index of the head token, i.e. token.head.i.\nThe provided gold-standard tokenization can also be used to train the parser to split/merge tokens (coming in v2.1.x). This could be an argument / a flag to set during training.\nspaCy v2.1.x (nightly) already includes a spacy.gold.docs2json helper that generates the training format from Doc objects. It's intended to help keep the converters (.conllu etc.) in sync, since they can now all produce Doc objects and call into the same helper to convert to spaCy's format. This would also make the transition to a new format easy, because we'd only have to change the logic in docs2json.\nPros\nEasier to read and much closer to how the linguistic annotations are presented in spaCy's data structures.\nEasier to mix and match, and compose different types of data. With this format, you could easily omit the \"tokens\" and only train on the \"ents\" or update the \"sents\" to improve the sentence boundary detection.\nEasier to generate from other sources and corpora, because there are fewer restrictions around the shape of the text. While the previous format enforced a strict separation of paragraphs and sentences, this format will let you use longer and shorter texts and define sentence boundaries within each example.\nEasier to extend. If there are ever new annotations to be trained from, they can be added in a backwards-compatible way. Document-level annotations (spans like sentences or entities) at the root, and token-level annotations (other predicted attributes) within the tokens.\nRelated ideas\nUse a JSON schema to validate the training data format (!!!) and provide helpful feedback if there are problems. For example, imagine an error like: \"tokens -> 20 -> start has the wrong format: integer required, received string (\"5\")\".\nSpeaking of validation: We could also add more in-depth data debugging and warnings (e.g. via an optional flag or command the user can run). For example: \"Your data contains a new entity type ANIMAL that currently isn't present in the model. 1) You only have 15 examples of ANIMAL. This likely isn't enough to teach the model anything meaningful about this type. 2) Your data doesn't contain any examples of texts that do not contain an entity. This will make it harder for the model to generalise and learn what's not an entity.\"\nMake spacy train accept data in both .json and .jsonl (newline-delimited JSON). JSONL allows reading the file in line-by-line and doesn't require parsing the entire document.\nspacy train should make it much easier to update existing models or, alternatively, we should provide an analogous command with the same / similar arguments that takes the name of an existing model package instead of just the language to initialize. (Basically, if you know Prodigy, we want to provide the same smooth batch training experience natively in spaCy!)\nMake it easy for custom components and third-party models to hook into the training format! spaCy already supports begin_training and update methods on components (if you call nlp.update, spaCy will iterate over the components and call their update methods if available \u2013 just like nlp.from_disk). So we could, for instance, allow an _ space in the training data, just like in the custom extension attributes, that can contain additional data \u2013 think coreference annotations, entity links etc.! Those would then automatically be added to the gold-standard Doc and become available in the custom component's update method.\nWhat do you think? I'd love to hear your feedback in the comments!\n17\n11", "issue_status": "Open", "issue_reporting_time": "2018-11-15T00:33:54Z"}, "136": {"issue_url": "https://github.com/explosion/spaCy/issues/2920", "issue_id": "#2920", "issue_summary": "Pipelines with duplicate models", "issue_description": "Contributor\nDeNeutoy commented on 12 Nov 2018\nFeature description\nI would like to contribute a fix to make it possible to isolate the NER and Dependency Parser models using custom attribute namespaces. Currently, because they share a global namespace Doc.ents which determine a TransitionSystem state (in the NER case), duplicate models clobber each other when proceeding through the pipeline.\nAs an example, here is some code we had to write to have multiple NER models for different types of entities for biomedical text:\ndef ner_aggregate(doc):\n    doc._._tmp_ents = doc._._tmp_ents + list(doc.ents)\n    doc.ents = []\n    return doc\n\ndef ner_finalize(doc):\n    doc.ents = doc._._tmp_ents + list(doc.ents)\n    return doc\n\ndef load(**overrides):\n    \"\"\"Use category_paths to specify UMLS ontology paths, category_searches to\n        specify specific UMLS categories, or categories to specify filesystem\n        paths or mapped category names.\n        Additionally, a position can be specified as 'first',\n        'last', or left absent for a default value of 'first'.\n    \"\"\"\n    Doc.set_extension('_tmp_ents', default=[], force=True)\n\n    Language.factories['ner_bc5cdr'] = Language.factories['ner']\n    Language.factories['aggre_bc5cdr'] = lambda nlp, **cfg: ner_aggregate\n    Language.factories['ner_chemdner'] = Language.factories['ner']\n    Language.factories['aggre_chemdner'] = lambda nlp, **cfg: ner_aggregate\n    Language.factories['ner_msh'] = Language.factories['ner']\n    Language.factories['aggre_msh'] = lambda nlp, **cfg: ner_aggregate\n    Language.factories['finalize_ner'] = lambda nlp, **cfg: ner_finalize\nThis hack is functional in the sense that at inference time, you get all the entities and the different models don't stomp on each other. However, I'm a bit scared of what would happen if for example, someone tried to fine tune the pipeline on some new data. It also means the NER models have to be trained independently.\nProposed Solution\nAdd an optional parameter to these models which the set_annotations method of nn_parser. The current functionality of the NER and Parser models would be completely unchanged.\nhttps://github.com/explosion/spaCy/blob/master/spacy/syntax/nn_parser.pyx#L780\nHowever, it looks like the transition systems for NER and Parsing are intimately tied to the StateC struct - I wasn't sure how this related to the underlying state of the Doc?\nhttps://github.com/explosion/spaCy/blob/master/spacy/syntax/_state.pxd\nSo basically this issue is just a request for you to summarise how much work this would be, if it's possible at all, and the things which need to be completed such that this is possible.\n1", "issue_status": "Open", "issue_reporting_time": "2018-11-11T22:50:22Z"}, "137": {"issue_url": "https://github.com/explosion/spaCy/issues/2875", "issue_id": "#2875", "issue_summary": "is_oov doesn't work correctly when using en_vectors_web_lg model", "issue_description": "nikolaypavlov commented on 25 Oct 2018\nHow to reproduce the behaviour\nHere is an example:\nIn [3]: nlp = spacy.load('en_vectors_web_lg')                                                                                                                                                               \n\nIn [4]: doc = nlp(\"The quick brown fox jumps over the lazy dog\")                                                                                                                                            \n\nIn [5]: [token.is_oov for token in doc]                                                                                                                                                                     \nOut[5]: [True, True, True, True, True, True, True, True, True]\n\nIn [6]: nlp = spacy.load('en_core_web_lg')                                                                                                                                                                  \n\nIn [7]: doc = nlp(\"The quick brown fox jumps over the lazy dog\")                                                                                                                                            \n\nIn [8]: [token.is_oov for token in doc]                                                                                                                                                                     \nOut[8]: [False, False, False, False, False, False, False, False, False]\nInfo about spaCy\nspaCy version: 2.0.12\nPlatform: Darwin-17.7.0-x86_64-i386-64bit\nPython version: 3.6.4\nModels: en_core_web_lg, en_vectors_web_lg, en", "issue_status": "Open", "issue_reporting_time": "2018-10-25T11:07:13Z"}, "138": {"issue_url": "https://github.com/explosion/spaCy/issues/2767", "issue_id": "#2767", "issue_summary": "Detecting future tense using morphological features", "issue_description": "jbuzza commented on 17 Sep 2018\nFeature description\nI am trying to use morphological features to determine tense of text like \"is rising\", \"has risen\", \"will rise\".\nSome examples would be:\n'is rising'\ntoken.text token.tag_ token.dep_ nlp.vocab.morphology.tag_map[token.tag_])\nis VBZ aux {74: 99, 'VerbForm': 'fin', 'Tense': 'pres', 'Number': 'sing', 'Person': 3}\nrising VBG ROOT {74: 99, 'VerbForm': 'part', 'Tense': 'pres', 'Aspect': 'prog'}\n'has risen'\ntoken.text token.tag_ token.dep_ nlp.vocab.morphology.tag_map[token.tag_])\nhas VBZ aux {74: 99, 'VerbForm': 'fin', 'Tense': 'pres', 'Number': 'sing', 'Person': 3}\nrisen VBN ROOT {74: 99, 'VerbForm': 'part', 'Tense': 'past', 'Aspect': 'perf'}\n'will rise'\ntoken.text token.tag_ token.dep_ nlp.vocab.morphology.tag_map[token.tag_])\nwill MD aux {74: 99, 'VerbType': 'mod'}\nrise VB ROOT {74: 99, 'VerbForm': 'inf'}\nAs seen above, 'Tense' is populated for the root with 'pres' and 'past' but not 'fut'.\nIt looks like tag_map.py is used to assign the morphological features based on POS tag. There are also some exception rules defined in morph_rules.py.\nHowever, I do not see 'Tense' value of 'fut' included for any of the POS tags in those files.\nI am right in assuming that setting Tense to 'fut' is not implemented ? Is this a feature that could be added or am I doing something wrong ?\nYour Environment\nspaCy version: 2.0.12\nPlatform: Windows-10-10.0.17134-SP0\nPython version: 3.6.6\nModels: en_core_web_lg\nCould the feature be a custom component or spaCy plugin?\nUnknown", "issue_status": "Open", "issue_reporting_time": "2018-09-17T14:15:24Z"}, "139": {"issue_url": "https://github.com/explosion/spaCy/issues/2766", "issue_id": "#2766", "issue_summary": "Distinguish adverbs of manner in POS tagging.", "issue_description": "kamihamiha123 commented on 17 Sep 2018 \u2022\nedited\nFeature description\nAdverbs modify verbs to specify different information (time, manner, place or direction ) of the event described by the verb . It would be a great feature if spacy provides a way to distinguish each category of adverbs in POS tagging.\nCould the feature be a custom component or spaCy plugin?\nNo, Just extend tagging system to be more detailed", "issue_status": "Open", "issue_reporting_time": "2018-09-17T13:33:51Z"}, "140": {"issue_url": "https://github.com/explosion/spaCy/issues/2762", "issue_id": "#2762", "issue_summary": "Spacy fine tuning of embeddings", "issue_description": "GeoffNN commented on 15 Sep 2018 \u2022\nedited\nFeature description\nSpaCy has great tokenization/lemmatization. It would be great to use this, and current word vectors to fine tune the embeddings (given by spaCy's vocabulary) on another domain specific dataset. For now, docs recommend to retrain word embeddings from scratch using gensim and reloading the vectors, but this does not leverage spaCy's functionalities from my understanding.\nCould the feature be a custom component or spaCy plugin?\nCould be", "issue_status": "Open", "issue_reporting_time": "2018-09-14T21:11:50Z"}, "141": {"issue_url": "https://github.com/explosion/spaCy/issues/2668", "issue_id": "#2668", "issue_summary": "\ud83d\udcab Improve rule-based lemmatization and replace lookups", "issue_description": "Member\nines commented on 14 Aug 2018 \u2022\nedited\nThis is an enhancement issue that's been high on our list for a while, and something we'd love to tackle for as many languages as possible. At the moment, spaCy only implements rule-based lemmatization for very few languages. Many of the other languages to support lemmatization, but only use lookup tables, which are less reliable and don't always produce good results. They're also large and add significant bloat to the library.\nExisting rule-based lemmatizers\nLanguage Source\nEnglish lang/en/lemmatizer\nGreek lang/el/lemmatizer\nNorwegian lang/nb/lemmatizer\nMost problematic lookup lemmatizers\nJust my subjective selection, but these are the languages we want to prioritise. Of course, we'd also appreciate contributions to any of the other languages!\nLanguage Related issues\nGerman #2486, #2368, #2120\nFrench #2659, #2251, #2079\nSpanish #2710\n7", "issue_status": "Open", "issue_reporting_time": "2018-08-14T15:59:45Z"}, "142": {"issue_url": "https://github.com/explosion/spaCy/issues/2561", "issue_id": "#2561", "issue_summary": "The word \"number\" is not in Spacy's vocab?", "issue_description": "Enumaris commented on 18 Jul 2018\nHow to reproduce the behaviour\nnlp = spacy.load('en_core_web_md')\n'number' in nlp.vocab\nFalse\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.6.5\nspaCy Version Used: 2.0.11\nEnvironment Information:\nThis behavior happens when using both en_core_web_lg-2.0.0 and en_core_web_md-2.0.0.\nOddly, the word Number (capital \"N\") is in the vocab. This seems very odd.", "issue_status": "Open", "issue_reporting_time": "2018-07-17T22:31:00Z"}, "143": {"issue_url": "https://github.com/explosion/spaCy/issues/2498", "issue_id": "#2498", "issue_summary": "RNN Doc similarity, example available?", "issue_description": "JulianKingman commented on 2 Jul 2018\nI saw a recent comment in a blog post that you\u2019re working actively on creating a better doc similarity function, I\u2019d that the RNN method that you mention in this article?\nhttps://explosion.ai/blog/deep-learning-formula-nlp\nDo you have a public example that shows an implementation of that, using user hooks and all that?\nThanks, and thanks for a great Library. I\u2019m just getting started with python, and this lib is so approachable and usable", "issue_status": "Open", "issue_reporting_time": "2018-07-02T03:40:43Z"}, "144": {"issue_url": "https://github.com/explosion/spaCy/issues/2460", "issue_id": "#2460", "issue_summary": "In-Built Coreferencing?", "issue_description": "meksor commented on 18 Jun 2018 \u2022\nedited\nFor Coreferencing, an additional piece of software is needed right now (would be good to advertise this more, ive been using SpaCy for a year now and just found out about that, I was already thinking of using CoreNLP for this feature). Are there plans to include this in SpaCy's pipeline?\nThanks for preserving spaces by the way, SPACE IS A WORD TOO. This makes detokenization easily possible! I had to implement lots of questionable logic and make a lot of assumptions for detokenization after switching to Google Cloud NLP for Coreferencing.\nPS:\nThis (a fake news generator) is what I plan on using SpaCy for again, if anyone has any nice ideas.", "issue_status": "Open", "issue_reporting_time": "2018-06-18T12:54:30Z"}, "145": {"issue_url": "https://github.com/explosion/spaCy/issues/2338", "issue_id": "#2338", "issue_summary": "Language Code for languages not listed in ISO 639-1", "issue_description": "gurudevrao commented on 17 May 2018\nCurrently spaCy uses the two letter ISO 639-1 language codes. However that list does not cover all the languages.\nAs more and more language models are added, we may come across situation where the language does not have a two letter code, or the model is trained on a specific dialect. So, I guess we should switch to ISO 639-3 language codes, or find some way to use both?", "issue_status": "Open", "issue_reporting_time": "2018-05-17T10:30:23Z"}, "146": {"issue_url": "https://github.com/explosion/spaCy/issues/2336", "issue_id": "#2336", "issue_summary": "spaCy as SRL?", "issue_description": "mosynaq commented on 17 May 2018\nAs stated here, spaCy's parser can be used for purposes other than syntactical parsing. Can the parser be used as a Semantic Role Labeler (SRL)?\nThe example presented in above link, only shows single tokens as arguments. Can the parser detect multiple tokens as single argument, as below?\n1", "issue_status": "Open", "issue_reporting_time": "2018-05-17T07:27:11Z"}, "147": {"issue_url": "https://github.com/explosion/spaCy/issues/2328", "issue_id": "#2328", "issue_summary": "Thoughts around adding features to NER model", "issue_description": "rbhambriiit commented on 14 May 2018\nFeature description\nNER model to use parser, tagger features.\nBackground:\nI have watched Spacy's video to get an idea on how NER model is trained based on Embed, Encode, Attend, Predict.\nI have trained custom blank NER models on my dataset.\nIdea:\nThe NER model should rely on other pipelines such as parser, tagger.\nFeatures for pos, dependency label, previous-word-pos etc. should be considered at the embed stage.\nI would like to hear more from @honnibal if this makes sense?\nIs it somewhere already done. If yes, please help get me to the right place.", "issue_status": "Open", "issue_reporting_time": "2018-05-14T07:46:57Z"}, "148": {"issue_url": "https://github.com/explosion/spaCy/issues/2281", "issue_id": "#2281", "issue_summary": "Word Vectors are case insensitive", "issue_description": "r-wheeler commented on 1 May 2018 \u2022\nedited\nIs it expected that en_core_web_lg is using the lower cased glove vectors?\nnlp = spacy.load(\"en_core_web_lg\")\nnp.array_equal(nlp.vocab.get_vector(\"Apple\"), nlp.vocab.get_vector(\"apple\")) \nTrue\nThe StringStore has different hashes for each:\nnlp.vocab.strings['apple'] \n8566208034543834098\nnlp.vocab.strings['Apple']\n6418411030699964375\nBut even using these hashing to look up vectors with nlp.vocab.get_vector It is true:\nnlp = spacy.load(\"en_core_web_lg\")\nnp.array_equal(nlp.vocab.get_vector(6418411030699964375),nlp.vocab.get_vector(8566208034543834098))\nTrue\nInfo about spaCy\nspaCy version: 2.0.8\nPlatform: Linux-4.4.0-1054-aws-x86_64-with-Ubuntu-16.04-xenial\nPython version: 3.6.3\nModels: en_core_web_lg, en_vectors_web_lg", "issue_status": "Open", "issue_reporting_time": "2018-05-01T17:17:52Z"}, "149": {"issue_url": "https://github.com/explosion/spaCy/issues/2262", "issue_id": "#2262", "issue_summary": "feature request: zero-width lookahead/-behind expression in the Matcher", "issue_description": "martijnvanbeers commented on 26 Apr 2018\nThe new Matcher in #1971 looks really neat! Maybe I haven't looked closely enough, but it seems that the idea in the title isn't part of the feature set. Say you want to find all loops in a text, but want to ignore fruit loops. You could do something like: [{'LOWER': 'fruit', 'op': '!'}, {'LEMMA': 'loop'}] (even with the old matcher), but that returns matches including whatever word is in front of loop.\nI don't know how hard it would be to implement, but adding an extra flag, include (possibly with a better name) could make it possible to return the match as just 'loop':[{'LOWER': 'fruit', 'op': '!', 'include' : False}, {'LEMMA': 'loop'}]", "issue_status": "Open", "issue_reporting_time": "2018-04-26T08:18:20Z"}, "150": {"issue_url": "https://github.com/explosion/spaCy/issues/2253", "issue_id": "#2253", "issue_summary": "Feature Request for TextCategorizer: add numeric features for text classification", "issue_description": "mhigginslp commented on 23 Apr 2018\nI have extra, numeric data that comes along with the text that I am trying to classify. I suspect that these features will help the model learn.\nI also have sentence/paragraph level embeddings that would help the classifier out.\nI imagine this situation comes up fairly frequently for text classification - there is data that comes along with the text that may be able to help classification.\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:\n10", "issue_status": "Open", "issue_reporting_time": "2018-04-23T16:33:54Z"}, "151": {"issue_url": "https://github.com/explosion/spaCy/issues/2240", "issue_id": "#2240", "issue_summary": "Feature request: Custom attributes/pipeline for input to Language", "issue_description": "kfkelvinng commented on 18 Apr 2018 \u2022\nedited\nThe custom attributes/pipeline can be useful for following pipeline to obtain relevant context from:\nmarking the speaker from input\nmarking the title/headings from rich text document (HTML, etc)\nmarking the text category given from dataset\nadditional doc from previous parse\nadditional pipeline within Language.make_doc for spell correction, merging tokens, etc\nIn the process of integrating neuralCoref (single speaker) to spaCy pipeline by redirecting the doc on this line in neuralcoref within my Pipe.__call__(doc).\nIn order to fully integrate neuralCoref (with multiple speakers) into spaCy the pipeline components, Language would need to be able to consume custom attributes for marking speaker to nlp(text). Something along the line of adding extension to text_holder._.speaker={...} before nlp(text_holder).\nSince OntoNote5, the speaker is annotated. This could be useful for future CoNLL tasks.", "issue_status": "Open", "issue_reporting_time": "2018-04-18T13:38:04Z"}, "152": {"issue_url": "https://github.com/explosion/spaCy/issues/2235", "issue_id": "#2235", "issue_summary": "Feature request: tag_map equivalent for dependency relations", "issue_description": "Contributor\ndanielhers commented on 18 Apr 2018\nThe pos attribute is very useful for cross-lingual applications, as it has a universal set of values.\nHowever, dep has different values for English (CLEAR) and German (TIGER) than other languages (UD). While I think a UD model for every language would be nice, in the meantime perhaps it would be easier to add a dep_map for English and German, and an attribute called something like udep providing the value of the mapped relation.", "issue_status": "Open", "issue_reporting_time": "2018-04-18T09:02:10Z"}, "153": {"issue_url": "https://github.com/explosion/spaCy/issues/2229", "issue_id": "#2229", "issue_summary": "feature: Merge multiple `Doc()` objects into one", "issue_description": "kyoungrok0517 commented on 17 Apr 2018\nWhen processing large documents, I usually process sentence by sentence. Then I have numerous Doc() objects per document. It'll be great if I could merge those objects into one then serialize/save to disk.\n21", "issue_status": "Open", "issue_reporting_time": "2018-04-17T02:47:19Z"}, "154": {"issue_url": "https://github.com/explosion/spaCy/issues/2227", "issue_id": "#2227", "issue_summary": "Spell Checker suggestions", "issue_description": "DonaldTsang commented on 16 Apr 2018 \u2022\nedited\nHaving a spell checker that involves:\nKeyboard typos with adjacent keys from QWERTY and QWERTZ\nMisspellings, including addition, deletion, repitition and phonetic replacement\nOut-Of-Vocabulary handling with OOV selection, clustering and misspelling", "issue_status": "Open", "issue_reporting_time": "2018-04-16T04:42:58Z"}, "155": {"issue_url": "https://github.com/explosion/spaCy/issues/2220", "issue_id": "#2220", "issue_summary": "Trainable Tokenizer", "issue_description": "mosynaq commented on 14 Apr 2018 \u2022\nedited\nHi there! I am trying to train Persian models for spaCy and have done a lot so far. But the tokenization part suffers some deficiencies: It cannot recognize clitics and split them and since those clitics connect to a large number of words, this phenomenon cannot be described using RegEx. Conllu tree banks provide a good deal of information about tokens and their boundaries. See below for a good example\n# text = \u0635\u0648\u0631\u062a\u0634 \u0631\u0627 \u062f\u06cc\u062f\u0645.\n1-2 \u0635\u0648\u0631\u062a\u0634 _ _ _ _ _ _ _ _\n1 \u0635\u0648\u0631\u062a \u0635\u0648\u0631\u062a NOUN N_SING Number=Sing 4 obj _ _\n2 \u0634 \u0627\u0648 PRON PRO Number=Sing|Person=3|PronType=Prs 1 nmod:poss _ _\n3 \u0631\u0627 \u0631\u0627 PART CLITIC _ 1 case _ _\n4 \u062f\u06cc\u062f\u0645 \u062f\u06cc\u062f#\u062f\u06cc\u062f\u0646 VERB V_PA Number=Sing|Person=1|Tense=Past 0 root _ SpaceAfter=No\n5 . . PUNCT DELM _ 4 punct _ SpaceAfter=No\nThe above sample in picture:\n\nThe sentence means literally : \"His/her-face object-marker-token I-saw.\" meaning \"I saw his/her face.\".\nIn the first aolumn you can see \"1-2\" with the token and after that you can see the token properly tokenized. I believe this tree and others are a great source of information for a trainable tokenizer.\nQuestions\nDoes spaCy have such ability under other names?\nIs there any workarounds?\nCan this trainable tokenizer be used in accordance with spaCy's rule-based tokenzer?\np.s.\nUDPipe Project has exploited such feature in conllu trees so I believe it is possible for spaCy too.\nThis tokenizer can be implemented using UDPipe REST server, but I prefer spaCy for several reasons.\nYour Environment\nOperating System: Linux-4.13.0-32-generic-x86_64-with-LinuxMint-18.3-sylvia\nPython Version Used: 2.7.12 and 3.5\nspaCy Version Used: 2.0.11\nModels: en, fa, es\n1", "issue_status": "Open", "issue_reporting_time": "2018-04-14T08:20:44Z"}, "156": {"issue_url": "https://github.com/explosion/spaCy/issues/2154", "issue_id": "#2154", "issue_summary": "Support for FastText word embeddings", "issue_description": "Contributor\nraphael0202 commented on 28 Mar 2018 \u2022\nedited\nCurrently, Vectors.__getitem__ returns the vector associated with the given key (https://github.com/explosion/spaCy/blob/master/spacy/vectors.pyx#L88).\nThis is expected for most word vectors, but it makes the integration of FastText word embeddings difficult. Indeed, the vector of a word is generated by summing the vectors associated with its char n-grams (see the gensim implementation for more details: https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L1657).\nThis is convenient as we can generate an embedding for an OOV word if we have a vector for at least one of its char n-grams.\nOne way to support FastText word embedding would be to let __getitem__ behave differently given a backend attribute of the Vectors (which in this case would be fasttext).\nIf you think this feature is worth adding to spaCy, I can participate in the implementation.\nYour Environment\nPython version     3.5.2          \nspaCy version      2.0.7          \nPlatform           Linux-4.4.0-116-generic-x86_64-with-Ubuntu-16.04-xenial", "issue_status": "Open", "issue_reporting_time": "2018-03-28T14:06:30Z"}, "157": {"issue_url": "https://github.com/explosion/spaCy/issues/2125", "issue_id": "#2125", "issue_summary": "Doc Similarity: Tf-idf weighting?", "issue_description": "mralexpopa commented on 21 Mar 2018\nI am currently using a German model (loaded with fasttext embeddings) to compute similarities between documents.\nI am finding that as the documents get longer, the similarity values stop making sense (basically every long document becomes very similar to ever other long document). I suppose this is due to the averaging method that is used in the similarity method. It is not due to preprocessing, as I'm removing stopwords, lemmatizing etc etc the common procedure.\nIntuitively, as a document's vocabulary increases, the document vector loses it's \"originality\" due to averaging, so I guess this behavior is expected. In order to fix this, I was thinking of weighting each word embedding with the respective tf-idf score, sum up the word vectors in the Doc and normalize the result. Hopefully, this would provide a better Doc representation and a more relevant similarity score.\nAre there any efforts towards integrating such a feature? If not, is it for lack of time or because the suggested solution does not make sense?\nIn order to calculate the tf-idf scores, the concept of a Corpus would need to be implemented, but a quick solution could be allowing to feed in a precomputed score table for each word in the corpus.\n(not really an issue, but rather a suggestion)\nYour Environment\nOperating System: Linux-4.10.12-041012-generic-x86_64-with-Ubuntu-16.04-xenial\nPython Version Used: 3.5.2\nspaCy Version Used: 2.0.9\nEnvironment Information: loaded fasttext embeddings as in the documentation\n5", "issue_status": "Open", "issue_reporting_time": "2018-03-21T17:46:35Z"}, "158": {"issue_url": "https://github.com/explosion/spaCy/issues/1997", "issue_id": "#1997", "issue_summary": "multi class text classifier", "issue_description": "Contributor\nazarezade commented on 18 Feb 2018 \u2022\nedited\nI have problem deciding which way is better to use for multi-class text-classification. For example, the output of the following two class problem:\nimport spacy\nnlp = spacy.load('en')\n\ntrain_data = [\n    (u\"That was very bad\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n    (u\"it is so bad\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n    (u\"so terrible\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n    (u\"I like it\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n    (u\"It is very good.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n    (u\"That was great!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}})\n]\n\ntextcat = nlp.create_pipe('textcat')\nnlp.add_pipe(textcat, last=True)\ntextcat.add_label('POSITIVE')\ntextcat.add_label('NEGATIVE')\noptimizer = nlp.begin_training()\nfor itn in range(10):\n    for doc, gold in train_data:\n        nlp.update([doc], [gold], sgd=optimizer)\ndoc = nlp(u'It is good.')\nprint(doc.cats)\ndoc = nlp(u'It is bad.')\nprint(doc.cats)\nis\n{'POSITIVE': 0.9940504431724548, 'NEGATIVE': 0.03267182409763336}\n{'POSITIVE': 0.6032131314277649, 'NEGATIVE': 0.047026827931404114}\nbut when I use\ntrain_data = [\n    (u\"That was very bad\", {\"cats\": {\"NEGATIVE\": 1}}),\n    (u\"it is so bad\", {\"cats\": {\"NEGATIVE\": 1}}),\n    (u\"so terrible\", {\"cats\": {\"NEGATIVE\": 1}}),\n    (u\"I like it\", {\"cats\": {\"POSITIVE\": 1}}),\n    (u\"It is very good.\", {\"cats\": {\"POSITIVE\": 1}}),\n    (u\"That was great!\", {\"cats\": {\"POSITIVE\": 1}})\n]\nthe result is different:\n{'POSITIVE': 0.9999344348907471, 'NEGATIVE': 0.06132430210709572}\n{'POSITIVE': 0.9991464614868164, 'NEGATIVE': 0.21219134330749512}\nI don't think this is due to randomization, since in different runs of the same code the results remains more of less the same.\nI want to know, which one is the correct way to define the training data in multi-class textcat?\nEnvironment\nOperating System: Mac OS X\nPython Version Used: 3.6\nspaCy Version Used: 2.0.7", "issue_status": "Open", "issue_reporting_time": "2018-02-18T11:52:35Z"}, "159": {"issue_url": "https://github.com/explosion/spaCy/issues/1985", "issue_id": "#1985", "issue_summary": "KeyError when looking up a hash in vocab.strings from a vocab.vector", "issue_description": "fnielsen commented on 14 Feb 2018\nIs this a bug or a feature? I get a KeyError when looking up a hash in vocab.strings from a vocab.vector with the en_vectors_web_lg model.\nimport spacy\nnlp = spacy.load('en_vectors_web_lg')\nfor key, vector in nlp.vocab.vectors.items():\n    print(key, nlp.vocab.strings[key])\nI get\n[...]\n(6292516164439924713, u'MAKE-WAR-NOT-LAW')\n(16510263609655693249, u'make-war-not-law')\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\n  File \"strings.pyx\", line 118, in spacy.strings.StringStore.__getitem__\nKeyError: 4035656307355538346\nIt may be related to #1427 (the 4035656307355538346 key is the same), but I have\n>>> import spacy.symbols\n>>> 'LAW' in spacy.symbols.IDS\nTrue\nand nlp.vocab.strings.add('LAW') does not help.\nInfo about spaCy\nPython version: 2.7.14\nPlatform: Linux-4.13.0-32-generic-x86_64-with-Ubuntu-17.10-artful\nspaCy version: 2.0.7\nModels: en_core_web_lg, en, en_vectors_web_lg, xx", "issue_status": "Open", "issue_reporting_time": "2018-02-14T17:38:19Z"}, "160": {"issue_url": "https://github.com/explosion/spaCy/issues/1871", "issue_id": "#1871", "issue_summary": "could Matcher using python multiprocessing ?", "issue_description": "meshiguge commented on 22 Jan 2018\nNo description provided.", "issue_status": "Open", "issue_reporting_time": "2018-01-22T07:36:41Z"}, "161": {"issue_url": "https://github.com/explosion/spaCy/issues/1772", "issue_id": "#1772", "issue_summary": "tokenizer_exceptions problem with Persian", "issue_description": "mosynaq commented on 27 Dec 2017\nI am trying to train Persian to spaCy. One of the problems is here: in tokenizer_exceptions.py, spaCy expects concatenation of two orths, form the word itself like do + n't = don't, but for Persian, this expectation is not valid for some cases.\nFor example, the verb \"\u0628\u0631 \u0646\u062e\u0648\u0627\u0647\u062f \u06af\u0634\u062a\" ( = s/he will not return), is made up of \"\u0628\u0631\" + \"\u0646\u0640\" + \"\u062e\u0648\u0627\u0647\u062f \u06af\u0634\u062a\".\n(\"\u0646\u0640\" negates a Persian verb. Most of the times the negation thing comes in the beginning, but in some, cases like this one, it comes in between. )\nAs you can see you cannot simply concatenate orthes to form the full form.\nShould spaCy expectation be changed? Or should I do something?\nMy Environment\nspaCy version: 2.0.5\nPlatform: Linux-4.11.0-14-generic-x86_64-with-LinuxMint-18.2-sonya\nModels: en, fa\nPython version: 3.5.2", "issue_status": "Open", "issue_reporting_time": "2017-12-27T15:58:31Z"}, "162": {"issue_url": "https://github.com/explosion/spaCy/issues/1592", "issue_id": "#1592", "issue_summary": "npy_endian.h cannot detect the correct processor endianness/byte order for ppc64le", "issue_description": "sdmonov commented on 16 Nov 2017\nMy Environment\nOperating System: Ubuntu 16.04.3 LTS on Power8\nPython Version Used: Python 3.6.2 |Anaconda, Inc.| (default, Sep 15 2017, 20:38:23) [GCC 4.8.4] on linux\nspaCy Version Used: Master from 11/16/2017 (commit a3d4dd1)\nSystem Information: PowerNV 8335-GTB with POWER8NVL CPU\nNumpy headers included in spaCy come from older version of numpy and cannot detect the byte order for ppc64le CPU correctly. If I replace the spaCy/include/numpy/ with the latest headers, spaCy works fine on ppc64le.\n2", "issue_status": "Open", "issue_reporting_time": "2017-11-16T14:40:02Z"}, "163": {"issue_url": "https://github.com/explosion/spaCy/issues/1215", "issue_id": "#1215", "issue_summary": "CoNLL-U to displaCy treebank conversion", "issue_description": "GruffPrys commented on 24 Jul 2017 \u2022\nedited\nCoNLL-U is the format used by the Universal Dependencies initiative to annotate dependency treebanks (http://universaldependencies.org/format.html). A large number of treebanks for many different languages are available in the CoNLL-U format. However, currently displaCy cannot parse the CoNLL format to visualize those trees, and it would be nice if it could.\nThe following code converts a single sentence from the CoNLL format to the json format used by displaCy:\nimport json\nfrom collections import OrderedDict\n\nconll_u_string = \"\"\"1 As as SCONJ IN _ 4 mark _ _\n2 the the DET DT Definite=Def|PronType=Art 3 det _ _\n3 year year NOUN NN Number=Sing 4 nsubj _ _\n4 progresses progress VERB VBZ Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin 13 advcl _ SpaceAfter=No\n5 , , PUNCT , _ 13 punct _ _\n6 the the DET DT Definite=Def|PronType=Art 13 mark _ _\n7 more more ADV RBR _ 13 advmod _ _\n8 impressive impressive ADJ JJ Degree=Pos 13 xcomp _ _\n9 your you PRON PRP$ Person=2|Poss=Yes|PronType=Prs 10 nmod:poss _ _\n10 ideas idea NOUN NNS Number=Plur 13 nsubj _ _\n11 and and CCONJ CC _ 12 cc _ _\n12 works work NOUN NNS Number=Plur 10 conj _ _\n13 become become VERB VBP Mood=Ind|Tense=Pres|VerbForm=Fin 0 root _ SpaceAfter=No\n14 . . PUNCT . _ 8 punct _ _\"\"\"\n\n\ndef set_arrow_direction(word_line):\n    \"\"\"\n    Sets the orientation of the arrow that notes the directon of the dependency\n    between the two units.\n\n    \"\"\"\n    if int(word_line[\"id\"]) > int(word_line[\"head\"]):\n        word_line[\"dir\"] = \"right\"\n    elif int(word_line[\"id\"]) < int(word_line[\"head\"]):\n        word_line[\"dir\"] = \"left\"\n    return word_line\n\ndef convert2zero_based_numbering(word_line_field):\n    \"CONLL-U numbering starts at 1, displaCy's at 0...\"\n    word_line_field = str(int(word_line_field) - 1)\n    return word_line_field\n\ndef get_start_and_end(word_line):\n    \"\"\"\n    Displacy's 'start' value is the lowest value amongst the ID and HEAD values,\n    and the 'end' is always the highest. 'Start' and 'End' have nothing to do\n    with dependency which is indicated by the arrow direction, not the line\n    direction.\n    \"\"\"\n    word_line[\"start\"] = min([int(word_line[\"id\"]), int(word_line[\"head\"])])\n    word_line[\"end\"] = max([int(word_line[\"id\"]), int(word_line[\"head\"])])\n    return word_line\n                           \ndef conll_u_string2displacy_json(conll_u_sent_string): \n    \"\"\"\n    Converts a single CONLL-U formatted sentence to the displaCy json format.\n    CONLL-U specification: http://universaldependencies.org/format.html\n    \"\"\"\n    conll_u_lines = [line for line in conll_u_sent_string.split(\"\\n\") \\\n                     if line[0].isnumeric()]\n\n    displacy_json = {\"arcs\": [], \"words\": []}\n    for tabbed_line in conll_u_lines:\n\n        word_line = OrderedDict()\n        word_line[\"id\"], word_line[\"form\"], word_line[\"lemma\"], \\\n        word_line[\"upostag\"], word_line[\"xpostag\"], word_line[\"feats\"], \\\n        word_line[\"head\"], word_line[\"deprel\"], word_line[\"deps\"], \\\n        word_line[\"misc\"] = tabbed_line.split(\"\\t\")\n\n        word_line[\"id\"] = convert2zero_based_numbering(word_line[\"id\"])\n        word_line[\"head\"] = convert2zero_based_numbering(word_line[\"head\"])       \n        \n        if word_line[\"deprel\"] != \"root\":\n            word_line = get_start_and_end(word_line)\n            word_line = set_arrow_direction(word_line)\n            displacy_json[\"arcs\"].append({\"dir\": word_line[\"dir\"],\n                                          \"end\": word_line[\"end\"],\n                                          \"label\": word_line[\"deprel\"],\n                                          \"start\": word_line[\"start\"]})\n            \n        displacy_json[\"words\"].append({\"tag\": word_line[\"upostag\"],\n                                       \"text\": word_line[\"form\"]})\n\n    displacy_json = (json.dumps(displacy_json, indent=4))\n    return displacy_json\n\ndisplacy_json = conll_u_string2displacy_json(conll_u_string)\nprint (displacy_json)\nExample output:\n``` { \"arcs\": [ { \"label\": \"mark\", \"start\": 0, \"dir\": \"left\", \"end\": 3 }, { \"label\": \"det\", \"start\": 1, \"dir\": \"left\", \"end\": 2 }, { \"label\": \"nsubj\", \"start\": 2, \"dir\": \"left\", \"end\": 3 }, { \"label\": \"advcl\", \"start\": 3, \"dir\": \"left\", \"end\": 12 }, { \"label\": \"punct\", \"start\": 4, \"dir\": \"left\", \"end\": 12 }, { \"label\": \"mark\", \"start\": 5, \"dir\": \"left\", \"end\": 12 }, { \"label\": \"advmod\", \"start\": 6, \"dir\": \"left\", \"end\": 12 }, { \"label\": \"xcomp\", \"start\": 7, \"dir\": \"left\", \"end\": 12 }, { \"label\": \"nmod:poss\", \"start\": 8, \"dir\": \"left\", \"end\": 9 }, { \"label\": \"nsubj\", \"start\": 9, \"dir\": \"left\", \"end\": 12 }, { \"label\": \"cc\", \"start\": 10, \"dir\": \"left\", \"end\": 11 }, { \"label\": \"conj\", \"start\": 9, \"dir\": \"right\", \"end\": 11 }, { \"label\": \"punct\", \"start\": 7, \"dir\": \"right\", \"end\": 13 } ], \"words\": [ { \"tag\": \"SCONJ\", \"text\": \"As\" }, { \"tag\": \"DET\", \"text\": \"the\" }, { \"tag\": \"NOUN\", \"text\": \"year\" }, { \"tag\": \"VERB\", \"text\": \"progresses\" }, { \"tag\": \"PUNCT\", \"text\": \",\" }, { \"tag\": \"DET\", \"text\": \"the\" }, { \"tag\": \"ADV\", \"text\": \"more\" }, { \"tag\": \"ADJ\", \"text\": \"impressive\" }, { \"tag\": \"PRON\", \"text\": \"your\" }, { \"tag\": \"NOUN\", \"text\": \"ideas\" }, { \"tag\": \"CCONJ\", \"text\": \"and\" }, { \"tag\": \"NOUN\", \"text\": \"works\" }, { \"tag\": \"VERB\", \"text\": \"become\" }, { \"tag\": \"PUNCT\", \"text\": \".\" } ] } ```\nApologies that this isn't submitted as a Pull Request - I've not been able to get Spacy 2.0 Alpha up and running on my Windows machines. I've tested the output in displaCy using Jupyter and compared the resulting visualizations with the brat visualizations here:\nhttp://bionlp-www.utu.fi/dep_search/\nFor example:\nSo far, they seem to match:\nHowever, further testing is advisable.\nCoNLL-U for the above examples:\n1 Thursday Thursday PROPN NNP Number=Sing 2 nsubj _ _\n2 works work VERB VBZ Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin 0 root _ _\n3 for for ADP IN _ 4 case _ _\n4 me I PRON PRP Case=Acc|Number=Sing|Person=1|PronType=Prs 2 obl _ SpaceAfter=No\n5 . . PUNCT . _ 2 punct _ _\n2\n1", "issue_status": "Open", "issue_reporting_time": "2017-07-23T23:21:31Z"}, "164": {"issue_url": "https://github.com/explosion/spaCy/issues/1107", "issue_id": "#1107", "issue_summary": "\ud83d\udcab Idea: Example sentences for testing languages", "issue_description": "Member\nines commented on 6 Jun 2017 \u2022\nedited\nWhen testing models and spaCy's performance in general, it's always nice to have a few text examples handy \u2013 especially for the languages you don't speak. We noticed this while testing the new Spanish and multi-language NER models. It's not always easy to find good, representative sentences, and copy-pasting whatever is on the Wikipedia front page that day isn't ideal either.\nSo here's the idea: For each language, we want to add a simple list of text examples that can be imported and processed \u2013 for example, to test if a model is working as expected. Ideally, those examples should include different types of sentences, grammatical constructions and named entities. The examples could also be used in spaCy's test suite.\nExamples could live with the language's data in spacy.lang.\nexamples = [\n    'Apple is looking at buying U.K. startup for $1 billion',\n    'Autonomous cars shift insurance liability toward manufacturers',\n    'San Francisco considers banning sidewalk delivery robots',\n    'London is a big city in the United Kingdom.'\n]\nfrom spacy.lang.en import examples\n\ndoc = nlp(examples[0]) # use one example\nprint([(t.text, t.pos_, t.tag_) for t in doc])\n\nfrom spacy import displacy\ndocs = [nlp(doc) for doc in examples]\ndisplacy.serve(*docs) # visualise all example sentences on one page\nLanguages with models (8/8)\nEnglish en (can be covered by us)\nGerman de (can be covered by us)\nFrench fr (thanks @Gregory-Howard & @Tpt)\nSpanish es (thanks @lucianosilvi)\nPortuguese pt (thanks @oximer)\nItalian it (thanks @MartinoMensio)\nDutch nl (thanks @redevries)\nGreek el (thanks @Eleni170 & @giannisdaras)\nLanguages with only tokenizers (29/40)\nAlbanian sq (thanks @AlbionaHoti)\nArabic ar (thanks @tzano)\nBengali bn (thanks @roshni-b)\nBulgarian bg (thanks @KristiyanVachev)\nCatalan ca (thanks @mpuig)\nChinese zh (thanks @renewang)\nCroatian hr (thanks @isaric)\nDanish da (thanks @mollerhoj)\nFinnish fi (thanks @tjkemp)\nHebrew he (thanks @beneyal)\nHindi hi (thanks @weavermonkey)\nHungarian hu (thanks @oroszgy)\nIndonesian id (thanks @geovedi)\nJapanese ja (thanks @tokestermw)\nLithuanian lt (thanks @rokasramas)\nNorwegian nb (thanks @luvogels)\nPersian fa (thanks @azarezade)\nPolish pl (thanks @adam-ra)\nRomanian ro (thanks @janimo)\nRussian ru (thanks @gsoul)\nSinhala si (thanks @keshan)\nSwedish sv (thanks @RickardSjogren)\nTamil ta (thanks @Loghijiaha)\nTatar tt (thanks @aliiae)\nTelugu te (thanks @sainathadapa)\nThai th (thanks @korakot & @wannaphongcom)\nTurkish tr (thanks @DuyguA)\nUkrainian uk (thanks @juliamakogon)\nUrdu ur (thanks @mirfan899)\nAfrikaans af\nCzech cs\nIcelandic is\nIrish ga\nKannada kn\nLatvian lv\nLuxembourgish lb\nSerbian sr\nSlovak sk\nSlovenian sl\nVietnamese vi\n\"This is a sentence\" example sentence (27/49)\nIf you happen to speak one of the tokenization-only languages above, a translation of \"This is a sentence.\" would also be very helpful! The new model docs have a \"Quickstart\" widget that shows the install command, plus an example sentence. If we ever add a model for one of the alpha languages, it'd be nice to have the example sentence available (and mistakes in the example would be pretty embarrassing, haha).\nAlbanian sq: \"Kjo \u00ebsht\u00eb nj\u00eb fjali.\" (thanks @AlbionaHoti)\nArabic ar: \u0647\u0630\u0647 \u062c\u0645\u0644\u0629 (thanks @Ahmed0Sultan)\nBulgarian bg: \"\u0422\u043e\u0432\u0430 \u0435 \u0438\u0437\u0440\u0435\u0447\u0435\u043d\u0438\u0435\" (thanks @KristiyanVachev)\nDanish da: \"Dette er en s\u00e6tning.\" (thanks @philiphoyos)\nDutch nl: \"Dit is een zin.\" (thanks @redevries)\nEnglish en: \"This is a sentence.\"\nFrench fr: \"C'est une phrase.\"\nGerman de: \"Dies ist ein Satz.\"\nGreek el: \"\u0391\u03c5\u03c4\u03ae \u03b5\u03af\u03bd\u03b1\u03b9 \u03bc\u03b9\u03b1 \u03c0\u03c1\u03cc\u03c4\u03b1\u03c3\u03b7.\" (thanks @Eleni170)\nHebrew he: \"\u05d6\u05d4\u05d5 \u05de\u05e9\u05e4\u05d8.\" (thanks @beneyal)\nHindi hi: \"\u092f\u0939 \u090f\u0915 \u0935\u093e\u0915\u094d\u092f \u0939\u0948\u0964\" (thanks @weavermonkey & @psmehta21)\nHungarian hu: \"Ez egy mondat.\" (thanks @oroszgy)\nIndonesian id: \"Ini adalah sebuah kalimat.\" (thanks @prdx)\nItalian it: \"Questa \u00e8 una frase.\" (thanks @MartinoMensio)\nJapanese ja: \"\u3053\u308c\u306f\u6587\u7ae0\u3067\u3059\u3002\" (thanks @tokestermw)\nNorwegian nb: \"Dette er en setning.\" (thanks @luvogels)\nPolish pt: \"To jest zdanie.\" (thanks @adam-ra)\nPortuguese pt: \"Esta \u00e9 uma frase.\"\nRomanian ro: \"Aceasta este o propozi\u021bie\" (thanks @janimo)\nSinhala si: \"\u0db8\u0dd9\u0dba \u0dc0\u0dcf\u0d9a\u0dca\u200d\u0dba\u0dba\u0d9a\u0dd2.\" (thanks @keshan)\nSpanish es: \"Esto es una frase.\"\nTelugu te: \"\u0c07\u0c26\u0c3f \u0c12\u0c15 \u0c35\u0c3e\u0c15\u0c4d\u0c2f\u0c02.\" (thanks @sainathadapa)\nThai th: \"\u0e19\u0e35\u0e48\u0e04\u0e37\u0e2d\u0e1b\u0e23\u0e30\u0e42\u0e22\u0e04\" (thanks @cvorasucha)\nTurkish tr: \"Bu bir c\u00fcmledir.\" (thanks @DuyguA)\nUkrainian uk: \"\u0426\u0435 \u0440\u0435\u0447\u0435\u043d\u043d\u044f.\" (thanks juliamakogon)\nUrdu ur: \"\u06cc\u06c1 \u0627\u06cc\u06a9 \u062c\u0645\u0644\u06c1 \u06c1\u06d2\u06d4\" (thanks @mirfan899)\nAfrikaans af\nBengali bn\nCatalan ca\nChinese zh\nCroatian hr\nCzech cs\nFinnish fi\nIcelandic is\nIrish ga\nKannada kn\nLatvian lv\nLithuanian lt\nLuxembourgish lb\nPersian fa\nRussian ru\nSerbian sr\nSlovak sk\nSlovenian sl\nSwedish sv\nTamil ta\nTatar tt\nVietnamese vi\nWe haven't fully decided how the text examples should be implemented The examples will live in a file examples.py in the language data \u2013 for example lang/en/examples.py. Currently, there's only a sentences list, but more examples can be added in the future.\nIf you want to help out and contribute a few examples, you can submit a PR, or simply post them here!", "issue_status": "Open", "issue_reporting_time": "2017-06-06T12:32:38Z"}, "165": {"issue_url": "https://github.com/explosion/spaCy/issues/1066", "issue_id": "#1066", "issue_summary": "Obtain the size of training data used to build a model", "issue_description": "adam-ra commented on 17 May 2017 \u2022\nedited\nIt could be quite useful to be able to get (accurate or estimated, whatever) size of the training data that has been used to train a tagging and parsing model (I guess it also holds for NER).\nThis could be for instance available as nlp.tagger.model.examples_seen or sth alike (perhaps a meta dict with more statistics if available).\nThis would be useful to guesstimate the number of examples needed to post-train a tagger (as in #1015). Making the post-training work as expected is obviously more complex than repeating the same few training examples FRACTION * ORIGINAL_CORPUS_SIZE but it's still better than hardcoding an out-of-the-blue absolute number.", "issue_status": "Open", "issue_reporting_time": "2017-05-17T09:05:53Z"}, "166": {"issue_url": "https://github.com/explosion/spaCy/issues/1036", "issue_id": "#1036", "issue_summary": "n-best POS tags", "issue_description": "sjmielke commented on 3 May 2017\nDoes the option to output n-best POS tags (or even all tags with score > 0) exist with some hidden option or in some branch?\nIf not, would such an additional function be welcome or should I just hack it into my own fork? Technically it should be very easy (basically just replace the arg_max_if_true in spacy/tagger.pyx#L211 with an appropriate collection of results), but I have a feeling it will be more difficult to expose it nicely...", "issue_status": "Open", "issue_reporting_time": "2017-05-03T09:09:01Z"}, "167": {"issue_url": "https://github.com/explosion/spaCy/issues/993", "issue_id": "#993", "issue_summary": "Provide a command line method to create models from UniversalDependencies and Wikipedia", "issue_description": "Contributor\nTpt commented on 19 Apr 2017 \u2022\nedited\nIt would be great to have a command line method to build automatically a basic model from Wikipedia and UniversalDependencies for all supported languages. It would provides word vectors, a tagger and a parser.\nWe could also do a variant using Common Crawl.\n2", "issue_status": "Open", "issue_reporting_time": "2017-04-19T13:04:15Z"}, "168": {"issue_url": "https://github.com/explosion/spaCy/issues/881", "issue_id": "#881", "issue_summary": "\ud83d\udcab Train parser and NER with regression objective, to make scores express expected parse quality", "issue_description": "Member\nhonnibal commented on 10 Mar 2017 \u2022\nedited\nMore and more people have been asking about confidence scores for the parser and NER. The current model can't answer this, so I decided to dust off some almost-complete research from last year to fix this.\nThis work is almost complete, and should be up on master within a day or two. . Here's how it works. Edit: I spoke too soon....The problem was that the regression loss objective I describe here produced extremely non-sparse solutions with the linear model. It should be possible to find a good compromise with L1 regularisation, but I switched efforts to the v2 experiments instead.\nEdit 2: spaCy 2 uses neural networks, so the sparsity isn't a problem. But I haven't been able to get the regression loss working well at all. I think something's wrong with my implementation.\nv2 now has beam parsing implemented, which supports one way to get quality estimates for parses --- see below. However, I'd like to resume efforts on the regression loss objective. I think there's a bug in the current implementation of this loss function. See below.\nCurrently the parser and NER are trained with a hinge-loss objective (specifically, using the averaged perceptron update rule). At each word, the model asks \"What's the highest scoring action?\". It makes its prediction, and then it asks the oracle to assign a cost to each action, where the cost represents the number of new errors that will be introduced if that action is taken. For instance, if we're at the start of an ORG entity, and we perform the action O, we introduce two errors: we miss the entity, and we miss the label. The actions B-PER and U-ORG each introduce one, and the action B-ORG introduces zero. If our predicted action isn't zero-cost, we update the weights such that in future this action will score a bit lower for this example, and the best zero-cost action will score a bit higher.\nIf we're only looking at the quality of the output parse, this setup performs well. But it means the scores on the actions have no particular interpretation. We don't force them into any useful scale, and we don't train them to reflect the wider parse quality. If the parser is in a bad state, it's not trained to give uniformly lower scores. It's trained to make the best of bad situations.\nThe changes I'm merging improve this in two ways. They're looking forwards to the spaCy 2.0 neural network models, but they're useful with the current linear models too, so I decided to get them in early.\n1. Beam search with global objective\nThis is the standard solution: use a global objective, so that the parser model is trained to prefer parses that are better overall. Keep N different candidates, and output the best one. This can be used to support confidence by looking at the alternate analyses in the beam. If an entity occurs in every analysis, the NER is more confident it's correct.\n2. Optimize the negative cost explicitly (i.e. do numeric regression, not multiclass classification)\nThis idea has been kicking around for a while. I think a few people have tried it with negative results. It was first raised to me in 2015 by Mark Johnson. I guess to a lot of folks it's obvious.\nThe idea is this: we have an oracle that tells us the number of errors an action will introduce. Instead of arbitrary high/low scores, we try to make the model output a score that matches the oracle's output. This means that if an action would introduce 2 errors, we want to predict \"2\". We don't just want it to score lower than some other class, that would introduce 0 errors. It's handy to flip the sign on this, so that we're still taking an argmax to choose the action.\nIn my previous experiments, this regression loss produced parse accuracies that were very slightly worse --- the difference in accuracy was 0.2%. In parsing research, this is indeed a negative result :).\nHowever, this difference in accuracy doesn't matter at all --- and the upside of the regression setup is quite significant! With the regression model, the scores output by the parser have a meaningful interpretation: the sum of the scores is the expected number of errors in the analysis. This is exactly what people are looking for, and it comes with no increase in complexity or run-time. It's just a change to the objective used to train the model.\n15", "issue_status": "Open", "issue_reporting_time": "2017-03-10T10:43:45Z"}, "169": {"issue_url": "https://github.com/explosion/spaCy/issues/831", "issue_id": "#831", "issue_summary": "Is there a way to find the probability or confidence score of the extracted named entities?", "issue_description": "faizan30 commented on 16 Feb 2017 \u2022\nedited\nUsage\nYour Environment\nOperating System: Windows 10\nPython Version Used: 2.7, 64 bit\nspaCy Version Used: 1.6.0\nEnvironment Information: Anaconda\n2", "issue_status": "Open", "issue_reporting_time": "2017-02-16T06:30:43Z"}, "170": {"issue_url": "https://github.com/explosion/spaCy/issues/828", "issue_id": "#828", "issue_summary": "Word segmentation", "issue_description": "lminer commented on 15 Feb 2017\nIt would be great to have word segmentation capabilities in spacy.\nSomething like:\n['bring', 'back', 'the', 'spaces!'] = [w.text for w in nlp.segment(u'bringbackthespaces!')]\n4", "issue_status": "Open", "issue_reporting_time": "2017-02-14T22:32:10Z"}, "171": {"issue_url": "https://github.com/explosion/spaCy/issues/780", "issue_id": "#780", "issue_summary": "TAG_MAP definition and POS training for languages with complex grammar", "issue_description": "ecamaj commented on 27 Jan 2017\nCroatian language has very complex grammar and POS tagging consists of many elements (tags):\nhttp://nlp.ffzg.hr/data/tagging/msd-hr.html#msd.msds-hr\nso, when all those elements are added to TAG_MAP, example:\n'Agcfpi': {'Degree': 'Cmp', 'Gender': 'Fem', 'Number': 'Plur', 'pos': 'ADJ'},\n'Ps3npap-n-a': {'Gender': 'Neut',\n                    'Number': 'Plur',\n                    'Number[psor]': 'Plur',\n                    'Person': '3',\n                    'Poss': 'Yes',\n                    'PronType': 'Prs',\n                    'Variant': 'Long',\n                    'pos': 'DET'},\n...\nand then using training script for POS traning there is problem with morphology/attrs scripts.\nConclusion after chat with Matt - morphology and attrs scripts should be rewritten for better support of the new languages.\nEnvironment\nOperating System: MacOS Sierra\nPython Version Used: 3.5\nspaCy Version Used: 1.6", "issue_status": "Open", "issue_reporting_time": "2017-01-27T17:54:11Z"}, "172": {"issue_url": "https://github.com/explosion/spaCy/issues/580", "issue_id": "#580", "issue_summary": "DRT Parser?", "issue_description": "ehknight commented on 26 Oct 2016\nAre there any plans to implement a DRT (discourse representation theory) system in spaCy, perhaps similar to C&C Tools+Boxer?", "issue_status": "Open", "issue_reporting_time": "2016-10-26T05:17:19Z"}, "173": {"issue_url": "https://github.com/explosion/spaCy/issues/425", "issue_id": "#425", "issue_summary": "Disambiguation for tokenization", "issue_description": "ozwiz commented on 12 Jun 2016\nFirst of all thanks for your great work.\nI have read many issues posted here about tokenization. It is a tough task. For example \"Adam's\" may mean \"of Adam\" or \"Adam is\".\nWould you consider any disambiguation step that will merge or split \"'s\" automatically based on later functions?", "issue_status": "Open", "issue_reporting_time": "2016-06-11T23:38:40Z"}, "174": {"issue_url": "https://github.com/explosion/spaCy/issues/331", "issue_id": "#331", "issue_summary": "Add lexical constraints for POS tags (like a tag dictionary)", "issue_description": "chrisjbryant commented on 11 Apr 2016\nHi,\nI've noticed quite a few cases where the PTB tag on a verb is misleading, albeit in ungrammatical sentences:\nFor example:\n\"He didn't kept my secret.\" - \"kept\" gets tagged VB.\n\"Actually I can't swimming at all.\" - \"swimming\" gets tagged VB.\nNow while it's true you would expect a VB in those contexts, it seems worrying that any form of a verb can potentially be tagged as the base VB form if that's what licensed by context. Indeed this undermines the whole concept of a base form. Instead, I would much rather \"kept\" be tagged VBD (or even VBN) and \"swimming\" be tagged VBG even if these are unexpected in the context.\nPerhaps you can restrict each form to a specific label somehow?", "issue_status": "Open", "issue_reporting_time": "2016-04-10T20:42:26Z"}, "175": {"issue_url": "https://github.com/explosion/spaCy/issues/59", "issue_id": "#59", "issue_summary": "Support for constituency parsing", "issue_description": "Contributor\nelyase commented on 21 Apr 2015\nIt would be great if spacy offered some sort of constituency parsing information. I think the API can look similar to the one used for NER.\n11", "issue_status": "Open", "issue_reporting_time": "2015-04-21T12:41:06Z"}}, "closed_issues": {"1": {"issue_url": "https://github.com/explosion/spaCy/issues/4951", "issue_id": "#4951", "issue_summary": "Displacy does not display NER correctly when having list sorted", "issue_description": "Alea4jacta6est commented 2 days ago\nWhen I try to visualize entities, if the list of entities is not sorted by by start-end positions in the dictionary, I get a strange visualization.\nfrom spacy import displacy\ndict = {'text': 'Droits de rejet La loi f\u00e9d\u00e9rale russe n\u00b071 FZ du 06 mai 1998 \u00ab Sur les charges d\u2019exploitation des sources hydrologiques \u00bb ( modifi\u00e9e par les lois f\u00e9d\u00e9rales n\u00b054 FZ du 30 mars 1999 , n\u00b0111 FZ du 07 ao\u00fbt 2001 et n\u00b0194 FZ du 30 d\u00e9cembre 2001 ) rassemblait les charges d\u2019exploitation des ressources hydrologiques ( taxe sur l\u2019eau ) et des charges de protection et de restauration des ressources hydrologiques ( toutes deux introduites par le code sur l\u2019eau de la F\u00e9d\u00e9ration de Russie ) sous une seule appellation : les charges d\u2019exploitation des ressources hydrologiques .',\n  'ents': [{'entity': 'taxe sur l\u2019eau',\n    'start': 311,\n    'end': 325,\n    'label': '4 Water Management'},\n   {'entity': 'exploitation des sources hydrologiques',\n    'start': 81,\n    'end': 119,\n    'label': '4 Water Management'},\n   {'entity': 'exploitation des ressources hydrologiques',\n    'start': 267,\n    'end': 308,\n    'label': '4 Water Management'},\n   {'entity': 'exploitation des ressources hydrologiques',\n    'start': 525,\n    'end': 566,\n    'label': '4 Water Management'}]}\ndisplacy.render(dict, manual=True, style='ent', jupyter=True)\nIs it an expected behavour of displacy?\nInfo about spaCy\nspaCy version: 2.0.18\nPlatform: Linux-4.15.0-74-generic-x86_64-with-debian-stretch-sid\nPython version: 3.7.3\nModels: en_core_web_sm, en_core_web_lg", "issue_status": "Closed", "issue_reporting_time": "2020-01-29T13:41:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "2": {"issue_url": "https://github.com/explosion/spaCy/issues/4939", "issue_id": "#4939", "issue_summary": "UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.", "issue_description": "lmorillas commented 7 days ago\nHow to reproduce the behaviour\nimport spacy\nnlp =  spacy.load(\"es_core_news_md\")\n\ndef most_similar(word):\n    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n    return [w.orth_ for w in by_similarity[:10]]\n\nmost_similar(nlp.vocab['perro'])\nes_core_news_md include vectors [1] but it raises a warning: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n[1] https://github.com/explosion/spacy-models/releases//tag/es_core_news_md-2.2.5\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.13.0-46-generic-x86_64-with-Ubuntu-17.10-artful\nPython version: 3.6.3", "issue_status": "Closed", "issue_reporting_time": "2020-01-24T07:56:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "3": {"issue_url": "https://github.com/explosion/spaCy/issues/4937", "issue_id": "#4937", "issue_summary": "Span.as_doc() does not preserve values of Token custom extensions", "issue_description": "jack-rory-staunton commented 8 days ago \u2022\nedited\nHow to reproduce the behaviour\nI set a custom extension on Tokens, tok._.x. When I create a Doc, the Tokens all have the correct value of the token extension. If I then select a span and call span.as_doc(), the resulting document's tokens' .has_extension() is True, but the value is set to the default value. On closer inspection, it appears that .as_doc() does not process the span into a doc by using the nlp pipeline that originally created the span. If I process the doc returned from Span.as_doc() with the relevant pipeline component, I get the expected values on the tokens' custom extension. Is this the expected behavior? It seems like .as_doc() should respect the originating pipeline. I'm not 100% sure, but I never noticed this kind of discrepancy with Span or Doc custom extensions not being preserved in this way, so it may be specific to the token extensions.\nYour Environment\nspaCy version: 2.2.3\nPlatform: Darwin-19.2.0-x86_64-i386-64bit\nPython version: 3.7.4", "issue_status": "Closed", "issue_reporting_time": "2020-01-23T00:08:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "4": {"issue_url": "https://github.com/explosion/spaCy/issues/4935", "issue_id": "#4935", "issue_summary": "Batch processing doesn't speed up?", "issue_description": "lingvisa commented 8 days ago \u2022\nedited\nI loaded 300000 line of Chinese texts, each line is like a tweet length. The compared the batch and non-batch mode to compare its speed:\nBatch:\ndef process_text_with_batch(text_lines):\n    line_number = 0\n    print(\"Start to process: \")\n    for doc in nlp.pipe(text_lines):\n        line_number += 1\n        print(line_number)\nNon batch:\ndef process_text(text_lines):\n    line_number = 0\n    print(\"Start to process: \")\n    for text in text_lines:\n        doc = nlp(text)\n        line_number += 1\n        print(line_number)\nWhere 'text_lines' is an array. The two ways finish almost the same time consumed: 2.5 and 2.6 minutes. why doesn't it speed up? spaCy is using Jieba segmentation. Is possible that Jieba doesn't speed up?", "issue_status": "Closed", "issue_reporting_time": "2020-01-22T18:53:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "5": {"issue_url": "https://github.com/explosion/spaCy/issues/4930", "issue_id": "#4930", "issue_summary": "Partial match of spans in Matcher with ENT_TYPE", "issue_description": "Contributor\nfizban99 commented 10 days ago\nIf I create a Matcher trying to match two GPEs with a + sign in-between (GPE+GPE), the matcher only matches the last token of the first GPE and the first token of the second GPE. I would expect to match the full content of both GPEs.\nSo, for \"San Francisco + New York\" I would expect the matcher to match the whole sentence, while in reality it only matches \"Francisco + New\".\nThe Rule Matcher Explorer behaves the same way, so I am not sure this is in fact the expected behaviour.\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load('en_core_web_sm')\n\ndoc = nlp(\"San Francisco + New York\")\nfor ent in doc.ents:\n    print(ent.label_ + \": \" + ent.text)\n    \nmatcher = Matcher(nlp.vocab)\npatterns = [{\"ENT_TYPE\": \"GPE\"}, {\"ORTH\": \"+\"},{\"ENT_TYPE\": \"GPE\"}]\nmatcher.add(\"DOUBLE_GPE\", None, patterns)\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    print(nlp.vocab.strings[match_id] + \": \" + Span(doc,start,end).text)\nThis returns:\nGPE: San Francisco\nGPE: New York\nDOUBLE_GPE: Francisco + New\nI would have expected:\nGPE: San Francisco\nGPE: New York\nDOUBLE_GPE: San Francisco + New York\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-4.15.0-52-generic-x86_64-with-debian-buster-sid\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2020-01-21T13:36:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "6": {"issue_url": "https://github.com/explosion/spaCy/issues/4929", "issue_id": "#4929", "issue_summary": "How to train completely new entities instead of pre-trained entities using Spacy NER model?", "issue_description": "deepakiim commented 10 days ago\nHow do I do transfer learning i.e. take pre-trained Spacy NER model and make it learn new entities specific to my use case?\nFor this, I have 100 new annotated training samples. The new retrained model should only predict the new entities and not any of the existing entities in the pre-trained spacy model. Just adding/updating new entities to existing models and ignoring the old entities during prediction doesn't make sense.\nThis official example describes how to add new entities to existing pre-trained entities but that's not what I want. I also have very few examples i.e. 100 to completely built a new NER model from scratch.\nEdit: I want to identify all account numbers in an unstructured document.\nExample (\"I would like to change address corresponding to my account 12345. Kindly let me know how to do it. \" [34, 39, 'accountnumber'])", "issue_status": "Closed", "issue_reporting_time": "2020-01-21T09:26:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "7": {"issue_url": "https://github.com/explosion/spaCy/issues/4928", "issue_id": "#4928", "issue_summary": "Beam Density", "issue_description": "davidbren commented 11 days ago\nCan you let us know what the default beam density is ? I can't seem to find it in the documentation\nEnvironment\nspaCy version: 2.1.8\nPlatform: Linux-3.10.0-1062.el7.x86_64-x86_64-with-redhat-7.7-Maipo\nPython version: 3.7.0", "issue_status": "Closed", "issue_reporting_time": "2020-01-20T13:59:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "8": {"issue_url": "https://github.com/explosion/spaCy/issues/4926", "issue_id": "#4926", "issue_summary": "Phrase Matcher space sensitive issue", "issue_description": "AnishaMohandass commented 11 days ago\nterms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\ndoc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n\"converse in the Oval Office inside the White House in Washington, D.C.\")\nIf I enter an extra space between the words \"Barack Obama\", the phrase matcher does not work since it is space sensitive.\nIs there a way to overcome this space sensitive issue?\nYour Environment\nOperating System: Windows 8\nPython Version Used: 3.7\nspaCy Version Used: 2.2.3\nEnvironment Information: Conda", "issue_status": "Closed", "issue_reporting_time": "2020-01-20T05:10:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "9": {"issue_url": "https://github.com/explosion/spaCy/issues/4924", "issue_id": "#4924", "issue_summary": "`nlp.evaluate` with empty `docs_golds` causes a bug", "issue_description": "Contributor\ntamuhey commented 11 days ago\nHow to reproduce the behaviour\nnlp = spacy.blank(\"en\")\ndocs_golds = [(\"\", {})]\nnlp.evaluate(docs_golds)\nCauses the error:\n        gold_deps = set()\n        gold_deps_per_dep = {}\n        gold_tags = set()\n>       gold_ents = set(tags_to_entities([annot[-1] for annot in gold.orig_annot]))\nE       TypeError: 'NoneType' object is not iterable\n\nspacy/scorer.py:239: TypeError\nYour Environment\nOperating System: osx\nPython Version Used: 3.7.4\nspaCy Version Used: master\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2020-01-20T04:44:15Z", "fixed_by": "#4925", "pull_request_summary": "fix nlp.evaluate (#4924)", "pull_request_description": "Contributor\ntamuhey commented 11 days ago \u2022\nedited\nDescription\nFixes #4924.\nI found nlp.evaluate fails with empty doc and gold.\nThis seems to be unintended, because the error is somewhat confusing:\n        gold_deps = set()\n        gold_deps_per_dep = {}\n        gold_tags = set()\n>       gold_ents = set(tags_to_entities([annot[-1] for annot in gold.orig_annot]))\nE       TypeError: 'NoneType' object is not iterable\n\nspacy/scorer.py:239: TypeError\nI thought the following three ways to correct this:\nInforms the users that nlp.evaluate will not accept an empty gold, and fails.\nModify GoldParse to set gold.orig_annot = [] instead of None, if gold is empty.\nModify nlp.evaluate to properly handles empty gold\nI adopted the second way.\nIn the first way, users should write a boilerplate to check gold is empty.\nIn the third way, it is necessary to introduce multiple ifs, which complicates the code.\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-20T11:17:46Z", "files_changed": [["5", "spacy/gold.pyx"], ["16", "spacy/tests/regression/test_issue4924.py"]]}, "10": {"issue_url": "https://github.com/explosion/spaCy/issues/4921", "issue_id": "#4921", "issue_summary": "Inconsistent POS and Lemmas in Spanish", "issue_description": "lmorillas commented 14 days ago\nHow to reproduce the behaviour\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.13.0-46-generic-x86_64-with-glibc2.2.5\nPython version: 3.8.0\nSee this example with single phrases.\nimport spacy\nnlp_es = spacy.load(\"es_core_news_sm\")\n\ndef parse(f):\n    doc = nlp_es(f)\n    return [(x.pos_, x.lemma_) for x in doc]\n\nfrases = [\n    \"Como manzanas todos los d\u00edas\",  # como is a verb: comer\n    \"Yo comer\u00e9 manzanas todos los d\u00edas\",  # comer\u00e9 is a verb: comer\n    \"A veces como manzanas\", # como is a verb: comer\n    \"Yo como manzanas\" # como is a verb: comer\n]\nfor f in frases:\n    print(parse(f))\nAnd the result:\n[('SCONJ', 'Como'), ('NOUN', 'manzana'), ('DET', 'todo'), ('DET', 'lo'), ('NOUN', 'd\u00eda')] \n\n[('PRON', 'Yo'), ('VERB', 'comer'), ('NOUN', 'manzana'), ('DET', 'todo'), ('DET', 'lo'), ('NOUN', 'd\u00eda')] \n\n[('ADP', 'A'), ('NOUN', 'vez'), ('SCONJ', 'comer'), ('NOUN', 'manzana')] \n\n[('PRON', 'Yo'), ('SCONJ', 'comer'), ('NOUN', 'manzana')]\nWhere como is 'SCONJand lemmatized ascomonot ascomer`\nWhat is the best way to correct this kind of errors? Should we add exceptions? or is an easy way to train the model with new data? I can't find it in the docs.", "issue_status": "Closed", "issue_reporting_time": "2020-01-17T11:50:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "11": {"issue_url": "https://github.com/explosion/spaCy/issues/4915", "issue_id": "#4915", "issue_summary": "OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.", "issue_description": "LizMcQuillan commented 16 days ago \u2022\nedited\nI've gone through the troubleshooting steps in previous issues (like #4756 and #4577), but have been unable to resolve the error.\nI'm attempting to import and use version 2.2.3, which seems to be installed. Meaning, I installed it and previously everything worked fine. I haven't made any changes to my machine, just started a new script. and attempted to use:\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nparser = English()\nimport en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\nwhich yields\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n    return util.load_model(name, **overrides)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 169, in load_model\n    raise IOError(Errors.E050.format(name=name))\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nYour Environment\nspaCy version: 2.2.3\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.3\nMac OS\nI've also tried the following:\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nparser = English()\nfrom spacy.lang.en import en_core_web_sm\nWhich yields:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: cannot import name 'en_core_web_sm' from 'spacy.lang.en' (/usr/local/lib/python3.7/site-packages/spacy/lang/en/__init__.py)\n>>> nlp = spacy.load(\"en_core_web_sm\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n    return util.load_model(name, **overrides)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 169, in load_model\n    raise IOError(Errors.E050.format(name=name))\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nIf I run each line separately, the errors I get are:\n>>> from spacy.lang.en import en_core_web_sm\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: cannot import name 'en_core_web_sm' from 'spacy.lang.en' (/usr/local/lib/python3.7/site-packages/spacy/lang/en/__init__.py)\n>>> nlp = spacy.load(\"en_core_web_sm\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n    return util.load_model(name, **overrides)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 169, in load_model\n    raise IOError(Errors.E050.format(name=name))\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.", "issue_status": "Closed", "issue_reporting_time": "2020-01-15T15:40:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "12": {"issue_url": "https://github.com/explosion/spaCy/issues/4913", "issue_id": "#4913", "issue_summary": "Morphologizer/char_embed docs?", "issue_description": "buriy commented 16 days ago \u2022\nedited\nMorphologizer/char_embed docs? I'm eager to try it!\nWhich page or section is this issue related to?\nTraining? Architecture?", "issue_status": "Closed", "issue_reporting_time": "2020-01-15T12:03:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "13": {"issue_url": "https://github.com/explosion/spaCy/issues/4910", "issue_id": "#4910", "issue_summary": "Spacy langs does not work", "issue_description": "GuillemGSubies commented 17 days ago \u2022\nedited\nWhen using the next code:\nfrom spacy.lang.en import English\nEnglish()\nI get the following error:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n<ipython-input-8-a691f7ea7f90> in <module>\n      1 from spacy.lang.en import English\n----> 2 English()\n\n~/anaconda3/lib/python3.6/site-packages/spacy/language.py in __init__(self, vocab, make_doc, max_length, meta, **kwargs)\n    162         if vocab is True:\n    163             factory = self.Defaults.create_vocab\n--> 164             vocab = factory(self, **meta.get(\"vocab\", {}))\n    165             if vocab.vectors.name is None:\n    166                 vocab.vectors.name = meta.get(\"vectors\", {}).get(\"name\")\n\n~/anaconda3/lib/python3.6/site-packages/spacy/language.py in create_vocab(cls, nlp)\n     62     @classmethod\n     63     def create_vocab(cls, nlp=None):\n---> 64         lookups = cls.create_lookups(nlp)\n     65         lemmatizer = cls.create_lemmatizer(nlp, lookups=lookups)\n     66         lex_attr_getters = dict(cls.lex_attr_getters)\n\n~/anaconda3/lib/python3.6/site-packages/spacy/language.py in create_lookups(cls, nlp)\n     52         if LANG in cls.lex_attr_getters:\n     53             lang = cls.lex_attr_getters[LANG](None)\n---> 54             if lang in util.registry.lookups:\n     55                 filenames.update(util.registry.lookups.get(lang))\n     56         lookups = Lookups()\n\n~/anaconda3/lib/python3.6/site-packages/catalogue-0.0.8-py3.6.egg/catalogue.py in __contains__(self, name)\n     54         \"\"\"\n     55         namespace = tuple(list(self.namespace) + [name])\n---> 56         has_entry_point = self.entry_points and self.get_entry_point(name)\n     57         return has_entry_point or namespace in REGISTRY\n     58 \n\n~/anaconda3/lib/python3.6/site-packages/catalogue-0.0.8-py3.6.egg/catalogue.py in get_entry_point(self, name, default)\n    129         for entry_point in AVAILABLE_ENTRY_POINTS.get(self.entry_point_namespace, []):\n    130             if entry_point.name == name:\n--> 131                 return entry_point.load()\n    132         return default\n    133 \n\n~/anaconda3/lib/python3.6/site-packages/importlib_metadata/__init__.py in load(self)\n     92         \"\"\"\n     93         match = self.pattern.match(self.value)\n---> 94         module = import_module(match.group('module'))\n     95         attrs = filter(None, (match.group('attr') or '').split('.'))\n     96         return functools.reduce(getattr, attrs, module)\n\n~/anaconda3/lib/python3.6/importlib/__init__.py in import_module(name, package)\n    124                 break\n    125             level += 1\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\n    127 \n    128 \n\n~/anaconda3/lib/python3.6/importlib/_bootstrap.py in _gcd_import(name, package, level)\n\n~/anaconda3/lib/python3.6/importlib/_bootstrap.py in _find_and_load(name, import_)\n\n~/anaconda3/lib/python3.6/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\n\n~/anaconda3/lib/python3.6/importlib/_bootstrap.py in _load_unlocked(spec)\n\n~/anaconda3/lib/python3.6/importlib/_bootstrap.py in _load_backward_compatible(spec)\n\n~/anaconda3/lib/python3.6/site-packages/spacy_lookups_data-0.2.0-py3.6.egg/spacy_lookups_data/__init__.py in <module>\n     12 \n     13 \n---> 14 hr = {\"lemma_lookup\": get_file(\"hr_lemma_lookup.json\")}\n     15 pt = {\"lemma_lookup\": get_file(\"pt_lemma_lookup.json\")}\n     16 sv = {\n\n~/anaconda3/lib/python3.6/site-packages/spacy_lookups_data-0.2.0-py3.6.egg/spacy_lookups_data/__init__.py in get_file(filename)\n      9 \n     10 def get_file(filename):\n---> 11     return pkg_resources.resource_filename(__name__, os.path.join(\"data\", filename))\n     12 \n     13 \n\n~/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py in resource_filename(self, package_or_requirement, resource_name)\n   1143         \"\"\"Return a true filesystem path for specified resource\"\"\"\n   1144         return get_provider(package_or_requirement).get_resource_filename(\n-> 1145             self, resource_name\n   1146         )\n   1147 \n\n~/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py in get_resource_filename(self, manager, resource_name)\n   1733             for name in eagers:\n   1734                 self._extract_resource(manager, self._eager_to_zip(name))\n-> 1735         return self._extract_resource(manager, zip_path)\n   1736 \n   1737     @staticmethod\n\n~/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py in _extract_resource(self, manager, zip_path)\n   1754             return os.path.dirname(last)\n   1755 \n-> 1756         timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])\n   1757 \n   1758         if not WRITE_SUPPORT:\n\nKeyError: 'spacy_lookups_data/data/hr_lemma_lookup.json'\nI get the same error with every language I try.\nOperating System: Ubuntu 18\nPython Version Used: I tried with 3.6 and 3.7\nspaCy Version Used: 2.2.2 and 2.2.3 (with lookups)", "issue_status": "Closed", "issue_reporting_time": "2020-01-14T16:14:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "14": {"issue_url": "https://github.com/explosion/spaCy/issues/4908", "issue_id": "#4908", "issue_summary": "Possible copy-paste bug in NN parser", "issue_description": "jakepoz commented 17 days ago\nspaCy/spacy/syntax/nn_parser.pyx\nLines 59 to 60 in a3c43a1\n conv_depth = util.env_opt('conv_depth', cfg.get('conv_depth', 4)) \n conv_window = util.env_opt('conv_window', cfg.get('conv_depth', 1)) \nIs it possible there is a copy/paste mistake when reading the conv_window parameter, and it just reads the same value as conv_depth?", "issue_status": "Closed", "issue_reporting_time": "2020-01-14T00:31:20Z", "fixed_by": "#4909", "pull_request_summary": "bugfix typo conv_window", "pull_request_description": "Member\nsvlandeg commented 17 days ago\nFixes #4908\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-14T10:23:15Z", "files_changed": [["2", "spacy/syntax/nn_parser.pyx"]]}, "15": {"issue_url": "https://github.com/explosion/spaCy/issues/4905", "issue_id": "#4905", "issue_summary": "Dutch model 'nl_core_news_sm' never returns lemma", "issue_description": "jeroensilvis commented 18 days ago\nHow to reproduce the behaviour\nThe \".lemma_\" attribute always returns the text itself, never a lemma, using \"nl_core_news_sm\".\nYour Environment\nOperating System: Windows-10-10.0.17763-SP0\nPython Version Used: 3.7.3\nspaCy Version Used: 2.2.2\nEnvironment Information: most recent Anaconda distribution", "issue_status": "Closed", "issue_reporting_time": "2020-01-13T16:09:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "16": {"issue_url": "https://github.com/explosion/spaCy/issues/4902", "issue_id": "#4902", "issue_summary": "Error when trying to train NER component with CLI starting from a base model", "issue_description": "JohnGiorgi commented 19 days ago \u2022\nedited\nHow to reproduce the behaviour\nI am trying to train the NER component using the CLI, starting from a base model. I first convert my data to the spaCy json format, then use debug-data to make sure it is ready for training:\npython -m spacy debug-data en ./train.json ./test.json -b en_core_web_lg -p ner -V\n\n=========================== Data format validation ===========================\n\u2714 Corpus is loadable\n\n=============================== Training stats ===============================\nTraining pipeline: ner\nStarting with base model 'en_core_web_lg'\n50 training docs\n50 evaluation docs\n\u2714 No overlap between training and evaluation data\n\n============================== Vocab & Vectors ==============================\n\u2139 54993 total words in the data (6899 unique)\n10 most common words: ' ' (5087), '.' (3259), ',' (2269), ':' (1223), 'and'\n(1104), 'was' (1015), 'the' (999), 'of' (796), 'to' (737), 'patient' (653)\n\u2139 684831 vectors (684830 unique keys, 300 dimensions)\n\n========================== Named Entity Recognition ==========================\n\u2139 1 new label, 0 existing labels\n0 missing values (tokens with '-' label)\nNew: 'CUI' (6554)\n\u2714 Good amount of examples for all labels\n\u2714 Examples without occurrences available for all labels\n\u2714 No entities consisting of or starting/ending with whitespace\n\n================================== Summary ==================================\n\u2714 5 checks passed\nThen, I try to train it with the following command:\npython -m spacy train en ./cui train.json test.json -b en_core_web_lg -p ner -n 10\nBut training errors out before the first epoch. The traceback is:\nTraining pipeline: ['ner']\nStarting with base model 'en_core_web_lg'\nCounting training words (limit=0)\n\nItn  NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n---  ---------  ------  ------  ------  -------  -------\n\u2714 Saved model to output directory                                                                                                   \ncui/model-final\n\u280f Creating best model...\nTraceback (most recent call last):\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/site-packages/spacy/cli/train.py\", line 368, in train\n    losses=losses,\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/site-packages/spacy/language.py\", line 515, in update\n    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n  File \"nn_parser.pyx\", line 445, in spacy.syntax.nn_parser.Parser.update\n  File \"nn_parser.pyx\", line 547, in spacy.syntax.nn_parser.Parser._init_gold_batch\n  File \"ner.pyx\", line 107, in spacy.syntax.ner.BiluoPushDown.preprocess_gold\n  File \"ner.pyx\", line 165, in spacy.syntax.ner.BiluoPushDown.lookup_transition\nKeyError: \"[E022] Could not find a transition with the name 'B-CUI' in the NER model.\"\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/site-packages/spacy/__main__.py\", line 33, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/site-packages/plac_core.py\", line 367, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/site-packages/plac_core.py\", line 232, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/site-packages/spacy/cli/train.py\", line 497, in train\n    best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names)\n  File \"/Users/johngiorgi/miniconda3/envs/drspacy/lib/python3.7/site-packages/spacy/cli/train.py\", line 565, in _collate_best_model\n    path2str(best_component_src / component), path2str(best_dest / component)\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'str'\nAny help as to why this happens is appreciated!\nYour Environment\nspaCy version: 2.2.3\nPlatform: Darwin-19.2.0-x86_64-i386-64bit\nPython version: 3.7.6", "issue_status": "Closed", "issue_reporting_time": "2020-01-11T21:53:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "17": {"issue_url": "https://github.com/explosion/spaCy/issues/4901", "issue_id": "#4901", "issue_summary": "Lemma is not consistent", "issue_description": "zeeqy commented 20 days ago\nHow to reproduce the behaviour\n>>> doc = nlp('cities in england')\n>>> doc[0].lemma_\n>>> cities\n>>> doc = nlp('cities in ontario')\n>>> doc[0].lemma_\n>>> city\n>>> doc = nlp('cities in michigan')\n>>> doc[0].lemma_\n>>> city\n>>> doc = nlp('cities in china')\n>>> doc[0].lemma_\n>>> city\nV: England Prevails!!\nYour Environment\nOperating System: Linux\nPython Version Used: 3.7.4\nspaCy Version Used: spacy==2.2.3\nEnvironment Information: N/A", "issue_status": "Closed", "issue_reporting_time": "2020-01-11T01:14:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "18": {"issue_url": "https://github.com/explosion/spaCy/issues/4899", "issue_id": "#4899", "issue_summary": "index attribute for sentences", "issue_description": "jack-rory-staunton commented 20 days ago\nFeature description\nAlthough technically they are Spans (which do not have indices per se), it would be nice to somehow access the indices of a Doc's sentences:\ndef get_sentence_index(ent):\n    return [i for i, sent in enumerate(doc.sents)\n                 if sent.start_char <= ent.start_char \n                 and sent.end_char >= ent.end_char][0]\nperhaps by having the generator supply a tuple containing the index.", "issue_status": "Closed", "issue_reporting_time": "2020-01-10T21:13:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "19": {"issue_url": "https://github.com/explosion/spaCy/issues/4898", "issue_id": "#4898", "issue_summary": "token attribute to return containing ent", "issue_description": "jack-rory-staunton commented 20 days ago\nFeature description\nConverting things back and forth between tokens, spans and entities is often laborious. A function like\ndef get_ent_from_token(token):\n    return [ent for ent in doc.ents \n            if ent.start_char <= token.idx <= ent.end_char][0]\nsimply returns the entity of which the supplied token is a part. I submit that Token should have Token.ent as an attribute that would supply the entity's span if it exists or return [] is tok.ent_type == 0.", "issue_status": "Closed", "issue_reporting_time": "2020-01-10T21:02:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "20": {"issue_url": "https://github.com/explosion/spaCy/issues/4895", "issue_id": "#4895", "issue_summary": "Span.ent_id_ returns first token ent_id in span", "issue_description": "Contributor\nmr-bjerre commented 22 days ago\nIt seems that span.ent_id_ just gives you the first non empty value of [t.ent_id_ for t in span]. I'd expect span.ent_id_ = '' if there are multiple non unique ids in the span.\nIs this expected behavior?\nspaCy version: 2.2.3\nPlatform: Linux-5.0.0-37-generic-x86_64-with-glibc2.27\nPython version: 3.8.1", "issue_status": "Closed", "issue_reporting_time": "2020-01-09T13:40:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "21": {"issue_url": "https://github.com/explosion/spaCy/issues/4894", "issue_id": "#4894", "issue_summary": "Illegal Characters", "issue_description": "davidbren commented 23 days ago\nWe OCR'd a number of images, the OCR was fairly noisy so we ended up with some exotic characters. These characters were causing the NER to fail during training\nI suppose it was due to the fact that some of the noisey characters were not part of the english language character set\nnlp = spacy.blank('en')\nIn our initial approach we tried replacing all non word characters with spaces using some simple regex\nocrCleanedText=str(re.sub(r\"(?!/)\\W\",\"\",ocrCleanedText))\nHowever this causes an issue where we introduced spaces at the start & end of entities\nwhich causes the training to fail. We can work around this but it seems to be a wrong approach to process with.\nI am beginning to look at ASCII conversion type methods now but I would like to know what would you recommend as a gold standard approach ?\nAre there any helper method in spaCy that would allow us to identify any non English characters set ?\nYour Environment\nOperating System: RHEL 7\nPython Version Used: 3.7\nspaCy Version Used: 2.1.8\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2020-01-08T14:10:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "22": {"issue_url": "https://github.com/explosion/spaCy/issues/4893", "issue_id": "#4893", "issue_summary": "Why spacy is not installing?", "issue_description": "mcp111 commented 23 days ago\nGives following error\nMicrosoft Windows [Version 10.0.18362.535]\n(c) 2019 Microsoft Corporation. All rights reserved.\nC:\\Users\\admin>pip install spacy\nCollecting spacy\nUsing cached https://files.pythonhosted.org/packages/b7/f2/052bfe5861761599b5421916aba3eb0064d83145ff3072390ecdc5a836de/spacy-2.2.3.tar.gz\nInstalling build dependencies ... done\nGetting requirements to build wheel ... done\nInstalling backend dependencies ... error\nERROR: Command errored out with exit status 1:\ncommand: 'c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\python.exe' 'c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pip' install --ignore-installed --no-user --prefix 'C:\\Users\\admin\\AppData\\Local\\Temp\\pip-build-env-6sjxrq37\\normal' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'cython>=0.25' 'murmurhash<1.1.0,>=0.28.0' wheel 'thinc<7.4.0,>=7.3.0' 'cymem<2.1.0,>=2.0.2' 'preshed<3.1.0,>=3.0.2'\ncwd: None\nComplete output (111 lines):\nCollecting cython>=0.25\nDownloading https://files.pythonhosted.org/packages/22/03/510503cfbf20f62810a9548c9be13ab86181f00cca9a3a56717c4595d952/Cython-0.29.14-cp38-cp38-win32.whl (1.6MB)\nCollecting murmurhash<1.1.0,>=0.28.0\nUsing cached https://files.pythonhosted.org/packages/22/e9/411be1845f1ac07ae3bc40a4b19ba401819baed4fa63b4f5ef28b2300eb4/murmurhash-1.0.2.tar.gz\nCollecting wheel\nUsing cached https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\nCollecting thinc<7.4.0,>=7.3.0\nUsing cached https://files.pythonhosted.org/packages/d4/38/f79bb496ced36f8d69cdbdfe57a322205582ed9508bda5bd0227969d5a77/thinc-7.3.1.tar.gz\nERROR: Command errored out with exit status 1:\ncommand: 'c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\Users\\admin\\AppData\\Local\\Temp\\pip-install-8hmutbpz\\thinc\\setup.py'\"'\"'; file='\"'\"'C:\\Users\\admin\\AppData\\Local\\Temp\\pip-install-8hmutbpz\\thinc\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(file);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, file, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\admin\\AppData\\Local\\Temp\\pip-install-8hmutbpz\\thinc\\pip-egg-info'\ncwd: C:\\Users\\admin\\AppData\\Local\\Temp\\pip-install-8hmutbpz\\thinc\nComplete output (97 lines):\nProcessing numpy/random_bounded_integers.pxd.in\nProcessing numpy/random\\mtrand.pyx\nTraceback (most recent call last):\nFile \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\tools\\cythonize.py\", line 61, in process_pyx\nfrom Cython.Compiler.Version import version as cython_version\nModuleNotFoundError: No module named 'Cython'\n  During handling of the above exception, another exception occurred:\n\n  Traceback (most recent call last):\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\tools\\cythonize.py\", line 238, in <module>\n      main()\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\tools\\cythonize.py\", line 234, in main\n      find_process_files(root_dir)\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\tools\\cythonize.py\", line 225, in find_process_files\n      process(root_dir, fromfile, tofile, function, hash_db)\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\tools\\cythonize.py\", line 191, in process\n      processor_function(fromfile, tofile)\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\tools\\cythonize.py\", line 66, in process_pyx\n      raise OSError('Cython needs to be installed in Python as a module')\n  OSError: Cython needs to be installed in Python as a module\n  Running from numpy source directory.\n  C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\setup.py:461: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n    run_build = parse_setuppy_commands()\n  Traceback (most recent call last):\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 154, in save_modules\n      yield saved\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 195, in setup_context\n      yield\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 250, in run_setup\n      _execfile(setup_script, ns)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 45, in _execfile\n      exec(code, globals, locals)\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\setup.py\", line 488, in <module>\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\setup.py\", line 469, in setup_package\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\setup.py\", line 275, in generate_cython\n  RuntimeError: Running cythonize failed!\n\n  During handling of the above exception, another exception occurred:\n\n  Traceback (most recent call last):\n    File \"<string>\", line 1, in <module>\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\pip-install-8hmutbpz\\thinc\\setup.py\", line 263, in <module>\n      setup_package()\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\pip-install-8hmutbpz\\thinc\\setup.py\", line 201, in setup_package\n      setup(\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\__init__.py\", line 144, in setup\n      _install_setup_requires(attrs)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\__init__.py\", line 139, in _install_setup_requires\n      dist.fetch_build_eggs(dist.setup_requires)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\dist.py\", line 716, in fetch_build_eggs\n      resolved_dists = pkg_resources.working_set.resolve(\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pkg_resources\\__init__.py\", line 780, in resolve\n      dist = best[req.key] = env.best_match(\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pkg_resources\\__init__.py\", line 1065, in best_match\n      return self.obtain(req, installer)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pkg_resources\\__init__.py\", line 1077, in obtain\n      return installer(requirement)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\dist.py\", line 786, in fetch_build_egg\n      return cmd.easy_install(req)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\command\\easy_install.py\", line 679, in easy_install\n      return self.install_item(spec, dist.location, tmpdir, deps)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\command\\easy_install.py\", line 705, in install_item\n      dists = self.install_eggs(spec, download, tmpdir)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\command\\easy_install.py\", line 890, in install_eggs\n      return self.build_and_install(setup_script, setup_base)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\command\\easy_install.py\", line 1158, in build_and_install\n      self.run_setup(setup_script, setup_base, args)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\command\\easy_install.py\", line 1144, in run_setup\n      run_setup(setup_script, args)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 253, in run_setup\n      raise\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\contextlib.py\", line 131, in __exit__\n      self.gen.throw(type, value, traceback)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 195, in setup_context\n      yield\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\contextlib.py\", line 131, in __exit__\n      self.gen.throw(type, value, traceback)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 166, in save_modules\n      saved_exc.resume()\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 141, in resume\n      six.reraise(type, exc, self._tb)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\_vendor\\six.py\", line 685, in reraise\n      raise value.with_traceback(tb)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 154, in save_modules\n      yield saved\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 195, in setup_context\n      yield\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 250, in run_setup\n      _execfile(setup_script, ns)\n    File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\setuptools\\sandbox.py\", line 45, in _execfile\n      exec(code, globals, locals)\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\setup.py\", line 488, in <module>\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\setup.py\", line 469, in setup_package\n    File \"C:\\Users\\admin\\AppData\\Local\\Temp\\easy_install-rlk2rdhi\\numpy-1.18.1\\setup.py\", line 275, in generate_cython\n  RuntimeError: Running cythonize failed!\n  Cythonizing sources\n  ----------------------------------------\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\nERROR: Command errored out with exit status 1: 'c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\python.exe' 'c:\\users\\admin\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pip' install --ignore-installed --no-user --prefix 'C:\\Users\\admin\\AppData\\Local\\Temp\\pip-build-env-6sjxrq37\\normal' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'cython>=0.25' 'murmurhash<1.1.0,>=0.28.0' wheel 'thinc<7.4.0,>=7.3.0' 'cymem<2.1.0,>=2.0.2' 'preshed<3.1.0,>=3.0.2' Check the logs for full command output.\nC:\\Users\\admin>", "issue_status": "Closed", "issue_reporting_time": "2020-01-08T12:26:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "23": {"issue_url": "https://github.com/explosion/spaCy/issues/4888", "issue_id": "#4888", "issue_summary": "Calling nlp.evaluate over a large number dev documents cause the process to crash", "issue_description": "Contributor\nZhuoruLin commented 23 days ago \u2022\nedited\nI trained a custom textcat pipeline where the model is replaced by a custom PyTorch module (very simple CBOW model) wrapped by thinc wrapper. When I tried to evaluate the model using nlp.evaluate the process would crash.\nMy dev_docs consist of around 90000 documents (of over 6000 mutual exclusive classes), they are not preprocessed and therefore are in the base string type. I suspect whether in line 673-675 in language.py\n    docs = [\n        self.make_doc(doc) if isinstance(doc, basestring_) else doc for doc in docs\n    ]\nthe storages of these docs are creating the memory issue? (Although I don't think it is very likely to be the case).\nYour Environment\nspaCy version: 2.0.18\nPlatform: Linux-4.9.0-8-amd64-x86_64-with-debian-9.7\nPython version: 3.7.2", "issue_status": "Closed", "issue_reporting_time": "2020-01-07T17:58:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "24": {"issue_url": "https://github.com/explosion/spaCy/issues/4887", "issue_id": "#4887", "issue_summary": "Splitting document into sentences for Portuguese", "issue_description": "mananeau commented 24 days ago\nHi and thanks for this cool repo :)\nI'm trying to split a big Portuguese corpora into sentences and the split is not always clean. See for instance where sentences 48 and 49 should be one:\nHere is my code:\nAnything I'm doing wrong?\nYour Environment\nOperating System: Ubuntu Linux\nPython Version Used: 3.7.4\nspaCy Version Used: 2.2.3\nModel: pt_core_news_sm", "issue_status": "Closed", "issue_reporting_time": "2020-01-07T16:20:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "25": {"issue_url": "https://github.com/explosion/spaCy/issues/4885", "issue_id": "#4885", "issue_summary": "Strip Quotations from Document", "issue_description": "Contributor\nAlJohri commented 25 days ago \u2022\nedited\nI asked this question on StackOverflow more than a week ago so I wanted to to try my luck here: https://stackoverflow.com/questions/59504980/how-do-i-robustly-strip-quotations-from-news-articles-using-spacy\nI'm looking to essentially take this Github Issue a step further: #3196.\nMy goal is to remove or tag quotations from news articles, but I'm not sure how to robustly do so. I want to treat tokens inside quotes differently than tokens outside. For example, I would like to ignore entities found within quotations for my current use case.\nMy current implementation is brittle to unmatched quotes and also seems inefficient since its using the Matcher in a while loop for each document. One idea I had was ditching the Matcher which matches at a token level and start using something that allows me to take whitespace into account so I can match \"\\s. Would love any help.\nThanks!\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.6", "issue_status": "Closed", "issue_reporting_time": "2020-01-06T17:07:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "26": {"issue_url": "https://github.com/explosion/spaCy/issues/4880", "issue_id": "#4880", "issue_summary": "Creating custom French NER model", "issue_description": "CatarinaPC commented 25 days ago \u2022\nedited\nI want to train a French NER model using some data I have. I decided to follow this example that besides showing how to update a pre-trained model with new examples, also shows how to train a model from scratch by using a blank Language class (at least that's what I understood).\nI took this code and since I want to train a French model, the first thing I did was change the language ID in the spacy.blank() function to \"fr\". I ran the code and I got the following error:\nImportError: [E048] Can't import language fr from spacy.lang: cannot import name 'FrenchLemmatizer'\nI also tried importing the \"French\" class and I get the same kind of error: ImportError: cannot import name 'FrenchLemmatizer'\nHow to reproduce the behaviour\nTo get the first error, run the example code on here and change the language ID in the spacy.blank() function to \"fr\", like in the following code:\nfrom __future__ import unicode_literals, print_function\n\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\n\n\n# training data\nTRAIN_DATA = [\n    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n]\n\n\n@plac.annotations(\n    model=(\"Model name. Defaults to blank 'fr' model.\", \"option\", \"m\", str),\n    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n)\ndef main(model=None, output_dir=None, n_iter=100):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"fr\")  # create blank Language class\n        print(\"Created blank 'fr' model\")\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # reset and initialize the weights randomly \u2013 but only if we're\n        # training a new model\n        if model is None:\n            nlp.begin_training()\n        for itn in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n            print(\"Losses\", losses)\n\n    # test the trained model\n    for text, _ in TRAIN_DATA:\n        doc = nlp(text)\n        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        # test the saved model\n        print(\"Loading from\", output_dir)\n        nlp2 = spacy.load(output_dir)\n        for text, _ in TRAIN_DATA:\n            doc = nlp2(text)\n            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\n\nif __name__ == \"__main__\":\n    plac.call(main)\n\n    # Expected output:\n    # Entities [('Shaka Khan', 'PERSON')]\n    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\nThe result should be:\nTraceback (most recent call last):\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/util.py\", line 74, in get_lang_class\n    module = importlib.import_module(\".lang.%s\" % lang, \"spacy\")\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/lang/fr/__init__.py\", line 9, in <module>\n    from .lemmatizer import FrenchLemmatizer\nImportError: cannot import name 'FrenchLemmatizer'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/catarinapc/cl-extraction-models/eval/test_spacy.py\", line 91, in <module>\n    plac.call(main)\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/plac_core.py\", line 367, in call\n    cmd, result = parser.consume(arglist)\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/plac_core.py\", line 232, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/home/catarinapc/cl-extraction-models/eval/test_spacy.py\", line 28, in main\n    nlp = spacy.blank(\"fr\")  # create blank Language class\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/__init__.py\", line 34, in blank\n    LangClass = util.get_lang_class(name)\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/util.py\", line 76, in get_lang_class\n    raise ImportError(Errors.E048.format(lang=lang, err=err))\nImportError: [E048] Can't import language fr from spacy.lang: cannot import name 'FrenchLemmatizer'\nFor the second error simply run:\nfrom spacy.lang.fr import French\nThe result should be:\nTraceback (most recent call last):\n  File \"/home/catarinapc/cl-extraction-models/eval/test_spacy.py\", line 1, in <module>\n    from spacy.lang.fr import French\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/lang/fr/__init__.py\", line 9, in <module>\n    from .lemmatizer import FrenchLemmatizer\nImportError: cannot import name 'FrenchLemmatizer'\nYour Environment\nspaCy version 2.2.3\nLocation /home/catarinapc/.local/lib/python3.6/site-packages/spacy\nPlatform Linux-5.0.0-37-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version 3.6.8\nEdit\nI tried training the model using the command-line interface and also got the same error:\nI ran: python3 -m spacy train fr test train.json dev.json -p ner\nAnd the result was:\nTraining pipeline: ['ner']\nStarting with blank model 'fr'\nTraceback (most recent call last):\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/util.py\", line 74, in get_lang_class\n    module = importlib.import_module(\".lang.%s\" % lang, \"spacy\")\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/lang/fr/__init__.py\", line 9, in <module>\n    from .lemmatizer import FrenchLemmatizer\nImportError: cannot import name 'FrenchLemmatizer'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/__main__.py\", line 33, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/plac_core.py\", line 367, in call\n    cmd, result = parser.consume(arglist)\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/plac_core.py\", line 232, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 196, in train\n    lang_cls = util.get_lang_class(lang)\n  File \"/home/catarinapc/.local/lib/python3.6/site-packages/spacy/util.py\", line 76, in get_lang_class\n    raise ImportError(Errors.E048.format(lang=lang, err=err))\nImportError: [E048] Can't import language fr from spacy.lang: cannot import name 'FrenchLemmatizer'", "issue_status": "Closed", "issue_reporting_time": "2020-01-06T02:36:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "27": {"issue_url": "https://github.com/explosion/spaCy/issues/4879", "issue_id": "#4879", "issue_summary": "DataCamp course exercise problems", "issue_description": "guney1 commented 25 days ago \u2022\nedited\nHi,\nI am running datacamp exercises in my own system but I have some problems, datacamp uses 2.1.0 when I try to use this version in my system I get a bad escape error when loading the models, same thing happens when I use the latest version, problem disappears when I use version 2.0.16. Also when I use conda to install spacy, it auto. installs 2.0.16 rather than latest version.\nMy specific problem is when I do the following exercise in datacamp:\ndef get_wikipedia_url(span):\n# Get a Wikipedia URL if the span has one of the labels\nif span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\nentity_text = span.text.replace(' ', '_')\nreturn \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\nSpan.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\ndoc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\nfor ent in doc.ents:\n# Print the text and Wikipedia URL of the entity\nprint(ent.text, ent._.wikipedia_url)\nI get the named entities but when I run the same piece of code in my own system I get a empty tuple as output\nMy Environment\nOperating System: macOS High Sierra 10.13.6\nspaCy version: 2.0.16\nPlatform: Darwin-17.7.0-x86_64-i386-64bit\nPython version:** 3.7.0\nModels: en\nThanks a lot", "issue_status": "Closed", "issue_reporting_time": "2020-01-05T17:49:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "28": {"issue_url": "https://github.com/explosion/spaCy/issues/4876", "issue_id": "#4876", "issue_summary": "Shape mismatch when trying to follow provided pretrain_kb.py script", "issue_description": "JohnGiorgi commented 27 days ago \u2022\nedited\nHow to reproduce the behaviour\nSimply run the provided pretrain_kb.py script. Here is my exact setup:\nconda create -n spacy python=3.7 -y\nconda activate spacy\npip install spacy==2.2.3\npython -m spacy download en_core_web_sm \nwget https://raw.githubusercontent.com/explosion/spaCy/master/examples/training/pretrain_kb.py\npython pretrain_kb.py -m en_core_web_sm -n 1 -o ./tmp\nDuring the execution, a Shape mismatch is raised with the following traceback\nLoaded model 'en_core_web_sm'\nTraceback (most recent call last):\n  File \"pretrain_kb.py\", line 131, in <module>\n    plac.call(main)\n  File \"/Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/plac_core.py\", line 367, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/plac_core.py\", line 232, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"pretrain_kb.py\", line 77, in main\n    encoder.train(description_list=descriptions, to_print=True)\n  File \"/Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/bin/wiki_entity_linking/train_descriptions.py\", line 61, in train\n    processed, loss = self._train_model(description_list)\n  File \"/Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/bin/wiki_entity_linking/train_descriptions.py\", line 95, in _train_model\n    loss = self._update(batch)\n  File \"/Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/bin/wiki_entity_linking/train_descriptions.py\", line 143, in _update\n    np.asarray(vectors), drop=self.DROP\n  File \"/Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n  File \"/Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/thinc/check.py\", line 155, in checked_function\n    check(arg_id, fix_args, kwargs)\n  File \"/Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/thinc/check.py\", line 74, in has_shape_inner\n    raise ShapeMismatchError(arg.shape, shape_values, shape)\nthinc.exceptions.ShapeMismatchError: \n\n  Shape mismatch: input (2, 0) not compatible with [None, 300].\n\n  Traceback:\n  \u251c\u2500 train in /Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/bin/wiki_entity_linking/train_descriptions.py:61\n  \u251c\u2500\u2500\u2500 _train_model in /Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/bin/wiki_entity_linking/train_descriptions.py:95\n  \u2514\u2500\u2500\u2500\u2500\u2500 _update in /Users/johngiorgi/miniconda3/envs/spacy/lib/python3.7/site-packages/bin/wiki_entity_linking/train_descriptions.py:143\n         >>> np.asarray(vectors), drop=self.DROP\nYour Environment\nspaCy version: 2.2.3\nPlatform: Darwin-19.2.0-x86_64-i386-64bit\nPython version: 3.7.5", "issue_status": "Closed", "issue_reporting_time": "2020-01-04T16:53:36Z", "fixed_by": "#4881", "pull_request_summary": "Friendly error warning for NEL example script", "pull_request_description": "Member\nsvlandeg commented 25 days ago\nDescription\nThe NEL pretrain_kb script needs a model with pretrained vectors, and would crash otherwise. While this was documented in the plac arguments, I've also added a check to raise a more descriptive error.\nFixes #4876\nTypes of change\nenhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-14T00:51:14Z", "files_changed": [["1", "bin/wiki_entity_linking/README.md"], ["2", "bin/wiki_entity_linking/wikidata_pretrain_kb.py"], ["37", "examples/training/pretrain_kb.py"]]}, "29": {"issue_url": "https://github.com/explosion/spaCy/issues/4874", "issue_id": "#4874", "issue_summary": "Lemmatizer on \"best\" returns token \"good\"", "issue_description": "timforr commented 27 days ago\nThe 'en' lemmatizer lemmatizes the word \"best\" in to \"good\"\nHow to reproduce the behaviour:\nnlp = spacy.load('en')\ntest = nlp(\"best\")\ntest[0].lemma_\n'good'", "issue_status": "Closed", "issue_reporting_time": "2020-01-03T21:07:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "30": {"issue_url": "https://github.com/explosion/spaCy/issues/4873", "issue_id": "#4873", "issue_summary": "Removing a sentence in a document changes predicted entities throughout document", "issue_description": "erotavlas commented 27 days ago \u2022\nedited\nI'm encountering an issue where when I load my latest NER model and test it on a document, it produces a different result if I remove a single sentence from the beginning of the document\nI tested with two separate models and you can see an example below\nI added a sentence to the first document with a fictitious person entity\nThen removed the first sentence\nIs this normal?\nI find it strange that such a short sentence would have such a profound effect on the predictions\nModels were trained using spacy 2.1.8\nwith custom vocabulary using word2vec using the following parameters\nfrom gensim.models import Word2Vec\nword2vec = Word2Vec(all_words, min_count=2, window=5, size=200, sg=0, workers=5)", "issue_status": "Closed", "issue_reporting_time": "2020-01-03T18:30:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "31": {"issue_url": "https://github.com/explosion/spaCy/issues/4872", "issue_id": "#4872", "issue_summary": "Loading one model after another results in incorrect predictions for the second model", "issue_description": "erotavlas commented 28 days ago \u2022\nedited\n[spacy version 2.1.8]\nI think I'm encountering an issue relating to this\n#3853 (comment)\nWhat I have in my script is this piece of code that loads a new NER model on demand. the script starts by first loading an initial model\ntry:\n dataPath = Path(input())\n modelPath = Path(input())\n\n set_data_path(dataPath)\n nlp_model = spacy.load(modelPath, disable=[\"parser\", \"tagger\"])\n print(\"ready\")\nexcept BaseException as error:\n print(\"Error 1:\", sys.exc_info()[0], error, dataPath, modelPath)\nThen I enter a loop where I can continually submit text for evaluation and get back results. At any time I can change the model\n elif a == 'loadmodel':\n  try:\n   modelPath = input()\n   nlp_model = spacy.load(modelPath, disable=[\"parser\", \"tagger\"])\n   print(\"ready\")\n  except BaseException as error:\n   print(\"Error 2:\", sys.exc_info()[0], error,  modelPath)\nI've been comparing models built using different training data as well using different word vectors and when I start with an old model and load a new model, the predictions are different than those I would get if I started with the new model alone (no reloading)\nI think something is being held in memory, perhaps the word vectors and it is affecting the predictions of the model that has been loaded.\nExample\nGreen highlighted text is Date, and Purple highlight is Organization\n1 - Start script\n2 - Load the new model (spacy.load called only once) - these the expected results for the new model\n1 - Start script\n2 - Load old model and don't change it yet (spacy.load called first time)\n3 - Load new model after the old one (spacy.load called a second time)\nIs there any way to solve this?", "issue_status": "Closed", "issue_reporting_time": "2020-01-03T16:13:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "32": {"issue_url": "https://github.com/explosion/spaCy/issues/4870", "issue_id": "#4870", "issue_summary": "Entity Linking : Empty Kb being generated while generating KB with wikidata", "issue_description": "akhilnegi commented 28 days ago\nHow to reproduce the behavior\npython wikidata_pretrain_kb.py 'latest-all.json.bz2' 'enwiki-latest-pages-articles-multistream.xml.bz2' './output' 'en_core_web_lg'\nYour Environment\nspaCy version 2.2.1\nPlatform Windows-10-10.0.18362-SP0\nPython version 3.6.9\nExpected behaviour\nThis should generate a knowledge base using Wikidata.\nWhat is happening\nThe step ran for 18 hours and generated an ~empty KB file, below is how the output directory looks:\n\u2502 entity_alias.csv 206,414 KB\n\u2502 entity_defs.csv 172,606 KB\n\u2502 entity_descriptions.csv 153,340 KB\n\u2502 entity_freq.csv 1 KB\n\u2502 gold_entities.jsonl 0 KB\n\u2502 kb 1 KB\n\u2502 prior_prob.csv 1 KB\n\u2514\u2500\u2500\u2500nlp_kb\n\u2502 meta.json 2 KB\n\u2502 tokenizer 68 KB\n\u251c\u2500\u2500\u2500ner\n\u2502 cfg 1 KB\n\u2502 model 4147 KB\n\u2502 moves 2 KB\n\u251c\u2500\u2500\u2500parser\n\u2502 cfg 1KB\n\u2502 model 4253 KB\n\u2502 moves 2 KB\n\u251c\u2500\u2500\u2500tagger\n\u2502 cfg 1 KB\n\u2502 model 3856 KB\n\u2502 tag_map 2 KB\n\u2514\u2500\u2500\u2500vocab\nkey2row 9235 KB\nlexemes.bin 125648 KB\nlookups.bin 1658 KB\nstrings.json 23573 KB\nvectors 802537 KB", "issue_status": "Closed", "issue_reporting_time": "2020-01-03T08:11:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "33": {"issue_url": "https://github.com/explosion/spaCy/issues/4869", "issue_id": "#4869", "issue_summary": "Underscores are tagged as NOUN or PROPN", "issue_description": "james-daily commented 28 days ago \u2022\nedited\nHow to reproduce the behaviour\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This _ is missing _.\")\nprint([(t, t.pos_) for t in doc])\n\n# outputs [(This, 'DET'), (_, 'PROPN'), (is, 'AUX'), (missing, 'VERB'), (_, 'PROPN'), (., 'PUNCT')]\nWith en_core_web_lg you get the same result, just with NOUN instead of PROPN. The result is the same regardless of the part of speech of the word or words it's replacing in the sentence. It also tags strings of underscores as several proper nouns or nouns in a row (e.g. \"The _______ is on _____ table.\")\nYour Environment\nspaCy version: 2.2.3\nPlatform: Darwin-17.7.0-x86_64-i386-64bit\nPython version: 3.7.2\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2020-01-03T03:54:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "34": {"issue_url": "https://github.com/explosion/spaCy/issues/4865", "issue_id": "#4865", "issue_summary": "Training the model to NOT recognize certain NE", "issue_description": "Erin59 commented 28 days ago\nHi! Please forgive me if this question was already raised somewhere, I tried searching first but couldn't formulate it properly for Google. Also, sorry if this is not formatted correctly.\nMy question is - I know that you can train the model to recognize new named entities, however, my problem is the opposite. Is there any way to tell the model to \"forget\" certain entities that are not recognized correctly and not to recognize them anymore? For example, I was going through some law voting transcripts, there are words \"Aye\" and \"No\", and \"Aye\" gets recognized as a person, even when it's lowcase. Is there any better solution for this apart from just building a stop-word list for such cases?", "issue_status": "Closed", "issue_reporting_time": "2020-01-02T18:02:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "35": {"issue_url": "https://github.com/explosion/spaCy/issues/4863", "issue_id": "#4863", "issue_summary": "Multithread operations: thinc.exceptions.UndefinedOperatorError", "issue_description": "TimurNurlygayanov commented 29 days ago\nHow to reproduce the behavior\nRun several threads with the following code:\nimport spacy\nfrom joblib import Parallel, delayed\n\nsentences = ['test', 'test', 'test', 'test']\n\ndef test(sentence):\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n    doc = nlp(sentence)\n\nParallel(n_jobs=10, timeout=300, backend='threading') \\\n        (delayed(test)(s) for s in sentences))\nResult:\nTraceback (most recent call last):\n  File \"get_films_by_description.py\", line 76, in <module>\n    for i, task in enumerate(keys))\n  File \"/usr/local/lib/python3.7/site-packages/joblib/parallel.py\", line 934, in __call__\n    self.retrieve()\n  File \"/usr/local/lib/python3.7/site-packages/joblib/parallel.py\", line 833, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n    raise self._value\n  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"get_films_by_description.py\", line 56, in get_text\n    text = prepare_text(text)\n  File \"get_films_by_description.py\", line 34, in prepare_text\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n  File \"/usr/local/lib/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n    return util.load_model(name, **overrides)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 162, in load_model\n    return load_model_from_link(name, **overrides)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 179, in load_model_from_link\n    return cls.load(**overrides)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/data/en/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 228, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 211, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/language.py\", line 941, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 654, in from_disk\n    reader(path / key)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/language.py\", line 936, in <lambda>\n    p, exclude=[\"vocab\"]\n  File \"pipes.pyx\", line 661, in spacy.pipeline.pipes.Tagger.from_disk\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 654, in from_disk\n    reader(path / key)\n  File \"pipes.pyx\", line 640, in spacy.pipeline.pipes.Tagger.from_disk.load_model\n  File \"pipes.pyx\", line 548, in spacy.pipeline.pipes.Tagger.Model\n  File \"/usr/local/lib/python3.7/site-packages/spacy/_ml.py\", line 586, in build_tagger_model\n    pretrained_vectors=pretrained_vectors,\n  File \"/usr/local/lib/python3.7/site-packages/spacy/_ml.py\", line 323, in Tok2Vec\n    return _legacy_tok2vec.Tok2Vec(width, embed_size, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/spacy/ml/_legacy_tok2vec.py\", line 59, in Tok2Vec\n    (norm | prefix | suffix | shape)\n  File \"/usr/local/lib/python3.7/site-packages/thinc/check.py\", line 134, in checker\n    raise UndefinedOperatorError(op, instance, args[0], instance._operators)\nthinc.exceptions.UndefinedOperatorError: \n\n  Undefined operator: |\n  Called by (<thinc.neural._classes.function_layer.FunctionLayer object at 0x12d437bd0>, <thinc.neural._classes.hash_embed.HashEmbed object at 0x12d437e50>)\n  Available: \n\n  Traceback:\n  \u251c\u2500 from_disk in /usr/local/lib/python3.7/site-packages/spacy/util.py:654\n  \u251c\u2500\u2500\u2500 build_tagger_model in /usr/local/lib/python3.7/site-packages/spacy/_ml.py:586\n  \u2514\u2500\u2500\u2500\u2500\u2500 Tok2Vec in /usr/local/lib/python3.7/site-packages/spacy/_ml.py:323\n         >>> return _legacy_tok2vec.Tok2Vec(width, embed_size, **kwargs)\nEnvironment\nOperating System: MacOS\nPython Version Used: 3.7.4\nspaCy Version Used: spacy==2.2.3\n python3 -m spacy info --markdown\n\n## Info about spaCy\n\n* **spaCy version:** 2.2.3\n* **Platform:** Darwin-18.7.0-x86_64-i386-64bit\n* **Python version:** 3.7.4\n* **Models:** en", "issue_status": "Closed", "issue_reporting_time": "2020-01-02T07:53:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "36": {"issue_url": "https://github.com/explosion/spaCy/issues/4862", "issue_id": "#4862", "issue_summary": "Tokenization of URLs is different between English language model 2.2.0 and 2.2.5", "issue_description": "Contributor\nclippered commented 29 days ago\nHow to reproduce the behaviour\nHi,\nThere was a change in tokenizing URLs between sPacy language model en_core_web_sm 2.2.0 and 2.2.5\nThis code:\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp('the url is https://www.example.com.ai/foo/bar/)\nfor t in doc:\n  print(t.lower_)\nUsing en_core_web_sm 2.2.0, the output is:\nthe\nurl\nis\nhttps://www.example.com.ai/foo/bar/\nUsing en_core_web_sm 2.2.5, the output is:\nthe\nurl\nis\nhttps://www.example.com.ai\n/\nfoo\n/\nbar/\nBut the weird thing is when using just https://www.example.com/foo/bar/ without .ai in domain, the output is the same for both model versions\nthe\nurl\nis\nhttps://www.example.com/foo/bar/\nAny ideas? Thanks.\nYour Environment\nOperating System: macOS Catalina 10.15.2\nPython Version Used: 3.7.6\nspaCy Version Used: 2.2.3\nEnvironment Information: Language model en_core_web_sm 2.2.0 and 2.2.5\n1", "issue_status": "Closed", "issue_reporting_time": "2020-01-02T00:40:55Z", "fixed_by": "#4882", "pull_request_summary": "Fix and improve URL pattern", "pull_request_description": "Collaborator\nadrianeboyd commented 25 days ago \u2022\nedited\nDescription\nmatch domains longer than hostname.domain.tld like www.foo.co.uk\nexpand allowed characters in domain names while only matching lowercase TLDs so that \"this.That\" isn't matched as a URL and can be split on the period as an infix (relevant for at least English, German, and Tatar)\nFixes #4862.\nTypes of change\nBugfix and enhancement.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-06T13:58:31Z", "files_changed": [["27", "spacy/lang/tokenizer_exceptions.py"], ["18", "spacy/tests/tokenizer/test_urls.py"]]}, "37": {"issue_url": "https://github.com/explosion/spaCy/issues/4861", "issue_id": "#4861", "issue_summary": "Advanced NLP with spaCy: Chapter #4", "issue_description": "nick250386 commented on 1 Jan \u2022\nedited\nI am a newbie trying to learn NLP and started with this framework.\nWhile going through the wonderful interactive course you have provided here, I ran into an error and am unable to understand the issue from the error message reported.\nI did go through the below references before raising this issue:\nRef#1\nRef#2\nRef#3\nSECTION:\nChapter 4: Training a neural network model\n----> The training loop\n------------>Example loop\nBelow is my code:\nimport json \nfrom spacy.matcher import Matcher\n\nfrom spacy.lang.en import English\n\nwith open(\"iphone.json\") as f:\n    TEXTS = json.loads(f.read())\n\nnlp = English()\nmatcher = Matcher(nlp.vocab)\n\n#Two tokens whose lowercase forms match 'iphone' and 'x'\npattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n\n#Token whose lowercase form matches 'iphone' and an optional digit\npattern2 = [{'LOWER': 'iphone'}, {'IS_DIGIT': True, 'OP': '?'}]\n\n#Add patterns to the matcher\nmatcher.add(\"GADGET\", None, pattern1, pattern2)\n\nTRAINING_DATA = []\n\n#Create a Doc object for each text in TEXTS\nfor doc in nlp.pipe(TEXTS):\n    #Match on the doc and create a list of matched spans\n    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n    #Get (start character, end character, label) tuples of matches\n    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n    #Format the matches as a (doc.text, entities) tuple\n    training_example = (doc.text, {\"entities\": entities})\n    #Append the example to the training data\n    TRAINING_DATA.append(training_example)\n\nimport spacy\n\nprint('Training data:\\n', TRAINING_DATA)\n#*****BELOW CODE NOT WORKING\n##Loop for 10 iterations\nfor i in range(10):\n    #Shuffle the training data\n    random.shuffle(TRAINING_DATA)\n    #Create batches and iterate over them\n    for batch in spacy.util.minibatch(TRAINING_DATA):\n        #Split the batch in texts and annotations\n        texts = [text for text, annotation in batch]\n        annotations = [annotation for text, annotation in batch]\n        #Update the model\n        nlp.update(texts, annotations)\n\n#Save the model\nnlp.to_disk('.')>\nOUTPUT (with error stack):\nTraining data:\n [('The iPhone 8 reviews are here', {'entities': [(4, 10, 'GADGET'), (4, 12, 'GADGET')]}), \n('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]}), \n('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]}), \n('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]}), \n('I need a new phone! Any tips?', {'entities': []}), \n('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})]\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-27-2bc6b845db70> in <module>\n     35         annotations = [annotation for text, annotation in batch]\n     36         # Update the model\n---> 37         nlp.update(texts, annotations)\n     38 \n     39 # Save the model\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\language.py in update(self, docs, golds, drop, sgd, losses, component_cfg)\n    494             sgd = self._optimizer\n    495         # Allow dict of args to GoldParse, instead of GoldParse objects.\n--> 496         docs, golds = self._format_docs_and_golds(docs, golds)\n    497         grads = {}\n    498 \n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\language.py in _format_docs_and_golds(self, docs, golds)\n    466                     err = Errors.E151.format(unexp=unexpected, exp=expected_keys)\n    467                     raise ValueError(err)\n--> 468                 gold = GoldParse(doc, **gold)\n    469             doc_objs.append(doc)\n    470             gold_objs.append(gold)\n\ngold.pyx in spacy.gold.GoldParse.__init__()\n\ngold.pyx in spacy.gold.biluo_tags_from_offsets()\n\nValueError: [E103] Trying to set conflicting doc.ents: '(20, 28, 'GADGET')' and '(20, 26, 'GADGET')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.\nI am trying to understand the issue and what went wrong here. If this is not the right forum to ask this, then please feel free to let me know so that I can take it to relevant forum for requested information.", "issue_status": "Closed", "issue_reporting_time": "2020-01-01T13:40:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "38": {"issue_url": "https://github.com/explosion/spaCy/issues/4860", "issue_id": "#4860", "issue_summary": "Noun Phrase Matcher implementation", "issue_description": "naveenjafer commented on 1 Jan \u2022\nedited\nFeature description\nNoun Phrase Matcher as a part of the Doc object.\nCould the feature be a custom component or spaCy plugin?\nSource code change?\nI had come across a previous enhancement request for Verb phrase matcher request in SpaCy. While tinkering with the existing noun phrase matcher, I had come across a few sources that show how noun phrase matching is generally done(http://www.nltk.org/book_1ed/ch07.html#ref-chunkex-grammar). While the implementation in NLTK and few other resources I found is a fairly simple grammar rule such as DT?JJ*NN. I could see that the implementation in spacy was different. In fact the simple example of \"Autonomous cars shift insurance liability toward manufacturers\" gives three noun phrases according to the spacy implementation -\nAutonomous cars(ADJ NOUN)\ninsurance liability(NOUN NOUN)\nmanufacturers(NOUN)\nIf we go by the grammar rule as defined in NLTK, the \"insurance liability\" NP will not be captured. Therefore, I took the liberty to modify the regex to DT?JJ*{NN PROPN PRON}* (I am not sure if this is incorrect). When I wrote down the implementation for this in spaCy, it consistently performs ~30 percent faster than the existing implementation for noun phrase matcher and passes all the test cases as a part of the \"test_parser.py\".\nIs the regex based method less reliable than the subtree based method used in spaCy? Additionally, both of these happen to be rule based noun phrase matcher, does someone have any idea about how one would go about implementing a phrase matcher that can be trained on data? The only piece of I came across was \"textblob.np_extractors.ConllExtractor, which uses the CoNLL 2000 corpus to train a tagger.\"\nThe code I had put down is\ndef noun_chunks_2(obj):\n    doc = obj.doc  # Ensure works on both Doc and Span.\n    np_label = doc.vocab.strings.add(\"NP\")\n    lastValid = -1\n    for i, word in enumerate(obj):\n        start = 0\n        end = 0\n        validNP = False\n        if i > lastValid:\n            if word.pos == DET:\n                start = i\n                cur = i\n                while cur+1 < len(obj) and obj[cur+1].pos == ADJ:\n                    cur = cur + 1\n                while cur+1 < len(obj) and obj[cur+1].pos in (NOUN,PROPN,PRON):\n                    cur = cur + 1\n                    end = cur\n                    validNP = True\n                if validNP:\n                    lastValid = end\n                    yield start,end+1,np_label\n            elif word.pos == ADJ:\n                validNP = False\n                start = i\n                cur = i\n                while cur+1 < len(obj) and obj[cur+1].pos == ADJ:\n                    cur = cur + 1\n                while cur+1 < len(obj) and obj[cur+1].pos in (NOUN,PROPN,PRON):\n                    cur = cur + 1\n                    end = cur\n                    validNP = True\n                if validNP:\n                    lastValid = end\n                    yield start,end+1,np_label\n            elif word.pos in (NOUN,PROPN,PRON):\n                start = i\n                cur = i\n                end = start\n                while cur+1 < len(obj) and obj[cur+1].pos == NOUN:\n                    cur = cur + 1\n                    end = cur\n                yield start,end+1,np_label", "issue_status": "Closed", "issue_reporting_time": "2020-01-01T09:12:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "39": {"issue_url": "https://github.com/explosion/spaCy/issues/4859", "issue_id": "#4859", "issue_summary": "Spacy Entity Linking model for NER", "issue_description": "zainbnv commented on 1 Jan \u2022\nedited\nI wanted to use the spacy entity linking model with the version spacy==2.0.11. I just want the NER from this model, not the linking. I am getting this error\nKeyError: \"[E002] Can't find factory for 'entity_linker'. This usually happens when spaCy calls nlp.create_pipe with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to Language.factories['entity_linker'] or remove it from the model meta and add it via nlp.add_pipe instead.\"\nAlthough I have used this entity linking model on a newer version of spacy but my deployment server is having some prodigy based model trained on older version.Is there a way I can use this with previous version?\nYour Environment\nInfo about spaCy\nspaCy version: 2.0.18\nPlatform: Windows-10-10.0.17134-SP0\nPython version: 3.6.5", "issue_status": "Closed", "issue_reporting_time": "2020-01-01T06:08:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "40": {"issue_url": "https://github.com/explosion/spaCy/issues/4856", "issue_id": "#4856", "issue_summary": "Tokens aren't added to the vocab in the same way in models with and without vectors", "issue_description": "Collaborator\nadrianeboyd commented on 31 Dec 2019\nHow to reproduce the behaviour\nWhen a model has vectors (?) previously unseen tokens are not added to the vocab in the same way. They are added to the StringStore but not the vocab. I'm not sure this has any problematic side effects in typical usage, but I suspect it's a bug that it's inconsistent. The results with the small model are what I expected to happen for all three.\nimport spacy\n\ndef test_process_unseen_vocab(model, token):\n    nlp = spacy.load(model)\n    print(model, \"\\n==============\")\n    print(\"vocab length:\", len(nlp.vocab))\n    print(token, \"in vocab:\", token in nlp.vocab)\n    print(\"processing text...\", nlp(token))\n    print(\"vocab length:\", len(nlp.vocab))\n    print(token, \"in vocab:\", token in nlp.vocab)\n    print(token, \"in vocab.strings:\", token in nlp.vocab.strings)\n    print()\n    \nfor model in [\"en_core_web_sm\", \"en_core_web_md\", \"en_core_web_lg\"]:\n    test_process_unseen_vocab(model, \"asdfasdfasdf\")\nOutput:\nen_core_web_sm \n==============\nvocab length: 478\nasdfasdfasdf in vocab: False\nprocessing text... asdfasdfasdf\nvocab length: 479\nasdfasdfasdf in vocab: True\nasdfasdfasdf in vocab.strings: True\n\nen_core_web_md \n==============\nvocab length: 1340242\nasdfasdfasdf in vocab: False\nprocessing text... asdfasdfasdf\nvocab length: 1340242\nasdfasdfasdf in vocab: False\nasdfasdfasdf in vocab.strings: True\n\nen_core_web_lg \n==============\nvocab length: 1340242\nasdfasdfasdf in vocab: False\nprocessing text... asdfasdfasdf\nvocab length: 1340242\nasdfasdfasdf in vocab: False\nasdfasdfasdf in vocab.strings: True\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-5.3.0-0.bpo.2-amd64-x86_64-with-debian-10.2\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-12-31T10:09:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "41": {"issue_url": "https://github.com/explosion/spaCy/issues/4855", "issue_id": "#4855", "issue_summary": "EntityRuler does not match an entity for pancreas", "issue_description": "ewaldatsensentia commented on 31 Dec 2019\nHow to reproduce the behaviour\nimport spacy\nfrom spacy import displacy\nfrom spacy.pipeline import EntityRuler\nnlp = spacy.load('en_core_web_md')\nruler = EntityRuler(nlp, validate=True, overwrite_ents=True)\npatterns = [{\"label\":\"ORGAN\", \"pattern\":[{\"LEMMA\":\"heart\"}]},\n{\"label\":\"ORGAN\", \"pattern\":[{\"LEMMA\":\"pancreas\"}]},\n{\"label\":\"ORGAN\", \"pattern\":[{\"LEMMA\":\"stomach\"}]}]\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler, after=\"ner\")\ntext = \"This affects the heart, stomach, and pancreas only.\"\ndoc = nlp(text)\ndisplacy.render(doc, style=\"ent\", jupyter=True)\nThis affects the heart ORGAN , stomach ORGAN , and pancreas only.\nIt doesn't find pancreas as an ORGAN.\nOddly, if you remove the last comma, it works. If you remove the \"only\" it works. If you interchange \"pancreas\" and \"stomach\" it works. That is:\ntext = \"This affects the heart, stomach, and pancreas.\"\ndoc = nlp(text)\ndisplacy.render(doc, style=\"ent\", jupyter=True)\nThis affects the heart ORGAN , stomach ORGAN , and pancreas ORGAN .\nThe lemma of the token in the working phrases for \"pancreas\" is \"pancreas\" and for the bug it is \"pancrea\". That isn't a real word.\nYour Environment\nspaCy version: 2.2.1\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-12-31T01:49:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "42": {"issue_url": "https://github.com/explosion/spaCy/issues/4850", "issue_id": "#4850", "issue_summary": "spaCy CLI train fails after using pretrain with custom hyperparameter", "issue_description": "kormilitzin commented on 30 Dec 2019\nHow to reproduce the behaviour\nBeing inspired by: https://explosion.ai/blog/sense2vec-reloaded example with custom hyper parameters with spacy pretrain, I wanted to train a larger (than the default) model, namely:\n$ python -m spacy pretrain ./data/data_pretrain.jsonl en_vectors_web_lg ./tokens_expr -lstm 3 -uv -er 5000 -sa 4 -cw 128 -cd 8 -i 1\nall finished nicely. However, when I wanted to use the pertained weights with spaCy NER model:\n$ token_vector_width=128 embed_size=5000 hidden_width=128 python -m spacy train en ./med7_128_5000_v00_shuffle ./data/train_test/train/_med7_train_shuffle.json ./data/train_test/test/_med7_test_shuffle.json -t2v ./tokens_expr/model0.bin -v en_vectors_web_lg -p ner -n 200 -ne 5 -g 0\nIt crashed with:\nTraining pipeline: ['ner']\nStarting with blank model 'en'\nLoading vector from model 'en_vectors_web_lg'\nCounting training words (limit=0)\nTraceback (most recent call last):\nFile \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\nexec(code, run_globals)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/main.py\", line 33, in\nplac.call(commands[command], sys.argv[1:])\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py\", line 367, in call\ncmd, result = parser.consume(arglist)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py\", line 232, in consume\nreturn cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/cli/train.py\", line 244, in train\ncomponents = _load_pretrained_tok2vec(nlp, init_tok2vec)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/cli/train.py\", line 551, in _load_pretrained_tok2vec\ncomponent.tok2vec.from_bytes(weights_data)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/thinc/neural/_classes/model.py\", line 367, in from_bytes\nfor dim, value in weights[i][b\"dims\"].items():\nTypeError: byte indices must be integers or slices, not bytes\nIs there any workaround to introduce a larger model for spaCy NER? Thanks.\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.15.0-70-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-12-30T00:41:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "43": {"issue_url": "https://github.com/explosion/spaCy/issues/4849", "issue_id": "#4849", "issue_summary": "entity id is not retained when using multiple processes", "issue_description": "Contributor\nAlJohri commented on 29 Dec 2019\nThe ent_id and ent_id_ is not retained when using multiple processes during nlp.pipe. Presumably it is not getting serialized properly.\nHow to reproduce the behaviour\nThis example should be fully reproducible. The output should look like this:\n-----------------------\nUSING 1 PROCESS\n-----------------------\nJoe Biden 61 70 PERSON 12900564667790333946 joe-biden\nBernie Sanders 500 514 PERSON 17935300311999639713 bernie-sanders\n-----------------------\nUSING >1 PROCESS\n-----------------------\nCode:\nimport spacy\nfrom spacy.pipeline import EntityRuler\n\nnlp = spacy.load('en')\n\nruler = EntityRuler(\n    nlp, patterns=[\n        {\"label\": \"PERSON\", \"pattern\": 'joe biden', \"id\": 'joe-biden'},\n        {\"label\": \"PERSON\", \"pattern\": 'bernie sanders', \"id\": 'bernie-sanders'},\n    ],\n    phrase_matcher_attr=\"LOWER\"\n)\n\nnlp.add_pipe(ruler, before=\"ner\")\n\ntext = \"\"\"\nThe left is starting to take aim at Democratic front-runner Joe Biden.\nAt a conference this week, liberal activists repeatedly booed when told that Mr. Biden wanted to find \"middle ground\" on climate policy. When an audience member shouted \"No middle ground!\" Rep. Alexandria Ocasio-Cortez, D-N.Y., replied, \"No middle ground is right!\" and declared: \"I will be damned if the same politicians who refused to act come back today and say we need a middle-of-the-road approach to save our lives.\" Sen. Bernie Sanders, I-Vt., joined in her criticism: \"There is no 'middle ground' when it comes to climate policy.\"\nThe left's issues with the former vice president go far beyond his position on climate policy. To the neo-socialists now driving the debate in the Democratic primary campaign, Mr. Biden's entire approach to politics - reaching across the aisle and forging compromise built on consensus - is anathema.\nMr. Biden's supposed heresy is that he believes in working with Republicans. He says on the stump that Donald Trump is an \"aberration\" and predicts that if the president is defeated, Republicans will work toward bipartisan reform, which Mr. Biden insists is the only way to get anything worthwhile done. \"This nation cannot function without generating consensus,\" he said in New Hampshire this week.\nWell, generating consensus is not what the left wants. It is not simply opposed to Mr. Trump. Many liberals believe, as Ms. Ocasio-Cortez has put it, that \"capitalism is irredeemable.\" So for many Democrats, the Obama-Biden approach to governing is now considered too moderate. On climate, they don't want the government to simply invest in green energy, like President Barack Obama did. They want to spend tens of trillions of dollars to replace every vehicle that uses a combustion engine, bring high-speed rail to every corner of the country, upgrade or replace every building in the United States and eliminate all fossil-fuel energy.\nOn health care, they no longer make a pretense of promising voters they can keep their health plans, like Mr. Obama did. They openly advocate abolishing private insurance altogether. Mr. Biden's support for a \"public option\" that would give Americans a choice of buying into a Medicare-like health plan is seen on the left as capitulation. There will be no choices in the brave new world of democratic socialism. We will have government-run health care for all, whether we want it or not.\nOf course, Mr. Biden is no moderate. He is an old-fashioned, liberal Democrat. But to the Sanders and Ocasio-Cortez wing of the party, that makes him too far to the right - and too willing to compromise with the far-right. I saw Mr. Biden's willingness to do so up close when I worked on the staff of the Senate Foreign Relations Committee during the 1990s. As the ranking Democrat, Mr. Biden prided himself on his ability to compromise with committee chairman Jesse Helms, R-N.C., arguably the most uncompromising conservative in the Senate. Together, they passed legislation - the so-called Helms-Biden Act - to reform the United Nations and cut deals to restructure the State Department.\n\"As chairman and ranking member, we passed some of the most significant legislation passed in the last 40 years,\" Mr. Biden explained during a 2015 speech. He continues to tout his relationship with Helms (who died more than a decade ago) on the stump as an example of how he can work with die-hard conservatives to get things done.\nIs this what Democratic primary voters want? Mr. Biden's lead in the national polls suggests it may be. But it is early. After all, at this time in 2015, Scott Walker, the Wisconsin governor at the time, appeared to be the front-runner for the Republican nomination and no one was taking Mr. Trump seriously. Mr. Biden may be ahead for now, but all the energy inside the Democratic Party seems to be with the uncompromising left. It sees Mr. Biden standing in the way of its takeover of the Democratic Party. So as his lead in the polls expands, their efforts to stop him - and his heretical calls for compromise - will escalate.\n\"We have to unify this country,\" Mr. Biden said at a speech in Iowa earlier this month. \"The other side is not my enemy, it's my opposition.\" How sad that has become a controversial statement.\n\"\"\"\n\nprint('-----------------------')\nprint('USING 1 PROCESS')\nprint('-----------------------')\n\nfor doc in nlp.pipe([text], n_process=1):\n    for x in doc.ents:\n        if x.ent_id > 0:\n            print(x, x.start_char, x.end_char, x.label_, x.ent_id, x.ent_id_)\n\nprint('-----------------------')\nprint('USING >1 PROCESS')\nprint('-----------------------')\n\nfor doc in nlp.pipe([text], n_process=2):\n    for x in doc.ents:\n        if x.ent_id > 0:\n            print(x, x.start_char, x.end_char, x.label_, x.ent_id, x.ent_id_)\nInfo about spaCy\nI'm using master branch as of commit 3431ac42de470a4bb73f1c6852a5ccffc07da7b1.\nspaCy version: 2.2.3\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.5\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-12-29T18:26:16Z", "fixed_by": "#4852", "pull_request_summary": "serialize ENT_ID", "pull_request_description": "Member\nsvlandeg commented on 30 Dec 2019\nDescription\nAllow token.ent_id to be serialized through the attr ENT_ID, which wasn't defined yet in the symbols or attrs list.\nFixes #4849 + unit test\nTypes of change\nbug fix ?\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-06T13:57:35Z", "files_changed": [["1", "spacy/attrs.pxd"], ["1", "spacy/attrs.pyx"], ["8", "spacy/language.py"], ["1", "spacy/symbols.pxd"], ["1", "spacy/symbols.pyx"], ["36", "spacy/tests/regression/test_issue4849.py"], ["10", "spacy/tests/serialize/test_serialize_extension_attrs.py"], ["6", "spacy/tokens/doc.pyx"], ["2", "spacy/tokens/span.pyx"], ["4", "spacy/tokens/token.pxd"]]}, "44": {"issue_url": "https://github.com/explosion/spaCy/issues/4848", "issue_id": "#4848", "issue_summary": "spaCy on macOS is slower than Linux?", "issue_description": "rivamarco commented on 29 Dec 2019\nI'm using spaCy for NER Training for adding a new entity type.\nMy initial setup was Manjaro Linux on a Dell Latitude laptop with an Intel i5-5300u and 16GB of RAM, and after one iteration of my data using spacy train en output train.json validation.json -p ner I obtained ~16000 CPU WPS with 100% CPU usage (on one core only).\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.19.88-1-MANJARO-x86_64-with-glibc2.2.5\nPython version: 3.8.0\nModels: en\n(same results with python 3.7.6)\nI moved this project (same training files, same python packages, all the same) on a MacBook Pro 15 2015 with i7 2.2Ghz (should be i7-4770HQ) 16GB and macOS Catalina.\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Darwin-19.2.0-x86_64-i386-64bit\nPython version: 3.7.6\nModels: en\nNow with this configuration, I obtain ~13000 CPU WPS with the CPU that is far away to reach the 100% mark, even on a single-core, like the Dell with Linux.\nI think that with the MacBook's CPU I have to get better results (maybe only a little), am I wrong? Is this the correct behavior or for macOS there is something that I have to keep in mind?\nThank you for your help", "issue_status": "Closed", "issue_reporting_time": "2019-12-29T18:01:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "45": {"issue_url": "https://github.com/explosion/spaCy/issues/4847", "issue_id": "#4847", "issue_summary": "Error handling with nlp.pipe", "issue_description": "Contributor\nAlJohri commented on 29 Dec 2019\nI'm using nlp.pipe (with multiple processes) in the latest spaCy (master branch) and having trouble doing proper error handling in a generator.\nI have a custom pipeline component which occasionally errors on some documents with ValueError: [E103] Trying to set conflicting doc.ents: '(1387, 1388, 'PERSON')' and '(1387, 1388, 'PERSON')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap..\nIssue\nI can't easily figure out which document ID caused the issue since my custom pipeline components doesn't have access to the context. The context is provided to nlp.pipe via the as_tuples=True argument.\ngen = tqdm(\n    nlp.pipe(zip(df.text, df.id), as_tuples=True, batch_size=10, n_process=16),\n    total=len(df),\n)\nI can't figure out how to ignore the exception and continue processing since it doesn't seem like there's an easy way to catch the error and continue the generator, especially in a multiprocessing setting.\nPotential Solution\nCan we add an ignore_exceptions=True, error_handler=callable which allows ignoring exceptions and running some error handler function that has access to the handler(error, doc, context)?\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.5", "issue_status": "Closed", "issue_reporting_time": "2019-12-29T15:10:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "46": {"issue_url": "https://github.com/explosion/spaCy/issues/4845", "issue_id": "#4845", "issue_summary": "''Empty label found in new labels\" with `spacy debug-data` and Prodigy", "issue_description": "kormilitzin commented on 29 Dec 2019 \u2022\nedited\nI was annotating with Prodigy (ner.make-gold, 7 categories), and when wanted to validate the training dataset, got the following error:\nThat's a bit strange, as I haven't missed tokens and used labels from the list. Moreover, I have explicitly checked, and none of the labels in the annotated examples, contains the empty label (' ').\nIs it possible to introduce an automated way to remove such empty labels in future releases? It will be very helpful.\nIs there a simple solution to remove such labels from the data? Thanks.", "issue_status": "Closed", "issue_reporting_time": "2019-12-28T22:45:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "47": {"issue_url": "https://github.com/explosion/spaCy/issues/4841", "issue_id": "#4841", "issue_summary": "Pre-trained Vectors are not stored with pickling + saving then loading + un-pickling trained model", "issue_description": "acherednychenko commented on 28 Dec 2019\nPre-train model with vectors\nTrain and pickle model. Save to file\nLoad from file, unpickle model, call predict:\nOSError: [E050] Can't find model 'en_core_web_md.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-4.14.154-128.181.amzn2.x86_64-x86_64-with-debian-stretch-sid\nPython version: 3.6.9\nPlease assist", "issue_status": "Closed", "issue_reporting_time": "2019-12-28T00:47:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "48": {"issue_url": "https://github.com/explosion/spaCy/issues/4838", "issue_id": "#4838", "issue_summary": "Segmentation fault during model download (Spacy-2.2.3)", "issue_description": "mike94043 commented on 27 Dec 2019 \u2022\nedited\nHow to reproduce the problem\nmkg@vicky:~$ python3 -m spacy download de\nCollecting de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5\nDownloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.9MB 8.1MB/s\nCollecting spacy>=2.2.2 (from de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4MB 6.7MB/s\nCollecting murmurhash<1.1.0,>=0.28.0 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/a6/e6/63f160a4fdf0e875d16b28f972083606d8d54f56cd30cb8929f9a1ee700e/murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl\nCollecting numpy>=1.15.0 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/92/e6/45f71bd24f4e37629e9db5fb75caab919507deae6a5a257f9e4685a5f931/numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.1MB 7.6MB/s\nCollecting catalogue<1.1.0,>=0.0.7 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/4b/4c/0e0fa8b1e193c1e09a6b72807ff4ca17c78f68f0c0f4459bc8043c66d649/catalogue-0.2.0-py2.py3-none-any.whl\nCollecting requests<3.0.0,>=2.13.0 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 2.2MB/s\nCollecting setuptools (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/54/28/c45d8b54c1339f9644b87663945e54a8503cfef59cf0f65b3ff5dd17cf64/setuptools-42.0.2-py2.py3-none-any.whl (583kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 583kB 6.4MB/s\nCollecting blis<0.5.0,>=0.4.0 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.7MB 8.0MB/s\nCollecting thinc<7.4.0,>=7.3.0 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.2MB 9.0MB/s\nCollecting cymem<2.1.0,>=2.0.2 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/e7/b5/3e1714ebda8fd7c5859f9b216e381adc0a38b962f071568fd00d67e1b1ca/cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl\nCollecting wasabi<1.1.0,>=0.4.0 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/ff/ef/e8266e158ed32bf5f723fac862b6518833d0b53ca183165a8718f212c0d5/wasabi-0.4.2-py3-none-any.whl\nCollecting srsly<1.1.0,>=0.1.0 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/d7/fb/34136c7b2ad04d4472dd9ea86536f5e9fb71fb0eb78edc8dad76a3f9edf2/srsly-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (185kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 194kB 14.3MB/s\nCollecting preshed<3.1.0,>=3.0.2 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90\nba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 4.8MB/s\nCollecting plac<1.2.0,>=0.9.6 (from spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\nCollecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/e9/71/1a1e0ed0981bb6a67bce55a210f168126b7ebd2065958673797ea66489ca/importlib_metadata-1.3.0-py2.py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 9.3MB/s\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 4.9MB/s\nCollecting idna<2.9,>=2.5 (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 17.3MB/s\nCollecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 10.7MB/s\nCollecting tqdm<5.0.0,>=4.10.0 (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/8c/c3/d049cf3fb31094ee045ec1ee29fffac218c91e82c8838c49ab4c3e52627b/tqdm-4.41.0-py2.py3-none-any.whl (56kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 63.0MB/s\nCollecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/74/3d/1ee25a26411ba0401b43c6376d2316a71addcc72ef8690b101b4ea56d76a/zipp-0.6.0-py2.py3-none-any.whl\nCollecting more-itertools (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5)\nDownloading https://files.pythonhosted.org/packages/68/03/0604cec1ea13c9f063dd50f900d1a36160334dd3cfb01fd0e638f61b46ba/more_itertools-8.0.2-py3-none-any.whl (40kB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 51.5MB/s\nInstalling collected packages: murmurhash, numpy, more-itertools, zipp, importlib-metadata, catalogue, certifi, urllib3, idna, chardet, requests, setuptools, blis, cymem, preshed, srsly, tqdm, wasabi, plac, thinc, spacy, de-core-news-sm\nRunning setup.py install for de-core-news-sm ... done\nSuccessfully installed blis-0.4.1 catalogue-0.2.0 certifi-2019.11.28 chardet-3.0.4 cymem-2.0.3 de-core-news-sm-2.2.5 idna-2.8 importlib-metadata-1.3.0 more-itertools-8.0.2 murmurhash-1.0.2 numpy-1.18.0 plac-1.1.3 preshed-3.0.2 requests-2.22.0 setuptools-42.0.2 spacy-2.2.3 srsly-0.2.0 thinc-7.3.1 tqdm-4.41.0 urllib3-1.25.7 wasabi-0.4.2 zipp-0.6.0\n\u2714 Download and installation successful\nYou can now load the model via spacy.load('de_core_news_sm')\nSegmentation fault (core dumped)\n(Also happens with python3 -m spacy download en)\nSegmentation fault (core dumped)\n## Your Environment\n<!-- Include details of your environment. If you're using spaCy 1.7+, you can also type `python -m spacy info --markdown` and copy-paste the result here.-->\n\nmkg@vicky:~$ python3 -m spacy info --markdown\n\n## Info about spaCy\n\n* **spaCy version:** 2.2.3\n* **Platform:** Linux-5.0.0-37-generic-x86_64-with-Ubuntu-18.04-bionic\n* **Python version:** 3.6.9\n\n* Environment Information:\n\nmkg@vicky:~$ uname -a\nLinux vicky 5.0.0-37-generic #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux", "issue_status": "Closed", "issue_reporting_time": "2019-12-27T03:12:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "49": {"issue_url": "https://github.com/explosion/spaCy/issues/4837", "issue_id": "#4837", "issue_summary": "Multi-language model doesn't support doc.sents", "issue_description": "m-marina commented on 25 Dec 2019\nWhen I try:\nimport spacy\nnlp = spacy.load(\"xx_ent_wiki_sm\")\ntext = \"...\"\ndoc = nlp(text)\nfor sent in doc.sents:\n    print(sent)\nOutput: ValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start.\nBut if I add nlp.add_pipe(nlp.create_pipe('sentencizer')) - it works.\nIf we compare the English-language model (en_core_web_sm), it divides the text into sentences without adding 'sentencizer' component.\nThe question is how to do the same for the Multi-language model?\nEnvironment\nOperating System: Ubuntu 18.04\nPython Version: 3.6\nspaCy Version: 2.2.3", "issue_status": "Closed", "issue_reporting_time": "2019-12-25T14:09:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "50": {"issue_url": "https://github.com/explosion/spaCy/issues/4833", "issue_id": "#4833", "issue_summary": "Unable to train textcat with en_trf_bertbaseuncased_lg model", "issue_description": "acherednychenko commented on 24 Dec 2019\nHow to reproduce the behaviour\nUse train_textcat.py to reproduce. Training works for core models, but unfortunately not for en_trf_bertbaseuncased_lg\nRunning:\npython train_textcat.py -m \"en_trf_bertbaseuncased_lg\" -n 1\nreturns:\n  File \"train_textcat.py\", line 159, in <module>\n    plac.call(main)\n  File \"/workspace/code/activity-classification/venv_act/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/workspace/code/activity-classification/venv_act/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"train_textcat.py\", line 86, in main\n    nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n  File \"/workspace/code/activity-classification/venv_act/lib/python3.6/site-packages/spacy_transformers/language.py\", line 81, in update\n    tok2vec = self.get_pipe(PIPES.tok2vec)\n  File \"/workspace/code/activity-classification/venv_act/lib/python3.6/site-packages/spacy/language.py\", line 286, in get_pipe\n    raise KeyError(Errors.E001.format(name=name, opts=self.pipe_names))\nKeyError: \"[E001] No component 'trf_tok2vec' found in pipeline. Available names: ['textcat']\"\n(venv_act) ```\n\nRunning training on the core model, works fine though:\n`python train_textcat.py -m \"en_core_web_md\" -n 1`\n\n\n## Your Environment\n* **spaCy version:** 2.2.1 (also tested with 2.2.3)\n* **Platform:** Linux-4.14.62-70.117.amzn2.x86_64-x86_64-with-debian-stretch-sid\n* **Python version:** 3.6.9\n\nPlease assist,\n\nPS: Love your products :-)", "issue_status": "Closed", "issue_reporting_time": "2019-12-23T19:55:11Z", "fixed_by": "#4834", "pull_request_summary": "run normal textcat train script with transformers", "pull_request_description": "Member\nsvlandeg commented on 24 Dec 2019\nDescription\nThe current example training scripts don't work with the transformer models, as they need access to their tok2vec component during nlp.update() but that component gets disabled in the train script.\nIf we want to support this use-case, we can specifically prevent disabling this component as in this PR. I know we have specific train scripts for the transformer models, but on the other hand we also claim they can be used like normal spaCy models, so it's not a strange expectation that the example scripts would work with the transformers.\nFixes #4833\nTypes of change\nenhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-16T01:01:24Z", "files_changed": [["3", "examples/training/pretrain_textcat.py"], ["3", "examples/training/rehearsal.py"], ["3", "examples/training/train_entity_linker.py"], ["3", "examples/training/train_intent_parser.py"], ["3", "examples/training/train_ner.py"], ["3", "examples/training/train_new_entity_type.py"], ["3", "examples/training/train_parser.py"], ["3", "examples/training/train_textcat.py"]]}, "51": {"issue_url": "https://github.com/explosion/spaCy/issues/4832", "issue_id": "#4832", "issue_summary": "How to train multiple labels of NER at the same time?", "issue_description": "youikim commented on 23 Dec 2019 \u2022\nedited\nHi there, I am having a issue with creating & training multiple labels of NER. It works fine with single label tho. As I am a coding beginner, please take a look my code below and please give me a proper solution to resolve the issue.\nTRAIN_DATA = [\n(\"If Contractor is not liable to pay any compensation to Contractor for loss of profits or loss of goodwill or for any other loss or damage howsoever arising as a result of the termination\",\n{\"entities\": [(70, 86, \"LOP\")]}),\n(\"For the portion of the Contract Price that exceeds NOK 1 billion, the liability for liquidated damages is limited to 5 % of the excessive amount\",\n{\"entities\": [(107, 115, \"Limited\")]}),\n(\"Nor the BUILDER shall in any circumstance be responsible or liable for any consequential or special loss, damage or expense\",\n{\"entities\": [(75, 89, \"Consequential\")]}),\n(\"Contractor shall pay to Owner, as liquidated damages for such delay, the amount corresponding to the percentage of the Contract Price of the FLNG as per Section 9.2.1.\",\n{\"entities\": [(34, 54, \"Governed\")]}),\n(\"CONTRACTOR shall not take and is not authorized to take any action in the name of or otherwise on behalf of COMPANY which would violate applicable LAW or which would subject either party to liability or penalty under any laws, rules, regulations or decrees of any governmental authority\",\n{\"entities\": [(203, 211, \"Penalty\")]}),\n(\"FIDIC strongly recommends that the Employer ensures that the Employers Requirements contain a clearly identified, defined and described specific purpose or purposes for which the facility will be used when complete, in order that the Contractor can comply with the obligation to provide Works which are fit for the purpose(s) for which they are intended as stated in this Sub-Clause.\",\n{\"entities\": [(303, 322, \"FFP\")]}),\n(\"Upon notice to Contractor, Owner may in its absolute discretion withhold or deduct payment of any amount described in an Invoice or portions thereof in an amount and to such extent as may in its sole opinion be reasonably necessary to protect Owner due to Liquidated damages such as Delay Damages which Contractor owes.\",\n{\"entities\": [(283, 296, \"D-Damages\")]}),\n(\"when completed, not being fit for the purpose(s) for which they are intended under Sub-Clause 4.1.\",\n{\"entities\": [(26, 45, \"FFP\")]}),\n(\"The parties hereto agree that the validity and the interpretation of this Contract and of each Article and part thereof shall be governed by the laws of England.\",\n{\"entities\": [(130, 139, \"Governed\")]}),\n(\"COMPANY shall not be liable for any other damages including, without limitation, loss of anticipated profits.\",\n{\"entities\": [(83, 110, \"LOP\")]}),\n(\"shall mean (i) indirect or consequential losses and/or (ii) loss of production, loss of product, loss of use and loss of revenue, profit or antic ipated profit\",\n{\"entities\": [(27, 41, \"Consequential\")]}),\n(\"The Contractor shall be responsible for this part and it shall, when the Works are completed, be fit for such purpose(s) for which the part is intended as are specified in the Contract (or, where no purpose(s) are so defined and described, fit for their ordinary purpose(s).\",\n{\"entities\": [(97, 117, \"FFP\"), (240, 270,\"FFP\")]}),\n(\"This Contract is governed by and interpreted under the laws of England, without regard to its choice of law rules.\",\n{\"entities\": [(18, 27, \"Governed\")]}),\n(\"All exclusions and indemnities given under this ARTICLE 22(save for those under Section 22.1.1(c), Section 22.2.1(c), and Section 22.9(Consequential Damages) shall apply irrespective of cause and notwithstanding the negligence or breach of duty(whether statutory or otherwise)of the indemnified party or any other entity or party and shall apply irrespective of any claim in tort, under contract or otherwise at law.\",\n{\"entities\": [(135, 156, \"D-Damages\")]}),\n(\"If these revised methods cause the Employer to incur additional costs, the Contractor shall subject to Sub Clause 2.5 pay these costs to the Employer, in addition to delay damages under Sub Clause 8.7 below.\",\n{\"entities\": [(166, 181, \"D-Damages\")]}),\n(\"To the extent, if any, that the Contractor is responsible for the design of part of the Permanent Works under Sub-Clause 4.1, and/or any other design under the Contract, the Contractor shall also indemnify and hold harmless the Employer against all acts, errors or omissions by the Contractor in carrying out the Contractors design obligations that result in the Works, when completed, not being fit for the purpose(s) for which they are intended Under Sub-Clause 4.1.\",\n{\"entities\": [(396, 415, \"FFP\")]}),\n(\"Contractor's liability for any delay this might cause shall be limited to the maximum liability for delays that the Subcontractor can incur towards Contractor\",\n{\"entities\": [(64, 72, \"Limited\")]}),\n(\"24.1 CONTRACTOR shall compensate COMPANY for any consequential, special, or indirect damages or loss of anticipated profits sustained by COMPANY, limited, however to the extent CONTRACTOR may recover from insurance carried by it and to the extent it may recover such damages or losses from vendors, subcontractors, renters of construction tools and construction equipment, or others.\",\n{\"entities\":[(76, 91, \"D-Damages\")]}),\n(\"Contractor must provide all Construction Equipment necessary for the performance of the Work. Such Construction Equipment must be fit for the use it is intended for and maintained at all times in good operating condition with appropriate and uninterrupted valid certification in accordance with Applicable Laws and the Contract requirements.\",\n{\"entities\": [(130, 145, \"FFP\")]}),\n(\"Interest, penalties, or other liabilities arising from such failures shall be for CONTRACTOR's account.\",\n{\"entities\": [(10, 20, \"Penalty\")]}),\n(\"the PERMANENT WORK shall be fit for the purposes specified in the CONTRACT, or where no such purpose is specified, fit for its ordinary purpose.\",\n{\"entities\": [(115, 144, \"FFP\")]}),\n(\"any sums that would be payable under this ARTICLE 20 are in the nature of liquidated damages.\",\n{\"entities\": [(74, 93, \"D-Damages\")]}),\n(\"The Work shall be fit for the purposes specified in the Contract or, where no such purpose is specified, fit for its ordinary purpose.\",\n{\"entities\": [(105, 144, \"FFP\")]}),\n(\"Except as provided in ARTICLE 21 and Sections 10.8 and Section 22.1.2, payment of the Delay Damages shall be Owner\u2019s sole and exclusive remedy for Contractor\u2019s failure to achieve the Handover of the FLNG on or before the applicable Handover Date or achieve Commencement of Stable Operation on or before the applicable Commercial Operation Date.\",\n{\"entities\": [(86, 99, \"D-Damages\")]}),\n(\"This Sub-Clause shall not limit liability in any case of fraud, gross negligence, deliberate default or reckless misconduct by the defaulting Party\",\n{\"entities\": [(26, 42, \"Limited\")]}),\n(\"The maximum total amount of liquidated damages payable under Section20.2 (\u201cDelay Damages\u201d) in respect of the Physical Work or FLNG shall be equal to ten percent of the Contract Price of the FLNG.\",\n{\"entities\": [(77, 90, \"D-Damages\")]}),\n(\"The proposal shall contain a breakdown of the total compensation for the Work, including all claims to be made by Contractor, less any liquidated damages and other amounts due to Company.\",\n{\"entities\": [(135, 155, \"D-Damages\")]}),\n(\"Williams Field Services Group LLC, here in after referred to as Company, a Delaware limited liability company\",\n{\"entities\": [(85, 103, \"Limited\")]}),\n(\"The Contractor has no liability for any indirect or consequential loss or damage suffered by the Employer as a result of any negligence by the Contractor in designing the Works to be fit for purpose.\",\n{\"entities\": [(183, 198, \"FFP\")]}),\n(\"if there is no Schedule of Performance Guarantees under the Contract, or no applicable Performance Damages, a reduction in the Contract Price.\",\n{\"entities\": [(87, 107, \"D-Damages\")]}),\n(\"If CONTRACTOR fails to achieve COMPLETION of PLANT by the SCHEDULED COMPLETION DATE indicated in 3.2 above for reasons other than those resulting from force majeure or those directly attributable to actions taken by COMPANY, CONTRACTOR shall pay to COMPANY, for each week or part of a week of delay, in place of actual damages incurred by COMPANY due to such delay in COMPLETION of PLANT, the fixed sum(s) and agreed liquidated damages specified below.\",\n{\"entities\": [(417, 435, \"D-Damages\")]}),\n(\"Such Construction Equipment must be fit for the use it is intended for and maintained at all times in good operating condition with appropriate and uninterrupted valid certification in accordance with Applicable Laws and the Contract requirements.\",\n{\"entities\": [(36, 52, \"FFP\")]}),\n(\"Nor the BUILDER shall in any circumstance be responsible or liable for any consequential or special loss, damage or expense including but not limited to loss of time, loss of profit of earning or demurrage directly or indirectly occasioned to the BUYER.\",\n{\"entities\": [(168, 183, \"LOP\")]}),\n(\"The payment of liquidated damages shall not relieve CONTRACTOR from all of its other obligations and liabilities under CONTRACT, including its obligation to perform WORK.\",\n{\"entities\": [(15, 33, \"D-Damages\")]}),\n(\"Performance Damages means the damages to be paid by the Contractor to the Employer for the failure to achieve the guaranteed performance of the Plant and/or the Works or any part of the Works.\",\n{\"entities\": [(0, 20, \"D-Damages\")]}),\n(\"Company shall indemnify Contractor Group from and against claims mentioned in the first paragraph above, to the extent that they exceed the limitations of liability mentioned above, regardless of any form of liability, whether strict or by negligence, in whatever form, on the part of Contractor Group\",\n{\"entities\": [(141, 165, \"Limited\")]}),\n(\"Conciliation Act, Cap. A18, Laws of the Federation of Nigeria, 2004 and any amendments thereto, by three arbitrators each party will appoint an arbitrator.\",\n{\"entities\": [(19, 22, \"CAP\")]}),\n(\"the Works (or Section or major item of Plant, if any) shall be fit for the purpose(s) for which they are intended, as defined and described in the Employer\u2019s Requirements or, where no purpose(s) are so defined and described, fit for their ordinary purpose(s)\",\n{\"entities\": [(63, 82, \"FFP\"), (227, 257, \"FFP\")]}),\n(\"CONTRACTOR shall pay COMPANY, in lieu of actual damages, one half of one percent (0.5%) of the sum of (i) CONTRACT PRICE indicated in paragraph 1.1 of Exhibit A herein plus (ii) contract price paid to ENGINEERING CONTRACTOR for work performed by it in connection with PLANT, (both prices as adjusted for approved CHANGE ORDERS ) for each week or part of a week the COMPLETION of PLANT is delayed beyond the SCHEDULED COMPLETION DATE (as adjusted for approved CHANGE ORDERS).\",\n{\"entities\":[(41, 54, \"D-Damages\")]}),\n(\"the Works, or a Section, fail to pass any or all of the Tests after Completion; and (b) applicable Performance Damages are set out in the Schedule of Performance Guarantees the Employer shall be entitled subject to Sub-Clause 20.2 [Claims For Payment and/or EOT] to payment of these Performance Damages by the Contractor in full satisfaction of this failure.\",\n{\"entities\": [(99, 119, \"D-Damages\")]}),\n(\"Any VARIATION shall be governed by all the provisions of the CONTRACT.\",\n{\"entities\": [(24, 33, \"Governed\")]}),\n(\"the Contractor\u2019s liability to the Company will instead be for general damages at law with the same maximum amount.\",\n{\"entities\": [(86, 102, \"D-Damages\")]}),\n(\"it may adversely affect the Contractor\u2019s obligation to complete the Works so that they shall be fit for the purpose(s) for which they are intended.\",\n{\"entities\": [(98, 117, \"FFP\")]}),\n(\"Acceptance and payment by Company of any such Work or Materials shall not relieve Contractor of any liability to Company under this Contract\",\n{\"entities\": [(101, 110, \"Liability\")]}),\n(\" If the proceeds of sale are insufficient to pay such total costs and loss of profit as aforesaid, the BUYER shall promptly pay the deficiency to the BUILDER upon request.\",\n{\"entities\": [(70, 85, \"LOP\")]}),\n(\"when completed, not being fit for the use for which they are intended under Sub-Clause 4.1.\",\n{\"entities\": [(26, 41, \"FFP\")]}),\n(\"if such information is Excluded Work Document, it shall be Confidential Information and such disclosure shall be governed by Section 25.14.\",\n{\"entities\": [(114, 123, \"Governed\")]}),\n(\"worker's compensation and employer's liability insurance, unemployment compensation insurance, old age benefits, welfare funds, pensions and annuities, and disability insurance\",\n{\"entities\": [(38, 47, \"Liability\")]}),\n(\"Contractor shall be liable to Owner for all resulting from FLNG underperformance, provided always that the cap on total liability under Section 22.7.1, shall apply.\",\n{\"entities\": [(107, 111, \"CAP\")]}),\n]\n@plac.annotations(\nmodel=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\noutput_dir=(\"Optional output directory\", \"option\", \"o\", Path),\nn_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n)\ndef main(model='en_core_web_sm', output_dir=None, n_iter=100):\n\"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\nif model is not None:\nnlp = spacy.load(model) # load existing spaCy model\nprint(\"Loaded model '%s'\" % model)\nelse:\nnlp = spacy.blank(\"en\")\nprint(\"Created blank 'en' model\")\nif \"ner\" not in nlp.pipe_names:\n    ner = nlp.create_pipe(\"ner\")\n    nlp.add_pipe(ner, last=True)\n\nelse:\n    ner = nlp.get_pipe(\"ner\")\n\n# add labels\nfor _, annotations in TRAIN_DATA:\n    for ent in annotations.get(\"entities\"):\n        ner.add_label(ent[2])\n\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    if model is None:\n        nlp.begin_training()\n    for itn in range(n_iter):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(\n                texts,  # batch of texts\n                annotations,  # batch of annotations\n                drop=0.5,  # dropout - make it harder to memorise data\n                losses=losses,\n            )\n        print(\"Losses\", losses)\n\n# test the trained model\nfor text, _ in TRAIN_DATA:\n    doc = nlp(text)\n    print(\"Entities - \", [(ent.text, ent.label_) for ent in doc.ents])\n    print(\"Tokens - \", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\n# save model to output directory\nif output_dir is not None:\n    output_dir = Path(output_dir)\n    if not output_dir.exists():\n        output_dir.mkdir()\n    nlp.to_disk(output_dir)\n    print(\"Saved model to\", output_dir)\n\n    # test the saved model\n    print(\"Loading from\", output_dir)\n    nlp2 = spacy.load(output_dir)\n    for text, _ in TRAIN_DATA:\n        doc = nlp2(text)\n        print(\"Entities - \", [(ent.text, ent.label_) for ent in doc.ents])\n        print(\"Tokens - \", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\nlabel_ = [\"FFP\", \"D-Damages\", \"LOP\", \"Governed\", \"CAP\", \"Penalty\", \"Consequential\", \"Limited\", \"Liability\"]\n@plac.annotations(new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str))\ndef main(model='en_core_web_sm', new_model_name= label_, output_dir=None, n_iter=30):\nrandom.seed(0)\nif model is not None:\nnlp = spacy.load(model) # load existing spaCy model\nprint(\"Loaded model '%s'\" % model)\nelse:\nnlp = spacy.blank(\"en\") # create blank Language class\nprint(\"Created blank 'en' model\")\n# Add entity recognizer to model if it's not in the pipeline\n# nlp.create_pipe works for built-ins that are registered with spaCy\nif \"ner\" not in nlp.pipe_names:\nner = nlp.create_pipe(\"ner\")\nnlp.add_pipe(ner)\n# otherwise, get it, so we can add labels to it\nelse:\nner = nlp.get_pipe(\"ner\")\nfor LABEL in label_:\n    ner.add_label(LABEL)  # add new entity label to entity recognizer\n\nif model is not None:\n    optimizer = nlp.begin_training()\nelse:\n    optimizer = nlp.resume_training()\nmove_names = list(ner.move_names)\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    sizes = compounding(1.0, 4.0, 1.001)\n    # batch up the examples using spaCy's minibatch\n    for itn in range(n_iter):\n        random.shuffle(TRAIN_DATA)\n        batches = minibatch(TRAIN_DATA, size=sizes)\n        losses = {}\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n        print(\"Losses\", losses)\n\n\ntest_text = \"Contractor warrants that the Plant will: (a) be fit for the purpose and use for which it was intended; (b) be free from defects in materials, design, construction and workmanship; and (c) comply with Applicable Laws. (a) Procurement and Supply : (i) Contractor must procure, manufacture, fabricate, supply and deliver all Contractor Items so as to ensure that the Work are performed in accordance with the Work Time Schedule, including performing all related operations such as testing, inspecting, packing, handling and transport and the like, necessary for carrying out the Work. (ii) All Contractor Items must: (A) be new and comply with the description, quality and quantity required by the Contract; (B) be of sound design, specification, materials and workmanship; (C) be capable of the degree of performance specified in the Contract; and (D) be fit for the purpose and use for which they were intended. If Contractor makes any change in breach of these provisions, then Company is entitled to apply as liquidated damages the amounts set out in Exhibit K.(e) Each Party acknowledges that the liquidated damages payable pursuant to sub-Article 26.1(d) are a genuine pre-estimate of Company loss and does not constitute a penalty. If the liquidated damages set out in Exhibit K are found not to be enforceable or the clauses in this Contract in relation to liquidated damages are found to be invalid or unenforceable for any reason, Contractor remains liable to Company for any loss or damage suffered by Company up to the amount of the relevant liquidated damages referred to in Exhibit K. (a) Contractor must provide all Construction Equipment necessary for the performance of the Work. Such Construction Equipment must be fit for the use it is intended for and maintained at all times in good operating condition with appropriate and uninterrupted valid certification in accordance with Applicable Laws and the Contract requirements. (a) Contractor acknowledges that time is of the utmost importance and that the completion of the Work by the Completion Date(s) is an essential condition of the Contract and of the overall Project development programme. Contractor must complete the Work, and each separate designated part of the Work, on or before the key dates and times for completion set out in the Work Time Schedule. (b) Contractor warrants that it will perform the Work in accordance with the Work Time Schedule, failing which Company has the remedies specified under the Contract, including those provided for under Article 51 and sub-Article 36.1 concerning defective performance by Contractor and liquidated damages for late completion. Subject to the provisions of sub-Article 20.3, if the results of the Performance Tests are within the range specified in Exhibit D, Company may either: (i) apply liquidated damages in accordance with the provisions of sub-Article 36.2. In such a case, Contractor is entitled to formally request the issuance of the Provisional Acceptance Certificate in accordance with the procedure defined in sub-Article 19.2(a); or (ii) request Contractor to promptly perform the necessary corrections in accordance with the provisions of sub-Article 19.2(e). If liquidated damages are found not to be payable or the Articles in this Contract in relation to liquidated damages are found to be invalid or unenforceable for any reason, then the Parties agree that Contractors liability to Company will instead be for general damages at law for Contractors failure to comply with the relevant obligation. If Contractor fails to complete the relevant part of the Work by the relevant Completion Date then Contractor will pay liquidated damages to Company in accordance with Exhibit B. (b) Subject to Company rights and remedies provided for under Article 35, sub-Articles 36.2 to 36.5 and Article 51, the payment of liquidated damages under sub-Article 36.1(a) are the sole and exclusive financial remedy of Company in respect of Contractors failure to complete the relevant part of the Work by the Completion Date. It is acknowledged and agreed by the Parties that  any sums that would be payable under this are in the nature of liquidated damages, and are not a penalty or consequential damages. If Company decides to apply liquidated damages, in accordance with the provisions of sub-Article 19.2(f) due to a failure of the Plant to achieve the Performance Tests, Contractor will pay to Company liquidated damages in accordance with Exhibit B. The payment of liquidated damages under this Article 36 or elsewhere in this Contract does not prevent Company from exercising any other rights and remedies provided for under this Contract (including for the avoidance of doubt its rights under sub-Article 35.3 and Article 51), and does not relieve Contractor from its obligations to diligently complete the Work or from any other obligations and liabilities under the Contract. Delay Damages also are not be deemed to cover the cost  of completion of the Works or other damages and Owner shall also been titled to rely on its other remedies under this Agreement for all Defaults. Contractor shall indemnify, hold harmless and defend each member of Owner Group from and against any and all claims, losses, damages (including consequential damages), liabilities, judgments, fines, costs and expenses (including arbitration and court procedures costs in any level of jurisdiction and experts\u2019 and attorneys\u2019 fees) and claims, demands, proceedings, actions,causes of action.\"\ndoc = nlp(test_text)\nprint('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n\n#test_text = sent_tokenize(doc)\n#print(\"running nlp\")\nprint(\"Entities in - '%s'\" % test_text, \"-\", [(ent.text, ent.label_) for ent in doc.ents] )\n\n#spacy.displacy.serve(doc, style='ent')\nprint(spacy.displacy.render(doc, style=\"ent\", page=\"true\"))\n\n# ents = []\n# \n# for ent in doc.ents:\n#     print(ent.label_,\"-\", ent.text)\n#     if ent.label in ents:\n#         ents.append(ents)\n#         risk_sentences = [sentence for sentence in sentences if ent.text in sentence]\n#         for sent in risk_sentences:\n#             print(sent)\n\n# save model to output directory\nif output_dir is not None:\n    output_dir = Path(output_dir)\n    if not output_dir.exists():\n        output_dir.mkdir()\n    nlp.meta[\"name\"] = new_model_name  # rename model\n    nlp.to_disk(output_dir)\n    print(\"Saved model to\", output_dir)\n\n    # test the saved model\n    print(\"Loading from\", output_dir)\n    nlp2 = spacy.load(output_dir)\n    # Check the classes have loaded back consistently\n    assert nlp2.get_pipe(\"ner\").move_names == move_names\n    doc2 = nlp2(text)\n    for ent in doc2.ents:\n        print(ent.label_, ent.text)\nif name == \"main\":\nplac.call(main)\nYour Environment\nOperating System: ubuntu 18.04\nPython Version Used: 2.7\nspaCy Version Used: how to check?\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-23T00:29:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "52": {"issue_url": "https://github.com/explosion/spaCy/issues/4830", "issue_id": "#4830", "issue_summary": "spaCy CLI NER fails after several epochs: ValueError: [E024]", "issue_description": "kormilitzin commented on 22 Dec 2019 \u2022\nedited\nI am CLI-training a NER model, however, after 3-4 epochs, it crashes with the following error:\nTraceback (most recent call last): File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main \"main\", mod_spec)\nFile \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code exec(code, run_globals)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/main.py\", line 33, in plac.call(commands[command], sys.argv[1:])\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py\", line 367, in call cmd, result = parser.consume(arglist)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py\", line 232, in consume return cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/cli/train.py\", line 368, in train losses=losses,\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/language.py\", line 515, in update proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\nFile \"nn_parser.pyx\", line 456, in spacy.syntax.nn_parser.Parser.update File \"nn_parser.pyx\", line 587, in spacy.syntax.nn_parser.Parser.get_batch_loss File \"transition_system.pyx\", line 156, in spacy.syntax.transition_system.TransitionSystem.set_costs\nValueError: [E024] Could not find an optimal move to supervise the parser. Usually, this means that the model can't be updated in a way that's valid and satisfies the correct annotations specified in the GoldParse. For example, are all labels added to the model? If you're training a named entity recognizer, also make sure that none of your annotated entity spans have leading or trailing whitespace. You can also use the experimental debug-data command to validate your JSON-formatted training data. For details, run: python -m spacy debug-data --help\nI've run debug-data, and no problems were occurred with NER model (no white spaces). Moreover, Parser is not being trained, but only ner. It is also a bit strange, as it crashes after several successful epochs, so the model is capable of learning from data, but then something is happening and then it crashes. Very strange. Thanks in advance.\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.15.0-70-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-12-21T23:18:44Z", "fixed_by": "#4853", "pull_request_summary": "Warn for punctuation in entities when training with noise", "pull_request_description": "Member\nsvlandeg commented on 30 Dec 2019\nDescription\nWhen training with noise, punctuation may be converted into whitespace which can then crash the training process as this may result in invalid entities.\nThis is a quick fix in the debug_data script that alerts the user to this potential issue.\nFixes #4830\nTypes of change\nenhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-06T13:59:29Z", "files_changed": [["22", "spacy/cli/debug_data.py"], ["3", "spacy/errors.py"]]}, "53": {"issue_url": "https://github.com/explosion/spaCy/issues/4825", "issue_id": "#4825", "issue_summary": "PhraseMatcher not matching LEMMA even using 'nlp(term)'", "issue_description": "seljaseppala commented on 20 Dec 2019\nHow to reproduce the behaviour\nI have the following function to process lists of terms with the PhraseMatcher:\ndef add_patterns_to_phrasematcher(nlp, term_list, attr=None):\n \n    if attr is None:\n        nlp = English()\n\n    matcher = PhraseMatcher(nlp.vocab, attr, validate=True)\n    \n    # If default PhraseMatcher (i.e., attr='ORTH')\n    if attr is None:\n        term_patterns_list = [nlp.make_doc(term) for term in term_list]\n\n    # If attribute is specified\n    else:\n        term_patterns_list = [nlp(term) for term in term_list]\n\n    # Add term patterns in list to PhraseMatcher\n    term_label = 'USE_' + str(attr)\n    matcher.add(term_label, None, *term_patterns_list)\n\n    return matcher\nThe idea is to call the function with different attributes to create different matchers, and then combine and filter the matches output by matchers.\nHowever, when using the PhraseMatcher with attr=LEMMA and nlp = spacy.load('en_core_web_md', disable=['ner']), the returned matches are the same as when using it without any attributes (with the default ORTH) and nlp = English().\nThis seems to be confirmed by the warning message that is displayed when running the PhraseMatcher with attr=LEMMA:\nUserWarning: [W012] A Doc object you're adding to the PhraseMatcher for pattern 'USE_LEMMA' is parsed and/or tagged, but to match on 'ORTH', you don't actually need this information. This means that creating the patterns is potentially much slower, because all pipeline components are applied. To only create tokenized Doc objects, try using `nlp.make_doc(text)` or process all texts as a stream using `list(nlp.tokenizer.pipe(all_texts))`.\nSimilar issues were already reported on the issue tracker, the closest one being #4100, but the solutions don't seem to be working in my case.\nWould this be an issue with the PhraseMatcher or with the way I am using it?\nYour Environment\nspaCy version: 2.2.3\nPlatform: Darwin-19.2.0-x86_64-i386-64bit\nPython version: 3.7", "issue_status": "Closed", "issue_reporting_time": "2019-12-20T14:48:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "54": {"issue_url": "https://github.com/explosion/spaCy/issues/4824", "issue_id": "#4824", "issue_summary": "Matching on lemma without POS tagger", "issue_description": "Contributor\nmr-bjerre commented on 20 Dec 2019 \u2022\nedited\nI am getting the following error when running a matcher on spacy.lang.en.English.\nValueError: [E155] The pipeline needs to include a tagger in order to use Matcher or PhraseMatcher with the attributes POS, TAG, or LEMMA. Try using nlp() instead of nlp.make_doc() or list(nlp.pipe()) instead of list(nlp.tokenizer.pipe()).\nThat puzzles me since Token.lemma_ does exist on the tokens, e.g. does the following yield True\nnlp('Dec.')[0].lemma_ == 'December'\nI am aware that you probably shouldn't be able to match on lemma but I think the above is still confusing. I assume lemma is replaced by norm, maybe?\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nPython version: 3.7.5", "issue_status": "Closed", "issue_reporting_time": "2019-12-20T12:41:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "55": {"issue_url": "https://github.com/explosion/spaCy/issues/4823", "issue_id": "#4823", "issue_summary": "CLI spacy train fails with large amount of data", "issue_description": "kormilitzin commented on 19 Dec 2019\nI am training NER model with 7 categories and the data set contains 200K examples (texts) with average 60K annotated spans per category. However spacy train fails if I use all data. When I randomly subsample, then it works normally. The error I receive when use all data:\n$ python -m spacy train en ....\nTraining pipeline: ['ner']\nStarting with blank model 'en'\nCounting training words (limit=0)\nTraceback (most recent call last):\nFile \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\nexec(code, run_globals)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/main.py\", line 33, in\nplac.call(commands[command], sys.argv[1:])\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py\", line 367, in call\ncmd, result = parser.consume(arglist)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py\", line 232, in consume\nreturn cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/cli/train.py\", line 230, in train\ncorpus = GoldCorpus(train_path, dev_path, limit=n_examples)\nFile \"gold.pyx\", line 224, in spacy.gold.GoldCorpus.init\nFile \"gold.pyx\", line 235, in spacy.gold.GoldCorpus.write_msgpack\nFile \"gold.pyx\", line 280, in read_tuples\nFile \"gold.pyx\", line 545, in read_json_file\nFile \"gold.pyx\", line 592, in _json_iterate\nOverflowError: value too large to convert to int\nIs there any way to overcome this problem? Thanks.", "issue_status": "Closed", "issue_reporting_time": "2019-12-19T17:04:25Z", "fixed_by": "#4827", "pull_request_summary": "facilitate larger training files", "pull_request_description": "Member\nsvlandeg commented on 21 Dec 2019\nDescription\nFacilitate large training file, but also throw a warning to point towards the possibility of splitting up into multiple files.\nFixes #4703\nFixes #4823\nTypes of change\nenhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-21T20:12:19Z", "files_changed": [["4", "spacy/errors.py"], ["10", "spacy/gold.pyx"]]}, "56": {"issue_url": "https://github.com/explosion/spaCy/issues/4822", "issue_id": "#4822", "issue_summary": "spacy-lookups-data conda package creates spacy_lookups entry points as console scripts", "issue_description": "geektoni commented on 19 Dec 2019\nHow to reproduce the behaviour\nI have installed spaCy inside my conda environment by following the instructions provided by the official website https://spacy.io/usage. I obviously selected the instructions for conda. However, I have noticed that the spaCy package causes a conflict with the bash command tr. Below the error that appears:\n(spacy) geektoni@uriel:~/Github/spacy$ tr\nTraceback (most recent call last):\n  File \"/home/geektoni/miniconda3/envs/spacy/bin/tr\", line 10, in <module>\n    sys.exit(tr())\nTypeError: 'dict' object is not callable\nThis happens because spaCy install inside the bin directory of the conda environment an executable called tr and this will be called instead of the \"standard\" tr (calling which tr results in /home/geektoni/miniconda3/envs/spacy/bin/tr instead of /usr/bin/tr)\nThe following steps can be used to reproduce the error:\nconda create -n spacy python=3.6\nconda activate spacy\nconda install -c conda-forge spacy\nconda install -c conda-forge spacy-lookups-data\npython -m spacy download en_core_web_sm\ntr # This will cause the error\nYour Environment\nOperating System: Ubuntu 18.04\nspaCy version: 2.2.3\nPlatform: Linux-5.0.0-37-generic-x86_64-with-debian-buster-sid\nPython version: 3.6.9", "issue_status": "Closed", "issue_reporting_time": "2019-12-19T15:36:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "57": {"issue_url": "https://github.com/explosion/spaCy/issues/4820", "issue_id": "#4820", "issue_summary": "Tokenizer doesn't split on '//'", "issue_description": "mapadofu commented on 19 Dec 2019 \u2022\nedited\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_md')\n>>> doc = nlp(\"These//are//not//split   this/is\")\nMy expected behavior is that all of the words would be split from one another.\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.4.0-169-generic-x86_64-with-Ubuntu-16.04-xenial\nPython version: 3.6.9", "issue_status": "Closed", "issue_reporting_time": "2019-12-19T02:02:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "58": {"issue_url": "https://github.com/explosion/spaCy/issues/4817", "issue_id": "#4817", "issue_summary": "UserWarning: [W008] Evaluating Doc.similarity based on empty vectors occurs despite using language models that have vectors", "issue_description": "venkarafa commented on 17 Dec 2019 \u2022\nedited\nI am trying to find similar words between one list of words with the the other through the .similarity function.\nAlso, after reading one suggestion to not use smaller language models like 'en_core_web_sm' for similarity purpose, I started to use \"en_core_web_lg\" and \"en_vectors_web_lg\".\nThe warning \"UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\" occurs despite using language models \"en_core_web_lg\" and \"en_vectors_web_lg\". I have successfully suppressed the warning by setting the environment variable ['SPACY_WARNING_IGNORE'] = 'W008'. But intermittently the variable setting seems to be ignored and the code takes longer time to render the results.\nCan this issue be resolved pls ?\nHow to reproduce the behaviour\nimport en_vectors_web_lg\nnlp = en_vectors_web_lg.load()\nlistx =['HSBC', 'JP Morgan',......] #500 words lists\nlisty = ['Currency','Blockchain'.......] #1000 words lists\ns_words = []\nfor token1 in listy:\n    list_to_sort = [] \n    for token2 in listx:    \n        list_to_sort.append((token1, token2,nlp(str(token1)).similarity(nlp(str(token2)))))\n        sorted_list = sorted(list_to_sort, key = itemgetter(2), reverse=True)[0][:2]\n        s_words.append(sorted_list)\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.2.3\nIDE: spyder", "issue_status": "Closed", "issue_reporting_time": "2019-12-17T18:03:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "59": {"issue_url": "https://github.com/explosion/spaCy/issues/4816", "issue_id": "#4816", "issue_summary": "Spacy 2.2.3 is not predicting any values", "issue_description": "jerilkuriakose commented on 17 Dec 2019\nI have the following code to train ner using spacy for our custom dataset.\nspacy.prefer_gpu()\nif model_dir:\n    print('loading an existing model')\n    nlp = spacy.load(model_dir)\nelse:\n    print('creating a new model')\n    nlp = spacy.blank('en')  # create blank Language class\n# create the built-in pipeline components and add them to the pipeline\n# nlp.create_pipe works for built-ins that are registered with spaCy\nif 'ner' not in nlp.pipe_names:\n    ner = nlp.create_pipe(pipe_name)\n    nlp.add_pipe(ner, last=True)\n# otherwise, get it so we can add labels\nelse:\n    ner = nlp.get_pipe(\"ner\")\n# add labels\nfor __, annotations in TRAIN_DATA:\n    for ent in annotations.get('entities'):\n        ner.add_label(ent[2])\n\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    if not model_dir:\n        print('Starting a new training')\n        optimizer = nlp.begin_training()\n    else:\n        optimizer = nlp.entity.create_optimizer()\n    for itn in range(iterations):\n        print(\"Starting iteration \" + str(itn + 1))\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        batches = minibatch(TRAIN_DATA,\n                            size=compounding(4.0, 32.0, 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(\n                texts,  # batch of texts\n                annotations,  # batch of annotations\n                drop=0.2,  # dropout - make it harder to memorise data\n                sgd=optimizer,  # callable to update weights\n                losses=losses)\n        print(losses)\n        # if losses[pipe_name] < constants.LOSSES_THRESHOLD:\n        #     break\n    print('Completed training')\nAnd following the code for prediction:\nnlp = spacy.load(model_dir)\ndoc = nlp(page_data)\nnlp_output = [(ent.text, ent.label_) for ent in doc.ents]\nNow we used the above mentioned code in two spacy versions: spacy 2.1.8 and spacy 2.2.3, and following are the results:\nTasks Version 2.1.8 Version 2.2.3\nDataset size 10 10\nIterations 500 500\nTraining Success Success\nPrediction Success Did not predict a single entity\nLoss 20.35429 1056.54712\nOperating System: Windows 7\nPython Version Used: 3.6.5\nWe have also noticed the losses are decreasing at a good rate in spacy 2.1.8, whereas it takes more iterations in spacy 2.2.3.\nTo summarise Spacy 2.2.3 did not return any predictions whereas we were getting the expected results in Spacy 2.1.8, what might be the issue, kindly help.\n1", "issue_status": "Closed", "issue_reporting_time": "2019-12-17T08:36:24Z", "fixed_by": "explosion/thinc#149", "pull_request_summary": "Add __reduce__ to Tokenizer so that English pickles.", "pull_request_description": "Contributor\nchrisdubois commented on 24 Oct 2015\nAdd tests to test_pickle and test_tokenizer that save to tempfiles.", "pull_request_status": "Merged", "issue_fixed_time": "2015-10-25T04:30:31Z", "files_changed": [["1", "spacy/tokenizer.pxd"], ["10", "spacy/tokenizer.pyx"], ["18", "tests/test_pickle.py"], ["15", "tests/tokenizer/test_tokenizer.py"]]}, "60": {"issue_url": "https://github.com/explosion/spaCy/issues/4814", "issue_id": "#4814", "issue_summary": "Phrasematcher not always matching (overlapping) patterns", "issue_description": "carschno commented on 16 Dec 2019 \u2022\nedited\nHow to reproduce the behaviour\ndef _matcher(spacy_model: Language) -> PhraseMatcher:\n    known_entities: Dict[str, Set[str]] = {\n        \"PERSON\": {\"Mr. G.B.H. Smith:, \"G.B.H. Smith\", \"Smith\", ...},\n        \"ORG\":  {...},\n        \"LOC\":  {...},\n    }\n    matcher: PhraseMatcher = PhraseMatcher(spacy_model.vocab, attr=\"TEXT\")\n    for ne_type, entities in known_entities.items():\n        patterns = spacy_model.tokenizer.pipe(entities)\n        matcher.add(ne_type, patterns)\n    return matcher\n\nmatcher = _matcher(spacy_model)\n\ndocs = [spacy_model(\"This is Mr. G.B.H. Smith\"), ...]\nfor doc in docs:\n            entities = [\n                Span(sent, match_start, match_end, match_id)\n                for match_id, match_start, match_end in matcher(sent)\n            ]\nAt this point, I've set a pdb breakpoint because the results were unexpected in some cases (no matches):\nipdb> doc\nDocument name\nThis is Mr. G.B.H. Smith\nipdb> matcher(spacy_model(doc.text))\n[(380, 5, 8), (380, 6, 8), (380, 7, 8)]\nipdb> matcher(doc)\n[]\nmatcher(doc) (the last call) is expected to have the same output as the one before.\nThis is reproducible for this particular example, but hard to find out which patterns are affected exactly.\nNote that I had to anonymize the example name.\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-4.15.0-72-generic-x86_64-with\nPython version: 3.6.9\nOperating System: Ubuntu Linux 18.04.3\nOperating System: Alpine 3.10.3 (running in Docker)", "issue_status": "Closed", "issue_reporting_time": "2019-12-16T15:07:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "61": {"issue_url": "https://github.com/explosion/spaCy/issues/4812", "issue_id": "#4812", "issue_summary": "Initializing language with a vocab is not working", "issue_description": "oliviercwa commented on 16 Dec 2019\nHow to reproduce the behavior\n1- Initialize a Language class with a vocab.\n2- Add NER component\n3- run on one sentence\n(Code below / Method 1)\nResult: No Named Entity detected\nExpected: NER to work, and no difference between the two methods\nNote: interestingly enough, if you assign the vocab property directly (method 2 below), it works. I am not quite sure why.\nimport spacy\nimport os\nimport distutils.sysconfig\nfrom spacy.vocab import Vocab\nfrom spacy.language import Language\nfrom pathlib import Path\n\nmodel_path = os.path.join(distutils.sysconfig.get_python_lib(),'en_core_web_sm', 'en_core_web_sm-2.1.0')\nvocab_path = os.path.join(model_path, 'vocab')\nner_path = os.path.join(model_path, 'ner')\nsentence = \"John Smith went to Berlin\"\n\nvocab = spacy.vocab.Vocab()\nvocab.from_disk(vocab_path)\n\n# Method 1: creating language with vocab ==> Named entities is empty\nnlp = Language(vocab)\nsentencizer = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sentencizer, first=True)\n\nner_pipe = nlp.create_pipe('ner')\nner_pipe.from_disk(Path(ner_path), exclude=['vocab'])\nnlp.add_pipe(ner_pipe, last=True)\nmethod1 = nlp(sentence)\nassert(method1.ents != ())  # ASSERT TRIGGERS\n\n#Method 2: assigning vocab ==> Named entity works as expected\nnlp2 = Language()\nnlp2.vocab = vocab\nsentencizer = nlp2.create_pipe('sentencizer')\nnlp2.add_pipe(sentencizer, first=True)\n\nner_pipe2 = nlp2.create_pipe('ner')\nner_pipe2.from_disk(Path(ner_path), exclude=['vocab'])\nnlp2.add_pipe(ner_pipe2, last=True)\nmethod2 = nlp2(sentence)\nassert(method2.ents != ())  # ASSERT DOES NOT TRIGGER\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-16T14:06:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "62": {"issue_url": "https://github.com/explosion/spaCy/issues/4810", "issue_id": "#4810", "issue_summary": "Spacy reload models each time I hit ctrl+s", "issue_description": "chaouiy commented on 16 Dec 2019 \u2022\nedited\nIs it normal to wait for 10 seconds (time to load models) each time I hit \"ctrl+s\" to save file in dev environment because of server reload (I use spacy inside django project) ?", "issue_status": "Closed", "issue_reporting_time": "2019-12-15T19:56:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "63": {"issue_url": "https://github.com/explosion/spaCy/issues/4809", "issue_id": "#4809", "issue_summary": "Inconsistent return value type for .vector on GPU", "issue_description": "Contributor\njarib commented on 16 Dec 2019\nUsing a GPU, the return value of .vector is inconsistent depending on whether the token in question has a vector or not. Example:\n>>> type(nlp('test').vector)\n<class 'cupy.core.core.ndarray'>\n>>> type(nlp('testasdasdasd').vector)\n<class 'numpy.ndarray'>\nspaCy version: 2.2.2\nPlatform: Linux-5.0.0-1026-gcp-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8", "issue_status": "Closed", "issue_reporting_time": "2019-12-15T19:00:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "64": {"issue_url": "https://github.com/explosion/spaCy/issues/4808", "issue_id": "#4808", "issue_summary": "locate start dates", "issue_description": "yishairasowsky commented on 15 Dec 2019 \u2022\nedited\nIs there a way to write a rule based system to catch things like start/end dates from a contract text. Here are a few real examples. I am bolding the date entities which I want spacy to automatically detect. If you have other ideas different than spacy that is also OK!\nThe initial term of this Lease shall be for a period of Five (5) years commencing on\nFebruary 1, 2012, (the \u201cLease Commencement Date\u201d) and expiring on January 31, 2017\n(the \u201cInitial Lease Term\u201d).\nTerm: One (1) year commencing January 1, 2007 (\"Commencement Date\") and ending\nDecember 31, 2007 (\"Expiration Date\").\nThis Lease Agreement is entered into for term of 15 years, beginning January 1, 2014 and ending on December 31, 2028.\nYour Environment\nOperating System: windows 10\nPython Version Used: 3.7.4\nspaCy Version Used: 2.1.9\nEnvironment Information: I am not sure what this means... I use VS Code. Does that help you?", "issue_status": "Closed", "issue_reporting_time": "2019-12-15T13:11:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "65": {"issue_url": "https://github.com/explosion/spaCy/issues/4807", "issue_id": "#4807", "issue_summary": "embedded xml tags", "issue_description": "moh55m55 commented on 15 Dec 2019 \u2022\nedited\nHello all,\nI am just wondering if anyone has an idea of how to tokenize the below text without considering the xml elements but keeping the attribute values using set_extension for the corresponding token. For example, I want to save the attribute values (i.e. tid, type,etc) for \"now\" token. Any suggestion?\ne.g. parsing the below text and not considering the xml element as a token\n The Navy has changed its account of the attack on the USS Cole in Yemen.\n Officials <TIMEX3 tid=\"t1\" type=\"DATE\" value=\"PRESENT_REF\" temporalFunction=\"true\" anchorTimeID=\"t0\">now</TIMEX3> say the ship was hit <TIMEX3 tid=\"t2\" type=\"DURATION\" value=\"PT2H\">nearly two hours </TIMEX3>after it had docked.\n Initially the Navy said the explosion occurred while several boats were helping\n the ship to tie up. The change raises new questions about how the attackers\n were able to get past the Navy security.\n, but saving the attributes values as well:\nToken.set_extension('tid',force=True, default=None)\nToken.set_extension('type',force=True, default=None )\nnow._.tid=\"t1\"\nnow._.type=Date", "issue_status": "Closed", "issue_reporting_time": "2019-12-15T04:00:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "66": {"issue_url": "https://github.com/explosion/spaCy/issues/4806", "issue_id": "#4806", "issue_summary": "Merging spans after creating custom attributes", "issue_description": "Fourthought commented on 14 Dec 2019 \u2022\nedited\nI've been using the code examples from the documentation, particularly the 'Custom pipeline components and attribute extensions via a REST API' example. After creating the custom attribute extension there is a section of the code for creating the new spans as follows:\nfor span in spans:\n    # Iterate over all spans and merge them into one token. This is done\n    # after setting the entities \u2013 otherwise, it would cause mismatched\n    # indices!\n    span.merge()\nI see, however, that merge() has been deprecated, so have developed the following (rather inelegant) solution for the same task:\nfor span in spacy.util.filter_spans(spans):     ## span is a list of spans with custom attribute\n    with doc.retokenize() as retokenizer:\n        retokenizer.merge(span)                 ## merge the spans\n\n        doc._.named_concepts = spacy.util.filter_spans(list(doc._.named_concepts) + [span])     ## filter the list of `docs.ents` and new spans to resolve any overlapping entities\nspans is a list object containing the spans derived from the matcher\nThis code is throwing up the following error message:\nIndexError: [E036] Error calculating span: Can't find a token starting at character offset 1992.\nHave also tried this code:\ndoc._.named_concepts = spacy.util.filter_spans(list(doc._.named_concepts) + spans) # concept span\n        \n        for span in spacy.util.filter_spans(spans):\n            with doc.retokenize() as retokenizer:\n                retokenizer.merge(span)\n...but have got the same result.\nIs there a more elegant solution to allow the creation of custom extensions please?\nI would use the Entity Ruler, but am looking to modify the spans as they are created.\nWhich page or section is this issue related to?\n(https://spacy.io/usage/examples)", "issue_status": "Closed", "issue_reporting_time": "2019-12-13T21:18:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "67": {"issue_url": "https://github.com/explosion/spaCy/issues/4805", "issue_id": "#4805", "issue_summary": "Why Spacy always recognising apple as an Organisation", "issue_description": "vkancharla5 commented on 13 Dec 2019\nOperating System: Windows\nPython Version Used: 3.7.4\nspaCy Version Used: 2.2.1\nEnvironment Information: Ipython notebook\nIn named entity recognition Spacy always recognizing apple as an organization, never recognizing as a fruit.\nI tried the below code:\nimport spacy\nnlp=spacy.load('en_core_web_lg')\ndoc= nlp(u'Apple is my favourite fruit , it's cost around \\u20B9 10 in India')\nfor token in doc:\nprint(token,end='|')\nprint('\\n....')\nfor ent in doc.ents:\nprint(ent.text+'-'+ent.label_+'-'+str(spacy.explain(ent.label_)))\nWhy Spacy is not recognizing apple as fruit and \u20b910 as monetary value.\nPlease help me on this.\nRegards,\nVenkat", "issue_status": "Closed", "issue_reporting_time": "2019-12-13T16:36:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "68": {"issue_url": "https://github.com/explosion/spaCy/issues/4803", "issue_id": "#4803", "issue_summary": "Compatibility with allennlp", "issue_description": "moh55m55 commented on 13 Dec 2019\nHello,\nI installed spacy==2.1.9 and the corresponding en model from :\nhttps://pypi.org/project/spacy/2.1.9/\nand the reason is just to be compatible with allennlp.\nhowever, I got this error :\nnlp = spacy.load(\"en_core_web_lg\")\n  File \"/Users/aldawsari/opt/miniconda3/envs/background/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2465, in require\n    items = working_set.resolve(reqs, env, installer, extras=self.extras)\n  File \"/Users/aldawsari/opt/miniconda3/envs/background/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 791, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.VersionConflict: (spacy 2.1.9 (/Users/aldawsari/opt/miniconda3/envs/background/lib/python3.6/site-packages), Requirement.parse('spacy<2.3.0,>=2.2.1'))\nany idea?\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-13T01:03:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "69": {"issue_url": "https://github.com/explosion/spaCy/issues/4802", "issue_id": "#4802", "issue_summary": "Can token_match handle the complex regex without prefix/suffix_search overriding", "issue_description": "attenton commented on 12 Dec 2019\nI'm using spacy to do customized tokenizer. I want to take the api name as one token. Such as srcs[offset].remaining() in sentence:Up to the first srcs[offset].remaining() bytes of this sequence are written from buffer srcs[offset].\nI have added a token_match to tokenizer, however it was overridedd by suffixes.\nThe code is shown below:\nimport re\nimport spacy\nfrom spacy.tokenizer import Tokenizer\n\ndef customize_tokenizer_api_name_recognition(nlp):\n    # add api_name regex, such aaa.bbb(cccc) or aaa[bbb].ccc(ddd)\n    api_name_match = re.compile(\"(\\w+(\\[[\\w+-]+\\])?\\.)+\\w+\\(.*?\\)\", re.UNICODE).match\n    nlp.tokenizer.token_match = api_name_match\n\nif __name__ == '__main__':\n    nlp = spacy.load('en_core_web_sm')\n    customize_tokenizer_api_name_recognition(nlp)\n    sentence = \"Up to the first srcs[offset].remaining() bytes of this sequence are written from buffer srcs[offset].\"\n    doc = nlp(sentence)\n    print([token.text for token in doc])\n    # output:  ['Up', 'to', 'the', 'first', 'srcs[offset].remaining', '(', ')', 'bytes', 'of', 'this', 'sequence', 'are', 'written', 'from', 'buffer', 'srcs[offset', ']', '.']\n    # expected output: ['Up', 'to', 'the', 'first', 'srcs[offset].remaining()', 'bytes', 'of', 'this', 'sequence', 'are', 'written', 'from', 'buffer', 'srcs[offset', ']', '.']\nI have read some related issues #4573, #4645 and the doc. However all the examples are simple regex, and they were solved by removing some prefixes_search. what about the complex regex? How to solve this type of problem?\nYour Environment\nOperating System: Windows 10 18362.476\nPython Version Used: python 3.7.3\nspaCy Version Used: spacy 2.2.3\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-12T14:34:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "70": {"issue_url": "https://github.com/explosion/spaCy/issues/4800", "issue_id": "#4800", "issue_summary": "NER with spaCy Bert", "issue_description": "darshan99 commented on 12 Dec 2019 \u2022\nedited\nHow to reproduce the behaviour\n!pip install spacy-transformers\n!python -m spacy download en_trf_bertbaseuncased_lg\nimport spacy\nfrom spacy.util import minibatch\nimport random\nimport torch\nner = nlp.create_pipe(\"trf_ner\", config={\"exclusive_classes\": True})\nError :\nComponent named \"trf_ner\" not found , where as i see it in lib files\nYour Environment\nOperating System: Windows\nPython Version Used: 3.7\nspaCy Version Used: 2.0\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-12T10:10:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "71": {"issue_url": "https://github.com/explosion/spaCy/issues/4798", "issue_id": "#4798", "issue_summary": "cupy.cuda.cublas.CUBLASError: CUBLAS_STATUS_NOT_INITIALIZED", "issue_description": "erotavlas commented on 11 Dec 2019 \u2022\nedited\nAfter installing spacy 2.2.3 using this command pip install -U spacy[cuda90] I get the following error when I run my training script\nError\nTraceback (most recent call last):\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\multiprocessing\\pool.py\", line 119, in worker\nresult = (True, func(*args, **kwds))\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\multiprocessing\\pool.py\", line 47, in starmapstar\nreturn list(itertools.starmap(args[0], args[1]))\nFile \"D:\\SOURCECODE\\GitRepositories\\Named Entity Recognition\\ner_train.py\", line 95, in worker\nnlp.begin_training()\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\spacy\\language.py\", line 624, in begin_training\n**kwargs\nFile \"nn_parser.pyx\", line 632, in spacy.syntax.nn_parser.Parser.begin_training\nFile \"_parser_model.pyx\", line 276, in spacy.syntax._parser_model.ParserModel.begin_training\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\thinc\\check.py\", line 156, in checked_function\nreturn wrapped(*args, **kwargs)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\thinc\\neural_classes\\model.py\", line 124, in begin_training\nhook(self, train_X, train_y)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\spacy_ml.py\", line 151, in\n_set_dimensions_if_needed, lambda model, X, y: model.init_weights(model)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\spacy_ml.py\", line 270, in init_weights\nacts1 = predict(ids, tokvecs)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\spacy_ml.py\", line 250, in predict\nhiddens = model(tokvecs[:-1]) # (nW, f, o, p)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\thinc\\neural_classes\\model.py\", line 169, in call\nreturn self.predict(x)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\thinc\\neural_classes\\model.py\", line 133, in predict\ny, _ = self.begin_update(X, drop=None)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\spacy_ml.py\", line 179, in begin_update\nX, self.W.reshape((self.nF * self.nO * self.nP, self.nI)), trans2=True\nFile \"ops.pyx\", line 986, in thinc.neural.ops.CupyOps.gemm\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\cupy\\linalg\\product.py\", line 35, in dot\nreturn a.dot(b, out)\nFile \"cupy\\core\\core.pyx\", line 1364, in cupy.core.core.ndarray.dot\nFile \"cupy\\core\\core.pyx\", line 2250, in cupy.core.core.dot\nFile \"cupy\\core\\core.pyx\", line 2619, in cupy.core.core.tensordot_core\nFile \"cupy\\cuda\\device.pyx\", line 44, in cupy.cuda.device.get_cublas_handle\nFile \"cupy\\cuda\\device.pyx\", line 45, in cupy.cuda.device.get_cublas_handle\nFile \"cupy\\cuda\\device.pyx\", line 204, in cupy.cuda.device.Device.cublas_handle.get\nFile \"cupy\\cuda\\device.pyx\", line 191, in cupy.cuda.device.Device._get_handle\nFile \"cupy\\cuda\\device.pyx\", line 192, in cupy.cuda.device.Device._get_handle\nFile \"cupy\\cuda\\cublas.pyx\", line 318, in cupy.cuda.cublas.create\nFile \"cupy\\cuda\\cublas.pyx\", line 322, in cupy.cuda.cublas.create\nFile \"cupy\\cuda\\cublas.pyx\", line 311, in cupy.cuda.cublas.check_status\ncupy.cuda.cublas.CUBLASError: CUBLAS_STATUS_NOT_INITIALIZED\n\"\"\"\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \"D:\\SOURCECODE\\GitRepositories\\Named Entity Recognition\\ner_kfoldcv.py\", line 247, in\nplac.call(main)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\plac_core.py\", line 367, in call\ncmd, result = parser.consume(arglist)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\site-packages\\plac_core.py\", line 232, in consume\nreturn cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"D:\\SOURCECODE\\GitRepositories\\Named Entity Recognition\\ner_kfoldcv.py\", line 86, in main\nresults = pool.starmap(ner_train.worker, args)\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\multiprocessing\\pool.py\", line 274, in starmap\nreturn self._map_async(func, iterable, starmapstar, chunksize).get()\nFile \"D:\\Anaconda\\envs\\spacy223\\lib\\multiprocessing\\pool.py\", line 644, in get\nraise self._value\ncupy.cuda.cublas.CUBLASError: CUBLAS_STATUS_NOT_INITIALIZED\nYour Environment\nOperating System: Windows 10 Pro x64\nPython Version Used: 3.6\nspaCy Version Used: 2.2.3 gpu version installed using pip install -U spacy[cuda90]\nEnvironment Information:\nname: spacy22\nchannels:\nanaconda\nconda-forge\ndefaults\ndependencies:\ncertifi=2019.11.28=py36_0\npip=19.3.1=py36_0\npython=3.6.9=h5500b2f_0\nsetuptools=42.0.2=py36_0\nsqlite=3.30.1=hfa6e2cd_0\nvc=14.1=h21ff451_3\nvs2015_runtime=15.5.2=3\nwheel=0.33.6=py36_0\nwincertstore=0.2=py36h7fe50ca_0\npip:\nblis==0.4.1\ncatalogue==0.0.8\nchardet==3.0.4\ncupy-cuda92==7.0.0\ncymem==2.0.3\nfastrlock==0.4\nidna==2.8\nimportlib-metadata==1.3.0\nmore-itertools==8.0.2\nmurmurhash==1.0.2\nnumpy==1.17.4\nplac==1.1.3\npreshed==3.0.2\nrequests==2.22.0\nsix==1.13.0\nspacy==2.2.3\nsrsly==0.2.0\nthinc==7.3.1\ntqdm==4.40.2\nurllib3==1.25.7\nwasabi==0.4.2\nzipp==0.6.0\nprefix: D:\\Anaconda\\envs\\spacy22", "issue_status": "Closed", "issue_reporting_time": "2019-12-11T17:38:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "72": {"issue_url": "https://github.com/explosion/spaCy/issues/4797", "issue_id": "#4797", "issue_summary": "Missing `doc.is_sentenced` in `Sentencizer` requirements", "issue_description": "Contributor\ntamuhey commented on 11 Dec 2019\nspaCy/spacy/pipeline/pipes.pyx\nLine 1423 in 38e1bc1\n @component(\"sentencizer\", assigns=[\"token.is_sent_start\", \"doc.sents\"]) \nI think this line should be\n@component(\"sentencizer\", assigns=[\"token.is_sent_start\", \"doc.sents\", \"doc.is_sentenced\"])", "issue_status": "Closed", "issue_reporting_time": "2019-12-11T17:09:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "73": {"issue_url": "https://github.com/explosion/spaCy/issues/4795", "issue_id": "#4795", "issue_summary": "Can I select a word as ROOT before dependency parsing?", "issue_description": "JScarlet commented on 11 Dec 2019\nHow to reproduce the behaviour\nI'm using the dependency parser to parse a set of sentences with the same verb. I wanted the results that the ROOT of every sentence is always the same verb. For example, I hope the ROOTs of the two sentences \"pin shortcuts on a package.\" and \"pins a contact at a provided position.\" is \"pin\". However, the ROOT of the first sentence is \"shortcuts\". \"pin\" is labeled as a \"NOUN\", and the dep is \"amod\". The second sentence is what I want.\nSo I use the add_special_case method of the tokenizer, and label the word \"pin\" as a \"VERB\" before parsing. However, it just changes the POS tag and the dep is still \"amod\".\nMy codes and outputs look like the following:\nimport spacy\nfrom spacy.attrs import ORTH, LEMMA, POS, TAG\n\nnlp = spacy.load('en')\nsentence1 = 'pin shortcuts on a package.'\nsentence2 = 'pins a contact at a provided position'\n\nnlp_text1 = nlp(sentence1)\nnlp_text2 = nlp(sentence2)\n\nfor token in nlp_text1:\n    print(token.text, token.pos_, token.dep_)\n\nprint('---------')\n\nfor token in nlp_text2:\n    print(token.text, token.pos_, token.dep_)\n\nprint(\"============\")\n\nspecial_case1 = [{ORTH: 'pin', LEMMA: 'pin', POS: 'VERB', TAG: 'VERB'}]\nspecial_case2 = [{ORTH: 'pins', LEMMA: 'pin', POS: 'VERB', TAG: 'VERB'}]\nnlp.tokenizer.add_special_case('pin', special_case1)\nnlp.tokenizer.add_special_case('pins', special_case2)\n\nnew_nlp_text1 = nlp(sentence1)\nnew_nlp_text2 = nlp(sentence2)\n\nfor token in new_nlp_text1:\n    print(token.text, token.pos_, token.dep_)\n\nprint('-------')\n\nfor token in new_nlp_text2:\n    print(token.text, token.pos_, token.dep_)\npin NOUN amod\nshortcuts NOUN ROOT\non ADP prep\na DET det\npackage NOUN pobj\n. PUNCT punct\n---------\npins VERB ROOT\na DET det\ncontact NOUN dobj\nat ADP prep\na DET det\nprovided VERB amod\nposition NOUN pobj\n============\npin VERB amod\nshortcuts NOUN ROOT\non ADP prep\na DET det\npackage NOUN pobj\n. PUNCT punct\n-------\npins VERB ROOT\na DET det\ncontact NOUN dobj\nat ADP prep\na DET det\nprovided VERB amod\nposition NOUN pobj\nSo I wonder whether I can select a word like \"pin\" as a ROOT before dependency parsing, like what the add_special_case method of the tokenizer does?\nYour Environment\nOperating System: macOS Catalina 10.15.1\nPython Version Used: python 3.7\nspaCy Version Used: 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-11T15:35:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "74": {"issue_url": "https://github.com/explosion/spaCy/issues/4788", "issue_id": "#4788", "issue_summary": "Shipping model with custom tokenizer and custom preprocessor", "issue_description": "Contributor\nmr-bjerre commented on 10 Dec 2019 \u2022\nedited\nI'd like to ship my model with a custom tokenizer but it doesn't load properly.\nfrom spacy import load\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\n\n\nclass CustomTokenizer(Tokenizer):\n    def __call__(self, text):\n        return super().__call__(text.replace(\"\\u2013\", \"-\"))\n\n\nnlp = English()\nnlp.tokenizer = CustomTokenizer(nlp.vocab)\n\nassert nlp(\"\\u2013\").text == \"-\"  # passes\n\nnlp.to_disk(\"test-model\")\nnlp = load(\"test-model\")\nassert nlp(\"\\u2013\").text == \"-\"  # fails\nHow is this achieved? I assume I have to create my own language, i.e. extend English? I tried the following as well with same result\nfrom spacy import load\nfrom spacy.lang.en import English\n\n\nclass CustomEnglish(English):\n\n    def make_doc(self, text):\n        return self.tokenizer(text.replace(\"\\u2013\", \"-\"))\n\n\nnlp = CustomEnglish()\n\nassert nlp(\"\\u2013\").text == \"-\"  # passes\n\nnlp.to_disk(\"test-model\")\nnlp = load(\"test-model\")\nassert nlp(\"\\u2013\").text == \"-\"  # fails\nI also want to ship custom components and I recall Ines writing a post with Snake but I can't find it now. Link anyone?\nBtw. I do realise that right now I only preprocess the text but I intent to change the tokenizer as well.\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nPython version: 3.7.5", "issue_status": "Closed", "issue_reporting_time": "2019-12-10T14:37:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "75": {"issue_url": "https://github.com/explosion/spaCy/issues/4786", "issue_id": "#4786", "issue_summary": "Explicitly prevent Doc.to_bytes() from saving the Vocab", "issue_description": "Contributor\nldorigo commented on 10 Dec 2019 \u2022\nedited\nHello,\nNot entirely sure that this isn't currently possible, but if it is, then this should be seen as a request for better documentation :-)\nI am doing large-scale automated processing of medical literature, and am storing the texts (and related information) in a relational database. Since parsing a text with Spacy takes a significant amount of time, I'd like to store the parsed docs as a blob of bytes alongside each text in the database.\nAt the moment, unless I misunderstand something, the choice is between serializing docs directly (with doc.to_bytes()), which saves the whole vocab for each doc, or using DocBin for a list of docs, which serializes the vocab only once. Unfortunately neither of these are optimal for my use-case: serializing the vocab in each and every doc is a huge waste of space, and DocBin only makes sense if I stored all of the docs together - which doesn't allow me to retrieve them individually.\nIdeally, it should be possible to serialize the doc without the vocab, and to add an optional parameter to Doc.from_bytes that allows using a specific vocab when deserializing.\nIf this is already possible, please let me know how and I'd be happy to add a small section in the documentation to make it clearer for future users.", "issue_status": "Closed", "issue_reporting_time": "2019-12-10T13:16:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "76": {"issue_url": "https://github.com/explosion/spaCy/issues/4785", "issue_id": "#4785", "issue_summary": "Labeling tool for dependency parser retraining", "issue_description": "vonum commented on 10 Dec 2019\nI am aware of the https://prodi.gy/ tool for labeling named entities.\nHowever, I have not found an existing tool for handling the dependency parser.\nIs there an existing tool that would solve labeling dep_ and head tags through some kind of UI (like prodigy)?\nIf there is not, how would you recommend to go on with this problem?\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-4.15.0-60-generic-x86_64-with-debian-buster-sid\nPython version: 3.6.3", "issue_status": "Closed", "issue_reporting_time": "2019-12-10T13:02:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "77": {"issue_url": "https://github.com/explosion/spaCy/issues/4784", "issue_id": "#4784", "issue_summary": "Issue with loading lookups", "issue_description": "christophermfeld commented on 10 Dec 2019\nModuleNotFoundError: No module named 'spacy.lookups'\nGetting stuck on the lookups import.\nCode Snippet:\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\nlookups = Lookups()\nlookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\nlemmatizer = Lemmatizer(lookups)\nEnvironment\nspaCy version 2.1.3\nLocation /data01/anaconda3/lib/python3.7/site-packages/spacy\nPlatform Linux-4.1.12-61.1.18.el7uek.x86_64-x86_64-with-redhat-7.6-Maipo\nPython version 3.7.3\nModels", "issue_status": "Closed", "issue_reporting_time": "2019-12-10T06:16:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "78": {"issue_url": "https://github.com/explosion/spaCy/issues/4782", "issue_id": "#4782", "issue_summary": "spacy import fail", "issue_description": "guzhang480 commented on 8 Dec 2019\nI install spacy using multiple method, all are success, like this below:\npip install -U spacy[cuda100]\npip install -U spacy\npip install spacy\n...\nSuccessfully installed spacy-2.2.3\nBut import always fail. I got error:\nAttributeError: type object 'cupy.core.core.Indexer' has no attribute 'reduce_cython'\nfull error are below (Using \"pip install spacy\")\nimport spacy\nTraceback (most recent call last):\nFile \"\", line 1, in\nimport spacy\nFile \"C:\\Users\\guzha\\anaconda3\\lib\\site-packages\\spacy_init_.py\", line 10, in\nfrom thinc.neural.util import prefer_gpu, require_gpu\nFile \"C:\\Users\\guzha\\anaconda3\\lib\\site-packages\\thinc\\neural_init_.py\", line 4, in\nfrom ._classes.model import Model # noqa: F401\nFile \"C:\\Users\\guzha\\anaconda3\\lib\\site-packages\\thinc\\neural_classes\\model.py\", line 11, in\nfrom ..train import Trainer\nFile \"C:\\Users\\guzha\\anaconda3\\lib\\site-packages\\thinc\\neural\\train.py\", line 7, in\nfrom .optimizers import Adam, linear_decay\nFile \"optimizers.pyx\", line 14, in init thinc.neural.optimizers\nFile \"ops.pyx\", line 24, in init thinc.neural.ops\nFile \"C:\\Users\\guzha\\anaconda3\\lib\\site-packages\\thinc\\neural_custom_kernels.py\", line 8, in\nimport cupy\nFile \"C:\\Users\\guzha\\anaconda3\\lib\\site-packages\\cupy_init_.py\", line 15, in\nfrom cupy import core # NOQA\nFile \"C:\\Users\\guzha\\anaconda3\\lib\\site-packages\\cupy\\core_init_.py\", line 1, in\nfrom cupy.core import core # NOQA\nFile \"cupy\\core\\carray.pxi\", line 50, in init cupy.core.core\nAttributeError: type object 'cupy.core.core.Indexer' has no attribute 'reduce_cython'\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.2\nEnvironment Information: regular installation", "issue_status": "Closed", "issue_reporting_time": "2019-12-08T14:17:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "79": {"issue_url": "https://github.com/explosion/spaCy/issues/4780", "issue_id": "#4780", "issue_summary": "Pyinstaller packaging with SpaCy: Runtime error", "issue_description": "kpburgula commented on 8 Dec 2019 \u2022\nedited\nI have successfully packaged into windows executable but I am getting a runtime error. I have used hook-en_core_web_sm.py as a hook file using --additional-hooks-dir command\nFile \"test.py\", line 1, in <module>\n  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 621, in exec_module\n    exec(bytecode, module.__dict__)\n  File \"site-packages\\spacy\\__init__.py\", line 10, in <module>\n  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 621, in exec_module\n    exec(bytecode, module.__dict__)\n  File \"site-packages\\thinc\\neural\\__init__.py\", line 4, in <module>\n  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 621, in exec_module\n    exec(bytecode, module.__dict__)\n  File \"site-packages\\thinc\\neural\\_classes\\model.py\", line 11, in <module>\n  File \"c:\\programdata\\miniconda3\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 621, in exec_module\n    exec(bytecode, module.__dict__)\n  File \"site-packages\\thinc\\neural\\train.py\", line 7, in <module>\n  File \"optimizers.pyx\", line 14, in init thinc.neural.optimizers\n  File \"ops.pyx\", line 24, in init thinc.neural.ops\nImportError: cannot import name _custom_kernels\n[23076] Failed to execute script test\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7.4\nspaCy Version Used: 2.2.3\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-08T03:29:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "80": {"issue_url": "https://github.com/explosion/spaCy/issues/4777", "issue_id": "#4777", "issue_summary": "Wrong link to user hooks in docs", "issue_description": "Contributor\nmr-bjerre commented on 6 Dec 2019\nIn the custom similarity section the link to user hooks should be\nhttps://spacy.io/usage/processing-pipelines#custom-components-user-hooks\ninstead of\nhttps://spacy.io/usage/processing-pipelines#user-hooks.", "issue_status": "Closed", "issue_reporting_time": "2019-12-06T07:31:30Z", "fixed_by": "#4778", "pull_request_summary": "Fix link to user hooks in docs", "pull_request_description": "Contributor\nmr-bjerre commented on 6 Dec 2019\nFixes #4777\nTypes of change\nBug in documentation.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-06T18:17:13Z", "files_changed": [["87", ".github/contributors/mr-bjerre.md"], ["2", "website/docs/usage/vectors-similarity.md"]]}, "81": {"issue_url": "https://github.com/explosion/spaCy/issues/4776", "issue_id": "#4776", "issue_summary": "After updated, verb pos tagging seems to yield inconsistent results", "issue_description": "carlos-takeapps commented on 6 Dec 2019 \u2022\nedited\nGiven this code...\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple is one of the most profitable companies in the world.\")\nfor token in doc:\n    print(token.text, token.pos_, token.dep_)\nthe result is\nApple PROPN nsubj\nis AUX ROOT\none NUM attr\nof ADP prep\nthe DET det\nmost ADV advmod\nprofitable ADJ amod\ncompanies NOUN pobj\nin ADP prep\nthe DET det\nworld NOUN pobj\n. PUNCT punct\nI would have expected the word is to be VERB ROOT as in\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Apple stands among the most profitable companies in the world.\")\nfor token in doc:\n    print(token.text, token.pos_, token.dep_)\nApple PROPN nsubj\nstands VERB ROOT\namong ADP prep\nthe DET det\nmost ADV advmod\nprofitable ADJ amod\ncompanies NOUN pobj\nin ADP prep\nthe DET det\nworld NOUN pobj\n. PUNCT punct", "issue_status": "Closed", "issue_reporting_time": "2019-12-05T21:26:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "82": {"issue_url": "https://github.com/explosion/spaCy/issues/4773", "issue_id": "#4773", "issue_summary": "\"Unsupported type <class 'numpy.ndarray'> spacy on GPU using python3 multiprocessing", "issue_description": "kbmlcoding commented on 5 Dec 2019 \u2022\nedited\nI am trying to run spacy similarity on bunch of documents on machine which has GPU tesla card\nand using python3 multiprocessing to run in parallel but running into below issue\n\"\"\"\nTraceback (most recent call last):\nFile \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 119, in worker\nresult = (True, func(*args, **kwds))\nFile \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 44, in mapstar\nreturn list(map(*args))\nFile \"gpu.py\", line 35, in chan_similarity\ndf['sim_measure'] = df.apply(lambda x : run_sim(x), axis=1)\nFile \"/home/kadu/venv/lib/python3.6/site-packages/pandas/core/frame.py\", line 6913, in apply\nreturn op.get_result()\nFile \"/home/kadu/venv/lib/python3.6/site-packages/pandas/core/apply.py\", line 186, in get_result\nreturn self.apply_standard()\nFile \"/home/kadu/venv/lib/python3.6/site-packages/pandas/core/apply.py\", line 292, in apply_standard\nself.apply_series_generator()\nFile \"/home/kadu/venv/lib/python3.6/site-packages/pandas/core/apply.py\", line 321, in apply_series_generator\nresults[i] = self.f(v)\nFile \"gpu.py\", line 35, in\ndf['sim_measure'] = df.apply(lambda x : run_sim(x), axis=1)\nFile \"gpu.py\", line 23, in run_sim\nreturn (nlp_glob(x['all_train_data_with_desc_trim_100_null_removed.mov_chan_desc']).similarity(nlp_glob(x['all_train_data_with_desc_trim_100_null_removed.proj_chan_desc'])))\nFile \"doc.pyx\", line 395, in spacy.tokens.doc.Doc.similarity\nFile \"doc.pyx\", line 464, in spacy.tokens.doc.Doc.vector_norm.get\nFile \"doc.pyx\", line 312, in iter\nFile \"cupy/core/core.pyx\", line 935, in cupy.core.core.ndarray.add\nFile \"cupy/core/_kernel.pyx\", line 828, in cupy.core._kernel.ufunc.call\nFile \"cupy/core/_kernel.pyx\", line 90, in cupy.core._kernel._preprocess_args\nTypeError: (\"Unsupported type <class 'numpy.ndarray'>\", 'occurred at index 2')\n\"\"\"\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\nFile \"gpu.py\", line 78, in\ndt = parallelize(chunk, chan_similarity)\nFile \"gpu.py\", line 54, in parallelize\ndata = pd.concat(pool.map(func, data_split))\nFile \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 266, in map\nreturn self._map_async(func, iterable, mapstar, chunksize).get()\nFile \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 644, in get\nraise self._value\nTypeError: (\"Unsupported type <class 'numpy.ndarray'>\", 'occurred at index 2')\nYour Environment\nOperating System: Centos\nPython Version Used: python 3.6.8\nspaCy Version Used: spacy 2.2.3\nEnvironment Information: Running on Tesla GPU card", "issue_status": "Closed", "issue_reporting_time": "2019-12-05T18:25:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "83": {"issue_url": "https://github.com/explosion/spaCy/issues/4772", "issue_id": "#4772", "issue_summary": "EntityLinker predict function loops through ents instead of sents", "issue_description": "lilysikes commented on 5 Dec 2019 \u2022\nedited\nThe predict function loops through ents twice instead of looping through the sents. The code should be\nfor sent in doc.sents:\n                    sent_doc = sent.as_doc()\n                    # currently, the context is the same for each entity in a sentence (should be refined)\n                    sentence_encoding = self.model([sent_doc])[0]\n                    xp = get_array_module(sentence_encoding)\n                    sentence_encoding_t = sentence_encoding.T\n                    sentence_norm = xp.linalg.norm(sentence_encoding_t) ...\n1", "issue_status": "Closed", "issue_reporting_time": "2019-12-05T16:00:07Z", "fixed_by": "#4779", "pull_request_summary": "fix bug in EL predict", "pull_request_description": "Member\nsvlandeg commented on 6 Dec 2019\nDescription\nFixed typo bug in the EntityLinkers's predict function - hopefully by fixing this the EL results will become more meaningful (will run experiments to test this hopeful hypothesis).\nFixes #4772 - thanks to @lilysikes for the report and fix!\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-06T18:18:15Z", "files_changed": [["2", "spacy/pipeline/pipes.pyx"]]}, "84": {"issue_url": "https://github.com/explosion/spaCy/issues/4771", "issue_id": "#4771", "issue_summary": "Installing spacy on spark kubernetes docker image takes a long time", "issue_description": "jdukatz commented on 5 Dec 2019 \u2022\nedited\nHow to reproduce the problem\nI'm running spark on a kubernetes cluster to do some data processing, so I'm using spark's recommended image to build my spark containers, which is found here:\nhttps://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/bindings/python/Dockerfile\nI want to do some distributed processing with spacy, so I added the following lines to it:\nRUN pip3 install spacy==2.1.1 \\\n    && python3 -m spacy download en\nTo be clear, spacy works just fine once the container is built, but the installation takes a long time, around 3 hours, mostly due to installing dependencies, e.g. Running setup.py install for blis: still running... (I can paste the full output if it would help).\nI've messed around with trying to add some of the dependency libraries via apk install which seems to help a bit, but it's still over 2 hours.\nSo my question is, is this something I have to live with, or is there a way to install some of the dependencies manually so it goes faster?\nYour Environment\nOperating System: alpine linux\nPython Version Used: 3\nspaCy Version Used: 2.1.1\nEnvironment Information: spark kubernetes docker image", "issue_status": "Closed", "issue_reporting_time": "2019-12-05T15:55:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "85": {"issue_url": "https://github.com/explosion/spaCy/issues/4770", "issue_id": "#4770", "issue_summary": "Text classification results (Document.cat) missing when multiprocessing enabled", "issue_description": "EdwardJB commented on 5 Dec 2019\nHow to reproduce the behaviour\nOn a model that contains some text classifiers, run this code with n_process=1. It works as expected; classifications are present:\ntext='my test text'\ndocs = self.model.pipe([text],n_process=1)\nresponse = []\nfor i, (text, doc) in enumerate(zip(texts, docs), start=1):\n    print(doc.to_json())\n{'text': 'my test text', 'sents': [{'start': 0, 'end': 12}], 'cats': {...all of my cats are here..}, 'tokens': [{'id': 0, 'start': 0, 'end': 2}, {'id': 1, 'start': 3, 'end': 7}, {'id': 2, 'start': 8, 'end': 12}]}\nNow run the same with n_process=-1, and they are missing:\ntext='my test text'\ndocs = self.model.pipe([text],n_process=-1)\nresponse = []\nfor i, (text, doc) in enumerate(zip(texts, docs), start=1):\n    print(doc.to_json())\n    print(doc.cats)\n{'text': 'my test text', 'sents': [{'start': 0, 'end': 12}], 'tokens': [{'id': 0, 'start': 0, 'end': 2}, {'id': 1, 'start': 3, 'end': 7}, {'id': 2, 'start': 8, 'end': 12}]}\n{}\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-4.4.0-18362-Microsoft-x86_64-with-glibc2.2.5\nPython version: 3.8.0", "issue_status": "Closed", "issue_reporting_time": "2019-12-05T14:27:12Z", "fixed_by": "#4774", "pull_request_summary": "Include Doc.cats in serialization of Doc and DocBin", "pull_request_description": "Collaborator\nadrianeboyd commented on 6 Dec 2019 \u2022\nedited\nDescription\nInclude Doc.cats in Doc serialization\nInclude Doc.cats in DocBin serialization\nAdd tests for Doc.cats in Doc and DocBin serialization\nFixes #4770.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-06T13:07:40Z", "files_changed": [["8", "spacy/tests/serialize/test_serialize_doc.py"], ["6", "spacy/tokens/_serialize.py"], ["4", "spacy/tokens/doc.pyx"]]}, "86": {"issue_url": "https://github.com/explosion/spaCy/issues/4768", "issue_id": "#4768", "issue_summary": "Can't load the NER model by itself without disabling the vocab", "issue_description": "oliviercwa commented on 5 Dec 2019\nHow to reproduce the behaviour\n1- Load vocab from any spacy model\n2- Instantiate EntityRecognizer with vocab\n3- Load EntityRecognizer model from disk by passing either the 'ner' folder path or the model root pat\nimport os\nimport sys\nfrom distutils.sysconfig import get_python_lib\nfrom spacy.pipeline import EntityRecognizer\nfrom spacy.vocab import Vocab\nimport spacy\n\nmodel_path = os.path.join(get_python_lib(), 'en_core_web_sm')\nvocab = Vocab()  \nvocab = vocab.from_disk(os.path.join(model_path,'en_core_web_sm-2.1.0', 'vocab'))\nner = EntityRecognizer(vocab)\nner.from_disk(os.path.join(model_path,'en_core_web_sm-2.1.0', 'ner'))\nResult:\nPassing the NER folder path ==>\n\"ValueError: Can't read file: ...\\Python\\Python37\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.1.0\\ner\\vocab\\strings.json\"\nPassing the root folder path ==>\nFileNotFoundError: [Errno 2] No such file or directory: ...\\Python\\Python37\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.1.0\\moves'\nExpected:\nBe able to load independently each component model\nNotes:\n1- the model is located in the folder and all the folders exist.\n2- You can load the full pipeline using spacy.load, but in certain scenarios we need to load independently each component model.\n3- The issue is that the EntityRecognizer needs files in both the NER folder and in the VOCAB folder and there does not seem to be a way to pass both paths.\n4- The workaround is to disable the vocab:\nner.from_disk(os.path.join(model_path,'en_core_web_sm-2.1.0', 'ner'), exclude = ['vocab'])\nYour Environment\nOperating System: Win 10\nPython Version Used: 3.7.4\nspaCy Version Used: 2.1.9\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-05T09:25:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "87": {"issue_url": "https://github.com/explosion/spaCy/issues/4766", "issue_id": "#4766", "issue_summary": "Procesing text by batch or by single string gives inconsistent doc.tensor values", "issue_description": "wavymazy commented on 5 Dec 2019\nHow to reproduce the behaviour\nSo I've been working with the en_trf_bertbaseuncased_lg model and ended up getting different classification results depending on how I processed text initially.\nIf I process text with the Language object using only 1 string\nfor ex: nlp(my_text) or nlp.pipe(my _text, batchsize=1)\nI end up with different doc.tensor had I used :\nnlp.pipe(my_texts, batchsize=500)\nI compared the resulting doc, for each string, on a 1 by 1 basis, and the resulting vectors weren'T the same. I believe this to be the reason my classification results vary, and possibly why I get bad results when ingesting the same data by batch.\nThanks for reading and please let me know if I'm missing something, or if this is expected results.\ncheers!\nYour Environment\nOperating System: ubuntu 18.04\nPython Version Used: 3.6\nspaCy Version Used: 2.2.1\nEnvironment Information: I'm working with RASA.", "issue_status": "Closed", "issue_reporting_time": "2019-12-04T22:55:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "88": {"issue_url": "https://github.com/explosion/spaCy/issues/4765", "issue_id": "#4765", "issue_summary": "Why is not doc[0].is_sent_start = True by default when you add a sentencizer", "issue_description": "joanPlepi commented on 4 Dec 2019\nHow to reproduce the behaviour\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.pipeline import Sentencizer\nfrom spacy.lang.de import German\nnlp = German()\nnlp.add_pipe(Tokenizer(nlp.vocab))\nnlp.add_pipe(Sentencizer())\nprint(nlp.pipeline) # for debugging\ndoc = nlp.make_doc(\"Er beteiligte sich in zahlreichen Gremien und Kreisen und hatte 1908 entscheidenden Anteil an der Gr\u00fcndung der Gesellschaft der B\u00fccherfreunde zu Hamburg. W\u00e4hrend des Ersten Weltkriegs diente M\u00fcnzel als Hauptmann der Landwehr.\")\nfor sent in doc.sents:\n    print(sent) \nI am trying to use spacy for the german language, however I am getting an error when I try to access to sentences and is not clear to me why as I have alread added the Sentencizer component.\nValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start.\nI even tried to add the sentencizer using the code below, but I still have the same bug.\nnlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\nYour Environment\nOperating System: Ubuntu 18.04\nPython Version Used: 3.7.0\nspaCy Version Used: 2.2.3\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-04T17:20:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "89": {"issue_url": "https://github.com/explosion/spaCy/issues/4764", "issue_id": "#4764", "issue_summary": "NER training with custom loss", "issue_description": "basque21 commented on 4 Dec 2019\nHi, I'm training my custom NER model with Spacy. Everything is OK, with the training and the model, but I have a problem: My dataset is partially labeled. So, say I have one only label. I want to weight this label in order to force model to predict it more usually although it sees examples where it's not labeled.\nI don't know if I explained properly the problem... If someone has any idea I would really appreciate it!\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-12-04T15:53:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "90": {"issue_url": "https://github.com/explosion/spaCy/issues/4759", "issue_id": "#4759", "issue_summary": "AttributeError: 'English' object has no attribute 'noun_chunks'", "issue_description": "dldisha commented on 4 Dec 2019\nI'm trying to use spacy noun_chunks.\nI downloaded the model with python -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\nnoun_chunks = nlp.noun_chunks\nbut it's throwing an error:\nAttributeError: 'English' object has no attribute 'noun_chunks'\nOperating System: Ubuntu\nPython Version Used: 3.6\nspaCy Version Used: 2.2.3\nCan somebody help me fix this issue? Thanks in advance!", "issue_status": "Closed", "issue_reporting_time": "2019-12-04T11:09:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "91": {"issue_url": "https://github.com/explosion/spaCy/issues/4758", "issue_id": "#4758", "issue_summary": "Using GPU on Windows leads to unexpected results", "issue_description": "Contributor\nBramVanroy commented on 4 Dec 2019 \u2022\nedited\nAs reported here and here, using a GPU on Windows returns unexpected parsing results. This topic was made per request of @adrianeboyd. An example of sentences with their tokenisation, unexpected POS-tags, and unexpected DEP labels:\ns = \"The decrease in 2008 primarily relates to the decrease in cash and cash equivalents 1.\\n\"\n['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\\n']\n['VERB', 'PRON', 'PROPN', 'NOUN', 'VERB', 'ADV', 'VERB', 'NUM', 'PRON', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'VERB', 'VERB', 'NOUN', 'SPACE']\n['dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'ROOT', '']\n\ns = \"The Company's current liabilities of &euro;32.6 million primarily relate to deferred income from collaborative arrangements and trade payables.\\n\"\n['The Company', \"'s\", 'current', 'liabilities', 'of', '&', 'euro;32.6', 'million', 'primarily', 'relate', 'to', 'deferred', 'income', 'from', 'collaborative', 'arrangements', 'and', 'trade', 'payables', '.', '\\n']\n['NOUN', 'VERB', 'AUX', 'NOUN', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'VERB', 'VERB', 'ADV', 'VERB', 'VERB', 'NOUN', 'NOUN', 'PROPN', 'NOUN', 'PROPN', 'VERB', 'NUM', 'NOUN', 'SPACE']\n['dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'punct', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'ROOT', '']\n\ns = 'The increase in deferred income is related to new deals with partners.\\n'\n['The', 'increase', 'in', 'deferred', 'income', 'is', 'related', 'to', 'new', 'deals', 'with', 'partners', '.', '\\n']\n['NOUN', 'PROPN', 'PROPN', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'VERB', 'NOUN', 'VERB', 'NOUN', 'SPACE']\n['dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'punct', 'dep', 'dep', 'ROOT', '']\nExample repo with data here.\nIs it perhaps possible to include Windows in the integrated testing? It might be interesting to have an idea about how many people who use this package are actually on Windows. (I use it for testing and prototyping, mainly.)\nspaCy version info\nspaCy version: 2.2.3\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.5", "issue_status": "Closed", "issue_reporting_time": "2019-12-03T19:17:13Z", "fixed_by": "explosion/thinc#149", "pull_request_summary": "Add __reduce__ to Tokenizer so that English pickles.", "pull_request_description": "Contributor\nchrisdubois commented on 24 Oct 2015\nAdd tests to test_pickle and test_tokenizer that save to tempfiles.", "pull_request_status": "Merged", "issue_fixed_time": "2015-10-25T04:30:31Z", "files_changed": [["1", "spacy/tokenizer.pxd"], ["10", "spacy/tokenizer.pyx"], ["18", "tests/test_pickle.py"], ["15", "tests/tokenizer/test_tokenizer.py"]]}, "92": {"issue_url": "https://github.com/explosion/spaCy/issues/4757", "issue_id": "#4757", "issue_summary": "Possible Memory leak in beam_parse while processing a pipe of documents.", "issue_description": "pramodith commented on 3 Dec 2019 \u2022\nedited\nDescription\nI seem to be facing a memory leak when I try to use beam_parse on a large number of documents.\nThe below code is a function that's called hundreds of times, each call to the function will pass in a dataframe containing around 5000 documents.\nI obeserve that there's a spike in memory when I call beam_parse and it continues to spike despite leaving the function and being called again. I've tried using the del command to delete my dataframe before I leave the function, using gc.collect() and even reloading the spacy model at the end of every call to the function but the problem persists.\nI've seen the thread over here (#3618) discussing the same issue but none of them seemed to mention beam_parse.\nAlso this is how I load the model :\nnlp = spacy.load(\"en_core_web_lg\", disable=['tagger', 'parser'])\nHow to reproduce the behaviour\n`\ndef ner(nlp, df, threshold=0.8):\n'''\n:param nlp: Spacy object\n:param df: Dataframe that contains the documents.\n:param threshold: Threshold score for an entity to be deemed to be a positive.\n:return:\n'''\ntry:\n print(f'Creating pipe : memory , {psutil.virtual_memory().percent}')\n docs = list(nlp.pipe(df['text'],batch_size=64,n_threads=4,disable=['ner']))\n print(f'Creating beam : memory , {psutil.virtual_memory().percent}')\n beams = nlp.entity.beam_parse(docs, beam_width=4, beam_density=0.0001)\n print(f'Processing beam : memory , {psutil.virtual_memory().percent}')\n for i, doc_beam in enumerate(zip(docs, beams)):\n  doc, beam = doc_beam\n  for score, ents in nlp.entity.moves.get_beam_parses(beam):\n   if score > threshold:\n    for start, end, label in ents:\n     print(label)\n print(f'Done : memory , {psutil.virtual_memory().percent}')\nexcept Exception as e:\n raise e\n`\nLog Statements:\nBelow are the logs for the first 5 iterations. As it goes on longer I see the continued increase in consumption of memory.\nProcessing 5000 documents.\nEntered NER : memory , 26.0\nCreating pipe : memory , 26.0\nCreating beam : memory , 26.2\nProcessing beam : memory , 28.9\nDone : memory , 28.9\nProcessing 5000 documents.\nEntered NER : memory , 28.6\nCreating pipe : memory , 28.6\nCreating beam : memory , 28.6\nProcessing beam : memory , 30.8\nDone : memory , 30.8\nProcessing 5001 documents.\nEntered NER : memory , 30.5\nCreating pipe : memory , 30.5\nCreating beam : memory , 30.5\nProcessing beam : memory , 32.4\nDone : memory , 32.3\nProcessing 5003 documents.\nEntered NER : memory , 33.9\nCreating pipe : memory , 33.9\nCreating beam : memory , 33.9\nProcessing beam : memory , 36.0\nDone : memory , 36.0\nProcessing 5004 documents.\nEntered NER : memory , 35.7\nCreating pipe : memory , 35.7\nCreating beam : memory , 35.7\nProcessing beam : memory , 37.8\nDone : memory , 37.8\nProcessing 5001 documents.\nEntered NER : memory , 37.4\nCreating pipe : memory , 37.4\nCreating beam : memory , 37.4\nProcessing beam : memory , 39.5\nDone : memory , 39.5\nEnvironment\nOperating System: Linux\nPython Version Used: 3.6.9\nspaCy Version Used: 2.2.3", "issue_status": "Closed", "issue_reporting_time": "2019-12-03T18:23:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "93": {"issue_url": "https://github.com/explosion/spaCy/issues/4756", "issue_id": "#4756", "issue_summary": "OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.", "issue_description": "dldisha commented on 3 Dec 2019\nI have installed spacy and downloaded en_core_web_sm with:\npip3 install spacy\npython3 -m spacy download en_core_web_sm\nWhen I try to run the en_core_web_sm module in Python IDE with:\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nit shows an error message:\n File \"/home/disha/Desktop/project/python/parser/venv/lib/python3.6/site-packages/spacy/__init__.py\", line 30, in load\n    return util.load_model(name, **overrides)\n  File \"/home/disha/Desktop/project/python/parser/venv/lib/python3.6/site-packages/spacy/util.py\", line 169, in load_model\n    raise IOError(Errors.E050.format(name=name))\nOSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nI have tried every possible solution which was suggested on GitHub and Stack Overflow but nothing worked\nOperating System: Ubuntu\nPython Version Used: 3.6\nspaCy Version Used: 2.2.3\nCan somebody help me fix this issue? Thanks in advance!", "issue_status": "Closed", "issue_reporting_time": "2019-12-03T17:46:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "94": {"issue_url": "https://github.com/explosion/spaCy/issues/4754", "issue_id": "#4754", "issue_summary": "PyPI distributions contain files unnecessary to the installation", "issue_description": "ddelange commented on 3 Dec 2019\nHow to reproduce the behaviour\nDownload the latest tar from PyPI and inspect\nWhen creating dists for upload to PyPI, by default all top-level files are excluded, but there are many more files included in the tar than specified in MANIFEST.in. Minimal suggestion is to remove test files from the tar by adding recursive-exclude spacy/tests/ to MANIFEST.in (which are now all installed along spacy, unnecessarily taking up space), but a safer way of setting this up would be building the tar up from scratch: starting empty, and building up selectively from there (for instance with recursive includes using graft), e.g. something like\nglobal-exclude *\ninclude setup.py\ninclude README*\ninclude LICENSE*\ngraft bin/\ngraft include/\ngraft spacy/\nrecursive-exclude spacy/tests/\nrecursive-exclude * __pycache__\ninclude pyproject.toml\nrecursive-exclude spacy/lang *.json\nrecursive-include spacy/lang *.json.gz", "issue_status": "Closed", "issue_reporting_time": "2019-12-03T15:09:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "95": {"issue_url": "https://github.com/explosion/spaCy/issues/4753", "issue_id": "#4753", "issue_summary": "Inaccurate pretrained model: conjugation adjectives [de-core-news-md][spaCy 2.18]", "issue_description": "yano27 commented on 3 Dec 2019\nHow to reproduce the behaviour\nHello,\nI'm trying to extract a list of adjectives from a sentence into a span.\nSimple setence such as \"Die Farbe sollte rot, blau oder schwarz sein\" can be extracted properly. I gained my wished result which is \"rot, blau, oder schwarz\" as the new tag/span (by using spaCy dep_ and children_)\nBut it's not working as I expected, when I added a subclause into it \"Die Farbe sollte rot, blau oder schwarz sein, aber nicht grau oder braun\"\n\nThe word \"rot\" and \"blau\" have the same dependency, which is \"pd\".\nI use \"pd\" in my code as an indicator of the (new) span starts token, because of it I gain \"rot\" and \"blau oder schwarz\" as seperated span.\nI would like to know which approach should I use fix change the word \"blau\" dependency into \"cj\", should I trained the model using what it said on this website https://spacy.io/usage/training? Thanks in advance.\nP.S : I try also following examples..\n\"Die Stadt sollte in Karlsruhe, Berlin oder Stuttgart sein, aber nicht in London oder Manchester\": accurately extracted\n\"Das System sollte schnell, einfach und robust sein, aber nicht teuer oder komplex\": inaccurate extracted\nSo maybe the issues is only for adjectives..\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.6\nspaCy Version Used: 2.1.8\nmodel Version Used: de-core-news-md 2.1.0\nEnvironment Information: Django, VisualStudio", "issue_status": "Closed", "issue_reporting_time": "2019-12-03T14:54:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "96": {"issue_url": "https://github.com/explosion/spaCy/issues/4752", "issue_id": "#4752", "issue_summary": "Wrong version when installing from pip", "issue_description": "ddelange commented on 3 Dec 2019 \u2022\nedited\nWhen installing in a fresh environment (dockerized python:3.7-slim in this case), the __version__ attribute does not correspond with pkg_resources.get_distribution('spacy').version (the version passed in setup.py - setup()). This is leading to unexpected errors in our implementation, and makes me doubt whether pip install is a stable way of installing older versions of spacy.\nHow to reproduce the behaviour\npip install --upgrade 'spacy<2.1'\nCollecting spacy<2.1\n  Using cached https://files.pythonhosted.org/packages/cd/70/65504a011d7b262e73cfe470c36ca245a1f8e45b3b6661081289ffd72009/spacy-2.0.18-cp37-cp37m-manylinux1_x86_64.whl\n...\nInstalling collected packages: spacy\nSuccessfully installed spacy-2.2.3\n\npython\n>>> import spacy\n>>> spacy.__version__\n'2.0.18'\n\npip list | grep spacy\nspacy                2.2.3\nYour Environment\nOperating System: buster (debian 10)\nPython Version Used: 3.7.5\nspaCy Version Used: not clear!\nEnvironment Information: clean dockerized env using python:3.7-slim\n1", "issue_status": "Closed", "issue_reporting_time": "2019-12-03T14:40:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "97": {"issue_url": "https://github.com/explosion/spaCy/issues/4751", "issue_id": "#4751", "issue_summary": "add_label doesn't add extra_label to ner model in Spacy 2.1.9", "issue_description": "anilberkaltuner commented on 3 Dec 2019 \u2022\nedited\nWhen adding label for existing custom trained named entity model, it doesnt add labels.\ndef main(model=<model_path>,\n         output_dir=Path(<output_path>), n_iter=100):\n    spacy.util.use_gpu(0)\n    if model is not None:\n        nlp = spacy.load(model)\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"tr\")\n        print(\"Created blank 'en' model\")\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    ner.add_label(\"LOCATION\")\n    ner.add_label(\"MISC\")\n    ner.add_label(\"PERSON\")\n    ner.add_label(\"ORGANIZATION\")\n\n\n    optimizer = nlp.resume_training()\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):\n        for itn in range(n_iter):\n\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,\n                    annotations,\n                    drop=0.25,\n                    losses=losses,\n                    sgd=optimizer\n                )\n            print(\"Losses\", losses)\nI trained model with spacy cli\npython3 -m spacy train tr [output_path] [train_path] [dev_path] --n=200\nAfter train, when \u0131 looked to ner/cfg file\n{ \"beam_width\":1, \"beam_density\":0.0, \"cnn_maxout_pieces\":3, \"deprecation_fixes\":{ \"vectors_name\":\"spacy_pretrained_vectors\" }, \"beam_update_prob\":1.0, \"nr_class\":17, \"hidden_depth\":1, \"token_vector_width\":96, \"hidden_width\":64, \"maxout_pieces\":2, \"pretrained_vectors\":null, \"bilstm_depth\":0 }\nFirst model was 2.0.16 version of Spacy and its work well. When upgrade to 2.1.9, \u0131 have problem about that.\n{ \"beam_width\":1, \"beam_density\":0.0, \"cnn_maxout_pieces\":3, \"deprecation_fixes\":{ \"vectors_name\":\"spacy_pretrained_vectors\" }, \"nr_class\":1, \"hidden_depth\":1, \"token_vector_width\":128, \"hidden_width\":200, \"maxout_pieces\":2, \"pretrained_vectors\":null, \"hist_size\":0, \"hist_width\":0, \"extra_labels\":[ \"PERSON\", \"LOCATION\", \"MISC\", \"ORGANIZATION\" ] }\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2019-12-03T14:29:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "98": {"issue_url": "https://github.com/explosion/spaCy/issues/4747", "issue_id": "#4747", "issue_summary": "Can't assign a name different from the factory name then save and load", "issue_description": "oliviercwa commented on 3 Dec 2019\nSave/Load cycle fails if pipe name differs from the factory name\nHow to reproduce the behaviour\n1- Create a pipe using create_pipe\n2- Insert the component inside a pipeline using add_pipe and assign a name other than the factory name\n3- Save and Load back the pipeline\nResult: Exception E002 raised\nCode below repros the problems with a single component\nExpected: Be able to save and load a pipeline that was previously created. If Spacy lets me give an arbitrary pipe name, it should be able to load the pipeline back. Today there's an expectation that the pipe name == factory name\nNote: This issue was raised in #4712 and I believe it was incorrectly closed.\nimport spacy\nimport os\nfrom spacy.language import Language\n\nclass MockComponent():\n  def __call__(self, doc):\n    return doc\n\nLanguage.factories['MockComponent'] = lambda nlp, **cfg: MockComponent(**cfg) \n\nnlp = spacy.blank('en')\nfoobar = nlp.create_pipe('MockComponent')\nnlp.add_pipe(foobar, 'foobar', first=True)\nnlp.to_disk(os.path.join(os.path.dirname(__file__), 'temp'))\nnlp2 = spacy.load(os.path.join(os.path.dirname(__file__), 'temp'))\nThe code above throws on loading:\n[E002] Can't find factory for 'foobar'. This usually happens when spaCy calls nlp.create_pipe with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to Language.factories['foobar'] or remove it from the model meta and add it via nlp.add_pipe instead.\"\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7.4\nspaCy Version Used: 2.1.9\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-12-03T10:36:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "99": {"issue_url": "https://github.com/explosion/spaCy/issues/4744", "issue_id": "#4744", "issue_summary": "Dependency Parsing stability", "issue_description": "frakar637 commented on 2 Dec 2019 \u2022\nedited\nHi\nI'm new to spaCy and at the moment, I'm looking for the best ways to use it for dependency parsing.\nResults from https://explosion.ai/demos/displacy\nen_core_web_lg\nChange the color to red. => color => red[dep=relcl]\nChange the color to green. => change => to[dep=prep] => green[dep=pboj]\nChange the color to yellow. => change => yellow[dep=advcl]\nChange the color to pink. => change => pink[dep=advcl]\nen_core_web_sm\nChange the color to pink. => change => pink[dep=advcl]\nChange the color to green. => change => green[dep=advcl]\nChange the color to yellow. => change => yellow[dep=advcl]\nChange the color to red. => change => red[dep=advcl]\nIt seems all the models do not have the same stability\nI would expect the lg model to be more stable than the sm.\nWhat the best way to insure the most stable parsing ?", "issue_status": "Closed", "issue_reporting_time": "2019-12-02T17:38:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "100": {"issue_url": "https://github.com/explosion/spaCy/issues/4742", "issue_id": "#4742", "issue_summary": "Very slow matcher pattern", "issue_description": "Contributor\nmr-bjerre commented on 2 Dec 2019\nFirst of all, this might not be a bug at all. It might just be heavy misuse of pattern matching. I have a pattern where I want to match all phrases starting with profit or profits and ending with ebitda (at max 12 tokens in between). I.e. I want to encapsulate that.\nfrom spacy.matcher import Matcher\nfrom spacy.lang.en import English\n\npattern = [{'LOWER': {'IN': [\"profit\", 'profits']}}, {'OP': '?'}, {'OP': '?'}, {'OP': '?'},\n           {'OP': '?'}, {'OP': '?'}, {'OP': '?'}, {'OP': '?'}, {'OP': '?'}, {'OP': '?'}, {'OP': '?'}, {'OP': '?'},\n           {'OP': '?'}, {'LOWER': 'ebitda'}, {'ORTH': ')', 'OP': '?'}]\n\ntext = '\"improving operating profit and maintaining a disciplined approach to capital allocation.\"'\n\nnlp = English()\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"ThisHangs\", None, pattern)\n\nmatcher(nlp(text))  # this takes several seconds\nYour Environment\nspaCy version: 2.1.9\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nPython version: 3.7.5\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-12-02T11:08:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "101": {"issue_url": "https://github.com/explosion/spaCy/issues/4740", "issue_id": "#4740", "issue_summary": "Pipe missing after training other feature", "issue_description": "pythonBerg commented on 2 Dec 2019\nI am probably doing something stupid, but this appears to be something new in code I've used for quite some time.\nI have a custom model that contains both a text classifier and a named entity model. When training each component, I disable to other pipe so as to not attempt to train with nonsense. This is right out of example code.\nHere's what happening though...when I train ner, any previous training for textcat and the named pipe disappears. Same when I train textcat on model that contains ner, it disappears? I have no idea what's happening\nShould I be enabling the previously disabled pipe before I save to disk? What am I missing? I've had stable models so just now getting back into intense training.\npython 3.6\nspacy 2.1.8", "issue_status": "Closed", "issue_reporting_time": "2019-12-02T02:49:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "102": {"issue_url": "https://github.com/explosion/spaCy/issues/4737", "issue_id": "#4737", "issue_summary": "nlp.pipe(n_processes=x) does not work with token._.attribute", "issue_description": "theudas commented on 30 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\nI have a pipeline that sets some custom ._.attribute.\nIf I run my pipeline with nlp.pipe(texts, n_processes=16) it forgot that any of my custom._.attributes do exist:\nTraceback (most recent call last):\nFile \"C:\\Users\\psodm\\Anaconda3\\lib\\multiprocessing\\process.py\", line 297, in _bootstrap\nself.run()\nFile \"C:\\Users\\psodm\\Anaconda3\\lib\\multiprocessing\\process.py\", line 99, in run\nself._target(*self._args, **self._kwargs)\nFile \"C:\\Users\\psodm\\Anaconda3\\lib\\site-packages\\spacy\\language.py\", line 1124, in _apply_pipes\nsender.send([doc.to_bytes() for doc in docs])\nFile \"C:\\Users\\psodm\\Anaconda3\\lib\\site-packages\\spacy\\language.py\", line 1124, in\nsender.send([doc.to_bytes() for doc in docs])\nFile \"C:\\Users\\psodm\\Anaconda3\\lib\\site-packages\\spacy\\language.py\", line 1105, in _pipe\nfor doc in docs:\nFile \"C:\\Users\\psodm\\Anaconda3\\lib\\site-packages\\spacy\\language.py\", line 1106, in pipe\ndoc = proc(doc, **kwargs)\nFile \"C:\\python\\pipeline\\pipeline\\modules\\module_inherited.py\", line 26, in call\nself.apply(doc, p)\nFile \"C:\\python\\pipeline\\pipeline\\modules\\module_base.py\", line 28, in apply\ntoken..affirmed = category == 0\nFile \"C:\\Users\\psodm\\Anaconda3\\lib\\site-packages\\spacy\\tokens\\underscore.py\", line 63, in setattr\nraise AttributeError(Errors.E047.format(name=name))\nAttributeError: [E047] Can't assign a value to unregistered extension attribute 'affirmed'. Did you forget to call the set_extension method?\nMy guess is, that during the pickle process the custom attributes are getting lost.\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.4", "issue_status": "Closed", "issue_reporting_time": "2019-11-30T15:59:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "103": {"issue_url": "https://github.com/explosion/spaCy/issues/4733", "issue_id": "#4733", "issue_summary": "ImportError: DLL load failed: The specified module could not be found.", "issue_description": "ahmedbr commented on 29 Nov 2019\nThe error occurs after installation and running the following command for checking the validation:\npython -m spacy validate\n(venv) C:\\Users\\user_profile\\Desktop\\new_pro>python -m spacy validate\nTraceback (most recent call last):\n  File \"C:\\Users\\user_profile\\AppData\\Local\\Programs\\Python\\Python36\\lib\\runpy.py\", line 183, in _run_module_as_main\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n  File \"C:\\Users\\user_profile\\AppData\\Local\\Programs\\Python\\Python36\\lib\\runpy.py\", line 142, in _get_module_details\n    return _get_module_details(pkg_main_name, error)\n  File \"C:\\Users\\user_profile\\AppData\\Local\\Programs\\Python\\Python36\\lib\\runpy.py\", line 109, in _get_module_details\n    __import__(pkg_name)\n  File \"C:\\Users\\user_profile\\Desktop\\new_pro\\venv\\lib\\site-packages\\spacy\\__init__.py\", line 12, in <module>\n    from . import pipeline\n  File \"C:\\Users\\user_profile\\Desktop\\new_pro\\venv\\lib\\site-packages\\spacy\\pipeline\\__init__.py\", line 4, in <module>\n    from .pipes import Tagger, DependencyParser, EntityRecognizer, EntityLinker\n  File \"pipes.pyx\", line 1, in init spacy.pipeline.pipes\nImportError: DLL load failed: The specified module could not be found.\nYour Environment\nOperating System: Windows 10 Pro N\nPython Version Used: 3.6.6\nspaCy Version Used: 2.2.3\nEnvironment Information: The result of running python -m spacy info --markdown is the same error message included above.", "issue_status": "Closed", "issue_reporting_time": "2019-11-29T11:09:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "104": {"issue_url": "https://github.com/explosion/spaCy/issues/4732", "issue_id": "#4732", "issue_summary": "Spacy Hanging in a Django app when deployed on Apache Server", "issue_description": "shuvayan-wgt commented on 29 Nov 2019\nI am developing a resume parser where I have used spacy to parse 'Name' from a resume. Everything is working fine when I'm running it using 'python3 manage.py runserver' command. But after I have deployed it in a apache server, its hanging just after loading the spacy language module. Earlier it was freezing while loading only, but that got resolved after installing spacy-nightly. But now it is freezing while I'm passing any text through through the object created after loading the module.\nnlp = spacy.load('en_core_web_sm')\nprint('encoreweb')\ndoc = nlp(text)\nprint('name')\nIts printing 'encoreweb' but its not going to the next print.\nYour Environment\nOperating System: Ubuntu 16.04\nPython Version Used: 3.5\nspaCy Version Used: spacy-nightly\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-29T08:35:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "105": {"issue_url": "https://github.com/explosion/spaCy/issues/4731", "issue_id": "#4731", "issue_summary": "make_doc() rejects subtypes of str", "issue_description": "MaxG87 commented on 29 Nov 2019\nHow to reproduce the behaviour\nPlease consider the following minimal working example (mwe.py).\n#!/usr/bin/python\n\nimport spacy\n\n\nclass MyString(str):\n    pass\n\n\nnlp = spacy.load('en_core_web_sm')\nmy_str = MyString('Lets crash spaCy for no reason.')\ndoc = nlp(my_str)\nIf I execute mwe.py, I get the following error message:\nTraceback (most recent call last):\n  File \"/home/mgoerner/mwe.py\", line 12, in <module>\n    doc = nlp(my_str)\n  File \"/home/mgoerner/.miniconda3/envs/minerva/lib/python3.8/site-packages/spacy/language.py\", line 427, in __call__\n    doc = self.make_doc(text)\n  File \"/home/mgoerner/.miniconda3/envs/minerva/lib/python3.8/site-packages/spacy/language.py\", line 453, in make_doc\n    return self.tokenizer(text)\nTypeError: Argument 'string' has incorrect type (expected str, got MyString)\nHowever, I consider this to be incorrect. MyString is a proper subtype of 'str' and thus should be accepted as well.\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Linux-5.3.0-050300-generic-x86_64-with-glibc2.10\nPython version: 3.8.0", "issue_status": "Closed", "issue_reporting_time": "2019-11-28T20:50:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "106": {"issue_url": "https://github.com/explosion/spaCy/issues/4730", "issue_id": "#4730", "issue_summary": "Display Custom Attributes in the Named Entity Recogniser", "issue_description": "Fourthought commented on 28 Nov 2019\nFeature description\nThe Named Entity Recogniser is great for displaying named entities, what would also be great is the ability to also display custom attributes along with the named entities.\nUsing the token based matching structures, initialistion might go from:\ndisplacy.serve(doc, style=\"ent\")\nto:\ncustom_attributes = [{\"\" : {\"ATTRIBUTE1\" : \"VALUE\"}, {\"\" : {\"ATTRIBUTE2\" : \"VALUE\"}]\ndisplacy.serve(doc, style=\"ent\", custom = custom_attributes)\nCould the feature be a custom component or spaCy plugin?\nDisplacy is a custom component so this suggestion should be possible.", "issue_status": "Closed", "issue_reporting_time": "2019-11-28T15:36:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "107": {"issue_url": "https://github.com/explosion/spaCy/issues/4727", "issue_id": "#4727", "issue_summary": "Cannot replace spaCy lemmatized pronouns (-PRON-) through text", "issue_description": "val-st commented on 28 Nov 2019 \u2022\nedited\nI'm trying to lemmatise a text with spaCy. Since spaCy uses -PRON- as lemma for personal pronouns, I want to keep the original text in all those cases.\nHere's the relevant section of my code:\n...\nfout = open('test.txt', 'w+')\ndoc = nlp(text)\nfor word in doc:\n     if word.lemma_ == \"-PRON-\":\n          write = word.text\n          print(write)\n     else:\n          write = word.lemma_\n          fout.write(str(write))\n          fout.write(\" \")\n...\nIt correctly identifies all the instances where it should produce text instead of '-PRON-' and the print statement does indeed print the original words rather than '-PRON-'\nHowever, my output file (test.txt) always contains '-PRON-' for those cases, even though I would expect it to write the original words for those cases (I, us etc.)\nI tried different versions, including using the pos_ tag to identify the pronouns etc. but always with the same result, i.e., that my output contains '-PRON-'s\nI have spaCy version 2.2.2, Python version 3.7.4., running on a mac (Mojave)", "issue_status": "Closed", "issue_reporting_time": "2019-11-28T09:27:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "108": {"issue_url": "https://github.com/explosion/spaCy/issues/4724", "issue_id": "#4724", "issue_summary": "Unexpected (almost random) parses and no sentence segmentation when using GPU on Windows", "issue_description": "Contributor\nBramVanroy commented on 28 Nov 2019 \u2022\nedited\nFollowing a discussion on Twitter about spaCy vs stanfordnlp vs spacy-stanfordnlp, I figured I'd do a speed comparison. The task is putting 1K lines through the whole pipeline. The test includes nlp() as well as nlp.pipe().\nThe testing scripts and data are available in this test repo: https://github.com/BramVanroy/parsers-test\nI was extremely surprised by the big differences in speed (spaCy being much faster than stanfordnlp) so I went digging. I found that for the whole input in nlp() only one doc.sent sentence was created. In other words, no sentence segmentation was done. In addition, I saw extremely bad parsing results from spaCy. An example from using readlines() + nlp.pipe():\n['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\\n']\n['VERB', 'PRON', 'PROPN', 'NOUN', 'VERB', 'ADV', 'VERB', 'NUM', 'PRON', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'VERB', 'VERB', 'NOUN', 'SPACE']\n['dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'ROOT', '']\n\n['The', 'Company', \"'s\", 'current', 'liabilities', 'of', '&', 'euro;32.6', 'million', 'primarily', 'relate', 'to', 'deferred', 'income', 'from', 'collaborative', 'arrangements', 'and', 'trade', 'payables', '.', '\\n']\n['NOUN', 'VERB', 'AUX', 'NOUN', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'VERB', 'VERB', 'ADV', 'VERB', 'VERB', 'NOUN', 'NOUN', 'PROPN', 'NOUN', 'PROPN', 'VERB', 'NUM', 'NOUN', 'SPACE']\n['dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'punct', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'ROOT', '']\n\n['The', 'increase', 'in', 'deferred', 'income', 'is', 'related', 'to', 'new', 'deals', 'with', 'partners', '.', '\\n']\n['NOUN', 'PROPN', 'PROPN', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'VERB', 'NOUN', 'VERB', 'NOUN', 'SPACE']\n['dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'punct', 'dep', 'dep', 'ROOT', '']\nThis is not how I've seen spaCy behave in the past (non-GPU version). The parses seem almost random, as if the model is not initialized correctly. Also note how basically all dependencies are dep. Again, the posted repo above may prove some insight. After more digging:\nI found that using the CPU version of spaCy does work as expected.\nI did a complete reinstall of CUDA and tried the supported version (9.2 per spaCy documentation). So I installed CUDA 9.2 (+patch) followed by installing the specifc cupy package cupy-cuda92 and spacy[cuda92]. Unfortunately, this leads to the same result.\nI found that it does work perfectly fine on Linux (Ubuntu 18.04) with 9.2. So this seems like a Windows issue.\nI tried installing spacy[cuda92]==2.1.8 to check whether the issue also arises in spaCy 2.1. Even though installation works, I get an error propagating from thinc (dependency issue?). Possibly related: explosion/thinc#79\n  File \"C:/dev/python/parsers-test/spacy_test.py\", line 7, in parse\n    doc = nlp(text)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\spacy\\language.py\", line 402, in __call__\n    doc = proc(doc, **component_cfg.get(name, {}))\n  File \"pipes.pyx\", line 392, in spacy.pipeline.pipes.Tagger.__call__\n  File \"pipes.pyx\", line 411, in spacy.pipeline.pipes.Tagger.predict\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 40, in predict\n    X = layer(X)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 310, in predict\n    X = layer(layer.ops.flatten(seqs_in, pad=pad))\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 40, in predict\n    X = layer(X)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 133, in predict\n    y, _ = self.begin_update(X, drop=None)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 379, in uniqued_fwd\n    Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 46, in begin_update\n    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\Users\\bramv\\.virtualenvs\\parsers-test-NtX--KFc\\lib\\site-packages\\thinc\\neural\\_classes\\hash_embed.py\", line 59, in begin_update\n    keys = self.ops.hash(ids, self.seed) % self.nV\n  File \"ops.pyx\", line 967, in thinc.neural.ops.CupyOps.hash\nAttributeError: module 'thinc_gpu_ops' has no attribute 'hash'\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.5\nModels: en, nl", "issue_status": "Closed", "issue_reporting_time": "2019-11-27T20:57:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "109": {"issue_url": "https://github.com/explosion/spaCy/issues/4723", "issue_id": "#4723", "issue_summary": "KeyError when trying to follow provided train_entity_linker.py script", "issue_description": "JohnGiorgi commented on 27 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\nSimply run the provided pretrain_kb.py and train_entity_linker.py scripts. E.g.\npython pretrain_kb.py -m en_core_web_lg -n 1 -o ./tmp\npython train_entity_linker.py ./tmp/kb ./tmp/vocab -o ./tmp -n 1       \nDuring the execution of the second command, a KeyError is raised with the following traceback\nCreated blank 'en' model with vocab from 'tmp/vocab'\nLoaded Knowledge Base from 'tmp/kb'\n('Russ Cochran his reprints include EC Comics.', 'Russ Cochran captured his first major title with his son as caddie.', 'Russ Cochran has been publishing comic art.', \"Russ Cochran was a member of University of Kentucky's golf team.\") ({'links': {(0, 12): {'Q7381115': 1.0, 'Q2146908': 0.0}}}, {'links': {(0, 12): {'Q7381115': 0.0, 'Q2146908': 1.0}}}, {'links': {(0, 12): {'Q7381115': 1.0, 'Q2146908': 0.0}}}, {'links': {(0, 12): {'Q7381115': 0.0, 'Q2146908': 1.0}}})\nTraceback (most recent call last):\n  File \"train_entity_linker.py\", line 167, in <module>\n    plac.call(main)\n  File \"/Users/johngiorgi/miniconda3/envs/el/lib/python3.7/site-packages/plac_core.py\", line 367, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/johngiorgi/miniconda3/envs/el/lib/python3.7/site-packages/plac_core.py\", line 232, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"train_entity_linker.py\", line 127, in main\n    sgd=optimizer,\n  File \"/Users/johngiorgi/miniconda3/envs/el/lib/python3.7/site-packages/spacy/language.py\", line 515, in update\n    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n  File \"pipes.pyx\", line 1219, in spacy.pipeline.pipes.EntityLinker.update\nKeyError: (0, 12)\nThere is another closed issue (#4469) that reports this same problem. Suggestions are to use the latest version of SpaCy (I tried with 2.2.2 and 2.2.3) and to update the line\nkb = KnowledgeBase(vocab=nlp.vocab)\nin pretrain_kb.py with\nkb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=64)\nThis solution did not work for me (I recieve the same error as above).\nYour Environment\nspaCy version: 2.2.2\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nPython version: 3.7.5\n1", "issue_status": "Closed", "issue_reporting_time": "2019-11-27T16:06:50Z", "fixed_by": "#4789", "pull_request_summary": "Update EL example", "pull_request_description": "Member\nsvlandeg commented on 10 Dec 2019\nDescription\nThe Entity Linking example scripts needed an update after a previous refactor. Also added a few more user-friendly errors.\nFixes #4723\nTypes of change\nbug fix & enhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-11T17:19:42Z", "files_changed": [["4", "examples/training/pretrain_kb.py"], ["51", "examples/training/train_entity_linker.py"], ["3", "spacy/errors.py"], ["9", "spacy/pipeline/pipes.pyx"]]}, "110": {"issue_url": "https://github.com/explosion/spaCy/issues/4720", "issue_id": "#4720", "issue_summary": "Texcat accuracy settling on 0.5 while loss goes to 0 for binary classification", "issue_description": "tsoernes commented on 27 Nov 2019 \u2022\nedited\nI've mimicked the example in the docs, and for a binary classification problem used two output neurons with exclusive_classes=True and labels as such:\n{\n        'cats': {\n            'POS': False,\n            'NEG': True\n        }\n    }\n.. and architecture either bow or ensemble. I observe a strange issue:\nAfter training 80k examples for an epoch, the loss, as reported by Spacy (from nlp.update), is 0.0000, while the accuracy, as reported by scikit-learn's accuracy_score tends toward 0.5000 exactly and the log_loss just keeps increasing.\nfrom sklearn.metrics import accuracy_score, log_loss\n\ndocs = nlp.pipe(valid_texts, batch_size=256)\npred_probs = []\nfor doc in docs:\n    pred_probs.append(list(doc.cats.values()))\ny_pred_probs = np.array(pred_probs)\n\navg_log_loss = log_loss(y_true, y_pred_probs)\ny_pred = y_pred_probs > 0.5\nacc = accuracy_score(y_true, y_pred)\nI've made sure the the order of Spacy's internal labels (as given by doc.cats.keys()) is the same as the one-hot encoded numpy array y_true, and that y_pred and y_true are the same shape, that is, (n_examples, 2). Evaluations are done using with nlp.use_params(optimizer.averages).\nThese results seems highly suspect to me and I'm not sure where I've stepped wrong. I'm using the en_core_web_lg model; disabling any pipes besides textcat, and training from scratch (i.e. nlp.begin_training()) without word vectors.\nThe label distribution is exactly 50-50 for both the training and the test set.\nThanks for any advice,\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-4.4.0-1098-aws-x86_64-with-debian-stretch-sid\nPython version: 3.7.4", "issue_status": "Closed", "issue_reporting_time": "2019-11-27T00:50:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "111": {"issue_url": "https://github.com/explosion/spaCy/issues/4718", "issue_id": "#4718", "issue_summary": "Chaining models or specifying priorities for overlapping entities?", "issue_description": "bulutsuzku commented on 27 Nov 2019\nWhich page or section is this issue related to?\nhttps://spacy.io/usage/training\nHello,\nI am a newbie with spaCy, currently trying to understand the terminologies and workflow - BTW I find sometimes the spaCy documentation introduces NLP jargon and tasks assuming the reader already knows the subject -\nI wrote this question in scispaCy issues, with no answers so far, which made me think it could be more relevant to spaCy too: I tried scispaCy with a NER task using en_ner_bionlp13cg_md. Unfortunately it recognizes entities in a wrong way i.e.: \"Americas\" as ORGANISM, \"Columbus\" as CANCER, etc. so this is my question:\nCould I chain different models?\nLike using both en_ner_bionlp13cg_md and en_ner_bc5cdr_md?\nIf yes, how do I specify \"priority\"? For example: I would like to prioritize on geopolitical entities using model A but for gene names use model B?\nDoes this make sense?\nIs this what the \"pipeline\" concept refers to?\nThank you for reading.", "issue_status": "Closed", "issue_reporting_time": "2019-11-26T18:50:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "112": {"issue_url": "https://github.com/explosion/spaCy/issues/4717", "issue_id": "#4717", "issue_summary": "Error on token attribute in pattern matcher", "issue_description": "RandomJungle commented on 26 Nov 2019\nHello,\nI noticed a strange behavior when updating to new version 2.2.3 from the version I was using before, which was 2.1.4. The 'LENGTH' attribute of token used in patterns started to match on anything. Below is the piece of code I used to reproduce the error:\nimport spacy\nfrom spacy.matcher import Matcher\nnlp = spacy.load(\"fr_core_news_md\")\nmatcher = Matcher(nlp.vocab)\npattern = [{'SHAPE': 'dddd', 'LENGTH': 9}]\nmatcher.add(\"number\", None, pattern)\ndoc = nlp(\"Bonjour nous sommes bien en 2005 ! Il est 9h du matin\")\nmatches = matcher(doc)\nprint(matches)\nOn former version, there was no match, which is expected behavior. On newer version however, I get a match on 2005, [(432, 5, 6)]. Now here's a run of the same code, without the \"shape\" ingredient :\nimport spacy\nfrom spacy.matcher import Matcher\nnlp = spacy.load(\"fr_core_news_md\")\nmatcher = Matcher(nlp.vocab)\npattern = [{'LENGTH': 10}]\nmatcher.add(\"number\", None, pattern)\ndoc = nlp(\"Bonjour nous sommes bien en 2005 ! Il est 9h du matin, surprenant !\")\nmatches = matcher(doc)\nif matches:\n    for match in matches:\n        print(doc[match[1]:match[2]])\nIt prints out every token of the sentence, which is odd to me, since LENGTH is listed as one of the token attribute that can be used in matcher on the official doc. Now if I run the exact same code with the former version (2.1.4) with model 2.1 of fr_core_news_md, it prints out \"surprenant\", which makes sense.\nIs the LENGTH attribute of the token deprecated in the new model version ?", "issue_status": "Closed", "issue_reporting_time": "2019-11-26T11:37:36Z", "fixed_by": "#4749", "pull_request_summary": "Fix int value handling in Matcher", "pull_request_description": "Collaborator\nadrianeboyd commented on 3 Dec 2019 \u2022\nedited\nDescription\nAdd int values (for LENGTH) in _get_attr_values() instead of treating int like dict.\n(The validation fix in #4444 was incorrect.)\nFixes #4717.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-06T18:22:58Z", "files_changed": [["4", "spacy/matcher/matcher.pyx"]]}, "113": {"issue_url": "https://github.com/explosion/spaCy/issues/4714", "issue_id": "#4714", "issue_summary": "Add tag map argument to CLI", "issue_description": "Collaborator\nadrianeboyd commented on 26 Nov 2019\nFeature description\nSupport a tag map argument in debug-data and train CLI commands. Something like: -tm '', --tag-map tag_map.py?", "issue_status": "Closed", "issue_reporting_time": "2019-11-26T09:51:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "114": {"issue_url": "https://github.com/explosion/spaCy/issues/4712", "issue_id": "#4712", "issue_summary": "Pipe Name is used when deserializing instead of Pipe Type => Can't deserialize", "issue_description": "oliviercwa commented on 26 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\n1- Create a pipeline with 2 components of the same type each with different name. (One is enough, but this is to show that sometimes we want the name different from the pipe type)\n[code below reproduces the problem]\n2- Pipe these components together\n3- Save to disk (or to_byte)\n4- Load from disk\nResult:\nThe loader does not find the pipe as it's looking for the name of the pipe in the factory instead of its Python type\nExpected:\nPersistence to persist the name and the pipe type\nimport spacy\nimport os\n\nclass MockComponent():\n  ComponentName: str = 'MockComponent'\n  def __init__(self, name = ''):\n    self.name = name\n  def __call__(self, doc):\n    return doc\n\nfrom spacy.language import Language\nLanguage.factories[MockComponent.ComponentName] = lambda nlp, **cfg: MockComponent(**cfg) \nnlp = spacy.blank('en')\n\nfirst = nlp.create_pipe(MockComponent.ComponentName, {'name':'first'})\nlast = nlp.create_pipe(MockComponent.ComponentName, {'name':'last'})\nnlp.add_pipe(first, first.name, first=True)\nnlp.add_pipe(last, last.name, last=True)\nnlp.to_disk(os.path.join(os.path.dirname(__file__), 'temp'))\nnlp2 = spacy.load(os.path.join(os.path.dirname(__file__), 'temp'))\nResult:\n\"KeyError: \"[E002] Can't find factory for 'first'. This usually happens when spaCy calls nlp.create_pipe with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to Language.factories['first'] or remove it\nfrom the model meta and add it via nlp.add_pipe instead.\"\nExpected:\nThe serializer to serialize the Pipe Type (here MockComponent) and the Pipe Name. On load it would use the Pipe Type to create the object and assign the name in the pipeline.\nYour Environment\nOperating System: Win 10\nPython Version Used: 3.7.4\nspaCy Version Used: 2.1.9\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-25T21:06:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "115": {"issue_url": "https://github.com/explosion/spaCy/issues/4709", "issue_id": "#4709", "issue_summary": "ent.start, ent.end being incorrect with custom tokenizer", "issue_description": "lynochka commented on 25 Nov 2019 \u2022\nedited\nHow to reproduce the behavior\nI use a custom tokenizer:\nimport pickle\nimport random\nimport regex as re\n\nre_compile = re.compile(\"[\\p{L}\\d]+|[^\\p{L}\\d]+\") \n\ndef _tokenizer(name):\n    return re_compile.findall(name)\n\n\nclass CustomTokenizer(object):\n    def __init__(self, vocab):\n        self.vocab = vocab\n    def __call__(self, text):\n        words = _tokenizer(text)\n        spaces = [\"\"] * len(words)\n        return Doc(self.vocab, words=words, spaces=spaces)\n    def to_bytes(self, **kwargs):\n        return pickle.dumps(self.__dict__)\n    def from_bytes(self, data, **kwargs):\n        self.__dict__.update(pickle.loads(data))\n    def to_disk(self, path, **kwargs):\n        with open(path, \"wb\") as file_:\n            file_.write(self.to_bytes())\n    def from_disk(self, path, **kwargs):\n        with open(path, \"rb\") as file_:\n            self.from_bytes(file_.read())\n            \nAnd train a small model\ndata = [['SA1_360_05_KA401s', {'entities': [[4, 10, 'SYSTEM']]}],\n ['SA1_360_02_RP501_TL', {'entities': [[4, 10, 'SYSTEM']]}],\n ['SA1_360_08_ORS breakpoint Y2', {'entities': [[4, 10, 'SYSTEM']]}],\n ['SA1.390_01_360_11_RF401m3h', {'entities': [[11, 17, 'SYSTEM']]}],\n ['SA1_360_02_JV401s', {'entities': [[4, 10, 'SYSTEM']]}],\n ['SA1_360_06_Cool output 1', {'entities': [[4, 10, 'SYSTEM']]}],\n ['SA1_360_10_JP401_TV', {'entities': [[4, 10, 'SYSTEM']]}],\n ['SA1_360_05_LR501%_TL', {'entities': [[4, 10, 'SYSTEM']]}],\n ['SA1_360_09_EA AHU filter pressure level',\n  {'entities': [[4, 10, 'SYSTEM']]}],\n ['SA1.390_01_360_12_mod_SD', {'entities': [[11, 17, 'SYSTEM']]}]]\n\nnlp = spacy.blank(\"en\") \nner = nlp.create_pipe(\"ner\")\nner.add_label(\"SYSTEM\")\n\nnlp.add_pipe(ner, last=True)\nnlp.tokenizer = CustomTokenizer(nlp.vocab)\nnlp.begin_training()\n\nfor itn in range(1):\n    random.shuffle(data)\n    losses = {}\n    batches = minibatch(data, size=compounding(4.0, 32.0, 1.001))\n    for batch in batches:\n        texts, annotations = zip(*batch)\n        nlp.update(\n            texts,  \n            annotations, \n            drop=0.2,  \n            losses=losses,\n        )\n    print(\"Losses\", losses)\nTHE PROBLEM: Predicting the start and end of the entity has wrong indices:\ndef get_entities(string):\n    doc = nlp(string)\n    return  [(ent.text, ent.label_, ent.start, ent.end) for ent in doc.ents]\n\nget_entities(\"SA1_360_05_KA401s\")\nOutput: [('360_05', 'SYSTEM', 2, 5)]\nCould you help?\nYour Environment\nspaCy version: 2.2.3\nPlatform: Linux-5.0.0-36-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-11-25T13:23:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "116": {"issue_url": "https://github.com/explosion/spaCy/issues/4707", "issue_id": "#4707", "issue_summary": "load_model_from_path does not pass through the list of Exclude", "issue_description": "oliviercwa commented on 25 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\n1- Create a simple pipeline with a Tokenizer and one element\n2- Save the pipeline and exclude the tokenizer:\nnlp.to_disk(output_name, exclude = ['vocab', 'tokenizer'])\n3- Load the pipeline again\nnlp.from_disk(output_name, exclude = ['vocab', 'tokenizer'], disable = ['vocab', 'tokenizer'])\nResult:\n  File \"\\external\\spacy\\spacy\\__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"\\external\\spacy\\spacy\\util.py\", line 136, in load_model\n    return load_model_from_path(Path(name), **overrides)\n  File \"\\external\\spacy\\spacy\\util.py\", line 179, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"\\external\\spacy\\spacy\\language.py\", line 836, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"\\external\\spacy\\spacy\\util.py\", line 636, in from_disk\n    reader(path / key)\n  File \"\\external\\spacy\\spacy\\language.py\", line 823, in <lambda>\n    p, exclude=[\"vocab\"]\n  File \"tokenizer.pyx\", line 389, in spacy.tokenizer.Tokenizer.from_disk\n  File \"\\AppData\\Local\\Programs\\Python\\Python37\\lib\\pathlib.py\", line 1193, in open\n    opener=self._opener)\n  File \\AppData\\Local\\Programs\\Python\\Python37\\lib\\pathlib.py\", line 1046, in _opener\n    return self._accessor.open(self, flags, mode)\nFileNotFoundError: [Errno 2] No such file or directory: '\\train\\\\resources\\\\repository\\\\models\\\\test\\\\tokenizer'\nReason\nthe exclude list is not carried through from util.load_model_from_path to nlp.from_disk\nPossible fix\ninside util.py, pass the **overrides to nlp.from_disk\ndef load_model_from_path(model_path, meta=False, **overrides):\n   . ...\n    return nlp.from_disk(model_path, **overrides)\nYour Environment\nOperating System:\nWindows 10\nPython Version Used:\nPython 3.7.4\nspaCy Version Used:\nSpacy 2.1.9\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-25T09:23:31Z", "fixed_by": "#4708", "pull_request_summary": "Auto-exclude disabled when calling from_disk during load", "pull_request_description": "Member\nines commented on 25 Nov 2019\nFixes #4707.\nDescription\nWhen you disable a component when loading a model, it should also be excluded when calling nlp.from_disk afterwards to load the individual components (especially the tokenizer, which will otherwise raise an error).\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-25T15:01:23Z", "files_changed": [["23", "spacy/tests/regression/test_issue4707.py"], ["2", "spacy/util.py"]]}, "117": {"issue_url": "https://github.com/explosion/spaCy/issues/4706", "issue_id": "#4706", "issue_summary": "Evaluate : which corpus format are supported for?", "issue_description": "Contributor\nphiedulxp commented on 25 Nov 2019 \u2022\nedited\nWhen I use ner_data.json[only entities annotation], corpus read nothing.\nWhich page or section is this issue related to?\nhttps://spacy.io/api/cli#evaluate", "issue_status": "Closed", "issue_reporting_time": "2019-11-25T08:21:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "118": {"issue_url": "https://github.com/explosion/spaCy/issues/4705", "issue_id": "#4705", "issue_summary": "Why is the updated model faster than the original pretrained model?", "issue_description": "Boreaso commented on 25 Nov 2019\nI use the example code provided here to update the pretraind model en_core_web_lg-2.2.5 by my own dataset. I found that, the updated model is faster than the the origianal during inference stage(about 3 times faster). Is it due to the model architecture or some other settings has changed?\nEnvironment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.2.3\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-25T07:47:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "119": {"issue_url": "https://github.com/explosion/spaCy/issues/4703", "issue_id": "#4703", "issue_summary": "gold.pyx: OverflowError in _json_iterate", "issue_description": "vitaly-d commented on 25 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\nspacy debug or spacy train with large JSON-formatted training data file (>2^31 bytes) fails with OverflowError: value too large to convert to int\nTraining pipeline: ['ner']\nStarting with blank model 'en'\nLoading vector from model 'en_vectors_web_lg'\nCounting training words (limit=0)\nTraceback (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/vitaly/Code/Wiley/transformation-poc/.env/lib/python3.7/site-packages/spacy/__main__.py\", line 33, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/Users/vitaly/Code/Wiley/transformation-poc/.env/lib/python3.7/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/vitaly/Code/Wiley/transformation-poc/.env/lib/python3.7/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/Users/vitaly/Code/Wiley/transformation-poc/.env/lib/python3.7/site-packages/spacy/cli/train.py\", line 230, in train\n    corpus = GoldCorpus(train_path, dev_path, limit=n_examples)\n  File \"gold.pyx\", line 224, in spacy.gold.GoldCorpus.__init__\n  File \"gold.pyx\", line 235, in spacy.gold.GoldCorpus.write_msgpack\n  File \"gold.pyx\", line 280, in read_tuples\n  File \"gold.pyx\", line 545, in read_json_file\n  File \"gold.pyx\", line 592, in _json_iterate\nOverflowError: value too large to convert to int\nMost likely, the problem is very minor as with the current implementation the training file size is enough for most use cases. In my case the enormous size is the result of an attempt to implement the named entities augmentation :)\nThe fix is trivial:\n(.env) vitaly@iMac spaCy % git diff\ndiff --git a/spacy/gold.pyx b/spacy/gold.pyx\nindex 5aecc2584..138de13f1 100644\n--- a/spacy/gold.pyx\n+++ b/spacy/gold.pyx\n@@ -562,7 +562,7 @@ def _json_iterate(loc):\n     cdef int curly_depth = 0\n     cdef int inside_string = 0\n     cdef int escape = 0\n-    cdef int start = -1\n+    cdef size_t start = -1\n     cdef char c\n     cdef char quote = ord('\"')\n     cdef char backslash = ord(\"\\\\\")\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.3\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-11-24T19:30:06Z", "fixed_by": "#4827", "pull_request_summary": "facilitate larger training files", "pull_request_description": "Member\nsvlandeg commented on 21 Dec 2019\nDescription\nFacilitate large training file, but also throw a warning to point towards the possibility of splitting up into multiple files.\nFixes #4703\nFixes #4823\nTypes of change\nenhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-21T20:12:19Z", "files_changed": [["4", "spacy/errors.py"], ["10", "spacy/gold.pyx"]]}, "120": {"issue_url": "https://github.com/explosion/spaCy/issues/4701", "issue_id": "#4701", "issue_summary": "'FeedForward' object has no attribute 'W' --When load a saved nlp model.", "issue_description": "kevingeng commented on 24 Nov 2019 \u2022\nedited\nI am training a TextCategories Pipeline by reference to spacy-transformers/blob/master/examples/train_textcat.py. The training process has been successfully completed and passed the test on the nlp object for training.\nI used nlp.to_disk(\"mydir\") to save the model and then use spacy.load(\"mydir\") to loaded it as nlp2 and encountered an error.\nAttributeError Traceback (most recent call last)\nPipes.pyx in spacy.pipeline.pipes.Pipe.from_disk.load_model()\nE:\\G\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py in from_bytes(self, bytes_data)\n    374 name = name.decode(\"utf8\")\n--> 375 dest = getattr(layer, name)\n    376 copy_array(dest, param[b\"value\"])\n\nAttributeError: 'FeedForward' object has no attribute 'W'\n....\n....\nPipes.pyx in spacy.pipeline.pipes.Pipe.from_disk.load_model()\n\nValueError: [E149] Error deserializing model. Check that the config used to create the component matches the model being loaded.\nThe code I used during training was as follows:\n    Textcat = nlp.create_pipe(\"trf_textcat\")#,config={\"architecture\": \"softmax_last_hidden\", \"words_per_batch\": max_wpb},)\n    For v in labels: textcat.add_label(v)\n    Nlp.add_pipe(textcat, last=True)\nI tried both config and default without using config, which is the same as loading.\nI saw something similar in another issue, but I didn't understand the solution.\nHow can I try to solve this problem?\nThank you.\nADDING:\nthis train is based on 'en_trf_xlnetbasecased_lg' model", "issue_status": "Closed", "issue_reporting_time": "2019-11-24T06:46:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "121": {"issue_url": "https://github.com/explosion/spaCy/issues/4700", "issue_id": "#4700", "issue_summary": "Huge RAM consumption during NER training", "issue_description": "mosynaq commented on 23 Nov 2019\nI am trying to train a NER model from scratch. I have successfully built train.json, dev.josn, and test.json. I issue the below command to start training:\npython -m spacy train xx out train.json dev.json -p ner\nI have a PC with +60GB of RAM. spaCy occupies all but ~500MB. After some time and doing nothing, it stops and spits something like \"not enough memory\"!\nI had this very problem with spaCy 2.2.2 too.\nI use python v3.7.5, spaCy 2.2.3, and Windows 10", "issue_status": "Closed", "issue_reporting_time": "2019-11-23T12:00:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "122": {"issue_url": "https://github.com/explosion/spaCy/issues/4699", "issue_id": "#4699", "issue_summary": "Cannot install spacy 1.9.0 with python 3.7", "issue_description": "Shawn-Guo-CN commented on 22 Nov 2019\nIt seems that Spacy 1.9.0 doesn't support being installed with python 3.7?\nYour Environment\nOperating System: Ubuntu 18.04\nPython Version Used: 3.7.5\nspaCy Version Used: 1.9.0\nEnvironment Information: new conda environment", "issue_status": "Closed", "issue_reporting_time": "2019-11-22T15:34:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "123": {"issue_url": "https://github.com/explosion/spaCy/issues/4698", "issue_id": "#4698", "issue_summary": "Pipeline components with with > 2 mb file is slow on processing", "issue_description": "absolution54321 commented on 22 Nov 2019\nI decided to create a pattern/regex based matching approach with Matcher embedded inside pipeline. For small file MAX_TOKEN < 10000 it performs great but texts with size > 2 mb takes time for processing.\nI have tried to speed up processing by chunking text into smaller batch, i.e every batch contain maximum of 10000 tokens, text processing was faster but some of the matcher rules missed out matches because chunking process moved part entities to different batch.\nExample:\nI want to match New York Boston\nA text file of 2 mb\n'text started here some blah blah ....... New York Boston .....'\nfor faster processing I am using nlp.pipe with batches of large file\nabove text becomes\ntexts = [ 'text started here some blah blah ....', '......... New York', 'Boston ..........']\nnlp.pipe/ matcher.pipe(texts) this speeds up the process but 'New York Boston' is not to be found in matches because entity exists in two different batch.\nI have disabled all memory consuming pipe components - NER, POS, DEPS\nCurrently, I have to trade off functionality for scaling. Do anyone have a suggestion, how to tackle large files? I want to use PhraseMatcher & Matcher.\nYour Environment\nOperating System: Linux-3.10.0-1062.1.1.el7.x86_64-x86_64-with-centos-7.7.1908-Core\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.8\nEnvironment Information: 16 GB, 8 Cores No GPU", "issue_status": "Closed", "issue_reporting_time": "2019-11-22T12:15:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "124": {"issue_url": "https://github.com/explosion/spaCy/issues/4694", "issue_id": "#4694", "issue_summary": "SpaCy for Android/iOS ??", "issue_description": "himanshurobo commented on 22 Nov 2019\nFeature description\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.", "issue_status": "Closed", "issue_reporting_time": "2019-11-22T05:24:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "125": {"issue_url": "https://github.com/explosion/spaCy/issues/4693", "issue_id": "#4693", "issue_summary": "cannot import name 'prefer_gpu'", "issue_description": "Julius-ZCJ commented on 22 Nov 2019\nHow to reproduce the behaviour\nI install spaCy with command: pip install spacy==2.0.18\nwhen I import spacy I got this error:\nFile \"train.py\", line 13, in\nfrom training.nlu.ner import NerTrainer\nFile \"C:\\Users\\two\\Project\\mtwoai\\training\\nlu\\ner.py\", line 9, in\nimport spacy\nFile \"C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\spacy_init_.py\", line 8, in\nfrom thinc.neural.util import prefer_gpu, require_gpu\nImportError: cannot import name 'prefer_gpu'\nYour Environment\nOperating System: win 10\nPython Version Used: python3.6.8\nspaCy Version Used: 2.0.18\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-22T02:33:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "126": {"issue_url": "https://github.com/explosion/spaCy/issues/4692", "issue_id": "#4692", "issue_summary": "Altered NER", "issue_description": "rodrigoheck commented on 22 Nov 2019\nI have a task which is half a matcher and half an entity extraction. I want to label some words that in some contexts refer to some label. Named entity extraction would be the way to go, but these words do not necessarily share structure (they can be verbs, nouns... etc). I could simply use a dictionary, but I would like to use context to label them. I am having trouble finding a solution to this problem. Can NER function of spacy be used for this or is this a completely different task?", "issue_status": "Closed", "issue_reporting_time": "2019-11-21T18:43:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "127": {"issue_url": "https://github.com/explosion/spaCy/issues/4689", "issue_id": "#4689", "issue_summary": "Different/random dependency tag depending on runtime", "issue_description": "Contributor\nf11r commented on 21 Nov 2019\nHow to reproduce the behaviour\nRunning\npython -c \"import de_core_news_sm; nlp = de_core_news_sm.load(); d = nlp('Dies ist ein Test Text .'); print([t.dep_ for t in d])\"\ngives ['sb', 'ROOT', 'nk', 'pnc', 'pd', 'punct'], while running\nipython -c \"import de_core_news_sm; nlp = de_core_news_sm.load(); d = nlp('Dies ist ein Test Text .'); print([t.dep_ for t in d])\"\ngives ['sb', 'ROOT', 'nk', 'nk', 'pd', 'punct']. The only difference is python vs ipython. In py.test I've randomly run into both variants of this parse.\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Linux-4.19.81-x86_64-with-glibc2.10\nPython version: 3.7.3\nipython version: 7.5.0\nde_core_news_sm version: 2.2.5", "issue_status": "Closed", "issue_reporting_time": "2019-11-21T12:00:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "128": {"issue_url": "https://github.com/explosion/spaCy/issues/4688", "issue_id": "#4688", "issue_summary": "Parser assigns head from different sentence", "issue_description": "pglock commented on 21 Nov 2019 \u2022\nedited\nrelated to #3170\nimport de_core_news_sm\n\nnlp = de_core_news_sm.load()\ndoc = nlp('''Au\u00dferdem ist Zimmer davon \u00fcberzeugt, dass auch epige-\nnetische Mechanismen eine Rolle spielen, also Vorg\u00e4nge, die\nsich darauf auswirken, welche Gene abgelesen werden und\nwelche nicht.\n''')\nfor sent in doc.sents:\n    for token in sent:\n        assert token.head in sent, (token, token.head, sent)\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Linux-5.3.11-1-MANJARO-x86_64-with-arch-Manjaro-Linux\nPython version: 3.7.5", "issue_status": "Closed", "issue_reporting_time": "2019-11-21T08:29:28Z", "fixed_by": "#4702", "pull_request_summary": "Iterate over lr_edges until sents are correct", "pull_request_description": "Collaborator\nadrianeboyd commented on 24 Nov 2019\nDescription\nIterate over lr_edges until all heads are within the current sentence. Instead of iterating over them for a fixed number of iterations, check whether the sentence boundaries are correct for the heads and stop when all are correct. Stop after a maximum of 10 iterations, providing a warning in this case since the sentence boundaries may not be correct.\nFixes #4688.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-25T12:06:37Z", "files_changed": [["1", "spacy/errors.py"], ["17", "spacy/tests/parser/test_parse.py"], ["3", "spacy/tokens/doc.pxd"], ["80", "spacy/tokens/doc.pyx"]]}, "129": {"issue_url": "https://github.com/explosion/spaCy/issues/4687", "issue_id": "#4687", "issue_summary": "Doc.similarity GPU bug still present in 2.2.2: TypeError: Unsupported type <class 'numpy.ndarray'>", "issue_description": "rjurney commented on 21 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\nFor full, working example using data from S3, see: https://github.com/rjurney/weakly_supervised_learning_code/blob/master/ch05/Snorkel.ipynb\nimport cupy\nimport spacy\n\nspacy.prefer_gpu()\n\n# Download the spaCy english model\nspacy.cli.download('en_core_web_lg')\n\nnlp = spacy.load(\"en_core_web_lg\")\n\ndoc1 = nlp(df['_Body'][0])\ndoc2 = nlp(df['_Body'][1])\n\nprint(type(doc1), type(doc2))\ndoc1.similarity(doc2)\nCell output / error:\n\u2714 Download and installation successful\nYou can now load the model via spacy.load('en_core_web_lg')\n<class 'spacy.tokens.doc.Doc'> <class 'spacy.tokens.doc.Doc'>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-9-319329f4354b> in <module>\n     10 \n     11 print(type(doc1), type(doc2))\n---> 12 doc1.similarity(doc2)\n     13 \n     14 # df_sample['spacy'] = df['_Body'].apply(lambda x: nlp(x))\n\ndoc.pyx in spacy.tokens.doc.Doc.similarity()\n\ndoc.pyx in spacy.tokens.doc.Doc.vector_norm.__get__()\n\ndoc.pyx in __iter__()\n\ncupy/core/core.pyx in cupy.core.core.ndarray.__add__()\n\ncupy/core/_kernel.pyx in cupy.core._kernel.ufunc.__call__()\n\ncupy/core/_kernel.pyx in cupy.core._kernel._preprocess_args()\n\nTypeError: Unsupported type <class 'numpy.ndarray'>\nNote that without spacy.prefer_gpu(), this works fine. Also note that cupy alone works fine. CUDA/cudNN setup is fine.\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Linux-4.15.0-66-generic-x86_64-with-debian-buster-sid\nPython version: 3.7.4\nCUDA version: 10.0\nCUDNN version: 7.6.5.32-1+cuda10.0_amd64\nOperating System: Ubuntu Linux 18.04\nPython Version Used: Anaconda3-2019.07 Python 3.7.5\nEnvironment Information: pip 19.3.1 / pip-tools 4.2.0 / pip-compile --output-file requirements.linux.txt requirements.in / pip install -r requirements.linux.txt\nHere is my requirements.linux.txt:\n#\n# This file is autogenerated by pip-compile\n# To update, run:\n#\n#    pip-compile --output-file=requirements.linux.txt requirements.in\n#\n-e git+git://github.com/snorkel-team/snorkel@master#egg=snorkel\nabsl-py==0.8.1            # via tensorboard, tensorflow-gpu\nargh==0.26.2              # via watchdog\nastor==0.8.0              # via tensorflow-gpu\nattrs==19.3.0             # via jsonschema\nbackcall==0.1.0           # via ipython\nbeautifulsoup4==4.8.1\nbleach==3.1.0             # via nbconvert\nblessings==1.7            # via gpustat\nblis==0.4.1               # via spacy, thinc\nboto3==1.10.23\nboto==2.49.0              # via smart-open\nbotocore==1.13.23         # via boto3, s3fs, s3transfer\ncachetools==3.1.1         # via google-auth\ncertifi==2019.9.11        # via requests, sentry-sdk\nchardet==3.0.4            # via requests\nclick==7.0                # via pip-tools, wandb\nconfigparser==4.0.2       # via wandb\ncupy-cuda100==6.5.0       # via spacy\ncycler==0.10.0            # via matplotlib\ncymem==2.0.3              # via preshed, spacy, thinc\ndecorator==4.4.1          # via ipython, networkx, traitlets\ndefusedxml==0.6.0         # via nbconvert\ndill==0.3.1.1\ndocker-pycreds==0.4.0     # via wandb\ndocutils==0.15.2          # via botocore\nentrypoints==0.3          # via nbconvert\nfastrlock==0.4            # via cupy-cuda100\nfrozendict==1.2\nfsspec==0.6.0             # via s3fs\ngast==0.2.2               # via tensorflow-gpu\ngensim==3.8.1\ngitdb2==2.0.6             # via gitpython\ngitpython==3.0.5          # via wandb\ngoogle-auth-oauthlib==0.4.1  # via tensorboard\ngoogle-auth==1.7.1        # via google-auth-oauthlib, tensorboard\ngoogle-pasta==0.1.8       # via tensorflow-gpu\ngpustat==0.6.0\ngql==0.1.0                # via wandb\ngraphql-core==2.2.1       # via gql\ngrpcio==1.25.0            # via tensorboard, tensorflow-gpu\nh5py==2.10.0              # via keras-applications\nidna==2.8                 # via requests\nimportlib-metadata==0.23  # via jsonschema, spacy\nipykernel==5.1.3          # via ipywidgets, jupyter, jupyter-console, notebook, qtconsole\nipython-genutils==0.2.0   # via nbformat, notebook, qtconsole, traitlets\nipython==7.9.0\nipywidgets==7.5.1         # via jupyter\niso8601==0.1.12\njedi==0.15.1              # via ipython\njinja2==2.10.3            # via nbconvert, notebook\njmespath==0.9.4           # via boto3, botocore\njoblib==0.14.0            # via scikit-learn\njsonschema==3.2.0         # via nbformat\njupyter-client==5.3.4     # via ipykernel, jupyter-console, notebook, qtconsole\njupyter-console==6.0.0    # via jupyter\njupyter-core==4.6.1       # via jupyter-client, nbconvert, nbformat, notebook, qtconsole\njupyter==1.0.0\nkeras-applications==1.0.8  # via tensorflow-gpu\nkeras-preprocessing==1.1.0  # via tensorflow-gpu\nkiwisolver==1.1.0         # via matplotlib\nlxml==4.4.1\nmarkdown==3.1.1           # via tensorboard\nmarkupsafe==1.1.1         # via jinja2\nmatplotlib==3.1.1         # via seaborn\nmistune==0.8.4            # via nbconvert\nmore-itertools==7.2.0     # via zipp\nmunkres==1.1.2\nmurmurhash==1.0.2         # via preshed, spacy, thinc\nnbconvert==5.6.1          # via jupyter, notebook\nnbformat==4.4.0           # via ipywidgets, nbconvert, notebook\nnetworkx==2.3\nnltk==3.4.5\nnotebook==6.0.2           # via jupyter, widgetsnbextension\nnumpy==1.17.4\nnvidia-ml-py3==7.352.0    # via gpustat, wandb\noauthlib==3.1.0           # via requests-oauthlib\nopt-einsum==3.1.0         # via tensorflow-gpu\npandas==0.25.3\npandocfilters==1.4.2      # via nbconvert\nparso==0.5.1              # via jedi\npathtools==0.1.2          # via watchdog\npexpect==4.7.0            # via ipython\npickleshare==0.7.5        # via ipython\npip-tools==4.2.0\nplac==1.1.3               # via spacy, thinc\npreshed==3.0.2            # via spacy, thinc\nprometheus-client==0.7.1  # via notebook\npromise==2.2.1            # via gql, graphql-core\nprompt-toolkit==2.0.10    # via ipython, jupyter-console\nprotobuf==3.10.0          # via tensorboard, tensorboardx, tensorflow-gpu, tensorflow-hub\npsutil==5.6.5             # via gpustat, wandb\nptyprocess==0.6.0         # via pexpect, terminado\npy4j==0.10.7              # via pyspark\npyarrow==0.14.1\npyasn1-modules==0.2.7     # via google-auth\npyasn1==0.4.8             # via pyasn1-modules, rsa\npygments==2.4.2           # via ipython, jupyter-console, nbconvert, qtconsole\npyparsing==2.4.5          # via matplotlib\npyrsistent==0.15.5        # via jsonschema\npyspark==2.4.4\npython-dateutil==2.8.0    # via botocore, jupyter-client, matplotlib, pandas, wandb\npytz==2019.3              # via pandas\npyyaml==5.1.2             # via watchdog\npyzmq==18.1.1             # via jupyter-client, notebook\nqtconsole==4.6.0          # via jupyter\nrequests-oauthlib==1.3.0  # via google-auth-oauthlib\nrequests==2.22.0\nrsa==4.0                  # via google-auth\nrx==1.6.1                 # via graphql-core\ns3fs==0.4.0\ns3transfer==0.2.1         # via boto3\nscikit-learn==0.21.3\nscipy==1.3.2              # via gensim, scikit-learn, seaborn\nseaborn==0.9.0\nsend2trash==1.5.0         # via notebook\nsentry-sdk==0.13.2        # via wandb\nshortuuid==0.5.0          # via wandb\nsix==1.13.0               # via absl-py, bleach, blessings, cupy-cuda100, cycler, docker-pycreds, gensim, google-auth, google-pasta, gpustat, gql, graphql-core, grpcio, h5py, jsonschema, keras-preprocessing, nltk, pip-tools, promise, prompt-toolkit, protobuf, pyarrow, pyrsistent, python-dateutil, tensorboard, tensorboardx, tensorflow-gpu, tensorflow-hub, traitlets, wandb\nsmart-open==1.9.0         # via gensim\nsmmap2==2.0.5             # via gitdb2\nsoupsieve==1.9.5          # via beautifulsoup4\nspacy[cuda100]==2.2.2\nsrsly==0.2.0              # via spacy, thinc\nsubprocess32==3.5.4       # via wandb\ntensorboard==2.0.1        # via tensorflow-gpu\ntensorboardx==1.9\ntensorflow-estimator==2.0.1  # via tensorflow-gpu\ntensorflow-gpu==2.0.0\ntensorflow-hub==0.7.0\ntermcolor==1.1.0          # via tensorflow-gpu\nterminado==0.8.3          # via notebook\ntestpath==0.4.4           # via nbconvert\ntextblob==0.15.3\ntexttable==1.6.2\nthinc==7.3.1              # via spacy\ntorch==1.1.0\ntornado==6.0.3            # via ipykernel, jupyter-client, notebook, terminado\ntqdm==4.38.0              # via thinc\ntraitlets==4.3.3          # via ipykernel, ipython, ipywidgets, jupyter-client, jupyter-core, nbconvert, nbformat, notebook, qtconsole\nurllib3==1.25.7           # via botocore, requests, sentry-sdk\nwandb==0.8.15\nwasabi==0.4.0             # via spacy, thinc\nwatchdog==0.9.0           # via wandb\nwcwidth==0.1.7            # via prompt-toolkit\nwebencodings==0.5.1       # via bleach\nwerkzeug==0.16.0          # via tensorboard\nwheel==0.33.6             # via tensorboard, tensorflow-gpu\nwidgetsnbextension==3.5.1  # via ipywidgets\nwrapt==1.11.2             # via tensorflow-gpu\nzipp==0.6.0               # via importlib-metadata\n\n# The following packages are considered to be unsafe in a requirements file:\n# setuptools==41.6.0        # via google-auth, ipython, jsonschema, kiwisolver, markdown, protobuf, spacy, tensorboard\nHere is my pip freeze output:\nabsl-py==0.8.1\nargh==0.26.2\nastor==0.8.0\nattrs==19.3.0\nbackcall==0.1.0\nbeautifulsoup4==4.8.1\nbleach==3.1.0\nblessings==1.7\nblis==0.4.1\nboto==2.49.0\nboto3==1.10.23\nbotocore==1.13.23\ncachetools==3.1.1\ncertifi==2019.9.11\nchardet==3.0.4\nClick==7.0\nconfigparser==4.0.2\ncupy-cuda100==6.5.0\ncycler==0.10.0\ncymem==2.0.3\ndecorator==4.4.1\ndefusedxml==0.6.0\ndill==0.3.1.1\ndocker-pycreds==0.4.0\ndocutils==0.15.2\nentrypoints==0.3\nfastrlock==0.4\nfrozendict==1.2\nfsspec==0.6.0\ngast==0.2.2\ngensim==3.8.1\ngitdb2==2.0.6\nGitPython==3.0.5\ngoogle-auth==1.7.1\ngoogle-auth-oauthlib==0.4.1\ngoogle-pasta==0.1.8\ngpustat==0.6.0\ngql==0.1.0\ngraphql-core==2.2.1\ngrpcio==1.25.0\nh5py==2.10.0\nidna==2.8\nimportlib-metadata==0.23\nipykernel==5.1.3\nipython==7.9.0\nipython-genutils==0.2.0\nipywidgets==7.5.1\niso8601==0.1.12\njedi==0.15.1\nJinja2==2.10.3\njmespath==0.9.4\njoblib==0.14.0\njsonschema==3.2.0\njupyter==1.0.0\njupyter-client==5.3.4\njupyter-console==6.0.0\njupyter-core==4.6.1\nKeras-Applications==1.0.8\nKeras-Preprocessing==1.1.0\nkiwisolver==1.1.0\nlxml==4.4.1\nMarkdown==3.1.1\nMarkupSafe==1.1.1\nmatplotlib==3.1.1\nmistune==0.8.4\nmore-itertools==7.2.0\nmunkres==1.1.2\nmurmurhash==1.0.2\nnbconvert==5.6.1\nnbformat==4.4.0\nnetworkx==2.3\nnltk==3.4.5\nnotebook==6.0.2\nnumpy==1.17.4\nnvidia-ml-py3==7.352.0\noauthlib==3.1.0\nopt-einsum==3.1.0\npandas==0.25.3\npandocfilters==1.4.2\nparso==0.5.1\npathtools==0.1.2\npexpect==4.7.0\npickleshare==0.7.5\npip-tools==4.2.0\nplac==1.1.3\npreshed==3.0.2\nprometheus-client==0.7.1\npromise==2.2.1\nprompt-toolkit==2.0.10\nprotobuf==3.10.0\npsutil==5.6.5\nptyprocess==0.6.0\npy4j==0.10.7\npyarrow==0.14.1\npyasn1==0.4.8\npyasn1-modules==0.2.7\nPygments==2.4.2\npyparsing==2.4.5\npyrsistent==0.15.5\npyspark==2.4.4\npython-dateutil==2.8.0\npytz==2019.3\nPyYAML==5.1.2\npyzmq==18.1.1\nqtconsole==4.6.0\nrequests==2.22.0\nrequests-oauthlib==1.3.0\nrsa==4.0\nRx==1.6.1\ns3fs==0.4.0\ns3transfer==0.2.1\nscikit-learn==0.21.3\nscipy==1.3.2\nseaborn==0.9.0\nSend2Trash==1.5.0\nsentry-sdk==0.13.2\nshortuuid==0.5.0\nsix==1.13.0\nsmart-open==1.9.0\nsmmap2==2.0.5\n-e git://github.com/snorkel-team/snorkel@3cf96b797937c6a1d843e7e5dccd4ce8737c8a5a#egg=snorkel\nsoupsieve==1.9.5\nspacy==2.2.2\nsrsly==0.2.0\nsubprocess32==3.5.4\ntensorboard==2.0.1\ntensorboardX==1.9\ntensorflow-estimator==2.0.1\ntensorflow-gpu==2.0.0\ntensorflow-hub==0.7.0\ntermcolor==1.1.0\nterminado==0.8.3\ntestpath==0.4.4\ntextblob==0.15.3\ntexttable==1.6.2\nthinc==7.3.1\ntorch==1.1.0\ntornado==6.0.3\ntqdm==4.38.0\ntraitlets==4.3.3\nurllib3==1.25.7\nwandb==0.8.15\nwasabi==0.4.0\nwatchdog==0.9.0\nwcwidth==0.1.7\nwebencodings==0.5.1\nWerkzeug==0.16.0\nwidgetsnbextension==3.5.1\nwrapt==1.11.2\nzipp==0.6.0", "issue_status": "Closed", "issue_reporting_time": "2019-11-20T20:38:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "130": {"issue_url": "https://github.com/explosion/spaCy/issues/4684", "issue_id": "#4684", "issue_summary": "Error when using offsets_from_biluo_tags", "issue_description": "Niels-Dekker commented on 20 Nov 2019\nI am trying to convert IOB tags from another NER model to the spaCy Span format.\nConsider the following example sentence:\nThe merger of Hema and Bijenkorf B.V. takes place on monday the 10th of august.\nThe IOB output from my model looks like this:\n['O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O']\nI can correctly transform this to BILUO using spacy.gold's iob_to_biluo:\n['O', 'O', 'O', 'U-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'L-ORG', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'L-DATE', 'O']\nI used to be able to use offsets_from_biluo_tags to transform this to the required offsets. However, when used now, I get:\nIndexError: [E035] Error creating span with start 14 and end 18 for Doc of length 16.\nI suspect that it has to do with the periods in 'B.V.', because running the same steps for this text works just fine:\nThe merger of Hema and Bijenkorf takes place on monday the 10th of august.\nThis issue breaks both the offsets_from_biluo_tags and the spans_from_biluo_tags from spacy.gold.\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.0", "issue_status": "Closed", "issue_reporting_time": "2019-11-20T17:40:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "131": {"issue_url": "https://github.com/explosion/spaCy/issues/4683", "issue_id": "#4683", "issue_summary": "Packing spacy console application using Pyinstaller throws error?", "issue_description": "AbhishekKargawal commented on 20 Nov 2019\nI have trained a spacy ner model and using the trained model in console application to predict the entities.\nWhen I run the application from IDE(PyCharm), it runs smoothly no error, properly loads the Spacy model predict the entities and all. But when I convert this application to executable file and try to run it. It throws me an error. I have attached screenshot of the error.\nI believe that #2536 this issue is similar to mine. But not able to resolve it.\nOperating system: Windows\nPython: 3.7\nSpacy: 2.2.1", "issue_status": "Closed", "issue_reporting_time": "2019-11-20T17:04:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "132": {"issue_url": "https://github.com/explosion/spaCy/issues/4682", "issue_id": "#4682", "issue_summary": "SpaCy for JAVA language ??", "issue_description": "himanshurobo commented on 20 Nov 2019\nFeature description\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.", "issue_status": "Closed", "issue_reporting_time": "2019-11-20T16:50:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "133": {"issue_url": "https://github.com/explosion/spaCy/issues/4681", "issue_id": "#4681", "issue_summary": "how do i fine tune an entity label type?", "issue_description": "yishairasowsky commented on 20 Nov 2019 \u2022\nedited\nThank you for your service and support for spacy and prodigy which we just bought!\nI want to teach the model to recognize specific types of dates in a text document.\nSo I wish to run through all the dates, perhaps using...\nprodigy ner.teach your_set en_core_web_sm C:\\Users\\Yishai\\Downloads\\lease-7.txt --label DATE\nthen I want to tell the machine which ones should be labeled as start dates and which are end dates.\nIs this considered training a new entity type?\nThanks!\nWhich page or section is this issue related to?\nhttps://support.prodi.gy/t/how-to-modify-labels-entities-in-default-models-en-en-core-web-lg-etc-and-retrain/1085/3", "issue_status": "Closed", "issue_reporting_time": "2019-11-20T16:47:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "134": {"issue_url": "https://github.com/explosion/spaCy/issues/4678", "issue_id": "#4678", "issue_summary": "Entities found by EntityRuler are modified by NER pipe in French", "issue_description": "olivier-compilatio commented on 20 Nov 2019\nProblem Description\nI want to use the EntityRuler for French, but when applying it before the NER pipe, it seems that the NER pipe changes the desired output and expands the entities (found by the EntityRuler) until the end of the sentence of each entity.\nAlso :\nWhen disabling the NER pipe the output is as expected.\nI can't reproduce the issue with the English model.\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.pipeline import EntityRuler  \n\nnlp = spacy.load(\"fr_core_news_md\")\nruler = EntityRuler(nlp)\npatterns = [\n    {\n        \"label\": \"QUANTITY\",\n        \"pattern\": [\n            {\"POS\": \"NUM\", \"DEP\": \"nummod\"},\n            {\"POS\": \"ADJ\", \"DEP\": \"amod\", \"OP\": \"*\"},\n            {\"POS\": {\"IN\":  [\"NOUN\", \"PNOUN\"]}},\n        ]\n    },\n]\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler, before=\"ner\")\ndoc = nlp(\n    \"J'ai trouve 10 euros par terre. Les 6 enfants vivent dans trois grandes maisons. Il fait 3\u00b0C aujourd'hui, malgr\u00e9 la brume et la tramontane. 6.4 M$ ont \u00e9t\u00e9 \u00e9chang\u00e9s en bourse aujourd'hui.\"\n)\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n10 euros par terre. QUANTITY # expected output : \"10 euros\"\n6 enfants vivent dans QUANTITY # expected output : \"6 enfants\"\ntrois grandes maisons. QUANTITY # expected output : \"trois grandes maisons\"\n3\u00b0C aujourd'hui, malgr\u00e9 la brume et la tramontane. QUANTITY # expected output : \"3\u00b0C\"\n6.4 M$ ont \u00e9t\u00e9 \u00e9chang\u00e9 en bourse aujourd'hui. QUANTITY # expected output : \"6.4 M$\"\nYour Environment\nOperating System: Ubuntu 18.04 LTS\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.8\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-20T15:49:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "135": {"issue_url": "https://github.com/explosion/spaCy/issues/4677", "issue_id": "#4677", "issue_summary": "Tag \"DET\" missing from the Portuguese tagmap", "issue_description": "BLKSerene commented on 20 Nov 2019\nThe tag \"DET\" is missing from the Portuguese tagmap and is mapped to \"X\" rather than \"DET\" (the word \"no\" in the following example).\n>>> import spacy\n>>> nlp = spacy.load('pt_core_news_sm')\n>>> doc = nlp('em Dam\u00e3o e Diu e no estado de Goa')\n>>> for token in doc:\n print(token.text, token.tag_, token.pos_)\n\nem PRP|@ADVL> ADP\nDam\u00e3o <first-cjt>|PROP|M|S|@P< PROPN\ne <co-prparg>|KC|@CO CCONJ\nDiu PROP|M|S|@P< PROPN\ne <co-prparg>|KC|@CO CCONJ\nno DET X\nestado <cjt>|<prop>|<np-def>|N|M|S|@P< NOUN\nde PRP|@N< ADP\nGoa PROP|M|S|@P< PROPN\nOperating System: Windows10 x64\nPython Version Used: 3.7.5 x64\nspaCy Version Used: 2.2.2", "issue_status": "Closed", "issue_reporting_time": "2019-11-20T15:49:27Z", "fixed_by": "#4696", "pull_request_summary": "Add missing tags to el/es/pt tag maps", "pull_request_description": "Collaborator\nadrianeboyd commented on 22 Nov 2019\nDescription\nAdd missing tags to el/es/pt tag maps.\nFixes #4677.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-23T13:57:21Z", "files_changed": [["18", "spacy/lang/el/tag_map.py"], ["3", "spacy/lang/es/tag_map.py"], ["14", "spacy/lang/pt/tag_map.py"]]}, "136": {"issue_url": "https://github.com/explosion/spaCy/issues/4674", "issue_id": "#4674", "issue_summary": "Empty AssertionError: for KnowledgeBase.dump", "issue_description": "Contributor\nkabirkhan commented on 20 Nov 2019\nHow to reproduce the behaviour\nI'm creating a component for candidate generation using nearest neighbors and am trying to use the spacy KnowledgeBase.\nI'm running into this error in boty spacy 2.1.9 and spacy 2.2.2 where I get an empty AssertionError:\nwhen trying to dump the KnowledgeBase like below.\ndef to_disk(self, path, exclude=tuple(), **kwargs):\n        \"\"\"Save data to disk\"\"\"\n        path = ensure_path(path)\n        serialize = {}\n        serialize[\"cfg\"] = lambda p: srsly.write_json(p, {\"k_neighbors\": self.k})\n        serialize[\"kb\"] = lambda p: self.kb.dump(p)\n        exclude = util.get_serialization_exclude(serialize, exclude, kwargs)\n        util.to_disk(path, serialize, exclude)\n        self.cg.to_disk(path)\nSeems to be an issue actually writing the file using the cython c file I/O libraries.\nIn the file: kb.pyx in the Writer _write method. I guess the status is not 1.\ncdef int _write(self, void* value, size_t size) except -1:\n    status = fwrite(value, size, 1, self._fp)\n    assert status == 1, status\nYour Environment\nOperating System: Tried on Ubuntu 18.04 in WSL and Ubuntu 16.04 in Azure Machine Learning\nPython Version Used: 3.6\nspaCy Version Used: Issue in 2.1.9 and 2.2.2\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-19T23:05:38Z", "fixed_by": "#4794", "pull_request_summary": "More robust set entities method in KB", "pull_request_description": "Member\nsvlandeg commented on 11 Dec 2019 \u2022\nedited\nDescription\nMake set_entities in the KnowledgeBase more robust to input lists that define the same KB ID more than once. While this shouldn't really happen, the method now recovers by ignoring the duplicate entry and throwing a user-friendly warning.\nBefore this PR, this specific method would cause a crash if you attempted IO later, because the KB would think it had more entities than it did (counting the duplicate ones instead of only the unique set).\nFixes #4674 - added unit test.\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-13T09:45:30Z", "files_changed": [["3", "spacy/errors.py"], ["31", "spacy/kb.pyx"], ["34", "spacy/tests/regression/test_issue4674.py"]]}, "137": {"issue_url": "https://github.com/explosion/spaCy/issues/4673", "issue_id": "#4673", "issue_summary": "vector_norm throws error for unusual text in sentence with more than one word.", "issue_description": "Contributor\nmmaybeno commented on 20 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\nI found this bug that I'm not sure if it resides in spacy or cupy. It only appears on GPU instances and when you try to get vectors from a multiple word document containing non standard words. Any help tracking it down with a potential fix would be fantastic.\nimport en_core_web_md\nimport spacy\nspacy.prefer_gpu()\nnlp = en_core_web_md.load()\n\ndoc = nlp(\"somerandomword\")\ndoc.vector_norm\n# works\n\ndoc = nlp(\"somerandomword.\")\ndoc.vector_norm\n# throws type error\n\ndoc = nlp(\"The somerandomword\")\ndoc.vector_norm\n# throws type error\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-96-d19c7b8f8943> in <module>()\n----> 1 doc.vector_norm\ndoc.pyx in spacy.tokens.doc.Doc.vector_norm.__get__()\ndoc.pyx in __iter__()\ncupy/core/core.pyx in cupy.core.core.ndarray.__add__()\ncupy/core/_kernel.pyx in cupy.core._kernel.ufunc.__call__()\ncupy/core/_kernel.pyx in cupy.core._kernel._preprocess_args()\nTypeError: Unsupported type <class 'numpy.ndarray'>\nYour Environment\nOperating System: Ubuntu 18.04.3\nPython Version Used: 3.6.8\nspaCy Version Used: 2.2.2\nEnvironment Information: Running on Google Colab but also experienced it on other GPU instances.\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   34C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n!pip install spacy==2.2.2\n!pip install chainer\n!pip install thinc_gpu_ops thinc\n!python -m spacy download en_core_web_md ", "issue_status": "Closed", "issue_reporting_time": "2019-11-19T21:43:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "138": {"issue_url": "https://github.com/explosion/spaCy/issues/4669", "issue_id": "#4669", "issue_summary": "adding new lang to spacy-lookups-data", "issue_description": "ramilrg commented on 19 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\nHi,\nI want to add a new language to \"spacy-lookups-data\", I already made a necessary changes on setup.cfg, setup.py and init.py files, and a new generated az_lemma_lookup.json is also exists, but when I try to test it got an error:\n####################\naz_nlp = <spacy.lang.az.Azeri object at 0x000002A0078E92E8>\nstring = '\u00e7atmaqdak\u0131ndans\u0131n\u0131z', lemma = '\u00e7atmaq'\n\n    @pytest.mark.parametrize(\n        \"string,lemma\",\n        [\n            (\"\u015fik\u0259nc\u0259l\u0259rd\u0259kinin\", \"\u015fik\u0259nc\u0259\"),\n            (\"g\u00f6zayd\u0131nl\u0131\u011f\u0131ndak\u0131lardanm\u0131\u015f\", \"g\u00f6zayd\u0131nl\u0131\u011f\u0131\"),\n            (\"\u00e7atmaqdak\u0131ndans\u0131n\u0131z\", \"\u00e7atmaq\"),\n        ],\n    )\n    def test_az_lemmatizer_lookup_assigns(az_nlp, string, lemma):\n        tokens = az_nlp(string)\n>       assert tokens[0].lemma_ == lemma\nE       AssertionError: assert '\u00e7atmaqdak\u0131ndans\u0131n\u0131z' == '\u00e7atmaq'\nE         - \u00e7atmaqdak\u0131ndans\u0131n\u0131z\nE         + \u00e7atmaq\n\nspacy_lookups_data\\tests\\test_az.py:23: AssertionError\n####################\nYour Environment\nOperating System: Win10\nPython Version Used: 3.6\nspaCy Version Used: 2.2\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-19T08:04:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "139": {"issue_url": "https://github.com/explosion/spaCy/issues/4668", "issue_id": "#4668", "issue_summary": "Loading the same model repeatedly gradually consumes more memory", "issue_description": "mabraham commented on 19 Nov 2019\nWhen loading the same model in spaCy many times, it allocates a small amount of new memory when a model is loaded. That memory cannot be cleaned up with python gc. This behaviour does not suit use in a long-lived server where the model is a parameter derived from the user, because eventually the server process will run out of memory. The repro script below continues to allocate new memory even with the iteration count increased to at least 200. Apparently the core data persists in CPython memory space and can be re-used, but each new model loaded also allocates additional memory, and this is problematic.\nPossible solutions\nrun spaCy in its own process/container and accept the overhead of process creation, model load, and communication\nchange Language.load() to allocate data that is unique to the load() of that model in Python space, so that Python scoping means that gc will eventually clean it up after the reference goes out of scope\nchange Language.load() to not allocate data that is unique to the load() of that model, so that repeated load() of the same model does not lead to growth in memory allocated\nadd new method Language.unload() deallocates all memory in CPython for that model\nOutput of script below:\nHow to reproduce the behaviour\nScript\nYour Environment\nspaCy version: 2.2.2\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nPython version: 3.7.5", "issue_status": "Closed", "issue_reporting_time": "2019-11-18T19:28:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "140": {"issue_url": "https://github.com/explosion/spaCy/issues/4667", "issue_id": "#4667", "issue_summary": "BERT Model (German) does not work in multiprocessing mode", "issue_description": "engrsfi commented on 18 Nov 2019 \u2022\nedited\nThe BERT model for the German language works fine if I use it in the main process or use it in a multi-threaded way. However, as soon as, I use the model in a multi-processing mode, it gets stuck at the model inference time.\nNormal usage which works fine:\nimport spacy\n\n# Load model (German language)\nbert_model = spacy.load('de_trf_bertbasecased_lg')\n\n# Run model \nresult = bert_model(\"This is just a test input\")\nUsage in multiprocessing mode which does not work:\nimport spacy\n\nbert_model = None\n\ndef worker(text):\n    # Load model (German language)\n    if not bert_model:\n       bert_model = spacy.load('de_trf_bertbasecased_lg')\n\n    # Run model \n    result = bert_model(text) #It just gets stuck here.\n\nfrom multiprocessing import Pool\n\nlist_of_text_strings = [\"this is test 1\", \"this is test 2\", \"this is test 3\"]\n\nwith Pool(2) as p:\n    p.map(worker, list_of_text_strings)\nIn the multiprocessing model, the model loading statement is executed but then it gets stuck at inference time. Any ideas what is going wrong here? Is it a bug or am I doing something really wrong? or could it be that its a problem only with German-language Bert?\nYour Environment\nspaCy version: 2.2.2\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.9", "issue_status": "Closed", "issue_reporting_time": "2019-11-18T11:47:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "141": {"issue_url": "https://github.com/explosion/spaCy/issues/4663", "issue_id": "#4663", "issue_summary": "Segmentation fault when using pipe on large amounts of data", "issue_description": "gabeorlanski commented on 17 Nov 2019 \u2022\nedited\nThe code I had used when this error appeared:\nnlp = en_core_web_sm.load()\nnlp.pipe(reports)\nreports is a list of 2617 documents averaging 27K characters and 360 lines. In total, it has 71 million characters and is ~ 140 megabytes of data. This represents a single group of around 200 groups with a total of 400K documents. It is on the smaller side, so I assume it would also break for the groups with a lot more documents in them. One issue that could be causing this is that these reports are the result of parsing pdfs that include tables and tend to have strange characters in them. I am decoding with utf-8, but that could possibly be causing issues in the parser. The tables in the document do not have a uniform format, so they are quite hard to get rid of.\nEnvironment:\nspaCy version: 2.2.2\nPlatform: Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-centos-7.7.1908-Core\nPython version: 3.7.4\n128 GB of ram\nOne other thing that I noticed was that I was getting an error when trying to individually run nlp(report) using multiprocessing on the batched reports. The error I was getting:\nerror(\"'i' format requires -2147483648 <= number <= 2147483647\")\nIt came after it had printed out the entire document. The code I had used for that was:\nwith mp.Pool(cores) as p:\n    p.imap_unordered(parseBatch, (nlp,batches,))\n\ndef parseBatch(nlp, doc_batch):\n    return [nlp(report) for report in doc_batch]\nI had googled the report and found that it related to multiprocessing and joblib so that the segfault could be on their end. In terms of memory usage, while running pipe, it had not gone above 50%. I also tested this with and without documents that have more than 100K characters, but the issue persisted.", "issue_status": "Closed", "issue_reporting_time": "2019-11-16T22:29:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "142": {"issue_url": "https://github.com/explosion/spaCy/issues/4659", "issue_id": "#4659", "issue_summary": "model saving and loading difference", "issue_description": "chunlei2 commented on 16 Nov 2019\nHow to reproduce the behaviour\n#4649 Hi, this is a follow up with the problem since it is closed. I don't mean to create a representative model, just want to figure out why the saved model is different from the model before saved. I upgrade the spacy to 2.2.2 and still have the issue. I reproduce the error using Google Colab. link\nYour Environment\nOperating System: Google Colab\nPython Version Used: 3.6\nspaCy Version Used: 2.2.2\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-15T22:40:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "143": {"issue_url": "https://github.com/explosion/spaCy/issues/4658", "issue_id": "#4658", "issue_summary": "TypeError: Model() takes exactly 1 positional argument (0 given) in from_disk for Parser", "issue_description": "Contributor\nkabirkhan commented on 16 Nov 2019\nHow to reproduce the behaviour\nRunning\nimport spacy\n\nnlp = spacy.load('my_custom_ner_model')\nGets the following traceback\nTraceback (most recent call last):\n  File \"evaluate_model.py\", line 28, in <module>\n    plac.call(evaluate)\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/.spacy2.2-venv/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/.spacy2.2-venv/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"evaluate_model.py\", line 16, in evaluate\n    skills_model = EntityRecognizer(model_path=model, input_text_col=\"text\", input_id_col=\"id\")\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/src/skills_ner/pipeline.py\", line 40, in __init__\n    self.nlp = self.load_model()\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/src/skills_ner/pipeline.py\", line 111, in load_model\n    nlp = spacy.load(read_folder)\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/.spacy2.2-venv/lib/python3.6/site-packages/spacy/__init__.py\", line 30, in load\n    return util.load_model(name, **overrides)\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/.spacy2.2-venv/lib/python3.6/site-packages/spacy/util.py\", line 217, in load_model\n    return load_model_from_path(Path(name), **overrides)\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/.spacy2.2-venv/lib/python3.6/site-packages/spacy/util.py\", line 262, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/.spacy2.2-venv/lib/python3.6/site-packages/spacy/language.py\", line 940, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/.spacy2.2-venv/lib/python3.6/site-packages/spacy/util.py\", line 733, in from_disk\n    reader(path / key)\n  File \"/mnt/c/Users/kakh/Documents/CognitiveServices/API-TextAnalytics-NER.CloudServices/.spacy2.2-venv/lib/python3.6/site-packages/spacy/language.py\", line 935, in <lambda>\n    p, exclude=[\"vocab\"]\n  File \"nn_parser.pyx\", line 657, in spacy.syntax.nn_parser.Parser.from_disk\n  File \"nn_parser.pyx\", line 55, in spacy.syntax.nn_parser.Parser.Model\nTypeError: Model() takes exactly 1 positional argument (0 given)\nThe parser from_disk method doesn't initialize the Model with the nr_class argument.\nLooks like: #3221 is essentially the same issue.\nI'd fix this myself in a PR but the default arg of 1 seems obvious for textcat, less obvious what it should be for the parser since it might be different between the Parser and EntityRecognizer.\nYour Environment\nOperating System: Windows Subsystem Linux (18.04)\nPython Version Used: 3.6\nspaCy Version Used: 2.2.2\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-15T19:15:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "144": {"issue_url": "https://github.com/explosion/spaCy/issues/4654", "issue_id": "#4654", "issue_summary": "Segmentation fault when saving textcat model with numpy.int64 labels to disk", "issue_description": "samuelhoenle commented on 15 Nov 2019\nI'm using TextCategizer to train spaCy to categorize whole paragraphs. After training the model, I want to save it to disk using to_disk(). During that process, a segmentation fault occurs.\nThe stack trace:\nFatal Python error: Segmentation fault\n\nCurrent thread 0x0000000105128dc0 (most recent call first):\n  File \"/usr/local/lib/python3.7/site-packages/srsly/_json_api.py\", line 26 in json_dumps\n  File \"/usr/local/lib/python3.7/site-packages/srsly/_json_api.py\", line 74 in write_json\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 627 in to_disk\n  File \"/usr/local/lib/python3.7/site-packages/spacy/language.py\", line 798 in <lambda>\n  File \"/usr/local/lib/python3.7/site-packages/spacy/util.py\", line 627 in to_disk\n  File \"/usr/local/lib/python3.7/site-packages/spacy/language.py\", line 800 in to_disk\nHow to reproduce the behaviour\nThe labels being added to the textcat pipe is of the type numpy.int64. (A cast solves the problem.)\nYour Environment\nOperating System: macOS Catalina 10.15.1\nspaCy version: 2.1.8\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nPython version: 3.7.5\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-11-15T12:48:09Z", "fixed_by": "#4690", "pull_request_summary": "Add error for non-string labels", "pull_request_description": "Collaborator\nadrianeboyd commented on 21 Nov 2019 \u2022\nedited\nDescription\nAdd error when attempting to add non-string labels to Tagger or TextCategorizer.\nFixes #4654.\nTypes of change\nEnhancement.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-21T15:24:11Z", "files_changed": [["1", "spacy/errors.py"], ["5", "spacy/pipeline/pipes.pyx"], ["14", "spacy/tests/pipeline/test_tagger.py"], ["8", "spacy/tests/pipeline/test_textcat.py"]]}, "145": {"issue_url": "https://github.com/explosion/spaCy/issues/4653", "issue_id": "#4653", "issue_summary": "ADD some exceptions to TOKENIZER_EXCEPTIONS in spanish", "issue_description": "rbague5 commented on 15 Nov 2019\nNeed to add more abbreviation exceptions in Spanish because when tokenizing sentences isn't 100% accurate, those are the abbreviations:\nabbreviations = ['sra.', 'd\u00aa.', 'd\u00f1a.', 'sras.', 'sres.', 'sr.', 'excmos.', 'excmo.', 'excma.', 'excmas', 'ilma.', 'ilmas.','ilmo.', 'ilmos.', 'art.', 'arts.', 'n\u00fam.', 'cp.', 'c.p.', 's.l.', 'rcud.', 'rcuds.', 'rec.','Sra.', 'D\u00aa.', 'D\u00f1a.', 'Sras.', 'Sres.', 'sr.', 'Excmo.', 'Excmos.', 'Excma.', 'Excmas', 'Ilma.', 'Ilmas.','Ilmo.', 'Ilmos.', 'Art.', 'Arts.', 'N\u00fam.', 'CP.', 'C.P.', 'S.L.', 'Rcud.', 'Rcuds.', 'Rec.']\nYour Environment\nspaCy version: 2.2.1\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.4\nModels: es", "issue_status": "Closed", "issue_reporting_time": "2019-11-15T10:31:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "146": {"issue_url": "https://github.com/explosion/spaCy/issues/4652", "issue_id": "#4652", "issue_summary": "Is possible to run spacy model in Ray?", "issue_description": "swicaksono commented on 15 Nov 2019\nI'm trying to load spaCy model in ray.remote, but when I'm trying to get the results, it's getting an error in the serialization problem. Is it possible to load the model and run it in distributed clusters?\nThank you.", "issue_status": "Closed", "issue_reporting_time": "2019-11-15T08:49:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "147": {"issue_url": "https://github.com/explosion/spaCy/issues/4651", "issue_id": "#4651", "issue_summary": "EntityRuler from_disk method doesn't work when phrase_matcher_attr is specified", "issue_description": "Contributor\nGuiGel commented on 15 Nov 2019\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.lang.en import English\nfrom spacy.pipeline import EntityRuler\nnlp = English()\nruler = EntityRuler(nlp, phrase_matcher_attr='LOWER')\npatterns = [{\"label\": \"PYTHON_LIB\", \"pattern\": \"spacy\", \"id\": \"spaCy\"}]\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler)\nnlp.to_disk('spacy2.2_model')\ntext = \"Spacy is a python library for nlp\"\ndoc = nlp(text)\nprint('{}\\n{}'.format(text, '-'*100))\nprint(\"original model\\n\")\ndetection = [(ent.text, ent.label_, ent.ent_id_) for ent in doc.ents]\nprint('ENT TEXT -> {}\\nENT LABEL > {}\\nENT ID ---> {}\\n'.format(*detection[0]))\nnlp_loaded = spacy.load('spacy2.2_model')\nprint(\"{}\\nload model\\n\".format('-'*100))\nloaded_doc = nlp_loaded(\"Spacy is a python library for nlp.\")\ndetection = [(ent.text, ent.label_, ent.ent_id_) for ent in loaded_doc.ents]\nprint(detection)\n--> OUTPUT\nSpacy is a python library for nlp\noriginal model\nENT TEXT -> Spacy\nENT LABEL > PYTHON_LIB\nENT ID ---> spaCy\nload model\n[]\nYour Environment\nspaCy version: 2.2.2\nPlatform: Linux-4.9.0-11-amd64-x86_64-with-debian-9.11\nPython version: 3.7.3\nTemporarily, I have modified the EntityRuler from_disk method in order to have my exemple working.\nI have split the deserializer in 2 parts. One in order to deserialize the cfg first and and other one in order to deserialize the patterns. By this way the phrase_matcher_attr attribute can be take into account when the add_patterns method is called by the line: from_disk(path, deserializers_patterns, {})\nInfo\nThe code that I have used to solved the bug in order to continue working...\ndef from_disk(self, path, **kwargs):\n\"\"\"Load the entity ruler from a file. Expects a file containing\nnewline-delimited JSON (JSONL) with one entry per line.\n    path (unicode / Path): The JSONL file to load.\n    **kwargs: Other config paramters, mostly for consistency.\n    RETURNS (EntityRuler): The loaded entity ruler.\n\n    DOCS: https://spacy.io/api/entityruler#from_disk\n    \"\"\"\n    path = ensure_path(path)\n    depr_patterns_path = path.with_suffix(\".jsonl\")\n    if depr_patterns_path.is_file():\n        patterns = srsly.read_jsonl(depr_patterns_path)\n        self.add_patterns(patterns)\n    else:\n        cfg = {}\n'---------------------------------- MODIF ---------- SPLIT SERIALIZER -----------------------------'\ndeserializers_patterns = {\n\"patterns\": lambda p: self.add_patterns(\nsrsly.read_jsonl(p.with_suffix(\".jsonl\"))\n)}\ndeserializers_cfg = {\n\"cfg\": lambda p: cfg.update(srsly.read_json(p))\n}\nfrom_disk(path, deserializers_cfg, {})\n'--------------------------------------------------------------------------------------------------------------'\nself.overwrite = cfg.get(\"overwrite\", False)\nself.phrase_matcher_attr = cfg.get(\"phrase_matcher_attr\")\nself.ent_id_sep = cfg.get(\"ent_id_sep\", DEFAULT_ENT_ID_SEP)\n        if self.phrase_matcher_attr is not None:\n            self.phrase_matcher = PhraseMatcher(\n                self.nlp.vocab, attr=self.phrase_matcher_attr\n            )\n'---------------------------------- MODIF ---------- DESERIALIZE PATTERNS ----------------------'\nfrom_disk(path, deserializers_patterns, {})\nreturn self\nCan anyone reproduce the error?", "issue_status": "Closed", "issue_reporting_time": "2019-11-15T08:13:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "148": {"issue_url": "https://github.com/explosion/spaCy/issues/4647", "issue_id": "#4647", "issue_summary": "doc1.similarity(doc2)", "issue_description": "asif-khan17 commented on 14 Nov 2019\nYour Environment\nOperating System: windows 10\nPython Version Used: 3.7.0\nspaCy Version Used:1.7+\nEnvironment Information:\nHi I am new to sapcy, I want to develop a model which gives me the text similarity based on the intent.For example \"I like cats\" and \"I hate cats\" should be very dissimilar but when I am using \"similarity\" it gives me very high similarity.", "issue_status": "Closed", "issue_reporting_time": "2019-11-14T14:19:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "149": {"issue_url": "https://github.com/explosion/spaCy/issues/4646", "issue_id": "#4646", "issue_summary": "train new entity type using the command line", "issue_description": "yishairasowsky commented on 14 Nov 2019 \u2022\nedited\nThanks to your help, I was able to accomplish two things:\nTrain a new entity type, based on the example py file you provided here.\nTrain a model using the command line based on the example you provided here.\nBut what I want to do now is combine these two things.\nCan I train a new entity type via the command line?\nOr maybe no, I should just hard code my new entity types into the py file...\nWhat do you think?\nMany thanks!\nYishai\nWhich page or section is this issue related to?\nI cited the links above.", "issue_status": "Closed", "issue_reporting_time": "2019-11-14T13:43:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "150": {"issue_url": "https://github.com/explosion/spaCy/issues/4645", "issue_id": "#4645", "issue_summary": "Custom tokenizer token_match doesn't seem to take proper precedence", "issue_description": "mihail911 commented on 14 Nov 2019\nHow to reproduce the behaviour\nSo I'm using one of the non-english models to do some tokenization. Specifically I'm interested in using the french tokenizer. While it works pretty well out-of-the-box, there are a few extra cases I'd like to add as special rules. Using this doc, I added a custom tokenizer as follows:\nimport re\nfrom spacy.tokenizer import Tokenizer\nimport spacy\n\nnlp = spacy.load(\"fr_core_news_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\nhash_re = re.compile(r\"#[\\w]+\")\ndef custom_tokenizer(nlp):\n         return Tokenizer(nlp.vocab,\n                   rules=nlp.Defaults.tokenizer_exceptions,\n                   prefix_search=spacy.util.compile_prefix_regex(nlp.Defaults.prefixes).search,\n                   suffix_search=spacy.util.compile_suffix_regex(nlp.Defaults.suffixes).search,\n                   infix_finditer=spacy.util.compile_infix_regex(nlp.Defaults.infixes).finditer,\n                   token_match=hash_re.search)\n\nnlp.tokenizer = custom_tokenizer(nlp)\ndoc = nlp(\"C'est une j'ai #blah\")\nprint([t.text for t in doc])\nHowever, this produces:\n[\"C'\", 'est', 'une', \"j'\", 'ai', '#', 'blah']\nthereby not keeping the #blah token intact. My understanding given the docs is that the token_match field provides a regex that takes precedence over other rules (in this case, the trouble-maker i think is the prefix_search default regex). However, it seems like that regex precedence isn't being respected here.\nAm I doing something wrong? If this is not the way to add custom regex rules on top of existing tokenizers, then what is the proper way?\nYour Environment\nspaCy version: 2.1.8\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.9\nThanks for the help!", "issue_status": "Closed", "issue_reporting_time": "2019-11-14T03:45:04Z", "fixed_by": "#4596", "pull_request_summary": "Add tokenizer explain() debugging method", "pull_request_description": "Collaborator\nadrianeboyd commented on 6 Nov 2019 \u2022\nedited\nDescription\nThis is a series of updates to the tokenizer and tokenizer docs in order to sync the docs and the current tokenizer implementation. A new tokenizer function makes debugging the tokenizer a bit easier by implementing a slow debugging version that returns labels for each token about the pattern or rule matched, based on the pseudo-code in the docs.\nThere are some minor changes to tokenizer to bring it in sync with the intended behavior based on the docs. While updating the pseudo-code in the docs, it made sense to make a working version to check the details, so I expanded that just a bit to add some better debugging functionality.\nI think keeping this in sync with the actual tokenizer will be a similar amount of effort to keeping the pseudo-code in the docs in sync. Instead of being in the tokenizer itself, it could also be an example/demo script that takes nlp.tokenizer as an argument, but I liked being able to implement unit tests that compared its behavior to the actual tokenizer. I wish that it were easier to add tests for more of the tricky cases, since I haven't formally tested that the behavior is identical for all of the unusual test cases.\nIt's also kind of unsatisfying that it doesn't handle whitespace tokenization, but this isn't usually a source of confusion for users and I think adding it would make the pseudo-code harder to read, and it's already a bit too long. I wouldn't be opposed to adding it, though.\nFixes #4573, fixes #4645.\nEdited: implements explain() method that returns a list of (pattern_string, token_string) tuples to avoid the hacky displacy usage. Easy displacy integration is postponed to a future PR.\nOutdated description of displacy integration:\nIn the initial version, the information is stored in a Doc with the debugging information saved on the tags. This is a bit hacky since there's no way to mark the Doc as not-for-actual-use, but it makes for easy visualization. Alternatively, the labels could also be stored on a custom attribute and the visualization would require a few extra steps.\nI also tried visualizing the labels as entity spans, but the default entity visualization was pretty hard to read with labels on every token. With custom templates, it could be made a bit more readable and having the information as spans might be better than as hacky tags. (Or there could be a \"tag\" visualization where you can specify which attribute to display?)\nTypes of change\nDocs, enhancement.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-20T12:07:25Z", "files_changed": [["82", "spacy/tests/lang/en/test_customized_tokenizer.py"], ["30", "spacy/tests/tokenizer/test_explain.py"], ["95", "spacy/tokenizer.pyx"], ["51", "website/docs/api/tokenizer.md"], ["27", "website/docs/usage/linguistic-features.md"]]}, "151": {"issue_url": "https://github.com/explosion/spaCy/issues/4644", "issue_id": "#4644", "issue_summary": "Unable to find data used to trained Portuguese model", "issue_description": "thak123 commented on 14 Nov 2019\n#1886 has the list of documents used to train the Pt ner model. I am not able to find the dataset.\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Linux-4.4.0-18362-Microsoft-x86_64-with-debian-stretch-sid\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-11-13T19:49:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "152": {"issue_url": "https://github.com/explosion/spaCy/issues/4643", "issue_id": "#4643", "issue_summary": "cannot create model", "issue_description": "yishairasowsky commented on 13 Nov 2019\nI try to train a model using a trainin set and validation set. But when I write in the command line\nspacy train etc then i get an error\ni can show you what i did. first my python script is this...\nimport spacy\nfrom spacy import load \nfrom spacy.gold import docs_to_json\nnlp = load(\"en_core_web_sm\")\nf = open('train.txt')\ndocs = [] # initialize a list to be populated wih strings\nlines = f.readlines()  # returns a list of srings from the txt file\nfor line in lines:\n    # print(line[:]) # display the sentence from that line\n    doc = nlp(line) # convert string into a spacy doc object using nlp\n    docs.append(doc) # add new doc to the list of docs\njson_data = docs_to_json(docs) # convert doc into a json file\n\nimport json\nwith open('train.json', 'w+') as outfile:\n    json.dump(json_data, outfile)\nthen later in the command line i wrote\npython -m spacy train en models train.json validation.json\nthen i get this error\nYishai@LAPTOP-92TPUB0A MINGW64 ~/Documents/GitHub/spaCy/examples/training (master)\n$ python -m spacy train en models train.json validation.json\nC:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py:193: UserWarning: [W022] Training a new part-of-speech tagger using a model with no lemmatization rules or data. This means that the trained model may not be able to lemmatize correctly. If this is intentional or the language you're using doesn't have lemmatization data, you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. If this is surprising, make sure you have the spacy-lookups-data package installed.\n  \"__main__\", mod_spec)\n                  [!] Output directory is not empty\nThis can lead to unintended side effects when saving the model. Please use an\nempty directory or a different path instead. If the specified output path\ndoesn't exist, the directory will be created for you.\nTraining pipeline: ['tagger', 'parser', 'ner']\nStarting with blank model 'en'\nCounting training words (limit=0)\n\nItn  Tag Loss    Tag %    Dep Loss    UAS     LAS    NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n---  ---------  --------  ---------  ------  ------  ---------  ------  ------  ------  -------  -------\n[+] Saved model to output directory\nmodels\\model-final\nTraceback (most recent call last):\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\cli\\train.py\", line 397, in train\n    scorer = nlp_loaded.evaluate(dev_docs, verbose=verbose)\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\language.py\", line 672, in evaluate\n    docs, golds = zip(*docs_golds)\nValueError: not enough values to unpack (expected 2, got 0)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plac_core.py\", line 367, in call\n    cmd, result = parser.consume(arglist)\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plac_core.py\", line 232, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\cli\\train.py\", line 498, in train\n    best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names)\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\cli\\train.py\", line 560, in _collate_best_model\n    bests[component] = _find_best(output_path, component)\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\cli\\train.py\", line 579, in _find_best\n    accs = srsly.read_json(epoch_model / \"accuracy.json\")\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\srsly\\_json_api.py\", line 50, in read_json\n    file_path = force_path(location)\n  File \"C:\\Users\\Yishai\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\srsly\\util.py\", line 21, in force_path\n    raise ValueError(\"Can't read file: {}\".format(location))\nValueError: Can't read file: models\\model0\\accuracy.json\nWhich page or section is this issue related to?\nhttps://spacy.io/usage/training#spacy-train-cli", "issue_status": "Closed", "issue_reporting_time": "2019-11-13T17:39:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "153": {"issue_url": "https://github.com/explosion/spaCy/issues/4638", "issue_id": "#4638", "issue_summary": "OSError: [E050] Can't find model 'de_core_news_sm'... only on debug mode", "issue_description": "yennguyenh commented on 13 Nov 2019 \u2022\nedited\nAfter installing spacy pip3 install -U spacy and downloading python -m spacy download de_core_news_sm, I can run the following code:\nimport spacy\n\nnlp = spacy.load(\"de_core_news_sm\")\n\ntext = \"Das ist den Tisch\"\n\nmy_doc = nlp(text)\n\ntoken_list = []\nfor token in my_doc:\n    token_list.append(token.text)\nprint(token_list)\nThen I tried to debug the code (on Pycharm) and got the following error:\nOSError: [E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nI have also tried the solution from #4577 but did not work!\nimport de_core_news_sm\nnlp = de_core_news_sm.load()\nDoes anybody know the reason and how to fix it? Thank you in advanced!\nOperating System: MacOS\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.6\nIDE: Pycharm", "issue_status": "Closed", "issue_reporting_time": "2019-11-13T10:41:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "154": {"issue_url": "https://github.com/explosion/spaCy/issues/4637", "issue_id": "#4637", "issue_summary": "French model sentences segmentation problem", "issue_description": "cglacet commented on 13 Nov 2019\nThe problem\nThe character \"-\" is recognized as a sentence separator which is odd since it's never used as a sentence separator in French.\nHow to reproduce the behaviour\n>>> import spacy\n>>> nlp = spacy.load('fr_core_news_sm')\n>>> doc = nlp(\"Bon, me dis-je, s'il a faim, mon oncle, qui est le plus impatient des hommes, va pousser des cris de d\u00e9tresse.\")\n>>> next(doc.sents)\n`Bon, me dis`\nYour Environment\nspaCy version: 2.2.2\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-11-13T10:05:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "155": {"issue_url": "https://github.com/explosion/spaCy/issues/4634", "issue_id": "#4634", "issue_summary": "missing space treatment around punctuation & digits", "issue_description": "DSLituiev commented on 13 Nov 2019\nFeature description\nI am trying to clean / normalize a text with potential missing spaces around punctuation, like:\nword,digit or digit/digit.\nI could not find any documentation how to tweak it with the current default tokenizer.\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n[(x, x.pos_) for x in nlp('fibrous type,5 mm in diameter,partially resected')]\n\n# [(fibrous, 'ADJ'),\n#  (type,5, 'PROPN'),\n#  (mm, 'PROPN'),\n#  (in, 'ADP'),\n#  (diameter, 'NOUN'),\n#  (,, 'PUNCT'),\n#  (partially, 'ADV'),\n#  (resected, 'VERB')]\nThis also relates to the problem of inconsistent tokenization of / in fractions:\n[(x, x.pos_) for x in nlp('0/1 lymph nodes, 1/2 full')]\n# [(0/1, 'PUNCT'),\n#  (lymph, 'PROPN'),\n#  (nodes, 'PROPN'),\n#  (,, 'PUNCT'),\n#  (1/2, 'NUM'),\n#  (full, 'ADJ')]", "issue_status": "Closed", "issue_reporting_time": "2019-11-12T21:02:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "156": {"issue_url": "https://github.com/explosion/spaCy/issues/4631", "issue_id": "#4631", "issue_summary": "can't load \"en_cor_web_lg\"", "issue_description": "haleha commented on 12 Nov 2019\nI am trying to download en_core_web_lg using \"python -m spacy download en_core_web_lg\" . I keep getting bad handchake erro. Here is the complete error:\nraise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'tls_process_server_certificate', 'certificate verify failed')])\")))", "issue_status": "Closed", "issue_reporting_time": "2019-11-12T18:21:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "157": {"issue_url": "https://github.com/explosion/spaCy/issues/4630", "issue_id": "#4630", "issue_summary": "Where is the original spaCy model training data?", "issue_description": "yishairasowsky commented on 12 Nov 2019\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-12T16:29:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "158": {"issue_url": "https://github.com/explosion/spaCy/issues/4628", "issue_id": "#4628", "issue_summary": "Download SpaCy models in AWS SageMaker", "issue_description": "avissens commented on 12 Nov 2019\nHi!\nI'm trying to make SpaCy work in SageMaker JupyterLab notebook. I managed to install SpaCy using this line in terminal: conda install -n python3 -c conda-forge spacy.\nBut I struggle to download models.\nI can import spacy in my notebook but when I run this line in my terminal (in python3 environment) python -m spacy download en_core_web_lg\nI get this error:\nImportError: Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version.\nThe same error I get when I run this python -m spacy info --markdown\nI uninstalled and installed numpy again, but it didn't help.\nAny suggestions, please!\nYour Environment\nOperating System: IOS\nPython Version Used: 3.6.9\nspaCy Version Used: 2.2.2\nnumpy 1.17.3\nEnvironment Information: AWS SageMaker", "issue_status": "Closed", "issue_reporting_time": "2019-11-12T14:36:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "159": {"issue_url": "https://github.com/explosion/spaCy/issues/4627", "issue_id": "#4627", "issue_summary": "Matcher is returning some duplicates entry", "issue_description": "gargpuja commented on 12 Nov 2019\nI want output as [\"good customer service\",\"great ambience\"] but I am getting [\"good customer\",\"good customer service\",\"great ambience\"] because pattern is matching with \"good customer\" also but this phrase doesn't make any sense. How can I remove these kind of duplicates\nimport spacy\nfrom spacy.matcher import Matcher\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"good customer service and great ambience\")\nmatcher = Matcher(nlp.vocab)\n\n# Create a pattern matching two tokens: adjective followed by one or more noun\n pattern = [{\"POS\": 'ADJ'},{\"POS\": 'NOUN', \"OP\": '+'}]\n\nmatcher.add(\"ADJ_NOUN_PATTERN\", None,pattern)\n\nmatches = matcher(doc)\nprint(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])\nYour Environment\nOperating System: Mac OS\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.8", "issue_status": "Closed", "issue_reporting_time": "2019-11-12T09:50:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "160": {"issue_url": "https://github.com/explosion/spaCy/issues/4626", "issue_id": "#4626", "issue_summary": "Does SpaCy select the best model after iterating lets say 30 times? if so, how can i access the loss value & score details for that best iteration?", "issue_description": "jaipriyadarshicode commented on 12 Nov 2019\nYour Environment\nOperating System: Red hat linux\nPython Version Used: 3.7.4\nspaCy Version Used: 2.1.0\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-12T01:14:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "161": {"issue_url": "https://github.com/explosion/spaCy/issues/4623", "issue_id": "#4623", "issue_summary": "How to evaluate SpaCy NER model during training and after retraining (update model) in production? Should I divide the data into training & dev (cross validation) or spaCy does that internally? Should I use train/dev/test split approach 60/20/20? Please guide the steps.", "issue_description": "jaipriyadarshicode commented on 12 Nov 2019\nYour Environment\nOperating System: Redhat Linux\nPython Version Used: 3\nspaCy Version Used: 2.1\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-11T19:50:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "162": {"issue_url": "https://github.com/explosion/spaCy/issues/4620", "issue_id": "#4620", "issue_summary": "Dependency on cupy broken if a relative cupy directory exists", "issue_description": "ChrisPalmerNZ commented on 10 Nov 2019 \u2022\nedited\nHow to reproduce the problem\nI am on Windows 10, Python 3.6.8. I installed spaCy and ran it with no problem until I switched my conda prompt to a directory which has a lot of github sites in sub-directores, including an older cupy one. Then when I start python and issue import spacy I get the following error (below). If I rename the cupy directory and restart then the error goes away. So, it seems that the thinc component used by spaCy is looking for and finding a cupy directory relative to where it is invoked, rather than to the site-packages directory where spaCy and thinc are installed.\nThis may be \"by-design\" as Python may naturally look locally before referring back to site-directories for its libraries - please confirm if that's the case, and I will consider it as a \"trap for (relatively) young players\"...\nIn [1]: import spacy\n   ...:\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-1-76a01d9c502b> in <module>\n----> 1 import spacy\n\ng:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\spacy\\__init__.py in <module>\n      8\n      9 # These are imported as part of the API\n---> 10 from thinc.neural.util import prefer_gpu, require_gpu\n     11\n     12 from . import pipeline\n\ng:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\thinc\\neural\\__init__.py in <module>\n      2 from __future__ import unicode_literals\n      3\n----> 4 from ._classes.model import Model  # noqa: F401\n\ng:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\thinc\\neural\\_classes\\model.py in <module>\n      9\n     10 from .. import util\n---> 11 from ..train import Trainer\n     12 from ..ops import NumpyOps, CupyOps\n     13 from ..mem import Memory\n\ng:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\thinc\\neural\\train.py in <module>\n      5 from tqdm import tqdm\n      6\n----> 7 from .optimizers import Adam, linear_decay\n      8\n      9\n\noptimizers.pyx in init thinc.neural.optimizers()\n\nops.pyx in init thinc.neural.ops()\n\ng:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\thinc\\neural\\_custom_kernels.py in <module>\n     35 PWD = Path(__file__).parent\n     36 SRC = (PWD / \"_custom_kernels.cu\").open(\"r\", encoding=\"utf8\").read()\n---> 37 KERNELS = compile_kernels(SRC)\n     38\n     39 MMH_SRC = (PWD / \"_murmur3.cu\").open(\"r\", encoding=\"utf8\").read()\n\ng:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\thinc\\neural\\_custom_kernels.py in compile_kernels(src)\n     25         return defaultdict(lambda: None)\n     26     kernels = parse_kernels(src)\n---> 27     return {name: cupy.RawKernel(src, name) for name, src in kernels.items()}\n     28\n     29 def compile_mmh(src):\n\ng:\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\thinc\\neural\\_custom_kernels.py in <dictcomp>(.0)\n     25         return defaultdict(lambda: None)\n     26     kernels = parse_kernels(src)\n---> 27     return {name: cupy.RawKernel(src, name) for name, src in kernels.items()}\n     28\n     29 def compile_mmh(src):\n\nAttributeError: module 'cupy' has no attribute 'RawKernel'\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.6.8\nspaCy Version Used: 2.2.2\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-09T23:14:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "163": {"issue_url": "https://github.com/explosion/spaCy/issues/4617", "issue_id": "#4617", "issue_summary": "Deploying to Google Cloud - best practice", "issue_description": "Contributor\nmr-bjerre commented on 9 Nov 2019\nThis might not be the right place to state this question OR it might be.\nI'm wondering if anyone have had any experience on deploying/managing their spaCy models in Google Cloud Platform? E.g. it might be possible to use their AI Platform and list spaCy as a dependency when packaging the model. However it isn't clear to me if its actually feasible so maybe someone already knows?\nFYI; we are using spaCy on a stream of realtime news.", "issue_status": "Closed", "issue_reporting_time": "2019-11-09T16:12:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "164": {"issue_url": "https://github.com/explosion/spaCy/issues/4616", "issue_id": "#4616", "issue_summary": "Missing AttributeError: 'spacy.tokens.token.Token' object has no attribute 'check_morph'", "issue_description": "Branislava commented on 9 Nov 2019\nThis is my configuration\nspaCy version 2.2.2\nLocation /usr/local/lib/python3.6/dist-packages/spacy\nPlatform Linux-4.15.0-66-generic-x86_64-with-debian-buster-sid\nPython version 3.6.5\nThe script\nhttps://github.com/explosion/spaCy/blob/master/bin/ud/ud_run_test.py\ne.g. in line 139 invokes\ntoken.check_morph(Fused_begin)\nbut when I type\nimport spacy; print(dir(spacy.tokens.token.Token))\nI get the following output\n['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_extension', 'has_vector', 'head', 'i', 'idx', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'shape', 'shape_', 'similarity', 'string', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\ni.e. there is no method check_morph() and this script gives me the message\nAttributeError: 'spacy.tokens.token.Token' object has no attribute 'check_morph'\nIs this method perhaps hidden in some older version, or am I making a mistake somewhere?", "issue_status": "Closed", "issue_reporting_time": "2019-11-09T14:50:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "165": {"issue_url": "https://github.com/explosion/spaCy/issues/4615", "issue_id": "#4615", "issue_summary": "Can't SHAPE match against strings of digits that have leading 0's.", "issue_description": "mapadofu commented on 9 Nov 2019 \u2022\nedited\nThe SHAPE matching against tokens comprised of digits that have leading zeros, e.g. 01234 doesn't work correctly.\nWhat I'd expect is that the pattern [{'SHAPE':'ddddd'}] would match against the token 01234; this is not the case. Another way to say it is that you can have a token with text==\"01234\" whose shape==\"ddddd\", but does not match that pattern.\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.matcher import Matcher\n\ndef run_test(model_name, pattern, text):\n    nlp = spacy.load('en_core_web_md', disable=[])\n\n    m = Matcher(nlp.vocab)\n    m.add('Serial', None, pattern)\n\n    doc = nlp(text)\n\n    print(\"Text:\", text)\n    print(\"Tokens:\", list(doc))\n    print(\"Shapes:\", list(t.shape_ for t in doc))\n    print(\"Model: {0} ({1})\".format(model_name, spacy.__version__))\n    print(\"Pattern:\", pattern)\n    print(\"Matches:\", list(doc[s:t] for (i,s,t) in m(doc)))\n\n\ntext = \"I'm writing in reference to invoice no. 01234-0001\"\n\nmodel_name = 'en_core_web_sm'\n# same behaviour for medium and large\npattern = [{'SHAPE':'ddddd'}, {'ORTH':'-'}, {'SHAPE':'dddd'}]\n\nrun_test(model_name, pattern, text)\nprint(\"Bad\")\nprint()\nprint()\n\npattern = [{'TEXT':{'REGEX':r\"\\d{5}\"}}, {'ORTH':'-'}, {'TEXT':{'REGEX':r\"\\d{4}\"}}]\nrun_test(model_name, pattern, text)\nprint(\"Works\") \nprint()\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-4.15.0-66-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8", "issue_status": "Closed", "issue_reporting_time": "2019-11-09T00:56:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "166": {"issue_url": "https://github.com/explosion/spaCy/issues/4613", "issue_id": "#4613", "issue_summary": "Can't load German module: AttributeError: module 'de' has no attribute 'load'", "issue_description": "marielledado commented on 8 Nov 2019\nHow to reproduce the behaviour\nI'm trying to load the German module on a Jupyter environment but it doesn't seem to work:\nimport spacy\nimport de_core_news_sm\nnlp = spacy.load(\"de\")\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-8-ae3415a5efb2> in <module>()\n----> 1 nlp = spacy.load(\"de\")\n      2 \n      3 doc = nlp(df[\"review\"])\n      4 for token in doc:\n      5     print('\"' + token.text + '\"')\n\n~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py in load(name, **overrides)\n     28     if depr_path not in (True, False, None):\n     29         deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 30     return util.load_model(name, **overrides)\n     31 \n     32 \n\n~\\Anaconda3\\lib\\site-packages\\spacy\\util.py in load_model(name, **overrides)\n    213             return load_model_from_link(name, **overrides)\n    214         if is_package(name):  # installed as package\n--> 215             return load_model_from_package(name, **overrides)\n    216         if Path(name).exists():  # path to model data directory\n    217             return load_model_from_path(Path(name), **overrides)\n\n~\\Anaconda3\\lib\\site-packages\\spacy\\util.py in load_model_from_package(name, **overrides)\n    234     \"\"\"Load a model from an installed package.\"\"\"\n    235     cls = importlib.import_module(name)\n--> 236     return cls.load(**overrides)\n    237 \n    238 \n\nAttributeError: module 'de' has no attribute 'load'\nI read somewhere that the error comes up if you name your Python file spacy.py but since I'm working on a Jupyter notebook perhaps this is not relevant?\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.6.4", "issue_status": "Closed", "issue_reporting_time": "2019-11-08T14:04:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "167": {"issue_url": "https://github.com/explosion/spaCy/issues/4611", "issue_id": "#4611", "issue_summary": "German Lemmatization not working well.", "issue_description": "graftim commented on 8 Nov 2019 \u2022\nedited\nHow to reproduce the behaviour\nimport spacy\nnlp = spacy.load('de_core_news_md')\ndoc = nlp('Die Schweizer Wirtschaft d\u00fcrfte 2020 ein Wachstum von zwei Prozent verzeichnen, sagen die Datenschutzbeauftragten. '\n          'Dasselbe gilt f\u00fcr Versicherungspr\u00e4mien und Aufw\u00e4nde der Krankenkassen im Allgemeinen. Auch die B\u00e4ume haben ein Problem.')\nfor token in doc:\n    print('POS: {} {}'.format(token.pos_, token.lemma_))\nOutput:\nPOS: ADJ Schweizer\nPOS: NOUN Wirtschaft\nPOS: VERB d\u00fcrfen\nPOS: NUM 2020\nPOS: DET einen\nPOS: NOUN Wachstum\nPOS: ADP von\nPOS: NUM zwei\nPOS: NOUN Prozent\nPOS: VERB verzeichnen\nPOS: PUNCT ,\nPOS: VERB sagen\nPOS: DET der\nPOS: NOUN Datenschutzbeauftragten  #Datenschutzbeauftragter\nPOS: PUNCT .\nPOS: PRON derselbe\nPOS: VERB gelten\nPOS: ADP f\u00fcr\nPOS: NOUN Versicherungspr\u00e4mien     #Versicherungspr\u00e4mie\nPOS: CONJ und\nPOS: NOUN Aufw\u00e4nde                 #Aufwand\nPOS: DET der\nPOS: NOUN Krankenkasse\nPOS: ADP im\nPOS: NOUN Allgemeine\nPOS: PUNCT .\nPOS: ADV Auch\nPOS: DET der\nPOS: NOUN b\u00e4umen                   #Baum\nPOS: AUX haben\nPOS: DET einen\nPOS: NOUN Problem\nPOS: PUNCT .\nIt seems very strange that the word B\u00e4ume, even though it is tagged as NOUN gets lemmatized to b\u00e4umen instead of Baum. Also, it handles plural forms of longer nouns or words with Umlaut (\u00e4, \u00f6 ,\u00fc) very poorly. Is this expected behaviour or did something happen during the training of the models?\nI tried it with both de_core_news_sm and de_core_news_md.\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.4\nModels: de", "issue_status": "Closed", "issue_reporting_time": "2019-11-08T08:51:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "168": {"issue_url": "https://github.com/explosion/spaCy/issues/4610", "issue_id": "#4610", "issue_summary": "PhraseMatcher still only using the last label when multiple components are defined", "issue_description": "ChrisPalmerNZ commented on 8 Nov 2019 \u2022\nedited\nI may not correctly understand how to add multiple Matcher components, but after previously reporting this and having observed it agreed as being a bug and subsequently marked as resolved, I have waited until a newer version of spaCy has arrived (2.2.1) and have re-tested. Again, I am having a situation where if I have more than one phrase matcher in the pipeline only the label of the last matcher is used for any entities matched.\nPlease let me know if I am making a mistake in adding the multiple components, or if this is a bug and has been fixed then in which version of spaCy, and how I can update to it.\nHow to reproduce the behaviour\nimport spacy\nprint(\"spaCy version:\", spacy.__version__)\n# 2.2.1\n\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.tokens import Doc, Span\n\nnlp = spacy.load('en_core_web_sm')\n\n# Method to handle the custom login names component\ndef login_name_component(doc):\n    # Apply the matcher to the doc\n    matches = matcher(doc)\n    # Create a Span for each match and assign the label 'LOGIN_NAME'\n    spans = [Span(doc, start, end, label=\"LOGIN_NAME\") for match_id, start, end in matches]\n    # Overwrite the doc.ents with the matched spans\n    doc.ents = spans\n    return doc\n\n# Method to handle the custom State names component\ndef state_terr_component(doc):\n    # Apply the matcher to the doc\n    matches = matcher(doc)\n    # Create a Span for each match and assign the label 'STATE_TERR'\n    spans = [Span(doc, start, end, label=\"STATE_TERR\") for match_id, start, end in matches]\n    # Overwrite the doc.ents with the matched spans\n    doc.ents = spans\n    return doc\n\n# Method to print document entities\ndef print_ents(txt):\n    test_doc = nlp(txt)\n    for ent in test_doc.ents:\n        # Print the entity text and its label\n        print(ent.text, ent.label_)\n\ntxt = \"JimBrown - 10/11/2019 - Note: Christopher Robin visited ACT then NSW on 08 Nov 2019\\r\\nSonnyBill - 11/11/2019 - Noted\"\n\nprint()\nprint(\"Text being processed:\")\nprint(txt)\nprint()\nprint(\"Entities before any phrase matcher components:\")\nprint_ents(txt)\n\n# Add login names phrase matcher\nlogin_names = ['JimBrown', 'SonnyBill']\nlogin_name_pattern = list(nlp.pipe(login_names))\nlogin_name_pattern\nmatcher = PhraseMatcher(nlp.vocab)\nmatcher.add('LOGIN_NAME', None, *login_name_pattern )\n\nnlp.add_pipe(login_name_component, before=\"ner\")\nprint()\nprint(\"login_name_pattern added :\", login_name_pattern)\nprint(\"Pipe names after adding login_name_component\")\nprint(nlp.pipe_names)\n\n# login names correctly identified\nprint()\nprint(\"Entities after addition of login_name_component:\")\nprint_ents(txt)\n\n# Add State / Territory phrase matcher\nState_Terr  = ['ACT', 'NSW', 'NT', 'QLD', 'SA', 'TAS', 'VIC', 'WA']\nstate_terr_pattern = list(nlp.pipe(State_Terr))\n\nmatcher.add('STATE_TERR', None, *state_terr_pattern)\n# Add the component to the pipeline before the 'login_name_component'\nnlp.add_pipe(state_terr_component, before=\"login_name_component\")\n\nprint()\nprint(\"state_terr_pattern added :\", state_terr_pattern)\nprint(\"Pipe names after adding state_terr_component\")\nprint(nlp.pipe_names)\n\n# State Territory names incorrectly identified as login names\nprint()\nprint(\"Entities after addition of state_terr_component:\")\nprint_ents(txt)\nYour Environment\nOperating System: Windows-7-6.1.7601-SP1\nPython Version Used: 3.6.9\nspaCy Version Used: 2.2.1", "issue_status": "Closed", "issue_reporting_time": "2019-11-07T23:26:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "169": {"issue_url": "https://github.com/explosion/spaCy/issues/4609", "issue_id": "#4609", "issue_summary": "Can't install German models", "issue_description": "marielledado commented on 8 Nov 2019\nHow to reproduce the problem\nI've tried several times to install the German corpus based on what's written in the documentation.\nThis code:\npip3 install spacy download de_core_news_sm\ngives me this error:\nRequirement already satisfied: spacy in c:\\users\\marielle\\anaconda3\\lib\\site-packages (2.2.2)\nRequirement already satisfied: download in c:\\users\\marielle\\anaconda3\\lib\\site-packages (0.3.4)\nERROR: Could not find a version that satisfies the requirement de_core_news_sm (from versions: none)\nERROR: No matching distribution found for de_core_news_sm\nInstalling directly with a URL:\n pip install https://github.com/explosion/spacy-models/releases/download/de_core_news_md-1.0.0/de_core_news_md-1.0.0.tar.gz\nGave me an error so long that it's not humanly possible to copy-paste it here, but here's a snippet of it:\nBuilding wheels for collected packages: de-core-news-md, spacy, murmurhash, thinc, ujson, cytoolz\n  Building wheel for de-core-news-md (setup.py) ... done\n  Created wheel for de-core-news-md: filename=de_core_news_md-1.0.0-cp36-none-any.whl size=677182029 sha256=c0a6978488c42e220a982b9672c52e415706954b3f00e1882c1610152df09d33\n  Stored in directory: C:\\Users\\Marielle\\AppData\\Local\\pip\\Cache\\wheels\\7b\\6d\\eb\\f381ed9477b58e3dd9a2af69ab455c9fd4de8de39a7beab468\n  Building wheel for spacy (setup.py) ... error\nERROR: Command errored out with exit status 1:\n   command: 'c:\\users\\marielle\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\spacy\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\spacy\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-wheel-rjs0lil6' --python-tag cp36\n       cwd: C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-install-tuckphzu\\spacy\\\n  Complete output (412 lines):\n---\n  Running setup.py clean for spacy\n  Building wheel for murmurhash (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: 'c:\\users\\marielle\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\murmurhash\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\murmurhash\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-wheel-bbsp9wx4' --python-tag cp36\n       cwd: C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-install-tuckphzu\\murmurhash\\\n  Complete output (21 lines):\n---\n Running setup.py clean for murmurhash\n  Building wheel for thinc (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: 'c:\\users\\marielle\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\thinc\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\thinc\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-wheel-xma1mhib' --python-tag cp36\n       cwd: C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-install-tuckphzu\\thinc\\\n  Complete output (115 lines):\n----\n  Running setup.py clean for thinc\n  Building wheel for ujson (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: 'c:\\users\\marielle\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\ujson\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\ujson\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-wheel-bakt0y7w' --python-tag cp36\n       cwd: C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-install-tuckphzu\\ujson\\\n  Complete output (5 lines):\n  running bdist_wheel\n  running build\n  running build_ext\n  building 'ujson' extension\n  error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visualstudio.com/visual-cpp-build-tools\n  ----------------------------------------\n  ERROR: Failed building wheel for ujson\n  Running setup.py clean for ujson\n  Building wheel for cytoolz (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: 'c:\\users\\marielle\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\cytoolz\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\cytoolz\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-wheel-sh7wae4m' --python-tag cp36\n       cwd: C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-install-tuckphzu\\cytoolz\\\n  Complete output (49 lines):\n---\n Running setup.py clean for cytoolz\nSuccessfully built de-core-news-md\nFailed to build spacy murmurhash thinc ujson cytoolz\nInstalling collected packages: murmurhash, cymem, preshed, cytoolz, plac, dill, termcolor, pathlib, thinc, ujson, regex, ftfy, spacy, de-core-news-md\n  Found existing installation: murmurhash 1.0.2\n    Uninstalling murmurhash-1.0.2:\n      Successfully uninstalled murmurhash-1.0.2\n    Running setup.py install for murmurhash ... error\n    ERROR: Command errored out with exit status 1:\n     command: 'c:\\users\\marielle\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\murmurhash\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\murmurhash\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-record-9tjo15vj\\install-record.txt' --single-version-externally-managed --compile\n         cwd: C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-install-tuckphzu\\murmurhash\\\n    Complete output (6 lines):\n    running install\n    running build\n    running build_py\n    running build_ext\n    building 'murmurhash.mrmr' extension\n    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visualstudio.com/visual-cpp-build-tools\n    ----------------------------------------\n  Rolling back uninstall of murmurhash\n  Moving to c:\\users\\marielle\\anaconda3\\lib\\site-packages\\murmurhash-1.0.2.dist-info\\\n   from c:\\users\\marielle\\anaconda3\\lib\\site-packages\\~urmurhash-1.0.2.dist-info\n  Moving to c:\\users\\marielle\\anaconda3\\lib\\site-packages\\murmurhash\\\n   from c:\\users\\marielle\\anaconda3\\lib\\site-packages\\~urmurhash\nERROR: Command errored out with exit status 1: 'c:\\users\\marielle\\anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\murmurhash\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Marielle\\\\AppData\\\\Local\\\\Temp\\\\pip-install-tuckphzu\\\\murmurhash\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Marielle\\AppData\\Local\\Temp\\pip-record-9tjo15vj\\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.\nYour Environment\nspaCy version: 2.2.2\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.6.4", "issue_status": "Closed", "issue_reporting_time": "2019-11-07T19:00:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "170": {"issue_url": "https://github.com/explosion/spaCy/issues/4607", "issue_id": "#4607", "issue_summary": "label start and end date", "issue_description": "yishairasowsky commented on 7 Nov 2019 \u2022\nedited\nI want to extract the start and end date from the text of a contract. I am currently able to color and label all the DATES using displacy.render(doc, style=\"ent\"). But again, I need to distinguish between start and end date. Any idea how I can do that? Maybe apply more training to a model? How can I do that? Thanks for your help! yishairasowsky@gmail.com\nYour Environment\nOperating System: windows 8.1\nPython Version Used: 3.7.2\nspaCy Version Used: 2.2.2\nEnvironment Information: I prefer to use jupyter notebooks, so that i can see what i am doing. but py files i guess are ok too.", "issue_status": "Closed", "issue_reporting_time": "2019-11-07T16:07:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "171": {"issue_url": "https://github.com/explosion/spaCy/issues/4605", "issue_id": "#4605", "issue_summary": "Questions and Directives for Pretraining", "issue_description": "pvcastro commented on 7 Nov 2019\nMy Environment\nspaCy version: 2.2.2\nPlatform: Linux-5.0.0-25-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nMachine: AMD Ryzen Threadripper 2950X 16-Core Processor with 128 Gb RAM\nGPU: 3 x RTX 2080ti\nHi there!\nI'm trying to perform a pretraining for Portuguese, but I'm having a hard time getting the proper settings for my training. I have several corpora available I could use, ranging from a Wikipedia with around 230 million words to a domain specific corpus with 6 billion words. I first started with a general domain common crawl corpus with 3.5 billion words, but it was taking too long, even using GPU I was getting only around 3k wps. Increasing the batch size didn't help much. For every 5 rows of progress, I'm getting something like this:\n0 163883838 33337653 10823 2269\n0 163944872 33348885 11231 2304\n0 164004651 33359701 10816 2258\n0 164065755 33370757 11055 2297\n0 164125760 33381669 10911 36745\nNot sure what it means to get 4 logging steps with 2k wps each and one with 36k. This sample is using the default batch size. This ran for almost 24 hours and I couldn't even finish the first iteration.\nThen I realized that I should start smaller, and got a small sample from this corpus, getting only the first 100.000 sentences from the corpus (1.1 million tokens). The pretrain ran smoothly, with an average of 30k wps at every progress log, and the default 1000 iterations were finished after about 15 hours.\nThis small sample of 100k sentences was enough to boost a NER benchmark we have for Portuguese from 61% using word embeddings to 65% using only the pretrained model (which I still find very low, since CNN architectures similar to the SENNA model from Collobert were already getting 71% in this benchmark, but this is a subject for another issue). For another domain specific NER corpus I have, I got a smaller boost from 90.88% using static word embeddings to 91.50% using the pretrained model alone.\nSince I got relatively good results wich such a small corpus, I'm eager to train an optimal one. So I'm wondering how could I get the most out of this pretraining using larger corpora:\nHow big should be the corpus, at least?\nShould I expect that a domain specific (legal) corpus would give me the best results for downstream tasks?\nWhat should I do to get the best wps for the pretraining? I saw some reported 50k wps for every step of the iterations, how can I get this?\nHow many iterations are necessary to give the best results for downstream tasks, such as NER?\nHow do I pick the final model, from all the models available per iteration?\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-11-07T12:41:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "172": {"issue_url": "https://github.com/explosion/spaCy/issues/4604", "issue_id": "#4604", "issue_summary": "cymem assersion Error in `retokenizer.split`", "issue_description": "Contributor\ntamuhey commented on 7 Nov 2019 \u2022\nedited\ntext = \"Hyperglycemic adverse events following antipsychotic drug administration in\"\ndoc = en(text)\n\nwith doc.retokenize() as retokenizer:\n    token = doc[0]\n    heads = [(token, 0)] * len(token)\n    retokenizer.split(doc[token.i], list(token.text), heads=heads)\nError:\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n<ipython-input-75-cf40a2ff9ce8> in <module>\n      5     token = doc[0]\n      6     heads = [(token,0)] * len(token)\n----> 7     retokenizer.split(doc[token.i], list(token.text), heads=heads)\n\n_retokenize.pyx in spacy.tokens._retokenize.Retokenizer.__exit__()\n\n_retokenize.pyx in spacy.tokens._retokenize._split()\n\ndoc.pyx in spacy.tokens.doc.Doc._realloc()\n\ncymem.pyx in cymem.cymem.Pool.realloc()\n\nAssertionError:\nspacy version: 2.2.2", "issue_status": "Closed", "issue_reporting_time": "2019-11-07T11:41:28Z", "fixed_by": "#4606", "pull_request_summary": "Fix realloc in retokenizer.split()", "pull_request_description": "Collaborator\nadrianeboyd commented on 7 Nov 2019 \u2022\nedited\nDescription\nAlways realloc to a size larger than doc.max_length in retokenizer.split() (or cymem will throw errors).\nFixes #4604.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-11T15:26:47Z", "files_changed": [["15", "spacy/tests/doc/test_retokenize_split.py"], ["2", "spacy/tokens/_retokenize.pyx"]]}, "173": {"issue_url": "https://github.com/explosion/spaCy/issues/4603", "issue_id": "#4603", "issue_summary": "TextCategorizer does not learn with integer labels", "issue_description": "ValentinCalomme commented on 7 Nov 2019\nThe SpaCy text categorizer pipe does not learn if the training data has integer labels. For instance:\nannotations = {\"cats\": {0: True, 1: False}}\nPassing this as annotations to the update method doesn't lead to any learning. The loss returned is 0 and the model's weights do not seem to be updated. On the other hand, the following annotations will work:\nannotations = {\"cats\": {\"0\": True, \"1\": False}}\nTo me, there are two possibilities:\nUsing string labels should be strictly enforced and an error should be thrown if this isn't the case\nor\nInteger labels should work as well as string labels due to duck typing\nYour Environment\nspaCy version: 2.2.2\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.5\nModels: nl", "issue_status": "Closed", "issue_reporting_time": "2019-11-07T10:04:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "174": {"issue_url": "https://github.com/explosion/spaCy/issues/4602", "issue_id": "#4602", "issue_summary": "blacklist word/token from a specific label", "issue_description": "Khanifsaleh commented on 7 Nov 2019\nIs there a way in ner spacy feature to blacklist tokens/words so that they are not specific label?", "issue_status": "Closed", "issue_reporting_time": "2019-11-07T08:12:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "175": {"issue_url": "https://github.com/explosion/spaCy/issues/4598", "issue_id": "#4598", "issue_summary": "Documentation Ambiguity", "issue_description": "davidbren commented on 6 Nov 2019\nI have gone through the docs several times but I can't understand what spaCy is doing when creating a model from scratch ?\nSo I have my custom own training data & I am building a blank english model from this using the NER pipeline only.\nWhat type of neural network is used in this scenario ? Is it as suggested in this article ?\nhttps://medium.com/@b.terryjack/nlp-pretrained-named-entity-recognition-7caa5cd28d7b\nIf so how are the word embedding's being produced ?\nCould really do with help on this please & thanks for your great product (grovel, grovel :))\nYour Environment\nOperating System:\nPython Version Used: 3.7\nspaCy Version Used: 2.1.8\nEnvironment Information: RHEL 7", "issue_status": "Closed", "issue_reporting_time": "2019-11-06T15:35:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "176": {"issue_url": "https://github.com/explosion/spaCy/issues/4597", "issue_id": "#4597", "issue_summary": "Installation on PyCharm", "issue_description": "Fourthought commented on 6 Nov 2019\nHi,\nI've been attempting to install spaCy on Pycharm but with no luck.\nHow to reproduce the problem\nInstallation using: pip install -U spacy\nspaCy is working well in Jupyter notebook, just now need to transfer to Pycharm for handling a large set of code.\nThe installation seems to fail at the point of installing Blis. After looking through previous issues both on Github and Stackoverflow it seems this is a 32bit to 64bit problem. I don't seem to be able to create a win64 venv in pycharm (I realise this may be out of scope for a spaCy installation problem)\nHave been trying to sort this for three solid days now, so any assistance would be greatly appreciated!!\n    Failed building wheel for blis\n    Running setup.py clean for blis\n    Building wheel for srsly (setup.py): started\n    Building wheel for srsly (setup.py): finished with status 'done'\n    Stored in directory: C:\\Users\\Steve\\AppData\\Local\\pip\\Cache\\wheels\\20\\f7\\c0\\da1749f9bb8fc18fd8fcaadf0b2a420fd27d80a6e269025320\n    Building wheel for numpy (setup.py): started\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): finished with status 'done'\n    Stored in directory: C:\\Users\\Steve\\AppData\\Local\\pip\\Cache\\wheels\\5e\\e9\\4b\\dd5a8eb53e97dfcc1314eca9c6769edd3cad379d6644c1ad94\n  Successfully built thinc preshed cython srsly numpy\n  Failed to build blis\n  Installing collected packages: murmurhash, cymem, preshed, numpy, blis, wasabi, srsly, plac, tqdm, thinc, wheel, cython\n    Running setup.py install for blis: started\n      Running setup.py install for blis: finished with status 'error'\nYour Environment\nIDE: Pycharm\nData from the Python Console within Pycharm\nPython 3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)] on win32\n>>> import platform\n>>> print(platform.architecture())\n('64bit', 'WindowsPE')\nOperating System: windows 10\nPython Version Used: 3.8\nspaCy Version Used: 2.2.2\nEnvironment Information: while the IDE is the 64 bit installation, it seems to have provided win32.", "issue_status": "Closed", "issue_reporting_time": "2019-11-06T14:54:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "177": {"issue_url": "https://github.com/explosion/spaCy/issues/4595", "issue_id": "#4595", "issue_summary": "different model declaration sequences give different results", "issue_description": "Khanifsaleh commented on 6 Nov 2019\nI make class object that load ner spacy model and I have three NER Spacy models in various languages (lang A, B, C). When I call the class with this sequences:\nobj_ner = {\"A\": class_obj(lang=\"A\"), \"B\":class_obj(lang=\"B\"), \"C\": class_obj(lang=\"C\")}\nand predict ner labels for A language sentence, it give different results when I call the class with another sequences:\nobj_ner = {\"B\":class_obj(lang=\"B\"), \"C\": class_obj(lang=\"C\"), \"A\": class_obj(lang=\"A\")}\nYour Environment\nOperating System: Ubuntu 18.04\nPython Version Used: 3.7\nspaCy Version Used: 2.2.1", "issue_status": "Closed", "issue_reporting_time": "2019-11-06T06:44:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "178": {"issue_url": "https://github.com/explosion/spaCy/issues/4594", "issue_id": "#4594", "issue_summary": "PhraseMatcher _docs regression (of sorts) bug/feature request", "issue_description": "christian-storm commented on 6 Nov 2019\nI suppose this is a case of one persons feature being another persons bug. Before being converted to cython, one could access the patterns via _docs (see below). Am I stuck with building my own lookup table or is there a way to access this some other way?\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')\nits_patterns = [nlp.make_doc(s) for s in [\"it's\", \"it is\"]]\nmatcher.add(\"ITS\", on_match, *its_patterns)\ndoc = nlp(\"It's a real problem when it is.\")\nmatches = matcher(doc)\n\nfor match_id, start, end in matcher(doc):\n    for replacement_doc in matcher._docs[match_id]:\n        print(f\"Change {doc[start:end]} to {replacement_doc.text}\")\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.2", "issue_status": "Closed", "issue_reporting_time": "2019-11-06T00:05:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "179": {"issue_url": "https://github.com/explosion/spaCy/issues/4593", "issue_id": "#4593", "issue_summary": "Python version compatibility list is wrong", "issue_description": "arencambre commented on 5 Nov 2019\nThe instructions at https://spacy.io/usage suggests they work with Python 3.8: at the top, it says spaCy is compatible with \"64-bit CPython 2.7 / 3.5+\". That invites users to just jump in.\nThat needs to be clarified. Per #4581, Python 3.8 requires you to compile spaCy. Therefore, the instructions should state that you can't simply jump in at the top if you have Python 3.8. Otherwise, users will get errors that are hard to decipher.", "issue_status": "Closed", "issue_reporting_time": "2019-11-05T16:55:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "180": {"issue_url": "https://github.com/explosion/spaCy/issues/4591", "issue_id": "#4591", "issue_summary": "pip install -U spacy fails", "issue_description": "arencambre commented on 5 Nov 2019 \u2022\nedited\nHow to reproduce the problem\nOn a computer with vanilla Windows 10 Professional\nDownload and install the Windows x86-64 executable installer from https://www.python.org/downloads/release/python-380/.\nIn a command prompt window, run pip install -U spacy.\nInstead of a successful install, I get a litany of errors: errors.txt\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.8.0\nspaCy Version Used: Whatever downloads on Nov. 5, 2019 with pip install -U spacy\nEnvironment Information: N/A", "issue_status": "Closed", "issue_reporting_time": "2019-11-05T15:40:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "181": {"issue_url": "https://github.com/explosion/spaCy/issues/4590", "issue_id": "#4590", "issue_summary": "on_match functions with DependencyMatcher", "issue_description": "Contributor\nprilopes commented on 5 Nov 2019\nHow to reproduce the behaviour\nI'm trying to create a method to use on_match in DependencyMatcher. I've done this successfully for Matcher and follow much of the same logic.\nWhen I get the matches using something like this:\nmatches = dep_matcher(doc)\nfor match in matches:\n     print(match)\nIf I understand correctly match is a tuple with match_id (the rule that was triggered) and a list of nodes that match my pattern.\nWhen defining an on_match method, I would expect matches[i] to return the same tuple, but that is not the case. Am I missing something??\nI created the following script to help illustrate the problem\nfrom spacy.matcher import DependencyMatcher\nfrom spacy import load\n\n\ndef my_on_match(matcher, doc, i, matches):\n    print('inside my_on_match')\n    match = matches[i]\n    print(match)\n    print('================')\n\n\nnlp = load('en_core_web_sm')\ndoc = nlp('The quick brown fox jumped over the lazy dog.')\n\ndep_matcher = DependencyMatcher(nlp.vocab)\n\ndep_pattern = [\n    {\"SPEC\": {\"NODE_NAME\": \"jumped\"}, \"PATTERN\": {\"ORTH\": \"jumped\"}},\n    {\"SPEC\": {\"NODE_NAME\": \"fox\", \"NBOR_RELOP\": \">\", \"NBOR_NAME\": \"jumped\"}, \"PATTERN\": {\"ORTH\": \"fox\"}},\n    {\"SPEC\": {\"NODE_NAME\": \"quick\", \"NBOR_RELOP\": \".\", \"NBOR_NAME\": \"jumped\"}, \"PATTERN\": {\"ORTH\": \"fox\"}},\n]\n\ndep_matcher.add('no_on_match', None, dep_pattern)\ndep_matcher.add('with_on_match', my_on_match, dep_pattern)\n\nmatches = dep_matcher(doc)\n\nprint('all matches')\nfor match in matches:\n     print(match)\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-4.15.0-66-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.7\nI might have found the problem\nI was reading the code for DependencyMatcher and found these lines on dependencymatcher.pyx (start at line 240 on master branch and line 237 for tag 2.1.8):\n    for i, (ent_id, nodes) in enumerate(matched_key_trees):\n                on_match = self._callbacks.get(ent_id)\n                if on_match is not None:\n                    on_match(self, doc, i, matches)\nSo, if my line of thought is correct, on the last line of the excerpt (243 on master branch), should it be matched_key_trees instead of matches?\nI'd appreciate any light on this issue. :-)\n1", "issue_status": "Closed", "issue_reporting_time": "2019-11-05T15:34:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "182": {"issue_url": "https://github.com/explosion/spaCy/issues/4589", "issue_id": "#4589", "issue_summary": "Lookups data not serialized with latest nl_core_news_sm", "issue_description": "lonneke123 commented on 5 Nov 2019\nHow to reproduce the behaviour\ndoc = nlp(('Hallo, ik ben Piet. Ik heb gisteren iets gekocht.'))\nfor w in doc:\nprint(w.text, w.pos_, w.lemma_)\nThe result is:\nhallo X hallo\n, PUNCT ,\nik PRON ik\nben VERB ben\npiet NOUN piet\n. PUNCT .\nik PRON ik\nheb VERB heb\ngisteren ADV gisteren\niets PRON iets\ngekocht VERB gekocht\n. PUNCT .\nSo no word is lemmatized. This is happening since the new release. The pos tagger does work", "issue_status": "Closed", "issue_reporting_time": "2019-11-05T12:33:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "183": {"issue_url": "https://github.com/explosion/spaCy/issues/4588", "issue_id": "#4588", "issue_summary": "TypeError: beam_parse() takes at least 2 positional arguments", "issue_description": "Julius-ZCJ commented on 5 Nov 2019\nSpacy==2.2.2\nMy code is that:\nimport spacy\nnlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser'])\nutterance = \"Apple is looking at buying U.K. startup for $1 billion\"\ndoc = nlp(utterance)\nprint(doc.ents)\nfor ent in doc.ents:\nprint(ent.text, ent.start_char, ent.end_char, ent.label_)\nwith nlp.disable_pipes('ner'):\n_doc_entity_scores = nlp(utterance)\nprint(_doc_entity_scores)\n(beams, _) = nlp.entity.beam_parse([_doc_entity_scores])\nprint(beams)\nWhen I run I get error: TypeError: beam_parse() takes at least 2 positional arguments (1 given)\nAnd I go to Spacy official website to find document, but I'm failed, Where I can find document about function \"entity.beam_parse\" info?", "issue_status": "Closed", "issue_reporting_time": "2019-11-05T06:52:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "184": {"issue_url": "https://github.com/explosion/spaCy/issues/4586", "issue_id": "#4586", "issue_summary": "importing a binary word vectoring model complains abouy unicoding", "issue_description": "VahidTehrani commented on 5 Nov 2019\nHow to reproduce the behaviour\npython -m spacy init-model en /tmp/BioWordVec --vectors-loc /home/vahid/Downloads/BioWordVec_PubMed_MIMICIII_d200.bin\n\u2714 Successfully created model\n\u2819 Reading vectors from /home/vahid/Downloads/BioWordVec_PubMed_MIMICIII_d200.bin\nTraceback (most recent call last):\nFile \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\nexec(code, run_globals)\nFile \"/home/vahid/myProgs/python-venvs/python3.6/lib/python3.6/site-packages/spacy/main.py\", line 35, in\nplac.call(commands[command], sys.argv[1:])\nFile \"/home/vahid/myProgs/python-venvs/python3.6/lib/python3.6/site-packages/plac_core.py\", line 367, in call\ncmd, result = parser.consume(arglist)\nFile \"/home/vahid/myProgs/python-venvs/python3.6/lib/python3.6/site-packages/plac_core.py\", line 232, in consume\nreturn cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"/home/vahid/myProgs/python-venvs/python3.6/lib/python3.6/site-packages/spacy/cli/init_model.py\", line 90, in init_model\nadd_vectors(nlp, vectors_loc, prune_vectors, vectors_name)\nFile \"/home/vahid/myProgs/python-venvs/python3.6/lib/python3.6/site-packages/spacy/cli/init_model.py\", line 184, in add_vectors\nvectors_data, vector_keys = read_vectors(vectors_loc)\nFile \"/home/vahid/myProgs/python-venvs/python3.6/lib/python3.6/site-packages/spacy/cli/init_model.py\", line 209, in read_vectors\nshape = tuple(int(size) for size in next(f).split())\nFile \"/usr/lib/python3.6/codecs.py\", line 321, in decode\n(result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xba in position 0: invalid start byte\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Linux-4.15.0-64-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nDoes it mean init-model doesn't support binary models? or something else?", "issue_status": "Closed", "issue_reporting_time": "2019-11-05T02:38:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "185": {"issue_url": "https://github.com/explosion/spaCy/issues/4585", "issue_id": "#4585", "issue_summary": "nb: missing morph_rules and reproducability of official model", "issue_description": "Contributor\njarib commented on 5 Nov 2019 \u2022\nedited\nGreat to see nb_core_news_sm released!\nI would really like a model with word vectors, so I've picked up my work from #3082 (and https://github.com/jarib/spacy-nb) to build Norwegian Bokm\u00e5l models of various sizes against spacy 2.2.2.\nI've noticed two things:\nReproducability\nThe released nb_core_news_sm model gives much better accuracy scores than the sm model I build myself from what is presumably the same data source. I haven't found any code that shows how you preprocess the data from NorNE. Is it proprietary? Do you use the train/dev/test split that they provide? How many iterations do you run?\njarib/spacy-nb sm nb_core_news_sm\nTOK 100.00 100.00\nPOS 94.03 95.70\nUAS 88.51 89.31\nLAS 85.37 86.72\nNER P 71.90 83.69\nNER R 70.53 83.45\nNER F 71.21 83.57\nOutdated morph_rules\nThe NorNE data has been updated (improved) since I last used the data. It now uses 30 POS tags that are missing from nb/morph_rules.py. You can see the output of spacy debug-data here. If I run debug-data against the older version of NorNE it has no errors.\nWhat would be the best approach to fix this? Looking at nb/morph_rules.py it seems to do things differently from other languages (e.g. en).", "issue_status": "Closed", "issue_reporting_time": "2019-11-04T22:04:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "186": {"issue_url": "https://github.com/explosion/spaCy/issues/4581", "issue_id": "#4581", "issue_summary": "Can't install pip requirements when installing Spacy from source", "issue_description": "jamescrone1 commented on 4 Nov 2019\nI'm having extreme issues trying to install spaCy locally, I've read all the docs, and have tried searching online to see if I could find someone having the same problem as me, but haven't been able to solve it.\nI'm wanting to use spaCy for a machine learning project I'm working on with university.\nI'm using a new windows 10 laptop that I haven't used for any other development so far, so this means I've had to start from scratch in regards to build tools, python language etc.\nPython language -> 3.8.0 64 bit.\nVisual studio -> tried C++build tools/ express for 2017 & 2019 editions.\nHow to reproduce the problem\nI initially tried 'pip install spacy' & 'pip install -U spacy' but this fails when installing backend dependencies. When these both failed, I checked my pip was updated (tried deleting pip cache as well), and my python was definitely 64-bit. The summarised error I got is below:\nERROR: Command errored out with exit status 1: 'c:\\users\\james\\appdata\\local\\programs\\python\\python38\\python.exe' 'c:\\users\\james\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pip' install --ignore-installed --no-user --prefix 'C:\\Users\\james\\AppData\\Local\\Temp\\pip-build-env-0thrl73l\\normal' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'thinc<7.4.0,>=7.3.0' 'preshed<3.1.0,>=3.0.2' 'murmurhash<1.1.0,>=0.28.0' 'cython>=0.25' 'cymem<2.1.0,>=2.0.2' wheel Check the logs for full command output.\nI then tried to install it from source, using a virtual environment. So I followed the commands on the website. And it was cloning okay, but then it was failing when I tried to install the pip requirements. I don't think it can successfully install blis, I also tried to do this individually but got the same error.\npip install -r requirements.txt\npip install blis\nresulting in the following error\n  ERROR: Failed building wheel for blis\n  Running setup.py clean for blis\nFailed to build blis\nInstalling collected packages: blis, wasabi, thinc, idna, chardet, certifi, urllib3, requests, six, attrs, pyrsistent, jsonschema, cython, colorama, pyparsing, packaging, more-itertools, pluggy, wcwidth, py, atomicwrites, pytest, pytest-timeout, pbr, mock, pycodestyle, pyflakes, mccabe, flake8\n    Running setup.py install for blis ... error\n    ERROR: Command errored out with exit status 1:\n     command: 'c:\\users\\james\\appdata\\local\\programs\\python\\python38\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\james\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ji3x7tdk\\\\blis\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\james\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ji3x7tdk\\\\blis\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\james\\AppData\\Local\\Temp\\pip-record-oiva8fcv\\install-record.txt' --single-version-externally-managed --compile\n         cwd: C:\\Users\\james\\AppData\\Local\\Temp\\pip-install-ji3x7tdk\\blis\\\n    Complete output (25 lines):\n    BLIS_COMPILER? None\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\\lib.win-amd64-3.8\n    creating build\\lib.win-amd64-3.8\\blis\n    copying blis\\about.py -> build\\lib.win-amd64-3.8\\blis\n    copying blis\\benchmark.py -> build\\lib.win-amd64-3.8\\blis\n    copying blis\\__init__.py -> build\\lib.win-amd64-3.8\\blis\n    creating build\\lib.win-amd64-3.8\\blis\\tests\n    copying blis\\tests\\common.py -> build\\lib.win-amd64-3.8\\blis\\tests\n    copying blis\\tests\\test_dotv.py -> build\\lib.win-amd64-3.8\\blis\\tests\n    copying blis\\tests\\test_gemm.py -> build\\lib.win-amd64-3.8\\blis\\tests\n    copying blis\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\blis\\tests\n    copying blis\\cy.pyx -> build\\lib.win-amd64-3.8\\blis\n    copying blis\\py.pyx -> build\\lib.win-amd64-3.8\\blis\n    copying blis\\cy.pxd -> build\\lib.win-amd64-3.8\\blis\n    copying blis\\__init__.pxd -> build\\lib.win-amd64-3.8\\blis\n    running build_ext\n    msvc\n    py_compiler msvc\n    {'APPVEYOR_REPO_COMMIT_AUTHOR': 'Matthew Honnibal', 'LIB': 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\LIB\\\\amd64;C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\ATLMFC\\\\LIB\\\\amd64;C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\lib\\\\10.0.14393.0\\\\ucrt\\\\x64;C:\\\\Program Files (x86)\\\\Windows Kits\\\\NETFXSDK\\\\4.6.1\\\\lib\\\\um\\\\x64;C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\lib\\\\10.0.14393.0\\\\um\\\\x64;', 'VS100COMNTOOLS': 'c:\nAny ideas what's going on here? I have tried uninstalling python, visual studio and reinstalling these but nothing seems to work. Is spacy supported on python 3.8.0? I'm wondering if it's an issue with my compiler but I'm honestly not sure, i'm assuming if blis can be installed, it would work fine. Any help would be greatly appreciated.\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.8.0\nspaCy Version Used: latest version\nEnvironment Information: Using windows powercell & git bash to run commands, tried installing library on pycharm as well, but it also didnt't work", "issue_status": "Closed", "issue_reporting_time": "2019-11-04T11:57:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "187": {"issue_url": "https://github.com/explosion/spaCy/issues/4578", "issue_id": "#4578", "issue_summary": "offsets_from_bilou_tags ignores tags in Korean", "issue_description": "Contributor\nerip commented on 4 Nov 2019 \u2022\nedited\nIt seems like if len(doc) != len(tags), offsets_from_bilou_tags(doc, tags) will not return the appropriate entities list.\nThe data in the example come from sentence 1 here.\nHow to reproduce the behaviour\n>>> from spacy.lang.ko import Korean\n>>> from spacy.gold import offsets_from_biluo_tags\n>>> nlp = Korean()\n>>> text = \"\uc774\uc5b4 \uc606\uc73c\ub85c \uc6c0\uc9c1\uc5ec \uae40\uc77c\uc131\uc758 \uc624\ub978\ucabd\uc5d0\uc11c \ud55c \ucc28\ub840\uc529 \ub450 \ubc88  \uc0c1\uccb4\ub97c \uad7d\ud600 \uc870\ubb38\ud588\uc73c\uba70 \uc774\uc73d\uace0 \uc548\uacbd\uc744 \ubc97\uace0 \uc190\uc218\uac74\uc73c\ub85c \ub208\uc8fc\uc704\ub97c \ub2e6\uae30\ub3c4 \ud588\ub2e4.\"\n>>> doc = nlp(text)\n>>> tags = ['O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'B-NOH', 'I-NOH', 'O', 'B-NOH', 'I-NOH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n>>> entities = offsets_from_biluo_tags(doc, tags)\n>>> len(doc), len(tags)\n(35, 36)\n>>> entities\n[]\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.2\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.7", "issue_status": "Closed", "issue_reporting_time": "2019-11-03T21:01:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "188": {"issue_url": "https://github.com/explosion/spaCy/issues/4577", "issue_id": "#4577", "issue_summary": "OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.", "issue_description": "icmpnorequest commented on 2 Nov 2019 \u2022\nedited\nI have installed spacy and downloaded en_core_web_sm with:\npip3 install spacy\npython3 -m spacy download en_core_web_sm\nWhen running codes on Python3 default IDLE, it runs successfully:\nimport spacy\nspacy.load(\"en_core_web_sm\")\nHowever, when I run above codes in jupyter notebook, it shows error:\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nI tried several ways in Jupyter notebook like\n!python3 -m spacy download en_core_web_sm\nbut it still shows the error.\nOS: MacOS\nCould somebody help me fix this issue? Thanks in advance!", "issue_status": "Closed", "issue_reporting_time": "2019-11-02T14:44:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "189": {"issue_url": "https://github.com/explosion/spaCy/issues/4575", "issue_id": "#4575", "issue_summary": "Training NER fails", "issue_description": "ghost commented on 2 Nov 2019 \u2022\nedited by ghost\nHow to reproduce the behaviour\nI am trying to train spacy on CoNLL 2003 NER data. I used the CLI convertor to convert from conll to spacy format.\npython -m spacy convert eng.train . -c ner for all 3 files: train, dev and test and renamed the files to .json\nTo train, I am using this command:\npython -m spacy train en scner/ engtrain.json engtesta.json -p ner -VV -D\nItn  NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n---  ---------  ------  ------  ------  -------  -------\n\u2714 Saved model to output directory\nscner3/model-final\nI am getting this error:\nTraceback (most recent call last):\n  File \"/home/su/.env/lib/python3.5/site-packages/spacy/cli/train.py\", line 397, in train\n    scorer = nlp_loaded.evaluate(dev_docs, verbose=verbose)\n  File \"/home/su/.env/lib/python3.5/site-packages/spacy/language.py\", line 672, in evaluate\n    docs, golds = zip(*docs_golds)\nValueError: not enough values to unpack (expected 2, got 0)\nRunning the debug-data command\npython3 -m spacy debug-data en engtrain.json engtesta.json\ngives me this:\n=========================== Data format validation ===========================\n\u2714 Corpus is loadable\n\n=============================== Training stats ===============================\nTraining pipeline: tagger, parser, ner\nStarting with blank model 'en'\n946 training docs\n0 evaluation docs\n\u2714 No overlap between training and evaluation data\n\u26a0 Low number of examples to train from a blank model (946)\n\n============================== Vocab & Vectors ==============================\n\u2139 203621 total words in the data (23623 unique)\n\u2139 No word vectors present in the model\n\n========================== Named Entity Recognition ==========================\n\u2139 4 new labels, 0 existing labels\n0 missing values (tokens with '-' label)\n\u2714 Good amount of examples for all labels\n\u2714 Examples without occurrences available for all labels\n\u2714 No entities consisting of or starting/ending with whitespace\n\n=========================== Part-of-speech Tagging ===========================\n\u2139 45 labels in data (57 labels in tag map)\n\u2718 Label ')' not found in tag map for language 'en'\n\u2718 Label '(' not found in tag map for language 'en'\n\u2718 Label 'NN|SYM' not found in tag map for language 'en'\n\u2718 Label '\"' not found in tag map for language 'en'\n\n============================= Dependency Parsing =============================\n\u2139 Found 203621 sentences with an average length of 1.0 words.\n\u2139 1 label in train data\n\u2139 1 label in projectivized train data\n\n================================== Summary ==================================\n\u2714 5 checks passed\n\u26a0 2 warnings\n\u2718 4 errors\nYour Environment\nModels: en_core_web_lg\nPython version: 3.5.2\nPlatform: Linux-4.15.0-58-generic-x86_64-with-Ubuntu-16.04-xenial\nspaCy version: 2.2.2\nI am trying to train the model only on NER, not on POS.", "issue_status": "Closed", "issue_reporting_time": "2019-11-02T12:25:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "190": {"issue_url": "https://github.com/explosion/spaCy/issues/4574", "issue_id": "#4574", "issue_summary": "AttributeError: 'Lookups' object has no attribute 'get' -> after upgrade to spacy v2.2.2.", "issue_description": "filievski commented on 2 Nov 2019\nAfter upgrading spacy to v2.2.2 with pip, the Dutch model fails to parse documents. Perhaps this method got lost during the update?\nHow to reproduce the behaviour\nimport nl_core_news_sm\nspacy_nl = nl_core_news_sm.load()\ntext='Aucula tusora is een vlinder uit de familie van de uilen (Noctuidae). De wetenschappelijke naam van de soort is voor het eerst geldig gepubliceerd in 1981 door Todd & Poole.'spacy_nl = doc_nl=spacy_nl(text)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/fii800/anaconda3/lib/python3.7/site-packages/spacy/language.py\", line 435, in __call__\n    doc = proc(doc, **component_cfg.get(name, {}))\n  File \"pipes.pyx\", line 396, in spacy.pipeline.pipes.Tagger.__call__\n  File \"pipes.pyx\", line 441, in spacy.pipeline.pipes.Tagger.set_annotations\n  File \"morphology.pyx\", line 307, in spacy.morphology.Morphology.assign_tag_id\n  File \"morphology.pyx\", line 241, in spacy.morphology.Morphology.lemmatize\n  File \"/home/fii800/anaconda3/lib/python3.7/site-packages/spacy/lang/nl/lemmatizer/lemmatizer.py\", line 47, in __call__\n    lemma_index = self.index.get(univ_pos, {})\nAttributeError: 'Lookups' object has no attribute 'get'\nYour Environment\nOperating System: Ubuntu 18.04\nPython Version Used: 3.7\nspaCy Version Used: 2.2.2\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-11-02T08:44:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "191": {"issue_url": "https://github.com/explosion/spaCy/issues/4573", "issue_id": "#4573", "issue_summary": "prefix_search overriding token_match in tokenizer", "issue_description": "gorsat commented on 2 Nov 2019 \u2022\nedited\nThis may be working as intended, but I haven't seen it documented anywhere and for me it was unexpected behavior, so I figured I'd raise it as an issue.\nIf you add a new token_match to the tokenizer that conflicts with something in the prefix_search then the prefix_search will win. So, for instance, if you compile something like r\"#\\w+\" as your token match (to tokenize hashtags) it won't work because '#' is included in the default prefix_search. You can remedy this by explicitly removing '#' from the prefix_search.\nThis looks like it might be new behavior because I've found examples where folks just used token_match to tokenize hashtags without changing prefix_search and it seemed to work for them. At any rate, whether this behavior is intended or not, I think the precedence of these functions should be clearly documented in the tokenizer section of the reference.\nI'm on Python 3.6.5 with spaCy 2.2.2\nThis code:\nimport spacy\nimport re\nfrom spacy.tokenizer import _get_regex_pattern\nnlp = spacy.load(\"en_core_web_sm\")\nre_token_match = _get_regex_pattern(nlp.Defaults.token_match)\nre_token_match = f\"({re_token_match}|#\\w+)\"\nnlp.tokenizer.token_match = re.compile(re_token_match).match\ntext = \"I like #fresh #hashtags\"\ndoc = nlp(text)\nprint([e for e in doc])\nProduces this output:\n[I, like, #, fresh, #, hashtags]\nBut if you modify it as follows:\nimport spacy\nimport re\nfrom spacy.tokenizer import _get_regex_pattern\nnlp = spacy.load(\"en_core_web_sm\")\nre_token_match = _get_regex_pattern(nlp.Defaults.token_match)\nre_token_match = f\"({re_token_match}|#\\w+)\"\nnlp.tokenizer.token_match = re.compile(re_token_match).match\n\n# begin new stuff\n_prefixes = list(nlp.Defaults.prefixes)\n_prefixes.remove('#')\nnlp.tokenizer.prefix_search = spacy.util.compile_prefix_regex(_prefixes).search\n# end new stuff\n\ntext = \"I like #fresh #hashtags\"\ndoc = nlp(text)\nprint([e for e in doc])\nIt produces this output:\n[I, like, #fresh, #hashtags]", "issue_status": "Closed", "issue_reporting_time": "2019-11-02T02:12:21Z", "fixed_by": "#4596", "pull_request_summary": "Add tokenizer explain() debugging method", "pull_request_description": "Collaborator\nadrianeboyd commented on 6 Nov 2019 \u2022\nedited\nDescription\nThis is a series of updates to the tokenizer and tokenizer docs in order to sync the docs and the current tokenizer implementation. A new tokenizer function makes debugging the tokenizer a bit easier by implementing a slow debugging version that returns labels for each token about the pattern or rule matched, based on the pseudo-code in the docs.\nThere are some minor changes to tokenizer to bring it in sync with the intended behavior based on the docs. While updating the pseudo-code in the docs, it made sense to make a working version to check the details, so I expanded that just a bit to add some better debugging functionality.\nI think keeping this in sync with the actual tokenizer will be a similar amount of effort to keeping the pseudo-code in the docs in sync. Instead of being in the tokenizer itself, it could also be an example/demo script that takes nlp.tokenizer as an argument, but I liked being able to implement unit tests that compared its behavior to the actual tokenizer. I wish that it were easier to add tests for more of the tricky cases, since I haven't formally tested that the behavior is identical for all of the unusual test cases.\nIt's also kind of unsatisfying that it doesn't handle whitespace tokenization, but this isn't usually a source of confusion for users and I think adding it would make the pseudo-code harder to read, and it's already a bit too long. I wouldn't be opposed to adding it, though.\nFixes #4573, fixes #4645.\nEdited: implements explain() method that returns a list of (pattern_string, token_string) tuples to avoid the hacky displacy usage. Easy displacy integration is postponed to a future PR.\nOutdated description of displacy integration:\nIn the initial version, the information is stored in a Doc with the debugging information saved on the tags. This is a bit hacky since there's no way to mark the Doc as not-for-actual-use, but it makes for easy visualization. Alternatively, the labels could also be stored on a custom attribute and the visualization would require a few extra steps.\nI also tried visualizing the labels as entity spans, but the default entity visualization was pretty hard to read with labels on every token. With custom templates, it could be made a bit more readable and having the information as spans might be better than as hacky tags. (Or there could be a \"tag\" visualization where you can specify which attribute to display?)\nTypes of change\nDocs, enhancement.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-20T12:07:25Z", "files_changed": [["82", "spacy/tests/lang/en/test_customized_tokenizer.py"], ["30", "spacy/tests/tokenizer/test_explain.py"], ["95", "spacy/tokenizer.pyx"], ["51", "website/docs/api/tokenizer.md"], ["27", "website/docs/usage/linguistic-features.md"]]}, "192": {"issue_url": "https://github.com/explosion/spaCy/issues/4571", "issue_id": "#4571", "issue_summary": "AttributeError (no __reduce_cython__) while loading \"en_core_web_sm\"", "issue_description": "DSLituiev commented on 2 Nov 2019\nHow to reproduce the behaviour\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nError:\n-----------------------------------------------------------------------\nAttributeError                        Traceback (most recent call last)\n<ipython-input-142-e49198ee6ee6> in <module>\n      2 \n      3 \n----> 4 nlp = spacy.load(\"en_core_web_sm\")\n      5 \n      6 from negspacy.negation import Negex\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/__init__.py in load(name, **overrides)\n     19 if sys.maxunicode == 65535:\n     20     raise SystemError(Errors.E130)\n---> 21 \n     22 \n     23 def load(name, **overrides):\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in load_model(name, **overrides)\n    112     RETURNS: Path or original argument.\n    113     \"\"\"\n--> 114     if isinstance(path, basestring_):\n    115         return Path(path)\n    116     else:\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in load_model_from_package(name, **overrides)\n    133         if is_package(name):  # installed as package\n    134             return load_model_from_package(name, **overrides)\n--> 135         if Path(name).exists():  # path to model data directory\n    136             return load_model_from_path(Path(name), **overrides)\n    137     elif hasattr(name, \"exists\"):  # Path or Path-like to model data\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/en_core_web_sm/__init__.py in load(**overrides)\n     10 \n     11 def load(**overrides):\n---> 12     return load_model_from_init_py(__file__, **overrides)\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in load_model_from_init_py(init_file, **overrides)\n    171         pipeline = nlp.Defaults.pipe_names\n    172     elif pipeline in (False, None):\n--> 173         pipeline = []\n    174     for name in pipeline:\n    175         if name not in disable:\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in load_model_from_path(model_path, meta, **overrides)\n    141 \n    142 def load_model_from_link(name, **overrides):\n--> 143     \"\"\"Load a model from a shortcut link, or directory in spaCy data path.\"\"\"\n    144     path = get_data_path() / name / \"__init__.py\"\n    145     try:\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/util.py in get_lang_class(lang)\n     48 \n     49     lang (unicode): Two-letter language code, e.g. 'en'.\n---> 50     RETURNS (bool): Whether a Language class has been loaded.\n     51     \"\"\"\n     52     global LANGUAGES\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/__init__.py in import_module(name, package)\n    125                 break\n    126             level += 1\n--> 127     return _bootstrap._gcd_import(name[level:], package, level)\n    128 \n    129 \n\n/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _gcd_import(name, package, level)\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _find_and_load(name, import_)\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _load_unlocked(spec)\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap_external.py in exec_module(self, module)\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/lang/en/__init__.py in <module>\n     13 from ..tokenizer_exceptions import BASE_EXCEPTIONS\n     14 from ..norm_exceptions import BASE_NORMS\n---> 15 from ...language import Language\n     16 from ...attrs import LANG, NORM\n     17 from ...util import update_exc, add_lookups\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/language.py in <module>\n     12 import srsly\n     13 \n---> 14 from .tokenizer import Tokenizer\n     15 from .vocab import Vocab\n     16 from .lemmatizer import Lemmatizer\n\n/Applications/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/tokenizer.cpython-37m-darwin.so in init spacy.tokenizer()\n\nAttributeError: type object 'spacy.tokenizer.array' has no attribute '__reduce_cython__'\nYour Environment\nOperating System: Mac\nconda environment, freshly installed Spacy\nspaCy version: 2.1.8\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.2", "issue_status": "Closed", "issue_reporting_time": "2019-11-01T21:04:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "193": {"issue_url": "https://github.com/explosion/spaCy/issues/4569", "issue_id": "#4569", "issue_summary": "spacy.gold.align function misses some mappings", "issue_description": "filievski commented on 1 Nov 2019\nThe aligning function seems to work good in most cases, but for some reason, it fails in the case below. Namely, it is unable to map the first two SpaCy tokens to the initial 6 BERT tokens.\nHow to reproduce the behaviour\n    bert_tokens=['[CLS]', 'Sy', '##mpi', '##esis', 'kara', '##gios', '##is', 'is', 'een', 'vliesvleugelig', 'insect', 'uit', 'de', 'familie', 'Eulophidae', '.', '[SEP]']\n    our_tokens=['Sympiesis', 'karagiosis', 'is', 'een', 'vliesvleugelig', 'insect', 'uit', 'de', 'familie', 'Eulophidae', '.']\nYour Environment\nOperating System: Ubuntu 18.04\nPython Version Used: 3.7\nspaCy Version Used: 2.1.4", "issue_status": "Closed", "issue_reporting_time": "2019-11-01T11:46:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "194": {"issue_url": "https://github.com/explosion/spaCy/issues/4568", "issue_id": "#4568", "issue_summary": "Error loading german model in newest version", "issue_description": "graftim commented on 1 Nov 2019\nIn the latest version of Spacy, loading the standard model does not work:\n>>> spacy.load(\"de\")\nTraceback (most recent call last):\n  File \"pipes.pyx\", line 640, in spacy.pipeline.pipes.Tagger.from_disk.load_model\n  File \"/data/spacy-test/lib/python3.7/site-packages/thinc/neural/_classes/model.py\", line 375, in from_bytes\n    dest = getattr(layer, name)\nAttributeError: 'FunctionLayer' object has no attribute 'vectors'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n    return util.load_model(name, **overrides)\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/util.py\", line 213, in load_model\n    return load_model_from_link(name, **overrides)\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/util.py\", line 230, in load_model_from_link\n    return cls.load(**overrides)\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/data/de/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/util.py\", line 279, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/util.py\", line 262, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/language.py\", line 940, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/util.py\", line 733, in from_disk\n    reader(path / key)\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/language.py\", line 935, in <lambda>\n    p, exclude=[\"vocab\"]\n  File \"pipes.pyx\", line 658, in spacy.pipeline.pipes.Tagger.from_disk\n  File \"/data/spacy-test/lib/python3.7/site-packages/spacy/util.py\", line 733, in from_disk\n    reader(path / key)\n  File \"pipes.pyx\", line 638, in spacy.pipeline.pipes.Tagger.from_disk.load_model\n  File \"pipes.pyx\", line 642, in spacy.pipeline.pipes.Tagger.from_disk.load_model\nValueError: [E149] Error deserializing model. Check that the config used to create the component matches the model being loaded.\nHow to reproduce:\npip install spacy\npython -m download de\nThen fails with spacy.load(\"de\")\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Linux-4.19.0-6-amd64-x86_64-with-debian-10.1\nPython version: 3.7.3\nModels: de\nFix\nI can solve this problem by downgrading to spacy 2.2.1", "issue_status": "Closed", "issue_reporting_time": "2019-11-01T09:59:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "195": {"issue_url": "https://github.com/explosion/spaCy/issues/4567", "issue_id": "#4567", "issue_summary": "dependency parsing error", "issue_description": "xiaozw123 commented on 1 Nov 2019\nTraining data\n1 \u6211\u4eec \u6211\u4eec PN PN _ 7 Agt _ _\n2 \u5e94 \u5e94 VV VV _ 7 mMod _ _\n3 \u628a \u628a BA BA _ 6 mPrep _ _\n4 \u4ea7\u54c1 \u4ea7\u54c1 NN NN _ 6 Host _ _\n5 \u7684 \u7684 DEG DEG _ 4 mAux _ _\n6 \u8d28\u91cf \u8d28\u91cf NN NN _ 7 Pat _ _\n7 \u653e \u653e VV VV _ 0 Root _ _\n8 \u5728 \u5728 P P _ 10 mPrep _ _\n9 \u7b2c\u4e00 \u7b2c\u4e00 OD OD _ 10 Seq _ _\n10 \u4f4d \u4f4d M M _ 7 Loc _ _\n11 \uff0c \uff0c PU PU _ 7 mPunc _ _\n12 \u4ea7\u54c1 \u4ea7\u54c1 NN NN _ 14 Host _ _\n13 \u4e0e\u5176 \u4e0e\u5176 DEG DEG _ 15 mConj _ _\n14 \u6570\u91cf \u6570\u91cf NN NN _ 15 Exp _ _\n15 \u591a \u591a VA VA _ 7 eAban _ _\n16 \u800c \u800c CC CC _ 18 mConj _ _\n17 \u8d28\u91cf \u8d28\u91cf NN NN _ 18 Exp _ _\n18 \u5dee \u5dee NN NN _ 15 eCoo _ _\n19 \uff0c \uff0c PU PU _ 18 mPunc _ _\n20 \u4e0d\u5982 \u4e0d\u5982 VV VV _ 22 mConj _ _\n21 \u6570\u91cf \u6570\u91cf NN NN _ 22 Exp _ _\n22 \u5c11 \u5c11 VA VA _ 15 ePref _ _\n23 \u800c \u800c MSP MSP _ 25 mConj _ _\n24 \u8d28\u91cf \u8d28\u91cf AD AD _ 25 Exp _ _\n25 \u597d \u597d VA VA _ 22 eCoo _ _\n26 \u3002 \u3002 PU PU _ 25 mPunc _ _\nValueError: [E020] Could not find a gold-standard action to supervise the dependency parser. The tree is non-projective (i.e. it has crossing arcs - see spacy/syntax/nonproj.pyx for definitions). The ArcEager transition system only supports projective trees. To learn non-projective representations, transform the data before training and after parsing. Either pass make_projective=True to the GoldParse class, or use spacy.syntax.nonproj.preprocess_training_data.", "issue_status": "Closed", "issue_reporting_time": "2019-11-01T04:22:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "196": {"issue_url": "https://github.com/explosion/spaCy/issues/4566", "issue_id": "#4566", "issue_summary": "Spacy en_core_web_md error", "issue_description": "dhwani2410 commented on 1 Nov 2019\nCommand\n nlp =  spacy.load(\"en_core_web_md\")\n\nabc= \"Trypsin-sensitive photosynthetic activities in chloroplast membranes from Chlamydomonas reinhardi, y-1. Location of electron transport chain components in chloroplast membranes of chlamydomonas reinhardi, y-1 was investigated by use of proteolytic digestion with soluble or insolubilized trypsin. Digestion of intact membrane vesicles with soluble trypsin inactivates the water-splitting system, the 3-(3,4-dichlorophenyl)-1,1-dimethylurea inhibition site of Photosystem II, the electron transport between the two photosystems as well as the ferredoxin NADP reductase. Reduction of NADP with artificial electron donors for Photosystem I could be restored, however, by addition of purified reductase to trypsin-digested membranes. Electron transfer activities of Photosystems I and II reaction centers were resistant to trypsin digestion either from outside or from within the thylakoids when active trypsin was trapped inside the membrane vesicles by sonication and digestion carried out in the presence of trypsin inhibitor added from outside. In the latter case, the water-splitting system was also found to be resistant to digestion. Polyacrylamide-bound insolubilized trypsin inactivated only the ferredoxin NADP reductase. Photosynthetically active membranes obtained at different stages of development showed a basically similar behavior toward trypsin\"\n\nnlp(abc)\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/dhwani.dholakia/anaconda3/lib/python3.7/site-packages/spacy/language.py\", line 438, in __call__\n    doc = proc(doc, **component_cfg.get(name, {}))\n  File \"pipes.pyx\", line 393, in spacy.pipeline.pipes.Tagger.__call__\n  File \"pipes.pyx\", line 438, in spacy.pipeline.pipes.Tagger.set_annotations\n  File \"morphology.pyx\", line 312, in spacy.morphology.Morphology.assign_tag_id\n  File \"morphology.pyx\", line 200, in spacy.morphology.Morphology.add\nValueError: [E167] Unknown morphological feature: 'ConjType' (9141427322507498425). This can happen if the tagger was trained with a different set of morphological features. If you're using a pretrained model, make sure that your models are up to date:\npython -m spacy validate\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Linux-2.6.32-279.el6.x86_64-x86_64-with-redhat-6.7-Santiago\nPython version: 3.7.4", "issue_status": "Closed", "issue_reporting_time": "2019-10-31T19:36:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "197": {"issue_url": "https://github.com/explosion/spaCy/issues/4565", "issue_id": "#4565", "issue_summary": "Support all flags in Doc.from_array()", "issue_description": "Collaborator\nadrianeboyd commented on 1 Nov 2019\nHow to reproduce the behaviour\nDoc.from_array() supports is_parsed and is_tagged but not is_nered or is_sentenced.", "issue_status": "Closed", "issue_reporting_time": "2019-10-31T18:45:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "198": {"issue_url": "https://github.com/explosion/spaCy/issues/4559", "issue_id": "#4559", "issue_summary": "Tokenizer fails to split period following diacritics", "issue_description": "jeanm commented on 31 Oct 2019\nWhile playing with the Italian tokeniser, I have noticed the following odd behaviours:\nThe use of Unicode combining diacritical marks throws off the tokeniser. Consider the following example, which is handled correctly:\nfrom spacy.lang.it import Italian\nnlp = Italian()\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\n\n# ['LATIN SMALL LETTER E WITH GRAVE', 'FULL STOP']\noriginal = \"\u00e8.\"\nlist(next(nlp(original).sents))  # Yields: [\u00e8, .]  \u2013 OK!\nNow if I apply NFD normalisation to my original string, the string still looks the same (e\u0300.), but it's actually made up of three characters: E + ` + .. This breaks the tokeniser:\nimport unicodedata\n\n# ['LATIN SMALL LETTER E', 'COMBINING GRAVE ACCENT', 'FULL STOP']\nnormalised = unicodedata.normalize(\"NFD\", original)\nlist(next(nlp(normalised).sents))  # Yields: [e\u0300.]  \u2013 WRONG!\nI haven't dug into the code, but I imagine what might be happening is that COMBINING GRAVE ACCENT does not count as a word character, which must end up confusing some tokenisation rule.\nSome letters with diacritics are just never split up from the punctuation that follows:\n# OK\nlist(next(nlp(\"\u00e8.\").sents))  # [\u00e8, .]\nlist(next(nlp(\"\u00e2.\").sents))  # [\u00e2, .]\nlist(next(nlp(\"\u00e6.\").sents))  # [\u00e6, .]\nlist(next(nlp(\"\u00e7.\").sents))  # [\u00e7, .]\nlist(next(nlp(\"\u00f1.\").sents))  # [\u00f1, .]\n\n# WRONG!\nlist(next(nlp(\"\u00e4.\").sents))  # [\u00e4.]\nlist(next(nlp(\"\u00c8.\").sents))  # [\u00c8.]\nlist(next(nlp(\"\u00d3.\").sents))  # [\u00d3.]\nlist(next(nlp(\"\u00c2.\").sents))  # [\u00c2.]\nlist(next(nlp(\"\u00c6.\").sents))  # [\u00c6.]\nlist(next(nlp(\"\u00c7.\").sents))  # [\u00c7.]\nlist(next(nlp(\"\u00d1.\").sents))  # [\u00d1.]\nlist(next(nlp(\"\u00c4.\").sents))  # [\u00c4.]\nNote that none of these examples here in point 2 are using combining diacritics, i.e. len(_) == 2 for all strings above. Again, I haven't delved into the code but I imagine the problem is that certain characters aren't being recognised as letters.\nInfo about spaCy\nspaCy version: 2.2.2.dev4\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-10-31T00:35:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "199": {"issue_url": "https://github.com/explosion/spaCy/issues/4558", "issue_id": "#4558", "issue_summary": "error on debug-data", "issue_description": "ramilrg commented on 31 Oct 2019\nHow to reproduce the behaviour\nsometimes I am getting an error when running a spacy debug-data check, how to fix it?\n============ Data format validation ===============\n\u2834 Loading corpus...\nTraceback (most recent call last):\nFile \"G:\\Python35\\lib\\runpy.py\", line 184, in _run_module_as_main\n\"main\", mod_spec)\nFile \"G:\\Python35\\lib\\runpy.py\", line 85, in run_code\nexec(code, run_globals)\nFile \"G:\\Python35\\lib\\site-packages\\spacy_main.py\", line 35, in\nplac.call(commands[command], sys.argv[1:])\nFile \"G:\\Python35\\lib\\site-packages\\plac_core.py\", line 328, in call\ncmd, result = parser.consume(arglist)\nFile \"G:\\Python35\\lib\\site-packages\\plac_core.py\", line 207, in consume\nreturn cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"G:\\Python35\\lib\\site-packages\\spacy\\cli\\debug_data.py\", line 83, in debug_data\ntrain_docs = list(corpus.train_docs(nlp))\nFile \"gold.pyx\", line 222, in train_docs\nFile \"gold.pyx\", line 243, in iter_gold_docs\nFile \"gold.pyx\", line 266, in spacy.gold.GoldCorpus._make_golds\nFile \"gold.pyx\", line 541, in spacy.gold.GoldParse.from_annot_tuples\nFile \"gold.pyx\", line 621, in spacy.gold.GoldParse.init\nFile \"nonproj.pyx\", line 123, in spacy.syntax.nonproj.projectivize\nFile \"nonproj.pyx\", line 172, in spacy.syntax.nonproj._get_smallest_nonproj_arc\nFile \"nonproj.pyx\", line 58, in spacy.syntax.nonproj.is_nonproj_arc\nFile \"nonproj.pyx\", line 26, in ancestors\nIndexError: list index out of range\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.5\nspaCy Version Used: 2.2.1\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-30T20:22:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "200": {"issue_url": "https://github.com/explosion/spaCy/issues/4556", "issue_id": "#4556", "issue_summary": "Train NER model from a model with a tagger only", "issue_description": "miguelwon commented on 30 Oct 2019\nI have two sets of data one to train a pos-tagger and a different one to train a ner model. Using the train from CLI, I trained first the tagger only. Now I want to add in this model pipeline a ner system. I used the following command to train the ner and used the model-best from the previous tagger trained process:\npython -m spacy train pt_M /50gb_disk/paper_jorge_bruno_perguntas/train_models/ner_models data/spacy_jsons/train_cintil_all_comb_para_harem.json data/spacy_jsons/test_cintil_all_comb_para_harem.json -b /50gb_disk/paper_jorge_bruno_perguntas/train_models/pos_models/model-best --pipeline \"ner\" -v pt_models_init -g 1 -G -n 2\nand got this error that complains that I have not initiated the \"ner\".\nTraining pipeline: ['ner']\nStarting with base model\n'/50gb_disk/paper_jorge_bruno_perguntas/train_models/pos_models/model-best'\nLoading vector from model 'pt_models_init'\nCounting training words (limit=0)\n\nItn    Dep Loss    NER Loss      UAS    NER P    NER R    NER F    Tag %  Token %  CPU WPS  GPU WPS\n---  ----------  ----------  -------  -------  -------  -------  -------  -------  -------  -------\nTraceback (most recent call last):                                                                                                                                                                                                               \n  File \"/disk2/paper_jorge_bruno_perguntas/env/lib/python3.6/site-packages/spacy/cli/train.py\", line 257, in train\n    losses=losses,\n  File \"/disk2/paper_jorge_bruno_perguntas/env/lib/python3.6/site-packages/spacy/language.py\", line 475, in update\n    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n  File \"nn_parser.pyx\", line 392, in spacy.syntax.nn_parser.Parser.update\n  File \"nn_parser.pyx\", line 236, in spacy.syntax.nn_parser.Parser.require_model\nValueError: [E109] Model for component 'ner' not initialized. Did you forget to load a model, or forget to call begin_training()?\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/disk2/paper_jorge_bruno_perguntas/env/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/disk2/paper_jorge_bruno_perguntas/env/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/disk2/paper_jorge_bruno_perguntas/env/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/disk2/paper_jorge_bruno_perguntas/env/lib/python3.6/site-packages/spacy/cli/train.py\", line 363, in train\n    with nlp.use_params(optimizer.averages):\n  File \"/usr/lib/python3.6/contextlib.py\", line 81, in __enter__\n    return next(self.gen)\n  File \"/disk2/paper_jorge_bruno_perguntas/env/lib/python3.6/site-packages/spacy/language.py\", line 675, in use_params\n    next(context)\n  File \"nn_parser.pyx\", line 194, in use_params\nAttributeError: 'bool' object has no attribute 'use_params'\nInfo about spaCy\nspaCy version: 2.1.9\nPlatform: Linux-4.4.0-140-generic-x86_64-with-Ubuntu-16.04-xenial\nPython version: 3.6.7", "issue_status": "Closed", "issue_reporting_time": "2019-10-30T15:25:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "201": {"issue_url": "https://github.com/explosion/spaCy/issues/4554", "issue_id": "#4554", "issue_summary": "Issues with gold alignment", "issue_description": "Collaborator\nadrianeboyd commented on 30 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nThe new alignment method from @tamuhey is very nice (#4526) but it doesn't allow substitutions, which is problem for some of our corpora with minor differences between the raw texts and token orths in things like quotes. Ideally we would require that the token orths exactly match the texts, but since we do have some slightly noisy corpora, I think we need to keep using the old align, or at least a method that allows some noise.\nComparing the new and old align functions was very useful because it brought our attention to the old alignment code, which has some bugs we can fix:\nj2i_multi is incorrect (from #4525). The problem is that _get_missing() isn't symmetric, modifies miss regions as it's working, and doesn't quite backtrack enough when alignments aren't found (I think, anyway). It could probably be rewritten, but a quick solution is to call it twice with reversed arguments to get i2j_multi and j2i_multi separately, since the first returned alignment i2j_multi (which is all we use in spacy.gold, as far as I know) is correct.\nThe Levenshtein distance costs are different with and without substitutions (old vs. new). An example:\nwords1 = [\"a\", \"b\", \"cd\"]\nwords2 = [\"ab\", \"c\", \"d\"]\nThe old alignment code says the cost is 3 (3 substitutions) and the new alignment code says it's 6 (3 insertions and 3 deletions).\nThe old alignment produces this:\ncost:         3\ni2j:          [-1  1 -1]\nj2i:          [-1  1 -1]\ni2j_multi:    {}\nj2i_multi:    {}\nNew alignment:\ncost:         6\ni2j:          [-1 -1 -1]\nj2i:          [-1 -1 -1]\ni2j_multi:    {0: 0, 1: 0}\nj2i_multi:    {1: 2, 2: 2}\nLooking at the previous example, I think that the results for i2j are a little unexpected for the old alignment. I think it's odd that tokens of the same length count as substitutions while tokens of different lengths don't and that's the only thing taken into consideration.\nspaCy/spacy/_align.pyx\nLines 107 to 112 in c2f5f9f\n for i in range(i2j.shape[0]): \n     if i2j[i] >= 0 and len(S[i]) != len(T[i2j[i]]): \n         i2j[i] = -1 \n for j in range(j2i.shape[0]): \n     if j2i[j] >= 0 and len(T[j]) != len(S[j2i[j]]): \n         j2i[j] = -1 \nThis might (nearly) work for the kind of punctuation noise present in the English data, but I wouldn't be surprised if there are some strange alignments for other corpora.\nI think this could be solved in a few ways, either by increasing the cost of substitution in the Levenshtein calculation in most cases or modifying the \"unaligned\" filtering for i2j to be a little more sophisticated.\nI think a custom substitution cost would be the most general way to handle this, with a cost that's higher than insert+deletion for everything except certain character classes like punctuation. I kind of hesitate to have much language-specific or writing system-specific code here, but maybe it's a good idea to limit the kinds of noise in the data to some degree and not allow just any same-length string substitutions.\nThe simple alignment does not consider multiple equivalent alignments and the multi alignments do not find partial alignments (see #4569).\nwords1 = [\"a\", \"b\", \"c\", \"d\", \"d\", \"e\"]\nwords2 = [\"ab\", \"d\", \"e\"]\ncost:       4\ni2j:        [-1, -1, -1,  1, -1,  2]\nj2i:        [-1,  3,  5]\ni2j_multi:  {}\nj2i_multi:  {}\nThe first d is aligned and the equivalent alignment with the second d is not considered. The multi alignment does not find ab within a b c. I suspect that handling this well is going to get too complicated.\nThis is more GoldParse than the align code, but subtoks can lead to incorrect NER labels, including None in some cases. I haven't looked at the details yet, but here's an example:\n284 T Token subtok None\n285 o Token subtok None\n286 k Token subtok None\n287 e Token subtok None\n288 n Token compound:nn None\n289 \u4ee4 \u4ee4\u724c subtok I-PRODUCT\n290 \u724c \u4ee4\u724c nmod:prep L-PRODUCT\nIn a document with a leading space (that I introduced and could normalize, but this still shouldn't happen), the first space is included into subtok and shouldn't be:\n0   USBKey subtok B-PRODUCT\n1 U USBKey subtok I-PRODUCT\n2 S USBKey subtok I-PRODUCT\n3 B USBKey subtok I-PRODUCT\n4 K USBKey subtok I-PRODUCT\n5 e USBKey subtok I-PRODUCT\n6 y USBKey nsubj L-PRODUCT\nMinor issues\nThere's no need for separate fill_i2j() and fill_j2i() functions. fill_j2i() is just fill_i2j(matrix.transpose()).\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-30T11:54:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "202": {"issue_url": "https://github.com/explosion/spaCy/issues/4552", "issue_id": "#4552", "issue_summary": "ValueError: need more than 0 values to unpack", "issue_description": "miguelwon commented on 29 Oct 2019\nI have two files (train and test) in conll format and converted to spacy json format with python -m convert:\npython -m spacy convert train.conll . -c ner\npython -m spacy convert train.conll . -c ner\nI show bellow the content of train.conll and test.conll.\nThen I upload these files with GoldCorpus:\nimport spacy \nnlp = spacy.load('pt_core_news_sm')\nfrom spacy.gold import GoldCorpus\n\ngoldcorpus = GoldCorpus(\"train.json\",\"teste.json\")\ntrain_docs = corpus.train_docs(nlp,gold_preproc = True)\nand now if I try to loop over train_docs, I get an error:\n>>> train_docs = list(train_docs)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"gold.pyx\", line 222, in train_docs\n  File \"gold.pyx\", line 240, in iter_gold_docs\n  File \"gold.pyx\", line 258, in spacy.gold.GoldCorpus._make_docs\nValueError: need more than 0 values to unpack\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Linux-4.4.0-140-generic-x86_64-with-Ubuntu-16.04-xenial\nPython version: 3.6.7\ntrain.conll\n-DOCSTART- -X- O O\n\n- PNT _ O\n\u00ab PNT _ O\nA DA _ O\nmenina CN _ O\ndos PREP+DA _ O\nrouxin\u00f3is CN _ O\n? PNT _ O\n\nA DA _ O\nLeasimpor PNM _ O\n- PNT _ O\nCompanhia PNM _ O\nde PREP _ O\nLoca\u00e7\u00e3o PNM _ O\nFinanceira PNM _ O\nImobili\u00e1ria PNM _ O\n, PNT _ O\nvai V _ O\ndistribuir INF _ O\num UM _ O\njuro CN _ O\nl\u00edquido ADJ _ O\nno PREP+DA _ O\nvalor CN _ O\nde PREP _ O\n158 DGT _ O\n$ SYB _ O\n75 DGT _ O\n, PNT _ O\nreferente ADJ _ O\nao PREP+DA _ O\npagamento CN _ O\ndo PREP+DA _ O\ncup\u00e3o CN _ O\nN\u00ba CN _ O\n1 DGT _ O\ndo PREP+DA _ O\nempr\u00e9stimo CN _ O\nObriga\u00e7\u00f5es PNM _ O\nde PREP _ O\nCaixa PNM _ O\nLeasimpor PNM _ O\n96 DGT _ O\n. PNT _ O\n\nOs DA _ O\ncentros CN _ O\nde PREP _ O\ndi\u00e1lise CN _ O\nprivados PPA _ O\ntratam V _ O\ncerca ADV _ O\nde PREP _ O\n4500 DGT _ O\ndos PREP+DA _ O\n5500 DGT _ O\ninsuficientes ADJ _ O\nrenais ADJ _ O\ncr\u00f3nicos ADJ _ O\nexistentes ADJ _ O\nem PREP _ O\nPortugal PNM _ O\n. PNT _ O\n\nAs DA _ O\ninfec\u00e7\u00f5es CN _ O\nnos PREP+DA _ O\nfrutos CN _ O\ns\u00e3o V _ O\nfrequentes ADJ _ O\nquando CJ _ O\nchuvas CN _ O\nantecedem V _ O\na DA _ O\ncolheita CN _ O\n. PNT _ O\ntest.conll\n -DOCSTART- -X- O O\n\n\" PNT _ O\nTenho V _ O\ndois CARD _ O\nirm\u00e3os CN _ O\nmais ADV _ O\nnovos ADJ _ O\ne CJ _ O\nraramente ADV _ O\nos CL _ O\nvejo V _ O\n, PNT _ O\nporque CJ _ O\nestou V _ O\nquase ADV _ O\nsempre ADV _ O\nfora ADV _ O\n. PNT _ O\n\nFa\u00e7a V _ O\nde PREP _ O\nconta CN _ O\nque CJ _ O\neu PRS _ O\nsou V _ O\numa UM _ O\ndesgra\u00e7ada PPA _ O\n, PNT _ O\nque REL _ O\nvai V _ O\nganhar INF _ O\nseis CARD _ O\nvint\u00e9ns CN _ O\ncom PREP _ O\neste DEM _ O\nba\u00fa CN _ O\n\u00e0 PREP+DA _ O\ncabe\u00e7a CN _ O\n. PNT _ O\n\n- PNT _ O\nN\u00e3o ADV _ O\n, PNT _ O\nn\u00e3o ADV _ O\nest\u00e1 V _ O\nenganado PPA _ O\n. PNT _ O\n\nMinha POSS _ O\nirm\u00e3 CN _ O\n, PNT _ O\nque REL _ O\neu PRS _ O\ndeixara V _ O\nvi\u00e7osa ADJ _ O\ne CJ _ O\nbela ADJ _ O\ncom PREP _ O\nduas CARD _ O\ncrian\u00e7as CN _ O\na PREP _ O\nbrincarem INF _ O\nbrincarem-lhe INF _ O\nno PREP+DA _ O\nrega\u00e7o CN _ O\n, PNT _ O\nmostrou V _ O\nmostrou-me V _ O\na DA _ O\nfilha CN _ O\nem PREP _ O\nprojectos CN _ O\nde PREP _ O\ncasamento CN _ O\n, PNT _ O\ne CJ _ O\no DA _ O\nfilho CN _ O\n, PNT _ O\npouco ADV _ O\ndepois PREP _ O\n, PNT _ O\nacad\u00e9mico CN _ O\ndo PREP+DA _ O\nprimeiro ORD _ O\nano CN _ O\njur\u00eddico ADJ _ O\n. PNT _ O", "issue_status": "Closed", "issue_reporting_time": "2019-10-29T16:49:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "203": {"issue_url": "https://github.com/explosion/spaCy/issues/4548", "issue_id": "#4548", "issue_summary": "pyinstaller package size for spacy has significantly increased", "issue_description": "erotavlas commented on 29 Oct 2019 \u2022\nedited\nI noticed when creating a pyinstaller executable from my spacy python scripts, that the size of the resulting package is very large mostly due to the inclusion of something called 'mkl' (whose 'dll's take up over 500MB of the package. When I created my original pyinstaller package using spacy version 2.1.4 those were not present. Although there may be additional packages which are contributing to the file size increase.\nEDIT: I found that a folder called 'torch' takes up the most space (over 50 %) and is being included although I don't see any package relating to pytorch installed in the environment\nThis was definitely not present before when I was first using pyinstaller to create spacy executables sometime between spacy versions 2.0.16 and 2.1.4\nHere is a breakdown of what is taking up the most space in the pyintaller output folder\nI went from something around 40MB to 1.8GB which is pretty significant.\nI found another reference to this in the following regarding mkl in particular\nconda-forge/numpy-feedstock#84\nI'm using Anaconda environment with spacy installed using conda install spacy==2.1.8\nbelow are the packages installed to my environment\nvia conda install spacy==2.1.8\n# packages in environment at D:\\Anaconda\\envs\\spacy218_temp2:\n#\n# Name                    Version                   Build  Channel\nasn1crypto                1.2.0                    py36_0    anaconda\nattrs                     19.3.0                     py_0    anaconda\nblas                      1.0                         mkl    anaconda\nca-certificates           2019.10.16                    0    anaconda\ncertifi                   2019.9.11                py36_0    anaconda\ncffi                      1.12.3           py36h7a1dbc1_0    anaconda\nchardet                   3.0.4                 py36_1003    anaconda\ncryptography              2.7              py36h7a1dbc1_0    anaconda\ncymem                     2.0.2            py36h74a9793_0    anaconda\ncython-blis               0.2.4            py36hfa6e2cd_1    conda-forge\nicc_rt                    2019.0.0             h0cc432a_1    anaconda\nidna                      2.8                      py36_0    anaconda\nimportlib_metadata        0.23                     py36_0    anaconda\nintel-openmp              2019.5                      281    anaconda\njsonschema                3.1.1                    py36_0    anaconda\nmkl                       2019.5                      281    anaconda\nmkl_fft                   1.0.12           py36h14836fe_0    anaconda\nmkl_random                1.0.2            py36h343c172_0    anaconda\nmore-itertools            7.2.0                    py36_0    anaconda\nmurmurhash                1.0.2            py36h33f27b4_0    anaconda\nnumpy                     1.16.4           py36h19fb1c0_0    anaconda\nnumpy-base                1.16.4           py36hc3f5095_0    anaconda\nopenssl                   1.1.1                he774522_0    anaconda\npip                       19.3.1                   py36_0    anaconda\nplac                      0.9.6                    py36_0    anaconda\npreshed                   2.0.1            py36h33f27b4_0    anaconda\npycparser                 2.19                     py36_0    anaconda\npyopenssl                 19.0.0                   py36_0    anaconda\npyrsistent                0.14.11          py36he774522_0    anaconda\npysocks                   1.7.1                    py36_0    anaconda\npython                    3.6.9                h5500b2f_0    anaconda\nrequests                  2.22.0                   py36_0    anaconda\nsetuptools                41.4.0                   py36_0    anaconda\nsix                       1.12.0                   py36_0    anaconda\nspacy                     2.1.8            py36he980bc4_0    conda-forge\nsqlite                    3.29.0               he774522_0    anaconda\nsrsly                     0.1.0            py36h6538335_0    conda-forge\nthinc                     7.0.8            py36he980bc4_0    conda-forge\ntqdm                      4.36.1                     py_0    anaconda\nurllib3                   1.24.2                   py36_0    anaconda\nvc                        14.1                 h21ff451_3    anaconda\nvs2015_runtime            15.5.2                        3    anaconda\nwasabi                    0.3.0                      py_0    conda-forge\nwheel                     0.33.6                   py36_0    anaconda\nwin_inet_pton             1.1.0                    py36_0    anaconda\nwincertstore              0.2              py36h7fe50ca_0    anaconda\nzipp                      0.6.0                      py_0    anaconda\nMy pyinstaller command for 2.1.8 is\npyinstaller ner.py --hidden-import cymem.cymem --hidden-import thinc.linalg --hidden-import murmurhash.mrmr --hidden-import cytoolz.utils --hidden-import cytoolz._signatures --hidden-import spacy.strings --hidden-import spacy.morphology --hidden-import spacy.lexeme --hidden-import spacy.tokens --hidden-import spacy.gold --hidden-import spacy.tokens.underscore --hidden-import spacy.parts_of_speech --hidden-import dill --hidden-import spacy.tokens.printers --hidden-import spacy.tokens._retokenize --hidden-import spacy.syntax --hidden-import spacy.syntax.stateclass --hidden-import spacy.syntax.transition_system --hidden-import spacy.syntax.nonproj --hidden-import spacy.syntax.nn_parser --hidden-import spacy.syntax.arc_eager --hidden-import thinc.extra.search --hidden-import spacy.syntax._beam_utils --hidden-import spacy.syntax.ner --hidden-import thinc.neural._classes.difference --hidden-import spacy.vocab --hidden-import spacy.lemmatizer --hidden-import spacy._ml --hidden-import spacy.lang.en --hidden-import srsly.msgpack.util --hidden-import preshed.maps --hidden-import thinc.neural._aligned_alloc --hidden-import blis --hidden-import blis.py --hidden-import spacy.matcher._schemas --hidden-import spacy._align --hidden-import spacy.syntax._parser_model --hidden-import spacy.kb\nEDIT: I am unable to recreate my original package (size of 42MB) I think because every time I try to install spacy (earlier version such as 2.1.4) it still installs the latest of most other packages,, and somehow torch gets included somehow as well.", "issue_status": "Closed", "issue_reporting_time": "2019-10-28T20:10:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "204": {"issue_url": "https://github.com/explosion/spaCy/issues/4547", "issue_id": "#4547", "issue_summary": "Unable to update stop words list in en_core_web_sm", "issue_description": "LizMcQuillan commented on 29 Oct 2019\nHow to reproduce the behaviour\nAttempting to append the default stop words list, but can't seem to add anything using either\nmore_stops = ['like', 'vet', 'veteran', 'Veterans']\nSTOP_WORDS.update(more_stops)\nor\nnlp.Defaults.stop_words |= {\"like\", \"vet\", \"veteran\", \"Veterans\"}\nNeither command results in an error message, but when I go on to filter out stop words they remain in the dataset.", "issue_status": "Closed", "issue_reporting_time": "2019-10-28T19:53:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "205": {"issue_url": "https://github.com/explosion/spaCy/issues/4546", "issue_id": "#4546", "issue_summary": "No wheels for 2.1.9", "issue_description": "Contributor\nHiromuHota commented on 28 Oct 2019\nFirst of all, congratulations on the v2.1.9 release.\nIt's been released just two hours ago, but no wheels are available to download yet.\nPlease build and upload wheels.", "issue_status": "Closed", "issue_reporting_time": "2019-10-28T18:18:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "206": {"issue_url": "https://github.com/explosion/spaCy/issues/4545", "issue_id": "#4545", "issue_summary": "Model linked but not available", "issue_description": "jimbudarz commented on 28 Oct 2019\nHow to reproduce the behaviour\nHello, due to work network restrictions, I cannot use python -m download..., so I use a downloaded model and just try to link it. However, although it links successfully, it is not available in my environment (and isn't reflected in SpaCy info).\n$ python -m spacy link ./en_core_web_md-2.1.0.tar.gz en\nLinking successful\nDocuments/SPACY_en_Models/en_core_web_md-2.1.0.tar.gz\n--> /Users/username/anaconda3/lib/python3.7/site-packages/spacy/data/en\n\nYou can now load the model via spacy.load('en')\n$ python -m spacy info\nInfo about spaCy\n\nspaCy version      2.0.12         \nLocation           /Users/username/anaconda3/lib/python3.7/site-packages/spacy\nPlatform           Darwin-18.7.0-x86_64-i386-64bit\nPython version     3.7.3          \nModels                            \nYour Environment\nOperating System: Mac OSX Mojave\nPython Version Used: 3.7.3\nspaCy Version Used: 2.0.12\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-28T17:40:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "207": {"issue_url": "https://github.com/explosion/spaCy/issues/4544", "issue_id": "#4544", "issue_summary": "Step 2 in entity linking runs out of memory after 6-8 hours", "issue_description": "petulla commented on 28 Oct 2019 \u2022\nedited\nI've been stuck on Step 2 of the entity linking docs and could use some help. I'm sure I'm making a basic error.\nHow to reproduce the problem\nI create the knowledge base and training data. This seemed to work as it said \"Done!\" after running for 12 hours.\npython wikidata_pretrain_kb.py 'latest-all.json.bz2' 'enwiki-latest-pages-articles-multistream.xml.bz2' './output2' 'en_core_web_lg'\nThe second step is dying off for me. Checking the logs, I'm running out of memory (and a lot of memory is being used). I have 32gb of RAM.\nI'm running:\npython wikidata_train_entity_linker.py output2:\noutput2 is the directory with the knowledge base. Its contents are:\nentity_alias.csv gold_entities.jsonl\nentity_defs.csv  kb\nentity_descriptions.csv nlp_kb\nentity_freq.csv  prior_prob.csv\nHere is the command line output after running step 2's command. It simply escapes to the command line after running for 6+ hrs and running out of memory:\n2019-10-27 19:52:44,320 - INFO - __main__ - Creating Entity Linker with Wikipedia and WikiData\n2019-10-27 19:52:44,320 - INFO - __main__ - STEP 1a: Loading model from output2/nlp_kb\nI1027 19:52:44.486585 4612883904 file_utils.py:39] PyTorch version 1.2.0 available.\nI1027 19:52:45.090944 4612883904 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\nI1027 19:52:59.587772 4612883904 wikidata_train_entity_linker.py:67] STEP 1b: Loading KB from output2/kb\nI1027 19:53:07.744759 4612883904 wikidata_train_entity_linker.py:75] STEP 2: Reading training dataset from output2/gold_entities.jsonl\nI1027 19:53:07.744909 4612883904 wikipedia_processor.py:473] Reading train data with limit None\n2247226it [8:19:35,  3.76s/it]Killed: 9\nYour Environment\nOperating System: os Mojave\nPython Version Used: 3.7.4\nspaCy Version Used: 2.2.2.dev1 (built from source)\nEnvironment Information: PyEnv", "issue_status": "Closed", "issue_reporting_time": "2019-10-28T16:25:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "208": {"issue_url": "https://github.com/explosion/spaCy/issues/4538", "issue_id": "#4538", "issue_summary": "Backport memory leak fix to patch spacy 2.0 and 2.1", "issue_description": "Contributor\nkabirkhan commented on 28 Oct 2019\nFeature description\nMy team (and I assume a lot of other companies) are using spacy 2.0 and 2.1 in various places. We have A LOT of trained models in spacy 2.0 and 2.1. It would be great to have the fix @adrianeboyd made here #4486 for the memory leak ported back to the older versions as well since it's a really small change with a pretty huge impact.\ni.e. It's much easier to upgrade to spacy 2.1.9 from 2.1.8 rather than 2.2.2 and retrain all the models we have.\nThanks!\nAnd not sure this counts as a feature request but didn't seem like a bug either haha.\n2", "issue_status": "Closed", "issue_reporting_time": "2019-10-28T04:50:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "209": {"issue_url": "https://github.com/explosion/spaCy/issues/4532", "issue_id": "#4532", "issue_summary": "Spacy in Docker Container on Raspberry", "issue_description": "Chipandcharge commented on 27 Oct 2019\nHow to reproduce the problem\nDoes someone else experience problems installing spacy in a Docker container based on a amr python image?\nI am using a simple Dockerfile structure and it takes ages to installing back end dependencies for spacy, which ultimately results in an error.\nFROM arm32v7/python:3\n\nWORKDIR /usr/src/app\n\nCOPY requirements.txt ./\n\nRUN pip install --no-cache-dir -r requirements.txt\n\nRUN python3 -m spacy download de_core_news_sm\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n  ERROR: Command errored out with exit status 1:\n   command: /usr/local/bin/python /usr/local/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-iy4thlv7/normal --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'murmurhash<1.1.0,>=0.28.0' 'thinc<7.2.0,>=7.1.1' wheel 'preshed<3.1.0,>=3.0.2' 'cymem<2.1.0,>=2.0.2' 'cython>=0.25'\n       cwd: None\n  Complete output (228 lines):\n  Collecting murmurhash<1.1.0,>=0.28.0\n    Downloading https://files.pythonhosted.org/packages/22/e9/411be1845f1ac07ae3bc40a4b19ba401819baed4fa63b4f5ef28b2300eb4/murmurhash-1.0.2.tar.gz\n  Collecting thinc<7.2.0,>=7.1.1\n    Downloading https://files.pythonhosted.org/packages/38/13/fe9cbdc0a97578d13063352ecc9cc3d1e0dda2e59aa68cc91428b2a1b106/thinc-7.1.1.tar.gz (1.9MB)\n  Collecting wheel\n    Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n  Collecting preshed<3.1.0,>=3.0.2\n    Downloading https://files.pythonhosted.org/packages/5f/14/de231123ddbe0bf12bd9b1993122d67f22859643bee4dad3b6ce91986336/preshed-3.0.2.tar.gz (167kB)\n  Collecting cymem<2.1.0,>=2.0.2\n    Downloading https://files.pythonhosted.org/packages/8b/dc/0976e04cc46f86e0dd3ee3797ec68057eaafebf31daca9a076dc138b9920/cymem-2.0.2.tar.gz (47kB)\n  Collecting cython>=0.25\n    Downloading https://files.pythonhosted.org/packages/a5/1f/c7c5450c60a90ce058b47ecf60bb5be2bfe46f952ed1d3b95d1d677588be/Cython-0.29.13.tar.gz (2.1MB)\n  Collecting blis<0.5.0,>=0.4.0\n    Downloading https://files.pythonhosted.org/packages/98/5a/f9b8a78e3d1fdde1b0215413d88ab55d907ab81f95b62418a6e9cda30dec/blis-0.4.1.tar.gz (1.8MB)\n  Collecting wasabi<1.1.0,>=0.0.9\n    Downloading https://files.pythonhosted.org/packages/c9/a1/6a75283482e6662cc3b050aed656ab234b97d64c3e12df10d08f1c4932ad/wasabi-0.3.0-py3-none-any.whl\n  Collecting srsly<1.1.0,>=0.0.6\n    Downloading https://files.pythonhosted.org/packages/b0/63/b68061954228346cbab2c41adb36339678605c47da016f5c71c7ef65f510/srsly-0.1.0.tar.gz (186kB)\n  Collecting numpy>=1.7.0\n    Downloading https://files.pythonhosted.org/packages/b6/d6/be8f975f5322336f62371c9abeb936d592c98c047ad63035f1b38ae08efe/numpy-1.17.3.zip (6.4MB)\n  Collecting plac<1.0.0,>=0.9.6\n    Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n  Collecting tqdm<5.0.0,>=4.10.0\n    Downloading https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl (52kB)\n  Building wheels for collected packages: murmurhash, thinc, preshed, cymem, cython, blis, srsly, numpy\n    Building wheel for murmurhash (setup.py): started\n    Building wheel for murmurhash (setup.py): finished with status 'done'\n    Created wheel for murmurhash: filename=murmurhash-1.0.2-cp38-cp38-linux_armv7l.whl size=80517 sha256=6116f466ecdcd6a96b88b2ba07bd1944d380bb81f08f358a8a63d7398581579c\n    Stored in directory: /root/.cache/pip/wheels/77/f4/97/79cf3c5af8be5044c749bb449340a0f667ebb1d8c1f388cd65\n    Building wheel for thinc (setup.py): started\n    Building wheel for thinc (setup.py): still running...\n    Building wheel for thinc (setup.py): still running...\n    Building wheel for thinc (setup.py): finished with status 'done'\n    Created wheel for thinc: filename=thinc-7.1.1-cp38-cp38-linux_armv7l.whl size=5187587 sha256=cd18dfd6921de8335e257b0ea49375e6a23b89786ecacc6b5a9f3a62d3a9005b\n    Stored in directory: /root/.cache/pip/wheels/d3/b4/8a/2e6141a07f1e2d6a11f41f7d42169e10268c776ead03deb2c1\n    Building wheel for preshed (setup.py): started\n    Building wheel for preshed (setup.py): finished with status 'done'\n    Created wheel for preshed: filename=preshed-3.0.2-cp38-cp38-linux_armv7l.whl size=634661 sha256=5b398a415ac8b0d667071dc01db7798d2bd98448dd99b6494d584063fd720fcd\n    Stored in directory: /root/.cache/pip/wheels/17/94/15/50a48f534bac3ff98ef0394ae81d03babf76545cf6270433ec\n    Building wheel for cymem (setup.py): started\n    Building wheel for cymem (setup.py): finished with status 'done'\n    Created wheel for cymem: filename=cymem-2.0.2-cp38-cp38-linux_armv7l.whl size=141624 sha256=421e0ce32594e12997da81f27ef9673176347742766c3b729d2bcef68ef4a02e\n    Stored in directory: /root/.cache/pip/wheels/a9/50/b8/dffa92fefe7fd39bd1fdb8dc1fa790b981356b82a2275ffb81\n    Building wheel for cython (setup.py): started\n    Building wheel for cython (setup.py): still running...\n    Building wheel for cython (setup.py): still running...\n    Building wheel for cython (setup.py): still running...\n    Building wheel for cython (setup.py): finished with status 'done'\n    Created wheel for cython: filename=Cython-0.29.13-cp38-cp38-linux_armv7l.whl size=5806454 sha256=dd1690fda9a57fb746a53477d9a3e861903e9d26a3f0bc67085ac17b139a0ce8\n    Stored in directory: /root/.cache/pip/wheels/5b/1e/17/15dd6d435ec0644967eb8e1493af09ae28cd94184c6f416d02\n    Building wheel for blis (setup.py): started\n    Building wheel for blis (setup.py): finished with status 'error'\n    ERROR: Command errored out with exit status 1:\n     command: /usr/local/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-qri_89ty/blis/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-qri_89ty/blis/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-mnboxjbm --python-tag cp38\n         cwd: /tmp/pip-install-qri_89ty/blis/\n    Complete output (71 lines):\n    BLIS_COMPILER? None\n    running bdist_wheel\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-armv7l-3.8\n    creating build/lib.linux-armv7l-3.8/blis\n    copying blis/about.py -> build/lib.linux-armv7l-3.8/blis\n    copying blis/__init__.py -> build/lib.linux-armv7l-3.8/blis\n    copying blis/benchmark.py -> build/lib.linux-armv7l-3.8/blis\n    creating build/lib.linux-armv7l-3.8/blis/tests\n    copying blis/tests/test_dotv.py -> build/lib.linux-armv7l-3.8/blis/tests\n    copying blis/tests/common.py -> build/lib.linux-armv7l-3.8/blis/tests\n    copying blis/tests/__init__.py -> build/lib.linux-armv7l-3.8/blis/tests\n    copying blis/tests/test_gemm.py -> build/lib.linux-armv7l-3.8/blis/tests\n    copying blis/py.pyx -> build/lib.linux-armv7l-3.8/blis\n    copying blis/cy.pyx -> build/lib.linux-armv7l-3.8/blis\n    copying blis/__init__.pxd -> build/lib.linux-armv7l-3.8/blis\n    copying blis/cy.pxd -> build/lib.linux-armv7l-3.8/blis\n    running build_ext\n    unix\n    py_compiler gcc\n    {'BLIS_ARCH': 'generic', 'HOSTNAME': 'aa9d42588791', 'SSL_CERT_FILE': '/opt/_internal/certs.pem', 'TERM': 'xterm', 'OLDPWD': '/usr/local/repos/cython-blis', 'LD_LIBRARY_PATH': '/opt/rh/devtoolset-2/root/usr/lib64:/opt/rh/devtoolset-2/root/usr/lib:/usr/local/lib64:/usr/local/lib', 'LS_COLORS': 'no=00:fi=00:di=00;34:ln=00;36:pi=40;33:so=00;35:bd=40;33;01:cd=40;33;01:or=01;05;37;41:mi=01;05;37;41:ex=00;32:*.cmd=00;32:*.exe=00;32:*.com=00;32:*.btm=00;32:*.bat=00;32:*.sh=00;32:*.csh=00;32:*.tar=00;31:*.tgz=00;31:*.arj=00;31:*.taz=00;31:*.lzh=00;31:*.zip=00;31:*.z=00;31:*.Z=00;31:*.gz=00;31:*.bz2=00;31:*.bz=00;31:*.tz=00;31:*.rpm=00;31:*.cpio=00;31:*.jpg=00;35:*.gif=00;35:*.bmp=00;35:*.xbm=00;35:*.xpm=00;35:*.png=00;35:*.tif=00;35:', 'VIRTUAL_ENV': '/usr/local/repos/cython-blis/env3.6', 'PATH': '/usr/local/repos/cython-blis/env3.6/bin:/opt/rh/devtoolset-2/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'PWD': '/usr/local/repos/cython-blis/flame-blis', 'BLIS_ARCh': 'generic', 'LANG': 'en_US.UTF-8', 'HOME': '/root', 'SHLVL': '2', 'LANGUAGE': 'en_US.UTF-8', 'BLIS_ARC': 'generic', 'AUDITWHEEL_PLAT': 'manylinux1_x86_64', 'PKG_CONFIG_PATH': '/usr/local/lib/pkgconfig', 'LESSOPEN': '|/usr/bin/lesspipe.sh %s', 'G_BROKEN_FILENAMES': '1', '_': '/usr/local/repos/cython-blis/env3.6/bin/python'}\n    gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/generic/bli_cntx_init_generic.c -o /tmp/tmpz39samp7/bli_cntx_init_generic.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/haswell/bli_cntx_init_haswell.c -o /tmp/tmpz39samp7/bli_cntx_init_haswell.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/penryn/bli_cntx_init_penryn.c -o /tmp/tmpz39samp7/bli_cntx_init_penryn.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/piledriver/bli_cntx_init_piledriver.c -o /tmp/tmpz39samp7/bli_cntx_init_piledriver.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/sandybridge/bli_cntx_init_sandybridge.c -o /tmp/tmpz39samp7/bli_cntx_init_sandybridge.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/steamroller/bli_cntx_init_steamroller.c -o /tmp/tmpz39samp7/bli_cntx_init_steamroller.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c -o /tmp/tmpz39samp7/bli_amaxv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n    gcc: error: unrecognized -march target: core-avx2\n    gcc: note: valid arguments are: armv2 armv2a armv3 armv3m armv4 armv4t armv5 armv5t armv5e armv5te armv5tej armv6 armv6j armv6k armv6z armv6kz armv6zk armv6t2 armv6-m armv6s-m armv7 armv7-a armv7ve armv7-r armv7-m armv7e-m armv8-a armv8.1-a armv8.2-a armv8.3-a armv8.4-a armv8-m.base armv8-m.main armv8-r iwmmxt iwmmxt2 native\n    gcc: error: missing argument to \u2018-march=\u2019\n    gcc: error: unrecognized command line option \u2018-mavx2\u2019\n    gcc: error: unrecognized command line option \u2018-mfma\u2019\n    gcc: error: unrecognized command line option \u2018-mfpmath=sse\u2019\n    Traceback (most recent call last):\n      File \"<string>\", line 1, in <module>\n      File \"/tmp/pip-install-qri_89ty/blis/setup.py\", line 239, in <module>\n        setup(\n      File \"/usr/local/lib/python3.8/site-packages/setuptools/__init__.py\", line 145, in setup\n        return distutils.core.setup(**attrs)\n      File \"/usr/local/lib/python3.8/distutils/core.py\", line 148, in setup\n        dist.run_commands()\n      File \"/usr/local/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n        self.run_command(cmd)\n      File \"/usr/local/lib/python3.8/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/usr/local/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 192, in run\n        self.run_command('build')\n      File \"/usr/local/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n        self.distribution.run_command(command)\n      File \"/usr/local/lib/python3.8/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/usr/local/lib/python3.8/distutils/command/build.py\", line 135, in run\n        self.run_command(cmd_name)\n      File \"/usr/local/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n        self.distribution.run_command(command)\n      File \"/usr/local/lib/python3.8/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/usr/local/lib/python3.8/distutils/command/build_ext.py\", line 340, in run\n        self.build_extensions()\n      File \"/tmp/pip-install-qri_89ty/blis/setup.py\", line 103, in build_extensions\n        objects = self.compile_objects(compiler.split(\"-\")[0], arch, OBJ_DIR)\n      File \"/tmp/pip-install-qri_89ty/blis/setup.py\", line 188, in compile_objects\n        objects.append(self.build_object(env=env, **spec))\n      File \"/tmp/pip-install-qri_89ty/blis/setup.py\", line 201, in build_object\n        subprocess.check_call(command, cwd=BLIS_DIR)\n      File \"/usr/local/lib/python3.8/subprocess.py\", line 364, in check_call\n        raise CalledProcessError(retcode, cmd)\n    subprocess.CalledProcessError: Command '['gcc', '-c', '/tmp/pip-install-qri_89ty/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c', '-o', '/tmp/tmpz39samp7/bli_amaxv_zen_int.o', '-O3', '-mavx2', '-mfma', '-mfpmath=sse', '-march=core-avx2', '-fPIC', '-std=c99', '-D_POSIX_C_SOURCE=200112L', '-DBLIS_VERSION_STRING=\"0.5.1\"', '-DBLIS_IS_BUILDING_LIBRARY', '-Iinclude/linux-x86_64', '-I./frame/3/', '-I./frame/ind/ukernels/', '-I./frame/1m/', '-I./frame/1f/', '-I./frame/1/', '-I./frame/include', '-I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64']' returned non-zero exit status 1.\n    ----------------------------------------\n    ERROR: Failed building wheel for blis\n    Running setup.py clean for blis\n    Building wheel for srsly (setup.py): started\n    Building wheel for srsly (setup.py): finished with status 'done'\n    Created wheel for srsly: filename=srsly-0.1.0-cp38-cp38-linux_armv7l.whl size=553213 sha256=42303ed71cb5998ed9108e5fabaea32755d37eb920ac6328dd53d1712fcd200f\n    Stored in directory: /root/.cache/pip/wheels/f4/ec/a6/28e4169fc27cdfdd9f3ea828069a6bb02f7d251dfa2f9ad4f8\n    Building wheel for numpy (setup.py): started\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): still running...\n    Building wheel for numpy (setup.py): finished with status 'done'\n    Created wheel for numpy: filename=numpy-1.17.3-cp38-cp38-linux_armv7l.whl size=14981903 sha256=5065cce415f70364b27b9b1a4193a405135a8e85b35ea17433c180a278a60e6c\n    Stored in directory: /root/.cache/pip/wheels/5e/e9/4b/dd5a8eb53e97dfcc1314eca9c6769edd3cad379d6644c1ad94\n  Successfully built murmurhash thinc preshed cymem cython srsly numpy\n  Failed to build blis\n  Installing collected packages: murmurhash, cymem, preshed, numpy, blis, wasabi, srsly, plac, tqdm, thinc, wheel, cython\n      Running setup.py install for blis: started\n      Running setup.py install for blis: finished with status 'error'\n      ERROR: Command errored out with exit status 1:\n       command: /usr/local/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-qri_89ty/blis/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-qri_89ty/blis/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-goybt5fy/install-record.txt --single-version-externally-managed --prefix /tmp/pip-build-env-iy4thlv7/normal --compile\n           cwd: /tmp/pip-install-qri_89ty/blis/\n      Complete output (73 lines):\n      BLIS_COMPILER? None\n      running install\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-armv7l-3.8\n      creating build/lib.linux-armv7l-3.8/blis\n      copying blis/about.py -> build/lib.linux-armv7l-3.8/blis\n      copying blis/__init__.py -> build/lib.linux-armv7l-3.8/blis\n      copying blis/benchmark.py -> build/lib.linux-armv7l-3.8/blis\n      creating build/lib.linux-armv7l-3.8/blis/tests\n      copying blis/tests/test_dotv.py -> build/lib.linux-armv7l-3.8/blis/tests\n      copying blis/tests/common.py -> build/lib.linux-armv7l-3.8/blis/tests\n      copying blis/tests/__init__.py -> build/lib.linux-armv7l-3.8/blis/tests\n      copying blis/tests/test_gemm.py -> build/lib.linux-armv7l-3.8/blis/tests\n      copying blis/py.pyx -> build/lib.linux-armv7l-3.8/blis\n      copying blis/cy.pyx -> build/lib.linux-armv7l-3.8/blis\n      copying blis/__init__.pxd -> build/lib.linux-armv7l-3.8/blis\n      copying blis/cy.pxd -> build/lib.linux-armv7l-3.8/blis\n      running build_ext\n      unix\n      py_compiler gcc\n      {'BLIS_ARCH': 'generic', 'HOSTNAME': 'aa9d42588791', 'SSL_CERT_FILE': '/opt/_internal/certs.pem', 'TERM': 'xterm', 'OLDPWD': '/usr/local/repos/cython-blis', 'LD_LIBRARY_PATH': '/opt/rh/devtoolset-2/root/usr/lib64:/opt/rh/devtoolset-2/root/usr/lib:/usr/local/lib64:/usr/local/lib', 'LS_COLORS': 'no=00:fi=00:di=00;34:ln=00;36:pi=40;33:so=00;35:bd=40;33;01:cd=40;33;01:or=01;05;37;41:mi=01;05;37;41:ex=00;32:*.cmd=00;32:*.exe=00;32:*.com=00;32:*.btm=00;32:*.bat=00;32:*.sh=00;32:*.csh=00;32:*.tar=00;31:*.tgz=00;31:*.arj=00;31:*.taz=00;31:*.lzh=00;31:*.zip=00;31:*.z=00;31:*.Z=00;31:*.gz=00;31:*.bz2=00;31:*.bz=00;31:*.tz=00;31:*.rpm=00;31:*.cpio=00;31:*.jpg=00;35:*.gif=00;35:*.bmp=00;35:*.xbm=00;35:*.xpm=00;35:*.png=00;35:*.tif=00;35:', 'VIRTUAL_ENV': '/usr/local/repos/cython-blis/env3.6', 'PATH': '/usr/local/repos/cython-blis/env3.6/bin:/opt/rh/devtoolset-2/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'PWD': '/usr/local/repos/cython-blis/flame-blis', 'BLIS_ARCh': 'generic', 'LANG': 'en_US.UTF-8', 'HOME': '/root', 'SHLVL': '2', 'LANGUAGE': 'en_US.UTF-8', 'BLIS_ARC': 'generic', 'AUDITWHEEL_PLAT': 'manylinux1_x86_64', 'PKG_CONFIG_PATH': '/usr/local/lib/pkgconfig', 'LESSOPEN': '|/usr/bin/lesspipe.sh %s', 'G_BROKEN_FILENAMES': '1', '_': '/usr/local/repos/cython-blis/env3.6/bin/python'}\n      gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/generic/bli_cntx_init_generic.c -o /tmp/tmpr_3z_ol8/bli_cntx_init_generic.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/haswell/bli_cntx_init_haswell.c -o /tmp/tmpr_3z_ol8/bli_cntx_init_haswell.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/penryn/bli_cntx_init_penryn.c -o /tmp/tmpr_3z_ol8/bli_cntx_init_penryn.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/piledriver/bli_cntx_init_piledriver.c -o /tmp/tmpr_3z_ol8/bli_cntx_init_piledriver.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/sandybridge/bli_cntx_init_sandybridge.c -o /tmp/tmpr_3z_ol8/bli_cntx_init_sandybridge.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/config/steamroller/bli_cntx_init_steamroller.c -o /tmp/tmpr_3z_ol8/bli_cntx_init_steamroller.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-qri_89ty/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c -o /tmp/tmpr_3z_ol8/bli_amaxv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.1\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64\n      gcc: error: unrecognized -march target: core-avx2\n      gcc: note: valid arguments are: armv2 armv2a armv3 armv3m armv4 armv4t armv5 armv5t armv5e armv5te armv5tej armv6 armv6j armv6k armv6z armv6kz armv6zk armv6t2 armv6-m armv6s-m armv7 armv7-a armv7ve armv7-r armv7-m armv7e-m armv8-a armv8.1-a armv8.2-a armv8.3-a armv8.4-a armv8-m.base armv8-m.main armv8-r iwmmxt iwmmxt2 native\n      gcc: error: missing argument to \u2018-march=\u2019\n      gcc: error: unrecognized command line option \u2018-mavx2\u2019\n      gcc: error: unrecognized command line option \u2018-mfma\u2019\n      gcc: error: unrecognized command line option \u2018-mfpmath=sse\u2019\n      Traceback (most recent call last):\n        File \"<string>\", line 1, in <module>\n        File \"/tmp/pip-install-qri_89ty/blis/setup.py\", line 239, in <module>\n          setup(\n        File \"/usr/local/lib/python3.8/site-packages/setuptools/__init__.py\", line 145, in setup\n          return distutils.core.setup(**attrs)\n        File \"/usr/local/lib/python3.8/distutils/core.py\", line 148, in setup\n          dist.run_commands()\n        File \"/usr/local/lib/python3.8/distutils/dist.py\", line 966, in run_commands\n          self.run_command(cmd)\n        File \"/usr/local/lib/python3.8/distutils/dist.py\", line 985, in run_command\n          cmd_obj.run()\n        File \"/usr/local/lib/python3.8/site-packages/setuptools/command/install.py\", line 61, in run\n          return orig.install.run(self)\n        File \"/usr/local/lib/python3.8/distutils/command/install.py\", line 545, in run\n          self.run_command('build')\n        File \"/usr/local/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n          self.distribution.run_command(command)\n        File \"/usr/local/lib/python3.8/distutils/dist.py\", line 985, in run_command\n          cmd_obj.run()\n        File \"/usr/local/lib/python3.8/distutils/command/build.py\", line 135, in run\n          self.run_command(cmd_name)\n        File \"/usr/local/lib/python3.8/distutils/cmd.py\", line 313, in run_command\n          self.distribution.run_command(command)\n        File \"/usr/local/lib/python3.8/distutils/dist.py\", line 985, in run_command\n          cmd_obj.run()\n        File \"/usr/local/lib/python3.8/distutils/command/build_ext.py\", line 340, in run\n          self.build_extensions()\n        File \"/tmp/pip-install-qri_89ty/blis/setup.py\", line 103, in build_extensions\n          objects = self.compile_objects(compiler.split(\"-\")[0], arch, OBJ_DIR)\n        File \"/tmp/pip-install-qri_89ty/blis/setup.py\", line 188, in compile_objects\n          objects.append(self.build_object(env=env, **spec))\n        File \"/tmp/pip-install-qri_89ty/blis/setup.py\", line 201, in build_object\n          subprocess.check_call(command, cwd=BLIS_DIR)\n        File \"/usr/local/lib/python3.8/subprocess.py\", line 364, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['gcc', '-c', '/tmp/pip-install-qri_89ty/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c', '-o', '/tmp/tmpr_3z_ol8/bli_amaxv_zen_int.o', '-O3', '-mavx2', '-mfma', '-mfpmath=sse', '-march=core-avx2', '-fPIC', '-std=c99', '-D_POSIX_C_SOURCE=200112L', '-DBLIS_VERSION_STRING=\"0.5.1\"', '-DBLIS_IS_BUILDING_LIBRARY', '-Iinclude/linux-x86_64', '-I./frame/3/', '-I./frame/ind/ukernels/', '-I./frame/1m/', '-I./frame/1f/', '-I./frame/1/', '-I./frame/include', '-I/tmp/pip-install-qri_89ty/blis/blis/_src/include/linux-x86_64']' returned non-zero exit status 1.\n      ----------------------------------------\n  ERROR: Command errored out with exit status 1: /usr/local/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-qri_89ty/blis/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-qri_89ty/blis/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-goybt5fy/install-record.txt --single-version-externally-managed --prefix /tmp/pip-build-env-iy4thlv7/normal --compile Check the logs for full command output.\n  ----------------------------------------\nERROR: Command errored out with exit status 1: /usr/local/bin/python /usr/local/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-iy4thlv7/normal --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'murmurhash<1.1.0,>=0.28.0' 'thinc<7.2.0,>=7.1.1' wheel 'preshed<3.1.0,>=3.0.2' 'cymem<2.1.0,>=2.0.2' 'cython>=0.25' Check the logs for full command output.\nThe command '/bin/sh -c pip3 install --no-cache-dir -r requirements.txt' returned a non-zero code: 1\nYour Environment\nOperating System: raspberrian\nPython Version Used: 3\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-27T14:44:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "210": {"issue_url": "https://github.com/explosion/spaCy/issues/4531", "issue_id": "#4531", "issue_summary": "char_span not returning spans in some cases", "issue_description": "omri374 commented on 27 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nWeird behavior when calling char_span to create JSON for training.\nWhen the entity is 'Graematter Inc':\ntext = 'He offered to map it for Graematter Inc.'\nentities =[(25, 39, 'ORG')]\nimport spacy\nfrom spacy.lang.en import English\nnlp = English()\ndoc = nlp(text)\n[doc.char_span(start_idx, end_idx, label=label) for start_idx, end_idx, label in entities]\n# returns [None]\nWhereas when the entity is 'Graematter Int':\ntext = 'He offered to map it for Graematter Int.'\nentities =[(25, 39, 'ORG')]\nfrom spacy.lang.en import English\nnlp = English()\ndoc = nlp(text)\n[doc.char_span(start_idx, end_idx, label=label) for start_idx, end_idx, label in entities]\n# returns [Graematter Int]\nI get different results.\nI'm not sure what's causing the difference here. Note that the period at the end of the sentence is not a part of the entity.\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.4", "issue_status": "Closed", "issue_reporting_time": "2019-10-27T09:14:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "211": {"issue_url": "https://github.com/explosion/spaCy/issues/4529", "issue_id": "#4529", "issue_summary": "[Bug] GoldParse instantiation with Misaligned tokens.", "issue_description": "Contributor\ntamuhey commented on 27 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nIndexError\ntext = \"A'B C\"\nwords = [\"A\", \"'\", \"B\", \"C\"]\ndoc = tokenizer(text)\nGoldParse(doc, words=words)\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-7-45b5f2ec7502> in <module>\n      2 words = [\"A\", \"'\", \"B\", \"C\"]\n      3 doc = tokenizer(text)\n----> 4 GoldParse(doc, words=words)\n\ngold.pyx in spacy.gold.GoldParse.__init__()\n\nIndexError: list index out of range\nTypeError\ntext = \"A-B\"\nwords = [\"A-B\"]\ndoc = tokenizer(text)\nGoldParse(doc, words=words)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-8-9ae6959d65fe> in <module>\n      2 words = [\"A-B\"]\n      3 doc = tokenizer(text)\n----> 4 GoldParse(doc, words=words)\n\ngold.pyx in spacy.gold.GoldParse.__init__()\n\nTypeError: list indices must be integers or slices, not NoneType\nYour Environment\nOperating System: mac OSX\nPython Version Used: 3.7\nspaCy Version Used: master", "issue_status": "Closed", "issue_reporting_time": "2019-10-27T07:40:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "212": {"issue_url": "https://github.com/explosion/spaCy/issues/4528", "issue_id": "#4528", "issue_summary": "DocBin: serialization of user_data", "issue_description": "pzajec commented on 27 Oct 2019\nI think the line from the DocBin class should be changed from:\ndoc.user_data.update(srsly.msgpack_loads(self.user_data[i]))\nto\ndoc.user_data.update(srsly.msgpack_loads(self.user_data[i], use_list=False))\nto match the one from the Doc class.\nConsider the following two examples:\nUsing Doc.to_bytes() / Doc.from_bytes()\n# file serialize_doc.py\nimport spacy\nfrom spacy.tokens import Doc\n\nnlp = spacy.load('en')\nDoc.set_extension(\"my_custom_attr\", default=None)\ntext = \"Hello from New York!\"\n\ndoc = nlp(text)\ndoc._.my_custom_attr = \"hello\"\n\nwith open('test_doc.bin', 'wb') as f:\n    f.write(doc.to_bytes())\n# file deserialize_doc.py\nimport spacy\nfrom spacy.tokens import Doc\n\nnlp = spacy.blank(\"en\")\nwith open('test_doc.bin', 'rb') as f:\n    bytes_data = f.read()\n\ndoc = Doc(nlp.vocab).from_bytes(bytes_data)\nDoc.set_extension(\"my_custom_attr\", default=None)\n\nprint(doc._.my_custom_attr)\nOutput: \"hello\" as expected\nUsing DocBin.to_bytes() / DocBin.from_bytes()\nimport spacy\nfrom spacy.tokens import Doc, DocBin\n\nnlp = spacy.load('en')\nDoc.set_extension(\"my_custom_attr\", default=None)\ntext = \"Hello from New York!\"\n\ndoc = nlp(text)\ndoc._.my_custom_attr = \"hello\"\ndoc_bin = DocBin(attrs=None, store_user_data=True)\ndoc_bin.add(doc)\n\nwith open('test_docbin.bin', 'wb') as f:\n    f.write(doc_bin.to_bytes())\nimport spacy\nfrom spacy.tokens import DocBin\n\nnlp = spacy.blank(\"en\")\nwith open('test_docbin.bin', 'rb') as f:\n    bytes_data = f.read()\n\ndoc_bin = DocBin(store_user_data=True).from_bytes(bytes_data)\nOutput: TypeError: unhashable type: 'list'\nReason\nThe following code which reads serialized DocBin first prints the correct output when use_list=False and crashes when use_list=True\nimport srsly\nimport zlib\n\nwith open('test_docbin.bin', 'rb') as f:\n    bytes_data = f.read()\n\nmsg = srsly.msgpack_loads(zlib.decompress(bytes_data))\nprint(srsly.msgpack_loads(msg['user_data'][0], use_list=False))\nsrsly.msgpack_loads(msg['user_data'][0], use_list=True)\nOutput:\n> {('._.', 'my_custom_attr', None, None): 'hello'}\n> TypeError: unhashable type: 'list'\nHow to reproduce the behaviour\nFirst serialize the DocBin instance using:\nimport spacy\nfrom spacy.tokens import Doc, DocBin\n\nnlp = spacy.load('en')\nDoc.set_extension(\"my_custom_attr\", default=None)\ntext = \"Hello from New York!\"\n\ndoc = nlp(text)\ndoc._.my_custom_attr = \"hello\"\ndoc_bin = DocBin(attrs=None, store_user_data=True)\ndoc_bin.add(doc)\n\nwith open('test_docbin.bin', 'wb') as f:\n    f.write(doc_bin.to_bytes())\nThen de-serialize using:\nimport spacy\nfrom spacy.tokens import DocBin\n\nnlp = spacy.blank(\"en\")\nwith open('test_docbin.bin', 'rb') as f:\n    bytes_data = f.read()\n\ndoc_bin = DocBin(store_user_data=True).from_bytes(bytes_data)\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Linux-5.0.0-31-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nModels: en\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-27T02:32:38Z", "fixed_by": "#4540", "pull_request_summary": "Fix serialization of extension attr values in DocBin", "pull_request_description": "Member\nines commented on 28 Oct 2019\nFixes #4528.\nDescription\nExtension attribute values are stored in the doc.user_data dictionary, keyed by a tuple. This caused serialization issues if use_list=False wasn't set on srlsy.msgpack_loads.\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-28T15:02:14Z", "files_changed": [["2", "spacy/tests/regression/test_issue4528.py"], ["3", "spacy/tokens/_serialize.py"]]}, "213": {"issue_url": "https://github.com/explosion/spaCy/issues/4525", "issue_id": "#4525", "issue_summary": "gold.align seems to be broken", "issue_description": "Contributor\ntamuhey commented on 26 Oct 2019\nHow to reproduce the behaviour\n>>> from spacy.gold import align\n>>> tokens_a=[\"ab\",\"c\"]\n>>> tokens_b=[\"a\",\"b\",\"c\"]\n>>> align(tokens_a, tokens_b)\n(2, array([-1,  2], dtype=int32), array([-1, -1,  1], dtype=int32), {}, {})\nI think j2i_multi (the last of returned items) should be {0:0:,1:0}\nYour Environment\nOperating System: mac OSX\nPython Version Used: 3.7\nspaCy Version Used: master", "issue_status": "Closed", "issue_reporting_time": "2019-10-26T09:10:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "214": {"issue_url": "https://github.com/explosion/spaCy/issues/4515", "issue_id": "#4515", "issue_summary": "I need a MultiLevelTextCategorizer.", "issue_description": "Astral1020 commented on 24 Oct 2019\ntext = \"Dumplings are delicious.\"\nnlp.load(\"xxx\")\ndoc = nlp(text)\n-----------------\ndoc.cats = {\n                \"Shopping\": {\"score\": 0.01},\n                \"Food\": {\"score\":0.99, \"child\": {\"Chinese Food\": {\"score\": 0.99}, \"British Cuisine\": {\"score\": 0.01}}\n            }\nWhat I am doing:\nAdd multiple textcat pipelines first. (https://github.com/Astral1020/MultiLevelTextCategorizer)\nIssue:\nIf I call spacy.load(), I got an exception:\nKeyError: \"[E002] Can't find factory for 'ROOT'. This usually happens when spaCy calls nlp.create_pipe with a component name that's not built in - for example, when constructing the pipeline from a model's meta.json. If you're using a custom component, you can write to Language.factories['ROOT'] or remove it from the model meta and add it via nlp.add_pipe instead.\"", "issue_status": "Closed", "issue_reporting_time": "2019-10-24T07:49:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "215": {"issue_url": "https://github.com/explosion/spaCy/issues/4514", "issue_id": "#4514", "issue_summary": "spacy_factories in nlp.meta", "issue_description": "Contributor\ntamuhey commented on 24 Oct 2019 \u2022\nedited\nI made many custom components of spacy, and sometimes I want to put same components in one nlp pipeline.\nI tried changing one of their name and they can be put in one pipeline, but that made it impossible to restore with spacy.load.\nSo it is great if nlp.meta can holds spacy_factories to avoid confliction.", "issue_status": "Closed", "issue_reporting_time": "2019-10-24T07:16:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "216": {"issue_url": "https://github.com/explosion/spaCy/issues/4512", "issue_id": "#4512", "issue_summary": "Exception in ner.cp37-win_amd64.pyd when training NER", "issue_description": "SquirrelMaster619 commented on 24 Oct 2019\nGreetings,\nI've had this issue on stack overflow for a few weeks now and no responses so i thought i would post here:\nhttps://stackoverflow.com/questions/58397661/large-training-set-for-ner\nI have a project that involves taking property descriptions and labeling key data elements. I decided to use spaCy in an effort to train my own NER pipe since these descriptions are not written like conventional sentences. However, when I go to train it gets to about 20% and then crashes and i am unable to find an explanation.\nHow it's set up\nBelow is a sample of my JSON. The full JSON is 2.6 MB and contains >1000 descriptions ranging from 40 - ~500 tokens. The file contains ~54000 tokens in total. (Be advised the below data has been changed to not reflect the actual property)\n[{\n    \"id\": 1, \"paragraphs\": \n    [{\n    \"raw\": \"Lots 1 and 2 of Block 1, in the City of Santa Clarita, County of Los Angeles, State of California, as per Map recorded in Book 1, Page 1 of Miscellaneous Maps, in the Office of the County Recorder of said County \"\n        , \"sentences\": \n        [{\n            \"tokens\": \n            [\n                        {\"id\": 1, \"orth\": \"Lots\", \"ner\": \"B-LOT\"}\n                        , {\"id\": 2, \"orth\": \"1\", \"ner\": \"I-LOT\"}\n                        , {\"id\": 3, \"orth\": \"and\", \"ner\": \"I-LOT\"}\n                        , {\"id\": 4, \"orth\": \"2\", \"ner\": \"L-LOT\"}\n                        , {\"id\": 5, \"orth\": \"of\", \"ner\": \"O\"}\n                        , {\"id\": 6, \"orth\": \"Block\", \"ner\": \"B-BLOCK\"}\n                        , {\"id\": 7, \"orth\": \"1,\", \"ner\": \"L-BLOCK\"}\n                        , {\"id\": 8, \"orth\": \"in\", \"ner\": \"O\"}\n                        , {\"id\": 9, \"orth\": \"the\", \"ner\": \"O\"}\n                        , {\"id\": 10, \"orth\": \"City\", \"ner\": \"O\"}\n                        , {\"id\": 11, \"orth\": \"of\", \"ner\": \"O\"}\n                        , {\"id\": 12, \"orth\": \"Santa\", \"ner\": \"O\"}\n                        , {\"id\": 13, \"orth\": \"Clarita,\", \"ner\": \"O\"}\n                        , {\"id\": 14, \"orth\": \"County\", \"ner\": \"O\"}\n                        , {\"id\": 15, \"orth\": \"of\", \"ner\": \"O\"}\n                        , {\"id\": 16, \"orth\": \"Los\", \"ner\": \"O\"}\n                        , {\"id\": 17, \"orth\": \"Angeles,\", \"ner\": \"O\"}\n                        , {\"id\": 18, \"orth\": \"State\", \"ner\": \"O\"}\n                        , {\"id\": 19, \"orth\": \"of\", \"ner\": \"O\"}\n                        , {\"id\": 20, \"orth\": \"California,\", \"ner\": \"O\"}\n                        , {\"id\": 21, \"orth\": \"as\", \"ner\": \"O\"}\n                        , {\"id\": 22, \"orth\": \"per\", \"ner\": \"O\"}\n                        , {\"id\": 23, \"orth\": \"Map\", \"ner\": \"O\"}\n                        , {\"id\": 24, \"orth\": \"recorded\", \"ner\": \"O\"}\n                        , {\"id\": 25, \"orth\": \"in\", \"ner\": \"O\"}\n                        , {\"id\": 26, \"orth\": \"Book\", \"ner\": \"B-BOOK\"}\n                        , {\"id\": 27, \"orth\": \"1,\", \"ner\": \"L-BOOK\"}\n                        , {\"id\": 28, \"orth\": \"Page\", \"ner\": \"B-PAGE\"}\n                        , {\"id\": 29, \"orth\": \"1\", \"ner\": \"L-PAGE\"}\n                        , {\"id\": 30, \"orth\": \"of\", \"ner\": \"O\"}\n                        , {\"id\": 31, \"orth\": \"Miscellaneous\", \"ner\": \"B-MAPTYPE\"}\n                        , {\"id\": 32, \"orth\": \"Maps,\", \"ner\": \"L-MAPTYPE\"}\n                        , {\"id\": 33, \"orth\": \"in\", \"ner\": \"O\"}\n                        , {\"id\": 34, \"orth\": \"the\", \"ner\": \"O\"}\n                        , {\"id\": 35, \"orth\": \"Office\", \"ner\": \"O\"}\n                        , {\"id\": 36, \"orth\": \"of\", \"ner\": \"O\"}\n                        , {\"id\": 37, \"orth\": \"the\", \"ner\": \"O\"}\n                        , {\"id\": 38, \"orth\": \"County\", \"ner\": \"O\"}\n                        , {\"id\": 39, \"orth\": \"Recorder\", \"ner\": \"O\"}\n                        , {\"id\": 40, \"orth\": \"of\", \"ner\": \"O\"}\n                        , {\"id\": 41, \"orth\": \"said\", \"ner\": \"O\"}\n                        , {\"id\": 42, \"orth\": \"County\", \"ner\": \"O\"}\n            ]\n        }]\n    }]\n}]\nI took the Train.py file that comes with spaCy in the cli folder and created my own version for this process. I left the core functionality of the file in tact and just added a few things such as some new labels for my data set and a custom tokenizer that works with white space instead of the conventional tokenizer. The function is below:\ndef NERTrain(lang\n          , output_dir\n          , train_data\n          , dev_data\n          , n_iter=30\n          , n_sents=0\n          , parser_multitasks=''\n          , entity_multitasks=''\n          , use_gpu=-1\n          , vectors=None\n          , gold_preproc=False\n          , version=\"0.0.0\"\n          , meta_path=None\n          , verbose=False\n          , newLabels = None):\n    \"\"\"\n    Train a model. Expects data in spaCy's JSON format.\n    \"\"\"\n    util.fix_random_seed()\n    util.set_env_log(True)\n    n_sents = n_sents or None\n    output_path = util.ensure_path(output_dir)\n    train_path = util.ensure_path(train_data)\n    dev_path = util.ensure_path(dev_data)\n    meta_path = util.ensure_path(meta_path)\n    if not output_path.exists():\n        output_path.mkdir()\n    if not train_path.exists():\n        prints(train_path, title=Messages.M050, exits=1)\n    if dev_path and not dev_path.exists():\n        prints(dev_path, title=Messages.M051, exits=1)\n    if meta_path is not None and not meta_path.exists():\n        prints(meta_path, title=Messages.M020, exits=1)\n    meta = util.read_json(meta_path) if meta_path else {}\n    if not isinstance(meta, dict):\n        prints(Messages.M053.format(meta_type=type(meta)),\n               title=Messages.M052, exits=1)\n    meta.setdefault('lang', lang)\n    meta.setdefault('name', 'unnamed')\n\n    pipeline = ['ner']\n\n    # Take dropout and batch size as generators of values -- dropout\n    # starts high and decays sharply, to force the optimizer to explore.\n    # Batch size starts at 1 and grows, so that we make updates quickly\n    # at the beginning of training.\n    dropout_rates = util.decaying(util.env_opt('dropout_from', 0.2),\n                                  util.env_opt('dropout_to', 0.2),\n                                  util.env_opt('dropout_decay', 0.0))\n    batch_sizes = util.compounding(util.env_opt('batch_from', 1),\n                                   util.env_opt('batch_to', 16),\n                                   util.env_opt('batch_compound', 1.001))\n    max_doc_len = util.env_opt('max_doc_len', 5000)\n    corpus = GoldCorpus(train_path, dev_path, limit=n_sents)\n    n_train_words = corpus.count_train()\n\n    lang_class = util.get_lang_class(lang)\n    nlp = lang_class()\n\n    if \"ner\" in nlp.pipe_names:\n        nlp.remove_pipe(\"ner\")\n\n    ner = nlp.create_pipe(\"ner\")\n    nlp.add_pipe(ner, first=True)\n\n    meta['pipeline'] = pipeline\n    nlp.meta.update(meta)\n    if vectors:\n        print(\"Load vectors model\", vectors)\n        util.load_model(vectors, vocab=nlp.vocab)\n        for lex in nlp.vocab:\n            values = {}\n            for attr, func in nlp.vocab.lex_attr_getters.items():\n                # These attrs are expected to be set by data. Others should\n                # be set by calling the language functions.\n                if attr not in (CLUSTER, PROB, IS_OOV, LANG):\n                    values[lex.vocab.strings[attr]] = func(lex.orth_)\n            lex.set_attrs(**values)\n            lex.is_oov = False\n#    for name in pipeline:\n#        nlp.add_pipe(nlp.create_pipe(name), name=name)\n    if parser_multitasks:\n        for objective in parser_multitasks.split(','):\n            nlp.parser.add_multitask_objective(objective)\n    if entity_multitasks:\n        for objective in entity_multitasks.split(','):\n            nlp.entity.add_multitask_objective(objective)\n    optimizer = nlp.begin_training(lambda: corpus.train_tuples, device=use_gpu)\n    nlp._optimizer = None\n    nlp.tockenizer=WTok(nlp)\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n\n    if(newLabels != None):\n        for l in newLabels:\n            ner.add_label(l)\n\n    print(\"Itn.  Dep Loss  NER Loss  UAS     NER P.  NER R.  NER F.  Tag %   Token %  CPU WPS  GPU WPS\")\n    try:\n        train_docs = corpus.train_docs(nlp, projectivize=True, noise_level=0.0,\n                                       gold_preproc=gold_preproc, max_length=0)\n        train_docs = list(train_docs)\n        with nlp.disable_pipes(*other_pipes):\n            for i in range(n_iter):\n                with tqdm.tqdm(total=n_train_words, leave=False) as pbar:\n                    losses = {}\n                    for batch in minibatch(train_docs, size=batch_sizes):\n                        batch = [(d, g) for (d, g) in batch if len(d) < max_doc_len]\n                        if not batch:\n                            continue\n                        docs, golds = zip(*batch)\n                        nlp.update(docs, golds, sgd=optimizer,\n                                   drop=next(dropout_rates), losses=losses)\n                        pbar.update(sum(len(doc) for doc in docs))\n\n                with nlp.use_params(optimizer.averages):\n                    util.set_env_log(False)\n                    epoch_model_path = output_path / ('model%d' % i)\n                    nlp.to_disk(epoch_model_path)\n                    nlp_loaded = util.load_model_from_path(epoch_model_path)\n                    dev_docs = list(corpus.dev_docs(\n                                    nlp_loaded,\n                                    gold_preproc=gold_preproc))\n                    nwords = sum(len(doc_gold[0]) for doc_gold in dev_docs)\n                    start_time = timer()\n                    scorer = nlp_loaded.evaluate(dev_docs, verbose)\n                    end_time = timer()\n                    if use_gpu < 0:\n                        gpu_wps = None\n                        cpu_wps = nwords/(end_time-start_time)\n                    else:\n                        gpu_wps = nwords/(end_time-start_time)\n                        with Model.use_device('cpu'):\n                            nlp_loaded = util.load_model_from_path(epoch_model_path)\n                            dev_docs = list(corpus.dev_docs(\n                                            nlp_loaded, gold_preproc=gold_preproc))\n                            start_time = timer()\n                            scorer = nlp_loaded.evaluate(dev_docs)\n                            end_time = timer()\n                            cpu_wps = nwords/(end_time-start_time)\n                    acc_loc = (output_path / ('model%d' % i) / 'accuracy.json')\n                    with acc_loc.open('w') as file_:\n                        file_.write(json_dumps(scorer.scores))\n                    meta_loc = output_path / ('model%d' % i) / 'meta.json'\n                    meta['accuracy'] = scorer.scores\n                    meta['speed'] = {'nwords': nwords, 'cpu': cpu_wps,\n                                     'gpu': gpu_wps}\n                    meta['vectors'] = {'width': nlp.vocab.vectors_length,\n                                       'vectors': len(nlp.vocab.vectors),\n                                       'keys': nlp.vocab.vectors.n_keys}\n                    meta['lang'] = nlp.lang\n                    meta['pipeline'] = pipeline\n                    meta['spacy_version'] = '>=%s' % about.__version__\n                    meta.setdefault('name', 'model%d' % i)\n                    meta.setdefault('version', version)\n\n                    with meta_loc.open('w') as file_:\n                        file_.write(json_dumps(meta))\n                    util.set_env_log(True)\n                print_progress(i, losses, scorer.scores, cpu_wps=cpu_wps,\n                               gpu_wps=gpu_wps)\n    finally:\n        print(\"Saving model...\")\n        with nlp.use_params(optimizer.averages):\n            final_model_path = output_path / 'model-final'\n            nlp.to_disk(final_model_path)\nWhat I've Done\nWhen the attempting to run the full JSON file failed, I attempted the same with a smaller sample of 100. The process was able to run all the way through with no issues. Now before I go chopping up my dataset into bite sized chunks of 100 (which i really don't want / shouldn't have to do) i wanted to see if anyone could take a look and see if this is possibly some sort of 1. limit in spacy i have somehow hit, 2. memory issue, 3. or some sort of code issue i overlooked.\nI was able to get an error message related to the crash but i cannot find any information on it. Unhandled exception at 0x00007FF8EB9E2BE2 (ner.cp37-win_amd64.pyd) in python.exe: 0xC0000005: Access violation reading location 0x000001C4213D1FE4. occurred The error is marked on invoke_main() within exe_common.inl\nOperating System: Windows 10\nPython Version Used: 3.7.4\nspaCy Version Used: 2.0.16", "issue_status": "Closed", "issue_reporting_time": "2019-10-23T19:56:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "217": {"issue_url": "https://github.com/explosion/spaCy/issues/4511", "issue_id": "#4511", "issue_summary": "Example code for Spacy Entity Linking?", "issue_description": "davidbernat commented on 23 Oct 2019\nApologies for what is likely a simple failure to find the right documentation. I understand Spacy recently added Entity Linking. How do I enable this in the default pipeline? What model do I need to install to bring this capability? Sample code would be very helpful.\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(nlp.create_pipe(\"entity_linker\"))\ndoc = nlp(\"The Democratic Party has cycled through various candidates.\")\nThrows error:\nModel for component 'entity_linker' not initialized. Did you forget to load a model, or forget to call begin_training()?\nWhere do I get the model? Thanks!", "issue_status": "Closed", "issue_reporting_time": "2019-10-23T18:26:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "218": {"issue_url": "https://github.com/explosion/spaCy/issues/4508", "issue_id": "#4508", "issue_summary": "spacy sentencizer is breaking the sentences at irrelevant points.", "issue_description": "niki33 commented on 22 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nself.nlp = English()\nsentencizer = self.nlp.create_pipe(\"sentencizer\")\nself.nlp.add_pipe(sentencizer)\nfaq_answerr_token_doc = self.nlp(\"The weekly off/holidays, during the rainy period, shall not be counted as a leave in a government office. (Weekly off refers to employees who are in support roles working on a follow the moon model, granted a weekly day off in lieu of weekends).\")\nIt should break the sentence at \".\" but it is breaking at \"(\".\nHence the response looks like below:\n1.) The intervening weekly off/holidays, during the leave period, shall not be counted as leave. (\n2.) Weekly off refers to employees who are in support roles working on a follow the sun model, granted a weekly day off in lieu of weekends).\")\nYour Environment\nOperating System: Linux and windows\nPython Version Used:3.6.3\nspaCy Version Used:2.1.8\nEnvironment Information: I am using this with the Rasa framework 3.6.2.", "issue_status": "Closed", "issue_reporting_time": "2019-10-22T17:56:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "219": {"issue_url": "https://github.com/explosion/spaCy/issues/4506", "issue_id": "#4506", "issue_summary": "Vectors.most_similar should always return 1.0 for identical vectors", "issue_description": "Member\nines commented on 22 Oct 2019 \u2022\nedited\nWe should probably hard-code the workaround for the imprecision, just like we do for the built-in similarity methods.\nHow to reproduce the behaviour\nspaCy/spacy/tests/vocab_vectors/test_vectors.py\nLines 144 to 154 in 74a19ae\n @pytest.mark.xfail \n def test_vectors_most_similar_identical(): \n     \"\"\"Test that most similar identical vectors are assigned a score of 1.0.\"\"\" \n     data = numpy.asarray([[4, 2, 2, 2], [4, 2, 2, 2], [1, 1, 1, 1]], dtype=\"f\") \n     v = Vectors(data=data, keys=[\"A\", \"B\", \"C\"]) \n     keys, _, scores = v.most_similar(numpy.asarray([[4, 2, 2, 2]], dtype=\"f\")) \n     assert scores[0][0] == 1.0  # not 1.0000002 \n     data = numpy.asarray([[1, 2, 3], [1, 2, 3], [1, 1, 1]], dtype=\"f\") \n     v = Vectors(data=data, keys=[\"A\", \"B\", \"C\"]) \n     keys, _, scores = v.most_similar(numpy.asarray([[1, 2, 3]], dtype=\"f\")) \n     assert scores[0][0] == 1.0  # not 0.9999999 ", "issue_status": "Closed", "issue_reporting_time": "2019-10-22T16:20:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "220": {"issue_url": "https://github.com/explosion/spaCy/issues/4504", "issue_id": "#4504", "issue_summary": "KeyError: 'labels' using textcat on the train CLI", "issue_description": "louisguitton commented on 22 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nI've searched the repo for similar issues and I couldn't find anything so here comes nothing:\nI've read #4038 and #4226 and am very eager to use that feature. But when I try it, I get an error.\ndownload the example data from the PR\ncurl https://raw.githubusercontent.com/adrianeboyd/spaCy/50179e19ae5b600eeb959ee71d102fdf859aada8/examples/training/textcat_example_data/cooking.json > data/cooking.json\nuse the train CLI to train a textcat model\n\u2192 python -m spacy train en data/rules_investigation/output data/rules_investigation/cooking.json data/rules_investigation/cooking.json --base-model en_core_web_md --pipeline textcat --textcat-arch ensemble\nTraining pipeline: ['textcat']\nStarting with base model 'en_core_web_md'\nCounting training words (limit=0)\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/louis.guitton/workspace/my-secret-project/venv/lib/python3.7/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/Users/louis.guitton/workspace/my-secret-project/venv/lib/python3.7/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/louis.guitton/workspace/my-secret-project/venv/lib/python3.7/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/Users/louis.guitton/workspace/my-secret-project/venv/lib/python3.7/site-packages/spacy/cli/train.py\", line 251, in train\n    textcat_labels = nlp.get_pipe(\"textcat\").cfg[\"labels\"]\nKeyError: 'labels\nFrom what I understand, the code is missing the step that you would do usually with:\ntextcat = nlp.get_pipe(\"textcat\")\nfor cat in ALL_LABELS:\n    textcat.add_label(cat)\nso because labels are never set, textcat_labels = nlp.get_pipe(\"textcat\").cfg[\"labels\"] is failing.\nAm I missing something ?\n(PS: I was able to train the textcat model writing some python basically adapting this but I'd love to use that CLI feature)\nYour Environment\nOperating System: MacOS\nPython Version Used: Python 3.7.4\nspaCy Version Used: 2.2.1\nEnvironment Information:\n\u2192 python -m spacy info                                                                                                                                                       \n============================== Info about spaCy ==============================\n\nspaCy version    2.2.1\nLocation         /Users/louis.guitton/workspace/my-secret-project/venv/lib/python3.7/site-packages/spacy\nPlatform         Darwin-18.7.0-x86_64-i386-64bit\nPython version   3.7.4\nModels", "issue_status": "Closed", "issue_reporting_time": "2019-10-22T14:51:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "221": {"issue_url": "https://github.com/explosion/spaCy/issues/4503", "issue_id": "#4503", "issue_summary": "Losing pos feature when saving/loading with to_disk/from_disk", "issue_description": "johann-petrak commented on 22 Oct 2019\nHow to reproduce the behaviour\nThe following code demonstrates that all the POS tags (feature pos_/pos) are blank/0\nafter saving and loading using the to_disk/from_disk methods.\nOn the other hand the tag and dp features are restored fine.\nBut ... why????\nfrom spacy.tokens import Doc\nfrom spacy.vocab import Vocab\nimport spacy \nnlp = spacy.load(\"de\")\n\nt1 = \"Dies ist ein deutscher Satz. Und hier noch einer.\"\n\ndoc1 = nlp(t1)\nprint(\"Version: \", spacy.__version__)\nprint(\"After NLP: \", [t.pos_ for t in doc1])\nprint(\"After NLP: \", [t.pos for t in doc1])\nprint(\"As JSON: \", doc1.to_json())\n\ndoc1.to_disk(\"debug_spacy_save.spacy\")\ndoc1.vocab.to_disk(\"debug_spacy_save_vocab.spacy\")\n\nvoc2 = Vocab().from_disk(\"debug_spacy_save_vocab.spacy\")\ndoc2 = Doc(voc2).from_disk(\"debug_spacy_save.spacy\")\nprint(\"After Load: \", [t.pos_ for t in doc2])\nprint(\"After Load: \", [t.pos for t in doc2])\nprint(\"As JSON: \", doc2.to_json())\nInfo about spaCy\nspaCy version: 2.1.6\nPlatform: Linux-4.15.0-62-generic-x86_64-with-debian-buster-sid\nPython version: 3.6.9\nModels: es, en, de", "issue_status": "Closed", "issue_reporting_time": "2019-10-22T13:10:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "222": {"issue_url": "https://github.com/explosion/spaCy/issues/4502", "issue_id": "#4502", "issue_summary": "Train entitiy linking always get segmented fault", "issue_description": "swicaksono commented on 22 Oct 2019 \u2022\nedited\nI successfully created the KB model by using my own dataset. Well, when I'm gonna try to train and add the EL component into the pipeline alongside with NER, I've had always encountered the error segmented fault (core dump), although the vocab is the one I used to create the KB model. I trace it and it's always when load_bulk the KB model.\nKnowledgeBase(vocab=nlp.vocab)\n\n# here I've always get segmented fault\nkb.load_bulk(os.path.join(path_kb, lang))\nAny helps?\nThanks.\nYour Environment\nOperating System: Ubuntu 19\nPython Version Used: 3.6.6\nspaCy Version Used: 2.2.1", "issue_status": "Closed", "issue_reporting_time": "2019-10-22T11:18:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "223": {"issue_url": "https://github.com/explosion/spaCy/issues/4500", "issue_id": "#4500", "issue_summary": "pretrained KB", "issue_description": "Member\nsvlandeg commented on 22 Oct 2019\nI prefer answering questions here instead of on email, for the benefit of the larger community:\nQ: I was wondering if there were a way to get a pretrained KB from somewhere, as I couldn't find any.", "issue_status": "Closed", "issue_reporting_time": "2019-10-22T08:43:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "224": {"issue_url": "https://github.com/explosion/spaCy/issues/4496", "issue_id": "#4496", "issue_summary": "PhraseMatcher.add() docs should be more explicit about how to pass a list of Doc objects", "issue_description": "ZeroCool2u commented on 22 Oct 2019 \u2022\nedited\nThe documentation for PhraseMatcher should be more explicit about how to pass in a list to PhraseMatcher.add().\nThe documentation specifies the type of the 3rd parameter as *docs or List, which is a bit misleading. *docs is technically correct, but the add() method actually fails when providing a list of Doc objects with the error:\n  File \"phrasematcher.pyx\", line 193, in spacy.matcher.phrasematcher.PhraseMatcher.add\nTypeError: an integer is required\nWhich does not indicate a list cannot be passed. Perhaps it should be more explicit in the example? I lost my morning to this, so I figured I'd make this issue just in case anyone else has the same problem.\nWhich page or section is this issue related to?\nhttps://github.com/explosion/spaCy/blob/master/website/docs/api/phrasematcher.md", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T20:09:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "225": {"issue_url": "https://github.com/explosion/spaCy/issues/4495", "issue_id": "#4495", "issue_summary": "Multiple tokens generated for a plural form", "issue_description": "ewaldatsensentia commented on 21 Oct 2019\nThis happens for \"thats\" and \"whens\". Both those words are in the nlp.vocab. They are split into tokens \"that\" and \"s\" by both nlp and nlp.make_doc.\nHow to reproduce the behaviour\nimport spacy\nnlp = spacy.load('en_core_web_md')\nfor t in nlp(\"How many thats are in this sentence?\"): print (t,t.lemma_)\noutput\nHow how\nmany many\nthat that\ns s\nare be\nin in\nthis this\nsentence sentence\n? ?\nYour Environment\nspaCy version: 2.2.1\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T17:24:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "226": {"issue_url": "https://github.com/explosion/spaCy/issues/4494", "issue_id": "#4494", "issue_summary": "Tokenizer missing data during enumeration", "issue_description": "davidbren commented on 21 Oct 2019\nHow to reproduce the behaviour\nfrom spacy.tokenizer import Tokenizer\ntokens = tokenizer(sample)\nresult = len(tokens.text)\n\nprint(\"tokens length \" + str(result))\n\ntotalTokenSize = 0\n\nfor i in tokens:\n    totalTokenSize = totalTokenSize + len(i.text)\n\nprint(\"totalTokenSize \" + str(totalTokenSize))\nOutput is\ntokens length 34,980\ntotalTokenSize 30,034\nSo I am missing 4.946 when I loop vs when I take length\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.6\nspaCy Version Used: 2.1.8\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T16:56:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "227": {"issue_url": "https://github.com/explosion/spaCy/issues/4493", "issue_id": "#4493", "issue_summary": "Update Rasa info", "issue_description": "Contributor\nju-sh commented on 21 Oct 2019 \u2022\nedited\nRasa has been updated. The rasa NLU and rasa core repos have been merged.\nWhich page or section is this issue related to?\nhttps://spacy.io/universe/project/rasa", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T16:55:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "228": {"issue_url": "https://github.com/explosion/spaCy/issues/4491", "issue_id": "#4491", "issue_summary": "ImportError: cannot import name 'get_string_id' from 'spacy.strings'", "issue_description": "LizMcQuillan commented on 21 Oct 2019 \u2022\nedited\nInstalling and loading spaCy on a new machine. I've done this a few times, but have never run into errors before. I had a handful error messages to get through just to install and import spacy (things like #2514) and in the end I manually deleted all spaCy files and directories to do a clean installation.\nNot sure what to do here.\nMy code:\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nparser = English()\nfrom spacy import en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\nFull Traceback:\nImportError                               Traceback (most recent call last)\n<ipython-input-28-8db0160a0952> in <module>\n      2 #!python -m spacy download en\n      3 from spacy.lang.en.stop_words import STOP_WORDS\n----> 4 from spacy.lang.en import English\n      5 parser = English()\n      6 from spacy import en_core_web_sm\n\n~/anaconda3/lib/python3.7/site-packages/spacy/lang/en/__init__.py in <module>\n     12 from ..tokenizer_exceptions import BASE_EXCEPTIONS\n     13 from ..norm_exceptions import BASE_NORMS\n---> 14 from ...language import Language\n     15 from ...attrs import LANG, NORM\n     16 from ...util import update_exc, add_lookups\n\n~/anaconda3/lib/python3.7/site-packages/spacy/language.py in <module>\n     15 from .vocab import Vocab\n     16 from .lemmatizer import Lemmatizer\n---> 17 from .lookups import Lookups\n     18 from .pipeline import DependencyParser, Tagger\n     19 from .pipeline import Tensorizer, EntityRecognizer, EntityLinker\n\n~/anaconda3/lib/python3.7/site-packages/spacy/lookups.py in <module>\n      8 from .errors import Errors\n      9 from .util import SimpleFrozenDict, ensure_path\n---> 10 from .strings import get_string_id\n     11 \n     12 \n\nImportError: cannot import name 'get_string_id' from 'spacy.strings' (/Users/lmcquillan/anaconda3/lib/python3.7/site-packages/spacy/strings.cpython-37m-darwin.so)\nMy Environment\n**Operating System: Mac\nspaCy version: 2.2.1\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.3\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T14:44:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "229": {"issue_url": "https://github.com/explosion/spaCy/issues/4490", "issue_id": "#4490", "issue_summary": "Tokenizer edge condition - back-to-back parentheses", "issue_description": "pythonBerg commented on 21 Oct 2019\nThe tokenizer is return unexpected results for me in 2.1.8. I know I can enhance by extending, but I though this obvious enough to report...\n\"Twenty-four thousand, two hundred dollars ($24,000,200)(the loan amount)\"\nI am seeing where tokens include \"(\", $24,000,200)(the\" and the remainder as expected. It seems like the prefix and suffix regexs are conflicting and thus ignored. Perhaps adding each of ) ( to infix would solve...it has for me in testing", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T13:10:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "230": {"issue_url": "https://github.com/explosion/spaCy/issues/4485", "issue_id": "#4485", "issue_summary": "Universal POS Tag scheme discrepancies", "issue_description": "chrisjbryant commented on 21 Oct 2019\nAlthough I raised this issue ages ago (#593) and I know some work has been done on it, I just want to let you know there is still a discrepancy in the way spacy maps fine POS tags to coarse POS tags for English.\nSpecifically, the current spacy tag_map maps PRP$ and WP$ to PRON while Universal Dependencies map them to DET (UD tag map). These are words like \"my car\" and \"whose car\", which UD lists as examples of determiners (link).\nI also noticed some old information on the annotation specifications page:\nhttps://spacy.io/api/annotation#pos-tagging\nSpecifically, the header in the Universal Part-of-speech Tags tab says you use the Universal Dependencies scheme, while the headers in the English and German tabs say you use the Google Universal Tagset. The latter is no longer true (although maybe it is for German?)!", "issue_status": "Closed", "issue_reporting_time": "2019-10-20T22:29:51Z", "fixed_by": "#4501", "pull_request_summary": "Update tag maps and docs for English and German", "pull_request_description": "Collaborator\nadrianeboyd commented on 22 Oct 2019 \u2022\nedited\nDescription\nUpdate English tag_map based on this conversion table:\nhttps://universaldependencies.org/tagset-conversion/en-penn-uposf.html\nUpdate German tag_map based on this conversion table:\nhttps://universaldependencies.org/tagset-conversion/de-stts-uposf.html\nAdd missing Tiger dependencies to glossary\nAdd quotes to definition of TO\nUpdate POS/TAG tables for English and German docs using current information generated from the tag_maps and GLOSSARY.\nUpdate warning that -PRON- is specific to English\nFixes #4485.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-24T10:56:05Z", "files_changed": [["8", "spacy/glossary.py"], ["10", "spacy/lang/de/tag_map.py"], ["10", "spacy/lang/en/tag_map.py"], ["165", "website/docs/api/annotation.md"]]}, "231": {"issue_url": "https://github.com/explosion/spaCy/issues/4482", "issue_id": "#4482", "issue_summary": "TextCategorizer unclear how it operates, unexpected behaviour when switching to other lang than texts are", "issue_description": "haratyma commented on 20 Oct 2019\nI have a general question regarding the way of textcategorizer in spacy works.\nFrom different pieces of information on the spacy documentation and my tests it seems that it doesn't use the pretrained models and their features at least in languages that have small models like German or of course are so called apha supported languages like Polish without models.\nI am using spacy 2.1.8 and my categorization task is to categorize German text to 18 exclusive categories.\nIt works well and I gained surprisingly well cross validated accuracy, however while trying to understand the mechanics I found some surprising things when testing\nI am not loading the model as using spacy.blank\nThe core of my code looks like follows:\n`\nnlp = spacy.blank(\"de\")\ntextcat = nlp.create_pipe(\n            \"textcat\",\n            config={\n                \"exclusive_classes\": True,\n                \"architecture\": \"simple_cnn\",\n            }\n        )\nnlp.add_pipe(textcat, last=True)\nfor c in categories:\n    textcat.add_label(c)  \n\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\nwith nlp.disable_pipes(*other_pipes):  \n    optimizer = nlp.begin_training()\n    print(\"what vectors:\",nlp.vocab.vectors.name)\n    #here I get always the output spacy_pretrained_vectors,f.e. if I call \"pl\" instead \"de\"\n`\nThen I am using nlp.update in minibatches and optimizing with averages\n`\nnlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\nwith textcat.model.use_params(optimizer.averages):\n# evaluate on the dev data split off in load_data()\nscores = eval_save_multi_scores(nlp.tokenizer, textcat, x_dev_skf, y_dev_skf)\n`\nFirst strange things was that switch to completly different language like Polish and doing the same categorization on German texts doesn't change the results, final accuracy is the same after 5folds CV.\nI also tried Finnish and Spanish even also the same.\nSo it proves for me it doesn't use the properties of the pretrained models even if there are available context-sensitive tensors that are shared across the pipeline like for German (from issue #2523)\nAs per description from spacy.io for the architecture that I am using:\n\"A neural network model where token vectors are calculated using a CNN. The vectors are mean pooled and used as features in a feed-forward network.\"\nso from that I understand that it uses tokenizer( specific for the language only and then calculate ! (so not using the ones from model pretrained)the so context-specific token vectors for texts and this enough for NN to adjust weights to have right categorization even if the token are not so true while choosing other lang than text is.\nCould you confirm it works like that?\nit mainly refers to :\nhttps://spacy.io/api/textcategorizer", "issue_status": "Closed", "issue_reporting_time": "2019-10-20T10:21:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "232": {"issue_url": "https://github.com/explosion/spaCy/issues/4481", "issue_id": "#4481", "issue_summary": "Custom Pipeline", "issue_description": "Khanifsaleh commented on 20 Oct 2019 \u2022\nedited\nI want to make the model can return full name of entites (PERSON).\nFor example:\ndef my_component(doc):\nmy_dict = {\"Thrump\":\"Donald Thrump\",\"Jokowi\":\"Joko Widodo\"}\nfor ent in doc.ents:\nif ent.text in my_dict.keys():\nreturn my_dict[ent.text]\nnlp = spacy.load(path/to/model)\nnlp.add_pipe(my_component, before='ner')\ntext = \"Thrump is President of America and Jokowi is president of Indonesia\"\ndoc = nlp(text)\nSo, if I run:\nfor ent in doc.ents:\nthe model return ent.text : Donald Trumph and Jokowi Widodo", "issue_status": "Closed", "issue_reporting_time": "2019-10-20T05:06:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "233": {"issue_url": "https://github.com/explosion/spaCy/issues/4480", "issue_id": "#4480", "issue_summary": "Jupyter notebook on Anaconda OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.", "issue_description": "toshreyansjain commented on 20 Oct 2019 \u2022\nedited\nspaCy validation on Jupyter\n!python -m spacy validate\n\u2714 Loaded compatibility table\n====================== Installed models (spaCy v2.1.8) ======================\n\u2139 spaCy installation:\n/Users/shreyansjain_2/anaconda3/envs/ENV3/lib/python3.7/site-packages/spacy\nTYPE NAME MODEL VERSION\npackage en-core-web-sm en_core_web_sm 2.1.0 \u2714\npackage en-core-web-lg en_core_web_lg 2.1.0 \u2714\nlink en_core_web_lg en_core_web_lg 2.1.0 \u2714\nlink en_default en_core_web_sm 2.1.0 \u2714\nlink en en_core_web_sm 2.1.0 \u2714\nSpacy works well on terminal :\n(ENV3) Shreyanss-MBP:~ shreyansjain_2$ python -m spacy validate\n\u2714 Loaded compatibility table\n====================== Installed models (spaCy v2.1.8) ======================\n\u2139 spaCy installation:\n/Users/shreyansjain_2/anaconda3/envs/ENV3/lib/python3.7/site-packages/spacy\nTYPE NAME MODEL VERSION\npackage en-core-web-sm en_core_web_sm 2.1.0 \u2714\npackage en-core-web-lg en_core_web_lg 2.1.0 \u2714\nlink en_core_web_lg en_core_web_lg 2.1.0 \u2714\nlink en en_core_web_sm 2.1.0 \u2714\n(ENV3) Shreyanss-MBP:~ shreyansjain_2$ python\nPython 3.7.4 (default, Aug 13 2019, 15:17:50)\n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(\"She ate the pizza\")\nfor token in doc:\n... print(token.text, token.pos_)\n...\nShe PRON\nate VERB\nthe DET\nSPACE\npizza NOUN\nI am not able to fix this on jupyter.\nAlready did following on Jupyter notbook :\n!python -m spacy download en\n!python -m spacy link en_core_web_sm en_default\nHere is the error on jupyter when I run any of following commands :\nimport spacy\nnlp = spacy.load('en')\nOSError Traceback (most recent call last)\nin\n1 #Spacy Statistical model\n2 import spacy\n----> 3 nlp = spacy.load('en')\n~/anaconda3/lib/python3.7/site-packages/spacy/init.py in load(name, **overrides)\n25 if depr_path not in (True, False, None):\n26 deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 27 return util.load_model(name, **overrides)\n28\n29\n~/anaconda3/lib/python3.7/site-packages/spacy/util.py in load_model(name, **overrides)\n169 elif hasattr(name, \"exists\"): # Path or Path-like to model data\n170 return load_model_from_path(name, **overrides)\n--> 171 raise IOError(Errors.E050.format(name=name))\n172\n173\nOSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nor\nnlp = spacy.load('en_core_web_sm')\nOSError Traceback (most recent call last)\nin\n----> 1 nlp = spacy.load('en_core_web_sm')\n~/anaconda3/lib/python3.7/site-packages/spacy/init.py in load(name, **overrides)\n25 if depr_path not in (True, False, None):\n26 deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 27 return util.load_model(name, **overrides)\n28\n29\n~/anaconda3/lib/python3.7/site-packages/spacy/util.py in load_model(name, **overrides)\n169 elif hasattr(name, \"exists\"): # Path or Path-like to model data\n170 return load_model_from_path(name, **overrides)\n--> 171 raise IOError(Errors.E050.format(name=name))\n172\n173\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nPlease suggest a solution.\nHow to reproduce the problem\nHere is the error on jupyter when I run any of following commands :\nimport spacy\nnlp = spacy.load('en')\nor\nnlp = spacy.load('en_core_web_sm')\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nYour Environment\nOperating System: macos : 10.15 (19A602)\nPython Version Used: 3.7.4 (Anaconda installation)\nspaCy Version Used: 2.1.0\nEnvironment Information: macbook pro", "issue_status": "Closed", "issue_reporting_time": "2019-10-19T20:47:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "234": {"issue_url": "https://github.com/explosion/spaCy/issues/4478", "issue_id": "#4478", "issue_summary": "how does training parser for custom semantics works?", "issue_description": "curiousgeek0 commented on 19 Oct 2019\nI am referring to this part of documentation.\nDoes it takes into consideration the meaning of words while labeling dependencies.\nI have multiple entities of same type in a sentence and I want to check their dependencies on some words to distinguish those entities.\nFor ex: Workshop starts at 9 am and ends at 10 pm.\nHere 9 am , 10 pm are time entities.\nHow do I get to recognize 9 am as start_time and 10 pm as end_time.\nHow do I get my model to work even when starts is replaced with starting or strt (misspelled form) or begin.\nHow do i get my model to label correctly even if the positions are reversed, '\n\" Workshop ends at 10 am and starts at 9 am \"\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-19T10:52:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "235": {"issue_url": "https://github.com/explosion/spaCy/issues/4477", "issue_id": "#4477", "issue_summary": "Sentence boundary detection is not trained for Vietnamese Universal Dependencies", "issue_description": "Contributor\ntrungtv commented on 19 Oct 2019 \u2022\nedited\nI have developed a spacy model for vietnamese language (vi_spacy) in which the dependency parser is trained with data in the following format.\nI observed that sentence boundary detection is not available as It is said to be within the dependency parser.\nCould you have any recommendation for my problem?\nHow to reproduce the behaviour\nInstall vi_spacy\nimport spacy\nnlp = spacy.load('vi_spacy_model')\ndoc = nlp('C\u1ed9ng \u0111\u1ed3ng x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. Tr\u1eddi \u0111\u1eb9p qu\u00e1.')\nfor token in doc:\n    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n            token.shape_, token.is_alpha, token.is_stop, token.is_sent_start)\n=== output ===\nC\u1ed9ng_\u0111\u1ed3ng C\u1ed9ng_\u0111\u1ed3ng X N nsubj xxxxxxxxx False False True\nx\u1eed_l\u00fd x\u1eed_l\u00fd X V ROOT xxxxx False True None\nng\u00f4n_ng\u1eef ng\u00f4n_ng\u1eef X N obj xxxxxxxx False False None\nt\u1ef1_nhi\u00ean t\u1ef1_nhi\u00ean X A amod xxxxxxxx False False None\n. . X . punct . False False None\nTr\u1eddi Tr\u1eddi X N nsubj xxxx True False None\n\u0111\u1eb9p \u0111\u1eb9p X A parataxis xxx True False None\nqu\u00e1 qu\u00e1 X R advmod xxx True True None\n. . X . punct . False False None\nYour Environment\nOperating System: Mac OS\nPython Version Used: 3.6\nspaCy Version Used: 2.1.4\nEnvironment Information: Anaconda", "issue_status": "Closed", "issue_reporting_time": "2019-10-19T03:50:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "236": {"issue_url": "https://github.com/explosion/spaCy/issues/4476", "issue_id": "#4476", "issue_summary": "Adding pipe for aliasing name entities.", "issue_description": "Khanifsaleh commented on 19 Oct 2019 \u2022\nedited\nIs there a way to add pipeline that can aliasing entities name?\nEx, text = \"Obama is President of America\".\nLets the model can return ent.text \"Obama\" as \"Barack Obama\"?\nSo, if we run\ndoc = nlp(text)\nfor ent in doc.ents: print(ent.text, ent.label_)\ncan return (Barack Obama, Person)", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T23:58:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "237": {"issue_url": "https://github.com/explosion/spaCy/issues/4475", "issue_id": "#4475", "issue_summary": "Bug in example", "issue_description": "jack-rory-staunton commented on 19 Oct 2019\nThere is a slight error in the logic that causes incorrect deletion of entities which should be kept.\nWhich page or section is this issue related to?\nhttps://spacy.io/usage/rule-based-matching\nin the section Models and Rules,\nin the example code snippet below,\nfrom spacy.tokens import Span\n\n\ndef expand_person_entities(doc):\n    new_ents = []\n    for ent in doc.ents:\n        # Only check for title if it's a person and not the first token\n        if ent.label_ == \"PERSON\" and ent.start != 0:\n            prev_token = doc[ent.start - 1]\n            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n                new_ents.append(new_ent)\n        else:\n            new_ents.append(ent)\n    doc.ents = new_ents\n    return doc\nOne quick way to fix this would be:\nfrom spacy.tokens import Span\n\ndef expand_person_entities(doc):\n    new_ents = []\n    for ent in doc.ents:\n        # Only check for title if it's a person and not the first token\n        if ent.label_ == \"PERSON\" and ent.start != 0:\n            prev_token = doc[ent.start - 1]\n            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n                new_ents.append(new_ent)\n           else:\n              new_ents.append(ent)\n        else:\n            new_ents.append(ent)\n    doc.ents = new_ents\n    return doc", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T20:46:24Z", "fixed_by": "#4510", "pull_request_summary": "Fix logic in rules+model entity example", "pull_request_description": "Collaborator\nadrianeboyd commented on 23 Oct 2019\nDescription\nFixes #4475.\nTypes of change\nDocumentation bugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-23T12:41:22Z", "files_changed": [["2", "website/docs/usage/rule-based-matching.md"]]}, "238": {"issue_url": "https://github.com/explosion/spaCy/issues/4474", "issue_id": "#4474", "issue_summary": "How to link to model in entity linking wiki pretrainer?", "issue_description": "petulla commented on 19 Oct 2019\nWhen running wikipedia_pretrain_kb.py how should the model be linked to? I've tried passing in my parameters like:\npython wikipedia_pretrain_kb.py wd_json='./' wp_xml='./' output_dir=\"./output' model='/Users/spetulla/...lib/python3.7/site-packages/en_core_web_md/'\nThis doesn't work, however. Despite my best efforts, still getting the \"can't find model\" exception thrown.\nI can load the model from spacy with the same path using spacy.load() however.\nMaybe this is the wrong model altogether?\nWhich page or section is this issue related to?\nhttps://github.com/explosion/spaCy/tree/master/bin/wiki_entity_linking", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T19:19:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "239": {"issue_url": "https://github.com/explosion/spaCy/issues/4473", "issue_id": "#4473", "issue_summary": "Phrase Matcher fails on OOV tokens", "issue_description": "tnmcneil commented on 19 Oct 2019\nHow to reproduce the behaviour\nI created a phrasematcher to match titles (eg: queen, manager, mayor, etc.) and it fails when applied to a document containing out of vocabulary tokens.\nThe error it throws is:\nERROR:root:error: \"[E018] Can't retrieve string for hash '4332798303416328849'.\"\nI got around this by creating a \"clean doc\" from the original doc to feed through the phrase matcher like so:\nif any([t.is_oov for t in doc]):\n        clean_toks = [t.text_with_ws if not t.is_oov else 'OOV ' if t.text_with_ws != t.text or re.match('\\s', t.text) else 'OOV' for t in doc]\n        clean_doc = nlp(''.join(clean_toks))\nmatches = phrase_matcher(clean_doc)\nspans = [doc[start:end] for match_id, start, end in matches]\n(I added string 'OOV' to replace the oov tokens because I needed the token indices to match the original doc)\nI am wondering if there is a better way around this or a way for the phrase matcher code to inherently ignore oov tokens rather than trying to process them\nInfo about spaCy\nModels: en\nPython version: 3.5.1\nspaCy version: 2.1.6\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nOperating System: Mac OS", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T18:37:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "240": {"issue_url": "https://github.com/explosion/spaCy/issues/4470", "issue_id": "#4470", "issue_summary": "converting IOB2 file writes invalid characters to stdout", "issue_description": "regel commented on 18 Oct 2019\nHow to reproduce the behaviour\nWriting to stdout (-) with spacy convert produces invalid characters.\nStandard shell redirect cannot be used to create the output file since the JSON file is corrupted. I guess the \"information\" messages should be printed to stderr rather than stdout and this will fix the issue.\n(venv) $ spacy convert -c iob -s -n 10 -l de sentences.iob - > bug.json\n(venv) $ head bug.json \n\u2139 Auto-detected token-per-line NER format\n\u2139 Grouping every 10 sentences into a document.\n\u2139 Segmenting sentences with sentencizer. (Use `-b model` for improved\nparser-based sentence segmentation.)\n[\n  {\n    \"id\":0,\n    \"paragraphs\":[\n      {\n...\nYour Environment\nOperating System: CentOS Linux release 7.7.1908 (Core)\nPython Version Used: python 3.6\nspaCy Version Used: spacy==2.2.1\nEnvironment Information: virtualenv", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T13:12:13Z", "fixed_by": "#4472", "pull_request_summary": "Suppress convert output if writing to stdout", "pull_request_description": "Collaborator\nadrianeboyd commented on 18 Oct 2019\nDescription\nSuppress convert output if writing to stdout.\nFixes #4470.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-18T16:13:00Z", "files_changed": [["4", "spacy/cli/convert.py"], ["4", "spacy/cli/converters/conll_ner2json.py"], ["4", "spacy/cli/converters/iob2json.py"]]}, "241": {"issue_url": "https://github.com/explosion/spaCy/issues/4469", "issue_id": "#4469", "issue_summary": "EntityLinker, pipes.pyx KeyError: '0_12' using sample code given in guides", "issue_description": "curiousgeek0 commented on 18 Oct 2019\nHad to change\nkb = KnowledgeBase(vocab=nlp.vocab)\nto\nkb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=64)\nkb_pretrain.py file, I have made some changes to resolve error that I got after copying and pasting pretrain code from the guide pretrain_kb.py\n#!/usr/bin/env python\n# coding: utf8\n\n\"\"\"Example of defining and (pre)training spaCy's knowledge base,\nwhich is needed to implement entity linking functionality.\n\nFor more details, see the documentation:\n* Knowledge base: https://spacy.io/api/kb\n* Entity Linking: https://spacy.io/usage/linguistic-features#entity-linking\n\nCompatible with: spaCy v2.2\nLast tested with: v2.2\n\"\"\"\nfrom __future__ import unicode_literals, print_function\n\nimport plac\nfrom pathlib import Path\n\nfrom spacy.vocab import Vocab\nimport spacy\nfrom spacy.kb import KnowledgeBase\n\nfrom bin.wiki_entity_linking.train_descriptions import EntityEncoder\n\n\n# Q2146908 (Russ Cochran): American golfer\n# Q7381115 (Russ Cochran): publisher\nENTITIES = {\"Q2146908\": (\"American golfer\", 342), \"Q7381115\": (\"publisher\", 17)}\n\nINPUT_DIM = 300  # dimension of pretrained input vectors\nDESC_WIDTH = 64  # dimension of output entity vectors\n\n\n@plac.annotations(\n    vocab_path=(\"Path to the vocab for the kb\", \"option\", \"v\", Path),\n    model=(\"Model name, should have pretrained word embeddings\", \"option\", \"m\", str),\n    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n)\ndef main(vocab_path=None, model=None, output_dir=None, n_iter=50):\n    \"\"\"Load the model, create the KB and pretrain the entity encodings.\n    Either an nlp model or a vocab is needed to provide access to pretrained word embeddings.\n    If an output_dir is provided, the KB will be stored there in a file 'kb'.\n    When providing an nlp model, the updated vocab will also be written to a directory in the output_dir.\"\"\"\n    if model is None and vocab_path is None:\n        raise ValueError(\"Either the `nlp` model or the `vocab` should be specified.\")\n\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        vocab = Vocab().from_disk(vocab_path)\n        # create blank Language class with specified vocab\n        nlp = spacy.blank(\"en\", vocab=vocab)\n        print(\"Created blank 'en' model with vocab from '%s'\" % vocab_path)\n\n    kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=64)\n\n    # set up the data\n    entity_ids = []\n    descriptions = []\n    freqs = []\n    for key, value in ENTITIES.items():\n        desc, freq = value\n        entity_ids.append(key)\n        descriptions.append(desc)\n        freqs.append(freq)\n\n    # training entity description encodings\n    # this part can easily be replaced with a custom entity encoder\n    encoder = EntityEncoder(\n        nlp=nlp,\n        input_dim=INPUT_DIM,\n        desc_width=DESC_WIDTH,\n    )\n    encoder.train(description_list=descriptions, to_print=True)\n\n    # get the pretrained entity vectors\n    embeddings = encoder.apply_encoder(descriptions)\n\n    # set the entities, can also be done by calling `kb.add_entity` for each entity\n    kb.set_entities(entity_list=entity_ids, freq_list=freqs, vector_list=embeddings)\n\n    # adding aliases, the entities need to be defined in the KB beforehand\n    kb.add_alias(\n        alias=\"Russ Cochran\",\n        entities=[\"Q2146908\", \"Q7381115\"],\n        probabilities=[0.24, 0.7],  # the sum of these probabilities should not exceed 1\n    )\n\n    # test the trained model\n    print()\n    _print_kb(kb)\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        kb_path = str(output_dir / \"kb\")\n        kb.dump(kb_path)\n        print()\n        print(\"Saved KB to\", kb_path)\n\n        # only storing the vocab if we weren't already reading it from file\n        if not vocab_path:\n            vocab_path = output_dir / \"vocab\"\n            kb.vocab.to_disk(vocab_path)\n            print(\"Saved vocab to\", vocab_path)\n\n        print()\n\n        # test the saved model\n        # always reload a knowledge base with the same vocab instance!\n        print(\"Loading vocab from\", vocab_path)\n        print(\"Loading KB from\", kb_path)\n        vocab2 = Vocab().from_disk(vocab_path)\n        kb2 = KnowledgeBase(vocab=vocab2, entity_vector_length=64)\n        kb2.load_bulk(kb_path)\n        _print_kb(kb2)\n        print()\n\n\ndef _print_kb(kb):\n    print(kb.get_size_entities(), \"kb entities:\", kb.get_entity_strings())\n    print(kb.get_size_aliases(), \"kb aliases:\", kb.get_alias_strings())\n\n\nif __name__ == \"__main__\":\n    plac.call(main)\n\n    # Expected output:\n\n    # 2 kb entities: ['Q2146908', 'Q7381115']\n    # 1 kb aliases: ['Russ Cochran']\nrunning kb_train file with\npython kb_train.py testKB/kb testKB/vocab -o trainedKB\nCreated blank 'en' model with vocab from 'testKB/vocab'\nLoaded Knowledge Base from 'testKB/kb'\nTraceback (most recent call last):\n  File \"kb_train.py\", line 155, in <module>\n    plac.call(main)\n  File \"/home/geek/anaconda3/lib/python3.7/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/home/geek/anaconda3/lib/python3.7/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"kb_train.py\", line 115, in main\n    sgd=optimizer,\n  File \"/home/geek/anaconda3/lib/python3.7/site-packages/spacy/language.py\", line 475, in update\n    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n  File \"pipes.pyx\", line 1191, in spacy.pipeline.pipes.EntityLinker.update\nKeyError: '0_12'\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Linux-5.0.0-31-generic-x86_64-with-debian-buster-sid\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T10:15:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "242": {"issue_url": "https://github.com/explosion/spaCy/issues/4465", "issue_id": "#4465", "issue_summary": "spacy CLI train - KeyError [E022] when training an additional NER entity type with --base-model", "issue_description": "qcho commented on 18 Oct 2019 \u2022\nedited\nHi,\nI'm working on training a new entity type. I've been using the python code from the documentation and worked great but now I want to use the CLI because it gives me lot's of new possibilities.\nFirst I'm running debug-data to check if my data is OK and it is.\n{The only issue \u26a0 2655 training examples also in evaluation data is expected because I'm using the same information for testing purposes} :\npython -m spacy debug-data es --pipeline 'ner' -b 'es_core_news_md' corpus/trainings.json corpus/trainings.json\n\n=========================== Data format validation ===========================\n\u2714 Corpus is loadable\n\n=============================== Training stats ===============================\nTraining pipeline: ner\nStarting with base model 'es_core_news_md'\n2660 training docs\n2660 evaluation docs\n\u26a0 2655 training examples also in evaluation data\n\n============================== Vocab & Vectors ==============================\n\u2139 37058 total words in the data (7295 unique)\n\u2139 20000 vectors (533736 unique keys, 50 dimensions)\n\n========================== Named Entity Recognition ==========================\n\u2139 1 new label, 4 existing labels\n0 missing values (tokens with '-' label)\n\u2714 Good amount of examples for all labels\n\u2714 Examples without occurrences available for all labels\n\u2714 No entities consisting of or starting/ending with whitespace\n\n================================== Summary ==================================\n\u2714 4 checks passed\n\u26a0 1 warning\nGreat! as you can see:\n\u2139 1 new label, 4 existing labels\n\u2714 Good amount of examples for all labels\nUsing that information in train results in KeyError: \"[E022] Could not find a transition with the name 'U-DATE' in the NER model.\":\npython -m spacy train es models --pipeline 'ner' --base-model 'es_core_news_md' corpus/trainings.json corpus/trainings.json\nTraining pipeline: ['ner']\nStarting with base model 'es_core_news_md'\nCounting training words (limit=0)\n\nItn  NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n---  ---------  ------  ------  ------  -------  -------\n\u2714 Saved model to output directory\nmodels/model-final\n\u283c Creating best model...\nTraceback (most recent call last):\n  File \"/Users/qcho/.local/share/virtualenvs/trainings-iWUQvJK_/lib/python3.7/site-packages/spacy/cli/train.py\", line 365, in train\n    losses=losses,\n  File \"/Users/qcho/.local/share/virtualenvs/trainings-iWUQvJK_/lib/python3.7/site-packages/spacy/language.py\", line 516, in update\n    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n  File \"nn_parser.pyx\", line 419, in spacy.syntax.nn_parser.Parser.update\n  File \"nn_parser.pyx\", line 521, in spacy.syntax.nn_parser.Parser._init_gold_batch\n  File \"ner.pyx\", line 108, in spacy.syntax.ner.BiluoPushDown.preprocess_gold\n  File \"ner.pyx\", line 166, in spacy.syntax.ner.BiluoPushDown.lookup_transition\nKeyError: \"[E022] Could not find a transition with the name 'U-DATE' in the NER model.\"\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/qcho/.local/share/virtualenvs/trainings-iWUQvJK_/lib/python3.7/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/Users/qcho/.local/share/virtualenvs/trainings-iWUQvJK_/lib/python3.7/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/qcho/.local/share/virtualenvs/trainings-iWUQvJK_/lib/python3.7/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/Users/qcho/.local/share/virtualenvs/trainings-iWUQvJK_/lib/python3.7/site-packages/spacy/cli/train.py\", line 486, in train\n    best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names)\n  File \"/Users/qcho/.local/share/virtualenvs/trainings-iWUQvJK_/lib/python3.7/site-packages/spacy/cli/train.py\", line 554, in _collate_best_model\n    path2str(best_component_src / component), path2str(best_dest / component)\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'str'\nIn a closed Issue #4149 (comment) someone mentioned that he wanted to train a NER model from scratch and @adrianeboyd's suggestion was to use --vectors instead of --base-model. Because when using --base-model the system implies the --base-model already has all the LABEL information.\nI think that assumption IS WRONG or at least there are problems with consistency.\nI expect the train command to infuse the old base-model with my new entities automatically. Or at least have an option to allow that.\nIf for some valid reason you think that's not to be expected, at least the debug-data command should be checking this issue and telling me something will be wrong.\nSo my workaround for the time being is to run -v instead of -b and is at least working. But I would love to use -b instead.\npython -m spacy train es models --pipeline 'ner' -v 'es_core_news_md' corpus/trainings.json corpus/trainings.json -g0\n\nTraining pipeline: ['ner']\nStarting with blank model 'es'\nLoading vector from model 'es_core_news_md'\nCounting training words (limit=0)\n\nItn  NER Loss   NER P   NER R   NER F   Token %  CPU WPS  GPU WPS\n---  ---------  ------  ------  ------  -------  -------  -------\n  1   5057.718  72.773  69.679  71.193  100.000    27431    27264\n  2   2769.560  84.002  82.410  83.198  100.000    27650    26380\n  3   2194.448  88.356  88.120  88.237  100.000    26348    25580\n  4   1759.354  90.661  90.573  90.617  100.000    28263    29517\n  5   1545.240  91.703  92.104  91.903  100.000    24587    23934\n  6   1298.310  92.855  93.780  93.316  100.000    25262    25663\n  7   1192.217  93.854  94.971  94.409  100.000    25783    25885\n  8   1074.796  94.299  95.651  94.970  100.000    23272    25073\n  9    962.257  95.130  96.331  95.727  100.000    24626    25347\n 10    830.074  95.681  96.890  96.282  100.000    23892    27996\n 11    793.578  96.180  97.255  96.714  100.000    22107    23434\n 12    681.465  96.755  97.789  97.269  100.000    22992    25304\n 13    644.355  97.093  98.202  97.645  100.000    25474    23495\n 14    625.250  97.401  98.348  97.872  100.000    25864    25032\n 15    603.803  97.501  98.567  98.031  100.000    21746    25071\n 16    507.126  97.669  98.737  98.200  100.000    25211    25085\n 17    519.947  97.763  98.761  98.260  100.000    24430    24816\n 18    418.207  98.003  98.955  98.477  100.000    21390    22326\n 19    393.087  98.027  98.980  98.501  100.000    24128    23566\n 20    411.454  98.099  99.052  98.574  100.000    24304    24201\n 21    394.488  98.243  99.150  98.694  100.000    25394    29081\n 22    349.580  98.244  99.198  98.719  100.000    26097    26228\n 23    333.129  98.361  99.174  98.766  100.000    24661    23984\n 24    320.657  98.363  99.247  98.803  100.000    26171    22576\n 25    285.909  98.457  99.247  98.851  100.000    23868    23846\n 26    330.953  98.411  99.295  98.851  100.000    21357    28768\n 27    275.963  98.578  99.368  98.972  100.000    27311    24998\n 28    309.472  98.649  99.344  98.995  100.000    25955    25420\n 29    332.513  98.650  99.393  99.020  100.000    25233    26991\n 30    257.369  98.721  99.417  99.068  100.000    25139    26668\n\u2714 Saved model to output directory\nmodels/model-final\n\u2714 Created best model\nmodels/model-best\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.4", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T06:55:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "243": {"issue_url": "https://github.com/explosion/spaCy/issues/4463", "issue_id": "#4463", "issue_summary": "GoldParse.__init__ arguments different from GoldParse.from_annot_tuples", "issue_description": "jessecoleman commented on 18 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nin gold.pyx#L539 it appears as if the static from_annot_tuples function unpacks a tuple of length 7, but in the constructor gold.pyx#L635, self.orig_annot is populated with a length 6 tuple (discarding the cat annotation). This is creating a bug in the scorer.py score function where the gold object is reconstructed with from_annot_tuples function. I discovered this bug when trying to override the cli train function.\nYour Environment\nOperating System: Ubuntu 16.04.6 LTS\nPython Version Used: 3.6.8\nspaCy Version Used: 2.2.1\nEnvironment Information: N/A", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T00:33:29Z", "fixed_by": "#4466", "pull_request_summary": "Add missing cats to gold annot_tuples in Scorer", "pull_request_description": "Collaborator\nadrianeboyd commented on 18 Oct 2019\nDescription\nAdd missing cats in Scorer call to GoldParse.from_annot_tuples() when the doc and gold have differing lengths.\nFixes #4463.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-18T09:00:02Z", "files_changed": [["2", "spacy/scorer.py"]]}, "244": {"issue_url": "https://github.com/explosion/spaCy/issues/4462", "issue_id": "#4462", "issue_summary": "Improve CLI handling for JSONL", "issue_description": "Collaborator\nadrianeboyd commented on 18 Oct 2019\nHow to reproduce the behaviour\nThere are few things to improve for the CLI docs and implementation related to jsonl files:\nthe spacy convert API docs have jsonl listed as the default output, but the default was reverted back to json\nspacy train can only read in tuple-style jsonl, not training-style jsonl\nmore unit tests are needed for the converters\nRelated issues: #3975, #4458.", "issue_status": "Closed", "issue_reporting_time": "2019-10-17T19:21:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "245": {"issue_url": "https://github.com/explosion/spaCy/issues/4461", "issue_id": "#4461", "issue_summary": "Train NamedEntityRecognizer via cli throws OverflowError", "issue_description": "adridjs commented on 17 Oct 2019 \u2022\nedited\nYour Environment\nOperating System: Ubuntu 16.04\nPython Version Used: Python 3.6\nspaCy Version Used: 2.2\nI'm trying to add Hindi Language from scratch in Spacy. I already formatted the train/dev data into the correct json format as shown in https://spacy.io/api/annotation#named-entities\nWhen using this command: python -m spacy train hi /data/disk2/adrinerhindi/models /data/disk2/adrinerhindi/cli_formatted/wiki_train.json /data/disk2/adrinerhindi/cli_formatted/wiki_dev.json -ne 5 -G -p ner\nit throws the following error:\nTraining pipeline: ['ner']\nStarting with blank model 'hi'\nCounting training words (limit=0)\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/adria/envs/hindi-nlp/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/home/adria/envs/hindi-nlp/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/home/adria/envs/hindi-nlp/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/home/adria/envs/hindi-nlp/lib/python3.6/site-packages/spacy/cli/train.py\", line 205, in train\n    corpus = GoldCorpus(train_path, dev_path, limit=n_examples)\n  File \"gold.pyx\", line 130, in spacy.gold.GoldCorpus.__init__\n  File \"gold.pyx\", line 141, in spacy.gold.GoldCorpus.write_msgpack\n  File \"gold.pyx\", line 181, in read_tuples\n  File \"gold.pyx\", line 339, in read_json_file\n  File \"gold.pyx\", line 386, in _json_iterate\nOverflowError: value too large to convert to int\nWhat am I doing wrong?", "issue_status": "Closed", "issue_reporting_time": "2019-10-17T09:22:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "246": {"issue_url": "https://github.com/explosion/spaCy/issues/4460", "issue_id": "#4460", "issue_summary": "Entity Extraction with Knowledge Bases and RDF triples (Wikidata, Freebase)", "issue_description": "AmoghM commented on 17 Oct 2019\nSpacy already has a NER but it doesn't link with KBs due to which the NER looks erroneous. NER are also restricted with generic classes like PERSON, ORG, LOCATION. An extension with wiki data, freebase and DBPedia KBs can help in a natural extension of them.\nCurrent NER result:\nInput: I like watching Harry Potter\nOutput: Harry Potter (PERSON)\nProposed NER result:\nOutput: Harry Potter (book/movie)\nP.S: Examples like I like watching Netlfix find Netflix as PERSON tag. It would have been disambiguated correctly with the use mentioned KBs.\nTherefore, I think Spacy should have this feature to be superior in entity extractions.", "issue_status": "Closed", "issue_reporting_time": "2019-10-17T09:08:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "247": {"issue_url": "https://github.com/explosion/spaCy/issues/4457", "issue_id": "#4457", "issue_summary": "Custom Named Entity Recognizer", "issue_description": "rambabusure commented on 17 Oct 2019 \u2022\nedited\nspaCy version: 2.2.1\nPlatform: Linux-4.14.146-93.123.amzn1.x86_64-x86_64-with-glibc2.9\nPython version: 3.6.5\nModels: en\nWe are trying to build ner model for custom entity recognition . We tried with 10 samples as shown from https://spacy.io/usage/training\nIt is not recognizing any custom entity\ndo we need to provide the correct grammar data for training ?\nwhat is the minimum training data for custom named entity model ?\nDo we need to load 'en' or 'en_core_web_sm'?\nner losses are low if we train 10 samples. If we give same sentence from training data , it is not recognizing?", "issue_status": "Closed", "issue_reporting_time": "2019-10-16T19:57:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "248": {"issue_url": "https://github.com/explosion/spaCy/issues/4453", "issue_id": "#4453", "issue_summary": "Matcher remove not working on master", "issue_description": "Contributor\ndanielkingai2 commented on 16 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nHaving cloned master and building from source, it seems that the matcher remove functionality is not working properly. It does remove the pattern from _patterns but it is still matching on the pattern. I confirmed that this bug does not exist in the released version (2.2.1). I would also suggest that the test found here:\nspaCy/spacy/tests/matcher/test_matcher_logic.py\nLine 148 in 2d249a9\n def test_matcher_remove(en_vocab): \nbe modified to make sure that no matches are found once the pattern has been removed.\nRepro steps (on master)\n>>> import spacy\n>>> nlp = spacy.load('en_core_web_sm')\n>>> from spacy.matcher import Matcher\n>>> text = \"This is some text.\"\n>>> doc = nlp(text)\n>>> matcher = Matcher(nlp.vocab)\n>>> matcher(doc)\n[]\n>>> matcher.add(\"pattern\", None, [{\"ORTH\": \"some\"}])\n>>> matcher(doc)\n[(15329811787164753587, 2, 3)]\n>>> matcher.remove(\"pattern\")\n>>> matcher(doc)\n[(15329811787164753587, 2, 3)]\n>>> matcher._patterns\n{}\nInfo about spaCy\nspaCy version: 2.2.1 (I have actually compiled from source on master branch)\nPlatform: Linux-4.4.0-112-generic-x86_64-with-debian-stretch-sid\nPython version: 3.6.7\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-16T00:18:32Z", "fixed_by": "#4454", "pull_request_summary": "Fix remove pattern from matcher", "pull_request_description": "Member\nsvlandeg commented on 16 Oct 2019\nForgot to rewrite one variable in PR #4420, resulting in the matcher still matching on removed patterns. Also extended the current unit test. Fixes #4453\nTypes of change\nbugfix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-16T11:34:59Z", "files_changed": [["2", "spacy/matcher/matcher.pyx"], ["17", "spacy/tests/matcher/test_matcher_logic.py"]]}, "249": {"issue_url": "https://github.com/explosion/spaCy/issues/4452", "issue_id": "#4452", "issue_summary": "docs_to_json output", "issue_description": "Contributor\nakornilo commented on 15 Oct 2019\nThe output of the spacy.gold.docs_to_json format is inconsistent with the function description. The function says that it RETURNS (list): The data in spaCy's JSON format.. In reality, the function combines all the documents into one big json.\nEither the description or the function itself needs to be adjusted. I think that it should do the later - because that would be in line with the input for the training functions. I am not sure how the ids should be handled.\nWhich page or section is this issue related to?\nhttps://github.com/explosion/spaCy/blob/master/spacy/gold.pyx#L740\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-15T16:53:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "250": {"issue_url": "https://github.com/explosion/spaCy/issues/4451", "issue_id": "#4451", "issue_summary": "Tokenizer: add_special_case not working when special token not separated by whitespace", "issue_description": "nsaef commented on 15 Oct 2019\nHi! I'm trying to add special cases to the tokenizer, but they're only applied if the special tokens are separated by whitespace.\nCode example:\nimport spacy\nfrom spacy.symbols import ORTH, LEMMA, POS, DEP\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nword = \"<TITLE>\"\nrule = [{ORTH: word}]\nnlp.tokenizer.add_special_case(word, rule)\n\nword = \"</TITLE>\"\nrule = [{ORTH: word}]\nnlp.tokenizer.add_special_case(word, rule)\n\nresult = [t.text for t in nlp(\"<TITLE>Hello World</TITLE>\")]\nExpected result: ['<TITLE>', 'Hello', 'World', '</TITLE>']\nActual result: ['<', 'TITLE', '>', 'Hello', 'World</TITLE', '>']\nThis, however, works:\nresult = [t.text for t in nlp(\"<TITLE> Hello World </TITLE>\")]\n>> ['<TITLE>', 'Hello', 'World', '<TITLE>']\nThis bug seems to have come up in the past already (#1061) and crept back in? At least the linked issue is what pointed me in the right direction.\nEnvironment\nspaCy version: 2.2.1\nPlatform: Windows-10-10.0.17763-SP0\nPython version: 3.7.4\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-10-15T15:25:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "251": {"issue_url": "https://github.com/explosion/spaCy/issues/4450", "issue_id": "#4450", "issue_summary": "Training a blank NER with custom beam settings", "issue_description": "davidbren commented on 15 Oct 2019\nI am training a NER model from scratch. I have added the beam search functionality to the query logic. This means that I can get a probability for Test data I send to the model. This is in line with the code in previous posts #881\nHowever it has also been suggested that to get more stable results you should train your model with the beam settings also being set to customized values of your choosing.\nHave you any example of code to do this ?\nI had been looking at doing it using\nos.environ['SPACY_BEAM_WIDTH'] = '16'\nos.environ['SPACY_BEAM_DENSITY'] = '0.0001'\nHowever when I look at the ner\\cfg file it still has the default values\n{\n\"beam_width\":1,\n\"beam_density\":0.0,\n\"beam_update_prob\":1.0,\n....\n}\nThis seems to indicate it hasn't picked up my settings ?\nYour Environment\nOperating System: Unix\nPython Version Used: 2.6.5\nspaCy Version Used: 2.1.8", "issue_status": "Closed", "issue_reporting_time": "2019-10-15T13:35:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "252": {"issue_url": "https://github.com/explosion/spaCy/issues/4448", "issue_id": "#4448", "issue_summary": "doc.char_span() issues", "issue_description": "AnnaAnia commented on 15 Oct 2019\nHi,\nI'm aware that None will be returned if attempting to create a span starting with a space.\nAny other reason why Non would be returned in the case below, please?\n<re.Match object; span=(109, 132), match='James A Cuthbertson Ltd'>\ndoc.char_span(match.span()[0],match.span()[1],label='COMPANY')", "issue_status": "Closed", "issue_reporting_time": "2019-10-15T10:46:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "253": {"issue_url": "https://github.com/explosion/spaCy/issues/4445", "issue_id": "#4445", "issue_summary": "Serialize PhraseMatcher for faster performance", "issue_description": "tnmcneil commented on 15 Oct 2019\nNot sure if this is something that exists already or if this is a request but -\nThe PhraseMatcher is obviously very fast at finding matches in a document which is great. However loading up the matcher takes quite a bit of time. since I am loading up the same patterns every time I am wondering if there is a way to serialize the matcher or save it after loading it once so that reloading is faster. I am creating the phrasematcher from a gazetteer with ~26k lines and it takes ~3 minutes to do so.\nOperating System: Mac OS\nPython Version Used: 3.5.1\nspaCy Version Used: 2.1.6", "issue_status": "Closed", "issue_reporting_time": "2019-10-15T00:14:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "254": {"issue_url": "https://github.com/explosion/spaCy/issues/4443", "issue_id": "#4443", "issue_summary": "Prune vectors error", "issue_description": "Contributor\ndanielkingai2 commented on 15 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nSame issue as #4386, #4388 doesn't quite fix it, as the first element of the tuple here:\nspaCy/spacy/vectors.pyx\nLine 340 in 2d249a9\n sorted_index = xp.arange(scores.shape[0])[:,None],xp.argsort(scores[i:i+batch_size], axis=1)[:,::-1] \nstill has the shape of the full array instead of the batch. Pretty sure it just needs to be turned into this:\nsorted_index = xp.arange(scores.shape[0])[:,None][i:i+batch_size],xp.argsort(scores[i:i+batch_size], axis=1)[:,::-1]\nWill open a PR in case you agree. I would also suggest not sorting if n=1 since its not necessary so I'll also add that in the PR.\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.1 (master)\nPlatform: Linux-4.4.0-112-generic-x86_64-with-debian-stretch-sid\nPython version: 3.6.7", "issue_status": "Closed", "issue_reporting_time": "2019-10-14T23:29:46Z", "fixed_by": "#4446", "pull_request_summary": "Most similar bug", "pull_request_description": "Contributor\ndanielkingai2 commented on 15 Oct 2019 \u2022\nedited\nDescription\nFixes #4443\nIn\nspaCy/spacy/vectors.pyx\nLine 340 in 2d249a9\n sorted_index = xp.arange(scores.shape[0])[:,None],xp.argsort(scores[i:i+batch_size], axis=1)[:,::-1] \n, the first element of the tuple needs to have the correct shape relative to the batch size, so should be replaced by sorted_index = xp.arange(scores.shape[0])[:,None][i:i+batch_size],xp.argsort(scores[i:i+batch_size], axis=1)[:,::-1]. Also there is no need to run the sorting code if only one most similar vector is requested.\nTypes of change\nBug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-16T21:18:55Z", "files_changed": [["15", "spacy/tests/vocab_vectors/test_vectors.py"], ["4", "spacy/vectors.pyx"]]}, "255": {"issue_url": "https://github.com/explosion/spaCy/issues/4442", "issue_id": "#4442", "issue_summary": "EntityRuler does not support LENGTH type patterns in 2.2.1; worked in 2.1.8", "issue_description": "ewaldatsensentia commented on 15 Oct 2019\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.pipeline import EntityRuler\nnlp = spacy.load('en_core_web_md')\nruler = EntityRuler(nlp, validate=True, overwrite_ents=True)\npatterns = [{\"label\":\"MYCODE\", \"pattern\":[{\"LENGTH\":5,\"SHAPE\":\"dddd\"}]}]\nruler.add_patterns(patterns)\nResult\nValueError Traceback (most recent call last)\nin\n4 ruler = EntityRuler(nlp, validate=True, overwrite_ents=True)\n5 patterns = [{\"label\":\"MYCODE\", \"pattern\":[{\"LENGTH\":5,\"SHAPE\":\"dddd\"}]}]\n----> 6 ruler.add_patterns(patterns)\n~\\Anaconda3\\lib\\site-packages\\spacy\\pipeline\\entityruler.py in add_patterns(self, patterns)\n200 raise ValueError(Errors.E097.format(pattern=pattern))\n201 for label, patterns in self.token_patterns.items():\n--> 202 self.matcher.add(label, None, *patterns)\n203 for label, patterns in self.phrase_patterns.items():\n204 self.phrase_matcher.add(label, None, *patterns)\nmatcher.pyx in spacy.matcher.matcher.Matcher.add()\nmatcher.pyx in spacy.matcher.matcher._preprocess_pattern()\nmatcher.pyx in spacy.matcher.matcher._get_attr_values()\nValueError: [E153] The value type int is not supported for token patterns. Please use the option validate=True with Matcher, PhraseMatcher, or EntityRuler for more details.\nYour Environment\nspaCy version: 2.2.1\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.3\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-14T23:15:00Z", "fixed_by": "#4444", "pull_request_summary": "Allow int values in token patterns", "pull_request_description": "Collaborator\nadrianeboyd commented on 15 Oct 2019\nDescription\nAllow int values in token patterns\nAdd missing int value option to top-level pattern validation in Matcher\nAdjust existing tests accordingly\nAdd new test for valid pattern {\"LENGTH\": int}\nFixes #4442 .\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-16T11:40:19Z", "files_changed": [["2", "spacy/matcher/matcher.pyx"], ["7", "spacy/tests/matcher/test_pattern_validation.py"]]}, "256": {"issue_url": "https://github.com/explosion/spaCy/issues/4438", "issue_id": "#4438", "issue_summary": "Wrong Wikidata dump format references in `wikidata_pretrain_kb.py`", "issue_description": "alepiscopo commented on 14 Oct 2019\nHow to reproduce the behaviour\nRun bin/wiki_entity_linking/wikidata_pretrain_kb.py: steps 1-3 runs with no issues;\nStep 4 raises an error while reading the Wikidata dumps file:\nOSError: Not a gzipped file (b'BZ')\nThis is because the docstring at the beginning of the script suggests to download the dumps in bz2 format:\nFor the Wikidata dump: get the latest-all.json.bz2 from https://dumps.wikimedia.org/wikidatawiki/entities/\nHowever, the script expects them to be in gz and uses the gzip library to open them.\nYour Environment\nspaCy version: 2.2.1\nPlatform: Linux-4.15.0-1037-gcp-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-14T05:39:54Z", "fixed_by": "#4375", "pull_request_summary": "KB extensions and better parsing of WikiData", "pull_request_description": "Member\nsvlandeg commented on 4 Oct 2019 \u2022\nedited\nDescription\nFixed a few bugs (overflow error on Windows and wrong location paths for temp files)\nAdded readme for bin folder describing how to run the WD/WP parsing scripts\nDerived prior probability only from non-dev articles (the prior-prob baseline was artificially high because of leaking information to dev eval - rookie mistake :|)\nRemoved Wikidata \"meta\" pages such as disambiguation pages, categories, list pages, ... which were unnecessarily confusing the disambiguation algorithm\nSome code refactoring to isolate IO and definitions of META pages on WP & WD\nExtended KB functionality with some utility functions\nAllow discarding certain NER types during training/evaluation (e.g. dates)\nTake shortcut during predictions when len(candidates) == 1\nHave the EL pipe process full docs instead of single sentences\nDiscard non-sentences such as enumerations (frequently occurs in WP texts)\nFixes #4418\nFixes #4438\nTypes of change\nbug fixes + enhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-14T10:28:54Z", "files_changed": [["34", "bin/wiki_entity_linking/README.md"], ["1", "bin/wiki_entity_linking/__init__.py"], ["118", "bin/wiki_entity_linking/entity_linker_evaluation.py"]]}, "257": {"issue_url": "https://github.com/explosion/spaCy/issues/4436", "issue_id": "#4436", "issue_summary": "spaCy existing model is forgetting the old entity types after training", "issue_description": "georgenv commented on 13 Oct 2019\nHi,\nI'm trying to train an existing NER model for a new set of entities, but after training it is forgetting the old entity types and remembering only the new ones. Anyone could help me, please?\nI have followed the tutorial from spaCy documentation: https://spacy.io/usage/training#ner\nSetup:\nOperating System: Ubuntu 18.04\nPython Version Used: 3.7.4\nspaCy Version Used: 2.2.1\nRegards,\nGeorge", "issue_status": "Closed", "issue_reporting_time": "2019-10-13T16:13:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "258": {"issue_url": "https://github.com/explosion/spaCy/issues/4435", "issue_id": "#4435", "issue_summary": "PhraseMatcher.remove throws an error if there are duplicates in the list of phrases", "issue_description": "slartibaartfast commented on 13 Oct 2019\nHow to reproduce the behaviour\nRemoving phrases from the PhraseMatcher causes an error when one of the phrases exists as part of another phrase. Sometimes, with long lists of phrases, it causes a segmentation fault. This snippet might reproduce the error (it does for me):\nimport spacy\nfrom spacy.matcher import PhraseMatcher, Matcher\n\nnlp = spacy.load(\"en_core_web_lg\")\nphrase_matcher = PhraseMatcher(nlp.vocab)\nphrases = [\"this is a pig\", \"this is a sheep\", \"this is a\", \"this is a dog\"]\npatterns = [nlp(phrase) for phrase in phrases]\nphrase_matcher.add(\"animals\", None, *patterns)\nprint(\"removing animals\")\nphrase_matcher.remove(\"animals\")\nwill throw this error:\nremoving animals\nTraceback (most recent call last):\n  File \"spacytest.py\", line 10, in <module>\n    phrase_matcher.remove(\"animals\")\n  File \"phrasematcher.pyx\", line 136, in spacy.matcher.phrasematcher.PhraseMatcher.remove\n  File \"cymem.pyx\", line 102, in cymem.cymem.Pool.free\nKeyError: 140393018962904\nYour Environment\nspaCy version: 2.2.1\nPlatform: Linux-5.0.0-31-generic-x86_64-with-Ubuntu-19.04-disco\nPython version: 3.7.3\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-13T12:37:16Z", "fixed_by": "#4437", "pull_request_summary": "Fix PhraseMatcher.remove for overlapping patterns", "pull_request_description": "Collaborator\nadrianeboyd commented on 14 Oct 2019 \u2022\nedited\nDescription\nFix PhraseMatcher.remove() for overlapping patterns:\nRemove patterns in order of reverse length\nReset stored path through trie between multiple removes for the same match ID\nAdd test\nFixes #4435.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-14T10:19:52Z", "files_changed": [["4", "spacy/matcher/phrasematcher.pyx"], ["10", "spacy/tests/matcher/test_phrase_matcher.py"]]}, "259": {"issue_url": "https://github.com/explosion/spaCy/issues/4434", "issue_id": "#4434", "issue_summary": "cannot store custom attributes for token information in JSON and use for training", "issue_description": "am15h commented on 12 Oct 2019 \u2022\nedited\nUsing gold.docs_to_json, I am unable to store custom token attributes in JSON\nMy JSON file here, has no mention of any custom attribute\n{\n    \"id\": 0,\n    \"paragraphs\": [\n        {\n            \"raw\": \"Complete the nlp task\",\n            \"sentences\": [\n                {\n                    \"tokens\": [\n                        {\n                            \"id\": 0,\n                            \"orth\": \"Complete\",\n                            \"tag\": \"VB\",\n                            \"head\": 0,\n                            \"dep\": \"ROOT\",\n                            \"ner\": \"O\"\n                        },\n                        {\n                            \"id\": 1,\n                            \"orth\": \"the\",\n                            \"tag\": \"DT\",\n                            \"head\": 2,\n                            \"dep\": \"det\",\n                            \"ner\": \"O\"\n                        },\n                        {\n                            \"id\": 2,\n                            \"orth\": \"nlp\",\n                            \"tag\": \"NN\",\n                            \"head\": 1,\n                            \"dep\": \"compound\",\n                            \"ner\": \"O\"\n                        },\n                        {\n                            \"id\": 3,\n                            \"orth\": \"task\",\n                            \"tag\": \"NN\",\n                            \"head\": -3,\n                            \"dep\": \"dobj\",\n                            \"ner\": \"O\"\n                        }\n                    ],\n                    \"brackets\": []\n                }\n            ]\n        }\n    ]\n}\nHow can I store custom attributes as well? Also can I train the custom attributes with spacy train as well.\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Linux-5.0.0-29-generic-x86_64-with-debian-buster-sid\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-10-12T12:01:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "260": {"issue_url": "https://github.com/explosion/spaCy/issues/4432", "issue_id": "#4432", "issue_summary": "Memory leak with beam_parse method", "issue_description": "DDouteaux commented on 11 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nWe are currently trying to obtain a confidence score for entities found with NER. In order to do so, we implement the solution that was suggested in these places :\nhttps://support.prodi.gy/t/displaying-a-confidence-score-next-to-a-user-defined-entity/403/7\n#881\nThe code is performing well for the functionality, but unfortunately implies a memory leak, as suggested by @usamec in the thread #881 . The first code was produced for SpaCy 2.0 and we first thought that it might have been corrected in newest releases, so we perform tests with version 2.2 but with same results.\nHere is the code used to highlight the memory leak :\n# Imports for SpaCy\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy.pipeline import EntityRecognizer\n\n# Imports for debug\nimport psutil\nimport os\nimport time\n\n# Imports for drawing\nimport matplotlib.pyplot as plt\n\n# Miscellaneous\nfrom collections import defaultdict\n\nnlp = spacy.load(\"fr_core_news_md\")\n\n# Process a \"long\" text\ntexts = [\"De deux choses l'une, ou le puits \u00e9tait vraiment bien profond, ou elle tombait bien doucement ; car elle eut tout le loisir, dans sa chute, de regarder autour d'elle et de se demander avec \u00e9tonnement ce qu'elle allait devenir. D'abord elle regarda dans le fond du trou pour savoir o\u00f9 elle allait ; mais il y faisait bien trop sombre pour y rien voir. Ensuite elle porta les yeux sur les parois du puits, et s'aper\u00e7ut qu'elles \u00e9taient garnies d'armoires et d'\u00e9tag\u00e8res ; \u00e7\u00e0 et l\u00e0, elle vit pendues \u00e0 des clous des cartes g\u00e9ographiques et des images. En passant elle prit sur un rayon un pot de confiture portant cette \u00e9tiquette, \u00ab MARMELADE D'ORANGES. \u00bb Mais, \u00e0 son grand regret, le pot \u00e9tait vide : elle n'osait le laisser tomber dans la crainte de tuer quelqu'un ; aussi s'arrangea-t-elle de mani\u00e8re \u00e0 le d\u00e9poser en passant dans une des armoires. \u00ab Certes, \u00bb dit Alice, \u00ab apr\u00e8s une chute pareille je ne me moquerai pas mal de d\u00e9gringoler l'escalier ! Comme ils vont me trouver brave chez nous ! Je tomberais du haut des toits que je ne ferais pas entendre une plainte. \u00bb (Ce qui \u00e9tait bien probable.) Tombe, tombe, tombe ! \u00ab Cette chute n'en finira donc pas ! Je suis curieuse de savoir combien de milles j'ai d\u00e9j\u00e0 faits, \u00bb dit-elle tout haut. \u00ab Je dois \u00eatre bien pr\u00e8s du centre de la terre. Voyons donc, cela serait \u00e0 quatre mille milles de profondeur, il me semble. \u00bb (Comme vous voyez, Alice avait appris pas mal de choses dans ses le\u00e7ons ; et bien que ce ne f\u00fbt pas l\u00e0 une tr\u00e8s-bonne occasion de faire parade de son savoir, vu qu'il n'y avait point d'auditeur, cependant c'\u00e9tait un bon exercice que de r\u00e9p\u00e9ter sa le\u00e7on.) \u00ab Oui, c'est bien \u00e0 peu pr\u00e8s cela ; mais alors \u00e0 quel degr\u00e9 de latitude ou de longitude est-ce que je me trouve ? \u00bb (Alice n'avait pas la moindre id\u00e9e de ce que voulait dire latitude ou longitude, mais ces grands mots lui paraissaient beaux et sonores.) Bient\u00f4t elle reprit : \u00ab Si j'allais traverser compl\u00e9tement la terre ? Comme \u00e7a serait dr\u00f4le de se trouver au milieu de gens qui marchent la t\u00eate en bas. Aux Antipathies, je crois. \u00bb (Elle n'\u00e9tait pas f\u00e2ch\u00e9e cette fois qu'il n'y e\u00fbt personne l\u00e0 pour l'entendre, car ce mot ne lui faisait pas l'effet d'\u00eatre bien juste.) \u00ab Eh mais, j'aurai \u00e0 leur demander le nom du pays. \u2014 Pardon, Madame, est-ce ici la Nouvelle-Zemble ou l'Australie ? \u00bb \u2014 En m\u00eame temps elle essaya de faire la r\u00e9v\u00e9rence. (Quelle id\u00e9e ! Faire la r\u00e9v\u00e9rence en l'air ! Dites-moi un peu, comment vous y prendriez-vous ?) \u00ab Quelle petite ignorante ! pensera la dame quand je lui ferai cette question. Non, il ne faut pas demander cela ; peut-\u00eatre le verrai-je \u00e9crit quelque part. \u00bb Tombe, tombe, tombe ! \u2014 Donc Alice, faute d'avoir rien de mieux \u00e0 faire, se remit \u00e0 se parler : \u00ab Dinah remarquera mon absence ce soir, bien s\u00fbr. \u00bb (Dinah c'\u00e9tait son chat.) \u00ab Pourvu qu'on n'oublie pas de lui donner sa jatte de lait \u00e0 l'heure du th\u00e9. Dinah, ma minette, que n'es-tu ici avec moi ? Il n'y a pas de souris dans les airs, j'en ai bien peur ; mais tu pourrais attraper une chauve-souris, et cela ressemble beaucoup \u00e0 une souris, tu sais. Mais les chats mangent-ils les chauves-souris ? \u00bb Ici le sommeil commen\u00e7a \u00e0 gagner Alice. Elle r\u00e9p\u00e9tait, \u00e0 moiti\u00e9 endormie : \u00ab Les chats mangent-ils les chauves-souris ? Les chats mangent-ils les chauves-souris ? \u00bb Et quelquefois : \u00ab Les chauves-souris mangent-elles les chats ? \u00bb Car vous comprenez bien que, puisqu'elle ne pouvait r\u00e9pondre ni \u00e0 l'une ni \u00e0 l'autre de ces questions, peu importait la mani\u00e8re de les poser. Elle s'assoupissait et commen\u00e7ait \u00e0 r\u00eaver qu'elle se promenait tenant Dinah par la main, lui disant tr\u00e8s-s\u00e9rieusement : \u00ab Voyons, Dinah, dis-moi la v\u00e9rit\u00e9, as-tu jamais mang\u00e9 des chauves-souris ? \u00bb Quand tout \u00e0 coup, pouf ! la voil\u00e0 \u00e9tendue sur un tas de fagots et de feuilles s\u00e8ches, \u2014 et elle a fini de tomber. Alice ne s'\u00e9tait pas fait le moindre mal. Vite elle se remet sur ses pieds et regarde en l'air ; mais tout est noir l\u00e0-haut. Elle voit devant elle un long passage et le Lapin Blanc qui court \u00e0 toutes jambes. Il n'y a pas un instant \u00e0 perdre ; Alice part comme le vent et arrive tout juste \u00e0 temps pour entendre le Lapin dire, tandis qu'il tourne le coin : \u00ab Par ma moustache et mes oreilles, comme il se fait tard ! \u00bb Elle n'en \u00e9tait plus qu'\u00e0 deux pas : mais le coin tourn\u00e9, le Lapin avait disparu. Elle se trouva alors dans une salle longue et basse, \u00e9clair\u00e9e par une rang\u00e9e de lampes pendues au plafond. Il y avait des portes tout autour de la salle : ces portes \u00e9taient toutes ferm\u00e9es, et, apr\u00e8s avoir vainement tent\u00e9 d'ouvrir celles du c\u00f4t\u00e9 droit, puis celles du c\u00f4t\u00e9 gauche, Alice se promena tristement au beau milieu de cette salle, se demandant comment elle en sortirait. Tout \u00e0 coup elle rencontra sur son passage une petite table \u00e0 trois pieds, en verre massif, et rien dessus qu'une toute petite clef d'or. Alice pensa aussit\u00f4t que ce pouvait \u00eatre celle d'une des portes ; mais h\u00e9las ! soit que les serrures fussent trop grandes, soit que la clef f\u00fbt trop petite, elle ne put toujours en ouvrir aucune. Cependant, ayant fait un second tour, elle aper\u00e7ut un rideau plac\u00e9 tr\u00e8s-bas et qu'elle n'avait pas vu d'abord ; par derri\u00e8re se trouvait encore une petite porte \u00e0 peu pr\u00e8s quinze pouces de haut ; elle essaya la petite clef d'or \u00e0 la serrure, et, \u00e0 sa grande joie, il se trouva qu'elle y allait \u00e0 merveille. Alice ouvrit la porte, et vit qu'elle conduisait dans un \u00e9troit passage \u00e0 peine plus large qu'un trou \u00e0 rat. Elle s'agenouilla, et, jetant les yeux le long du passage, d\u00e9couvrit le plus ravissant jardin du monde. Oh ! Qu'il lui tardait de sortir de cette salle t\u00e9n\u00e9breuse et d'errer au milieu de ces carr\u00e9s de fleurs brillantes, de ces fra\u00eeches fontaines ! Mais sa t\u00eate ne pouvait m\u00eame pas passer par la porte. \u00ab Et quand m\u00eame ma t\u00eate y passerait, \u00bb pensait Alice, \u00ab \u00e0 quoi cela servirait-il sans mes \u00e9paules ? Oh ! que je voudrais donc avoir la facult\u00e9 de me fermer comme un t\u00e9lescope ! \u00c7a se pourrait peut-\u00eatre, si je savais comment m'y prendre. \u00bb Il lui \u00e9tait d\u00e9j\u00e0 arriv\u00e9 tant de choses extraordinaires, qu'Alice commen\u00e7ait \u00e0 croire qu'il n'y en avait gu\u00e8re d'impossibles. Comme cela n'avan\u00e7ait \u00e0 rien de passer son temps \u00e0 attendre \u00e0 la petite porte, elle retourna vers la table, esp\u00e9rant presque y trouver une autre clef, ou tout au moins quelque grimoire donnant les r\u00e8gles \u00e0 suivre pour se fermer comme un t\u00e9lescope. Cette fois elle trouva sur la table une petite bouteille (qui certes n'\u00e9tait pas l\u00e0 tout \u00e0 l'heure). Au cou de cette petite bouteille \u00e9tait attach\u00e9e une \u00e9tiquette en papier, avec ces mots \u00ab BUVEZ-MOI \u00bb admirablement imprim\u00e9s en grosses lettres.\"]\n\n# Beam configuration\nbeam_width = 16\nbeam_density = 0.0001\n\n# For plots\nx = []\ny = []\n\n# Treat the document multiple times\nfor i in range(0, 30):\n    x.append(i)\n    process = psutil.Process(os.getpid())\n    y.append(process.memory_info().rss / 1000000)\n\n    with nlp.disable_pipes('ner'):\n        docs = list(nlp.pipe(texts))\n    beams = nlp.entity.beam_parse(docs, beam_width=beam_width, beam_density=beam_density)\n\n    for doc, beam in zip(docs, beams):\n        entity_scores = defaultdict(float)\n        for score, ents in nlp.entity.moves.get_beam_parses(beam):\n            for start, end, label in ents:\n                entity_scores[(start, end, label)] += score\n\n# Give time for gc to do its work if needed\ntime.sleep(30)\nx.append(x[-1]+1)\nprocess = psutil.Process(os.getpid())\ny.append(process.memory_info().rss / 1000000)\n\n# Display memory usage evolution\nplt.plot(x, y)\nplt.show()\nThe result is a linear curve with a 250 Mb increase of memory usage that is never released (see attachment). Tested with thousands of documents, we eventually crashed our server.\nAnother question comes with this bug :\nIs there another way of obtaining confidence score for NER entities ?\nYour Environment\nOperating System: Windows-10-10.0.17134-SP0 and CentOS 7 (when packaged)\nPython Version Used: 3.6.5\nspaCy Version Used: 2.2.1 and 2.0.18\nEnvironment Information: Windows environment is a dev environment. CentOS env is for deployment after we packaged the project. The first crash was produced on CentOS, and was reproduced on dev env after. The curve given above comes from a Windows environment.\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-11T12:29:04Z", "fixed_by": "#4686", "pull_request_summary": "Add destructors for states in TransitionSystem", "pull_request_description": "Collaborator\nadrianeboyd commented on 21 Nov 2019 \u2022\nedited\nDescription\nAdd destructors for states in TransitionSystem to fix memory in beam_parse().\nNeeds to be coordinated with the parallel modifications to thinc, see explosion/thinc#123.\nFixes #4432.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-10T12:23:28Z", "files_changed": [["3", "spacy/syntax/_beam_utils.pyx"], ["6", "spacy/syntax/arc_eager.pyx"], ["3", "spacy/syntax/transition_system.pxd"], ["9", "spacy/syntax/transition_system.pyx"]]}, "261": {"issue_url": "https://github.com/explosion/spaCy/issues/4431", "issue_id": "#4431", "issue_summary": "I think this is a better way of storing the data, instead of the current methods", "issue_description": "PROgramJEDI commented on 11 Oct 2019\nI don't really familiar with the spaCy library, but I don't think the way spaCy manipulate the 'text' is correct. Those are supportive reasons:\nThe 're' (regular expression) module\naldo the 're' module offers the best ways to manipulate text in High-Performance speed, it's written that spaCy don't take advantage of it.\nPerformance\nonly by writing this code takes spaCy 1sec in 'intel i5 k8' computer, which is bad:\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp('this is my text')\nAfter that, you need to iterate in 'doc' to get the tokens or token attributes.\nI think the Dictionary is a much better way to accomplish that.\nSomething like this:\nimport re\nfrom functools import wraps\n\n\ndef load(model):\n # process the model...\n @wraps(model)\n def wrapper(text):\n  # wrapper: process the 'text' with the given 'model'...\n\n  # initialize before loop (assign new memory to the objects)\n  ptt = None\n  mchs = None\n  position = None\n  # FIX: fix the split method to split it to only letters, dots, and columns\n  #  with no new lines ('\\n') or tabs...\n  for word in text.split(' '):\n   ptt = re.compile(word)\n   # find the mchs which are the positions of the given pattern\n   mchs = re.finditer(ptt, text)\n\n   for mch in mchs:\n    # the position of the word\n    position = (mch.start(), mch.end())\n\n    yield {\n     (position): \n      {   'name': text[mch.start():mch.end()],\n       'head': '',\n       'pos': '',\n      }\n     }\n return wrapper\n\n\nnlp = load('en')\ndoc = nlp('this is my text')\n\nfor obj in doc:\n print(obj)\nNOT IN THE EXAMPLE (extra functionality):\nAfter that, any time user requests to access an attribute of a token, the program would add it to the dictionary in the token index inside the head filed.\n>>> Please tell me if it's a good idea so I can pull new commit to the library...\n2", "issue_status": "Closed", "issue_reporting_time": "2019-10-11T08:02:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "262": {"issue_url": "https://github.com/explosion/spaCy/issues/4430", "issue_id": "#4430", "issue_summary": "Gold.align bug a2b_multi <--> b2a_multi", "issue_description": "ales004 commented on 11 Oct 2019\nHow to reproduce the behaviour\nRun this:\n    toka = ['a', 'terrific', 'log', '-', 'jam', 'of']\n    tokb = ['a', 'terrific', 'log-', 'jam', 'of']\n    _, a2b, b2a, b2a_multi, a2b_multi = align(toka, tokb)\n    print(f\"a2b: {a2b}\")\n    print(f\"b2a: {b2a}\")\n    print(f\"a2b_multi: {a2b_multi}\")\n    print(f\"b2a_multi: {b2a_multi}\")\noutput:\na2b: [ 0  1 -1 -1  3  4]\nb2a: [0 1 3 4 5]\na2b_multi: {}\nb2a_multi: {2: 2, 3: 2}\nIn my opinion a2b_multi and b2a_multi are interchanged. According to the documentation:\na2b_multi: A dictionary mapping indices in tokens_a to indices in tokens_b, where multiple tokens of tokens_a align to the same token of tokens_b.\nb2a_multi: A dictionary mapping indices in tokens_b to indices in tokens_a, where multiple tokens of tokens_b align to the same token of tokens_a.\nSo I would have expected a2b_multi={2: 2, 3: 2} and b2a_multi={} because toka[2] + toka[3] == tokb[2] instead I get the opposite.\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Ubuntu 18.04\nPython version: 3.6.8", "issue_status": "Closed", "issue_reporting_time": "2019-10-11T07:01:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "263": {"issue_url": "https://github.com/explosion/spaCy/issues/4428", "issue_id": "#4428", "issue_summary": "Adding new lemma rule to existing language", "issue_description": "pdimiel commented on 11 Oct 2019\nHi everyone.\nI am trying to add new lemma rules in existing languages and would like to know if this is possible.\nI have tried as an example to add the rule form nouns\nwindowz -> window\nbedz -> bed\nI looked in to how lemma_rules are declared in the language lemmatizer files and tried the following:\nnlp = spacy.load(model)\nnlp.vocab.morphology.lemmatizer.rules[\"noun\"].append([\"z\", \"\"])\nRunning some tests I noticed that when the model is an english model, the rule does not work for cases where the actual lemma exists in the language vocabulary like:\n\"I have broken some windowz\"\nwindowz tag=NN, lemma=windowz\nbut works for cases where the resulting lemma does not exists in the language vocabulary like:\n\"I have broken some mmmz\"\nmmmz tag=NNS, lemma=mmm\nAnd switching to the Greek model but using an english word works (as long as the pos tag is set correctly):\n\"\u0388\u03c3\u03c0\u03b1\u03c3\u03b1 \u03ba\u03ac\u03c0\u03bf\u03b9\u03b1 windowz\"\nwindowz tag=NOUN, lemma=window\nCould you tell me if I am using the correct approach to add a lemmatizer rule?\nIf yes, why is the rule activated only in some cases?", "issue_status": "Closed", "issue_reporting_time": "2019-10-10T20:12:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "264": {"issue_url": "https://github.com/explosion/spaCy/issues/4427", "issue_id": "#4427", "issue_summary": "ValueError: spacy.syntax.nn_parser.Parser size changed, may indicate binary incompatibility. Expected 72 from C header, got 64 from PyObject", "issue_description": "LizMcQuillan commented on 10 Oct 2019\nI've seen this issue pop up a few other times, but most people report solving with a quick uninstall and reinstall. I've tried that several times to no avail.\nMy code:\nimport spacy\n!python -m spacy download en\nfrom spacy.lang.en import English\nparser = English()\nimport en_core_web_sm\nnlp = spacy.load('en_core_web_sm')\nFull traceback:\nRequirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /Users/lmcquillan/anaconda3/lib/python3.7/site-packages (2.0.0)\n\n    Linking successful\n    /Users/lmcquillan/anaconda3/lib/python3.7/site-packages/en_core_web_sm\n    -->\n    /Users/lmcquillan/anaconda3/lib/python3.7/site-packages/spacy/data/en\n\n    You can now load the model via spacy.load('en')\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-8-9016a6b1ed29> in <module>\n      2 \n      3 \n----> 4 from spacy.lang.en import English\n      5 parser = English()\n      6 import en_core_web_sm\n\n~/anaconda3/lib/python3.7/site-packages/spacy/lang/en/__init__.py in <module>\n     13 from ..tokenizer_exceptions import BASE_EXCEPTIONS\n     14 from ..norm_exceptions import BASE_NORMS\n---> 15 from ...language import Language\n     16 from ...attrs import LANG, NORM\n     17 from ...util import update_exc, add_lookups\n\n~/anaconda3/lib/python3.7/site-packages/spacy/language.py in <module>\n     16 from .vocab import Vocab\n     17 from .lemmatizer import Lemmatizer\n---> 18 from .pipeline import DependencyParser, Tensorizer, Tagger, EntityRecognizer\n     19 from .pipeline import SimilarityHook, TextCategorizer, SentenceSegmenter\n     20 from .pipeline import merge_noun_chunks, merge_entities\n\n~/anaconda3/lib/python3.7/site-packages/spacy/pipeline/__init__.py in <module>\n      2 from __future__ import unicode_literals\n      3 \n----> 4 from .pipes import Tagger, DependencyParser, EntityRecognizer\n      5 from .pipes import TextCategorizer, Tensorizer, Pipe\n      6 from .entityruler import EntityRuler\n\npipes.pyx in init spacy.pipeline.pipes()\n\nValueError: spacy.syntax.nn_parser.Parser size changed, may indicate binary incompatibility. Expected 72 from C header, got 64 from PyObject\nMy Environment:\nspaCy version: 2.0.16\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.3\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-10-10T15:38:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "265": {"issue_url": "https://github.com/explosion/spaCy/issues/4426", "issue_id": "#4426", "issue_summary": "Matcher doesn't match when lemmas are set by lemma_lookup in spaCy 2.2.1", "issue_description": "zrlaida commented on 10 Oct 2019\nHow to reproduce the behaviour\nIn spacy==2.2.1, the following code snippet doesn't match. Whereas, in spacy==2.1.8, it matches correctly.\nimport spacy\n\nnlp = spacy.load(\"de_core_news_sm\")\n\nnlp.Defaults.lemma_lookup[\"therapie\"] = \"Therapie\"\nnlp.Defaults.lemma_lookup[\"versuch\"] = \"Versuch\"\n\ndoc = nlp(\"ein therapie versuch\")\n\nmatcher = spacy.matcher.Matcher(nlp.vocab)\n\npattern = [{'LEMMA': 'Therapie'}, {'LEMMA': 'Versuch'}]\n\nmatcher.add(\"test_id\", None, pattern)\n\nfor match in matcher(doc):\n    print(\"Match: \", match, nlp.vocab.strings[match[0]])\nYour Environment\nOperating System: macOS 10.14.6\nPython Version Used: 3.7.0\nspaCy Version Used: 2.2.1\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-10T13:56:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "266": {"issue_url": "https://github.com/explosion/spaCy/issues/4419", "issue_id": "#4419", "issue_summary": "Data preparation - how should we handle duplicate sentences?", "issue_description": "erotavlas commented on 9 Oct 2019 \u2022\nedited\nHello,\nAlthough I can run an experiment to see which way performs better, I wanted to hear from the developers regarding if there is a recommended approach to deal with duplicate sentences in the training data.\nI'm doing NER on medical reports. After sentencizing a report, its possible that eventually I'll encounter numerous sentences that are identical to each other with the exact same string. Even though all reports in my training set are different, there are sections of text which are identical\nCurrently I'm checking for these duplicate sentences (the ones without any entities) before adding them to my training set and keeping only one instance, but I wanted to know if that's ok or should I just use everything even if the training set would contain sentences that are duplicates.?", "issue_status": "Closed", "issue_reporting_time": "2019-10-09T18:11:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "267": {"issue_url": "https://github.com/explosion/spaCy/issues/4418", "issue_id": "#4418", "issue_summary": "Training entity linking classifier", "issue_description": "petulla commented on 9 Oct 2019\nI'm trying to follow the entity linking documentation but I don't really follow.\nIt seems like the documentation on an entity linker shows how to do custom linking. But what if you just want to use the out of the box Wikipedia linking? How do I set that up? Seems like it's a matter of running the scripts but I'm not sure which ones or in which order.\nWhich page or section is this issue related to?\nhttps://spacy.io/usage/linguistic-features#entity-linking", "issue_status": "Closed", "issue_reporting_time": "2019-10-09T16:02:53Z", "fixed_by": "#4375", "pull_request_summary": "KB extensions and better parsing of WikiData", "pull_request_description": "Member\nsvlandeg commented on 4 Oct 2019 \u2022\nedited\nDescription\nFixed a few bugs (overflow error on Windows and wrong location paths for temp files)\nAdded readme for bin folder describing how to run the WD/WP parsing scripts\nDerived prior probability only from non-dev articles (the prior-prob baseline was artificially high because of leaking information to dev eval - rookie mistake :|)\nRemoved Wikidata \"meta\" pages such as disambiguation pages, categories, list pages, ... which were unnecessarily confusing the disambiguation algorithm\nSome code refactoring to isolate IO and definitions of META pages on WP & WD\nExtended KB functionality with some utility functions\nAllow discarding certain NER types during training/evaluation (e.g. dates)\nTake shortcut during predictions when len(candidates) == 1\nHave the EL pipe process full docs instead of single sentences\nDiscard non-sentences such as enumerations (frequently occurs in WP texts)\nFixes #4418\nFixes #4438\nTypes of change\nbug fixes + enhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-14T10:28:54Z", "files_changed": [["34", "bin/wiki_entity_linking/README.md"], ["1", "bin/wiki_entity_linking/__init__.py"], ["118", "bin/wiki_entity_linking/entity_linker_evaluation.py"], ["82", "bin/wiki_entity_linking/kb_creator.py"], ["4", "bin/wiki_entity_linking/train_descriptions.py"], ["395", "bin/wiki_entity_linking/training_set_creator.py"], ["127", "bin/wiki_entity_linking/wiki_io.py"], ["128", "bin/wiki_entity_linking/wiki_namespaces.py"], ["142", "bin/wiki_entity_linking/wikidata_pretrain_kb.py"], ["63", "bin/wiki_entity_linking/wikidata_processor.py"], ["62", "bin/wiki_entity_linking/wikidata_train_entity_linker.py"], ["570", "bin/wiki_entity_linking/wikipedia_processor.py"], ["0", "examples/pipeline/dummy_entity_linking.py"], ["0", "examples/pipeline/wikidata_entity_linking.py"], ["2", "spacy/cli/pretrain.py"], ["11", "spacy/errors.py"], ["67", "spacy/kb.pyx"], ["4", "spacy/language.py"], ["115", "spacy/pipeline/pipes.pyx"], ["47", "spacy/tests/pipeline/test_entity_linker.py"], ["2", "spacy/tests/regression/test_issue1-1000.py"]]}, "268": {"issue_url": "https://github.com/explosion/spaCy/issues/4413", "issue_id": "#4413", "issue_summary": "Spaces from Doc", "issue_description": "AnnaAnia commented on 9 Oct 2019\nIs it possible to retrieve a list of spaces from Doc in the same way they are used to construct Doc\ndoc = Doc(nlp.vocab, words=words, spaces=spaces)\nThank you", "issue_status": "Closed", "issue_reporting_time": "2019-10-09T11:59:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "269": {"issue_url": "https://github.com/explosion/spaCy/issues/4408", "issue_id": "#4408", "issue_summary": "Sentence splitter not working for Greek", "issue_description": "pdimiel commented on 9 Oct 2019\nHello everyone.\nI have been using Spacy for a long time without any problems processing English texts.\nThe last coupe of weeks I have tried the Greek model and I noticed that it does not split the sentences but instead considers the full text to be one sentence.\nIf I use the English models on the same text, it is split to sentences.\nIt does not seem to be a setup problem as the same behaviour is noticed on the Displacy website\nHow to reproduce the behaviour\nDisplacy Greek Model:\nhttps://explosion.ai/demos/displacy?text=%CE%91%CF%85%CF%84%CE%AE%20%CE%B5%CE%AF%CE%BD%CE%B1%CE%B9%20%CE%BC%CE%B9%CE%B1%20%CF%80%CF%81%CF%8C%CF%84%CE%B1%CF%83%CE%B7.%20%CE%91%CF%85%CF%84%CE%AE%20%CE%B5%CE%AF%CE%BD%CE%B1%CE%B9%20%CE%BC%CE%B9%CE%B1%20%CE%AC%CE%BB%CE%BB%CE%B7%20%CF%80%CF%81%CF%8C%CF%84%CE%B1%CF%83%CE%B7.&model=el_core_news_sm&cpu=0&cph=0\nDisplacy English Model:\nhttps://explosion.ai/demos/displacy?text=%CE%91%CF%85%CF%84%CE%AE%20%CE%B5%CE%AF%CE%BD%CE%B1%CE%B9%20%CE%BC%CE%B9%CE%B1%20%CF%80%CF%81%CF%8C%CF%84%CE%B1%CF%83%CE%B7.%20%CE%91%CF%85%CF%84%CE%AE%20%CE%B5%CE%AF%CE%BD%CE%B1%CE%B9%20%CE%BC%CE%B9%CE%B1%20%CE%AC%CE%BB%CE%BB%CE%B7%20%CF%80%CF%81%CF%8C%CF%84%CE%B1%CF%83%CE%B7.&model=en_core_web_sm&cpu=0&cph=0\nKeep up the great work.\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-09T07:18:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "270": {"issue_url": "https://github.com/explosion/spaCy/issues/4407", "issue_id": "#4407", "issue_summary": "Which one of TextCategorizer architectures use our pre-trained word vectors?", "issue_description": "zfallahnejad commented on 9 Oct 2019\nI want to train a TextCategorizer model. The documentation says that there are three different architectures for TextCategorizer model. Which one of these TextCategorizer architectures use our pre-trained word vectors?\n(I have trained my own word2vec model in gensim and I load it in spacy.)", "issue_status": "Closed", "issue_reporting_time": "2019-10-09T06:50:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "271": {"issue_url": "https://github.com/explosion/spaCy/issues/4406", "issue_id": "#4406", "issue_summary": "Misspelling on Lemmatizer Example", "issue_description": "Contributor\nghollah commented on 9 Oct 2019\nWhich page or section is this issue related to?\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\nlookups = Loookups()\nlookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\nlemmatizer = Lemmatizer(lookups)\nlemmas = lemmatizer(\"ducks\", \"NOUN\")\nassert lemmas == [\"duck\"]\nSuggestion:\nRemove extra o in the lookups = Loookups()", "issue_status": "Closed", "issue_reporting_time": "2019-10-09T04:37:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "272": {"issue_url": "https://github.com/explosion/spaCy/issues/4405", "issue_id": "#4405", "issue_summary": "sentence segmentation at '/'", "issue_description": "frank-lin-liu commented on 9 Oct 2019\nHow to reproduce the behaviour\ncode example:\nfor sent in doc.sents:\nprint(sent)\nSentences are split at '/' rather than '.' by default if '/' is included in a sentence.\nYour Environment\nOperating System: Windows-7-6.1.7601-SP1\nPython Version Used: 3.68\nspaCy Version Used: 2.2.1\n*models: en\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-09T04:16:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "273": {"issue_url": "https://github.com/explosion/spaCy/issues/4404", "issue_id": "#4404", "issue_summary": "AttributeError: module 'thinc_gpu_ops' has no attribute 'hash' when running nlp on gpu", "issue_description": "tylerjthomas9 commented on 9 Oct 2019\ncode:\nimport spacy\nspacy.require_gpu() #returns True\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp('test sentence')\nerror:\nAttributeError Traceback (most recent call last)\nin\n2 spacy.require_gpu()\n3 nlp = spacy.load('en_core_web_sm')\n----> 4 doc = nlp('test sentence')\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/spacy/language.py in call(self, text, disable, component_cfg)\n436 if not hasattr(proc, \"call\"):\n437 raise ValueError(Errors.E003.format(component=type(proc), name=name))\n--> 438 doc = proc(doc, **component_cfg.get(name, {}))\n439 if doc is None:\n440 raise ValueError(Errors.E005.format(name=name))\npipes.pyx in spacy.pipeline.pipes.Tagger.call()\npipes.pyx in spacy.pipeline.pipes.Tagger.predict()\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/model.py in call(self, x)\n167 Must match expected shape\n168 \"\"\"\n--> 169 return self.predict(x)\n170\n171 def pipe(self, stream, batch_size=128):\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py in predict(self, X)\n38 def predict(self, X):\n39 for layer in self._layers:\n---> 40 X = layer(X)\n41 return X\n42\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/model.py in call(self, x)\n167 Must match expected shape\n168 \"\"\"\n--> 169 return self.predict(x)\n170\n171 def pipe(self, stream, batch_size=128):\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in predict(seqs_in)\n308 def predict(seqs_in):\n309 lengths = layer.ops.asarray([len(seq) for seq in seqs_in])\n--> 310 X = layer(layer.ops.flatten(seqs_in, pad=pad))\n311 return layer.ops.unflatten(X, lengths, pad=pad)\n312\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/model.py in call(self, x)\n167 Must match expected shape\n168 \"\"\"\n--> 169 return self.predict(x)\n170\n171 def pipe(self, stream, batch_size=128):\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py in predict(self, X)\n38 def predict(self, X):\n39 for layer in self._layers:\n---> 40 X = layer(X)\n41 return X\n42\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/model.py in call(self, x)\n167 Must match expected shape\n168 \"\"\"\n--> 169 return self.predict(x)\n170\n171 def pipe(self, stream, batch_size=128):\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/model.py in predict(self, X)\n131\n132 def predict(self, X):\n--> 133 y, _ = self.begin_update(X, drop=None)\n134 return y\n135\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in uniqued_fwd(X, drop)\n377 )\n378 X_uniq = layer.ops.xp.ascontiguousarray(X[ind])\n--> 379 Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n380 Y = Y_uniq[inv].reshape((X.shape[0],) + Y_uniq.shape[1:])\n381\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py in begin_update(self, X, drop)\n44 callbacks = []\n45 for layer in self._layers:\n---> 46 X, inc_layer_grad = layer.begin_update(X, drop=drop)\n47 callbacks.append(inc_layer_grad)\n48\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in begin_update(X, *a, **k)\n161 def begin_update(X, *a, **k):\n162 forward, backward = split_backward(layers)\n--> 163 values = [fwd(X, *a, **k) for fwd in forward]\n164\n165 output = ops.xp.hstack(values)\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in (.0)\n161 def begin_update(X, *a, **k):\n162 forward, backward = split_backward(layers)\n--> 163 values = [fwd(X, *a, **k) for fwd in forward]\n164\n165 output = ops.xp.hstack(values)\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in wrap(*args, **kwargs)\n254\n255 def wrap(*args, **kwargs):\n--> 256 output = func(*args, **kwargs)\n257 if splitter is None:\n258 to_keep, to_sink = output\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in begin_update(X, *a, **k)\n161 def begin_update(X, *a, **k):\n162 forward, backward = split_backward(layers)\n--> 163 values = [fwd(X, *a, **k) for fwd in forward]\n164\n165 output = ops.xp.hstack(values)\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in (.0)\n161 def begin_update(X, *a, **k):\n162 forward, backward = split_backward(layers)\n--> 163 values = [fwd(X, *a, **k) for fwd in forward]\n164\n165 output = ops.xp.hstack(values)\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in wrap(*args, **kwargs)\n254\n255 def wrap(*args, **kwargs):\n--> 256 output = func(*args, **kwargs)\n257 if splitter is None:\n258 to_keep, to_sink = output\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in begin_update(X, *a, **k)\n161 def begin_update(X, *a, **k):\n162 forward, backward = split_backward(layers)\n--> 163 values = [fwd(X, *a, **k) for fwd in forward]\n164\n165 output = ops.xp.hstack(values)\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in (.0)\n161 def begin_update(X, *a, **k):\n162 forward, backward = split_backward(layers)\n--> 163 values = [fwd(X, *a, **k) for fwd in forward]\n164\n165 output = ops.xp.hstack(values)\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/api.py in wrap(*args, **kwargs)\n254\n255 def wrap(*args, **kwargs):\n--> 256 output = func(*args, **kwargs)\n257 if splitter is None:\n258 to_keep, to_sink = output\n~/anaconda3/envs/tyler_spacy/lib/python3.7/site-packages/thinc/neural/_classes/hash_embed.py in begin_update(self, ids, drop)\n57 if ids.ndim >= 2:\n58 ids = self.ops.xp.ascontiguousarray(ids[:, self.column], dtype=\"uint64\")\n---> 59 keys = self.ops.hash(ids, self.seed) % self.nV\n60 vectors = self.vectors[keys].sum(axis=1)\n61 mask = self.ops.get_dropout_mask((vectors.shape[1],), drop)\nops.pyx in thinc.neural.ops.CupyOps.hash()\nAttributeError: module 'thinc_gpu_ops' has no attribute 'hash'", "issue_status": "Closed", "issue_reporting_time": "2019-10-08T19:53:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "274": {"issue_url": "https://github.com/explosion/spaCy/issues/4402", "issue_id": "#4402", "issue_summary": "An error while training the dependency parser", "issue_description": "ryszardtuora commented on 8 Oct 2019\nEver since the 2.2 update I've been having trouble training parser models. Training both taggers and NER is fine, but with parsers I get an error:\nTraining pipeline: ['parser']\nStarting with blank model 'pl'\nLoading vector from model 'vocab_kgr_100_handpruned22'\nCounting training words (limit=0)\n\nItn  Dep Loss    UAS     LAS    Token %  CPU WPS\n---  ---------  ------  ------  -------  -------\n\u2714 Saved model to output directory                                                                                                             \nbase_parser_22/model-final\n\u2834 Creating best model...\nTraceback (most recent call last):\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 356, in train\n    for batch in util.minibatch_by_words(train_docs, size=batch_sizes):\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/spacy/util.py\", line 569, in minibatch_by_words\n    doc, gold = next(items)\n  File \"gold.pyx\", line 222, in train_docs\n  File \"gold.pyx\", line 240, in iter_gold_docs\n  File \"gold.pyx\", line 258, in spacy.gold.GoldCorpus._make_docs\nValueError: need more than 0 values to unpack\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 486, in train\n    best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names)\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 554, in _collate_best_model\n    path2str(best_component_src / component), path2str(best_dest / component)\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'str'\nHow to reproduce the behaviour\nI've converted the data with spaCy 2.2 converter. I run the train command with this input:\nnice -n 19 python3 -m spacy train pl base_parser_22 LFG22/pl_lfg-ud-train.json LFG22/pl_lfg-ud-dev.json --pipeline parser --vectors vocab_kgr_100_handpruned22 --n-iter 50 --n-early-stopping 10 -G\nI've tried this with other treebanks and the error persists. I do not recall encountering anything like it in the previous version.\nYour Environment\nspaCy version: 2.2.1\nPlatform: Linux-4.15.0-58-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8", "issue_status": "Closed", "issue_reporting_time": "2019-10-08T13:36:25Z", "fixed_by": "#4516", "pull_request_summary": "Match pop with append for training format", "pull_request_description": "Member\nsvlandeg commented on 24 Oct 2019 \u2022\nedited\nDescription\nPR #4226 added a cats field to the training format, which needs to be pop()'d off before iterating over the sentences and their annotations. Afterwards however, the field needs to be added back in with append, in case multiple pipeline components would process the same TRAIN_DATA. If we don't do this, we could be losing training data throughout the pipeline.\nCaught this error while trying to run ner_multitask_objective.py, which was crashing. It runs again now, though I'm not sure whether it's still functional OK? I think it's been a while since this script was tested :-)\nEDIT: Also now fixes #4402\nMeanwhile, I'm working on rewriting the training tuple-format (as discussed) which would be quite an overhaul (+ not backwards compatible). But I thought this quick fix would be good to make sure the current master works properly in the meantime.\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-27T15:01:33Z", "files_changed": [["35", "examples/training/ner_multitask_objective.py"], ["41", "spacy/gold.pyx"], ["3", "spacy/language.py"], ["7", "spacy/pipeline/pipes.pyx"], ["3", "spacy/syntax/arc_eager.pyx"], ["3", "spacy/syntax/ner.pyx"], ["3", "spacy/syntax/nn_parser.pyx"], ["6", "spacy/syntax/nonproj.pyx"], ["97", "spacy/tests/regression/test_issue4402.py"]]}, "275": {"issue_url": "https://github.com/explosion/spaCy/issues/4401", "issue_id": "#4401", "issue_summary": "Norwegian Bokm\u00e5l sentence segmentation not working", "issue_description": "emilmuller commented on 8 Oct 2019\nDoing this:\nimport spacy\nnlp = spacy.load(\"nb_core_news_sm\")\ndoc = nlp(\"Hei p\u00e5 deg. Jeg har det fint.\")\nfor sent in doc.sents:\n  print(sent.text)\nOutputs:\nHei p\u00e5 deg. Jeg har det fint.\n:(\nThere's some mention in #3082 that maybe sbd needs to be added to the pipeline? I'm not sure if it is, how I can do it, or if it should be there by default?", "issue_status": "Closed", "issue_reporting_time": "2019-10-08T12:04:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "276": {"issue_url": "https://github.com/explosion/spaCy/issues/4400", "issue_id": "#4400", "issue_summary": "spacy.load('en_biobio') error", "issue_description": "dhwani2410 commented on 8 Oct 2019\nimport spacy\nnlp = spacy.load('en_biobio')\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/home/dhwani/.local/lib/python3.6/site-packages/spacy/init.py\", line 27, in load\nreturn util.load_model(name, **overrides)\nFile \"/home/dhwani/.local/lib/python3.6/site-packages/spacy/util.py\", line 131, in load_model\nreturn load_model_from_package(name, **overrides)\nFile \"/home/dhwani/.local/lib/python3.6/site-packages/spacy/util.py\", line 152, in load_model_from_package\nreturn cls.load(**overrides)\nFile \"/usr/local/lib/python3.6/dist-packages/en_biobio/init.py\", line 58, in load\nreturn load_model_from_init_py(file, **overrides)\nFile \"/home/dhwani/.local/lib/python3.6/site-packages/spacy/util.py\", line 190, in load_model_from_init_py\nreturn load_model_from_path(data_path, meta, **overrides)\nFile \"/home/dhwani/.local/lib/python3.6/site-packages/spacy/util.py\", line 171, in load_model_from_path\ncomponent = nlp.create_pipe(name, config=config)\nFile \"/home/dhwani/.local/lib/python3.6/site-packages/spacy/language.py\", line 256, in create_pipe\nraise KeyError(Errors.E108.format(name=name))\nKeyError: \"[E108] As of spaCy v2.1, the pipe name sbd has been deprecated in favor of the pipe name sentencizer, which does the same thing. For example, use nlp.create_pipeline('sentencizer')\"\nspaCy version: 2.1.4\nPlatform: Linux-4.15.0-62-generic-x86_64-with-Ubuntu-16.04-xenial\nPython version: 3.6.8", "issue_status": "Closed", "issue_reporting_time": "2019-10-08T09:00:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "277": {"issue_url": "https://github.com/explosion/spaCy/issues/4398", "issue_id": "#4398", "issue_summary": "EntityRuler to unmatch spans", "issue_description": "dalevskaya commented on 8 Oct 2019\nHello,\nI'm using Spacy for NER and would like to improve accuracy by adding a PhraseMatcher after the NER step. According to the EntityRuler doc, I can add spans to the Doc.ents, however I need exactly opposite - to exclude incorrectly predicted spans. For instance Spacy NER always predicts the type of the span AAAA to be X, which is always incorrect.\nIs it possible to use EntityRuler API to unmatch incorrectly recognised entities/spans.\nYour Environment\nOperating System: mac OS\nPython Version Used: 3.7\nspaCy Version Used: 2.1.3\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-08T08:23:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "278": {"issue_url": "https://github.com/explosion/spaCy/issues/4397", "issue_id": "#4397", "issue_summary": "PhraseMatcher does not execute callback function on matches", "issue_description": "zrlaida commented on 8 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nIn the example from the documentation https://spacy.io/api/phrasematcher\nthe callback function is not executed:\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef on_match(matcher, doc, id, matches):\n      print('Matched!', matches)\n\nmatcher = spacy.matcher.PhraseMatcher(nlp.vocab)\nmatcher.add(\"OBAMA\", on_match, nlp(\"Barack Obama\"))\nmatcher.add(\"HEALTH\", on_match, nlp(\"health care reform\"),\n                                  nlp(\"healthcare reform\"))\ndoc = nlp(\"Barack Obama urges Congress to find courage to defend his healthcare reforms\")\nmatches = matcher(doc)\n\nfor match in matches:\n    print(\"Match: \", match, nlp.vocab.strings[match[0]])\nAnd another example:\nimport spacy\n\ndef on_match(matcher, doc, id, matches):\n    print('Callback executes on matches!', matches)\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = spacy.matcher.PhraseMatcher(nlp.vocab)\nmatcher.add(\"OBAMA\", on_match, nlp(\"Barack Obama\"))\nmatcher.add(\"HEALTH\", on_match, nlp(\"health care reform\"), nlp(\"healthcare reform\"))\ndoc = nlp(\"Barack Obama urges Congress to find courage to defend his healthcare reform\")\nprint(\"Start matching. Print in on_match should come after this.\")\nmatches = matcher(doc)\nfor match in matches:\n    print(\"Match: \", match, nlp.vocab.strings[match[0]])\nMy Environment\nOperating System: MacOS 10.13.3\nPython Version Used: 3.7.0\nspaCy Version Used: 2.2.1\nEnvironment Information:\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-08T07:48:16Z", "fixed_by": "#4399", "pull_request_summary": "Fix PhraseMatcher callback and add tests", "pull_request_description": "Collaborator\nadrianeboyd commented on 8 Oct 2019\nDescription\nFix callback lookup in PhraseMatcher (string key rather than hash key)\nAdd callback tests for Matcher and PhraseMatcher\nFixes #4397.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-08T10:07:03Z", "files_changed": [["2", "spacy/matcher/phrasematcher.pyx"], ["11", "spacy/tests/matcher/test_matcher_api.py"], ["11", "spacy/tests/matcher/test_phrase_matcher.py"]]}, "279": {"issue_url": "https://github.com/explosion/spaCy/issues/4396", "issue_id": "#4396", "issue_summary": "PhraseMatcher does not execute callback function on matches", "issue_description": "zrlaida commented on 8 Oct 2019\nHow to reproduce the behaviour\nIn the example from the documentation https://spacy.io/api/phrasematcher\nthe callback function is not executed.\nimport spacy\nimport de_core_news_sm\nnlp = spacy.load(\"en_core_web_sm\")\ndef on_match(matcher, doc, id, matches):\nprint('Matched!', matches)\nmatcher = spacy.matcher.PhraseMatcher(nlp.vocab)\nmatcher.add(\"OBAMA\", on_match, nlp(\"Barack Obama\"))\nmatcher.add(\"HEALTH\", on_match, nlp(\"health care reform\"),\nnlp(\"healthcare reform\"))\ndoc = nlp(\"Barack Obama urges Congress to find courage to defend his healthcare reforms\")\nmatches = matcher(doc)\nfor match in matches:\nprint(\"Match: \", match, nlp.vocab.strings[match[0]])\nAnother example.\nimport spacy\nimport de_core_news_sm\ndef on_match(matcher, doc, id, matches):\nprint('Callback executes on matches!', matches)\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = spacy.matcher.PhraseMatcher(nlp.vocab)\nmatcher.add(\"OBAMA\", on_match, nlp(\"Barack Obama\"))\nmatcher.add(\"HEALTH\", on_match, nlp(\"health care reform\"), nlp(\"healthcare reform\"))\ndoc = nlp(\"Barack Obama urges Congress to find courage to defend his healthcare reform\")\nprint(\"Start matching. Print in on_match should come after this.\")\nmatches = matcher(doc)\nfor match in matches:\nprint(\"Match: \", match, nlp.vocab.strings[match[0]])\nYour Environment\nOperating System:\nMacOS\nPython Version Used:\npython 3.7\nspaCy Version Used:\n2.2.1\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-08T07:38:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "280": {"issue_url": "https://github.com/explosion/spaCy/issues/4389", "issue_id": "#4389", "issue_summary": "ner_jsonl2json() got an unexpected keyword argument 'seg_sents'", "issue_description": "vitaly-d commented on 7 Oct 2019 \u2022\nedited\nHow to reproduce the behaviour\nThe 'jsonl2json' convertor fails with the \"got an unexpected keyword argument\" message:\nTo reproduce:\npython -m spacy convert -c jsonl --lang en sample.jsonl /tmp\nNothing interesting within sample.jsonl, the behaviour does not depend on its content.\nsample.jsonl.zip\nAfter modifying the signature of ner_jsonl2json, my large dataset was converted without any issues:\n(.env) vitaly@iMac spaCy % git diff\ndiff --git a/spacy/cli/converters/jsonl2json.py b/spacy/cli/converters/jsonl2json.py\nindex 91dd4298..c6abc487 100644\n--- a/spacy/cli/converters/jsonl2json.py\n+++ b/spacy/cli/converters/jsonl2json.py\n@@ -7,7 +7,9 @@ from ...gold import docs_to_json\n from ...util import get_lang_class, minibatch\n \n \n-def ner_jsonl2json(input_data, lang=None, n_sents=10, use_morphology=False):\n+def ner_jsonl2json(\n+    input_data, lang=None, n_sents=10, use_morphology=False, seg_sents=None, model=None\n+):\n     if lang is None:\n         raise ValueError(\"No --lang specified, but tokenization required\")\n     json_docs = []\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-10-07T10:20:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "281": {"issue_url": "https://github.com/explosion/spaCy/issues/4386", "issue_id": "#4386", "issue_summary": "An error while pruning vectors", "issue_description": "ryszardtuora commented on 6 Oct 2019\nI'm trying to retrain my models because of the 2.2 version update (btw the old ones seem to work alright, but I'm retraining them to be sure), and while trying to initialize a model using CLI i get an error inside the vector pruning function.\nThe error is as follows\n\u2714 Successfully created model\n2123132it [01:09, 30354.25it/s]i-forms-all-100-skipg-hs.txt\n\u2714 Loaded vectors from nkjp+wiki-forms-all-100-skipg-hs.txt\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/spacy/cli/init_model.py\", line 90, in init_model\n    add_vectors(nlp, vectors_loc, prune_vectors, vectors_name)\n  File \"/home/rtuora/.local/lib/python3.6/site-packages/spacy/cli/init_model.py\", line 201, in add_vectors\n    nlp.vocab.prune_vectors(prune_vectors)\n  File \"vocab.pyx\", line 324, in spacy.vocab.Vocab.prune_vectors\n  File \"vectors.pyx\", line 341, in spacy.vectors.Vectors.most_similar\nValueError: could not broadcast input array from shape (1323132,1) into shape (1024,1)\nHow to reproduce the behaviour\nthe exact commend I use is\npython3 -m spacy init-model pl output --vectors-loc nkjp+wiki-forms-all-100-skipg-hs.txt --prune-vectors 800000\nI've tried this with 2 different embeddings, and all seems to work well unless I enable the pruning option.\nYour Environment\nspaCy version: 2.2.1\nPlatform: Linux-4.15.0-58-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nBTW: I'm not sure if my reasoning on pruning vectors is correct. The loading times of my models are very long, and I suspect this might be because of it. If my initial embeddings have 3.5 m entries, and I prune them down to the first 800.000, is this a good practice for a general purpose model? Perhaps I should delete these entries altogether, as their effect on regular parsing task must be marginal (or maybe even need not be positive).", "issue_status": "Closed", "issue_reporting_time": "2019-10-06T16:38:12Z", "fixed_by": "#4388", "pull_request_summary": "Consider batch_size when sorting similar vectors", "pull_request_description": "Collaborator\nadrianeboyd commented on 7 Oct 2019\nDescription\nConsider batch_size when sorting similar vectors\nThis is my best guess at a fix and needs to be reviewed by someone who's more familiar with this part of the code.\nFixes #4386.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-07T11:38:36Z", "files_changed": [["2", "spacy/vectors.pyx"]]}, "282": {"issue_url": "https://github.com/explosion/spaCy/issues/4385", "issue_id": "#4385", "issue_summary": "Bug in convert iob file to ner json format, no character after 'U' tag as entity", "issue_description": "m-idasa commented on 6 Oct 2019 \u2022\nedited\nFix ner convertor for tag 'U'\nI executed the following command:\npython3 -m spacy train fa ./model ./data/part1.json ./data/part2.json -p 'ner'\nbut it returns this error:\n\u26a0 Output directory is not empty\nThis can lead to unintended side effects when saving the model. Please use an\nempty directory or a different path instead. If the specified output path\ndoesn't exist, the directory will be created for you.\nTraining pipeline: ['ner']\nStarting with blank model 'fa'\nCounting training words (limit=0)\n\nItn  NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n---  ---------  ------  ------  ------  -------  -------\n\u2714 Saved model to output directory                                               \nmodel/model-final\n\nTraceback (most recent call last):\n  File \"/home/ali/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 365, in train\n    losses=losses,\n  File \"/home/ali/.local/lib/python3.6/site-packages/spacy/language.py\", line 516, in update\n    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n  File \"nn_parser.pyx\", line 419, in spacy.syntax.nn_parser.Parser.update\n  File \"nn_parser.pyx\", line 524, in spacy.syntax.nn_parser.Parser._init_gold_batch\n  File \"transition_system.pyx\", line 93, in spacy.syntax.transition_system.TransitionSystem.get_oracle_sequence\n  File \"transition_system.pyx\", line 154, in spacy.syntax.transition_system.TransitionSystem.set_costs\nValueError: [E024] Could not find an optimal move to supervise the parser. Usually, this means that the model can't be updated in a way that's valid and satisfies the correct annotations specified in the GoldParse. For example, are all labels added to the model? If you're training a named entity recognizer, also make sure that none of your annotated entity spans have leading or trailing whitespace. You can also use the experimental `debug-data` command to validate your JSON-formatted training data. For details, run:\npython -m spacy debug-data --help\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ali/.local/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/home/ali/.local/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/home/ali/.local/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/home/ali/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 486, in train\n    best_model_path = _collate_best_model(meta, output_path, nlp.pipe_names)\n  File \"/home/ali/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 548, in _collate_best_model\n    bests[component] = _find_best(output_path, component)\n  File \"/home/ali/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 567, in _find_best\n    accs = srsly.read_json(epoch_model / \"accuracy.json\")\n  File \"/home/ali/.local/lib/python3.6/site-packages/srsly/_json_api.py\", line 50, in read_json\n    file_path = force_path(location)\n  File \"/home/ali/.local/lib/python3.6/site-packages/srsly/util.py\", line 21, in force_path\n    raise ValueError(\"Can't read file: {}\".format(location))\nValueError: Can't read file: model/model-best/accuracy.json\nso I debug my data:\npython3 -m spacy debug-data fa ./data/part1.json ./data/part2.json -p 'ner'\nand it returns:\n=========================== Data format validation ===========================\n\u2714 Corpus is loadable\n\n=============================== Training stats ===============================\nTraining pipeline: ner\nStarting with blank model 'fa'\n5 training docs\n2 evaluation docs\n\u2714 No overlap between training and evaluation data\n\u2718 Low number of examples to train from a blank model (5)\n\n============================== Vocab & Vectors ==============================\n\u2139 14502 total words in the data (3204 unique)\n\u2139 No word vectors present in the model\n\n========================== Named Entity Recognition ==========================\n\u2139 6 new labels, 0 existing labels\n0 missing values (tokens with '-' label)\n\u26a0 Low number of examples for new label '' (2)\n\u2714 Examples without occurrences available for all labels\n\u2714 No entities consisting of or starting/ending with whitespace\n\n================================== Summary ==================================\n\u2714 4 checks passed\n\u26a0 1 warning\n\u2718 1 error\nand I had ')' and '\u060c' as label too in other input data. Then I checked out library and guess maybe there is a 'B' tag without entity label but there was no wrong sample in input data too.\nI have also used large data, but it returns the same warning.\nOperating System: ubuntu 18.04.3 LTS\nPython Version Used: 3.6\nspaCy Version Used: 2.2.1", "issue_status": "Closed", "issue_reporting_time": "2019-10-06T15:28:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "283": {"issue_url": "https://github.com/explosion/spaCy/issues/4382", "issue_id": "#4382", "issue_summary": "model VersionConflict on upgrading to 2.2.1", "issue_description": "Motorrat commented on 6 Oct 2019\nHow to reproduce the problem\nUse virtual environment. Upgrade spacy 2.1.8 to 2.2.0 and upgrade the models.\nmake symlink\nln -s /home/user/venv/full/lib/python3.6/site-packages/de_core_news_md -->\n/home/user/venv/full/lib/python3.6/site-packages/spacy/data/de_core_news_md\nThen try loading the model with nlp=spacy.load('de_core_news_md')\nand get the following error:\npkg_resources.VersionConflict: (spacy 2.2.1 (/home/user/venv/full/lib/python3.6/site-packages), Requirement.parse('spacy<2.2.0,>=2.1.7'))\nSo here are detailed steps (all run in virtual environment \"full\"):\npip install --upgrade spacy\nSuccessfully installed blis-0.4.1 preshed-3.0.2 spacy-2.2.1 thinc-7.1.1 urllib3-1.25.6\n\npython -m spacy validate\npackage   de-core-news-md            de_core_news_md            2.1.0   --> 2.2.0\n\npython -m spacy download de_core_news_md\n...\nSuccessfully installed de-core-news-md-2.2.0\n\npython -m spacy download de_core_news_md\nRequirement already satisfied: de_core_news_md==2.2.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_md-2.2.0/de_core_news_md-2.2.0.tar.gz#egg=de_core_news_md==2.2.0 in /home/user/venv/full/lib/python3.6/site-packages\nRequirement already satisfied: spacy>=2.2.0 in /home/user/venv/full/lib/python3.6/site-packages (from de_core_news_md==2.2.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: thinc<7.2.0,>=7.1.1 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: numpy>=1.15.0 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: plac<1.0.0,>=0.9.6 in /home/user/venv/full/lib/python3.6/site-packages (from spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/user/venv/full/lib/python3.6/site-packages (from thinc<7.2.0,>=7.1.1->spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: idna<2.9,>=2.5 in /home/user/venv/full/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/user/venv/full/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/user/venv/full/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->de_core_news_md==2.2.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/user/venv/full/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->de_core_news_md==2.2.0)\n\u2714 Download and installation successful\nYou can now load the model via spacy.load('de_core_news_md')\n\npython -m spacy validate\n\u2714 Loaded compatibility table\n\n====================== Installed models (spaCy v2.2.1) ======================\n\u2139 spaCy installation:\n/home/user/venv/full/lib/python3.6/site-packages/spacy\n\npackage   de-core-news-md        de_core_news_md        2.2.0   \u2714\n\n(full) $ python\nPython 3.6.8 (default, Aug 20 2019, 17:12:48) \n[GCC 8.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import spacy\n>>> nlp=spacy.load('de_core_news_md')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 166, in load_model\n    return load_model_from_package(name, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 187, in load_model_from_package\n    return cls.load(**overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/de_core_news_md/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 228, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 199, in load_model_from_path\n    nlp = cls(meta=meta, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/language.py\", line 175, in __init__\n    user_factories = util.get_entry_points(util.ENTRY_POINTS.factories)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 286, in get_entry_points\n    result[entry_point.name] = entry_point.load()\n  File \"/home/user/venv/full/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2433, in load\n    self.require(*args, **kwargs)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 2456, in require\n    items = working_set.resolve(reqs, env, installer, extras=self.extras)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 791, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.VersionConflict: (spacy 2.2.1 (/home/user/venv/full/lib/python3.6/site-packages), Requirement.parse('spacy<2.2.0,>=2.1.7'))\nAlso tried installing using the code\nfrom spacy.cli.download import download\nfrom spacy.cli import link\ndownload('de_core_news_md')\nlink('de_core_news_md', 'de_core_news_md', force=True)\nspacy_pipe = spacy.load('de_core_news_md')\n\n\u2714 Download and installation successful\nYou can now load the model via spacy.load('de_core_news_md')\n\u2714 Linking successful\n/home/user/venv/full/lib/python3.6/site-packages/de_core_news_md -->\n/home/user/venv/full/lib/python3.6/site-packages/spacy/data/de_core_news_md\nYou can now load the model via spacy.load('de_core_news_md')\nBut I get the same error there as well.\nDowngrading spacy to 2.1.8 fails using the new models (as expected) :\npip install spacy==2.1.8\n...\n      Successfully uninstalled spacy-2.2.0\nSuccessfully installed blis-0.2.4 preshed-2.0.1 spacy-2.1.8 thinc-7.0.8\n\n>>> nlp=spacy.load('de_core_news_md')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 132, in load_model\n    return load_model_from_link(name, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 149, in load_model_from_link\n    return cls.load(**overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/data/de_core_news_md/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 196, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 179, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/language.py\", line 836, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 636, in from_disk\n    reader(path / key)\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/language.py\", line 831, in <lambda>\n    p, exclude=[\"vocab\"]\n  File \"pipes.pyx\", line 641, in spacy.pipeline.pipes.Tagger.from_disk\n  File \"/home/user/venv/full/lib/python3.6/site-packages/spacy/util.py\", line 636, in from_disk\n    reader(path / key)\n  File \"pipes.pyx\", line 629, in spacy.pipeline.pipes.Tagger.from_disk.load_tag_map\n  File \"morphology.pyx\", line 56, in spacy.morphology.Morphology.__init__\n  File \"attrs.pyx\", line 148, in spacy.attrs.intify_attrs\nKeyError: 'PUNCTTYPE_BRCK'\nYour Environment\nInfo about spaCy\nspaCy version: 2.2.1\nPlatform: Linux-4.15.0-43-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nModels: de, fr, fr_core_news_sm, es, de_core_news_sm, la, xx, un, la_latin_spacy_model, en_core_web_sm, de_core_news_md, nl, en", "issue_status": "Closed", "issue_reporting_time": "2019-10-05T23:47:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "284": {"issue_url": "https://github.com/explosion/spaCy/issues/4380", "issue_id": "#4380", "issue_summary": "Missing PyPi wheel for Windows (amd64) with Python 3.7 (cp37)", "issue_description": "mborsetti commented on 5 Oct 2019 \u2022\nedited\nhttps://pypi.org/project/spacy/#files is missing the wheel for Windows amd64 with Python 3.7\n(i.e. spacy-2.2.1-cp37-cp37m-win_amd64.whl). All previous versions, including 2.2.0, had this wheel.\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7.4\nspaCy Version Used: 2.2.0 because of failure of > pip install -U spacy\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-04T23:17:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "285": {"issue_url": "https://github.com/explosion/spaCy/issues/4379", "issue_id": "#4379", "issue_summary": "nlp = spacy.load(\"en_pytt_robertabase_lg\") resulted in keyerror", "issue_description": "studiocardo commented on 4 Oct 2019 \u2022\nedited\nHi guys\nPer the subject line.\nFor comparison, I was able to load the en_pytt_bertbaseuncased_lg and en_pytt_xlnetbasecased_lg models and ran them successfully. But both en_pytt_robertabase_lg and en_pytt_distilbertbaseuncased_lg are causing KeyError problems.\nFor roberta, it's KeyError: vocab.\nFor distilledbert, it's ValueError: Unexpected config. Keys: finetuning_task, num_labels, output_attentions, output_hidden_states, torchscript, vocab_size, max_position_embeddings, sinusoidal_pos_embds, n_layers, n_heads, dim, hidden_dim, dropout, attention_dropout, activation, initializer_range, tie_weights_, qa_dropout, seq_classif_dropout, layer_norm_eps, vocab_size_or_config_json_file\nHow to reproduce the behaviour\nnlp = spacy.load(\"en_pytt_robertabase_lg\")\ndoc = nlp(\"Here is some text to encode.\")\nKeyError Traceback (most recent call last)\nin\n----> 1 nlp = spacy.load(\"en_pytt_robertabase_lg\")\n2 doc = nlp(\"Here is some text to encode.\")\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/init.py in load(name, **overrides)\n25 if depr_path not in (True, False, None):\n26 deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 27 return util.load_model(name, **overrides)\n28\n29\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/util.py in load_model(name, **overrides)\n132 return load_model_from_link(name, **overrides)\n133 if is_package(name): # installed as package\n--> 134 return load_model_from_package(name, **overrides)\n135 if Path(name).exists(): # path to model data directory\n136 return load_model_from_path(Path(name), **overrides)\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/util.py in load_model_from_package(name, **overrides)\n153 \"\"\"Load a model from an installed package.\"\"\"\n154 cls = importlib.import_module(name)\n--> 155 return cls.load(**overrides)\n156\n157\n~/miniconda3/envs/squad/lib/python3.6/site-packages/en_pytt_robertabase_lg/init.py in load(**overrides)\n10\n11 def load(**overrides):\n---> 12 return load_model_from_init_py(file, **overrides)\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/util.py in load_model_from_init_py(init_file, **overrides)\n194 if not model_path.exists():\n195 raise IOError(Errors.E052.format(path=path2str(data_path)))\n--> 196 return load_model_from_path(data_path, meta, **overrides)\n197\n198\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/util.py in load_model_from_path(model_path, meta, **overrides)\n177 component = nlp.create_pipe(name, config=config)\n178 nlp.add_pipe(component, name=name)\n--> 179 return nlp.from_disk(model_path)\n180\n181\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/language.py in from_disk(self, path, exclude, disable)\n834 # Convert to list here in case exclude is (default) tuple\n835 exclude = list(exclude) + [\"vocab\"]\n--> 836 util.from_disk(path, deserializers, exclude)\n837 self._path = path\n838 return self\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/util.py in from_disk(path, readers, exclude)\n634 # Split to support file names like meta.json\n635 if key.split(\".\")[0] not in exclude:\n--> 636 reader(path / key)\n637 return path\n638\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/language.py in (p, proc)\n829 continue\n830 deserializers[name] = lambda p, proc=proc: proc.from_disk(\n--> 831 p, exclude=[\"vocab\"]\n832 )\n833 if not (path / \"vocab\").exists() and \"vocab\" not in exclude:\npipes.pyx in spacy.pipeline.pipes.Pipe.from_disk()\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy/util.py in from_disk(path, readers, exclude)\n634 # Split to support file names like meta.json\n635 if key.split(\".\")[0] not in exclude:\n--> 636 reader(path / key)\n637 return path\n638\npipes.pyx in spacy.pipeline.pipes.Pipe.from_disk.load_model()\n~/miniconda3/envs/squad/lib/python3.6/site-packages/spacy_pytorch_transformers/_tokenizers.py in from_bytes(self, bytes_data, exclude, **kwargs)\n53 msg = srsly.msgpack_loads(bytes_data)\n54 for field in self.serialization_fields:\n---> 55 setattr(self, field, msg[field])\n56 self.finish_deserializing()\n57 return self\nKeyError: 'vocab'\nAnd here's the installation message from python -m spacy download en_pytt_robertabase_lg\nCollecting en_pytt_robertabase_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_pytt_robertabase_lg-2.1.0/en_pytt_robertabase_lg-2.1.0.tar.gz#egg=en_pytt_robertabase_lg==2.1.0\nDownloading https://github.com/explosion/spacy-models/releases/download/en_pytt_robertabase_lg-2.1.0/en_pytt_robertabase_lg-2.1.0.tar.gz (291.9MB)\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 291.9MB 119.4MB/s\nBuilding wheels for collected packages: en-pytt-robertabase-lg\nBuilding wheel for en-pytt-robertabase-lg (setup.py) ... done\nCreated wheel for en-pytt-robertabase-lg: filename=en_pytt_robertabase_lg-2.1.0-cp36-none-any.whl size=295936799 sha256=f3a25f88a794970bca6acb016e89e6eb74e695aedcc4f9d090bb1f9b170a9c2a\nStored in directory: /tmp/pip-ephem-wheel-cache-qjxyme81/wheels/46/ec/e1/01b719774e9b2323e7263c0f93aacb40091e136d62aa071403\nSuccessfully built en-pytt-robertabase-lg\nInstalling collected packages: en-pytt-robertabase-lg\nSuccessfully installed en-pytt-robertabase-lg-2.1.0\nYour Environment\nOperating System: Ubuntu 16.4\nPython Version Used: 3.6.7\nspaCy Version Used: 2.1.8\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-04T18:16:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "286": {"issue_url": "https://github.com/explosion/spaCy/issues/4377", "issue_id": "#4377", "issue_summary": "Extension is not recognized when reloading spacy doc", "issue_description": "moh55m55 commented on 4 Oct 2019\nHello,\ncustom extensions is not recognized when loading spacy doc object from disk. It does recognized it when the same kernal is active, however when restarting the kernal custom extensions are not recognized whereas other attributes are ok.\nFor example:\nimport spacy\nfrom spacy.tokens import Doc, Span, Token\n\nDoc.set_extension('doc_cat',force=True,default=None )\nnlp = spacy.load(\"en_core_web_sm\")\ndoc=nlp(\"I am trying to set an extension for doc object\")\ndoc._.set(\"doc_cat\",\"AN\")\ndoc.to_disk(\"corpus/mydoc\")\n\nloaded_doc = Doc(nlp.vocab).from_disk(\"corpus/mydoc\")\nprint(loaded_doc._.doc_cat )## output \"AN\"\nafter shutting down (restarting) the kernel, I got this error:\nimport spacy\nfrom spacy.tokens import Doc, Span, Token\nnlp = spacy.load(\"en_core_web_sm\")\nloaded_doc = Doc(nlp.vocab).from_disk(\"corpus/mydoc\")\nprint(loaded_doc._.doc_cat)\nAttributeError                            Traceback (most recent call last)\n<ipython-input-1-0b8d370be645> in <module>\n      3 nlp = spacy.load(\"en_core_web_sm\")\n      4 loaded_doc = Doc(nlp.vocab).from_disk(\"corpus/mydoc\")\n----> 5 loaded_doc._.doc_cat\n\n~/anaconda3/envs/back/lib/python3.6/site-packages/spacy/tokens/underscore.py in __getattr__(self, name)\n     33     def __getattr__(self, name):\n     34         if name not in self._extensions:\n---> 35             raise AttributeError(Errors.E046.format(name=name))\n     36         default, method, getter, setter = self._extensions[name]\n     37         if getter is not None:\n\nAttributeError: [E046] Can't retrieve unregistered extension attribute 'doc_cat'. Did you forget to call the `set_extension` method?\nAny idea why is that?\n====================== Installed models (spaCy v2.2.1) ======================\n\u2139 spaCy installation:\nanaconda3/envs/back/lib/python3.6/site-packages/spacy\nTYPE NAME MODEL VERSION\npackage en-core-web-sm en_core_web_sm 2.2.0 \u2714\npackage en-core-web-lg en_core_web_lg 2.2.0 \u2714", "issue_status": "Closed", "issue_reporting_time": "2019-10-04T15:02:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "287": {"issue_url": "https://github.com/explosion/spaCy/issues/4376", "issue_id": "#4376", "issue_summary": "Usage of word embeddings in NER/Textcat models", "issue_description": "luispsantos commented on 4 Oct 2019\nHello, this may be a rookie question but I need to know the answer for my use case of an email booking system. I want to train from scratch the EntityRecognizer and TextCategorizer models with annotated texts from my domain, and we don't have a lot of annotated data. While I don't need to load the pre-built components, I would like to use the pre-trained embeddings. My understanding is that if I call nlp = spacy.blank('en') and train the models using the training recipes, I wouldn't get the benefits of transfer learning from the embeddings. So if I were to call nlp = spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner']) or just spacy.load('en_vectors_web_lg'), would I get the benefit of the pre-trained embeddings on the first layer of the NN when training from scratch the NER and Textcat models?", "issue_status": "Closed", "issue_reporting_time": "2019-10-04T13:14:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "288": {"issue_url": "https://github.com/explosion/spaCy/issues/4373", "issue_id": "#4373", "issue_summary": "Cannot access `PhraseMatcher.vocab` though can access `Matcher.vocab`", "issue_description": "Contributor\ntamuhey commented on 4 Oct 2019\nHow to reproduce the behaviour\nMatcher.vocab can be accessed.\n>>> from spacy.matcher import Matcher\n>>> from spacy.vocab import Vocab\n>>> matcher = Matcher(Vocab())\n>>> matcher.vocab\n<spacy.vocab.Vocab at 0x11ac06ec8>\nHowever, PhraseMatcher.vocab cannot be accessed.\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.vocab import Vocab\nmatcher = PhraseMatcher(Vocab())\nmatcher.vocab\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-8-21b52467d217> in <module>\n      2 from spacy.vocab import Vocab\n      3 matcher = PhraseMatcher(Vocab())\n----> 4 matcher.vocab\n\nAttributeError: 'spacy.matcher.phrasematcher.PhraseMatcher' object has no attribute 'vocab'\nPhraseMatcher.vocab should be accessed because match_id can be decoded from the vocab.\nYour Environment\nOperating System: Mac OS\nPython Version Used: 3.7.3\nspaCy Version Used: master(e7ddc6f)\n1", "issue_status": "Closed", "issue_reporting_time": "2019-10-04T10:00:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "289": {"issue_url": "https://github.com/explosion/spaCy/issues/4372", "issue_id": "#4372", "issue_summary": "Getting KeyError: 'PUNCTSIDE_FIN'", "issue_description": "AlonEirew commented on 4 Oct 2019\nHow to reproduce the problem\nInstallation with:\npip install -U spacy\npython -m spacy download en_core_web_sm\nError is thrown when running the spacy load command:\nspacy_parser = spacy.load('en_core_web_sm')\nFile \"/Users/aeirew/workspace/cross-doc-coref/src/utils/string_utils.py\", line 21, in StringUtils\n    spacy_parser = spacy.load('en_core_web_sm')\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/util.py\", line 134, in load_model\n    return load_model_from_package(name, **overrides)\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/util.py\", line 155, in load_model_from_package\n    return cls.load(**overrides)\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/en_core_web_sm/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/util.py\", line 196, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/util.py\", line 179, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/language.py\", line 836, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/util.py\", line 636, in from_disk\n    reader(path / key)\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/language.py\", line 831, in <lambda>\n    p, exclude=[\"vocab\"]\n  File \"pipes.pyx\", line 641, in spacy.pipeline.pipes.Tagger.from_disk\n  File \"/anaconda3/envs/cross-doc-coref/lib/python3.6/site-packages/spacy/util.py\", line 636, in from_disk\n    reader(path / key)\n  File \"pipes.pyx\", line 629, in spacy.pipeline.pipes.Tagger.from_disk.load_tag_map\n  File \"morphology.pyx\", line 56, in spacy.morphology.Morphology.__init__\n  File \"attrs.pyx\", line 148, in spacy.attrs.intify_attrs\nKeyError: 'PUNCTSIDE_FIN'\nYour Environment\nOperating System: MacOs Mojave(10.14.6)\nspaCy version: 2.1.8\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.9\nConda version: conda 4.7.12", "issue_status": "Closed", "issue_reporting_time": "2019-10-04T07:54:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "290": {"issue_url": "https://github.com/explosion/spaCy/issues/4370", "issue_id": "#4370", "issue_summary": "Tokenizer for 2.2 is still extremely slow compared to 1.9", "issue_description": "ibarshai commented on 4 Oct 2019\nI used Spacy 1.9 in the past for tokenizing large quantities of small strings. When 2.0 came out, the tokenization speed for these smaller strings dropped dramatically. I believe tokenizer speed was targeted in 2.1 and was hoping to see more improvements on this front in 2.2. Is there an intent to address this in the future? Am I doing something wrong with disabling pipeline elements that's keeping the processing of these strings very heavy?\nTo illustrate my point, here is a comparison of tokenizing a small sentence with most of the pipeline disabled showing a 40x difference is speed:\nSpacy 1.9\nimport spacy\nnlp = spacy.load('en', parser=False, tagger=False, entity=False)\ns = 'this is a sentence to tokenize'\n%timeit nlp(s)\n\n83.3 \u00b5s \u00b1 418 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nSpacy 2.2\nimport spacy\nnlp = spacy.load('en_core_web_sm', \n                 disable=['parser' 'tagger', 'entity', 'ner', \n                          'entity_linker', 'entity_ruler', \n                          'textcat', 'sentencizer', \n                          'merge_noun_chunks', 'merge_entities', 'merge_subtokens'])\ns = 'this is a sentence to tokenize'\n%timeit nlp(s)\n\n3.27 ms \u00b1 47.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\nMy Environment\nspaCy version: 1.9.0\nPlatform: Linux-4.13.0-45-generic-x86_64-with-debian-stretch-sid\nPython version: 3.6.2\nInstalled models: en\nspaCy version: 2.2.1\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.4\nInstalled models: en_core_web_sm", "issue_status": "Closed", "issue_reporting_time": "2019-10-03T18:31:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "291": {"issue_url": "https://github.com/explosion/spaCy/issues/4367", "issue_id": "#4367", "issue_summary": "DocBin's init has an issue with `attrs` as a `list`", "issue_description": "paoloq commented on 3 Oct 2019\nError\nattrs = sorted(intify_attrs(attrs))\n  File \"attrs.pyx\", line 144, in spacy.attrs.intify_attrs\nAttributeError: 'list' object has no attribute 'items'\nHow to reproduce the behaviour\ndoc_bin = DocBin(attrs=[\"LEMMA\", \"ENT_IOB\", \"ENT_TYPE\"])\nYour Environment\nspaCy version: 2.2.0\nPlatform: Linux-4.9.0-9-amd64-x86_64-with-debian-10.1\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-10-03T09:33:53Z", "fixed_by": "#4368", "pull_request_summary": "Bugfix initializing DocBin with attributes", "pull_request_description": "Member\nsvlandeg commented on 3 Oct 2019 \u2022\nedited\nDescription\nWe called intify_attrs() with a list instead of with a dictionary when creating a new DocBin. I created a small method intify_attr() that takes just one string argument to apply it quickly on lists.\nI also added the example from the docs to the unit tests, and fixed a typo in the example.\nFixes #4367\nTypes of change\nbug fix + documentation fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-03T12:48:45Z", "files_changed": [["3", "bin/ud/ud_train.py"], ["38", "spacy/attrs.pyx"], ["11", "spacy/tests/regression/test_issue4367.py"], ["20", "spacy/tests/serialize/test_serialize_doc.py"], ["10", "spacy/tokens/_serialize.py"], ["2", "website/docs/usage/saving-loading.md"]]}, "292": {"issue_url": "https://github.com/explosion/spaCy/issues/4366", "issue_id": "#4366", "issue_summary": "SpaCy 2.2. fails to load Greek and French data models", "issue_description": "ericmclachlan commented on 3 Oct 2019\nSpaCy 2.2. fails to load Greek and French data models on WSL.\ngreek = spacy.load('el_core_news_sm')\nfrench = spacy.load('fr_core_news_sm')\nHow to reproduce the behaviour:\nimport spacy\n\nprint(spacy.info())\n\ngreek = spacy.load('el_core_news_sm')\nfrench = spacy.load('fr_core_news_sm')\n\nprint(\"Done.\")\nRunning this code via Windows Susbsytem for Linux (WSL) produces the following error output:\npython test2.py\n\n============================== Info about spaCy ==============================\n\nspaCy version    2.2.0\nLocation         /home/ericm/.local/lib/python3.7/site-packages/spacy\nPlatform         Linux-4.19.67-microsoft-standard-x86_64-with-Ubuntu-18.04-bionic\nPython version   3.7.3\nModels\n\n{'spaCy version': '2.2.0', 'Location': '/home/ericm/.local/lib/python3.7/site-packages/spacy', 'Platform': 'Linux-4.19.67-microsoft-standard-x86_64-with-Ubuntu-18.04-bionic', 'Python version': '3.7.3', 'Models': ''}\nTraceback (most recent call last):\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 80, in get_lang_class\n    module = importlib.import_module(\".lang.%s\" % lang, \"spacy\")\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/lang/el/__init__.py\", line 9, in <module>\n    from .lemmatizer import GreekLemmatizer\nImportError: cannot import name 'GreekLemmatizer' from 'spacy.lang.el.lemmatizer' (/home/ericm/.local/lib/python3.7/site-packages/spacy/lang/el/lemmatizer/__init__.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"test2.py\", line 6, in <module>\n    greek = spacy.load('el_core_news_sm')\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 166, in load_model\n    return load_model_from_package(name, **overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 187, in load_model_from_package\n    return cls.load(**overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/el_core_news_sm/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 228, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 198, in load_model_from_path\n    cls = get_lang_class(lang)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 82, in get_lang_class\n    raise ImportError(Errors.E048.format(lang=lang, err=err))\nImportError: [E048] Can't import language el from spacy.lang: cannot import name 'GreekLemmatizer' from 'spacy.lang.el.lemmatizer' (/home/ericm/.local/lib/python3.7/site-packages/spacy/lang/el/lemmatizer/__init__.py)\nError in sys.excepthook:\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/apport_python_hook.py\", line 63, in apport_excepthook\n    from apport.fileutils import likely_packaged, get_recent_crashes\n  File \"/usr/lib/python3/dist-packages/apport/__init__.py\", line 5, in <module>\n    from apport.report import Report\n  File \"/usr/lib/python3/dist-packages/apport/report.py\", line 30, in <module>\n    import apport.fileutils\n  File \"/usr/lib/python3/dist-packages/apport/fileutils.py\", line 23, in <module>\n    from apport.packaging_impl import impl as packaging\n  File \"/usr/lib/python3/dist-packages/apport/packaging_impl.py\", line 24, in <module>\n    import apt\n  File \"/usr/lib/python3/dist-packages/apt/__init__.py\", line 23, in <module>\n    import apt_pkg\nModuleNotFoundError: No module named 'apt_pkg'\n\nOriginal exception was:\nTraceback (most recent call last):\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 80, in get_lang_class\n    module = importlib.import_module(\".lang.%s\" % lang, \"spacy\")\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/lang/el/__init__.py\", line 9, in <module>\n    from .lemmatizer import GreekLemmatizer\nImportError: cannot import name 'GreekLemmatizer' from 'spacy.lang.el.lemmatizer' (/home/ericm/.local/lib/python3.7/site-packages/spacy/lang/el/lemmatizer/__init__.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"test2.py\", line 6, in <module>\n    greek = spacy.load('el_core_news_sm')\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 166, in load_model\n    return load_model_from_package(name, **overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 187, in load_model_from_package\n    return cls.load(**overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/el_core_news_sm/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 228, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 198, in load_model_from_path\n    cls = get_lang_class(lang)\n  File \"/home/ericm/.local/lib/python3.7/site-packages/spacy/util.py\", line 82, in get_lang_class\n    raise ImportError(Errors.E048.format(lang=lang, err=err))\nImportError: [E048] Can't import language el from spacy.lang: cannot import name 'GreekLemmatizer' from 'spacy.lang.el.lemmatizer' (/home/ericm/.local/lib/python3.7/site-packages/spacy/lang/el/lemmatizer/__init__.py)\nYour Environment\nWindows Subsystem for Linux (WSL):\nspaCy version    2.2.0\nLocation         /home/ericm/.local/lib/python3.7/site-packages/spacy\nPlatform         Linux-4.19.67-microsoft-standard-x86_64-with-Ubuntu-18.04-bionic\nPython version   3.7.3\nModels\n\n{'spaCy version': '2.2.0', 'Location': '/home/ericm/.local/lib/python3.7/site-packages/spacy', 'Platform': 'Linux-4.19.67-microsoft-standard-x86_64-with-Ubuntu-18.04-bionic', 'Python version': '3.7.3', 'Models': ''}\nInterestingly, the same code running on Windows itself, does not report this error:\npython .\\test2.py\n\n============================== Info about spaCy ==============================\n\nspaCy version    2.2.0\nLocation         C:\\Python37\\lib\\site-packages\\spacy\nPlatform         Windows-10-10.0.18990-SP0\nPython version   3.7.4\nModels\n\n{'spaCy version': '2.2.0', 'Location': 'C:\\\\Python37\\\\lib\\\\site-packages\\\\spacy', 'Platform': 'Windows-10-10.0.18990-SP0', 'Python version': '3.7.4', 'Models': ''}\nDone.", "issue_status": "Closed", "issue_reporting_time": "2019-10-03T09:06:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "293": {"issue_url": "https://github.com/explosion/spaCy/issues/4365", "issue_id": "#4365", "issue_summary": "Dutch tag map not updated with new models in spaCy v2.2", "issue_description": "BLKSerene commented on 3 Oct 2019\nHi, the tag map for the Dutch language hasn't been updated with new models in spaCy v2.2, so all universal POS tags become X.\n>>> import spacy\n>>> nlp = spacy.load('nl_core_news_sm')\n>>> doc = nlp('Het Nederlands is een West-Germaanse taal en de moedertaal van de meeste inwoners van Nederland, Belgi\u00eb en Suriname.')\n\n>>> for t in doc:\n print(t.tag_, t.pos_)\n\nLID|bep|stan|evon X\nN|eigen|ev|basis|onz|stan X\nWW|pv|tgw|ev X\nLID|onbep|stan|agr X\nADJ|prenom|basis|met-e|stan X\nN|soort|ev|basis|zijd|stan X\nVG|neven X\nLID|bep|stan|rest X\nN|soort|ev|basis|zijd|stan X\nVZ|init X\nLID|bep|stan|rest X\nVNW|onbep|grad|stan|prenom|met-e|agr|sup X\nN|soort|mv|basis X\nVZ|init X\nN|eigen|ev|basis|onz|stan X\nLET X\nN|eigen|ev|basis|onz|stan X\nVG|neven X\nN|eigen|ev|basis|onz|stan X\nLET X\nOperating System: Windows 10 x64\nPython Version Used: 3.7.4 x64\nspaCy Version Used: 2.2.0", "issue_status": "Closed", "issue_reporting_time": "2019-10-03T04:23:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "294": {"issue_url": "https://github.com/explosion/spaCy/issues/4363", "issue_id": "#4363", "issue_summary": "2.2 release: IS_ALPHA unsupported in Matcher", "issue_description": "Contributor\nkwhumphreys commented on 3 Oct 2019\nIS_ALPHA reported as unsupported by Matcher, but it's still listed at https://github.com/explosion/spaCy/blob/master/spacy/matcher/_schemas.py#L113\nHow to reproduce the behaviour\n   matcher.add(\"RULE1\", None, [{'IS_APLHA': True, 'IS_DIGIT': False, 'IS_PUNCT': False}, {}])\n  File \"matcher.pyx\", line 118, in spacy.matcher.matcher.Matcher.add\n  File \"matcher.pyx\", line 619, in spacy.matcher.matcher._preprocess_pattern\n  File \"matcher.pyx\", line 639, in spacy.matcher.matcher._get_attr_values\nValueError: [E152] The attribute IS_APLHA is not supported for token patterns. Please use the option validate=True with Matcher, PhraseMatcher, or EntityRuler for more details.\nYour Environment\nspaCy version: 2.2.0\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.5", "issue_status": "Closed", "issue_reporting_time": "2019-10-02T21:27:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "295": {"issue_url": "https://github.com/explosion/spaCy/issues/4362", "issue_id": "#4362", "issue_summary": "Spacy tokenizer hangs", "issue_description": "levon003 commented on 3 Oct 2019\nHow to reproduce the behaviour\nI identified a text that causes the tokenizer to [apparently] hang. See the sample script below.\ntext = \"https://si0.twimg.com/profile_images/2711056064/4399ea260e5608718ba4f54960b51627.jpeg?awesome=3dhiQfF57zkZOHOuyKnORdmiqZSVDU08swJ1KmtpbNdWZbTKt1anNwJJB99a9stVShtj59XpA3yLxYRigamlXlxfwDWZ2LH2xKOqs2mHN46qgLBqb1H156JGhBryicZb1gmlKuC2vLMonWOCCA8ngGOSMwlSQai5LBNB4ZVAVrVPz2JoYwxUNDmtdGdGEgcdHuFFrHk7\u2603\" + \"\u2603\" * 13200\nprint(text)\nimport spacy\nnlp = spacy.load(\"en_core_web_lg\")\ntokenizer = nlp.Defaults.create_tokenizer(nlp)\nprint(\"loaded\")\nprint(spacy.__version__)\nfor doc in tokenizer.pipe((text,)):\n    print(doc)\nI'm not sure if the tokenizer will ever terminate; from hanging up a batch job that was given about 48 hours for processing, I'm fairly certain that this text causes the tokenizer to at least block for 35+ hours. (The test code above blocks for at least > 5 mins; in contrast, the string of 13200 Snowman emoji '\u2603' tokenizes in about 22 seconds. [The link without the emoji tokenizes rapidly, as expected.])\nNote that neither the Snowman emoji nor the link are sufficient on their own to cause the tokenizer to hang; it's something about the combination of the link and the long string of emoji.\nOn interrupt:\n  File \"tokenizer.pyx\", line 142, in pipe\n  File \"tokenizer.pyx\", line 125, in spacy.tokenizer.Tokenizer.__call__\n  File \"tokenizer.pyx\", line 167, in spacy.tokenizer.Tokenizer._tokenize\n  File \"tokenizer.pyx\", line 184, in spacy.tokenizer.Tokenizer._split_affixes\nMaybe related to issues #2835 #2744\nNote that this is a naturally occurring text provided by a user in a social media environment; to decrease the size of the text, I multiplied out the snowman emoji by the number of times it occurred in the original text.\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core\nPython version: 3.7.3\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-10-02T20:05:07Z", "fixed_by": "#4374", "pull_request_summary": "Improve URL_PATTERN and handling in tokenizer", "pull_request_description": "Collaborator\nadrianeboyd commented on 4 Oct 2019\nDescription\nMove prefix and suffix detection for URL_PATTERN into the tokenizer. Remove associated lookahead and lookbehind from URL_PATTERN.\nFix tokenization for Hungarian given new modified handling of prefixes and suffixes.\nAnd since I'm already looking at the details, correct/simplify a few parts of the pattern and match a wider range of URI schemes. If you don't want these changes, the second commit can be reverted.\nIt would be nice to match IDNs, too, but I decided it looks too complicated for now.\nFixes #4362.\nTypes of change\nBugfix and enhancement.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n3\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-05T11:00:10Z", "files_changed": [["6", "spacy/lang/hu/punctuation.py"], ["14", "spacy/lang/tokenizer_exceptions.py"], ["6", "spacy/tests/tokenizer/test_urls.py"], ["6", "spacy/tokenizer.pyx"]]}, "296": {"issue_url": "https://github.com/explosion/spaCy/issues/4358", "issue_id": "#4358", "issue_summary": "Span object can not be serialized", "issue_description": "moh55m55 commented on 2 Oct 2019\nHello,\nI am using spacy to process collection of documents with adding many customized components to the pipe. But when i tried to save the Spacy documents (result) to disk, i got this. Not sure where that error comes from. any idea?\n~/anaconda3/envs/background/lib/python3.6/site-packages/srsly/_msgpack_api.py in msgpack_dumps(data)\n     14     RETURNS (bytes): The serialized bytes.\n     15     \"\"\"\n---> 16     return msgpack.dumps(data, use_bin_type=True)\n     17 \n     18 \n\n~/anaconda3/envs/background/lib/python3.6/site-packages/srsly/msgpack/__init__.py in packb(o, **kwargs)\n     38     Pack an object and return the packed bytes.\n     39     \"\"\"\n---> 40     return Packer(**kwargs).pack(o)\n     41 \n     42 \n\n_packer.pyx in srsly.msgpack._packer.Packer.pack()\n\n_packer.pyx in srsly.msgpack._packer.Packer.pack()\n\n_packer.pyx in srsly.msgpack._packer.Packer.pack()\n\n_packer.pyx in srsly.msgpack._packer.Packer._pack()\n\n_packer.pyx in srsly.msgpack._packer.Packer._pack()\n\n_packer.pyx in srsly.msgpack._packer.Packer._pack()\n\n_packer.pyx in srsly.msgpack._packer.Packer._pack()\n\nTypeError: can not serialize 'spacy.tokens.span.Span' object", "issue_status": "Closed", "issue_reporting_time": "2019-10-02T04:51:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "297": {"issue_url": "https://github.com/explosion/spaCy/issues/4357", "issue_id": "#4357", "issue_summary": "Not Updating the default NER model - en_core_web_sm", "issue_description": "AbhayGodbole commented on 2 Oct 2019 \u2022\nedited\nHi,\nThe use case I am currently working, required to extract Person Name, Amount, SSN No. Out of that default model extracts Person and Money (I need to change it to as Amount) and SSN would be my custom entity. So I am preparing the Tagged tsv for SSN and while training the Model I am providing en_core_web_sm model, my understanding is it will keep the default entities and add my custom entity SSN. But in actual, if I load this new model, its nigher extracting default entities not the custom.\nWhen I train a custom model, by passing None to the model parameter its extracting my custom entities. I am using following function to train the model:\n`\ndef trainModel(self,trainingFile=None,model=None, new_model_name='new_model', output_dir=None, n_iter=10):\n try:\n  with open (trainingFile, 'rb') as fp:\n   TRAIN_DATA = pickle.load(fp)\n\n  \"\"\"\n   Setting up the pipeline and entity recognizer, and training the new entity.\n  \"\"\"\n  if model is not None:\n   nlp = spacy.load(model)  # load existing spacy model\n   logging.info(\"Loaded model '%s'\" % model)\n  else:\n   nlp = spacy.blank('en')  # create blank Language class\n   logging.info(\"Created blank 'en' model\")\n   reset_weights = False\n  if 'ner' not in nlp.pipe_names:\n   ner = nlp.create_pipe('ner')\n   nlp.add_pipe(ner)\n   reset_weights = True        \n  else:\n   ner = nlp.get_pipe('ner')\n\n  # Add Custom Labels\n  for _, annotations in TRAIN_DATA:\n   for ent in annotations.get('entities'):\n    ner.add_label(ent[2])\n\n  if model is None or reset_weights:\n   optimizer = nlp.begin_training()\n  else:\n   optimizer = nlp.entity.create_optimizer()\n\n  # Get names of other pipes to disable them during training to train only NER\n  other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n  with nlp.disable_pipes(*other_pipes):  # only train NER\n   for itn in range(n_iter):\n       random.shuffle(TRAIN_DATA)\n       losses = {}\n       batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n       for batch in batches:\n           texts, annotations = zip(*batch)\n           nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n                      losses=losses)\n       logging.info('Losses: ' + str(losses))\n\n  # Save model \n  if output_dir is not None:\n   output_dir = Path(output_dir)\n   if not output_dir.exists():\n    output_dir.mkdir()\n   nlp.meta['name'] = new_model_name  # rename model\n   nlp.to_disk(output_dir)\n   logging.info(\"Saved model to \"  + str(output_dir))\n   return nlp\n  else:\n   return nlp\n except Exception as e:\n  logging.exception(\"Unable to process \" + \"\\n\" + \"Error = \" + str(e))\n  return None\n# End trainModel\n`\nPlease suggest.\nEnvironment\nspaCy version: 2.1.8\nPlatform: Windows-10-10.0.16299-SP0\nPython version: 3.7.1", "issue_status": "Closed", "issue_reporting_time": "2019-10-02T02:44:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "298": {"issue_url": "https://github.com/explosion/spaCy/issues/4356", "issue_id": "#4356", "issue_summary": "noun_chunks yield inconsistent results between language models and sentences.", "issue_description": "ggloial commented on 2 Oct 2019\nDescription of the issue\nIs this a bug or feature request? If a bug, include all the steps that led to the issue.\nThis might be a bug.\nI want to extract the noun_chunks from the sentence 'the quick brown fox jumps over the lazy dog.'\nUsing the three different models for en_core_web_ return different results.\nI also modified the original text to 'the quick brown foxes jump over the lazy dog.'\nAnd got yet another set of results.\nI saw #3867 and #3886 which are closed but I think this use case surfaces a deeper issue: the plural form of a sentence interacts with the results as do the models (sm, md, lg)\nlang. model \\ text the quick brown fox jumps over the lazy dog. the quick brown foxes jump over the lazy dog.\nen_core_web_sm [ 'the lazy dog' ] [ 'the quick brown foxes', 'the lazy dog' ]\nen_core_web_md [ 'the quick brown fox', 'the lazy dog' ] [ 'the quick brown foxes', 'the lazy dog' ]\nen_core_web_lg [ 'the quick brown', 'jumps', 'the lazy dog' ] [ 'the quick brown', 'foxes', 'the lazy dog' ]\nSteps to reproduce:\nimport spacy\n\ntheText1 = '''the quick brown fox jumps over the lazy dog.'''\ntheText2 = '''the quick brown foxes jump over the lazy dog.'''\n\ntheLangModel = \"en_core_web_lg\" # \"en_core_web_sm\" # \"en_core_web_md\"\n\ntheNLP = spacy.load(theLangModel)\n\ntheDoc1 = theNLP(theText1)\ntheDoc2 = theNLP(theText2)\n\nprint(' | ', \"lang. model \\ text\", ' | ', theText1, ' | ', theText2, ' | ')\nprint(\"|---|---|---|\")\nprint(' | ', theLangModel, ' | '\n         , '[', ', '.join([\"'\"+theChunk.text+\"'\" for theChunk in theDoc1.noun_chunks]), ']', ' | '\n         , '[', ', '.join([\"'\"+theChunk.text+\"'\" for theChunk in theDoc2.noun_chunks]), ']', ' | ')\nYour Environment\npython -m spacy info --markdown\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.7.1\nModels: en\npython -m spacy validate\n\u2714 Loaded compatibility table\n====================== Installed models (spaCy v2.1.8) ======================\n\u2139 spaCy installation:\n/..../Entrust/venv/lib/python3.7/site-packages/spacy\nTYPE NAME MODEL VERSION\npackage en-pytt-xlnetbasecased-lg en_pytt_xlnetbasecased_lg 2.1.1 \u2714\npackage en-pytt-bertbaseuncased-lg en_pytt_bertbaseuncased_lg 2.1.1 \u2714\npackage en-core-web-sm en_core_web_sm 2.1.0 \u2714\npackage en-core-web-md en_core_web_md 2.1.0 \u2714\npackage en-core-web-lg en_core_web_lg 2.1.0 \u2714\nlink en en_core_web_sm 2.1.0 \u2714\nOperating System: MacOS Mojave (OSX 10.14.6)\nPython Version Used: 3.7\nspaCy Version Used: 2.1.8\nEnvironment Information: MacBook pro", "issue_status": "Closed", "issue_reporting_time": "2019-10-02T02:40:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "299": {"issue_url": "https://github.com/explosion/spaCy/issues/4355", "issue_id": "#4355", "issue_summary": "Hi,", "issue_description": "AbhayGodbole commented on 2 Oct 2019\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-10-02T02:05:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "300": {"issue_url": "https://github.com/explosion/spaCy/issues/4354", "issue_id": "#4354", "issue_summary": "NER is lumping 2 MONEY entities as one", "issue_description": "rshahrabani commented on 2 Oct 2019\nHi,\nNER is incorrectly combining 2 MONEY entities as one in the following sentence:\n\"Q2 2019 revenue decreased $1.2 million to $23.5 million, from $24.7 million in Q2 2019.\" I am using en_core_web_lg for processing the above.\nIs there a way to correct this and have separate entities for \"$1.2 million\" and \"$23.5 million\"? (NOTE: It is correctly identifying \"$24.7 million\" as a separate MONEY entity).\nThe fully list of entities that the application is extracting for this sentence is:\n[second quarter 2019, $1.2 million to $23.5 million, $24.7 million, second quarter 2019]\nThanks for your help,\nRonny\nYour Environment\nspaCy version: 2.1.4\nPlatform: Windows-10-10.0.18362-SP0\nPython version: 3.7.3\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-10-02T01:38:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "301": {"issue_url": "https://github.com/explosion/spaCy/issues/4352", "issue_id": "#4352", "issue_summary": "cannot merge disjoint spans'", "issue_description": "tnmcneil commented on 1 Oct 2019 \u2022\nedited\nI wrote a Token Matcher to capture 'measures' in my corpus- eg a number followed by a unit like 5g. I want to expand it to capture proportions like 5g/mL but am receiving the error: cannot merge disjoint spans because 5g is already part of the tokens to merge. is it possible to use nested patterns?\nYour Environment\nOperating System: Mac OS\nPython Version Used: 3.5.1\nspaCy Version Used: 2.1.6", "issue_status": "Closed", "issue_reporting_time": "2019-10-01T15:40:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "302": {"issue_url": "https://github.com/explosion/spaCy/issues/4350", "issue_id": "#4350", "issue_summary": "to_disk() error in custom pipeline component", "issue_description": "AnnaAnia commented on 1 Oct 2019\nHi,\nI have created a custom pipe with a to_disk() function to package up my model. This is however throwing an error.\nModel built in spacy 2.1.5\nPython 3.7\nError appearing in Python 3.6 and 3.7 and Spacy 2.1.4 - 2.1.6.\nI would appreciate some guidance on how to resolve it\nclass EntityOptimiser(object):\n    \n    name=\"entity_optimiser\"\n    \n    def __init__(self):\n        with open(r\"PATH\\word_search_keys_cmpny.json\", 'r') as f:\n            self.search_keys=json.load(f)\n        f.close()\n        self.spacy = spacy.load(r\"PATH\\en_core_web_sm-2.1.0\")\n        self.ent_list=[]\n\n    def __call__(self,doc):\n        doc=self.function_1(doc)\n        doc=self.function_2(doc)\n        return(doc)\n\n    def function_1(self,doc):\n        \"\"\" Do something to doc \"\"\"\n        return (doc)    \n\n    def function_2(self,doc):   \n        \"\"\" Do something to doc \"\"\"\n        return (doc)\n\n    def to_disk(self, path):\n        search_keys_path = path / \"search_keys.json\"\n        with search_keys_path.open(\"w\", encoding=\"utf8\") as f:\n            f.write(json.dumps(self.search_keys))\n        \n    def from_disk(self, path, **cfg):\n        search_keys_path = path / \"search_keys.json\"\n        with search_keys_path.open(\"r\", encoding=\"utf8\") as f:\n            self.search_keys = json.loads(f)\n        return self\nThe error I'm getting is:\n  File \"C:\\Anaconda351\\envs\\python_36\\lib\\site-packages\\spacy\\language.py\", line 779, in <lambda>\n    serializers[name] = lambda p, proc=proc: proc.to_disk(p, exclude=[\"vocab\"])\nTypeError: to_disk() got an unexpected keyword argument 'exclude'", "issue_status": "Closed", "issue_reporting_time": "2019-10-01T09:00:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "303": {"issue_url": "https://github.com/explosion/spaCy/issues/4349", "issue_id": "#4349", "issue_summary": "Exception When Loading SpaCy Models in Parallel Threads", "issue_description": "ericmclachlan commented on 1 Oct 2019 \u2022\nedited\nHow to Reproduce the Behavior\nFor my application, I'm loading 8 language models simultaneously. The models are loaded as follows:\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef init_nlp_provider(language):\n    logging.debug(f'Language with \"{language.language_code}\": Loading...')\n    language.init_nlp_provider()\n    logging.debug(f'Language with \"{language.language_code}\": Done')\n    \ndef initialize_languages() -> List[ALanguage]:\n    languages = [\n        DutchLanguage(),\n        EnglishLanguage(),\n        FrenchLanguage(),\n        GermanLanguage(),\n        GreekLanguage(),\n        ItalianLanguage(),\n        PortugueseLanguage(),\n        SpanishLanguage(),\n    ]\n    with ThreadPoolExecutor(max_workers=1) as executor:\n        for language in languages:\n            executor.submit(init_nlp_provider, language)\n    \n    return languages\nEach of the language wrappers above quickly delegates to spacy.load(...) as illustrated by the implementation for the EnglishLanguage class below:\n    def init_nlp_provider(self):\n        self._nlp_provider = spacy.load('en_core_web_sm')\nThis all works well when max_workers=1; however it takes almost 10 minutes to load all language models.\nThe real problem starts when I set max_workers to 2 or more. This action results in the following exception:\nUndefined operator: >>\n  Called by (<thinc.neural._classes.function_layer.FunctionLayer object at 0x000001BAFD3CCD08>, <thinc.neural._classes.feed_forward.FeedForward object at 0x000001BAFD3833C8>)\n  Available: \n\n  Traceback:\n  \u251c\u2500 <lambda> [782] in C:\\Python37\\lib\\site-packages\\spacy\\language.py\n  \u251c\u2500\u2500\u2500 from_disk [611] in C:\\Python37\\lib\\site-packages\\spacy\\util.py\n  \u2514\u2500\u2500\u2500\u2500\u2500 build_tagger_model [511] in C:\\Python37\\lib\\site-packages\\spacy\\_ml.py\n         >>> pretrained_vectors=pretrained_vectors,\nYour Environment\nspaCy version: 2.1.3\nPlatform: Windows-10-10.0.18990-SP0\nPython version: 3.7.4\nSpecific Questions\nShould it be possible to load all language models concurrently as I've tried to do above?\nAnd, if not, what is the sub-10-minute solution to loading all language models?", "issue_status": "Closed", "issue_reporting_time": "2019-10-01T09:00:11Z", "fixed_by": "explosion/thinc#124", "pull_request_summary": "TODO: Finish English morphology table, in lang_data/en/morphs.json , to move forward on multi-lingual", "pull_request_description": "Member\nhonnibal commented on 3 Oct 2015\nThe English morphology table needs to be finished, in the language-independent format. This is blocking multi-lingual development, because we need to have the English data in standard format to serve as an example reference.\nSee here: UniversalDependencies/docs#212\nAnd here: http://spacy.io/tutorials/add-a-language/\nDocumentation on morphological scheme\nhttp://universaldependencies.github.io/docs/\nUseful search tool for reference: http://bionlp-www.utu.fi/dep_search/?db=English&search=Mine%7Cmine+%3Cnsubj+_\nThe morphological tables and lemmatization are the main pieces of language-specific work that need to be completed to support each new language. The code has now been updated to be language neutral.\nSome morphologically rich languages may challenge the current architecture, and require exceptional processing. The code is set up to allow each component to be subclassed easily, to support special-case pipelines.\nLinguistic Preliminaries\nInflectional morphology is the process by which a root form of a word is modified by adding prefixes or suffixes that specify its grammatical function but do not changes its part-of-speech.\nWe say that a lemma (root form) is inflected (modified/combined) with one or more morphological features to create a surface form.\nExamples:\nContext: I was reading the paper\nLemma: read\nPart of Speech: Verb\nMorphological Features: VerbForm=Ger\nSurface Form: reading\nContext: I don't watch the news, I read the paper.\nLemma: read\nPart of Speech: Verb\nMorphological Features: VerbForm=Fin, Mood=Ind, Tense=Pres\nSurface Form: read:\nContext: I read the paper yesteday\nLemma: read\nPart of Speech: Verb\nMorphological Features: VerbForm=Fin, Mood=Ind, Tense=Past\nSurface Form: read\nNote that the same lemma and different morphological features can be expressed by the same surface form, especially when written (the past tense of \"read\" is distinct in speech from the base form and present, but is written identically).\nSome languages are more regular than others in how surface forms are constructed from lemmas and morphemes. As far as I'm aware, all languages have at least a few irregular words, where the surface forms must be enumerated, and cannot be generated by the usual process.\nMorphology in spaCy\nspaCy's morphological processing proceeds as follows:\nDuring tokenization, the tokenizer consults a mapping table specials.json, which allows sequences of characters to be mapped to multiple tokens. Each token may be assigned a part of speech and one or more morphological features.\nThe part-of-speech tagger then assigns each token an extended POS tag. In the API, these tags are known as .tag. For now we will call these XPOS. An XPOS tag expresses the part-of-speech (e.g. VERB) and some amount of morphological information, e.g. that the verb is in the ING form. The set of XPOS tags differs by language, and represents a compromise between desired detail and economy of representation.\nA mapping table morphs.json is then consulted, which maps (surface form, XPOS) to (lemma, POS, morphological features). This table allows exceptional cases to be handled, where the specific surface form and XPOS return a result that can't be captured by generalized rules.\nFor words whose POS is not set by a prior process, a mapping table tag_map.json maps the XPOS to (POS, morphological features).\nFinally, a rule-based deterministic lemmatizer maps (surface form, XPOS, POS, morphological features) to (surface form, XPOS, lemma, POS, morphological features), without consulting the context of the token. Currently the lemmatizer also accepts list-based exception files, acquired from WordNet, for words whose surface form-to-lemma mapping is unpredictable, but where the XPOS --> (POS, morphological features) mapping is predictable. This should be changed --- these exceptions should be moved into the morphs.json as well.\nPreviously in spaCy\nAt launch, spaCy used an ad hoc set of morphological features, optimized for English. A small set of union features were added. This was a placeholder for a more formal scheme.\nSince I developed that in 2014, the Universal Dependencies and Interset projects have continued, with impressive results. spaCy is now adopting their morphological schema. (Eventually we should also adopt the UD syntax, at first as an alternate representation, and eventually as the standard form. We will probably want to learn an intermediate representation and map into UD as a post-process.)\nWhat needs to be done\nFinish the English morphs.json, found in lang_data/en/morphs.json The pronouns are reasonably complete. We need the auxiliaries next, and exceptional comparative adjectives (e.g. better, best etc). Finally, the noun.exc, verb.exc etc files from WordNet should be merged into morphs.json.\nCreate a German morphs.json, currently stubbed out in lang_data/de/morphs.json.\nUpdate the lemmatizer to avoid using the old exception files, and to allow more flexible rule writing.", "pull_request_status": "Merged", "issue_fixed_time": "2015-10-03T03:19:23Z", "files_changed": []}, "304": {"issue_url": "https://github.com/explosion/spaCy/issues/4348", "issue_id": "#4348", "issue_summary": "Getting IndexError: list index out of range", "issue_description": "AbhayGodbole commented on 1 Oct 2019 \u2022\nedited by ines\nHi\nI am trying to train the new NER model. Getting following error while calling \" nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\nlosses=losses)\"\nINFO : Created blank 'en' model\nERROR : Unable to process \nError = list index out of range\nTraceback (most recent call last):\n  File \"..\\entityextractor\\model.py\", line 84, in trainModel\n    losses=losses)\n  File \"c:\\abhay\\ai\\ris-auditsupport\\virris-auditsupport\\lib\\site-packages\\spacy\\language.py\", line 475, in update\n    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n  File \"nn_parser.pyx\", line 418, in spacy.syntax.nn_parser.Parser.update\n  File \"_parser_model.pyx\", line 214, in spacy.syntax._parser_model.ParserModel.begin_update\n  File \"_parser_model.pyx\", line 262, in spacy.syntax._parser_model.ParserStepModel.__init__\n  File \"c:\\abhay\\ai\\ris-auditsupport\\virris-auditsupport\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 46, in begin_update\n    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n  File \"c:\\abhay\\ai\\ris-auditsupport\\virris-auditsupport\\lib\\site-packages\\thinc\\api.py\", line 295, in begin_update\n    X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad), drop=drop)\n  File \"ops.pyx\", line 113, in thinc.neural.ops.Ops.flatten\nIndexError: list index out of range\nIn [ ]:\nI am following a standard process, which worked for my earlier project.\nI have .tsv annotated fie, which currently have only one Entity annotated and rest is marked as \"O\"\nconverting this tsv to json\nconverting this json to spacy required format.\npassing this file to train the model.\nLooks like, there is some issue with the data, which I am not able to figure out, as if I provide my previous json, its working fine.\nPlease help\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Windows-10-10.0.16299-SP0\nPython version: 3.7.1", "issue_status": "Closed", "issue_reporting_time": "2019-10-01T06:28:31Z", "fixed_by": "#4360", "pull_request_summary": "Ensure training doesn't crash with empty batches", "pull_request_description": "Member\nsvlandeg commented on 2 Oct 2019\nWhen providing an empty batch of training data, down the line thinc would throw an error. This is prevented by catching the empty batch early on and not do any training/prediction on it.\nFixes #4348. Also wrote a unit test for Issue #3456 which was kind of related.\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-02T10:50:48Z", "files_changed": [["13", "spacy/pipeline/pipes.pyx"], ["8", "spacy/tests/regression/test_issue3001-3500.py"], ["23", "spacy/tests/regression/test_issue4348.py"]]}, "305": {"issue_url": "https://github.com/explosion/spaCy/issues/4347", "issue_id": "#4347", "issue_summary": "Spacy Entity Rule doesn't work for cardinal (Social Security number)", "issue_description": "sgsmittal commented on 1 Oct 2019 \u2022\nedited\nI have used Entity Rule to add new label for social security number. I even set overwrite_ents=true but it still does't recognize\nI verified regular expression is correct. not sure what else I need to do I tried before=\"ner\" but same result\ntext = \"My name is ddex and I leave on 605 W Clinton Street. My social security 690-96-4032\"\nnlp = spacy.load(\"en_core_web_sm\")\nruler = EntityRuler(nlp, overwrite_ents=True)\nruler.add_patterns([{\"label\": \"SSN\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"\\d{3}[^\\w]\\d{2}[^\\w]\\d{4}\"}}]}])\nnlp.add_pipe(ruler)\ndoc = nlp(text)\nfor ent in doc.ents: print(\"{} {}\".format(ent.text, ent.label_))", "issue_status": "Closed", "issue_reporting_time": "2019-09-30T21:44:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "306": {"issue_url": "https://github.com/explosion/spaCy/issues/4342", "issue_id": "#4342", "issue_summary": "Simplify adding new component to existing model with CLI", "issue_description": "Collaborator\nadrianeboyd commented on 30 Sep 2019\nFeature description\nIt looks like these kinds of cases aren't easy to handle with the train CLI:\nI want to train a model from scratch given vectors, one corpus for the tagger, and one corpus for the parser. (See example in #4306.)\nI have a model with a tagger and want to add a parser trained on a separate corpus.\nFor the internal spacy models, it looks like each component is trained separately and then they are combined using custom scripts.\nCould it make sense to have a CLI component that combines models/components (with compatibility checks, of course)?\nCase (2) is really just a minor variant of (1), but the train CLI might be able to handle it relatively easily by combining the components in model-final, for example. I think case (1) could be handled by the train CLI in theory, but the command-line options would get too complicated and it would be easier to handle it with a separate CLI command.\n(What is the right way to combine the vocab directories (aside from vectors) for multiple models?)\n2", "issue_status": "Closed", "issue_reporting_time": "2019-09-30T07:14:29Z", "fixed_by": "#4911", "pull_request_summary": "Improve train CLI with base model", "pull_request_description": "Collaborator\nadrianeboyd commented 16 days ago \u2022\nedited\nDescription\nImprove train CLI with a provided base model so that you can:\nadd a new component\nextend an existing component\nreplace an existing component\nWhen the final model and best model are saved, disabled components are reenabled and the meta information is merged to include the full pipeline and accuracy information for all components in the base model plus the newly added components if needed.\nFixes #4342.\nTypes of change\nEnhancement.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-16T00:58:51Z", "files_changed": [["63", "spacy/cli/train.py"]]}, "307": {"issue_url": "https://github.com/explosion/spaCy/issues/4341", "issue_id": "#4341", "issue_summary": "Spanish parsing of verbs gets wrong lemma if the verb is start of sentence", "issue_description": "sontek commented on 30 Sep 2019\nHow to reproduce the behaviour\n>>> import spacy\n>>> nlp = spacy.load('es')\n>>> sentence = 'Vea nom\u00e1s, qu\u00e9 bonito, t\u00eda.'\n>>> doc = nlp(sentence)\n>>> for token in doc:\n...     print(token.text, token.lemma_, token.pos_)\n...\nVea Vea NOUN\nnom\u00e1s nom\u00e1s ADV\n, , PUNCT\nqu\u00e9 qu\u00e9 DET\nbonito bonito NOUN\n, , PUNCT\nt\u00eda t\u00edo NOUN\n. . PUNCT\nYou can see \"Vea\" was considered a now when it should've been the verb \"Ver\". I can fix this by lowering the string to make sure it parses it correctly, but this causes issues that I reported in #4340\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Darwin-18.0.0-x86_64-i386-64bit\nPython version: 3.6.7\nModels: es\nOperating System: OSX\nPython Version Used: 3.6", "issue_status": "Closed", "issue_reporting_time": "2019-09-29T20:49:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "308": {"issue_url": "https://github.com/explosion/spaCy/issues/4340", "issue_id": "#4340", "issue_summary": "Spanish parsing lower case \"no.\" doesn't work. \"No.\" does for tokenizing", "issue_description": "sontek commented on 30 Sep 2019\nHow to reproduce the behaviour\nWith lower case:\n>>> import spacy\n>>> nlp = spacy.load('es')\n>>> sentence = \"no.\"\n>>> doc = nlp(sentence)\n>>> for token in doc:\n...     print(token.text, token.lemma_, token.pos_)\n...\nno. no. ADJ\n>>>\nwith uppercase:\n>>> sentence = \"No.\"\n>>> doc = nlp(sentence)\n>>> for token in doc:\n...     print(token.text, token.lemma_, token.pos_)\n...\nNo No ADV\n. . PUNCT\nI don't expect punctuation to ever be included in my tokens.\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Darwin-18.0.0-x86_64-i386-64bit\nPython version: 3.6.7\nModels: es\nOperating System: OSX\nPython Version Used: 3.6", "issue_status": "Closed", "issue_reporting_time": "2019-09-29T20:35:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "309": {"issue_url": "https://github.com/explosion/spaCy/issues/4337", "issue_id": "#4337", "issue_summary": "Are there any plans to introduce type hints ?", "issue_description": "Contributor\ntamuhey commented on 29 Sep 2019 \u2022\nedited\nFeature description\nWhich area of the library is it related to?\nall\nWhat specific solution would you like?\nAdd type hints in code base, or add stub files (.pyi)", "issue_status": "Closed", "issue_reporting_time": "2019-09-29T11:48:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "310": {"issue_url": "https://github.com/explosion/spaCy/issues/4333", "issue_id": "#4333", "issue_summary": "PhraseMatcher Silently Fails After Pickling", "issue_description": "Contributor\naaronkub commented on 28 Sep 2019\nHow to reproduce the behaviour\nHello and thank you very much for this beautiful library. :)\nI am using spaCy in a PySpark application and am finding that the PhraseMatcher behaves differently than when I test locally. I believe the root cause is that I'm pickling the Language (nlp) object and deserializing on the workers.\nFrom what I can tell this is the same issue raised in #1939 and #3248.\nThe below code fails with python 3.6.9 and spaCy 2.1.8\nimport pickle\nimport spacy\nfrom spacy.matcher import PhraseMatcher\n\nnlp = spacy.load('en_core_web_sm')\n\nmatcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\nmatcher.add(\"test_words\", None, *[nlp.make_doc(\"random\")])\n\ntest_doc = nlp(\"This is a Random phrase.\")\n\nprint(\"Original Matcher Found {} matches.\"\n .format(len(matcher(test_doc)))) # 1 match\n\nwith open(\"matcher.pickle\", 'wb') as tmp:\n    pickle.dump(matcher, tmp)\n    \nwith open(\"matcher.pickle\", 'rb') as tmp:\n    unpickled_matcher = pickle.load(tmp)\n\nprint(\"Unpickled Matcher Found {} matches.\"\n .format(len(unpickled_matcher(test_doc)))) # 0 matches\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.9", "issue_status": "Closed", "issue_reporting_time": "2019-09-28T02:59:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "311": {"issue_url": "https://github.com/explosion/spaCy/issues/4332", "issue_id": "#4332", "issue_summary": "Incorrect lemma for plural of lens", "issue_description": "ewaldatsensentia commented on 28 Sep 2019\nSpaCy thinks the lemma of \"lenses\" is \"lense\" (not even a word in English.)\nimport spacy\nnlp = spacy.load('en_core_web_md')\ndoc = nlp(\"lenses\")\nprint(doc[0].lemma_)\nlense\nYour Environment\nspaCy version 2.1.8\nWindows-10-10.0.18362-SP0\nPython 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-09-27T19:56:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "312": {"issue_url": "https://github.com/explosion/spaCy/issues/4329", "issue_id": "#4329", "issue_summary": "Failure with huge texts due to unbounded memory usage by get_lca_matrix.", "issue_description": "Contributor\nalvaroabascar commented on 27 Sep 2019\nThe current implementation of get_lca_matrix was done by myself, so apologies for the following bug: on really big texts, get_lca_matrix produces a matrix with size (len(doc), len(doc)). This means RAM consumption will be O(n*n) and unbounded, which can produce MemoryError (it has happened to me with an abnormal text in a production environment).\nI think we should modify the function, maybe using a sparse matrix to avoid this problem.\nHow to reproduce the behaviour\nRun top or htop in a terminal.\nRun the following python code.\nimport spacy\nnlp = spacy.load('en')\ntext = 'word ' * 100000\nnlp(text)\nSee how your computer explodes.\nYour Environment\nSpacy 2.0.17, linux.", "issue_status": "Closed", "issue_reporting_time": "2019-09-27T12:41:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "313": {"issue_url": "https://github.com/explosion/spaCy/issues/4328", "issue_id": "#4328", "issue_summary": "Is there any way to use a model trained with previous version of spaCy on its newest version?", "issue_description": "tamarit commented on 27 Sep 2019\nI have been googling for a while but I 've not found anything which was close to this question. I suppose that due to several internal changes or models characteristics/technology/approach there is no way to reuse our models which were trained on previous spaCy version (1.8.*). However, we would like to be sure just in case we could avoid to recompute all our models.\nThanks in advance!", "issue_status": "Closed", "issue_reporting_time": "2019-09-27T12:28:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "314": {"issue_url": "https://github.com/explosion/spaCy/issues/4324", "issue_id": "#4324", "issue_summary": "Install fails on Windows 10", "issue_description": "JuanFF commented on 26 Sep 2019\nHow to reproduce the problem\npip install -U spacy\nCollecting spacy\n  Using cached https://files.pythonhosted.org/packages/58/f2/5a23bb7251988da474eec844b692760cb0a317912291afc77b516f399cff/spacy-2.1.8.tar.gz\n  Installing build dependencies ... error\n  ERROR: Command errored out with exit status 1:\n   command: 'c:\\users\\ju.fernandez\\appdata\\local\\programs\\python\\python37-32\\python.exe' 'c:\\users\\ju.fernandez\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pip' install --ignore-installed --no-user --prefix 'C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-build-env-r3z8nhkz\\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'wheel>0.32.0,<0.33.0' Cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=7.0.8,<7.1.0'\n       cwd: None\n  Complete output (62 lines):\n  Collecting setuptools\n    Using cached https://files.pythonhosted.org/packages/b2/86/095d2f7829badc207c893dd4ac767e871f6cd547145df797ea26baea4e2e/setuptools-41.2.0-py2.py3-none-any.whl\n  Collecting wheel<0.33.0,>0.32.0\n    Using cached https://files.pythonhosted.org/packages/ff/47/1dfa4795e24fd6f93d5d58602dd716c3f101cfd5a77cd9acbe519b44a0a9/wheel-0.32.3-py2.py3-none-any.whl\n  Collecting Cython\n    Using cached https://files.pythonhosted.org/packages/ba/08/4f0d09d63b713955f9c6937923f1b1432331f468912b65824b19b9d82d19/Cython-0.29.13-cp37-cp37m-win32.whl\n  Collecting cymem<2.1.0,>=2.0.2\n    Using cached https://files.pythonhosted.org/packages/eb/cb/4ff546a491f764f67284572d25c57927e3f17103adf979bc99d90128f3eb/cymem-2.0.2-cp37-cp37m-win32.whl\n  Collecting preshed<2.1.0,>=2.0.1\n    Using cached https://files.pythonhosted.org/packages/00/52/ef641ebb40e287b95e9742e7f3120dca0350d92b3d0ef050e5133acf8931/preshed-2.0.1-cp37-cp37m-win32.whl\n  Collecting murmurhash<1.1.0,>=0.28.0\n    Using cached https://files.pythonhosted.org/packages/22/e9/411be1845f1ac07ae3bc40a4b19ba401819baed4fa63b4f5ef28b2300eb4/murmurhash-1.0.2.tar.gz\n  Collecting thinc<7.1.0,>=7.0.8\n    Using cached https://files.pythonhosted.org/packages/92/39/ea2a3d5b87fd52fc865fd1ceb7b91dca1f85e227d53e7a086d260f6bcb93/thinc-7.0.8.tar.gz\n  Collecting blis<0.3.0,>=0.2.1 (from thinc<7.1.0,>=7.0.8)\n    Using cached https://files.pythonhosted.org/packages/59/9e/84a83616cbe5daa94909da38b780e93bf566dc2113c3dc35d7b4cad52f63/blis-0.2.4.tar.gz\n  Collecting wasabi<1.1.0,>=0.0.9 (from thinc<7.1.0,>=7.0.8)\n    Using cached https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n  Collecting srsly<1.1.0,>=0.0.6 (from thinc<7.1.0,>=7.0.8)\n    Using cached https://files.pythonhosted.org/packages/b0/63/b68061954228346cbab2c41adb36339678605c47da016f5c71c7ef65f510/srsly-0.1.0.tar.gz\n  Collecting numpy>=1.7.0 (from thinc<7.1.0,>=7.0.8)\n    Using cached https://files.pythonhosted.org/packages/a8/ce/36f9b4fbc7e675a7c8a3809dd5902e24cecfcdbc006e8a7b2417c2b830a2/numpy-1.17.2-cp37-cp37m-win32.whl\n  Collecting plac<1.0.0,>=0.9.6 (from thinc<7.1.0,>=7.0.8)\n    Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n  Collecting tqdm<5.0.0,>=4.10.0 (from thinc<7.1.0,>=7.0.8)\n    Using cached https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl\n  Installing collected packages: setuptools, wheel, Cython, cymem, preshed, murmurhash, numpy, blis, wasabi, srsly, plac, tqdm, thinc\n    Running setup.py install for murmurhash: started\n      Running setup.py install for murmurhash: finished with status 'done'\n    Running setup.py install for blis: started\n      Running setup.py install for blis: finished with status 'error'\n      ERROR: Command errored out with exit status 1:\n       command: 'c:\\users\\ju.fernandez\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\JUF073~1.FER\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w_w0v7kl\\\\blis\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\JUF073~1.FER\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w_w0v7kl\\\\blis\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-record-0drxda12\\install-record.txt' --single-version-externally-managed --prefix 'C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-build-env-r3z8nhkz\\overlay' --compile\n           cwd: C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-install-w_w0v7kl\\blis\\\n      Complete output (25 lines):\n      BLIS_COMPILER? None\n      running install\n      running build\n      running build_py\n      creating build\n      creating build\\lib.win32-3.7\n      creating build\\lib.win32-3.7\\blis\n      copying blis\\about.py -> build\\lib.win32-3.7\\blis\n      copying blis\\benchmark.py -> build\\lib.win32-3.7\\blis\n      copying blis\\__init__.py -> build\\lib.win32-3.7\\blis\n      creating build\\lib.win32-3.7\\blis\\tests\n      copying blis\\tests\\common.py -> build\\lib.win32-3.7\\blis\\tests\n      copying blis\\tests\\test_dotv.py -> build\\lib.win32-3.7\\blis\\tests\n      copying blis\\tests\\test_gemm.py -> build\\lib.win32-3.7\\blis\\tests\n      copying blis\\tests\\__init__.py -> build\\lib.win32-3.7\\blis\\tests\n      copying blis\\cy.pyx -> build\\lib.win32-3.7\\blis\n      copying blis\\py.pyx -> build\\lib.win32-3.7\\blis\n      copying blis\\cy.pxd -> build\\lib.win32-3.7\\blis\n      copying blis\\__init__.pxd -> build\\lib.win32-3.7\\blis\n      running build_ext\n      error: [WinError 2] El sistema no puede encontrar el archivo especificado\n      msvc\n      py_compiler msvc\n      {'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'HOSTTYPE': 'x86_64', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'LANG': 'C.UTF-8', 'OLDPWD': '/home/matt/repos/flame-blis', 'VIRTUAL_ENV': '/home/matt/repos/cython-blis/env3.6', 'USER': 'matt', 'PWD': '/home/matt/repos/cython-blis', 'HOME': '/home/matt', 'NAME': 'LAPTOP-OMKOB3VM', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SHELL': '/bin/bash', 'TERM': 'xterm-256color', 'SHLVL': '1', 'LOGNAME': 'matt', 'PATH': '/home/matt/repos/cython-blis/env3.6/bin:/tmp/google-cloud-sdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/Python37/Scripts:/mnt/c/Python37:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/WiFi/bin:/mnt/c/Program Files/Common Files/Intel/WirelessCommon:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/LLVM/bin:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/nodejs:/mnt/c/Users/matt/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/matt/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/matt/AppData/Roaming/npm:/snap/bin:/mnt/c/Program Files/Oracle/VirtualBox', 'PS1': '(env3.6) \\\\[\\\\e]0;\\\\u@\\\\h: \\\\w\\\\a\\\\]${debian_chroot:+($debian_chroot)}\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\]:\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ ', 'VAGRANT_HOME': '/home/matt/.vagrant.d/', 'LESSOPEN': '| /usr/bin/lesspipe %s', '_': '/home/matt/repos/cython-blis/env3.6/bin/python'}\n      clang -c C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-install-w_w0v7kl\\blis\\blis\\_src\\config\\bulldozer\\bli_cntx_init_bulldozer.c -o C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\tmp1xw1o0r7\\bli_cntx_init_bulldozer.o -O2 -funroll-all-loops -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude\\windows-x86_64 -I.\\frame\\3\\ -I.\\frame\\ind\\ukernels\\ -I.\\frame\\1m\\ -I.\\frame\\1f\\ -I.\\frame\\1\\ -I.\\frame\\include -IC:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-install-w_w0v7kl\\blis\\blis\\_src\\include\\windows-x86_64\n      ----------------------------------------\n  ERROR: Command errored out with exit status 1: 'c:\\users\\ju.fernandez\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\JUF073~1.FER\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w_w0v7kl\\\\blis\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\JUF073~1.FER\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w_w0v7kl\\\\blis\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-record-0drxda12\\install-record.txt' --single-version-externally-managed --prefix 'C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-build-env-r3z8nhkz\\overlay' --compile Check the logs for full command output.\n  ----------------------------------------\nERROR: Command errored out with exit status 1: 'c:\\users\\ju.fernandez\\appdata\\local\\programs\\python\\python37-32\\python.exe' 'c:\\users\\ju.fernandez\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pip' install --ignore-installed --no-user --prefix 'C:\\Users\\JUF073~1.FER\\AppData\\Local\\Temp\\pip-build-env-r3z8nhkz\\overlay' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'wheel>0.32.0,<0.33.0' Cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=7.0.8,<7.1.0' Check the logs for full command output.\nYour Environment\nWindows 10\nPython 3.7.4", "issue_status": "Closed", "issue_reporting_time": "2019-09-26T08:46:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "315": {"issue_url": "https://github.com/explosion/spaCy/issues/4323", "issue_id": "#4323", "issue_summary": "Spacy installation error", "issue_description": "harish2sista commented on 26 Sep 2019\nHow to reproduce the problem\npython -m spacy info --markdown\nTraceback (most recent call last):\n  File \"/Users/harish2sista/anaconda2/lib/python2.7/runpy.py\", line 163, in _run_module_as_main\n    mod_name, _Error)\n  File \"/Users/harish2sista/anaconda2/lib/python2.7/runpy.py\", line 111, in _get_module_details\n    __import__(mod_name)  # Do not catch exceptions initializing package\n  File \"/Users/harish2sista/anaconda2/lib/python2.7/site-packages/spacy/__init__.py\", line 20, in <module>\n    raise SystemError(Errors.E130)\nSystemError: [E130] You are running a narrow unicode build, which is incompatible with spacy >= 2.1.0. To fix this, reinstall Python and use a wide unicode build instead. You can also rebuild Python and set the --enable-unicode=ucs4 flag.\nYour Environment\nOperating System: Mac OS\nPython Version Used: 2.7\nspaCy Version Used: 2.1.0\nEnvironment Information: Mojave 10.14.6", "issue_status": "Closed", "issue_reporting_time": "2019-09-26T07:18:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "316": {"issue_url": "https://github.com/explosion/spaCy/issues/4319", "issue_id": "#4319", "issue_summary": "Glove embedding for multi-word phrases", "issue_description": "AishwaryaAgrawal commented on 25 Sep 2019\nHi there,\nI used spacy to load glove vectors as below --\nglove_vectors = spacy.load('en', vectors='en_glove_cc_300_1m_vectors')\nand then to obtain glove embeddings for phrases, as below --\nconcept = 'red and white'\nembedding = glove_vectors(u'%s' %concept).vector\nThe embedding returned from above is a 300 dim vector. But I could not find out how multi word phrases are dealt under the hood. Could someone explain me this?\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-09-24T19:06:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "317": {"issue_url": "https://github.com/explosion/spaCy/issues/4318", "issue_id": "#4318", "issue_summary": "csv to spacy format", "issue_description": "zakarianamikaz commented on 24 Sep 2019\nhow can i convert a CSV NER file (BIO) to spacy json format\nYour Environment\nOperating System:\nPython Version Used:3.7\nspaCy Version Used:2.0\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-09-24T10:35:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "318": {"issue_url": "https://github.com/explosion/spaCy/issues/4313", "issue_id": "#4313", "issue_summary": "(re-opening bug) thinc.extra.search.Beam.advance has assertion error with custom entity labels", "issue_description": "sshegheva commented on 20 Sep 2019\nI couldn't figure out how to re-open an issue, so I am creating a new one with a link to the previous #3047.\nThe behavior is still present.\nBasically, here is the line where the assertion is triggered.\nhttps://github.com/explosion/thinc/blob/master/thinc/extra/search.pyx#L149\nI can't quite figure out from the code what is the variable size trying to represent here. Perhaps, I can do some workaround, but I need to understand the root cause.\nAlso, if there is a different recipe for getting the confidence on the extracted entities, I am happy to consider that (I haven't found an alternative)", "issue_status": "Closed", "issue_reporting_time": "2019-09-20T18:26:39Z", "fixed_by": "#4330", "pull_request_summary": "Ensure the NER remains consistent after resizing", "pull_request_description": "Member\nsvlandeg commented on 27 Sep 2019 \u2022\nedited\nDescription\nWhen the ner got resized through an add_action(), the nn_parser would have an outdated field nr_class sitting in its cfg. So then when you write to disk this resized NER, it would throw an error about invalid dimensions when reading it back in, because it was using the outdated nr_class value.\nThe solution is to ensure that the resizing is done consistently by always calling the self._resize() method that updates both the model(s) as well as the nr_class variable. (it's still not a very clean solution ...)\nI encountered this behaviour first in Issue #4042, which actually harboured two bugs. The one described above was the second bug and can be tested in isolation with the unit test test_issue4042_bug2().\nI took the liberty of fixing the first bug of that issue too, which is a workaround for applying nlp() within the EntityLinker by disabling components later on in the pipeline (as suggested by @ines). I know that ideally we shouldn't use this pattern, but at least this is a quick fix, and the combination effectively closes #4042.\nI started to look into the internal behaviour of the NER because of Issue #4313 , which I feel should be related. I wrote a unit test for this issue, but it unfortunately still crashes so not quite there yet. I put it on skip for now so we could merge this at least, and continue the search. Removed that unit test, because it uses an external lib.\n[UPDATE:] also fixes #4313 with Matt's suggestion.\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-27T18:57:14Z", "files_changed": [["37", "spacy/pipeline/entityruler.py"], ["21", "spacy/syntax/nn_parser.pyx"], ["7", "spacy/syntax/transition_system.pyx"], ["83", "spacy/tests/regression/test_issue4042.py"], ["39", "spacy/tests/regression/test_issue4313.py"]]}, "319": {"issue_url": "https://github.com/explosion/spaCy/issues/4312", "issue_id": "#4312", "issue_summary": "ValueError when loading e_core_web_sm model", "issue_description": "Monida commented on 19 Sep 2019\nHi,\nI have a problem when loading the en_core_web_sm model. These are the steps I followed that lead to the error.\nI installed spacy using\nconda install -c conda-forge spacy\nfollowing instructions form here. And the en_core_web_sm model using:\nconda install -c conda-forge spacy-model-en_core_web_sm\nfollowing the instructions here.\nThen I import spacy and the en_core_web_sm model in Spyder (Python 3.6) with no issues:\nimport spacy\nimport en_core_web_sm\nHowever, when I do either:\nnlp = spacy.load('en_core_web_sm')\nor\nnlp = en_core_web_sm.load()\nI got the following message:\n\"ValueError: could not broadcast input array from shape (96) into shape (128)\"\nI read issues #3113 and #3226, but they don't really resemble my problem. I can't find any other resources on the internet to help me solve this issue. Does anyone have any idea of how to fix this?\nYour Environment\nOperating System: Windows x64\nPython Version Used: Python3.6\nspaCy Version Used: 2.0.16\nEnvironment Information: Running Spyder3 on Miniconda3", "issue_status": "Closed", "issue_reporting_time": "2019-09-19T16:32:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "320": {"issue_url": "https://github.com/explosion/spaCy/issues/4311", "issue_id": "#4311", "issue_summary": "Make configuration operations return new objects? (or least add_pipe)", "issue_description": "stuz5000 commented on 19 Sep 2019\nIt looks like the steps to load a model and attach a pipline are both expensive in time and mutating. This makes it very expensive to construct a suite of tests that runs many pipeline variants with the same base model, and also tedious within the REPL when a minor change cause the need to reload a model.\nIt would be great if:\nwe could assume the model to be immutable\noperations to construct new pipeline didn't require the expensive reload of a model and instead returned an object with a modified pipeline.\nFor example:\n@cached\ndef load_model(modelname):\n    # Time consuming load, just once and user cached it\n    return spacy.load(modelname)\n\n#@cached  # Not needed\ndef build_pipeline(modelname, feature1=False, feature2=False, ...):\n    nlp = load_model(modelname)\n    if feature1:\n        nlp = nlp.add_pipe( Feature1(), in_place=False  )  <<<<< in_place=False, returns new object\n    if feature2:\n        nlp = nlp.add_pipe( Feature2(), in_place=False )\n     ...\n    return nlp\n\ndef test1():\n    # First call is slow\n    nlp = build_pipeline(modelname, feature1=False, feature2=False, ...)\n    ...\n\ndef test2():\n    # Fast\n    nlp = build_pipeline(modelname, feature1=True, feature2=False, ...):\n\ndef test3():\n    # Fast\n    nlp = build_pipeline(modelname, feature1=False, feature2=False, ...):\n...\n^^^ In this example, all that's needed is for add_pipe to return an nlp object with a pipeline with the added feature, leaving the original nlp object unchanged. Pandas back-fitted this pattern by adding and in_place parameter to decide if the instance should be modified, or a new instance returned - seems to work OK.\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2019-09-19T15:00:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "321": {"issue_url": "https://github.com/explosion/spaCy/issues/4310", "issue_id": "#4310", "issue_summary": "Using en_pytt_bertbaseuncased_lg with spacy pretrain", "issue_description": "haroonhassan commented on 19 Sep 2019\nHow to reproduce the problem\n# copy-paste the error message here\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-09-19T14:52:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "322": {"issue_url": "https://github.com/explosion/spaCy/issues/4308", "issue_id": "#4308", "issue_summary": "PhraseMatcher starts missing matches if lots of new vocab has been added", "issue_description": "Collaborator\nadrianeboyd commented on 19 Sep 2019\nHow to reproduce the behaviour\nYou need this keywords file: https://github.com/mpuig/spacy-lookup/blob/master/data/keywords.txt\nIf you process the sample keywords before doing all the extra processing, the match is found. You can also process the sample keywords again after all the extra processing and the match is still found.\nIf you load the sample keywords after doing all the extra processing (as below) and keyword_limit is at least 10067, the match is not found. Any lower, the match is found.\nI think it's probably something related to the Vocab and not PhraseMatcher itself, but I really don't know?\nimport spacy\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.matcher import Matcher\n\ndef run_matching(matcher, docs):\n    matches = []\n    for doc in docs:\n        matches.extend([match for match in matcher(doc)])\n\n    return (len(matches), matches)\n\ndef run(with_extra_keyword_processing=False):\n    print(\"\\n\\nRunning test\", \"*with*\" if with_extra_keyword_processing else \"*without*\", \"extra document processing\")\n\n    keyword_limit = 10067\n\n    nlp = spacy.load('en')\n\n    if with_extra_keyword_processing:\n        print(\"Processing a lot of keywords, then discarding...\")\n        unused_keywords = []\n        count = 0\n        with open(\"keywords.txt\") as fileh:\n            for line in fileh:\n                unused_keywords.append(line.strip())\n                if count > keyword_limit:\n                    break\n                count += 1\n        unused_keywords = [nlp.make_doc(keyword) for keyword in unused_keywords]\n\n    keywords = sample_keywords()\n    keywords = [nlp.make_doc(keyword) for keyword in keywords]\n    print(\"Loaded sample keywords\")\n    print(\"Keywords:\", keywords)\n    print(\"# Keywords:\", len(keywords))\n\n    texts = sample_texts()\n    docs = list(nlp.tokenizer.pipe(texts))\n    print(\"Loaded sample docs\")\n    print(\"# Docs:\", len(docs))\n\n    smatcher = Matcher(nlp.vocab)\n    pmatcher = PhraseMatcher(nlp.vocab)\n\n    for keyword in keywords:\n        smatcher.add(keyword.text, None, [{\"ORTH\": token.text} for token in keyword])\n        pmatcher.add(keyword.text, None, keyword)\n\n    for matcher in [smatcher, pmatcher]:\n        print(\"\\nMatcher type:\", type(matcher))\n        (len_matches, matches) = run_matching(matcher, docs)\n        print(\"# Matches\", len_matches)\n        print(\"Matches:\", matches)\n\ndef sample_keywords():\n    return [\"Hanif Kureishi\"]\n\ndef sample_texts():\n    return [\n\"\"\"\nA real triumph for Roger Michell and Hanif Kureishi, and the rest of the team. A must see for serious film lovers.\n\"\"\",\n]\n\nif __name__ == \"__main__\":\n    run()\n    run(with_extra_keyword_processing=True)\nOutput:\nRunning test *without* extra document processing\nLoaded sample keywords\nKeywords: [Hanif Kureishi]\n# Keywords: 1\nLoaded sample docs\n# Docs: 1\n\nMatcher type: <class 'spacy.matcher.matcher.Matcher'>\n# Matches 1\nMatches: [(4725172808025326492, 8, 10)]\n\nMatcher type: <class 'spacy.matcher.phrasematcher.PhraseMatcher'>\n# Matches 1\nMatches: [(4725172808025326492, 8, 10)]\n\n\nRunning test *with* extra document processing\nProcessing a lot of keywords, then discarding...\nLoaded sample keywords\nKeywords: [Hanif Kureishi]\n# Keywords: 1\nLoaded sample docs\n# Docs: 1\n\nMatcher type: <class 'spacy.matcher.matcher.Matcher'>\n# Matches 1\nMatches: [(4725172808025326492, 8, 10)]\n\nMatcher type: <class 'spacy.matcher.phrasematcher.PhraseMatcher'>\n# Matches 0\nMatches: []\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-4.19.0-5-amd64-x86_64-with-debian-10.0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-09-19T08:35:19Z", "fixed_by": "#4309", "pull_request_summary": "Replace PhraseMatcher with trie-based search", "pull_request_description": "Collaborator\nadrianeboyd commented on 19 Sep 2019 \u2022\nedited\nDescription\nReplace PhraseMatcher with a trie-based search algorithm over numpy arrays of the hash values for the relevant attribute. The implementation is based on FlashText (https://github.com/vi3k6i5/flashtext).\nThe speed should be similar to the previous PhraseMatcher. It is now possible to easily remove match IDs and matches don't go missing with large keyword lists / vocabularies.\nFixes #3922, fixes #4308.\nTypes of change\nBugfix? Enhancement? One of the two.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-27T14:22:35Z", "files_changed": [["2", "spacy/errors.py"], ["26", "spacy/matcher/phrasematcher.pxd"], ["324", "spacy/matcher/phrasematcher.pyx"], ["89", "spacy/tests/matcher/test_phrase_matcher.py"]]}, "323": {"issue_url": "https://github.com/explosion/spaCy/issues/4306", "issue_id": "#4306", "issue_summary": "Multiple roots per sentence", "issue_description": "ryszardtuora commented on 18 Sep 2019\nHow to reproduce the behaviour\nI'm trying to train the parser for polish. I'm using the PDB treebank, in the conllu format (because it contains one sentence per paragraph, I've used the option --n-sents=1 while converting). The results are close to 84% on UAS, somewhat weaker than what I would expect (having used different parsers before), so I'm not sure if I'm doing everything right. The treebanks do contain few percent of nonprojective trees, so I apply the gold preprocessing. When I try to evaluate the results using the conllu ud 18 eval script, I get the error about multiple roots in a sentence. Indeed few out of 2 thousand sentences are parsed to include two roots. From my understanding spaCy mistakenly treats these sentences as documents composed of two separate sentences (despite the lack of '.'). This is somewhat undesirable, as I've used someone elses models for polish, and they do not have this problem. Would you have any ideas regarding how to fix this?\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.73\nspaCy Version Used: 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-09-18T07:59:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "324": {"issue_url": "https://github.com/explosion/spaCy/issues/4304", "issue_id": "#4304", "issue_summary": "False positives with text categorizer", "issue_description": "H20Watermelon commented on 17 Sep 2019\nYour Environment\nOperating System: Win10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.8\nEnvironment Information:\nI trained a couple of textcat models on approximately 8000 passages (most of which are one or two sentences long (max token count is about 60) for binary classification (\"PRESENT\": a passage contains descriptions of stock information; \"NOT_PRESENT\": a passage does not contain stock information). One model used the simple_cnn mode and the other the default ensemble mode.\nWhen applying the models to the test set, I noticed that the models would predict \"PRESENT\" with very high probabilities (> 0.999) for passages that are not even remotely related to stock information. In fact, the models would give high probabilities to sentences with either garbled text or very short text. A couple of the examples are below:\n3wqedsad \\n sadsads\n(etc\nBoth of the passages above received 1.0 for the PRESENT class.\nIt also seems that punctuation/symbols have a huge influence on predictions. For example, in the second example above, if the left parenthesis is removed, the probability for PRESENT drops to close to 0 (2.2164e-06).\nI wonder if there are ways to improve the model performances, in particular in the area of reducing false positives.", "issue_status": "Closed", "issue_reporting_time": "2019-09-17T17:05:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "325": {"issue_url": "https://github.com/explosion/spaCy/issues/4302", "issue_id": "#4302", "issue_summary": "Duplicated definition of `Parser.tok2vec`", "issue_description": "Contributor\ntamuhey commented on 17 Sep 2019\nParser.tok2vec seems to be defined twice.\nIs this intentional?\nspaCy/spacy/syntax/nn_parser.pyx\nLine 134 in 47055d5\n def tok2vec(self): \nspaCy/spacy/syntax/nn_parser.pyx\nLine 153 in 47055d5\n def tok2vec(self): ", "issue_status": "Closed", "issue_reporting_time": "2019-09-17T09:01:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "326": {"issue_url": "https://github.com/explosion/spaCy/issues/4301", "issue_id": "#4301", "issue_summary": "Extract recurrent patterns using rule-based matching", "issue_description": "prateekjoshi565 commented on 17 Sep 2019\nExtract recurrent patterns using rule-based matching: Rule-based matching.\nI want to extract the pattern \"X such as Y1, Y2,..., Yn-1, and Yn.\" from a sentence. X is a hypernym and Yi is a hyponym.\nLet's take an example:\ndoc = nlp(\"GDP in developing countries such as Vietnam, Indonesia and Bangladesh will continue growing at a high rate.\")\n\n# Matcher class object\nmatcher = Matcher(nlp.vocab)\n\n#define the pattern\npattern = [{'DEP':'amod', 'OP':\"?\"},\n           {'POS':'NOUN'},\n           {'LOWER': 'such'},\n           {'LOWER': 'as'},\n           {'POS': 'PROPN'}]\n\nmatcher.add(\"matching_1\", None, pattern)\nmatches = matcher(doc)\nspan = doc[matches[0][1]:matches[0][2]]\nspan.text\nOutput: 'developing countries such as Vietnam'\nThe desired output is 'developing countries such as Vietnam, Indonesia and Bangladesh'. How to perform this task using spaCy?\nThanks in advance.", "issue_status": "Closed", "issue_reporting_time": "2019-09-17T08:58:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "327": {"issue_url": "https://github.com/explosion/spaCy/issues/4299", "issue_id": "#4299", "issue_summary": "Custom pipes flooring all model scores for ner entity type", "issue_description": "AnnaAnia commented on 17 Sep 2019 \u2022\nedited\nHi,\nI'm adding a custom pipe to my custom NER model. It is inteded to enhance my entity of CMPNY with your \"ORG\" entity. As I'm adding the pipe in after ner, I was expecting to see better recall and possibly worse precision due more false positives. Instead all scores plumment. I can't work out where it is I'm going wrong or if something not working as expected. I would apprecite any help.\nCustom pipe:\ndef company(doc):\n    \n    Model_Spacy = spacy.load(\"en_core_web_sm-2.1.0\")\n    \n    Doc=Model_Spacy(doc.text)\n    \n    ent_doc=[]\n    for ent in doc.ents:\n        ent_doc.append(ent)\n    \n    add_ents=[]\n    for ent in Doc.ents:\n        if ent.label_ == 'ORG' and ent.end<= 300:\n            add_ent = Span(Doc,ent.start,ent.end,label=CMPNY )\n            add_ents.append(add_ent)\n        else:\n            continue\n    \n    ent_doc_temp=[]\n    for ent in ent_doc:\n        if ent.start<300:\n            ent_doc_temp.append(ent)\n    \n    new_ents=[]\n    for ent in add_ents:\n        count=[]\n        for e in ent_doc_temp:\n            ent_range=set(range(ent.start,ent.end))\n            e_range=set(range(e.start,e.end))\n            if ent_range.issubset(e_range) or e_range.issubset(ent_range):\n                count.append(1)\n                break\n            if not ent_range.isdisjoint(e_range):\n                count.append(1)\n                break\n            else:\n                continue\n        if len(count)==0:\n            new_ents.append(ent)\n    \n    for ent in new_ents:\n        ent_doc.append(ent)  \n    \n    doc.ents=ent_doc\n    \n    return (doc)\nAdd pipe to model:\nModel_S215_V2.add_pipe(company, after='ner')\nEvaluate new model:\ndef evaluate(ner_model, examples):\n       \"\"\" Evaluate spacy model with p,r & f overall scores and scores per entity \"\"\"\n       scorer = Scorer()\n       for input_, annot in examples:\n           doc_gold_text = ner_model.make_doc(str(input_))\n           gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n           pred_value = ner_model(str(input_))\n           scorer.score(pred_value, gold)\n       return scorer.scores\n\nresults = evaluate(Model_S215_V2,BLIND_S216_V2)  \nResults before adding the pipe in:\n'CMPNY': {'p': 40.625,\n'r': 71.65354330708661, |  \n'f': 51.85185185185185}, | \n``` \n\nResulft after:\n'CMPNY': {'p': 2.0091556459816884,\n'r': 16.08961303462322,\n'f': 3.572236038887632}", "issue_status": "Closed", "issue_reporting_time": "2019-09-17T05:40:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "328": {"issue_url": "https://github.com/explosion/spaCy/issues/4298", "issue_id": "#4298", "issue_summary": "Converting Multilingual FastText Vectors to Spacy Model", "issue_description": "Khanifsaleh commented on 17 Sep 2019\nI have a multilingual FastText Vector (Indonesia, Malay, and English. in one file vector). I want to converting into spacy model using\npython -m init-model ... vectors-loc [filename vectors].\nWhat init-model should I use?\nor Any other way to extract the vector?", "issue_status": "Closed", "issue_reporting_time": "2019-09-17T03:46:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "329": {"issue_url": "https://github.com/explosion/spaCy/issues/4297", "issue_id": "#4297", "issue_summary": "SSL error when downloading models", "issue_description": "yutingwum commented on 17 Sep 2019 \u2022\nedited\nSituation\nI tired to download Spacy models on virtual environment with Python 3.6.5. After successfully download Spacy with the line pip install rasa[spacy], I tried to download the English model with python -m spacy download en_core_web_md. But an SSLErrorr was raised. I looked into other similar issues including #3066, #2248, #2212\nI tried the following line python3 -m spacy download en_core_web_md-2.1.8 --direct but did not work and threw the same error as well.\nPlease see the below output.\nFile \"/Users/yuwu/venv/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 603, in urlopen\n    chunked=chunked)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 344, in _make_request\n    self._validate_conn(conn)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 843, in _validate_conn\n    conn.connect()\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/urllib3/connection.py\", line 370, in connect\n    ssl_context=context)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/urllib3/util/ssl_.py\", line 355, in ssl_wrap_socket\n    return context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 407, in wrap_socket\n    _context=self, _session=session)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 814, in __init__\n    self.do_handshake()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 1068, in do_handshake\n    self._sslobj.do_handshake()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 689, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n    timeout=timeout\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 641, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/urllib3/util/retry.py\", line 399, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)'),))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/spacy/cli/download.py\", line 38, in download\n    shortcuts = get_json(about.__shortcuts__, \"available shortcuts\")\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/spacy/cli/download.py\", line 84, in get_json\n    r = requests.get(url)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/requests/api.py\", line 75, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/requests/api.py\", line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/requests/sessions.py\", line 533, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/requests/sessions.py\", line 646, in send\n    r = adapter.send(request, **kwargs)\n  File \"/Users/yuwu/venv/lib/python3.6/site-packages/requests/adapters.py\", line 514, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)'),))\nYour Environment\nAnaconda is installed\nOperating System: MacOS Mojave 10.14.6\nPython Version Used: 3.6.6\nspaCy Version Used: 2.1.8\nEnvironment Information: Virtual environment in Python 3.6.5. Please see the below list for packages installed and version.\nabsl-py==0.8.0\naiofiles==0.4.0\naiohttp==3.6.0\nalembic==1.0.11\nAPScheduler==3.6.1\nasn1crypto==0.24.0\nastor==0.8.0\nasync-generator==1.10\nasync-timeout==3.0.1\nattrs==19.1.0\nblis==0.2.4\nboto3==1.9.228\nbotocore==1.12.228\ncertifi==2019.9.11\ncffi==1.12.3\nchardet==3.0.4\nClick==7.0\ncolorclass==2.2.0\ncoloredlogs==10.0\ncolorhash==1.0.2\nConfigArgParse==0.14.0\ncryptography==2.7\ncycler==0.10.0\ncymem==2.0.2\ndecorator==4.4.0\ndocopt==0.6.2\ndocutils==0.15.2\nfakeredis==1.0.5\nfbmessenger==6.0.0\nFlask==1.1.1\nFlask-Cors==3.0.8\nfuture==0.17.1\ngast==0.3.2\ngevent==1.4.0\ngreenlet==0.4.15\ngrpcio==1.23.0\nh5py==2.10.0\nhttptools==0.0.13\nhumanfriendly==4.18\nidna==2.8\nidna-ssl==1.1.0\nisodate==0.6.0\nitsdangerous==1.1.0\nJinja2==2.10.1\njmespath==0.9.4\njsonpickle==1.2\njsonschema==2.6.0\nkafka-python==1.4.6\nKeras-Applications==1.0.8\nKeras-Preprocessing==1.1.0\nkiwisolver==1.1.0\nMako==1.1.0\nMarkdown==3.1.1\nMarkupSafe==1.1.1\nmatplotlib==3.1.1\nmattermostwrapper==2.1\nmitie==0.7.0\nmock==3.0.5\nmultidict==4.5.2\nmurmurhash==1.0.2\nnetworkx==2.3\nnumpy==1.17.2\npackaging==19.1\npika==1.0.1\nplac==0.9.6\npreshed==2.0.1\nprompt-toolkit==2.0.9\nprotobuf==3.9.1\npycparser==2.19\npydot==1.4.1\nPyJWT==1.7.1\npykwalify==1.7.0\npymongo==3.9.0\npyparsing==2.4.2\nPySocks==1.7.0\npython-crfsuite==0.9.6\npython-dateutil==2.8.0\npython-editor==1.0.4\npython-engineio==3.9.3\npython-socketio==4.3.1\npython-telegram-bot==11.1.0\npytz==2019.2\nPyYAML==5.1.2\nquestionary==1.3.0\nrasa==1.2.8\nrasa-sdk==1.2.0\nrasa-x==0.20.2\nredis==3.3.8\nrequests==2.22.0\nrequests-toolbelt==0.9.1\nrocketchat-API==0.6.34\nruamel.yaml==0.15.100\ns3transfer==0.2.1\nsanic==19.3.1\nSanic-Cors==0.9.9.post1\nsanic-jwt==1.3.2\nSanic-Plugins-Framework==0.8.2\nscikit-learn==0.20.4\nscipy==1.3.1\nsetuptools-scm==3.3.3\nsimplejson==3.16.0\nsix==1.12.0\nsklearn-crfsuite==0.3.6\nslackclient==1.3.2\nsortedcontainers==2.1.0\nspacy==2.1.8\nSQLAlchemy==1.3.8\nsrsly==0.1.0\ntabulate==0.8.3\ntensorboard==1.13.1\ntensorflow==1.13.2\ntensorflow-estimator==1.13.0\ntermcolor==1.1.0\nterminaltables==3.1.0\nthinc==7.0.8\ntqdm==4.35.0\ntwilio==6.30.0\ntyping-extensions==3.7.4\ntzlocal==2.0.0\nujson==1.35\nurllib3==1.25.3\nuvloop==0.13.0\nwasabi==0.2.2\nwcwidth==0.1.7\nwebexteamssdk==1.1.1\nwebsocket-client==0.54.0\nwebsockets==6.0\nWerkzeug==0.15.6\nyarl==1.3.0", "issue_status": "Closed", "issue_reporting_time": "2019-09-16T19:39:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "330": {"issue_url": "https://github.com/explosion/spaCy/issues/4296", "issue_id": "#4296", "issue_summary": "Error in spacy.load() while sharing my model folder with others in my team", "issue_description": "santoshboina commented on 16 Sep 2019 \u2022\nedited\nCreated a custom spacy ner on new entities and saved model using nlp.to_disk() and tested it on my local machine by spacy.load ('path to model') which worked succesfully.\nWhen i tried to share my model folder (contains pipeline named folders,meta.json and vocab) with others and throws error for others when they try to run spacy.load('path to model')\nCan't find model custommodel\nNote: We both are using same version of Spacy 2.1.8 and python 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-09-16T13:45:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "331": {"issue_url": "https://github.com/explosion/spaCy/issues/4293", "issue_id": "#4293", "issue_summary": "Unclear how Spacy gets word embeddings for sentences.", "issue_description": "LostInBayes commented on 15 Sep 2019 \u2022\nedited\nSpacy offers pre-trained vectors for words. However I have noticed that you can get vectors for sentences too:\nspacy_nlp('hello I').has_vector == True\nHowever I can't figure how it calculates the word2vecs from the sentences. I've tried:\nspacy_nlp('hello I').vector == spacy_nlp('hello').vector + spacy_nlp('I').vector\nFalse\nspacy_nlp('hello I').vector/spacy_nlp('hello I').vector_norm == spacy_nlp('hello').vector/spacy_nlp('hello').vector_norm + spacy_nlp('I').vector/spacy_nlp('I').vector_norm\nFalse\nI can't seem to find or work out how spacy computes the w2v for sentences.", "issue_status": "Closed", "issue_reporting_time": "2019-09-15T12:22:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "332": {"issue_url": "https://github.com/explosion/spaCy/issues/4287", "issue_id": "#4287", "issue_summary": "'cupy.core.core.Indexer' has no attribute '__reduce_cython__'", "issue_description": "leimao commented on 14 Sep 2019 \u2022\nedited\nHow to reproduce the problem\nTo download the Spacy language model in Dockerfile, an error occurs:\nStep 5/7 : RUN python -m spacy download en\n ---> Running in d872921ac8e2\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/runpy.py\", line 174, in _run_module_as_main\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n  File \"/usr/lib/python3.5/runpy.py\", line 133, in _get_module_details\n    return _get_module_details(pkg_main_name, error)\n  File \"/usr/lib/python3.5/runpy.py\", line 109, in _get_module_details\n    __import__(pkg_name)\n  File \"/usr/local/lib/python3.5/dist-packages/spacy/__init__.py\", line 10, in <module>\n    from thinc.neural.util import prefer_gpu, require_gpu\n  File \"/usr/local/lib/python3.5/dist-packages/thinc/neural/__init__.py\", line 4, in <module>\n    from ._classes.model import Model  # noqa: F401\n  File \"/usr/local/lib/python3.5/dist-packages/thinc/neural/_classes/model.py\", line 11, in <module>\n    from ..train import Trainer\n  File \"/usr/local/lib/python3.5/dist-packages/thinc/neural/train.py\", line 7, in <module>\n    from .optimizers import Adam, linear_decay\n  File \"optimizers.pyx\", line 13, in init thinc.neural.optimizers\n  File \"ops.pyx\", line 40, in init thinc.neural.ops\n  File \"/usr/local/lib/python3.5/dist-packages/cupy/__init__.py\", line 11, in <module>\n    from cupy import core  # NOQA\n  File \"/usr/local/lib/python3.5/dist-packages/cupy/core/__init__.py\", line 1, in <module>\n    from cupy.core import core  # NOQA\n  File \"cupy/core/carray.pxi\", line 50, in init cupy.core.core\nAttributeError: type object 'cupy.core.core.Indexer' has no attribute '__reduce_cython__'\nHowever, without downloading en in the Dockerfile, the image could be created successfully. Then I could download the en without problem.\nYour Environment\nPlatform: Linux-5.0.0-27-generic-x86_64-with-Ubuntu-16.04-xenial\nPython version: 3.5.2\nspaCy version: 2.1.8", "issue_status": "Closed", "issue_reporting_time": "2019-09-14T00:00:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "333": {"issue_url": "https://github.com/explosion/spaCy/issues/4286", "issue_id": "#4286", "issue_summary": "Define Annotation Parameters", "issue_description": "mpfp commented on 14 Sep 2019\nIt'd be help to have definitions of the annotation parameters in the \"Training spaCy\u2019s Statistical Models\" article (e.g., in the \"Simple training style\" and in the code in the \"Updating Named Entity Recognizer\" section).\nFor instance, in the code you have:\ntraining data\nTRAIN_DATA = [\n(\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n(\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n]\nIt'd be nice to specify the the parameters of the entity annotation are (start_pos, end_pos, entity_type). It took me a while to figure that out.\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-09-13T21:56:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "334": {"issue_url": "https://github.com/explosion/spaCy/issues/4283", "issue_id": "#4283", "issue_summary": "Best strategy to use nlp object inside a custom component", "issue_description": "Contributor\nchkoar commented on 13 Sep 2019\nLet's say that I take an existing model and I add a custom component that classifies spans. So, my custom component take as parameter the nlp object itself because I need it to make some preprocessing (again) in the span. Like this I could fall in an infinite loop. What is the best strategy to handle this kind of problem? (You can assume that I need features from the pipeline and cannot use just the make_doc)\nA solution I found is to deep copy the nlp object. Is there a better strategy that I am not aware of?", "issue_status": "Closed", "issue_reporting_time": "2019-09-12T20:34:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "335": {"issue_url": "https://github.com/explosion/spaCy/issues/4280", "issue_id": "#4280", "issue_summary": "Licensing of norwegian spaCy model", "issue_description": "miktoki commented on 12 Sep 2019\nReferring to #3082, I have been in contact with the legal owner of noWaC, the dataset we wish to base a norwegian spaCy model on together with @jarib. They are unable to change the license of the corpus itself due to norwegian law (se explanation in section 1.2 in the noWaC paper), but are willing to set up some special agreement which would allow us to use the corpus to generate the language model. They have a standard contract available, where we could set the cost of the license to 0, and specific terms specified in Appendix 2. I also think we should further discuss the duration of the contract with them.\nAs pointed out by @ines here, the language models could count as derivative works based on the corpus.\n@honnibal @ines, what do you need the terms of the contract to include? Also, is it sufficient to have the agreement in Norwegian? Do you perhaps have a template we could base the contract no, or alternatively, if their contract is used, what should be included in Appendix 2?\n2", "issue_status": "Closed", "issue_reporting_time": "2019-09-12T07:49:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "336": {"issue_url": "https://github.com/explosion/spaCy/issues/4278", "issue_id": "#4278", "issue_summary": "invalid call `len(tuple)` in Pipe.__call__", "issue_description": "Contributor\ntamuhey commented on 12 Sep 2019\nspaCy/spacy/pipeline/pipes.pyx\nLine 70 in 8ebc371\n if isinstance(predictions, tuple) and len(tuple) == 2: \nlen(tuple) == 2 in the line will raise error.\nI think len(predictions) is valid.\n1", "issue_status": "Closed", "issue_reporting_time": "2019-09-12T05:18:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "337": {"issue_url": "https://github.com/explosion/spaCy/issues/4277", "issue_id": "#4277", "issue_summary": "displacy-ent demo color bug", "issue_description": "Contributor\nBramVanroy commented on 11 Sep 2019 \u2022\nedited\nWhen I visit the website of the displaCy demo, and I scroll down to see the code below the displaCy Named Entity Visualizer heading, some text seems to be missing. In reality, the color is simply set to the same color as the background. Specifically, this is the culprit\n[data-demo=displacy-ent] .d-wrapper code {\n    color: #1e1935;\n}\nWhen removing this style, the code is nice and white-ish.\nI could not find the code to the demo in this repo, so I just filed an issue here.", "issue_status": "Closed", "issue_reporting_time": "2019-09-11T13:57:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "338": {"issue_url": "https://github.com/explosion/spaCy/issues/4274", "issue_id": "#4274", "issue_summary": "How to make models with scispacy data for GPE", "issue_description": "dhwani2410 commented on 11 Sep 2019\nI want to extract GPE labels from biomedical data available in scispacy. Can you please share a link on how to proceed for the same", "issue_status": "Closed", "issue_reporting_time": "2019-09-11T12:25:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "339": {"issue_url": "https://github.com/explosion/spaCy/issues/4273", "issue_id": "#4273", "issue_summary": "GPE tagging not available using the scispacy package", "issue_description": "dhwani2410 commented on 11 Sep 2019\nHello i am using spacy binder to run the same text using two spacy and sci-spacy\nnlp = spacy.load(\"en_core_sci_sm\")\nnlp1 = spacy.load('en_core_web_sm')\nsent = \"HLA antigens in arterial occlusive diseases in Japan. Using a NIH standard lymphocytotoxicity test, a possible Japanese specific HLA antigen, HLA-BJW 22.2 was identified in 17 out of 48 patients with thromboangiitis obliterans (35.4 per cent), in 5 out of 15 patients with Takayasu's arteritis (33.3 per cent) and in 11 out of 113 normal controls (9.7 per cent). On the other hand, HLA-CWl was found in 4 out of 47 patients with arteriosclerosis obliterans (8.5 per cent) and in 41 out of 113 normal controls (36.3 per cent).\ndoc=nlp(sent)\ndoc1 = nlp1(sent)\nI am not able to get any GPE tags using the sci spacy method while getting using normal spacy.\nPlease see below for results.\nfor ent in doc.ents:\n... print(ent.text, ent.start_char, ent.end_char, ent.label_)\n...\nHLA antigens 0 12 ENTITY\narterial occlusive diseases 16 43 ENTITY\nJapan 47 52 ENTITY\nNIH standard 62 74 ENTITY\nlymphocytotoxicity test 75 98 ENTITY\nJapanese 111 119 ENTITY\nHLA antigen 129 140 ENTITY\nHLA-BJW 142 149 ENTITY\npatients 186 194 ENTITY\nthromboangiitis obliterans 200 226 ENTITY\npatients 259 267 ENTITY\nTakayasu's arteritis 273 293 ENTITY\ncent 304 308 ENTITY\nnormal controls 331 346 ENTITY\nHLA-CWl 382 389 ENTITY\npatients 415 423 ENTITY\narteriosclerosis 429 445 ENTITY\nobliterans 446 456 ENTITY\nnormal controls 493 508 ENTITY\nfor ent in doc1.ents:\n... print(ent.text, ent.start_char, ent.end_char, ent.label_)\n...\nJapan 47 52 GPE\nNIH 62 65 ORG\nJapanese 111 119 NORP\n17 173 175 CARDINAL\n48 183 185 CARDINAL\n35.4 per cent 228 241 MONEY\n5 247 248 CARDINAL\n15 256 258 CARDINAL\nTakayasu 273 281 PERSON\n33.3 per cent 295 308 MONEY\n11 317 319 CARDINAL\n113 327 330 CARDINAL\n9.7 per cent 348 360 MONEY\n4 403 404 CARDINAL\n47 412 414 CARDINAL\n8.5 per cent 458 470 MONEY\n41 479 481 CARDINAL\n113 489 492 CARDINAL\n36.3 per cent 510 523 MONEY", "issue_status": "Closed", "issue_reporting_time": "2019-09-11T11:18:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "340": {"issue_url": "https://github.com/explosion/spaCy/issues/4272", "issue_id": "#4272", "issue_summary": "GreekLemmatizer has no attribute lookup", "issue_description": "Collaborator\nadrianeboyd commented on 11 Sep 2019\nHow to reproduce the behaviour\nimport spacy\nnlp = spacy.load('el')\ndoc = nlp.make_doc('\u03a7\u03b8\u03b5\u03c2')\nprint(doc[0].lemma_)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"token.pyx\", line 860, in spacy.tokens.token.Token.lemma_.__get__\nAttributeError: 'GreekLemmatizer' object has no attribute 'lookup'\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-4.19.0-5-amd64-x86_64-with-debian-10.0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-09-11T09:19:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "341": {"issue_url": "https://github.com/explosion/spaCy/issues/4271", "issue_id": "#4271", "issue_summary": "Memory error while loading en_core_web_lg model.", "issue_description": "balavenkatesh3322 commented on 11 Sep 2019\nI got below error while loading the spacy model.\nspacy.load('en_core_web_lg')\nTraceback (most recent call last): File \"/usr/local/lib/python3.6/site-packages/sentry_sdk/integrations/celery.py\", line 64, in _inner return f(*args, **kwargs) File \"/phraze/recording/tasks.py\", line 92, in notes_aws_transcribe_audio_and_medical_data_task followup.FollowUp().save_follow_up(appointment_id, transcript_text) File \"/phraze/recording/followUp.py\", line 76, in save_follow_up numeric, unit, utterance, full_utterance_array = self.extract_follow_up(data) File \"/phraze/recording/followUp.py\", line 27, in extract_follow_up nlp = spacy.load('en_core_web_lg') File \"/usr/local/lib/python3.6/site-packages/spacy/__init__.py\", line 27, in load return util.load_model(name, **overrides) File \"/usr/local/lib/python3.6/site-packages/spacy/util.py\", line 134, in load_model return load_model_from_package(name, **overrides) File \"/usr/local/lib/python3.6/site-packages/spacy/util.py\", line 155, in load_model_from_package return cls.load(**overrides) File \"/usr/local/lib/python3.6/site-packages/en_core_web_lg/__init__.py\", line 12, in load return load_model_from_init_py(__file__, **overrides) File \"/usr/local/lib/python3.6/site-packages/spacy/util.py\", line 196, in load_model_from_init_py return load_model_from_path(data_path, meta, **overrides) File \"/usr/local/lib/python3.6/site-packages/spacy/util.py\", line 179, in load_model_from_path return nlp.from_disk(model_path) File \"/usr/local/lib/python3.6/site-packages/spacy/language.py\", line 836, in from_disk util.from_disk(path, deserializers, exclude) File \"/usr/local/lib/python3.6/site-packages/spacy/util.py\", line 636, in from_disk reader(path / key) File \"/usr/local/lib/python3.6/site-packages/spacy/language.py\", line 820, in <lambda> p File \"vocab.pyx\", line 453, in spacy.vocab.Vocab.from_disk File \"vectors.pyx\", line 431, in spacy.vectors.Vectors.from_disk File \"/usr/local/lib/python3.6/site-packages/spacy/util.py\", line 636, in from_disk reader(path / key) File \"vectors.pyx\", line 424, in spacy.vectors.Vectors.from_disk.load_vectors File \"/usr/local/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 453, in load pickle_kwargs=pickle_kwargs) File \"/usr/local/lib/python3.6/site-packages/numpy/lib/format.py\", line 738, in read_array array = numpy.fromfile(fp, dtype=dtype, count=count) numpy.core._exceptions.MemoryError: Unable to allocate array with shape (205449300,) and data type float32\nOperating System:Ubuntu\nPython Version Used:3.5\nspaCy Version Used:2.1.8\nEnvironment Information:AWS", "issue_status": "Closed", "issue_reporting_time": "2019-09-11T07:02:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "342": {"issue_url": "https://github.com/explosion/spaCy/issues/4270", "issue_id": "#4270", "issue_summary": "Documentation on --vectors-loc (for init-model) doesn't match read_vector()?", "issue_description": "mrxiaohe commented on 11 Sep 2019\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.8\nEnvironment Information:\nFor init-model, the documentation for the option --vectors-loc says that the file should be a tab-separated file in Word2Vec format where the first column contains the word and the remaining columns the values.\nThe function definition for read_vectors() appears to suggest that the function expects the first row to contain the dimensions of the vectors (this line: shape = tuple(int(size) for size in next(f).split())), followed by a word2vec formatted table?\nIn addition, the line pieces = line.rsplit(\" \", vectors_data.shape[1]) does not split the rows (at least on my Windows computer) if the vector file is indeed tab separated. It only works if I replace the space \" \" with a tab \"\\t\" (pieces = line.rsplit(\"\\t\", vectors_data.shape[1])\nNot sure if I misunderstood the formatting of the vectors (a distinct possibility!). Thanks!\ndef read_vectors(vectors_loc):\n    # temp fix to avoid import issues cf https://github.com/explosion/spaCy/issues/4200\n    from tqdm import tqdm\n\n    f = open_file(vectors_loc)\n    shape = tuple(int(size) for size in next(f).split())\n    vectors_data = numpy.zeros(shape=shape, dtype=\"f\")\n    vectors_keys = []\n    for i, line in enumerate(tqdm(f)):\n        line = line.rstrip()\n        pieces = line.rsplit(\" \", vectors_data.shape[1])\n        word = pieces.pop(0)\n        if len(pieces) != vectors_data.shape[1]:\n            msg.fail(Errors.E094.format(line_num=i, loc=vectors_loc), exits=1)\n        vectors_data[i] = numpy.asarray(pieces, dtype=\"f\")\n        vectors_keys.append(word)\n    return vectors_data, vectors_keys", "issue_status": "Closed", "issue_reporting_time": "2019-09-11T00:17:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "343": {"issue_url": "https://github.com/explosion/spaCy/issues/4269", "issue_id": "#4269", "issue_summary": "Sentencizer fails with languages like Bengali, Hindi, Kannada, Sinhala, Tamil, Telugu and Urdu", "issue_description": "BLKSerene commented on 11 Sep 2019 \u2022\nedited\nspaCy's sentencizer fails with languages like Bengali, Hindi, Kannada, Sinhala, Tamil, Telugu and Urdu, while these languages are said to be supported in the doc.\nWith languages like Bengali and Hindi, the vertical bar \"|\" instead of the period \".\" is used as the sentence terminator.\n>>> import spacy\n\n>>> TEXT_BEN = '\u09ac\u09be\u0982\u09b2\u09be \u09ad\u09be\u09b7\u09be (\u09ac\u09be\u0999\u09b2\u09be, \u09ac\u09be\u0999\u09cd\u0997\u09b2\u09be, \u09a4\u09a5\u09be \u09ac\u09be\u0999\u09cd\u0997\u09be\u09b2\u09be \u09a8\u09be\u09ae\u0997\u09c1\u09b2\u09cb\u09a4\u09c7\u0993 \u09aa\u09b0\u09bf\u099a\u09bf\u09a4) \u098f\u0995\u099f\u09bf \u0987\u09a8\u09cd\u09a6\u09cb-\u0986\u09b0\u09cd\u09af \u09ad\u09be\u09b7\u09be, \u09af\u09be \u09a6\u0995\u09cd\u09b7\u09bf\u09a3 \u098f\u09b6\u09bf\u09af\u09bc\u09be\u09b0 \u09ac\u09be\u0999\u09be\u09b2\u09bf \u099c\u09be\u09a4\u09bf\u09b0 \u09aa\u09cd\u09b0\u09a7\u09be\u09a8 \u0995\u09a5\u09cd\u09af \u0993 \u09b2\u09c7\u0996\u09cd\u09af \u09ad\u09be\u09b7\u09be\u0964 \u09ae\u09be\u09a4\u09c3\u09ad\u09be\u09b7\u09c0\u09b0 \u09b8\u0982\u0996\u09cd\u09af\u09be\u09af\u09bc \u09ac\u09be\u0982\u09b2\u09be \u0987\u09a8\u09cd\u09a6\u09cb-\u0987\u0989\u09b0\u09cb\u09aa\u09c0\u09af\u09bc \u09ad\u09be\u09b7\u09be \u09aa\u09b0\u09bf\u09ac\u09be\u09b0\u09c7\u09b0 \u099a\u09a4\u09c1\u09b0\u09cd\u09a5 \u0993 \u09ac\u09bf\u09b6\u09cd\u09ac\u09c7\u09b0 \u09b7\u09b7\u09cd\u09a0 \u09ac\u09c3\u09b9\u09a4\u09cd\u09a4\u09ae \u09ad\u09be\u09b7\u09be\u0964[\u09eb] \u09ae\u09cb\u099f \u09ac\u09cd\u09af\u09ac\u09b9\u09be\u09b0\u0995\u09be\u09b0\u09c0\u09b0 \u09b8\u0982\u0996\u09cd\u09af\u09be \u0985\u09a8\u09c1\u09b8\u09be\u09b0\u09c7 \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09bf\u09b6\u09cd\u09ac\u09c7\u09b0 \u09b8\u09aa\u09cd\u09a4\u09ae \u09ac\u09c3\u09b9\u09a4\u09cd\u09a4\u09ae \u09ad\u09be\u09b7\u09be\u0964 \u09ac\u09be\u0982\u09b2\u09be \u09b8\u09be\u09b0\u09cd\u09ac\u09ad\u09cc\u09ae \u09ad\u09be\u09b7\u09be\u09ad\u09bf\u09a4\u09cd\u09a4\u09bf\u0995 \u099c\u09be\u09a4\u09bf\u09b0\u09be\u09b7\u09cd\u099f\u09cd\u09b0 \u09ac\u09be\u0982\u09b2\u09be\u09a6\u09c7\u09b6\u09c7\u09b0 \u098f\u0995\u09ae\u09be\u09a4\u09cd\u09b0 \u09b0\u09be\u09b7\u09cd\u099f\u09cd\u09b0\u09ad\u09be\u09b7\u09be \u09a4\u09a5\u09be \u09b8\u09b0\u0995\u09be\u09b0\u09bf \u09ad\u09be\u09b7\u09be[\u09ec] \u098f\u09ac\u0982 \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u09aa\u09b6\u09cd\u099a\u09bf\u09ae\u09ac\u0999\u09cd\u0997, \u09a4\u09cd\u09b0\u09bf\u09aa\u09c1\u09b0\u09be, \u0986\u09b8\u09be\u09ae\u09c7\u09b0 \u09ac\u09b0\u09be\u0995 \u0989\u09aa\u09a4\u09cd\u09af\u0995\u09be\u09b0 \u09b8\u09b0\u0995\u09be\u09b0\u09bf \u09ad\u09be\u09b7\u09be\u0964 \u09ac\u0999\u09cd\u0997\u09cb\u09aa\u09b8\u09be\u0997\u09b0\u09c7 \u0985\u09ac\u09b8\u09cd\u09a5\u09bf\u09a4 \u0986\u09a8\u09cd\u09a6\u09be\u09ae\u09be\u09a8 \u09a6\u09cd\u09ac\u09c0\u09aa\u09aa\u09c1\u099e\u09cd\u099c\u09c7\u09b0 \u09aa\u09cd\u09b0\u09a7\u09be\u09a8 \u0995\u09a5\u09cd\u09af \u09ad\u09be\u09b7\u09be \u09ac\u09be\u0982\u09b2\u09be\u0964 \u098f\u099b\u09be\u09a1\u09bc\u09be \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u099d\u09be\u09a1\u09bc\u0996\u09a3\u09cd\u09a1, \u09ac\u09bf\u09b9\u09be\u09b0, \u09ae\u09c7\u0998\u09be\u09b2\u09af\u09bc, \u09ae\u09bf\u099c\u09cb\u09b0\u09be\u09ae, \u0989\u09a1\u09bc\u09bf\u09b7\u09cd\u09af\u09be \u09b0\u09be\u099c\u09cd\u09af\u0997\u09c1\u09b2\u09cb\u09a4\u09c7 \u0989\u09b2\u09cd\u09b2\u09c7\u0996\u09af\u09cb\u0997\u09cd\u09af \u09aa\u09b0\u09bf\u09ae\u09be\u09a3\u09c7 \u09ac\u09be\u0982\u09b2\u09be\u09ad\u09be\u09b7\u09c0 \u099c\u09a8\u0997\u09a3 \u09b0\u09af\u09bc\u09c7\u099b\u09c7\u0964 \u09ad\u09be\u09b0\u09a4\u09c7 \u09b9\u09bf\u09a8\u09cd\u09a6\u09bf\u09b0 \u09aa\u09b0\u09c7\u0987 \u09b8\u09b0\u09cd\u09ac\u09be\u09a7\u09bf\u0995 \u09aa\u09cd\u09b0\u099a\u09b2\u09bf\u09a4 \u09ad\u09be\u09b7\u09be \u09ac\u09be\u0982\u09b2\u09be\u0964[\u09ed][\u09ee] \u098f\u099b\u09be\u09a1\u09bc\u09be\u0993 \u09ae\u09a7\u09cd\u09af \u09aa\u09cd\u09b0\u09be\u099a\u09cd\u09af, \u0986\u09ae\u09c7\u09b0\u09bf\u0995\u09be \u0993 \u0987\u0989\u09b0\u09cb\u09aa\u09c7 \u0989\u09b2\u09cd\u09b2\u09c7\u0996\u09af\u09cb\u0997\u09cd\u09af \u09aa\u09b0\u09bf\u09ae\u09be\u09a3\u09c7 \u09ac\u09be\u0982\u09b2\u09be\u09ad\u09be\u09b7\u09c0 \u0985\u09ad\u09bf\u09ac\u09be\u09b8\u09c0 \u09b0\u09af\u09bc\u09c7\u099b\u09c7\u0964[\u09ef] \u09b8\u09be\u09b0\u09be \u09ac\u09bf\u09b6\u09cd\u09ac\u09c7 \u09b8\u09ac \u09ae\u09bf\u09b2\u09bf\u09af\u09bc\u09c7 \u09e8\u09ec \u0995\u09cb\u099f\u09bf\u09b0 \u0985\u09a7\u09bf\u0995 \u09b2\u09cb\u0995 \u09a6\u09c8\u09a8\u09a8\u09cd\u09a6\u09bf\u09a8 \u099c\u09c0\u09ac\u09a8\u09c7 \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09cd\u09af\u09ac\u09b9\u09be\u09b0 \u0995\u09b0\u09c7\u0964[\u09e8] \u09ac\u09be\u0982\u09b2\u09be\u09a6\u09c7\u09b6\u09c7\u09b0 \u099c\u09be\u09a4\u09c0\u09af\u09bc \u09b8\u0999\u09cd\u0997\u09c0\u09a4 \u098f\u09ac\u0982 \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u099c\u09be\u09a4\u09c0\u09af\u09bc \u09b8\u0999\u09cd\u0997\u09c0\u09a4 \u0993 \u09b8\u09cd\u09a4\u09cb\u09a4\u09cd\u09b0 \u09ac\u09be\u0982\u09b2\u09be\u09a4\u09c7 \u09b0\u099a\u09bf\u09a4\u0964'\n>>> TEXT_HIN = '\u0939\u093f\u0928\u094d\u0926\u0940 \u0935\u093f\u0936\u094d\u0935 \u0915\u0940 \u090f\u0915 \u092a\u094d\u0930\u092e\u0941\u0916 \u092d\u093e\u0937\u093e \u0939\u0948 \u090f\u0935\u0902 \u092d\u093e\u0930\u0924 \u0915\u0940 \u0930\u093e\u091c\u092d\u093e\u0937\u093e \u0939\u0948\u0964 \u0915\u0947\u0928\u094d\u0926\u094d\u0930\u0940\u092f \u0938\u094d\u0924\u0930 \u092a\u0930 \u092d\u093e\u0930\u0924 \u092e\u0947\u0902 \u0926\u0942\u0938\u0930\u0940 \u0906\u0927\u093f\u0915\u093e\u0930\u093f\u0915 \u092d\u093e\u0937\u093e \u0905\u0902\u0917\u094d\u0930\u0947\u091c\u0940 \u0939\u0948\u0964 \u092f\u0939 \u0939\u093f\u0902\u0926\u0941\u0938\u094d\u0924\u093e\u0928\u0940 \u092d\u093e\u0937\u093e \u0915\u0940 \u090f\u0915 \u092e\u093e\u0928\u0915\u0940\u0915\u0943\u0924 \u0930\u0942\u092a \u0939\u0948 \u091c\u093f\u0938\u092e\u0947\u0902 \u0938\u0902\u0938\u094d\u0915\u0943\u0924 \u0915\u0947 \u0924\u0924\u094d\u0938\u092e \u0924\u0925\u093e \u0924\u0926\u094d\u092d\u0935 \u0936\u092c\u094d\u0926\u094b\u0902 \u0915\u093e \u092a\u094d\u0930\u092f\u094b\u0917 \u0905\u0927\u093f\u0915 \u0939\u0948 \u0914\u0930 \u0905\u0930\u092c\u0940-\u092b\u093c\u093e\u0930\u0938\u0940 \u0936\u092c\u094d\u0926 \u0915\u092e \u0939\u0948\u0902\u0964 \u0939\u093f\u0902\u0926\u0940 \u0938\u0902\u0935\u0948\u0927\u093e\u0928\u093f\u0915 \u0930\u0942\u092a \u0938\u0947 \u092d\u093e\u0930\u0924 \u0915\u0940 \u0930\u093e\u091c\u092d\u093e\u0937\u093e \u0914\u0930 \u092d\u093e\u0930\u0924 \u0915\u0940 \u0938\u092c\u0938\u0947 \u0905\u0927\u093f\u0915 \u092c\u094b\u0932\u0940 \u0914\u0930 \u0938\u092e\u091d\u0940 \u091c\u093e\u0928\u0947 \u0935\u093e\u0932\u0940 \u092d\u093e\u0937\u093e \u0939\u0948\u0964 \u0939\u093e\u0932\u093e\u0901\u0915\u093f, \u0939\u093f\u0928\u094d\u0926\u0940 \u092d\u093e\u0930\u0924 \u0915\u0940 \u0930\u093e\u0937\u094d\u091f\u094d\u0930\u092d\u093e\u0937\u093e \u0928\u0939\u0940\u0902 \u0939\u0948,[3] \u0915\u094d\u092f\u094b\u0902\u0915\u093f \u092d\u093e\u0930\u0924 \u0915\u0947 \u0938\u0902\u0935\u093f\u0927\u093e\u0928 \u092e\u0947\u0902 \u0915\u094b\u0908 \u092d\u0940 \u092d\u093e\u0937\u093e \u0915\u094b \u0910\u0938\u093e \u0926\u0930\u094d\u091c\u093e \u0928\u0939\u0940\u0902 \u0926\u093f\u092f\u093e \u0917\u092f\u093e \u0925\u093e\u0964[4][5] \u091a\u0940\u0928\u0940 \u0915\u0947 \u092c\u093e\u0926 \u092f\u0939 \u0935\u093f\u0936\u094d\u0935 \u092e\u0947\u0902 \u0938\u092c\u0938\u0947 \u0905\u0927\u093f\u0915 \u092c\u094b\u0932\u0940 \u091c\u093e\u0928\u0947 \u0935\u093e\u0932\u0940 \u092d\u093e\u0937\u093e \u092d\u0940 \u0939\u0948\u0964 \u0935\u093f\u0936\u094d\u0935 \u0906\u0930\u094d\u0925\u093f\u0915 \u092e\u0902\u091a \u0915\u0940 \u0917\u0923\u0928\u093e \u0915\u0947 \u0905\u0928\u0941\u0938\u093e\u0930 \u092f\u0939 \u0935\u093f\u0936\u094d\u0935 \u0915\u0940 \u0926\u0938 \u0936\u0915\u094d\u0924\u093f\u0936\u093e\u0932\u0940 \u092d\u093e\u0937\u093e\u0913\u0902 \u092e\u0947\u0902 \u0938\u0947 \u090f\u0915 \u0939\u0948\u0964[6]'\n>>> TEXT_KAN = '\u0ca6\u0ccd\u0cb0\u0cbe\u0cb5\u0cbf\u0ca1 \u0cad\u0cbe\u0cb7\u0cc6\u0c97\u0cb3\u0cb2\u0ccd\u0cb2\u0cbf \u0caa\u0ccd\u0cb0\u0cbe\u0cae\u0cc1\u0c96\u0ccd\u0caf\u0cb5\u0cc1\u0cb3\u0ccd\u0cb3 \u0cad\u0cbe\u0cb7\u0cc6\u0caf\u0cc2 \u0cad\u0cbe\u0cb0\u0ca4\u0ca6 \u0caa\u0cc1\u0cb0\u0cbe\u0ca4\u0ca8\u0cb5\u0cbe\u0ca6 \u0cad\u0cbe\u0cb7\u0cc6\u0c97\u0cb3\u0cb2\u0ccd\u0cb2\u0cbf \u0c92\u0c82\u0ca6\u0cc2 \u0c86\u0c97\u0cbf\u0cb0\u0cc1\u0cb5 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cad\u0cbe\u0cb7\u0cc6\u0caf\u0ca8\u0ccd\u0ca8\u0cc1 \u0c85\u0ca6\u0cb0 \u0cb5\u0cbf\u0cb5\u0cbf\u0ca7 \u0cb0\u0cc2\u0caa\u0c97\u0cb3\u0cb2\u0ccd\u0cb2\u0cbf \u0cb8\u0cc1\u0cae\u0cbe\u0cb0\u0cc1 \u0cea\u0ceb \u0ca6\u0cb6\u0cb2\u0c95\u0ccd\u0cb7 \u0c9c\u0ca8\u0cb0\u0cc1 \u0c86\u0ca1\u0cc1 \u0ca8\u0cc1\u0ca1\u0cbf\u0caf\u0cbe\u0c97\u0cbf \u0cac\u0cb3\u0cb8\u0cc1\u0ca4\u0ccd\u0ca4\u0cb2\u0cbf\u0ca6\u0ccd\u0ca6\u0cbe\u0cb0\u0cc6. \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95 \u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0c86\u0ca1\u0cb3\u0cbf\u0ca4 \u0cad\u0cbe\u0cb7\u0cc6.[\u0ce7\u0ce7] \u0c9c\u0c97\u0ca4\u0ccd\u0ca4\u0cbf\u0ca8\u0cb2\u0ccd\u0cb2\u0cbf \u0c85\u0ca4\u0ccd\u0caf\u0c82\u0ca4 \u0cb9\u0cc6\u0c9a\u0ccd\u0c9a\u0cc1 \u0cae\u0c82\u0ca6\u0cbf \u0cae\u0cbe\u0ca4\u0ca8\u0cbe\u0ca1\u0cc1\u0cb5 \u0cad\u0cbe\u0cb7\u0cc6\u0caf\u0cc6\u0c82\u0cac \u0ca8\u0cc6\u0cb2\u0cc6\u0caf\u0cb2\u0ccd\u0cb2\u0cbf \u0c87\u0caa\u0ccd\u0caa\u0ca4\u0cca\u0c82\u0cac\u0ca4\u0ccd\u0ca4\u0ca8\u0cc6\u0caf \u0cb8\u0ccd\u0ca5\u0cbe\u0ca8 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\u0c95\u0ccd\u0c95\u0cbf\u0ca6\u0cc6. \u0ce8\u0ce6\u0ce7\u0ce7\u0cb0 \u0c9c\u0ca8\u0c97\u0ca3\u0ca4\u0cbf\u0caf \u0caa\u0ccd\u0cb0\u0c95\u0cbe\u0cb0 \u0c9c\u0c97\u0ca4\u0ccd\u0ca4\u0cbf\u0ca8\u0cb2\u0ccd\u0cb2\u0cbf \u0cec.\u0cea \u0c95\u0ccb\u0c9f\u0cbf \u0c9c\u0ca8\u0c97\u0cb3\u0cc1 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cae\u0cbe\u0ca4\u0ca8\u0cbe\u0ca1\u0cc1\u0ca4\u0ccd\u0ca4\u0cbe\u0cb0\u0cc6 \u0c8e\u0c82\u0ca6\u0cc1 \u0ca4\u0cbf\u0cb3\u0cbf\u0ca6\u0cc1\u0cac\u0c82\u0ca6\u0cbf\u0ca6\u0cc6. \u0c87\u0cb5\u0cb0\u0cb2\u0ccd\u0cb2\u0cbf \u0ceb.\u0ceb \u0c95\u0ccb\u0c9f\u0cbf \u0c9c\u0ca8\u0c97\u0cb3 \u0cae\u0cbe\u0ca4\u0cc3\u0cad\u0cbe\u0cb7\u0cc6 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\u0cb5\u0cbe\u0c97\u0cbf\u0ca6\u0cc6. \u0cac\u0ccd\u0cb0\u0cbe\u0cb9\u0ccd\u0cae\u0cbf \u0cb2\u0cbf\u0caa\u0cbf\u0caf\u0cbf\u0c82\u0ca6 \u0cb0\u0cc2\u0caa\u0cc1\u0c97\u0cca\u0c82\u0ca1 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cb2\u0cbf\u0caa\u0cbf\u0caf\u0ca8\u0ccd\u0ca8\u0cc1 \u0c89\u0caa\u0caf\u0ccb\u0c97\u0cbf\u0cb8\u0cbf \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cad\u0cbe\u0cb7\u0cc6\u0caf\u0ca8\u0ccd\u0ca8\u0cc1 \u0cac\u0cb0\u0cc6\u0caf\u0cb2\u0cbe\u0c97\u0cc1\u0ca4\u0ccd\u0ca4\u0ca6\u0cc6. \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cac\u0cb0\u0cb9\u0ca6 \u0cae\u0cbe\u0ca6\u0cb0\u0cbf\u0c97\u0cb3\u0cbf\u0c97\u0cc6 \u0cb8\u0cbe\u0cb5\u0cbf\u0cb0\u0ca6 \u0c90\u0ca8\u0cc2\u0cb0\u0cc1 \u0cb5\u0cb0\u0cc1\u0cb7\u0c97\u0cb3 \u0c9a\u0cb0\u0cbf\u0ca4\u0ccd\u0cb0\u0cc6\u0caf\u0cbf\u0ca6\u0cc6. \u0c95\u0ccd\u0cb0\u0cbf.\u0cb6. \u0c86\u0cb0\u0ca8\u0cc6\u0caf \u0cb6\u0ca4\u0cae\u0cbe\u0ca8\u0ca6 \u0caa\u0cb6\u0ccd\u0c9a\u0cbf\u0cae \u0c97\u0c82\u0c97 \u0cb8\u0cbe\u0cae\u0ccd\u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0c95\u0cbe\u0cb2\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf [\u0ce7\u0ce8] \u0cae\u0ca4\u0ccd\u0ca4\u0cc1 \u0c92\u0c82\u0cac\u0ca4\u0ccd\u0ca4\u0ca8\u0cc6\u0caf \u0cb6\u0ca4\u0cae\u0cbe\u0ca8\u0ca6 \u0cb0\u0cbe\u0cb7\u0ccd\u0c9f\u0ccd\u0cb0\u0c95\u0cc2\u0c9f \u0cb8\u0cbe\u0cae\u0ccd\u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0c95\u0cbe\u0cb2\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf \u0cb9\u0cb3\u0c97\u0ca8\u0ccd\u0ca8\u0ca1 \u0cb8\u0cbe\u0cb9\u0cbf\u0ca4\u0ccd\u0caf \u0c85\u0ca4\u0ccd\u0caf\u0c82\u0ca4 \u0cb9\u0cc6\u0c9a\u0ccd\u0c9a\u0cbf\u0ca8 \u0cb0\u0cbe\u0c9c\u0cbe\u0cb6\u0ccd\u0cb0\u0caf \u0caa\u0ca1\u0cc6\u0caf\u0cbf\u0ca4\u0cc1.[\u0ce7\u0ce9][\u0ce7\u0cea] \u0c85\u0ca6\u0cb2\u0ccd\u0cb2\u0ca6\u0cc6 \u0cb8\u0cbe\u0cb5\u0cbf\u0cb0 \u0cb5\u0cb0\u0cc1\u0cb7\u0c97\u0cb3 \u0cb8\u0cbe\u0cb9\u0cbf\u0ca4\u0ccd\u0caf \u0caa\u0cb0\u0c82\u0caa\u0cb0\u0cc6 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\u0c95\u0ccd\u0c95\u0cbf\u0ca6\u0cc6.[\u0ce7\u0ceb]\u0cb5\u0cbf\u0ca8\u0ccb\u0cac\u0cbe \u0cad\u0cbe\u0cb5\u0cc6 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cb2\u0cbf\u0caa\u0cbf\u0caf\u0ca8\u0ccd\u0ca8\u0cc1 \u0cb2\u0cbf\u0caa\u0cbf\u0c97\u0cb3 \u0cb0\u0cbe\u0ca3\u0cbf\u0caf\u0cc6\u0c82\u0ca6\u0cc1 \u0cb9\u0cca\u0c97\u0cb3\u0cbf\u0ca6\u0ccd\u0ca6\u0cbe\u0cb0\u0cc6.[\u0cb8\u0cc2\u0c95\u0ccd\u0ca4 \u0c89\u0cb2\u0ccd\u0cb2\u0cc7\u0c96\u0ca8 \u0cac\u0cc7\u0c95\u0cc1]'\n>>> TEXT_SIN = '\u0dc1\u0dca\u200d\u0dbb\u0dd3 \u0dbd\u0d82\u0d9a\u0dcf\u0dc0\u0dda \u0db4\u0dca\u200d\u0dbb\u0db0\u0dcf\u0db1 \u0da2\u0dcf\u0dad\u0dd2\u0dba \u0dc0\u0db1 \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0da2\u0db1\u0dba\u0dcf\u0d9c\u0dda \u0db8\u0dc0\u0dca \u0db6\u0dc3 \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0dc0\u0dd9\u0dba\u0dd2. \u0d85\u0daf \u0dc0\u0db1 \u0dc0\u0dd2\u0da7 \u0db8\u0dd2\u0dbd\u0dd2\u0dba\u0db1 20 \u0d9a\u0da7 \u0d85\u0db0\u0dd2\u0d9a \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0dc3\u0dc4 \u0db8\u0dd2\u0dbd\u0dd2\u0dba\u0db1 3\u0d9a\u0da7 \u0d85\u0db0\u0dd2\u0d9a \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0db1\u0ddc\u0dc0\u0db1 \u0da2\u0db1\u0d9c\u0dc4\u0db1\u0dba\u0d9a\u0dca \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0db7\u0dcf\u0dc2\u0dcf\u0dc0 \u0db7\u0dcf\u0dc0\u0dd2\u0dad \u0d9a\u0dbb\u0dad\u0dd2. \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\u200d \u0d89\u0db1\u0dca\u0daf\u0dd4-\u0dba\u0dd4\u0dbb\u0ddd\u0db4\u0dd3\u0dba \u0db7\u0dcf\u0dc2\u0dcf\u0dc0\u0dbd \u0d8b\u0db4 \u0d9c\u0dab\u0dba\u0d9a\u0dca \u0dc0\u0db1 \u0d89\u0db1\u0dca\u0daf\u0dd4-\u0d86\u0dbb\u0dca\u0dba \u0db7\u0dcf\u0dc2\u0dcf \u0d9c\u0dab\u0dba\u0da7 \u0d85\u0dba\u0dd2\u0dad\u0dd2 \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0db8\u0dcf\u0dbd \u0daf\u0dd2\u0dc0\u0dba\u0dd2\u0db1 \u0db7\u0dcf\u0dc0\u0dd2\u0dad \u0d9a\u0dbb\u0db1 \u0daf\u0dd2\u0dc0\u0dd9\u0dc4\u0dd2 \u0db7\u0dcf\u0dc2\u0dcf\u0dc0 \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\u0dba\u0dd9\u0db1\u0dca \u0db4\u0dd0\u0dc0\u0dad \u0d91\u0db1\u0dca\u0db1\u0d9a\u0dd2. \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0dc1\u0dca\u200d\u0dbb\u0dd3 \u0dbd\u0d82\u0d9a\u0dcf\u0dc0\u0dda \u0db1\u0dd2\u0dbd \u0db7\u0dcf\u0dc2\u0dcf\u0dc0\u0dba\u0dd2 .'\n>>> TEXT_TAM = '\u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0bae\u0bca\u0bb4\u0bbf (Tamil language) \u0ba4\u0bae\u0bbf\u0bb4\u0bb0\u0bcd\u0b95\u0bb3\u0bbf\u0ba9\u0ba4\u0bc1\u0bae\u0bcd, \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0baa\u0bc7\u0b9a\u0bc1\u0bae\u0bcd \u0baa\u0bb2\u0bb0\u0ba4\u0bc1\u0bae\u0bcd \u0ba4\u0bbe\u0baf\u0bcd\u0bae\u0bca\u0bb4\u0bbf \u0b86\u0b95\u0bc1\u0bae\u0bcd. \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0ba4\u0bbf\u0bb0\u0bbe\u0bb5\u0bbf\u0b9f \u0bae\u0bca\u0bb4\u0bbf\u0b95\u0bcd \u0b95\u0bc1\u0b9f\u0bc1\u0bae\u0bcd\u0baa\u0ba4\u0bcd\u0ba4\u0bbf\u0ba9\u0bcd \u0bae\u0bc1\u0ba4\u0ba9\u0bcd\u0bae\u0bc8\u0baf\u0bbe\u0ba9 \u0bae\u0bca\u0bb4\u0bbf\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0b92\u0ba9\u0bcd\u0bb1\u0bc1\u0bae\u0bcd \u0b9a\u0bc6\u0bae\u0bcd\u0bae\u0bca\u0bb4\u0bbf\u0baf\u0bc1\u0bae\u0bcd \u0b86\u0b95\u0bc1\u0bae\u0bcd. \u0b87\u0ba8\u0bcd\u0ba4\u0bbf\u0baf\u0bbe, \u0b87\u0bb2\u0b99\u0bcd\u0b95\u0bc8, \u0bae\u0bb2\u0bc7\u0b9a\u0bbf\u0baf\u0bbe, \u0b9a\u0bbf\u0b99\u0bcd\u0b95\u0baa\u0bcd\u0baa\u0bc2\u0bb0\u0bcd \u0b86\u0b95\u0bbf\u0baf \u0ba8\u0bbe\u0b9f\u0bc1\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0b85\u0ba4\u0bbf\u0b95 \u0b85\u0bb3\u0bb5\u0bbf\u0bb2\u0bc1\u0bae\u0bcd, \u0b90\u0b95\u0bcd\u0b95\u0bbf\u0baf \u0b85\u0bb0\u0baa\u0bc1 \u0b85\u0bae\u0bc0\u0bb0\u0b95\u0bae\u0bcd, \u0ba4\u0bc6\u0ba9\u0bcd\u0ba9\u0bbe\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0bbe, \u0bae\u0bca\u0bb0\u0bbf\u0b9a\u0bbf\u0baf\u0b9a\u0bc1, \u0baa\u0bbf\u0b9c\u0bbf, \u0bb0\u0bc0\u0baf\u0bc2\u0ba9\u0bbf\u0baf\u0ba9\u0bcd, \u0b9f\u0bbf\u0bb0\u0bbf\u0ba9\u0bbf\u0b9f\u0bbe\u0b9f\u0bcd \u0baa\u0bcb\u0ba9\u0bcd\u0bb1 \u0ba8\u0bbe\u0b9f\u0bc1\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0b9a\u0bbf\u0bb1\u0bbf\u0baf \u0b85\u0bb3\u0bb5\u0bbf\u0bb2\u0bc1\u0bae\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0baa\u0bc7\u0b9a\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0b95\u0bbf\u0bb1\u0ba4\u0bc1. 1997\u0b86\u0bae\u0bcd \u0b86\u0ba3\u0bcd\u0b9f\u0bc1\u0baa\u0bcd \u0baa\u0bc1\u0bb3\u0bcd\u0bb3\u0bbf \u0bb5\u0bbf\u0bb5\u0bb0\u0baa\u0bcd\u0baa\u0b9f\u0bbf \u0b89\u0bb2\u0b95\u0bae\u0bcd \u0bae\u0bc1\u0bb4\u0bc1\u0bb5\u0ba4\u0bbf\u0bb2\u0bc1\u0bae\u0bcd 8 \u0b95\u0bcb\u0b9f\u0bbf (80 \u0bae\u0bbf\u0bb2\u0bcd\u0bb2\u0bbf\u0baf\u0ba9\u0bcd) \u0bae\u0b95\u0bcd\u0b95\u0bb3\u0bbe\u0bb2\u0bcd \u0baa\u0bc7\u0b9a\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd[13], \u0b92\u0bb0\u0bc1 \u0bae\u0bca\u0bb4\u0bbf\u0baf\u0bc8\u0ba4\u0bcd \u0ba4\u0bbe\u0baf\u0bcd\u0bae\u0bca\u0bb4\u0bbf\u0baf\u0bbe\u0b95\u0b95\u0bcd \u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bc1 \u0baa\u0bc7\u0b9a\u0bc1\u0bae\u0bcd \u0bae\u0b95\u0bcd\u0b95\u0bb3\u0bbf\u0ba9\u0bcd \u0b8e\u0ba3\u0bcd\u0ba3\u0bbf\u0b95\u0bcd\u0b95\u0bc8 \u0b85\u0b9f\u0bbf\u0baa\u0bcd\u0baa\u0b9f\u0bc8\u0baf\u0bbf\u0bb2\u0bcd \u0baa\u0ba4\u0bbf\u0ba9\u0bc6\u0b9f\u0bcd\u0b9f\u0bbe\u0bb5\u0ba4\u0bc1 \u0b87\u0b9f\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0b89\u0bb3\u0bcd\u0bb3\u0ba4\u0bc1.[14] \u0b87\u0ba3\u0bc8\u0baf\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0b85\u0ba4\u0bbf\u0b95\u0bae\u0bcd \u0baa\u0baf\u0ba9\u0bcd\u0baa\u0b9f\u0bc1\u0ba4\u0bcd\u0ba4\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd \u0b87\u0ba8\u0bcd\u0ba4\u0bbf\u0baf \u0bae\u0bca\u0bb4\u0bbf\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0bae\u0bc1\u0ba4\u0ba9\u0bcd\u0bae\u0bc8\u0baf\u0bbe\u0b95 \u0b89\u0bb3\u0bcd\u0bb3\u0ba4\u0bbe\u0b95 2017 \u0b86\u0bb5\u0ba4\u0bc1 \u0b86\u0ba3\u0bcd\u0b9f\u0bbf\u0bb2\u0bcd \u0ba8\u0b9f\u0bc8\u0baa\u0bc6\u0bb1\u0bcd\u0bb1 \u0b95\u0bc2\u0b95\u0bc1\u0bb3\u0bcd \u0b95\u0ba3\u0b95\u0bcd\u0b95\u0bc6\u0b9f\u0bc1\u0baa\u0bcd\u0baa\u0bbf\u0bb2\u0bcd \u0ba4\u0bc6\u0bb0\u0bbf\u0baf \u0bb5\u0ba8\u0bcd\u0ba4\u0ba4\u0bc1.[15]'\n>>> TEXT_TEL = '\u0c06\u0c02\u0c27\u0c4d\u0c30 \u0c2a\u0c4d\u0c30\u0c26\u0c47\u0c36\u0c4d, \u0c24\u0c46\u0c32\u0c02\u0c17\u0c3e\u0c23 \u0c30\u0c3e\u0c37\u0c4d\u0c1f\u0c4d\u0c30\u0c3e\u0c32 \u0c05\u0c27\u0c3f\u0c15\u0c3e\u0c30 \u0c2d\u0c3e\u0c37 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41. \u0c2d\u0c3e\u0c30\u0c24 \u0c26\u0c47\u0c36\u0c02\u0c32\u0c4b \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0c2e\u0c3e\u0c24\u0c43\u0c2d\u0c3e\u0c37\u0c17\u0c3e \u0c2e\u0c3e\u0c1f\u0c4d\u0c32\u0c3e\u0c21\u0c47 8.7 \u0c15\u0c4b\u0c1f\u0c4d\u0c32 (2001) \u0c1c\u0c28\u0c3e\u0c2d\u0c3e\u0c24\u0c4b [1] \u0c2a\u0c4d\u0c30\u0c3e\u0c02\u0c24\u0c40\u0c2f \u0c2d\u0c3e\u0c37\u0c32\u0c32\u0c4b \u0c2e\u0c4a\u0c26\u0c1f\u0c3f \u0c38\u0c4d\u0c25\u0c3e\u0c28\u0c02\u0c32\u0c4b \u0c09\u0c02\u0c26\u0c3f. \u0c2a\u0c4d\u0c30\u0c2a\u0c02\u0c1a\u0c02\u0c32\u0c4b\u0c28\u0c3f \u0c2a\u0c4d\u0c30\u0c1c\u0c32\u0c41 \u0c05\u0c24\u0c4d\u0c2f\u0c27\u0c3f\u0c15\u0c2e\u0c41\u0c17\u0c3e \u0c2e\u0c3e\u0c1f\u0c4d\u0c32\u0c3e\u0c21\u0c47 \u0c2d\u0c3e\u0c37\u0c32\u0c32\u0c4b 15 \u0c38\u0c4d\u0c25\u0c3e\u0c28\u0c2e\u0c41\u0c32\u0c4b\u0c28\u0c42, \u0c2d\u0c3e\u0c30\u0c24 \u0c26\u0c47\u0c36\u0c2e\u0c41\u0c32\u0c4b \u0c39\u0c3f\u0c02\u0c26\u0c40, \u0c24\u0c30\u0c4d\u0c35\u0c3e\u0c24 \u0c38\u0c4d\u0c25\u0c3e\u0c28\u0c2e\u0c41\u0c32\u0c4b\u0c28\u0c42 \u0c28\u0c3f\u0c32\u0c41\u0c38\u0c4d\u0c24\u0c41\u0c02\u0c26\u0c3f. \u0c2a\u0c3e\u0c24\u0c35\u0c48\u0c28 \u0c2a\u0c4d\u0c30\u0c2a\u0c02\u0c1a \u0c2d\u0c3e\u0c37 \u0c17\u0c23\u0c3e\u0c02\u0c15\u0c3e\u0c32 (\u0c0e\u0c25\u0c4d\u0c28\u0c4b\u0c32\u0c3e\u0c17\u0c4d) \u0c2a\u0c4d\u0c30\u0c15\u0c3e\u0c30\u0c02 \u0c2a\u0c4d\u0c30\u0c2a\u0c02\u0c1a\u0c35\u0c4d\u0c2f\u0c3e\u0c2a\u0c4d\u0c24\u0c02\u0c17\u0c3e 7.4 \u0c15\u0c4b\u0c1f\u0c4d\u0c32\u0c41 \u0c2e\u0c02\u0c26\u0c3f\u0c15\u0c3f \u0c2e\u0c3e\u0c24\u0c43\u0c2d\u0c3e\u0c37\u0c17\u0c3e \u0c09\u0c02\u0c26\u0c3f.[2] \u0c2e\u0c4a\u0c26\u0c1f\u0c3f \u0c2d\u0c3e\u0c37\u0c17\u0c3e \u0c2e\u0c3e\u0c1f\u0c4d\u0c32\u0c3e\u0c21\u0c24\u0c3e\u0c30\u0c41. \u0c05\u0c24\u0c3f \u0c2a\u0c4d\u0c30\u0c3e\u0c1a\u0c40\u0c28 \u0c26\u0c47\u0c36 \u0c2d\u0c3e\u0c37\u0c32\u0c32\u0c4b \u0c38\u0c02\u0c38\u0c4d\u0c15\u0c43\u0c24\u0c2e\u0c41 \u0c24\u0c2e\u0c3f\u0c33\u0c2e\u0c41\u0c24\u0c4b \u0c2c\u0c3e\u0c1f\u0c41 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0c2d\u0c3e\u0c37\u0c28\u0c41 2008 \u0c05\u0c15\u0c4d\u0c1f\u0c4b\u0c2c\u0c30\u0c41 31\u0c28 \u0c2d\u0c3e\u0c30\u0c24 \u0c2a\u0c4d\u0c30\u0c2d\u0c41\u0c24\u0c4d\u0c35\u0c2e\u0c41 \u0c17\u0c41\u0c30\u0c4d\u0c24\u0c3f\u0c02\u0c1a\u0c3f\u0c02\u0c26\u0c3f.'\n>>> TEXT_URD = '\u0627\u064f\u0631\u062f\u064f\u0648 \u0644\u0634\u06a9\u0631\u06cc \u0632\u0628\u0627\u0646[8] (\u06cc\u0627 \u062c\u062f\u06cc\u062f \u0645\u0639\u06cc\u0627\u0631\u06cc \u0627\u0631\u062f\u0648) \u0628\u0631\u0635\u063a\u06cc\u0631 \u06a9\u06cc \u0645\u0639\u06cc\u0627\u0631\u06cc \u0632\u0628\u0627\u0646\u0648\u06ba \u0645\u06cc\u06ba \u0633\u06d2 \u0627\u06cc\u06a9 \u06c1\u06d2\u06d4 \u06cc\u06c1 \u067e\u0627\u06a9\u0633\u062a\u0627\u0646 \u06a9\u06cc \u0642\u0648\u0645\u06cc \u0627\u0648\u0631 \u0631\u0627\u0628\u0637\u06c1 \u0639\u0627\u0645\u06c1 \u06a9\u06cc \u0632\u0628\u0627\u0646 \u06c1\u06d2\u060c \u062c\u0628\u06a9\u06c1 \u0628\u06be\u0627\u0631\u062a \u06a9\u06cc \u0686\u06be\u06d2 \u0631\u06cc\u0627\u0633\u062a\u0648\u06ba \u06a9\u06cc \u062f\u0641\u062a\u0631\u06cc \u0632\u0628\u0627\u0646 \u06a9\u0627 \u062f\u0631\u062c\u06c1 \u0631\u06a9\u06be\u062a\u06cc \u06c1\u06d2\u06d4 \u0622\u0626\u06cc\u0646 \u06c1\u0646\u062f \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0627\u0633\u06d2 22 \u062f\u0641\u062a\u0631\u06cc \u0634\u0646\u0627\u062e\u062a \u0632\u0628\u0627\u0646\u0648\u06ba \u0645\u06cc\u06ba \u0634\u0627\u0645\u0644 \u06a9\u06cc\u0627 \u062c\u0627\u0686\u06a9\u0627 \u06c1\u06d2\u06d4 2001\u0621 \u06a9\u06cc \u0645\u0631\u062f\u0645 \u0634\u0645\u0627\u0631\u06cc \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0627\u0631\u062f\u0648 \u06a9\u0648 \u0628\u0637\u0648\u0631 \u0645\u0627\u062f\u0631\u06cc \u0632\u0628\u0627\u0646 \u0628\u06be\u0627\u0631\u062a \u0645\u06cc\u06ba 5.01% \u0641\u06cc\u0635\u062f \u0644\u0648\u06af \u0628\u0648\u0644\u062a\u06d2 \u06c1\u06cc\u06ba \u0627\u0648\u0631 \u0627\u0633 \u0644\u062d\u0627\u0638 \u0633\u06d2 \u06cc\u06c1 \u0628\u06be\u0627\u0631\u062a \u06a9\u06cc \u0686\u06be\u0679\u06cc \u0628\u0691\u06cc \u0632\u0628\u0627\u0646 \u06c1\u06d2 \u062c\u0628\u06a9\u06c1 \u067e\u0627\u06a9\u0633\u062a\u0627\u0646 \u0645\u06cc\u06ba \u0627\u0633\u06d2 \u0628\u0637\u0648\u0631 \u0645\u0627\u062f\u0631\u06cc \u0632\u0628\u0627\u0646 7.59% \u0641\u06cc\u0635\u062f \u0644\u0648\u06af \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u060c \u06cc\u06c1 \u067e\u0627\u06a9\u0633\u062a\u0627\u0646 \u06a9\u06cc \u067e\u0627\u0646\u0686\u0648\u06cc\u06ba \u0628\u0691\u06cc \u0632\u0628\u0627\u0646 \u06c1\u06d2\u06d4 \u0627\u0631\u062f\u0648 \u062a\u0627\u0631\u06cc\u062e\u06cc \u0637\u0648\u0631 \u067e\u0631 \u06c1\u0646\u062f\u0648\u0633\u062a\u0627\u0646 \u06a9\u06cc \u0645\u0633\u0644\u0645 \u0622\u0628\u0627\u062f\u06cc \u0633\u06d2 \u062c\u0691\u06cc \u06c1\u06d2\u06d4[\u062d\u0648\u0627\u0644\u06c1 \u062f\u0631\u06a9\u0627\u0631] \u0628\u0639\u0636 \u0630\u062e\u06cc\u0631\u06c1 \u0627\u0644\u0641\u0627\u0638 \u06a9\u06d2 \u0639\u0644\u0627\u0648\u06c1 \u06cc\u06c1 \u0632\u0628\u0627\u0646 \u0645\u0639\u06cc\u0627\u0631\u06cc \u06c1\u0646\u062f\u06cc \u0633\u06d2 \u0642\u0627\u0628\u0644 \u0641\u06c1\u0645 \u06c1\u06d2 \u062c\u0648 \u0627\u0633 \u062e\u0637\u06d2 \u06a9\u06cc \u06c1\u0646\u062f\u0648\u0624\u06ba \u0633\u06d2 \u0645\u0646\u0633\u0648\u0628 \u06c1\u06d2\u06d4[\u062d\u0648\u0627\u0644\u06c1 \u062f\u0631\u06a9\u0627\u0631] \u0632\u0628\u0627\u0646\u0650 \u0627\u0631\u062f\u0648 \u06a9\u0648 \u067e\u06c1\u0686\u0627\u0646 \u0648 \u062a\u0631\u0642\u06cc \u0627\u0633 \u0648\u0642\u062a \u0645\u0644\u06cc \u062c\u0628 \u0628\u0631\u0637\u0627\u0646\u0648\u06cc \u062f\u0648\u0631 \u0645\u06cc\u06ba \u0627\u0646\u06af\u0631\u06cc\u0632 \u062d\u06a9\u0645\u0631\u0627\u0646\u0648\u06ba \u0646\u06d2 \u0627\u0633\u06d2 \u0641\u0627\u0631\u0633\u06cc \u06a9\u06cc \u0628\u062c\u0627\u0626\u06d2 \u0627\u0646\u06af\u0631\u06cc\u0632\u06cc \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0634\u0645\u0627\u0644\u06cc \u06c1\u0646\u062f\u0648\u0633\u062a\u0627\u0646 \u06a9\u06d2 \u0639\u0644\u0627\u0642\u0648\u06ba \u0627\u0648\u0631 \u062c\u0645\u0648\u06ba \u0648 \u06a9\u0634\u0645\u06cc\u0631 \u0645\u06cc\u06ba \u0627\u0633\u06d2 \u0633\u0646\u06c1 1846\u0621 \u0627\u0648\u0631 \u067e\u0646\u062c\u0627\u0628 \u0645\u06cc\u06ba \u0633\u0646\u06c1 1849\u0621 \u0645\u06cc\u06ba \u0628\u0637\u0648\u0631 \u062f\u0641\u062a\u0631\u06cc \u0632\u0628\u0627\u0646 \u0646\u0627\u0641\u0630 \u06a9\u06cc\u0627\u06d4 \u0627\u0633 \u06a9\u06d2 \u0639\u0644\u0627\u0648\u06c1 \u062e\u0644\u06cc\u062c\u06cc\u060c \u06cc\u0648\u0631\u067e\u06cc\u060c \u0627\u06cc\u0634\u06cc\u0627\u0626\u06cc \u0627\u0648\u0631 \u0627\u0645\u0631\u06cc\u06a9\u06cc \u0639\u0644\u0627\u0642\u0648\u06ba \u0645\u06cc\u06ba \u0627\u0631\u062f\u0648 \u0628\u0648\u0644\u0646\u06d2 \u0648\u0627\u0644\u0648\u06ba \u06a9\u06cc \u0627\u06cc\u06a9 \u0628\u0691\u06cc \u062a\u0639\u062f\u0627\u062f \u0622\u0628\u0627\u062f \u06c1\u06d2 \u062c\u0648 \u0628\u0646\u06cc\u0627\u062f\u06cc \u0637\u0648\u0631 \u067e\u0631 \u062c\u0646\u0648\u0628\u06cc \u0627\u06cc\u0634\u06cc\u0627\u0621 \u0633\u06d2 \u06a9\u0648\u0686 \u06a9\u0631\u0646\u06d2 \u0648\u0627\u0644\u06d2 \u0627\u06c1\u0644\u0650 \u0627\u0631\u062f\u0648 \u06c1\u06cc\u06ba\u06d4 1999\u0621 \u06a9\u06d2 \u0627\u0639\u062f\u0627\u062f \u0648\u0634\u0645\u0627\u0631 \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0627\u0631\u062f\u0648 \u0632\u0628\u0627\u0646 \u06a9\u06d2 \u0645\u062c\u0645\u0648\u0639\u06cc \u0645\u062a\u06a9\u0644\u0645\u06cc\u0646 \u06a9\u06cc \u062a\u0639\u062f\u0627\u062f \u062f\u0633 \u06a9\u0631\u0648\u0691 \u0633\u0627\u0679\u06be \u0644\u0627\u06a9\u06be \u06a9\u06d2 \u0644\u06af \u0628\u06be\u06af \u062a\u06be\u06cc\u06d4 \u0627\u0633 \u0644\u062d\u0627\u0638 \u0633\u06d2 \u06cc\u06c1 \u062f\u0646\u06cc\u0627 \u06a9\u06cc \u0646\u0648\u06cc\u06ba \u0628\u0691\u06cc \u0632\u0628\u0627\u0646 \u06c1\u06d2\u06d4'\n\n>>> nlp = spacy.blank('bn');nlp.add_pipe(nlp.create_pipe('sentencizer'));doc = nlp(TEXT_BEN);list(doc.sents);len(list(doc.sents))\n[\u09ac\u09be\u0982\u09b2\u09be \u09ad\u09be\u09b7\u09be (\u09ac\u09be\u0999\u09b2\u09be, \u09ac\u09be\u0999\u09cd\u0997\u09b2\u09be, \u09a4\u09a5\u09be \u09ac\u09be\u0999\u09cd\u0997\u09be\u09b2\u09be \u09a8\u09be\u09ae\u0997\u09c1\u09b2\u09cb\u09a4\u09c7\u0993 \u09aa\u09b0\u09bf\u099a\u09bf\u09a4) \u098f\u0995\u099f\u09bf \u0987\u09a8\u09cd\u09a6\u09cb-\u0986\u09b0\u09cd\u09af \u09ad\u09be\u09b7\u09be, \u09af\u09be \u09a6\u0995\u09cd\u09b7\u09bf\u09a3 \u098f\u09b6\u09bf\u09af\u09bc\u09be\u09b0 \u09ac\u09be\u0999\u09be\u09b2\u09bf \u099c\u09be\u09a4\u09bf\u09b0 \u09aa\u09cd\u09b0\u09a7\u09be\u09a8 \u0995\u09a5\u09cd\u09af \u0993 \u09b2\u09c7\u0996\u09cd\u09af \u09ad\u09be\u09b7\u09be\u0964 \u09ae\u09be\u09a4\u09c3\u09ad\u09be\u09b7\u09c0\u09b0 \u09b8\u0982\u0996\u09cd\u09af\u09be\u09af\u09bc \u09ac\u09be\u0982\u09b2\u09be \u0987\u09a8\u09cd\u09a6\u09cb-\u0987\u0989\u09b0\u09cb\u09aa\u09c0\u09af\u09bc \u09ad\u09be\u09b7\u09be \u09aa\u09b0\u09bf\u09ac\u09be\u09b0\u09c7\u09b0 \u099a\u09a4\u09c1\u09b0\u09cd\u09a5 \u0993 \u09ac\u09bf\u09b6\u09cd\u09ac\u09c7\u09b0 \u09b7\u09b7\u09cd\u09a0 \u09ac\u09c3\u09b9\u09a4\u09cd\u09a4\u09ae \u09ad\u09be\u09b7\u09be\u0964[\u09eb] \u09ae\u09cb\u099f \u09ac\u09cd\u09af\u09ac\u09b9\u09be\u09b0\u0995\u09be\u09b0\u09c0\u09b0 \u09b8\u0982\u0996\u09cd\u09af\u09be \u0985\u09a8\u09c1\u09b8\u09be\u09b0\u09c7 \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09bf\u09b6\u09cd\u09ac\u09c7\u09b0 \u09b8\u09aa\u09cd\u09a4\u09ae \u09ac\u09c3\u09b9\u09a4\u09cd\u09a4\u09ae \u09ad\u09be\u09b7\u09be\u0964 \u09ac\u09be\u0982\u09b2\u09be \u09b8\u09be\u09b0\u09cd\u09ac\u09ad\u09cc\u09ae \u09ad\u09be\u09b7\u09be\u09ad\u09bf\u09a4\u09cd\u09a4\u09bf\u0995 \u099c\u09be\u09a4\u09bf\u09b0\u09be\u09b7\u09cd\u099f\u09cd\u09b0 \u09ac\u09be\u0982\u09b2\u09be\u09a6\u09c7\u09b6\u09c7\u09b0 \u098f\u0995\u09ae\u09be\u09a4\u09cd\u09b0 \u09b0\u09be\u09b7\u09cd\u099f\u09cd\u09b0\u09ad\u09be\u09b7\u09be \u09a4\u09a5\u09be \u09b8\u09b0\u0995\u09be\u09b0\u09bf \u09ad\u09be\u09b7\u09be[\u09ec] \u098f\u09ac\u0982 \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u09aa\u09b6\u09cd\u099a\u09bf\u09ae\u09ac\u0999\u09cd\u0997, \u09a4\u09cd\u09b0\u09bf\u09aa\u09c1\u09b0\u09be, \u0986\u09b8\u09be\u09ae\u09c7\u09b0 \u09ac\u09b0\u09be\u0995 \u0989\u09aa\u09a4\u09cd\u09af\u0995\u09be\u09b0 \u09b8\u09b0\u0995\u09be\u09b0\u09bf \u09ad\u09be\u09b7\u09be\u0964 \u09ac\u0999\u09cd\u0997\u09cb\u09aa\u09b8\u09be\u0997\u09b0\u09c7 \u0985\u09ac\u09b8\u09cd\u09a5\u09bf\u09a4 \u0986\u09a8\u09cd\u09a6\u09be\u09ae\u09be\u09a8 \u09a6\u09cd\u09ac\u09c0\u09aa\u09aa\u09c1\u099e\u09cd\u099c\u09c7\u09b0 \u09aa\u09cd\u09b0\u09a7\u09be\u09a8 \u0995\u09a5\u09cd\u09af \u09ad\u09be\u09b7\u09be \u09ac\u09be\u0982\u09b2\u09be\u0964 \u098f\u099b\u09be\u09a1\u09bc\u09be \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u099d\u09be\u09a1\u09bc\u0996\u09a3\u09cd\u09a1, \u09ac\u09bf\u09b9\u09be\u09b0, \u09ae\u09c7\u0998\u09be\u09b2\u09af\u09bc, \u09ae\u09bf\u099c\u09cb\u09b0\u09be\u09ae, \u0989\u09a1\u09bc\u09bf\u09b7\u09cd\u09af\u09be \u09b0\u09be\u099c\u09cd\u09af\u0997\u09c1\u09b2\u09cb\u09a4\u09c7 \u0989\u09b2\u09cd\u09b2\u09c7\u0996\u09af\u09cb\u0997\u09cd\u09af \u09aa\u09b0\u09bf\u09ae\u09be\u09a3\u09c7 \u09ac\u09be\u0982\u09b2\u09be\u09ad\u09be\u09b7\u09c0 \u099c\u09a8\u0997\u09a3 \u09b0\u09af\u09bc\u09c7\u099b\u09c7\u0964 \u09ad\u09be\u09b0\u09a4\u09c7 \u09b9\u09bf\u09a8\u09cd\u09a6\u09bf\u09b0 \u09aa\u09b0\u09c7\u0987 \u09b8\u09b0\u09cd\u09ac\u09be\u09a7\u09bf\u0995 \u09aa\u09cd\u09b0\u099a\u09b2\u09bf\u09a4 \u09ad\u09be\u09b7\u09be \u09ac\u09be\u0982\u09b2\u09be\u0964[\u09ed][\u09ee] \u098f\u099b\u09be\u09a1\u09bc\u09be\u0993 \u09ae\u09a7\u09cd\u09af \u09aa\u09cd\u09b0\u09be\u099a\u09cd\u09af, \u0986\u09ae\u09c7\u09b0\u09bf\u0995\u09be \u0993 \u0987\u0989\u09b0\u09cb\u09aa\u09c7 \u0989\u09b2\u09cd\u09b2\u09c7\u0996\u09af\u09cb\u0997\u09cd\u09af \u09aa\u09b0\u09bf\u09ae\u09be\u09a3\u09c7 \u09ac\u09be\u0982\u09b2\u09be\u09ad\u09be\u09b7\u09c0 \u0985\u09ad\u09bf\u09ac\u09be\u09b8\u09c0 \u09b0\u09af\u09bc\u09c7\u099b\u09c7\u0964[\u09ef] \u09b8\u09be\u09b0\u09be \u09ac\u09bf\u09b6\u09cd\u09ac\u09c7 \u09b8\u09ac \u09ae\u09bf\u09b2\u09bf\u09af\u09bc\u09c7 \u09e8\u09ec \u0995\u09cb\u099f\u09bf\u09b0 \u0985\u09a7\u09bf\u0995 \u09b2\u09cb\u0995 \u09a6\u09c8\u09a8\u09a8\u09cd\u09a6\u09bf\u09a8 \u099c\u09c0\u09ac\u09a8\u09c7 \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09cd\u09af\u09ac\u09b9\u09be\u09b0 \u0995\u09b0\u09c7\u0964[\u09e8] \u09ac\u09be\u0982\u09b2\u09be\u09a6\u09c7\u09b6\u09c7\u09b0 \u099c\u09be\u09a4\u09c0\u09af\u09bc \u09b8\u0999\u09cd\u0997\u09c0\u09a4 \u098f\u09ac\u0982 \u09ad\u09be\u09b0\u09a4\u09c7\u09b0 \u099c\u09be\u09a4\u09c0\u09af\u09bc \u09b8\u0999\u09cd\u0997\u09c0\u09a4 \u0993 \u09b8\u09cd\u09a4\u09cb\u09a4\u09cd\u09b0 \u09ac\u09be\u0982\u09b2\u09be\u09a4\u09c7 \u09b0\u099a\u09bf\u09a4\u0964]\n1\n>>> nlp = spacy.blank('hi');nlp.add_pipe(nlp.create_pipe('sentencizer'));doc = nlp(TEXT_HIN);list(doc.sents);len(list(doc.sents))\n[\u0939\u093f\u0928\u094d\u0926\u0940 \u0935\u093f\u0936\u094d\u0935 \u0915\u0940 \u090f\u0915 \u092a\u094d\u0930\u092e\u0941\u0916 \u092d\u093e\u0937\u093e \u0939\u0948 \u090f\u0935\u0902 \u092d\u093e\u0930\u0924 \u0915\u0940 \u0930\u093e\u091c\u092d\u093e\u0937\u093e \u0939\u0948\u0964 \u0915\u0947\u0928\u094d\u0926\u094d\u0930\u0940\u092f \u0938\u094d\u0924\u0930 \u092a\u0930 \u092d\u093e\u0930\u0924 \u092e\u0947\u0902 \u0926\u0942\u0938\u0930\u0940 \u0906\u0927\u093f\u0915\u093e\u0930\u093f\u0915 \u092d\u093e\u0937\u093e \u0905\u0902\u0917\u094d\u0930\u0947\u091c\u0940 \u0939\u0948\u0964 \u092f\u0939 \u0939\u093f\u0902\u0926\u0941\u0938\u094d\u0924\u093e\u0928\u0940 \u092d\u093e\u0937\u093e \u0915\u0940 \u090f\u0915 \u092e\u093e\u0928\u0915\u0940\u0915\u0943\u0924 \u0930\u0942\u092a \u0939\u0948 \u091c\u093f\u0938\u092e\u0947\u0902 \u0938\u0902\u0938\u094d\u0915\u0943\u0924 \u0915\u0947 \u0924\u0924\u094d\u0938\u092e \u0924\u0925\u093e \u0924\u0926\u094d\u092d\u0935 \u0936\u092c\u094d\u0926\u094b\u0902 \u0915\u093e \u092a\u094d\u0930\u092f\u094b\u0917 \u0905\u0927\u093f\u0915 \u0939\u0948 \u0914\u0930 \u0905\u0930\u092c\u0940-\u092b\u093c\u093e\u0930\u0938\u0940 \u0936\u092c\u094d\u0926 \u0915\u092e \u0939\u0948\u0902\u0964 \u0939\u093f\u0902\u0926\u0940 \u0938\u0902\u0935\u0948\u0927\u093e\u0928\u093f\u0915 \u0930\u0942\u092a \u0938\u0947 \u092d\u093e\u0930\u0924 \u0915\u0940 \u0930\u093e\u091c\u092d\u093e\u0937\u093e \u0914\u0930 \u092d\u093e\u0930\u0924 \u0915\u0940 \u0938\u092c\u0938\u0947 \u0905\u0927\u093f\u0915 \u092c\u094b\u0932\u0940 \u0914\u0930 \u0938\u092e\u091d\u0940 \u091c\u093e\u0928\u0947 \u0935\u093e\u0932\u0940 \u092d\u093e\u0937\u093e \u0939\u0948\u0964 \u0939\u093e\u0932\u093e\u0901\u0915\u093f, \u0939\u093f\u0928\u094d\u0926\u0940 \u092d\u093e\u0930\u0924 \u0915\u0940 \u0930\u093e\u0937\u094d\u091f\u094d\u0930\u092d\u093e\u0937\u093e \u0928\u0939\u0940\u0902 \u0939\u0948,[3] \u0915\u094d\u092f\u094b\u0902\u0915\u093f \u092d\u093e\u0930\u0924 \u0915\u0947 \u0938\u0902\u0935\u093f\u0927\u093e\u0928 \u092e\u0947\u0902 \u0915\u094b\u0908 \u092d\u0940 \u092d\u093e\u0937\u093e \u0915\u094b \u0910\u0938\u093e \u0926\u0930\u094d\u091c\u093e \u0928\u0939\u0940\u0902 \u0926\u093f\u092f\u093e \u0917\u092f\u093e \u0925\u093e\u0964[4][5] \u091a\u0940\u0928\u0940 \u0915\u0947 \u092c\u093e\u0926 \u092f\u0939 \u0935\u093f\u0936\u094d\u0935 \u092e\u0947\u0902 \u0938\u092c\u0938\u0947 \u0905\u0927\u093f\u0915 \u092c\u094b\u0932\u0940 \u091c\u093e\u0928\u0947 \u0935\u093e\u0932\u0940 \u092d\u093e\u0937\u093e \u092d\u0940 \u0939\u0948\u0964 \u0935\u093f\u0936\u094d\u0935 \u0906\u0930\u094d\u0925\u093f\u0915 \u092e\u0902\u091a \u0915\u0940 \u0917\u0923\u0928\u093e \u0915\u0947 \u0905\u0928\u0941\u0938\u093e\u0930 \u092f\u0939 \u0935\u093f\u0936\u094d\u0935 \u0915\u0940 \u0926\u0938 \u0936\u0915\u094d\u0924\u093f\u0936\u093e\u0932\u0940 \u092d\u093e\u0937\u093e\u0913\u0902 \u092e\u0947\u0902 \u0938\u0947 \u090f\u0915 \u0939\u0948\u0964[6]]\n1\n>>> nlp = spacy.blank('kn');nlp.add_pipe(nlp.create_pipe('sentencizer'));doc = nlp(TEXT_KAN);list(doc.sents);len(list(doc.sents))\n[\u0ca6\u0ccd\u0cb0\u0cbe\u0cb5\u0cbf\u0ca1 \u0cad\u0cbe\u0cb7\u0cc6\u0c97\u0cb3\u0cb2\u0ccd\u0cb2\u0cbf \u0caa\u0ccd\u0cb0\u0cbe\u0cae\u0cc1\u0c96\u0ccd\u0caf\u0cb5\u0cc1\u0cb3\u0ccd\u0cb3 \u0cad\u0cbe\u0cb7\u0cc6\u0caf\u0cc2 \u0cad\u0cbe\u0cb0\u0ca4\u0ca6 \u0caa\u0cc1\u0cb0\u0cbe\u0ca4\u0ca8\u0cb5\u0cbe\u0ca6 \u0cad\u0cbe\u0cb7\u0cc6\u0c97\u0cb3\u0cb2\u0ccd\u0cb2\u0cbf \u0c92\u0c82\u0ca6\u0cc2 \u0c86\u0c97\u0cbf\u0cb0\u0cc1\u0cb5 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cad\u0cbe\u0cb7\u0cc6\u0caf\u0ca8\u0ccd\u0ca8\u0cc1 \u0c85\u0ca6\u0cb0 \u0cb5\u0cbf\u0cb5\u0cbf\u0ca7 \u0cb0\u0cc2\u0caa\u0c97\u0cb3\u0cb2\u0ccd\u0cb2\u0cbf \u0cb8\u0cc1\u0cae\u0cbe\u0cb0\u0cc1 \u0cea\u0ceb \u0ca6\u0cb6\u0cb2\u0c95\u0ccd\u0cb7 \u0c9c\u0ca8\u0cb0\u0cc1 \u0c86\u0ca1\u0cc1 \u0ca8\u0cc1\u0ca1\u0cbf\u0caf\u0cbe\u0c97\u0cbf \u0cac\u0cb3\u0cb8\u0cc1\u0ca4\u0ccd\u0ca4\u0cb2\u0cbf\u0ca6\u0ccd\u0ca6\u0cbe\u0cb0\u0cc6. \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0c95\u0cb0\u0ccd\u0ca8\u0cbe\u0c9f\u0c95 \u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0c86\u0ca1\u0cb3\u0cbf\u0ca4 \u0cad\u0cbe\u0cb7\u0cc6.[\u0ce7\u0ce7] \u0c9c\u0c97\u0ca4\u0ccd\u0ca4\u0cbf\u0ca8\u0cb2\u0ccd\u0cb2\u0cbf \u0c85\u0ca4\u0ccd\u0caf\u0c82\u0ca4 \u0cb9\u0cc6\u0c9a\u0ccd\u0c9a\u0cc1 \u0cae\u0c82\u0ca6\u0cbf \u0cae\u0cbe\u0ca4\u0ca8\u0cbe\u0ca1\u0cc1\u0cb5 \u0cad\u0cbe\u0cb7\u0cc6\u0caf\u0cc6\u0c82\u0cac \u0ca8\u0cc6\u0cb2\u0cc6\u0caf\u0cb2\u0ccd\u0cb2\u0cbf \u0c87\u0caa\u0ccd\u0caa\u0ca4\u0cca\u0c82\u0cac\u0ca4\u0ccd\u0ca4\u0ca8\u0cc6\u0caf \u0cb8\u0ccd\u0ca5\u0cbe\u0ca8 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\u0c95\u0ccd\u0c95\u0cbf\u0ca6\u0cc6. \u0ce8\u0ce6\u0ce7\u0ce7\u0cb0 \u0c9c\u0ca8\u0c97\u0ca3\u0ca4\u0cbf\u0caf \u0caa\u0ccd\u0cb0\u0c95\u0cbe\u0cb0 \u0c9c\u0c97\u0ca4\u0ccd\u0ca4\u0cbf\u0ca8\u0cb2\u0ccd\u0cb2\u0cbf \u0cec.\u0cea \u0c95\u0ccb\u0c9f\u0cbf \u0c9c\u0ca8\u0c97\u0cb3\u0cc1 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cae\u0cbe\u0ca4\u0ca8\u0cbe\u0ca1\u0cc1\u0ca4\u0ccd\u0ca4\u0cbe\u0cb0\u0cc6 \u0c8e\u0c82\u0ca6\u0cc1 \u0ca4\u0cbf\u0cb3\u0cbf\u0ca6\u0cc1\u0cac\u0c82\u0ca6\u0cbf\u0ca6\u0cc6. \u0c87\u0cb5\u0cb0\u0cb2\u0ccd\u0cb2\u0cbf \u0ceb.\u0ceb \u0c95\u0ccb\u0c9f\u0cbf \u0c9c\u0ca8\u0c97\u0cb3 \u0cae\u0cbe\u0ca4\u0cc3\u0cad\u0cbe\u0cb7\u0cc6 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\u0cb5\u0cbe\u0c97\u0cbf\u0ca6\u0cc6. \u0cac\u0ccd\u0cb0\u0cbe\u0cb9\u0ccd\u0cae\u0cbf \u0cb2\u0cbf\u0caa\u0cbf\u0caf\u0cbf\u0c82\u0ca6 \u0cb0\u0cc2\u0caa\u0cc1\u0c97\u0cca\u0c82\u0ca1 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cb2\u0cbf\u0caa\u0cbf\u0caf\u0ca8\u0ccd\u0ca8\u0cc1 \u0c89\u0caa\u0caf\u0ccb\u0c97\u0cbf\u0cb8\u0cbf \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cad\u0cbe\u0cb7\u0cc6\u0caf\u0ca8\u0ccd\u0ca8\u0cc1 \u0cac\u0cb0\u0cc6\u0caf\u0cb2\u0cbe\u0c97\u0cc1\u0ca4\u0ccd\u0ca4\u0ca6\u0cc6. \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cac\u0cb0\u0cb9\u0ca6 \u0cae\u0cbe\u0ca6\u0cb0\u0cbf\u0c97\u0cb3\u0cbf\u0c97\u0cc6 \u0cb8\u0cbe\u0cb5\u0cbf\u0cb0\u0ca6 \u0c90\u0ca8\u0cc2\u0cb0\u0cc1 \u0cb5\u0cb0\u0cc1\u0cb7\u0c97\u0cb3 \u0c9a\u0cb0\u0cbf\u0ca4\u0ccd\u0cb0\u0cc6\u0caf\u0cbf\u0ca6\u0cc6. \u0c95\u0ccd\u0cb0\u0cbf.\u0cb6. \u0c86\u0cb0\u0ca8\u0cc6\u0caf \u0cb6\u0ca4\u0cae\u0cbe\u0ca8\u0ca6 \u0caa\u0cb6\u0ccd\u0c9a\u0cbf\u0cae \u0c97\u0c82\u0c97 \u0cb8\u0cbe\u0cae\u0ccd\u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0c95\u0cbe\u0cb2\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf [\u0ce7\u0ce8] \u0cae\u0ca4\u0ccd\u0ca4\u0cc1 \u0c92\u0c82\u0cac\u0ca4\u0ccd\u0ca4\u0ca8\u0cc6\u0caf \u0cb6\u0ca4\u0cae\u0cbe\u0ca8\u0ca6 \u0cb0\u0cbe\u0cb7\u0ccd\u0c9f\u0ccd\u0cb0\u0c95\u0cc2\u0c9f \u0cb8\u0cbe\u0cae\u0ccd\u0cb0\u0cbe\u0c9c\u0ccd\u0caf\u0ca6 \u0c95\u0cbe\u0cb2\u0ca6\u0cb2\u0ccd\u0cb2\u0cbf \u0cb9\u0cb3\u0c97\u0ca8\u0ccd\u0ca8\u0ca1 \u0cb8\u0cbe\u0cb9\u0cbf\u0ca4\u0ccd\u0caf \u0c85\u0ca4\u0ccd\u0caf\u0c82\u0ca4 \u0cb9\u0cc6\u0c9a\u0ccd\u0c9a\u0cbf\u0ca8 \u0cb0\u0cbe\u0c9c\u0cbe\u0cb6\u0ccd\u0cb0\u0caf \u0caa\u0ca1\u0cc6\u0caf\u0cbf\u0ca4\u0cc1.[\u0ce7\u0ce9][\u0ce7\u0cea] \u0c85\u0ca6\u0cb2\u0ccd\u0cb2\u0ca6\u0cc6 \u0cb8\u0cbe\u0cb5\u0cbf\u0cb0 \u0cb5\u0cb0\u0cc1\u0cb7\u0c97\u0cb3 \u0cb8\u0cbe\u0cb9\u0cbf\u0ca4\u0ccd\u0caf \u0caa\u0cb0\u0c82\u0caa\u0cb0\u0cc6 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\u0c95\u0ccd\u0c95\u0cbf\u0ca6\u0cc6.[\u0ce7\u0ceb]\u0cb5\u0cbf\u0ca8\u0ccb\u0cac\u0cbe \u0cad\u0cbe\u0cb5\u0cc6 \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u0cb2\u0cbf\u0caa\u0cbf\u0caf\u0ca8\u0ccd\u0ca8\u0cc1 \u0cb2\u0cbf\u0caa\u0cbf\u0c97\u0cb3 \u0cb0\u0cbe\u0ca3\u0cbf\u0caf\u0cc6\u0c82\u0ca6\u0cc1 \u0cb9\u0cca\u0c97\u0cb3\u0cbf\u0ca6\u0ccd\u0ca6\u0cbe\u0cb0\u0cc6.[\u0cb8\u0cc2\u0c95\u0ccd\u0ca4 \u0c89\u0cb2\u0ccd\u0cb2\u0cc7\u0c96\u0ca8 \u0cac\u0cc7\u0c95\u0cc1]]\n1\n>>> nlp = spacy.blank('si');nlp.add_pipe(nlp.create_pipe('sentencizer'));doc = nlp(TEXT_SIN);list(doc.sents);len(list(doc.sents))\n[\u0dc1\u0dca\u200d\u0dbb\u0dd3 \u0dbd\u0d82\u0d9a\u0dcf\u0dc0\u0dda \u0db4\u0dca\u200d\u0dbb\u0db0\u0dcf\u0db1 \u0da2\u0dcf\u0dad\u0dd2\u0dba \u0dc0\u0db1 \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0da2\u0db1\u0dba\u0dcf\u0d9c\u0dda \u0db8\u0dc0\u0dca \u0db6\u0dc3 \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0dc0\u0dd9\u0dba\u0dd2. \u0d85\u0daf \u0dc0\u0db1 \u0dc0\u0dd2\u0da7 \u0db8\u0dd2\u0dbd\u0dd2\u0dba\u0db1 20 \u0d9a\u0da7 \u0d85\u0db0\u0dd2\u0d9a \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0dc3\u0dc4 \u0db8\u0dd2\u0dbd\u0dd2\u0dba\u0db1 3\u0d9a\u0da7 \u0d85\u0db0\u0dd2\u0d9a \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0db1\u0ddc\u0dc0\u0db1 \u0da2\u0db1\u0d9c\u0dc4\u0db1\u0dba\u0d9a\u0dca \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0db7\u0dcf\u0dc2\u0dcf\u0dc0 \u0db7\u0dcf\u0dc0\u0dd2\u0dad \u0d9a\u0dbb\u0dad\u0dd2. \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\u200d \u0d89\u0db1\u0dca\u0daf\u0dd4-\u0dba\u0dd4\u0dbb\u0ddd\u0db4\u0dd3\u0dba \u0db7\u0dcf\u0dc2\u0dcf\u0dc0\u0dbd \u0d8b\u0db4 \u0d9c\u0dab\u0dba\u0d9a\u0dca \u0dc0\u0db1 \u0d89\u0db1\u0dca\u0daf\u0dd4-\u0d86\u0dbb\u0dca\u0dba \u0db7\u0dcf\u0dc2\u0dcf \u0d9c\u0dab\u0dba\u0da7 \u0d85\u0dba\u0dd2\u0dad\u0dd2 \u0dc0\u0db1 \u0d85\u0dad\u0dbb \u0db8\u0dcf\u0dbd \u0daf\u0dd2\u0dc0\u0dba\u0dd2\u0db1 \u0db7\u0dcf\u0dc0\u0dd2\u0dad \u0d9a\u0dbb\u0db1 \u0daf\u0dd2\u0dc0\u0dd9\u0dc4\u0dd2 \u0db7\u0dcf\u0dc2\u0dcf\u0dc0 \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\u0dba\u0dd9\u0db1\u0dca \u0db4\u0dd0\u0dc0\u0dad \u0d91\u0db1\u0dca\u0db1\u0d9a\u0dd2. \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0dc1\u0dca\u200d\u0dbb\u0dd3 \u0dbd\u0d82\u0d9a\u0dcf\u0dc0\u0dda \u0db1\u0dd2\u0dbd \u0db7\u0dcf\u0dc2\u0dcf\u0dc0\u0dba\u0dd2 .]\n1\n>>> nlp = spacy.blank('ta');nlp.add_pipe(nlp.create_pipe('sentencizer'));doc = nlp(TEXT_TAM);list(doc.sents);len(list(doc.sents))\n[\u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0bae\u0bca\u0bb4\u0bbf (Tamil language) \u0ba4\u0bae\u0bbf\u0bb4\u0bb0\u0bcd\u0b95\u0bb3\u0bbf\u0ba9\u0ba4\u0bc1\u0bae\u0bcd, \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0baa\u0bc7\u0b9a\u0bc1\u0bae\u0bcd \u0baa\u0bb2\u0bb0\u0ba4\u0bc1\u0bae\u0bcd \u0ba4\u0bbe\u0baf\u0bcd\u0bae\u0bca\u0bb4\u0bbf \u0b86\u0b95\u0bc1\u0bae\u0bcd. \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0ba4\u0bbf\u0bb0\u0bbe\u0bb5\u0bbf\u0b9f \u0bae\u0bca\u0bb4\u0bbf\u0b95\u0bcd \u0b95\u0bc1\u0b9f\u0bc1\u0bae\u0bcd\u0baa\u0ba4\u0bcd\u0ba4\u0bbf\u0ba9\u0bcd \u0bae\u0bc1\u0ba4\u0ba9\u0bcd\u0bae\u0bc8\u0baf\u0bbe\u0ba9 \u0bae\u0bca\u0bb4\u0bbf\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0b92\u0ba9\u0bcd\u0bb1\u0bc1\u0bae\u0bcd \u0b9a\u0bc6\u0bae\u0bcd\u0bae\u0bca\u0bb4\u0bbf\u0baf\u0bc1\u0bae\u0bcd \u0b86\u0b95\u0bc1\u0bae\u0bcd. \u0b87\u0ba8\u0bcd\u0ba4\u0bbf\u0baf\u0bbe, \u0b87\u0bb2\u0b99\u0bcd\u0b95\u0bc8, \u0bae\u0bb2\u0bc7\u0b9a\u0bbf\u0baf\u0bbe, \u0b9a\u0bbf\u0b99\u0bcd\u0b95\u0baa\u0bcd\u0baa\u0bc2\u0bb0\u0bcd \u0b86\u0b95\u0bbf\u0baf \u0ba8\u0bbe\u0b9f\u0bc1\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0b85\u0ba4\u0bbf\u0b95 \u0b85\u0bb3\u0bb5\u0bbf\u0bb2\u0bc1\u0bae\u0bcd, \u0b90\u0b95\u0bcd\u0b95\u0bbf\u0baf \u0b85\u0bb0\u0baa\u0bc1 \u0b85\u0bae\u0bc0\u0bb0\u0b95\u0bae\u0bcd, \u0ba4\u0bc6\u0ba9\u0bcd\u0ba9\u0bbe\u0baa\u0bcd\u0baa\u0bbf\u0bb0\u0bbf\u0b95\u0bcd\u0b95\u0bbe, \u0bae\u0bca\u0bb0\u0bbf\u0b9a\u0bbf\u0baf\u0b9a\u0bc1, \u0baa\u0bbf\u0b9c\u0bbf, \u0bb0\u0bc0\u0baf\u0bc2\u0ba9\u0bbf\u0baf\u0ba9\u0bcd, \u0b9f\u0bbf\u0bb0\u0bbf\u0ba9\u0bbf\u0b9f\u0bbe\u0b9f\u0bcd \u0baa\u0bcb\u0ba9\u0bcd\u0bb1 \u0ba8\u0bbe\u0b9f\u0bc1\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0b9a\u0bbf\u0bb1\u0bbf\u0baf \u0b85\u0bb3\u0bb5\u0bbf\u0bb2\u0bc1\u0bae\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0baa\u0bc7\u0b9a\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0b95\u0bbf\u0bb1\u0ba4\u0bc1. 1997\u0b86\u0bae\u0bcd \u0b86\u0ba3\u0bcd\u0b9f\u0bc1\u0baa\u0bcd \u0baa\u0bc1\u0bb3\u0bcd\u0bb3\u0bbf \u0bb5\u0bbf\u0bb5\u0bb0\u0baa\u0bcd\u0baa\u0b9f\u0bbf \u0b89\u0bb2\u0b95\u0bae\u0bcd \u0bae\u0bc1\u0bb4\u0bc1\u0bb5\u0ba4\u0bbf\u0bb2\u0bc1\u0bae\u0bcd 8 \u0b95\u0bcb\u0b9f\u0bbf (80 \u0bae\u0bbf\u0bb2\u0bcd\u0bb2\u0bbf\u0baf\u0ba9\u0bcd) \u0bae\u0b95\u0bcd\u0b95\u0bb3\u0bbe\u0bb2\u0bcd \u0baa\u0bc7\u0b9a\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd[13], \u0b92\u0bb0\u0bc1 \u0bae\u0bca\u0bb4\u0bbf\u0baf\u0bc8\u0ba4\u0bcd \u0ba4\u0bbe\u0baf\u0bcd\u0bae\u0bca\u0bb4\u0bbf\u0baf\u0bbe\u0b95\u0b95\u0bcd \u0b95\u0bca\u0ba3\u0bcd\u0b9f\u0bc1 \u0baa\u0bc7\u0b9a\u0bc1\u0bae\u0bcd \u0bae\u0b95\u0bcd\u0b95\u0bb3\u0bbf\u0ba9\u0bcd \u0b8e\u0ba3\u0bcd\u0ba3\u0bbf\u0b95\u0bcd\u0b95\u0bc8 \u0b85\u0b9f\u0bbf\u0baa\u0bcd\u0baa\u0b9f\u0bc8\u0baf\u0bbf\u0bb2\u0bcd \u0baa\u0ba4\u0bbf\u0ba9\u0bc6\u0b9f\u0bcd\u0b9f\u0bbe\u0bb5\u0ba4\u0bc1 \u0b87\u0b9f\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0b89\u0bb3\u0bcd\u0bb3\u0ba4\u0bc1.[14] \u0b87\u0ba3\u0bc8\u0baf\u0ba4\u0bcd\u0ba4\u0bbf\u0bb2\u0bcd \u0b85\u0ba4\u0bbf\u0b95\u0bae\u0bcd \u0baa\u0baf\u0ba9\u0bcd\u0baa\u0b9f\u0bc1\u0ba4\u0bcd\u0ba4\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd \u0b87\u0ba8\u0bcd\u0ba4\u0bbf\u0baf \u0bae\u0bca\u0bb4\u0bbf\u0b95\u0bb3\u0bbf\u0bb2\u0bcd \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0bae\u0bc1\u0ba4\u0ba9\u0bcd\u0bae\u0bc8\u0baf\u0bbe\u0b95 \u0b89\u0bb3\u0bcd\u0bb3\u0ba4\u0bbe\u0b95 2017 \u0b86\u0bb5\u0ba4\u0bc1 \u0b86\u0ba3\u0bcd\u0b9f\u0bbf\u0bb2\u0bcd \u0ba8\u0b9f\u0bc8\u0baa\u0bc6\u0bb1\u0bcd\u0bb1 \u0b95\u0bc2\u0b95\u0bc1\u0bb3\u0bcd \u0b95\u0ba3\u0b95\u0bcd\u0b95\u0bc6\u0b9f\u0bc1\u0baa\u0bcd\u0baa\u0bbf\u0bb2\u0bcd \u0ba4\u0bc6\u0bb0\u0bbf\u0baf \u0bb5\u0ba8\u0bcd\u0ba4\u0ba4\u0bc1.[15]]\n1\n>>> nlp = spacy.blank('te');nlp.add_pipe(nlp.create_pipe('sentencizer'));doc = nlp(TEXT_TEL);list(doc.sents);len(list(doc.sents))\n[\u0c06\u0c02\u0c27\u0c4d\u0c30 \u0c2a\u0c4d\u0c30\u0c26\u0c47\u0c36\u0c4d, \u0c24\u0c46\u0c32\u0c02\u0c17\u0c3e\u0c23 \u0c30\u0c3e\u0c37\u0c4d\u0c1f\u0c4d\u0c30\u0c3e\u0c32 \u0c05\u0c27\u0c3f\u0c15\u0c3e\u0c30 \u0c2d\u0c3e\u0c37 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41. \u0c2d\u0c3e\u0c30\u0c24 \u0c26\u0c47\u0c36\u0c02\u0c32\u0c4b \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0c2e\u0c3e\u0c24\u0c43\u0c2d\u0c3e\u0c37\u0c17\u0c3e \u0c2e\u0c3e\u0c1f\u0c4d\u0c32\u0c3e\u0c21\u0c47 8.7 \u0c15\u0c4b\u0c1f\u0c4d\u0c32 (2001) \u0c1c\u0c28\u0c3e\u0c2d\u0c3e\u0c24\u0c4b [1] \u0c2a\u0c4d\u0c30\u0c3e\u0c02\u0c24\u0c40\u0c2f \u0c2d\u0c3e\u0c37\u0c32\u0c32\u0c4b \u0c2e\u0c4a\u0c26\u0c1f\u0c3f \u0c38\u0c4d\u0c25\u0c3e\u0c28\u0c02\u0c32\u0c4b \u0c09\u0c02\u0c26\u0c3f. \u0c2a\u0c4d\u0c30\u0c2a\u0c02\u0c1a\u0c02\u0c32\u0c4b\u0c28\u0c3f \u0c2a\u0c4d\u0c30\u0c1c\u0c32\u0c41 \u0c05\u0c24\u0c4d\u0c2f\u0c27\u0c3f\u0c15\u0c2e\u0c41\u0c17\u0c3e \u0c2e\u0c3e\u0c1f\u0c4d\u0c32\u0c3e\u0c21\u0c47 \u0c2d\u0c3e\u0c37\u0c32\u0c32\u0c4b 15 \u0c38\u0c4d\u0c25\u0c3e\u0c28\u0c2e\u0c41\u0c32\u0c4b\u0c28\u0c42, \u0c2d\u0c3e\u0c30\u0c24 \u0c26\u0c47\u0c36\u0c2e\u0c41\u0c32\u0c4b \u0c39\u0c3f\u0c02\u0c26\u0c40, \u0c24\u0c30\u0c4d\u0c35\u0c3e\u0c24 \u0c38\u0c4d\u0c25\u0c3e\u0c28\u0c2e\u0c41\u0c32\u0c4b\u0c28\u0c42 \u0c28\u0c3f\u0c32\u0c41\u0c38\u0c4d\u0c24\u0c41\u0c02\u0c26\u0c3f. \u0c2a\u0c3e\u0c24\u0c35\u0c48\u0c28 \u0c2a\u0c4d\u0c30\u0c2a\u0c02\u0c1a \u0c2d\u0c3e\u0c37 \u0c17\u0c23\u0c3e\u0c02\u0c15\u0c3e\u0c32 (\u0c0e\u0c25\u0c4d\u0c28\u0c4b\u0c32\u0c3e\u0c17\u0c4d) \u0c2a\u0c4d\u0c30\u0c15\u0c3e\u0c30\u0c02 \u0c2a\u0c4d\u0c30\u0c2a\u0c02\u0c1a\u0c35\u0c4d\u0c2f\u0c3e\u0c2a\u0c4d\u0c24\u0c02\u0c17\u0c3e 7.4 \u0c15\u0c4b\u0c1f\u0c4d\u0c32\u0c41 \u0c2e\u0c02\u0c26\u0c3f\u0c15\u0c3f \u0c2e\u0c3e\u0c24\u0c43\u0c2d\u0c3e\u0c37\u0c17\u0c3e \u0c09\u0c02\u0c26\u0c3f.[2] \u0c2e\u0c4a\u0c26\u0c1f\u0c3f \u0c2d\u0c3e\u0c37\u0c17\u0c3e \u0c2e\u0c3e\u0c1f\u0c4d\u0c32\u0c3e\u0c21\u0c24\u0c3e\u0c30\u0c41. \u0c05\u0c24\u0c3f \u0c2a\u0c4d\u0c30\u0c3e\u0c1a\u0c40\u0c28 \u0c26\u0c47\u0c36 \u0c2d\u0c3e\u0c37\u0c32\u0c32\u0c4b \u0c38\u0c02\u0c38\u0c4d\u0c15\u0c43\u0c24\u0c2e\u0c41 \u0c24\u0c2e\u0c3f\u0c33\u0c2e\u0c41\u0c24\u0c4b \u0c2c\u0c3e\u0c1f\u0c41 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0c2d\u0c3e\u0c37\u0c28\u0c41 2008 \u0c05\u0c15\u0c4d\u0c1f\u0c4b\u0c2c\u0c30\u0c41 31\u0c28 \u0c2d\u0c3e\u0c30\u0c24 \u0c2a\u0c4d\u0c30\u0c2d\u0c41\u0c24\u0c4d\u0c35\u0c2e\u0c41 \u0c17\u0c41\u0c30\u0c4d\u0c24\u0c3f\u0c02\u0c1a\u0c3f\u0c02\u0c26\u0c3f.]\n1\n>>> nlp = spacy.blank('ur');nlp.add_pipe(nlp.create_pipe('sentencizer'));doc = nlp(TEXT_URD);list(doc.sents);len(list(doc.sents))\n[\u0627\u064f\u0631\u062f\u064f\u0648 \u0644\u0634\u06a9\u0631\u06cc \u0632\u0628\u0627\u0646[8] (\u06cc\u0627 \u062c\u062f\u06cc\u062f \u0645\u0639\u06cc\u0627\u0631\u06cc \u0627\u0631\u062f\u0648) \u0628\u0631\u0635\u063a\u06cc\u0631 \u06a9\u06cc \u0645\u0639\u06cc\u0627\u0631\u06cc \u0632\u0628\u0627\u0646\u0648\u06ba \u0645\u06cc\u06ba \u0633\u06d2 \u0627\u06cc\u06a9 \u06c1\u06d2\u06d4 \u06cc\u06c1 \u067e\u0627\u06a9\u0633\u062a\u0627\u0646 \u06a9\u06cc \u0642\u0648\u0645\u06cc \u0627\u0648\u0631 \u0631\u0627\u0628\u0637\u06c1 \u0639\u0627\u0645\u06c1 \u06a9\u06cc \u0632\u0628\u0627\u0646 \u06c1\u06d2\u060c \u062c\u0628\u06a9\u06c1 \u0628\u06be\u0627\u0631\u062a \u06a9\u06cc \u0686\u06be\u06d2 \u0631\u06cc\u0627\u0633\u062a\u0648\u06ba \u06a9\u06cc \u062f\u0641\u062a\u0631\u06cc \u0632\u0628\u0627\u0646 \u06a9\u0627 \u062f\u0631\u062c\u06c1 \u0631\u06a9\u06be\u062a\u06cc \u06c1\u06d2\u06d4 \u0622\u0626\u06cc\u0646 \u06c1\u0646\u062f \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0627\u0633\u06d2 22 \u062f\u0641\u062a\u0631\u06cc \u0634\u0646\u0627\u062e\u062a \u0632\u0628\u0627\u0646\u0648\u06ba \u0645\u06cc\u06ba \u0634\u0627\u0645\u0644 \u06a9\u06cc\u0627 \u062c\u0627\u0686\u06a9\u0627 \u06c1\u06d2\u06d4 2001\u0621 \u06a9\u06cc \u0645\u0631\u062f\u0645 \u0634\u0645\u0627\u0631\u06cc \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0627\u0631\u062f\u0648 \u06a9\u0648 \u0628\u0637\u0648\u0631 \u0645\u0627\u062f\u0631\u06cc \u0632\u0628\u0627\u0646 \u0628\u06be\u0627\u0631\u062a \u0645\u06cc\u06ba 5.01% \u0641\u06cc\u0635\u062f \u0644\u0648\u06af \u0628\u0648\u0644\u062a\u06d2 \u06c1\u06cc\u06ba \u0627\u0648\u0631 \u0627\u0633 \u0644\u062d\u0627\u0638 \u0633\u06d2 \u06cc\u06c1 \u0628\u06be\u0627\u0631\u062a \u06a9\u06cc \u0686\u06be\u0679\u06cc \u0628\u0691\u06cc \u0632\u0628\u0627\u0646 \u06c1\u06d2 \u062c\u0628\u06a9\u06c1 \u067e\u0627\u06a9\u0633\u062a\u0627\u0646 \u0645\u06cc\u06ba \u0627\u0633\u06d2 \u0628\u0637\u0648\u0631 \u0645\u0627\u062f\u0631\u06cc \u0632\u0628\u0627\u0646 7.59% \u0641\u06cc\u0635\u062f \u0644\u0648\u06af \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u060c \u06cc\u06c1 \u067e\u0627\u06a9\u0633\u062a\u0627\u0646 \u06a9\u06cc \u067e\u0627\u0646\u0686\u0648\u06cc\u06ba \u0628\u0691\u06cc \u0632\u0628\u0627\u0646 \u06c1\u06d2\u06d4 \u0627\u0631\u062f\u0648 \u062a\u0627\u0631\u06cc\u062e\u06cc \u0637\u0648\u0631 \u067e\u0631 \u06c1\u0646\u062f\u0648\u0633\u062a\u0627\u0646 \u06a9\u06cc \u0645\u0633\u0644\u0645 \u0622\u0628\u0627\u062f\u06cc \u0633\u06d2 \u062c\u0691\u06cc \u06c1\u06d2\u06d4[\u062d\u0648\u0627\u0644\u06c1 \u062f\u0631\u06a9\u0627\u0631] \u0628\u0639\u0636 \u0630\u062e\u06cc\u0631\u06c1 \u0627\u0644\u0641\u0627\u0638 \u06a9\u06d2 \u0639\u0644\u0627\u0648\u06c1 \u06cc\u06c1 \u0632\u0628\u0627\u0646 \u0645\u0639\u06cc\u0627\u0631\u06cc \u06c1\u0646\u062f\u06cc \u0633\u06d2 \u0642\u0627\u0628\u0644 \u0641\u06c1\u0645 \u06c1\u06d2 \u062c\u0648 \u0627\u0633 \u062e\u0637\u06d2 \u06a9\u06cc \u06c1\u0646\u062f\u0648\u0624\u06ba \u0633\u06d2 \u0645\u0646\u0633\u0648\u0628 \u06c1\u06d2\u06d4[\u062d\u0648\u0627\u0644\u06c1 \u062f\u0631\u06a9\u0627\u0631] \u0632\u0628\u0627\u0646\u0650 \u0627\u0631\u062f\u0648 \u06a9\u0648 \u067e\u06c1\u0686\u0627\u0646 \u0648 \u062a\u0631\u0642\u06cc \u0627\u0633 \u0648\u0642\u062a \u0645\u0644\u06cc \u062c\u0628 \u0628\u0631\u0637\u0627\u0646\u0648\u06cc \u062f\u0648\u0631 \u0645\u06cc\u06ba \u0627\u0646\u06af\u0631\u06cc\u0632 \u062d\u06a9\u0645\u0631\u0627\u0646\u0648\u06ba \u0646\u06d2 \u0627\u0633\u06d2 \u0641\u0627\u0631\u0633\u06cc \u06a9\u06cc \u0628\u062c\u0627\u0626\u06d2 \u0627\u0646\u06af\u0631\u06cc\u0632\u06cc \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0634\u0645\u0627\u0644\u06cc \u06c1\u0646\u062f\u0648\u0633\u062a\u0627\u0646 \u06a9\u06d2 \u0639\u0644\u0627\u0642\u0648\u06ba \u0627\u0648\u0631 \u062c\u0645\u0648\u06ba \u0648 \u06a9\u0634\u0645\u06cc\u0631 \u0645\u06cc\u06ba \u0627\u0633\u06d2 \u0633\u0646\u06c1 1846\u0621 \u0627\u0648\u0631 \u067e\u0646\u062c\u0627\u0628 \u0645\u06cc\u06ba \u0633\u0646\u06c1 1849\u0621 \u0645\u06cc\u06ba \u0628\u0637\u0648\u0631 \u062f\u0641\u062a\u0631\u06cc \u0632\u0628\u0627\u0646 \u0646\u0627\u0641\u0630 \u06a9\u06cc\u0627\u06d4 \u0627\u0633 \u06a9\u06d2 \u0639\u0644\u0627\u0648\u06c1 \u062e\u0644\u06cc\u062c\u06cc\u060c \u06cc\u0648\u0631\u067e\u06cc\u060c \u0627\u06cc\u0634\u06cc\u0627\u0626\u06cc \u0627\u0648\u0631 \u0627\u0645\u0631\u06cc\u06a9\u06cc \u0639\u0644\u0627\u0642\u0648\u06ba \u0645\u06cc\u06ba \u0627\u0631\u062f\u0648 \u0628\u0648\u0644\u0646\u06d2 \u0648\u0627\u0644\u0648\u06ba \u06a9\u06cc \u0627\u06cc\u06a9 \u0628\u0691\u06cc \u062a\u0639\u062f\u0627\u062f \u0622\u0628\u0627\u062f \u06c1\u06d2 \u062c\u0648 \u0628\u0646\u06cc\u0627\u062f\u06cc \u0637\u0648\u0631 \u067e\u0631 \u062c\u0646\u0648\u0628\u06cc \u0627\u06cc\u0634\u06cc\u0627\u0621 \u0633\u06d2 \u06a9\u0648\u0686 \u06a9\u0631\u0646\u06d2 \u0648\u0627\u0644\u06d2 \u0627\u06c1\u0644\u0650 \u0627\u0631\u062f\u0648 \u06c1\u06cc\u06ba\u06d4 1999\u0621 \u06a9\u06d2 \u0627\u0639\u062f\u0627\u062f \u0648\u0634\u0645\u0627\u0631 \u06a9\u06d2 \u0645\u0637\u0627\u0628\u0642 \u0627\u0631\u062f\u0648 \u0632\u0628\u0627\u0646 \u06a9\u06d2 \u0645\u062c\u0645\u0648\u0639\u06cc \u0645\u062a\u06a9\u0644\u0645\u06cc\u0646 \u06a9\u06cc \u062a\u0639\u062f\u0627\u062f \u062f\u0633 \u06a9\u0631\u0648\u0691 \u0633\u0627\u0679\u06be \u0644\u0627\u06a9\u06be \u06a9\u06d2 \u0644\u06af \u0628\u06be\u06af \u062a\u06be\u06cc\u06d4 \u0627\u0633 \u0644\u062d\u0627\u0638 \u0633\u06d2 \u06cc\u06c1 \u062f\u0646\u06cc\u0627 \u06a9\u06cc \u0646\u0648\u06cc\u06ba \u0628\u0691\u06cc \u0632\u0628\u0627\u0646 \u06c1\u06d2\u06d4]\n1\nOperating System: Windows 10 64-bit\nPython Version Used: 3.7.4 64-bit\nspaCy Version Used: 2.1.8", "issue_status": "Closed", "issue_reporting_time": "2019-09-10T19:15:02Z", "fixed_by": "#4290", "pull_request_summary": "Extend default punct for sentencizer", "pull_request_description": "Collaborator\nadrianeboyd commented on 14 Sep 2019\nDescription\nMost of these characters are for languages / writing systems that aren't supported by spacy, but I don't think it causes problems to include them. In the UD evals, Hindi and Urdu improve a lot as expected (from 0-10% to 70-80%) and Persian improves a little (90% to 96%). Tamil improves in combination with #4288.\nThe punctuation list is converted to a set internally because of its increased length.\nSentence final punctuation generated with:\nunichars -gas '[\\p{Sentence_Break=STerm}\\p{Sentence_Break=ATerm}]' '\\p{Terminal_Punctuation}'\nSee: https://stackoverflow.com/a/9508766/461847\nFixes #4269.\nTypes of change\nEnhancement.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-14T13:25:49Z", "files_changed": [["24", "spacy/pipeline/pipes.pyx"], ["4", "spacy/tests/pipeline/test_sentencizer.py"]]}, "344": {"issue_url": "https://github.com/explosion/spaCy/issues/4267", "issue_id": "#4267", "issue_summary": "Different ent_iob behavior after adding EntityRuler to pipeline", "issue_description": "Contributor\njenojp commented on 10 Sep 2019 \u2022\nedited\nI'm not totally sure of the expected behavior but after adding an EntityRuler to a pipeline, non entities seem to get .ent_iob tags of 0 rather than 2 when just using the EntityRecognizer. This affects what doc.is_nered returns.\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.pipeline import EntityRuler\n\nnlp = spacy.load(\"en_core_web_sm\")\nprint(nlp.pipe_names)\n## ['tagger', 'parser', 'ner']\n\ndoc = nlp(\"fgfgdghgdh\")\nprint(doc.is_nered)\n## True\n\nfor token in doc:\n    print(token.ent_iob)\n## 2\n\n#addd entity ruler and run again\nruler = EntityRuler(nlp)\npatterns = [{\"label\":\"SOFTWARE\", \"pattern\":\"spacy\"}]\n\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler)\nprint(nlp.pipe_names)\n## ['tagger', 'parser', 'ner', 'entity_ruler']\n\ndoc = nlp(\"fgfgdghgdh\")\nprint(doc.is_nered)\n## False\n\nfor token in doc:\n    print(token.ent_iob)\n## 0\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.8\n1", "issue_status": "Closed", "issue_reporting_time": "2019-09-10T14:10:54Z", "fixed_by": "#4307", "pull_request_summary": "Distinction between outside, missing and blocked NER annotations", "pull_request_description": "Member\nsvlandeg commented on 18 Sep 2019 \u2022\nedited\nDescription\nThis PR attempts to process \"empty\" NER annotations more consistently.\nAllow the NER algo to overwrite O (ent_iob == 2) annotations\nEnsure that the NER algo preserves preset entities\nAllow users to specify tokens that should never be in an entity. This \"blocking\" is done by setting doc.ents with a Span of tokens with empty ent_type. ent_iob is then set to 3. In the transition system, these are recognized as U- actions, i.e. UNIT actions without a label.\nAs a result of the rewrite, doc.ents = list(doc.ents) now actually keeps the annotations on the token level consistent, instead of resetting O to empty string. It does this by checking previous annotations for each token: if it was nered before, we put it at O, otherwise empty string. This seems to be the most intuitive behaviour for a user inspecting the token-level data.\nFixes #4267\nTests\nI added some new unit tests in test_ner.py and for Issue 4267.\nI tested the \"preserving previous entities\" functionality with statistical models, cf here, showing how that works properly. Removed those tests because they rely on the models to be installed.\ntest_doc_add_entities_set_ents_iob was in the repo twice so I removed one, and changed the other to have O annotations.\nOpen questions\nSome old tests failed because nn_parser.move_names now contains U-. For now I removed it explicitely from move_names, but we could also adjust the unit tests. Depends on whether or not we want to keep that action internal.\nCaveat\nFor the \"blocking\" functionality to work with the statistical models, they'll have to be retrained.\nTypes of change\nEnhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-18T19:37:18Z", "files_changed": [["2", "spacy/errors.py"], ["52", "spacy/syntax/ner.pyx"], ["4", "spacy/syntax/nn_parser.pyx"], ["17", "spacy/tests/doc/test_add_entities.py"], ["157", "spacy/tests/parser/test_ner.py"], ["2", "spacy/tests/regression/test_issue1-1000.py"], ["42", "spacy/tests/regression/test_issue4267.py"], ["56", "spacy/tokens/doc.pyx"], ["3", "spacy/tokens/token.pyx"]]}, "345": {"issue_url": "https://github.com/explosion/spaCy/issues/4262", "issue_id": "#4262", "issue_summary": "Confussing behaviour in Phrase Matcher for Japanese model", "issue_description": "lautel commented on 9 Sep 2019 \u2022\nedited\nHow to reproduce the behaviour\n0. Load libraries\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.lang.ja import Japanese\n1. Define a Phrase Matcher to find custom entities in Japanese text:\ndef phrase_matcher_test(text):\n\n    nlp = Japanese()\n\n    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n    patterns = [\"nakagawa jun\", \"\u4e2d\u5ddd \u6f64\", \"nakagawa@xxxx.jp\", \"japan\"]\n    patterns_doc = list(nlp.pipe(patterns))\n    matcher.add(\"ENTITY\", None, *patterns_doc)\n\n    doc = nlp(text)\n\n    matches = matcher(doc)\n    print(f'\\n{len(matches)} matches found!')\n\n    for match_id, start, end in matches:\n        print(doc.vocab.strings[match_id]+': ', doc[start:end].text)\n3. Call the function\ninput_text = \"Nakagawa Jun (\u4e2d\u5ddd\u6f64) \u306e\uff92\uff70\uff99\u306fnakagawa@xxxx.jp\u3067\u3059. \u5f7c\u306fJapan\u3067\u50cd\u3044\u3066\u3044\u307e\u3059\"\nphrase_matcher_test(input_text )\n4. Result\nOutput:\n3 matches found!\nENTITY: \u4e2d\u5ddd\u6f64\nENTITY: nakagawa@xxxx.jp\nENTITY: Japan\nExpected output:\n4 matches found!\nENTITY: Nakagawa Jun\nENTITY: \u4e2d\u5ddd\u6f64\nENTITY: nakagawa@xxxx.jp\nENTITY: Japan\n5. Additional info\nIf\ninput_text = \"nakagawa jun (\u4e2d\u5ddd\u6f64) \u306e\uff92\uff70\uff99\u306fnakagawa@xxxx.jp\u3067\u3059. \u5f7c\u306fJapan\u3067\u50cd\u3044\u3066\u3044\u307e\u3059\"\nit almost works as expected (note lowercase in 'nakagawa jun', but not in 'Japan'). By almost I mean that it matches the name but deletes the blank space between name and surname. See the following output.\nOutput:\nENTITY: nakagawajun\nENTITY: \u4e2d\u5ddd\u6f64\nENTITY: nakagawa@xxxx.jp\nENTITY: Japan\nYour Environment\nspaCy version: 2.1.8\nPlatform: Windows-10-10.0.17134-SP0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-09-09T10:44:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "346": {"issue_url": "https://github.com/explosion/spaCy/issues/4261", "issue_id": "#4261", "issue_summary": "sre_constants.error: bad escape \\p at position 257", "issue_description": "saurabhchandrapatel commented on 9 Sep 2019 \u2022\nedited\nHow to reproduce the behaviour\nimport spacy\nnlp = spacy.load(\"en_core_web_md\")\nYour Environment\nspaCy version 2.1.6\nLocation /srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy\nPlatform Linux-3.10.0-327.el7.x86_64-x86_64-with-centos-7.2.1511-Core\nPython version 3.6.3\nModels en, en_core_web_md ( 2.1.0 )\nOperating System:centos\nPython Version Used: 3.6.3\nspaCy Version Used: 2.1.6\nEnvironment Information:\noffline installation with pip , python running in vertualenv\ncan not run python -m spacy validate (its offline installation )\ncan not run python -m spacy download en_core_web_md\nError:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/util.py\", line 132, in load_model\n    return load_model_from_link(name, **overrides)\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/util.py\", line 149, in load_model_from_link\n    return cls.load(**overrides)\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/data/en/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/util.py\", line 193, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/util.py\", line 176, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/language.py\", line 811, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/util.py\", line 633, in from_disk\n    reader(path / key)\n  File \"/srv/yoga-chat-bot/env36/lib/python3.6/site-packages/spacy/language.py\", line 801, in <lambda>\n    deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(p, exclude=[\"vocab\"])\n  File \"tokenizer.pyx\", line 391, in spacy.tokenizer.Tokenizer.from_disk\n  File \"tokenizer.pyx\", line 437, in spacy.tokenizer.Tokenizer.from_bytes\n  File \"/srv/yoga-chat-bot/env36/lib64/python3.6/re.py\", line 233, in compile\n    return _compile(pattern, flags)\n  File \"/srv/yoga-chat-bot/env36/lib64/python3.6/re.py\", line 301, in _compile\n    p = sre_compile.compile(pattern, flags)\n  File \"/srv/yoga-chat-bot/env36/lib64/python3.6/sre_compile.py\", line 562, in compile\n    p = sre_parse.parse(p, flags)\n  File \"/srv/yoga-chat-bot/env36/lib64/python3.6/sre_parse.py\", line 855, in parse\n    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)\n  File \"/srv/yoga-chat-bot/env36/lib64/python3.6/sre_parse.py\", line 416, in _parse_sub\n    not nested and not items))\n  File \"/srv/yoga-chat-bot/env36/lib64/python3.6/sre_parse.py\", line 527, in _parse\n    code1 = _class_escape(source, this)\n  File \"/srv/yoga-chat-bot/env36/lib64/python3.6/sre_parse.py\", line 336, in _class_escape\n    raise source.error('bad escape %s' % escape, len(escape))\nsre_constants.error: bad escape \\p at position 257", "issue_status": "Closed", "issue_reporting_time": "2019-09-09T09:03:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "347": {"issue_url": "https://github.com/explosion/spaCy/issues/4260", "issue_id": "#4260", "issue_summary": "Spacy doesnot identify GPE correctly", "issue_description": "dhwani2410 commented on 9 Sep 2019\nI tried running my example in your binder (https://spacy.io/usage/linguistic-features#tokenization)\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(u\"Trypsin-sensitive photosynthetic activities in chloroplast membranes from Chlamydomonas reinhardi, y-1.;Location of electron transport chain components in chloroplast membranes of chlamydomonas reinhardi, y-1 was investigated by use of proteolytic digestion with soluble or insolubilized trypsin.\")\n\nfor ent in doc.ents:\nprint(ent.text, ent.start_char, ent.end_char, ent.label_)\nTrypsin 0 7 GPE\nChlamydomonas 74 87 GPE\nAs for I understand GPE is the geopolitical entity which includes Countries, Cities or location. However the result it is throwing is a chemical or an organism name.\nIs there some way to rectify it.", "issue_status": "Closed", "issue_reporting_time": "2019-09-09T07:48:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "348": {"issue_url": "https://github.com/explosion/spaCy/issues/4256", "issue_id": "#4256", "issue_summary": "Spacy pretraining", "issue_description": "keemsunguk commented on 8 Sep 2019\nI would like to learn the theory behind the Spacy pretraining. The website says BERT/ULMFit \"like\" but wish to know more detail. Where can I find references you've used to implement pretrain module?\nThanks\nYour Environment\nOperating System: OSX, Linux\nPython Version Used: 3.7\nspaCy Version Used: 2.1\nEnvironment Information: conda", "issue_status": "Closed", "issue_reporting_time": "2019-09-07T20:41:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "349": {"issue_url": "https://github.com/explosion/spaCy/issues/4255", "issue_id": "#4255", "issue_summary": "Spanish is giving the infinitive verb as the lemma for a SCONJ", "issue_description": "sontek commented on 8 Sep 2019\nHow to reproduce the behaviour\n>>> import spacy\n>>> nlp = spacy.load('es')\n>>> sentence = \"sencillo como habr\u00edan deseado.\"\n>>> doc = nlp(sentence)\n>>> for token in doc:\n...     print(token.text, token.lemma_, token.pos_)\n...\nsencillo sencillo ADJ\ncomo comer SCONJ\nhabr\u00edan haber AUX\ndeseado desear VERB\n. . PUNCT\n>>>\ncomo in this case is conjunction in this case but the lemma is showing comer which is the infinitive verb.\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Darwin-18.0.0-x86_64-i386-64bit\nPython version: 3.6.7\nModels: es\nOperating System: OSX\nPython Version Used: 3.6", "issue_status": "Closed", "issue_reporting_time": "2019-09-07T20:21:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "350": {"issue_url": "https://github.com/explosion/spaCy/issues/4254", "issue_id": "#4254", "issue_summary": "Spanish is making abbreviations as the end of a sentence", "issue_description": "sontek commented on 8 Sep 2019\nHow to reproduce the behaviour\n>>> import spacy\n>>> nlp = spacy.load('es')\n>>> sentence = \"\u00bfPodr\u00eda hacer el favor de comunicarme con Washington, EE. UU. ?\"\n>>> doc = nlp(sentence)\n>>> len(list(doc.sents))\n3\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Darwin-18.0.0-x86_64-i386-64bit\nPython version: 3.6.7\nModels: es\nOperating System: OSX\nPython Version Used: Python3.6", "issue_status": "Closed", "issue_reporting_time": "2019-09-07T20:05:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "351": {"issue_url": "https://github.com/explosion/spaCy/issues/4253", "issue_id": "#4253", "issue_summary": "Spanish is identifying compound verbs as nouns", "issue_description": "sontek commented on 8 Sep 2019\nHow to reproduce the behaviour\n>>> import spacy\n>>> nlp = spacy.load('es')\n>>> sentence = \"No puedo hacerlo.\"\n>>> doc = nlp(sentence)\n>>> for token in doc:\n...     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n...\nWhich returns:\nNo No ADV ADV__Polarity=Neg advmod Xx True True\npuedo poder AUX AUX__Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin ROOT xxxx True True\nhacerlo hacerlo NOUN NOUN__Gender=Masc|Number=Sing obj xxxx True True\n. . PUNCT PUNCT__PunctType=Peri punct . False False\nhacerlo is Compound of the infinitive hacer and the pronoun lo. It is not a noun.\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Darwin-18.0.0-x86_64-i386-64bit\nPython version: 3.6.7\nModels: es\nOperating System: OSX\nPython Version Used: Python3.6", "issue_status": "Closed", "issue_reporting_time": "2019-09-07T20:01:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "352": {"issue_url": "https://github.com/explosion/spaCy/issues/4250", "issue_id": "#4250", "issue_summary": "from_bytes, cant find model en_model.vectors", "issue_description": "smiles3983 commented on 6 Sep 2019\nI have been using the nlp.from_bytes method with the large english model version 1. We upgraded to the 2.1.0 model, and now im getting an error stating that it cant find the en_model.vectors. See below how I load the model. When I do the to_bytes, am I supposed to also save the en_model.vectors separately?\nwith open(os.path.join(filepath, \"large_2.1.0.txt\"), 'rb') as file:\nmodelContent = file.read()\nnlp = spacy.blank(meta[\"lang\"])\nfor pipe_name in meta[\"pipeline\"]:\npipe = nlp.create_pipe(pipe_name)\nnlp.add_pipe(pipe)\nnlp.from_bytes(modelContent)\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-09-06T13:57:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "353": {"issue_url": "https://github.com/explosion/spaCy/issues/4244", "issue_id": "#4244", "issue_summary": "Pseudo-rehearsal solution not getting trained", "issue_description": "jerilkuriakose commented on 6 Sep 2019\nHi,\nI was trying to find the ner entities from a paragraph, and I was using the en_core_web_sm model for it. The model was able to identify most of the required entities, but a few were not identified. The following is the code:\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nsentence = u\"\"\"\nICICI bank is a banking company. New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n\nAt least 285 people have contracted measles in the city since seventh of September, mostly in Brooklyn\u2019s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n\nThe mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\n\nThodupuzha is a nice play to stay, and is Pala.\n\"\"\"\nnytimes = nlp(sentence)\nentities = [(i, i.label_, i.label) for i in nytimes.ents]\nprint(entities)\nThe following is the output:\n[(, 'GPE', 382),\n (New York City, 'GPE', 382),\n (Tuesday, 'DATE', 388),\n (At least 285, 'CARDINAL', 394),\n (seventh, 'ORDINAL', 393),\n (September, 'DATE', 388),\n (Brooklyn, 'GPE', 382),\n (Williamsburg, 'GPE', 382),\n (four, 'CARDINAL', 394),\n (Zip, 'PERSON', 378),\n (Bill de Blasio, 'PERSON', 378),\n (Tuesday, 'DATE', 388),\n (Orthodox, 'NORP', 379),\n (Jews, 'NORP', 379),\n (6 months old, 'DATE', 388),\n (1,000, 'MONEY', 391),\n (Thodupuzha, 'PERSON', 378),\n (Pala, 'PERSON', 378),\n (, 'GPE', 382)]\nThe problem here was, it didn't identify ICICI bank as an ORG, and it identified two places / locations (LOC) such as Thodupuzha and Pala as PERSON. So I thought of training the existing en_core_web_sm model with the entities that were not identified. I Googled my requirement and found Pseudo-rehearsal to be a solution. The following is the code to update the model:\nimport random\nfrom spacy.gold import GoldParse\nfrom cytoolz import partition_all\n# training data\nTRAIN_DATA = [\n    (\"Where is ICICI bank located\", {\"entities\": [(9, 18, \"ORG\")]}),\n    (\"I like Thodupuzha and Pala\", {\"entities\": [(7, 16, \"LOC\"), (22, 25, \"LOC\")]}),\n    (\"Thodupuzha is a tourist place\", {\"entities\": [(0, 9, \"LOC\")]}),\n    (\"Pala is famous for mangoes\", {\"entities\": [(0, 3, \"LOC\")]}),\n    (\"ICICI bank is one of the largest bank in the world\", {\"entities\": [(0, 9, \"ORG\")]}),\n    (\"ICICI bank has a branch in Thodupuzha\", {\"entities\": [(0, 9, \"ORG\"), (27, 36, \"LOC\")]}),\n]\n# preparing the revision data\nrevision_data = []\nfor doc in nlp.pipe(list(zip(*TRAIN_DATA))[0]):\n    tags = [w.tag_ for w in doc]\n    heads = [w.head.i for w in doc]\n    deps = [w.dep_ for w in doc]\n    entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]\n    revision_data.append((doc, GoldParse(doc, tags=tags, heads=heads,\n                                         deps=deps, entities=entities)))\n# preparing the fine_tune_data\nfine_tune_data = []\nfor raw_text, entity_offsets in TRAIN_DATA:\n    doc = nlp.make_doc(raw_text)\n    gold = GoldParse(doc, entities=entity_offsets['entities'])\n    fine_tune_data.append((doc, gold))\n# training the model\nn_epoch = 10\nbatch_size = 2\nfor i in range(n_epoch):\n    examples = revision_data + fine_tune_data\n    losses = {}\n    random.shuffle(examples)\n    for batch in partition_all(batch_size, examples):\n        docs, golds = zip(*batch)\n        nlp.update(docs, golds, drop=0.0, losses=losses)\n# finding ner with the updated model\nnytimes = nlp(sentence)\nentities = [(i, i.label_, i.label) for i in nytimes.ents]\nprint(entities)\nThe following is the ouput after training:\n[(New York City, 'GPE', 382),\n (Tuesday, 'DATE', 388),\n (At least 285, 'CARDINAL', 394),\n (Brooklyn, 'GPE', 382),\n (Williamsburg, 'GPE', 382),\n (four, 'CARDINAL', 394),\n (Zip, 'PERSON', 378),\n (Bill de Blasio, 'PERSON', 378),\n (Tuesday, 'DATE', 388),\n (Orthodox, 'NORP', 379),\n (Jews, 'NORP', 379),\n (6 months old, 'DATE', 388),\n (1,000, 'MONEY', 391),\n (Thodupuzha, 'PERSON', 378),\n (Pala, 'PERSON', 378)]\nIn the output one ner entities went missing for e.g., (seventh, 'ORDINAL', 393), and the places / locations (LOC) such as Thodupuzha and Pala are still getting predicted as PERSON, whereas it was trained for LOC. Is there something that I am missing? Kindly help.\nEnvironment\nOperating System: Windows 7 Professional\nPython Version Used: 3.6.5\nspaCy Version Used: 2.0.11\nEnvironment Information: Jupyter notebook 4.4.0", "issue_status": "Closed", "issue_reporting_time": "2019-09-06T01:07:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "354": {"issue_url": "https://github.com/explosion/spaCy/issues/4243", "issue_id": "#4243", "issue_summary": "[T008] Bad configuration of Tagger.", "issue_description": "achyutjoshi commented on 6 Sep 2019 \u2022\nedited\nHi! I was just importing the model(en_core_web_lg) and I got this error - [T008] Bad configuration of Tagger. This is probably a bug within spaCy. We changed the name of an internal attribute for loading pre-trained vectors, and the class has been passed the old name (pretrained_dims) but not the new name (pretrained_vectors).\nMy Environment\nspaCy version: 2.0.18\nPlatform: Darwin-18.5.0-x86_64-i386-64bit\nPython version: 3.7.2\nModels: en_core_web_lg", "issue_status": "Closed", "issue_reporting_time": "2019-09-05T23:24:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "355": {"issue_url": "https://github.com/explosion/spaCy/issues/4242", "issue_id": "#4242", "issue_summary": "Docs: difference between POS and TAG / origins of these", "issue_description": "poke1024 commented on 5 Sep 2019 \u2022\nedited\nAs shown in the table in https://spacy.io/usage/linguistic-features#pos-tagging, spacy differentiates two kinds of POS tags (POS and TAG). Probably I'm just a newbie, but is there some docs, which kind of philosophy these are based on? TAG seem to be based on Penn TreeBank tags, POS on a more universal set (as in Petrov, Slav, et al. \"A Universal Part-of-Speech Tagset\"). Any additional explanation on this would be helpful for the docs I guess, especially for people like me ;-)", "issue_status": "Closed", "issue_reporting_time": "2019-09-05T10:30:03Z", "fixed_by": "#4246", "pull_request_summary": "Make pos/tag distinction more clear in docs", "pull_request_description": "Member\nsvlandeg commented on 6 Sep 2019 \u2022\nedited\nDescription\nMoved the \"Part-of-speech tag scheme\" infobox up on the page https://spacy.io/usage/linguistic-features\nCorrected one link to the dependencies doc (was pointing to POS doc)\nTypes of change\nChange to the documentation, closes #4242\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-06T08:31:22Z", "files_changed": [["19", "website/docs/usage/linguistic-features.md"]]}, "356": {"issue_url": "https://github.com/explosion/spaCy/issues/4241", "issue_id": "#4241", "issue_summary": "Weird Greek Lemmas", "issue_description": "Contributor\npolm commented on 5 Sep 2019\nI can not read Greek, but looking at the nouns in the lemma_index.json file for Greek, these are the first several entries:\n\"(\u03b9\u03c1\u03bb\u03b1\u03bd\u03b4\u03b9\u03ba\u03ac)\", \"(\u03c3\u03ba\u03c9\u03c4\u03b9\u03ba\u03ac)\", \"(\u03c3\u03bf\u03c1\u03ac\u03bd\u03b9)\", \"-\u03b1\u03bb\u03b3\u03af\u03b1\", \"-\u03b2\u03b1\u03c4\u03ce\", \"-\u03b2\u03b1\u03c4\u1ff6\", \"-\u03bf\u03cd\u03bb\u03b1\", \"-\u03c0\u03bb\u03b7\u03be\u03af\u03b1\", \"-\u03ce\u03bd\u03c5\u03bc\u03bf\", \"sofa\", \"table\", \"\u03ac\u03b2\u03b1\u03ba\u03b1\u03c2\", \"\u03ac\u03b2\u03b1\u03c4\u03bf\", \"\u03ac\u03b2\u03b1\u03c4\u03bf\u03bd\"\nI'm pretty sure parentheses don't belong there, and the things that begin with hyphens and \"table\" and \"sofa\" seem out of place.\nMaybe this is due to a bug in the Wiktionary parsing script mentioned by @giannisdaras in #2558?\nIf someone who speaks Greek could check and clarify what, if anything, should be removed, that would be great.", "issue_status": "Closed", "issue_reporting_time": "2019-09-05T05:53:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "357": {"issue_url": "https://github.com/explosion/spaCy/issues/4240", "issue_id": "#4240", "issue_summary": "Instruction of customer name entity recognition", "issue_description": "nguyenminhtuanfit commented on 5 Sep 2019 \u2022\nedited\nHi,\nI have 5 thousand emails and i want to create a custom name entity recognition.\nif model is None:\n        nlp.begin_training()\n    optimizer = nlp.begin_training()    \n    for itn in range(n_iter):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(\n                texts,  # batch of texts\n                annotations,  # batch of annotations\n                drop=0.5,  # dropout - make it harder to memorise data\n                losses=losses,\n                sgd=optimizer,\n            )\n        print(\"Losses\", losses)`\n\n> What is the ideal batch size of training process?\nI will do sent_tokenize for all dataset and pass it as a single batch or do i need to split in a smaller single batch?\nThanks \n ", "issue_status": "Closed", "issue_reporting_time": "2019-09-05T02:08:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "358": {"issue_url": "https://github.com/explosion/spaCy/issues/4238", "issue_id": "#4238", "issue_summary": "Tokenizer cache doesn't handle modifications to special cases or token_match correctly", "issue_description": "Collaborator\nadrianeboyd commented on 4 Sep 2019 \u2022\nedited\nHow to reproduce the behaviour\nThe github suggested related issues were actually helpful! #1061 seems to have snuck back in. It works in 2.0.18, not in 2.1.0.\nModifications to special cases and token_match don't work if the pipeline has been run at least once due to the tokenizer cache.\nimport spacy\nfrom spacy.symbols import ORTH\n\ntext = '(_SPECIAL_) A/B'\n\nnlp = spacy.load('en_core_web_sm')\nnlp.tokenizer.add_special_case('_SPECIAL_', [{ORTH: '_SPECIAL_'}])\nnlp.tokenizer.add_special_case('A/B', [{ORTH: 'A/B'}])\nprint([token.text for token in nlp(text)])\n# ['(', '_SPECIAL_', ')', 'A/B']\n\nnlp = spacy.load('en_core_web_sm')\nprint([token.text for token in nlp(text)])\n# ['(', '_', 'SPECIAL', '_', ')', 'A', '/', 'B']\nnlp.tokenizer.add_special_case('_SPECIAL_', [{ORTH: '_SPECIAL_'}])\nnlp.tokenizer.add_special_case('A/B', [{ORTH: 'A/B'}])\nprint([token.text for token in nlp(text)])\n# ['(', '_', 'SPECIAL', '_', ')', 'A/B']\n\ntext = \"This is a URL: http://example.com/file.html.\"\n\nnlp = spacy.load('en_core_web_sm')\nnlp.tokenizer.token_match = None\nprint([token.text for token in nlp(text)])\n# ['This', 'is', 'a', 'URL', ':', 'http://example.com', '/', 'file.html', '.']\n\nnlp = spacy.load('en_core_web_sm')\nprint([token.text for token in nlp(text)])\n# ['This', 'is', 'a', 'URL', ':', 'http://example.com/file.html', '.']\nnlp.tokenizer.token_match = None\nprint([token.text for token in nlp(text)])\n# ['This', 'is', 'a', 'URL', ':', 'http://example.com/file.html', '.']\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Linux-4.19.0-5-amd64-x86_64-with-debian-10.0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-09-04T18:26:27Z", "fixed_by": "#4258", "pull_request_summary": "Flush tokenizer cache when necessary", "pull_request_description": "Collaborator\nadrianeboyd commented on 8 Sep 2019\nDescription\nFlush tokenizer cache when affixes, token_match, or special cases are\nmodified.\nFixes #4238, same issue as in #1250.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-08T18:52:47Z", "files_changed": [["1", "spacy/tests/regression/test_issue1001-1500.py"], ["8", "spacy/tokenizer.pxd"], ["59", "spacy/tokenizer.pyx"]]}, "359": {"issue_url": "https://github.com/explosion/spaCy/issues/4236", "issue_id": "#4236", "issue_summary": "Can spaCy use vectors as training data?", "issue_description": "pranav4838 commented on 4 Sep 2019\nHello all ,\nI am currently working on a multi class text classification project. I have a set of .txt files for which I have created the equivalent vector representation using spaCy. I now want to create labelled data which will be my training data in order to train and create a model .\nThe training data should be vectors followed by the labels.\nCan someone please direct me on how to get spaCy to accept vectors. The examples I have seen so far use text data followed by labels as the the training data . Are there any examples available that use vectors followed by labels as the training data .\nThank you !!\nEnvironment\nspaCy version: 2.1.8\nPlatform: Linux-4.15.0-52-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nModels: en_pytt_bertbaseuncased_lg", "issue_status": "Closed", "issue_reporting_time": "2019-09-04T15:03:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "360": {"issue_url": "https://github.com/explosion/spaCy/issues/4234", "issue_id": "#4234", "issue_summary": "Subtlety in tokenizer documentation", "issue_description": "DomHudson commented on 4 Sep 2019 \u2022\nedited\nSummary\nHi!\nI was reading the description of how the tokenizer works and noticed a slight inconsistency between the described logic and what happens with the real code.\nRegarding substrings, the documentation says that there is a \"check whether we have an explicitly defined rule for this substring. If we do, use it.\"\nI'm interested in the case where there is a suffix and a special case, for example the token _SPECIAL_ (which should not be tokenized) within parentheses: (_SPECIAL_).\nIf I follow the logic in the example code with this case, the special case would never match (as the string being presented to the special case logic still has the suffix(es) attached), so the rule would not apply.\nFirst loop, input: \"(_SPECIAL_)\"\nNo special case matched identically so the prefix \"(\" matched and stripped\nSecond loop, input: _SPECIAL_)\"\nNo special case matched identically, so the prefix \"_\" matched and stripped.\nThird loop, input: SPECIAL_)\nNo longer resembles the special case so the remaining suffix punctuation is tokenized away from the main string.\nWith the real code the correct output is returned with the parentheses in their own tokens, but _SPECIAL_ as a single token.\nIs there special logic happening within the special case logic to match the first part of the string in case there is a trailing suffix? Or is the order of logic slightly differently to account for this? Or is there a different solution?\nMany thanks!\nCode illustrating inconsistency\nimport spacy\nfrom spacy.symbols import ORTH\n\nTEST_STRING = '(_SPECIAL_)'\n\ndef print_tokens(nlp):\n    \"\"\" Illustrative function to show the tokens detected on the test string\n    with a given spacy tokenizer.\n\n    :return void:\n    \"\"\"\n    print([token.text for token in nlp(TEST_STRING)])\n\n\nnlp = spacy.load('en_core_web_sm')\nprint_tokens(nlp)\n# This is the default tokenizer, so the prefix and suffix rules separate the\n# punctuation into their own tokens.\n# Output: ['(', '_', 'SPECIAL', '_', ')']\n\n\n# Now I define '-SPECIAL-' as its own token which should not be segmented.\nnlp.tokenizer.add_special_case('_SPECIAL_', [{ORTH: '_SPECIAL_'}])\nprint_tokens(nlp)\n# The output is correct. The punctuation outside of the special case is\n# segmented, but the special case returned as a single token.\n# Output: ['(', '_SPECIAL_', ')']\nWhich page or section is this issue related to?\nhttps://spacy.io/usage/linguistic-features#how-tokenizer-works", "issue_status": "Closed", "issue_reporting_time": "2019-09-04T14:24:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "361": {"issue_url": "https://github.com/explosion/spaCy/issues/4233", "issue_id": "#4233", "issue_summary": "How does textcat deal with multi-sentence passages?", "issue_description": "H20Watermelon commented on 4 Sep 2019\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.8\nI wonder how spaCy's textcat computes scores for longer passages. Since CNN typically requires a fixed length sequence (typically when I train CNN models for text classification). But for longer passages, does spaCy simply chops off part of a long passage, or does it do some kind of averaging based on individual sentences (if there are multiple sentences). Thanks!", "issue_status": "Closed", "issue_reporting_time": "2019-09-04T13:39:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "362": {"issue_url": "https://github.com/explosion/spaCy/issues/4231", "issue_id": "#4231", "issue_summary": "Unexpected Low similarity for similar tokens", "issue_description": "chikubee commented on 4 Sep 2019 \u2022\nedited\nHow to reproduce the behaviour\nYour Environment\nOperating System: Built on Google Colab\nPython Version Used: python 3\nspaCy Version Used: 2.1.8\nspacy-pytorch-transformers verison: 2.1.1\nI was wondering if you could help me understand why there is extremely low similarity for some genuinely similar cases.\nSentence1: I am a salaried person\nSentence2: I am a person who gets salary\nThe similarity between the tokens salaried and salary is extremely less.\nWhile Sentence2 actually matches with other sentences in the corpora relevant to salary, Sentence1 does not.\nDoes it have something to do with the word-pieces as salaried gets broken into sal ##ari ##ed?\nAnd how exactly is the final vector constructed from wordpieces for a particular token?", "issue_status": "Closed", "issue_reporting_time": "2019-09-04T05:59:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "363": {"issue_url": "https://github.com/explosion/spaCy/issues/4230", "issue_id": "#4230", "issue_summary": "pytorch-transformers version 1.1.0 support", "issue_description": "Code4SAFrankie commented on 4 Sep 2019\nFeature description\nCan you please support pytorch-transformers version 1.1.0 otherwise I can't run spacy and flair at the same time. I hate virtualenv.\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.", "issue_status": "Closed", "issue_reporting_time": "2019-09-03T20:36:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "364": {"issue_url": "https://github.com/explosion/spaCy/issues/4229", "issue_id": "#4229", "issue_summary": "Multi-word EntityRuler entities cause NER model to stop working", "issue_description": "Collaborator\nadrianeboyd commented on 4 Sep 2019 \u2022\nedited\nHow to reproduce the behaviour\nI've seen quite a few people noticing that using the EntityRuler before NER causes major changes in the NER model predictions (like #3775). I tried a few examples and it's pretty drastic.\nI think there must be a bug in how it manages its internal state when skipping over multi-word entities or something like that. If the EntityRuler entity is one token long, the NER model seems to keep working, but as soon as there's a pre-existing multi-word entity it doesn't label anything else after that point, even if the text is much longer.\nimport spacy\nfrom spacy.pipeline import EntityRuler\n\npatterns1 = [{'label': 'car', \"pattern\": [{'LOWER': 'cars'}]}]\npatterns2 = [{'label': 'driving_car', \"pattern\": [{'LOWER': 'driving'}, {'LOWER': 'cars'}]}]\n\ntext = (\"When Sebastian Thrun started working on self-driving cars at \"\n        \"Google in 2007, few people outside of the company took him \"\n        \"seriously. \u201cI can tell you very senior CEOs of major American \"\n        \"car companies would shake my hand and turn away because I wasn\u2019t \"\n        \"worth talking to,\u201d said Thrun, in an interview with Recode earlier \"\n        \"this week.\")\n\n## no EntityRuler\nnlp = spacy.load('en')\nprint([(ent.text, ent.label_, ent.start, ent.end) for ent in nlp(text).ents])\n# [('Sebastian Thrun', 'PERSON', 1, 3), ('Google', 'ORG', 11, 12),\n# ('2007', 'DATE', 13, 14), ('American', 'NORP', 35, 36),\n# ('Thrun', 'PERSON', 55, 56), ('Recode', 'ORG', 61, 62),\n# ('earlier this week', 'DATE', 62, 65)]\n\n## EntityRuler with one-token pattern\nnlp = spacy.load('en')\nruler = EntityRuler(nlp, validate=True, overwrite_ents=True)\nruler.add_patterns(patterns1)\nnlp.add_pipe(ruler, before='ner')\nprint([(ent.text, ent.label_, ent.start, ent.end) for ent in nlp(text).ents])\n# [('Sebastian Thrun', 'PERSON', 1, 3), ('cars', 'car', 9, 10),\n# ('Google', 'ORG', 11, 12), ('2007', 'DATE', 13, 14),\n# ('American', 'NORP', 35, 36), ('Thrun', 'PERSON', 55, 56),\n# ('Recode', 'ORG', 61, 62), ('earlier this week', 'DATE', 62, 65)]\n\n## EntityRuler with two-token pattern\nnlp = spacy.load('en')\nruler = EntityRuler(nlp, validate=True, overwrite_ents=True)\nruler.add_patterns(patterns2)\nnlp.add_pipe(ruler, before='ner')\nprint([(ent.text, ent.label_, ent.start, ent.end) for ent in nlp(text).ents])\n# [('Sebastian Thrun', 'PERSON', 1, 3), ('driving cars', 'driving_car', 8, 10)]\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-4.19.0-5-amd64-x86_64-with-debian-10.0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-09-03T19:42:10Z", "fixed_by": "#4232", "pull_request_summary": "Fix handling of preset entities in NER", "pull_request_description": "Collaborator\nadrianeboyd commented on 4 Sep 2019 \u2022\nedited\nDescription\nFix check of valid ent_type for B\nAdd valid L as preset-I followed by not-I\nFixes #4229.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-04T13:02:32Z", "files_changed": [["5", "spacy/syntax/ner.pyx"]]}, "365": {"issue_url": "https://github.com/explosion/spaCy/issues/4228", "issue_id": "#4228", "issue_summary": "nlp.use_params: 'bool' object has no attribute 'use_params'", "issue_description": "tsoernes commented on 3 Sep 2019\nHow to reproduce the behaviour\nWhen using multiprocessing or joblib to train multiple texcats. I get errors when trying to save the model, but only sometimes. Most of the time it works.\ntrainer_ = partial(\n    trainer,\n    labels=labels,\n    n_iter=n_iter,\n    dropout=dropout,\n    learn_rate=learn_rate,\n    batch_start=batch_start,\n    batch_max=batch_max\n)\nwith Pool(processes=4) as pool:\n    pool.starmap(trainer_, zip(model_dirs, train_data))\n\ndef trainer(\n    model_dir,\n    train_data,\n    labels,\n    n_iter,\n    dropout,\n    learn_rate,\n    batch_start,\n    batch_max,\n):\n    nlp = spacy.load('en_core_web_lg')\n    print(f\"Starting {model_dir}\")\n    config = {\"exclusive_classes\": False, \"architecture\": 'bow'}\n    textcat = nlp.create_pipe(\"textcat\", config=config)\n    nlp.add_pipe(textcat, last=True)\n\n    for label in labels:\n        textcat.add_label(label)\n\n    batch_sizes = compounding(batch_start, batch_max, 1.001)\n    other_pipes = [pipe for pipe in nlp.pipe_names if not pipe != \"textcat\"]\n    with nlp.disable_pipes(*other_pipes):\n        optimizer = nlp.begin_training()\n        # handle_ctrlc(nlp, optimizer, model_dir, None, None, None)\n        for epoch in range(1, n_iter + 1):\n            losses = {}\n            random.shuffle(train_data)\n            batches = minibatch(train_data[:100], size=batch_sizes)\n            for batch in batches:\n                texts, cats = zip(*batch)\n                nlp.update(texts, cats, sgd=optimizer, drop=dropout, losses=losses)\n    print(f\"Finished {model_dir}\")\n    with nlp.use_params(optimizer.averages):\n        textcat.to_disk(model_dir)\nYour Environment\nOperating System: Fedora 30\nPython Version Used: 3.7\nspaCy Version Used: spacy 2.1.8\nEnvironment Information:\n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/nlp/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/ubuntu/anaconda3/envs/nlp/lib/python3.7/multiprocessing/pool.py\", line 47, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/home/ubuntu/fintechdb/nlp/catml/textcat/train_hierarchy.py\", line 103, in trainer\n    with nlp.use_params(optimizer.averages):\n  File \"/home/ubuntu/anaconda3/envs/nlp/lib/python3.7/contextlib.py\", line 112, in __enter__\n    return next(self.gen)\n  File \"/home/ubuntu/anaconda3/envs/nlp/lib/python3.7/site-packages/spacy/language.py\", line 675, in use_params\n    next(context)\n  File \"pipes.pyx\", line 152, in use_params\nAttributeError: 'bool' object has no attribute 'use_params'\n\"\"\"", "issue_status": "Closed", "issue_reporting_time": "2019-09-03T17:18:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "366": {"issue_url": "https://github.com/explosion/spaCy/issues/4227", "issue_id": "#4227", "issue_summary": "Entity ruler and ner outputs not as expected", "issue_description": "AnnaAnia commented on 3 Sep 2019\nI have a custom NER model created in Spacy and loaded from file. Trying to add an entity ruler as per code below. Output is either the ruler or ner but not a combination where entities wouldnt overlap.\nIs this a bug or am I missing somthing?\nner_model_ruler = spacy.load(r\"***path_to_model***\")\npatterns=[{\"label\":\"COVER_TYPE_R\",\"pattern\":[{\"LOWER\":\"word_1\"},{\"LOWER\":\"word_2\"}]},\n          ..... ]\nruler = EntityRuler(ner_model_ruler,validate=True)\nruler.add_patterns(patterns)\nner_model_ruler.add_pipe(ruler,after='ner') \nHelp would be very much appreciated.\nYour Environment\nPython Version Used: 3.7\nspaCy Version Used: 2.1.5", "issue_status": "Closed", "issue_reporting_time": "2019-09-03T15:14:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "367": {"issue_url": "https://github.com/explosion/spaCy/issues/4225", "issue_id": "#4225", "issue_summary": "Email-like text tokenized wrongly", "issue_description": "yarongon commented on 3 Sep 2019\nProblem description\nTokens are merged (or didn't split, to begin with) when email-like text is parsed, however, the like_email flag is false.\nHow to reproduce the behavior\nimport spacy\n\nnlp = spacy.load(\"en\", disable=[\"parser\", \"tagger\", \"ner\"])\nresume_text = \"a,b@mail\"  # text that does not look like an email address\ndoc = nlp(resume_text)\nprint([t.text for t in doc]) # => ['a', ',', 'b@mail'], 3 tokens\n\nresume_text = \"a,b@mail.com\"  # Text that looks more like an email address \ndoc = nlp(resume_text)\nprint([t.text for t in doc])  # => ['a,b@mail.com'], 1 token, and `like_email` is False. AFAIK is should still be 3 tokens\nInfo about spaCy\nspaCy version: 2.1.3\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.8\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-09-03T10:44:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "368": {"issue_url": "https://github.com/explosion/spaCy/issues/4224", "issue_id": "#4224", "issue_summary": "No more matching on NORM?", "issue_description": "Contributor\nmr-bjerre commented on 2 Sep 2019\nI might be remembering wrong but wasn't it possible to match on the norm of a word a while back with the Token matcher?\nI can't seem to find anything about it in the documentation anymore, nor creating custom norm exceptions.\nI would use it for having a norm for single-token month expressions (that would be faster than having a matcher setting custom attributes, right?)", "issue_status": "Closed", "issue_reporting_time": "2019-09-02T10:41:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "369": {"issue_url": "https://github.com/explosion/spaCy/issues/4223", "issue_id": "#4223", "issue_summary": "Heroku build problem with fixed versions in requirements.txt", "issue_description": "loganyc1934 commented on 2 Sep 2019 \u2022\nedited\nI'm trying to use spacy with en_core_web_sm model on Heroku and had build issues. I looked at some similar previous issues but didn't see one that has exactly the same problem, so I'm opening a new one. Please let me know if this is a known issue, what is the best solution here (avoid pip freeze or something).\nHow to reproduce the problem\nI included\nspacy>=2.0.0,<3.0.0\nhttps://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm\nas instructed here. Then I did pip freeze and my requirements.txt looks like this\nalembic==1.1.0\nAPScheduler==3.6.1\nautopep8==1.4.4\nblis==0.2.4\ncertifi==2019.6.16\nchardet==3.0.4\nClick==7.0\ncycler==0.10.0\ncymem==2.0.2\nen-core-web-sm==2.1.0\nFlask==1.1.1\nFlask-Migrate==2.5.2\nFlask-SQLAlchemy==2.4.0\ngevent==1.4.0\ngreenlet==0.4.15\ngunicorn==19.9.0\nidna==2.8\nitsdangerous==1.1.0\njellyfish==0.5.6\nJinja2==2.10.1\njoblib==0.13.2\nkiwisolver==1.1.0\nMako==1.1.0\nMarkupSafe==1.1.1\nmatplotlib==3.1.1\nmurmurhash==1.0.2\nnumpy==1.17.1\nPillow==6.1.0\nplac==0.9.6\npreshed==2.0.1\npsycopg2-binary==2.8.3\npycodestyle==2.5.0\npyparsing==2.4.2\npython-dateutil==2.8.0\npython-dotenv==0.10.3\npython-editor==1.0.4\npytz==2019.2\nrequests==2.22.0\nscikit-learn==0.21.3\nscipy==1.3.1\nsix==1.12.0\nspacy==2.1.8\nSQLAlchemy==1.3.8\nsqlitedict==1.6.0\nsrsly==0.1.0\nthinc==7.0.8\ntqdm==4.35.0\ntweet-preprocessor==0.5.0\ntzlocal==2.0.0\nurllib3==1.25.3\nus==1.0.0\nwasabi==0.2.2\nWerkzeug==0.15.5\nwordcloud==1.5.0\nWith this, I push to Heroku and it fails with the following error\n-----> Installing requirements with pip\n       Collecting alembic==1.1.0 (from -r /tmp/build_1dda2d7d8fcdc780d5ec99f2fb927a0e/requirements.txt (line 1))\n         Downloading https://files.pythonhosted.org/packages/9a/0f/a5e8997d58882da8ecd288360dddf133a83145de6480216774923b393422/alembic-1.1.0.tar.gz (1.0MB)\n       Collecting en-core-web-sm==2.1.0 (from -r /tmp/build_1dda2d7d8fcdc780d5ec99f2fb927a0e/requirements.txt (line 10))\n         Could not find a version that satisfies the requirement en-core-web-sm==2.1.0 (from -r /tmp/build_1dda2d7d8fcdc780d5ec99f2fb927a0e/requirements.txt (line 10)) (from versions: )\n       No matching distribution found for en-core-web-sm==2.1.0 (from -r /tmp/build_1dda2d7d8fcdc780d5ec99f2fb927a0e/requirements.txt (line 10))\n !     Push rejected, failed to compile Python app.\n !     Push failed\nThen what I tried is I added the full url to this pip freeze version of requirements.txt. That give me the error Double requirements given:... because it clashes with the line en-core-web-sm==2.1.0.\nPlease do not close this issue if you think the solution is to include the url in the freeze, and only close it if there is a working version of frozen requirements.txt, or some other way of doing this in an optimal way. Currently I cannot use pip freeze.", "issue_status": "Closed", "issue_reporting_time": "2019-09-02T02:41:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "370": {"issue_url": "https://github.com/explosion/spaCy/issues/4221", "issue_id": "#4221", "issue_summary": "How to calculate Entity wise Accuracy, Precision, Recall", "issue_description": "AbhayGodbole commented on 31 Aug 2019\nHi\nI would like to calculate my custom entity extractor model's accuracy, precision etc. I found one solution where we need to provide the text and its start and end position in json form. its ok for couple of sentences, but for real data how to evaluate the model performance? ( I have trained the model on 3000 comments for 8 custom entities )\nPlease suggest", "issue_status": "Closed", "issue_reporting_time": "2019-08-31T01:57:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "371": {"issue_url": "https://github.com/explosion/spaCy/issues/4220", "issue_id": "#4220", "issue_summary": "Tips on how to increase GPU utilization", "issue_description": "H20Watermelon commented on 31 Aug 2019\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.8\nThe documents I work with are typically on the larger side (Articles, emails/email threads). When I do the following:\nresults = []\nfor doc in nlp.pipe(docs, batch_size = 100, as_tuples = True):\n    doc, docid = doc\n    results.append([docid, doc])\nThe GPU utilization rate is around 3% - 7%. I wonder if there is any suggestion/tips on how I can increase GPU utilization Also as a side note, my GPU's memory is 16 Gb, so this limits how big the batch size can be -- I tried 200 docs, but it was clearly too big as I got an cupy out of memory error.", "issue_status": "Closed", "issue_reporting_time": "2019-08-31T00:10:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "372": {"issue_url": "https://github.com/explosion/spaCy/issues/4218", "issue_id": "#4218", "issue_summary": "POS tagging after multi-word expression re-tokenization", "issue_description": "DBaker999 commented on 30 Aug 2019\nHello,\nI am wondering how the POS tagging is done for mult-word expressions.\n'carboxylic acid' gets tagged as 'ADJ' although it would seem more natural to have it tagged as a 'NOUN' because the term 'acid' the head of this multiword expression\n(see code below)\nAm I missing something? Is this something which I can adjust?\nThank you,\nDavid\nimport spacy\nfrom spacy.pipeline import EntityRuler\nsentence = \"A fatty acid is a carboxylic acid\"\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(sentence)\nprint([(t.text, t.pos_) for t in doc])\noutput: [('A', 'DET'), ('fatty', 'ADJ'), ('acid', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('carboxylic', 'ADJ'), ('acid', 'NOUN')]\nmwes = ['carboxylic acid', 'fatty acid']\nruler = EntityRuler(nlp)\nfor a in mwes:\nruler.add_patterns([{\"label\": \"x\", \"pattern\": a}])\nnlp.add_pipe(ruler)\ndoc = nlp(sentence)\nwith doc.retokenize() as retokenizer:\nfor ent in doc.ents:\nretokenizer.merge(doc[ent.start:ent.end])\nprint([(t.text, t.pos_) for t in doc])\noutput: [('A', 'DET'), ('fatty acid', 'ADJ'), ('is', 'VERB'), ('a', 'DET'), ('carboxylic acid', 'ADJ')]\ndoc = nlp('carboxylic acid')\nprint([(t.text, t.head, t.pos_) for t in doc])\noutput: [('carboxylic', acid, 'ADJ'), ('acid', acid, 'NOUN')]\nInfo about spaCy\nspaCy version: 2.1.3\nPlatform: Linux-4.15.0-20-generic-x86_64-with-debian-buster-sid\nPython version: 3.7.2\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-08-30T14:19:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "373": {"issue_url": "https://github.com/explosion/spaCy/issues/4216", "issue_id": "#4216", "issue_summary": "Bad interpretation for a french word using fr_core_news_sm-2.1.0", "issue_description": "dadadel commented on 30 Aug 2019\nHow to reproduce the behaviour\nUsing the french model fr_core_news_sm leads to a bad interpretation of word \"musique\" when it is given alone.\nIncorrect behaviour\nIt is recognized as ADJ while it can never be an adjective as it is always a noun (even if the verb \"musiquer\" exists, but if the word is used isolated it is a noun).\nimport spacy\nspacy = spacy.load(\"fr_core_news_sm\")\ntext = \"musique\"\nfor token in spacy(text):\n    print(token, token.tag_)\ntext2 = \"la musique\"\nfor token in self.spacy(text2):\n    print(token, token.tag_)\nmusique ADJ__Number=Sing\nCorrect behaviour\nWhen it is in a sentence like \"la musique\" it is well recognized as NOUN.\nimport spacy\nspacy = spacy.load(\"fr_core_news_sm\")\ntext2 = \"la musique\"\nfor token in self.spacy(text2):\n    print(token, token.tag_)\nla DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\nmusique NOUN__Gender=Fem|Number=Sing\nThe Environment\nOperating System: Ubuntu 19.04\nPython Version Used: python 3.7\nspaCy Version Used: spaCy 2.1.8 / fr_core_news_sm-2.1.0\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-30T09:19:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "374": {"issue_url": "https://github.com/explosion/spaCy/issues/4215", "issue_id": "#4215", "issue_summary": "Retokenization when text changes", "issue_description": "rrrepos commented on 30 Aug 2019\nFor my use-case, I read a document (multiple sentences) and perform some cleanup in the pipeline. This includes sending the document via a coref pipe to replace some text content. Another custom cleanup could include removing sentences that are irrelevant. The question I have is:\nA. is there a way to update and/ or delete text from the original text?\nB. If so, Is there some way I can retokenize the document?\nCurrently, I store the text in an intermediary file / stream and re-open it as a new doc. Thanks", "issue_status": "Closed", "issue_reporting_time": "2019-08-30T07:13:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "375": {"issue_url": "https://github.com/explosion/spaCy/issues/4214", "issue_id": "#4214", "issue_summary": "split span", "issue_description": "sr33kant commented on 30 Aug 2019\nHow to reproduce the behaviour\nYour Environment\nOperating System:windows\nPython Version Used:3.6\nspaCy Version Used:2.0.18\nEnvironment Information:\nmdl=spacy.load('en_core_web_sm')\ntxt=mdl('Our reported interest expense for the years ended December 31 ,2016 , 2015 and 2014 includes non - cash interest related to the accounting for convertible securities of $ 8.1 million , $ 7.9 million and $ 7.1 million , respectively .')\nprint( [(e.text, e.start_char, e.end_char, e.label_) for e in txt.ents])\n[('the years ended December 31 ,2016 , 2015 and 2014', 34, 83, 'DATE'), ('$ 8.1 million', 169, 182, 'MONEY'), ('$ 7.9 million', 185, 198, 'MONEY'), ('$ 7.1 million', 203, 216, 'MONEY')]\nwould have preferred to get three date entities as opposed to one. From documentation its not clear how to add a regex to split the span so it outputs three dates while not affecting other entity spans in the same sentence.", "issue_status": "Closed", "issue_reporting_time": "2019-08-29T22:10:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "376": {"issue_url": "https://github.com/explosion/spaCy/issues/4213", "issue_id": "#4213", "issue_summary": "span of date entity", "issue_description": "sr33kant commented on 30 Aug 2019 \u2022\nedited\nYour Environment\nOperating System: windows\nPython Version Used: 3.6\nspaCy Version Used: 2.0.18\nEnvironment Information:\nbug\nmdl=spacy.load('en_core_web_sm')\nttxt=mdl('Our reported interest expense for the years ended December 31 ,2016 ,2015 and 2014 includes non - cash interest related to the accounting for convertible securities of $ 8.1 million , $ 7.9 million and $ 7.1 million , respectively .')\nprint( [(e.text, e.start_char, e.end_char, e.label_) for e in txt.ents])\n('the years ended December 31 ,2016 ,2015 and 2014', 34, 82, 'DATE')\nI would like this to have been broken down into three dates as opposed to one big date it is currently outputting.\nLooking for suggestions on how to proceed Add custom tokenizer in the pipeline before ner ? Or entityruler ?", "issue_status": "Closed", "issue_reporting_time": "2019-08-29T21:16:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "377": {"issue_url": "https://github.com/explosion/spaCy/issues/4212", "issue_id": "#4212", "issue_summary": "Tokenize on $ seems to give issues", "issue_description": "Contributor\nmr-bjerre commented on 29 Aug 2019\nI am trying to tokenize US$50 into ['US', '$', '50'] but there seems to be an issue or I am misunderstanding something!?\nimport spacy.util\nimport spacy.lang.en\n\nnlp = spacy.lang.en.English()\n\ninfixes = nlp.Defaults.infixes + (\n    r\"(?<=\\S)\\$\",\n)\nnlp.tokenizer.infix_finditer = spacy.util.compile_infix_regex(infixes).finditer\n\nassert [t.text for t in nlp('US$50')] == ['US', '$', '50']  # fails\nE.g. replacing $ with X works as expected\ninfixes = nlp.Defaults.infixes + (\n    r\"(?<=\\S)X\", \n)\nnlp.tokenizer.infix_finditer = spacy.util.compile_infix_regex(infixes).finditer\n\nassert [t.text for t in nlp('USX50')] == ['US', 'X', '50']  # passes\nspaCy version: 2.1.8\nPlatform: Linux-5.0.0-25-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-29T15:05:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "378": {"issue_url": "https://github.com/explosion/spaCy/issues/4211", "issue_id": "#4211", "issue_summary": "Same text input, but got different token tag_ and dep_ with different version of spacy model&languages", "issue_description": "yano27 commented on 29 Aug 2019\nHello, I tested spacy with two different version and the same text produce two different dependency and tags.\nYour Environment\nOperating System: windows 10\nPython version: 3.6\nFirst Test:\nspacy version: 2.0.13\nmodel: de_core_news_sm 2.0.0\nenvironment: anaconda, jupyterNotebook\nSecond Test:\nspacy version: 2.1.8\nmodel: de_core_news_sm 2.1.0\nenvironment: visualStudio, django\nHow to reproduce the behaviour\nThe text that I tested is \"Das Auto muss gleich gelb sein\"\nAnd the first test result\nsecond test result\nAs you can see the dep and tag from \"gelb\" and \"muss\" are different.\nAnd when I change the word \"gelb\" to \"blau\" on the second test\n\nThe tag will change from VVPP into an ADJD.\nI would like to know if it is a bug by the language model or something else, and what should I do to get the same result from older version in the newer version from Spacy? Because I have a function that are using the dependecy and POS-Tagging as parameter from the older version (first test)\nThank you", "issue_status": "Closed", "issue_reporting_time": "2019-08-29T14:45:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "379": {"issue_url": "https://github.com/explosion/spaCy/issues/4209", "issue_id": "#4209", "issue_summary": "Improper dependencies in compound sentences when using proper nouns", "issue_description": "rrrepos commented on 29 Aug 2019 \u2022\nedited\nThis is to demonstrate different dependencies when Proper Nouns are the Subject of the individual clauses of a compound sentence.\ne.g He was born in New York and She lived in London\nToken    Pos Tagger    Dependency Relation    Parent Token      Parent Id\n-------  ------------  ---------------------  --------------  -----------\nHe       PRON          nsubjpass              born                      2\nwas      VERB          auxpass                born                      2\nborn     VERB          ROOT                   born                      2\nin       ADP           prep                   born                      2\nNew      PROPN         compound               York                      5\nYork     PROPN         pobj                   in                        3\nand      CCONJ         cc                     born                      2\nShe      PRON          nsubj                  lived                     8\nlived    VERB          conj                   born                      2\nin       ADP           prep                   lived                     8\nLondon   PROPN         pobj                   in                        9\nNotice that the parent of \"She\" is the parent id 8 which is the correct viz. \"lived\"\nHowever, change the sentence to: Tom was born in New York and Alice lived in London gives:\nToken    Pos Tagger    Dependency Relation    Parent Token      Parent Id\n-------  ------------  ---------------------  --------------  -----------\nTom      PROPN         nsubjpass              born                      2\nwas      VERB          auxpass                born                      2\nborn     VERB          ROOT                   born                      2\nin       ADP           prep                   born                      2\nNew      PROPN         compound               York                      5\nYork     PROPN         pobj                   in                        3\nand      CCONJ         cc                     York                      5\nAlice    PROPN         conj                   York                      5\nlived    VERB          advcl                  born                      2\nin       ADP           prep                   lived                     8\nLondon   PROPN         pobj                   in                        9\nNow, the parent of Alice points to id 5 which is \"York\" instead of 8 which it should be. I found the same with the online Stanford Parser too. Any reason for this behaviour and is there a way out of this? Thanks\nSpacy Version is: 2.1.8 and running on on a MacOS: 10.14.6", "issue_status": "Closed", "issue_reporting_time": "2019-08-29T07:52:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "380": {"issue_url": "https://github.com/explosion/spaCy/issues/4206", "issue_id": "#4206", "issue_summary": "Conjugation of verbs", "issue_description": "rrrepos commented on 28 Aug 2019\nFeature description\nIs there a plan to add conjugation (tense, 1st or 3rd, singular or plural) to the Spacy library?", "issue_status": "Closed", "issue_reporting_time": "2019-08-28T08:47:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "381": {"issue_url": "https://github.com/explosion/spaCy/issues/4204", "issue_id": "#4204", "issue_summary": "Training Custom NER model with Pretraining using ELMO / BERT", "issue_description": "keshav787 commented on 28 Aug 2019\nSince Spacy v2.1 supports BERT/ELMO pre-training . Is it possible somehow to use ELMO while training a custom NER model? If yes then how? If No then can we have this feature in the releases to come.", "issue_status": "Closed", "issue_reporting_time": "2019-08-28T04:37:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "382": {"issue_url": "https://github.com/explosion/spaCy/issues/4203", "issue_id": "#4203", "issue_summary": "Issue with custom SBD", "issue_description": "gsoul commented on 28 Aug 2019\nHey everyone.\nFirst of all I'd like to thank Spacy creators for such an amazing tool. Unfortunately either I'm doing something wrong or I found a bug.\nI've got the following text:\nMy dark eyes had the look of held back tears. \"It\u2019s okay,\" I said\nAnd what I'm trying to achieve is so that the split for the sentence would happen after '.' and not after '\"'. Here's the code I use to try solve this issue, which is based on this manual page (https://spacy.io/usage/linguistic-features#sbd-custom) and some Googling:\nimport spacy\n\n\ndef custom_sentencizer(doc):\n    for t in doc:\n        if (t.text == '\"') and (not t.whitespace_) and (t.nbor(-1).whitespace_): # and t.nbor(1).is_sent_start:\n            doc[t.i].is_sent_start = True\n            doc[t.nbor(1).i].is_sent_start = False\n\n    return doc\n\n\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger'])\nnlp.max_length = 1500000\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\nnlp.add_pipe(custom_sentencizer, after=\"sentencizer\")\n\n\n\ninput = \"My dark eyes had the look of held back tears. \\\"It\u2019s okay,\\\" I said\"\ndoc = nlp(input)\nfor sentence in doc.sents:\n    print(sentence.text)\nAny help would be greatly appreciated.\nAbout my environment:\nUbuntu 18.04\nPython 3.7\nspacy==2.1.1", "issue_status": "Closed", "issue_reporting_time": "2019-08-27T20:40:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "383": {"issue_url": "https://github.com/explosion/spaCy/issues/4201", "issue_id": "#4201", "issue_summary": "Split dev-dependencies into requirements-dev.txt", "issue_description": "Froskekongen commented on 27 Aug 2019 \u2022\nedited\nspaCy should split its development requirements into a separate file (e.g. requirements-dev.txt). There is no reason for keeping dependency and build requirements in the main requirements.txt file, and only makes it harder to integrate spaCy in complex projects.\nYour Environment\nThis suggestion is not environment dependent.", "issue_status": "Closed", "issue_reporting_time": "2019-08-27T09:46:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "384": {"issue_url": "https://github.com/explosion/spaCy/issues/4200", "issue_id": "#4200", "issue_summary": "text color dont working after spacy in jupyter notebook", "issue_description": "phayllia commented on 27 Aug 2019 \u2022\nedited\nHow to reproduce the behaviour\nIn a new jupyter notebook, print '\\033[34mtest\\033[0m' produces the text in blue as expected.\nimport spacy\nprint '\\033[34mtest\\033[0m'\nAfter importing, the text color print doesn't work anymore\nYour Environment\nOperating System: Windows\nPython Version Used: 2.7\nspaCy Version Used: 2.0.9\nEnvironment Information:\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-26T22:08:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "385": {"issue_url": "https://github.com/explosion/spaCy/issues/4199", "issue_id": "#4199", "issue_summary": "Loading large model affects predictions of medium model", "issue_description": "Contributor\ndanielkingai2 commented on 27 Aug 2019\nHow to reproduce the behaviour\nIt seems that simply loading the large model into the same environment as the medium model changes the predictions of the medium model. At a minimum, it changes the results of the tagger. Simple repro below\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \nType 'copyright', 'credits' or 'license' for more information\nIPython 7.7.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import spacy                                                                                                                                                                                                                       \n\nIn [2]: nlp_md = spacy.load('en_core_web_md')                                                                                                                                                                                              \n\nIn [3]: list(nlp_md(\"TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning\").noun_chunks)                                                                                                               \nOut[3]: \n[TreeQN,\n ATreeC,\n Differentiable Tree-Structured Models,\n Deep Reinforcement Learning]\n\nIn [4]: nlp_lg = spacy.load('en_core_web_lg')                                                                                                                                                                                              \n\nIn [5]: list(nlp_md(\"TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning\").noun_chunks)                                                                                                               \nOut[5]: [ATreeC, Differentiable Tree-Structured Models, Deep Reinforcement Learning]\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.3\nPlatform: Linux-4.4.0-139-generic-x86_64-with-debian-stretch-sid\nPython version: 3.6.9", "issue_status": "Closed", "issue_reporting_time": "2019-08-26T20:19:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "386": {"issue_url": "https://github.com/explosion/spaCy/issues/4198", "issue_id": "#4198", "issue_summary": "Identifying entities that are connected to other words via special characters", "issue_description": "erotavlas commented on 27 Aug 2019 \u2022\nedited\nI've noticed in my training data that the text hasn't been exactly entered correctly by whatever computer system or user has entered it. So say I'm dealing with medical reports and I've chosen to identify date as a named entity.\nI'll have the majority of cases like this which are perfectly fine and the date is treated as individual tokens from their heading. (separated by whitespace)\nRUN DATE: 11/21/11\nDOB: 03/07/56\nBut then I'll get cases like this which are treated as one whole token because there is no whitespace between the entity and preceding token)\nREPORTED:02/28/11\nAlthough there is a colon character at the end of the preceding token, it is being treated as one whole token during training. Thus say I annotate all my dates correctly, the model learns that that whole chunk of text is a date because I'm guessing it gets fed into the algorithm as a single token. Similarly when I evaluate any text which has a similar structure, the model picks up the entire set of tokens as a single unit.\nI've found other instances of this happening with other entity types. Although low occurrence, it is problematic if we are trying to achieve best results and identify as many as we can.\nHow do you recommend dealing with this issue?\n(For now I have avoided to annotate anything like this for fear it might be learning the wrong patterns)\nEDIT:\nI just noticed that the model was correctly identifying person names that were connected to a preceding token followed by a colon., For example\nSURGEON:John,Smith\nCorrectly identified as John Smith without including the preceding token.\nSo I have a feeling this issue might apply only to entities with numeric? (So far I noticed it happens with date and specimen id)", "issue_status": "Closed", "issue_reporting_time": "2019-08-26T18:32:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "387": {"issue_url": "https://github.com/explosion/spaCy/issues/4197", "issue_id": "#4197", "issue_summary": "Ways to improve speed for textcat training on CPU", "issue_description": "H20Watermelon commented on 26 Aug 2019\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.8\nspaCy Version Used: 2.1.8\nAdditional Info: 256 Gb RAM; CPU: Intel Xeon E5-2687W v4 3.00GHz\nI have a data set consisting of over 1.5 million sentences or passages. The average length of these sentences/passages is around 100 tokens. I am trying to use the Text Categorizer pipeline to train a model for 6 classes that are not mutually exclusive.\nI used the following code, which largely follows the sample code in the documentation:\nnlp = spacy.blank(\"en\")\ntextcat = nlp.create_pipe(\"textcat\", = { \"exclusive_classes\": False, \"architecture\": \"ensemble\", \"ngram_size\": 2}\n\nnlp.add_pipe(textcat, last=True)\nfor label in labels:\n    textcat.add_label(label)\n\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\nwith nlp.disable_pipes(*other_pipes):\n    optimizer = nlp.begin_training()\n    #optimizer = nlp.resume_training()\n    #if init_tok2vec is not None:\n    print(\"Training the model...\")\n    print(\"{}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LABEL\", \"LOSS\", \"P\", \"R\", \"F\"))\n    batch_sizes = compounding(4.0, 32.0, 1.001)\n    for i in range(n_iter):\n        losses = {}\n        random.shuffle(bias_train_data)\n        batches = minibatch(bias_train_data, size = batch_sizes)\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.3, losses = losses)\nAfter 5 hours, it did not finish a single iteration. I wonder if there is a way to increase the training speed (perhaps by increasing the batch size? or some other ways.)\nI cannot use GPU for training because I have not been able to successfully compile thinc_gpu_ops. I tried all the suggested solutions in the Issues section to no avail.\nAny suggestions would be greatly appreciated. Thanks!", "issue_status": "Closed", "issue_reporting_time": "2019-08-26T17:29:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "388": {"issue_url": "https://github.com/explosion/spaCy/issues/4196", "issue_id": "#4196", "issue_summary": "Getting Memory Error", "issue_description": "AbhayGodbole commented on 26 Aug 2019\nHi\nWhen I am trying to build a model to extract my custom entities, I am getting memory error.\nThere are 9000 comments data that I have from which I need to extract custom entities\nsteps:\ncreated tagged tsv file contents around 125000 words\nconverted tsv to josn\nconverted json to spacy required format.\nexecuted the function for creating new model using the file in step 4. While executing getting memory error. PFA the error SS and the .py file\nmemoryError.zip\nYour Environment\n========== Info about spaCy ======\nspaCy version 2.1.8\nPlatform Windows-10-10.0.16299-SP0\nPython version 3.7.1\nModels en\nMemory: 8GB\nPlease help", "issue_status": "Closed", "issue_reporting_time": "2019-08-26T06:57:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "389": {"issue_url": "https://github.com/explosion/spaCy/issues/4195", "issue_id": "#4195", "issue_summary": "Extension & User Data persist across multiple Language() calls.", "issue_description": "davidbernat commented on 26 Aug 2019 \u2022\nedited\nHow to reproduce the behaviour\nI use a custom class pipeline to create extensions (\"noun_chunks\", \"time_chunks\", etc..). These are created on the Doc object using the static class method.\nCalling nlp(text) works as expected.\nCalling nlp(more_text) will show create a new doc using nlp.make_doc() that already has doc._.noun_chunks populated (and verb chunks) and user_data populated as well, with data derived from nlp(text).\nThe unexpected behavior is persistence across nlp() calls.\nHere is a minimum reproduction. It appears to be due to an incorrect and/or illegal accessing of the doc._._extensions[\"noun_chunk\"][0] values by key, rather than by doc._._noun_chunk explicitly. This access by key functionality is what I desire, however. Is this possible?\nimport spacy\nname = \"en_core_web_sm\"\n\ndef add_sentence_lengths(doc):\n    if not doc.has_extension(\"lengths\"):\n        doc.set_extension(\"lengths\", default=[], force=True)\n    print(doc._._extensions)\n    for s in doc.sents:\n        doc._._extensions[\"lengths\"][0].append(len(s))\n    print(doc._._extensions, doc._.lengths, doc.get_extension(\"lengths\")\n          )\n    return doc\n\n\nnlp = spacy.load(name)\nnlp.add_pipe(add_sentence_lengths, name=\"my_pipe\")\n\nd = nlp(\"This sentence here. And here.\")\nd = nlp(\"This sentence is more.\")\nThis results in the following output, which is \"incorrect.\"\n{'lengths': ([], None, None, None)}\n{'lengths': ([4, 3], None, None, None)} [4, 3] ([4, 3], None, None, None)\n{'lengths': ([4, 3], None, None, None)}\n{'lengths': ([4, 3, 5], None, None, None)} [4, 3, 5] ([4, 3, 5], None, None, None)\nYour Environment\nOperating System: MacOS 10.14.6\nPython Version Used: 3.6\nspaCy Version Used: 2.1.8\nEnvironment Information: IntelliJ", "issue_status": "Closed", "issue_reporting_time": "2019-08-25T22:47:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "390": {"issue_url": "https://github.com/explosion/spaCy/issues/4194", "issue_id": "#4194", "issue_summary": "How to use PreTrained BERT Model for Textclassification?", "issue_description": "snape6666 commented on 25 Aug 2019\nHi,\ni saw the new release of spacy v2.1 and the new bert model for german. Iam new in this area, but would like to get started with the combination of BERT and spacy.\nI would like to classify German free text requirements. Since we don't have a lot of training data available (about 100-200 sentences per class with about 6 requirement classes), I became interested in BERT, which is why I would like to test it for this use case.\nHow large must the training data be for us to achieve meaningful results with the pre-trained model? Are there any examples of using the new models?\nThank you in advance!\nYour Environment\nnewest spacy version (today 2019.08.25)\nWindows 10\nAnaconda Python 3.7", "issue_status": "Closed", "issue_reporting_time": "2019-08-25T15:09:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "391": {"issue_url": "https://github.com/explosion/spaCy/issues/4193", "issue_id": "#4193", "issue_summary": "Processing a text causes spaCy to hang", "issue_description": "dhwani2410 commented on 25 Aug 2019\nHow to reproduce the behaviour\nnlp = spacy.load(\"en_core_web_sm\")\ndoc_temp = data_dict[pmid]\ndoc1=(re.sub('[^A-Za-z0-9 *-,]+', '', doc_temp))\ndoc = nlp(str(doc1))\nThis doesn't work. Also, there are many other cases where this does not work\ndoc1\n'A placebo controlled observer blind immunocytochemical and histologic study of epithelium adjacent to anogenital warts in patients treated with systemic interferon alpha in combination with cryotherapy or cryotherapy alone OBJECTIVETo examine biopsy specimens of tissue immediately adjacent to anogenital AG warts which had been treated with either cryotherapy plus subcutaneous interferon IFN alpha 2a or cryotherapy alone, for histological features of a human papilloma virus HPV infection b localised cellular immune responses, to further characterise any cellular immune infiltrates with tissue immunocytochemistry, and to relate any histological, immunocytochemical findings to the treatment response of nearby AG wartsDESIGNA randomised placebo controlled observer blind studySETTINGGenitourinary Medicine clinic, Department of Immunopathology, Royal Victoria Hospital, Belfast, N IrelandSUBJECTSThirty patients with AG warts 16 treated with IFN alpha 2a plus cryotherapy, and 14 treated with cryotherapy aloneOUTCOME MEASURES1 Light microscopic features associated with HPV infection and local cellular immune responses 2 Indirect immunofluorescence detection of the following cell surface markers HLA DR, alpha one antitrypsin, CD1, CD3, CD4, CD8, CD22 3 Clinical response of AG warts to treatmentRESULTSIn pretreatment biopsies only non specific indicators of HPV infection acanthosis, 2930 biopsies, and hyperkeratosis, 730 biopsies were seen on light microscopy Mononuclear cells were seen both throughout the upper dermis and centred around dermal blood vessels in 1930 633 biopsies, and infiltrating into the epidermis in 1230 40 biopsies On indirect immunofluorescence CD3, CD8, CD4 antigen was detected on the surface of cells throughout the upper dermis in 2429 827, 1529 517, and 329 103, of biopsy specimens respectively CD3 antigen, CD8 antigen and CD4 antigen was detected on the surface of cells infiltrating into the epidermis in 1829 62, 729 241, and 629 207 of biopsy specimens respectively CD1 antigen was seen on the surface of dendritic cells throughout the epidermis in all specimens CD1 positive cells infiltrated into the upper dermis in 529 172 HLA DR was detected on the surface of dendritic cells throughout the epidermis in 2229 759 of specimens, and on the surface of cells scattered both diffusely throughout the upper dermis and centred around dermal blood vessels in all specimens Alpha one antitrypsin A1AT antigen was seen on the surface of cells in the upper dermis in 629 207 of biopsy specimens no cells expressing CD22 surface antigen were seen The nature of this local cellular immune response was not altered by treatment of nearby warts with either cryotherapy alone or cryotherapy plus systemic IFN alpha 2a, or related to the therapeutic outcome of these wartsCONCLUSIONS1 No convincing histological evidence of HPV infection was seen in epithelium surrounding AG warts 2 A predominantly T cellmediated immune response the target of which is uncertain was seen in this perilesional epithelium 3 In the dosage regimens used in this study, treatment of AG warts with either systemic IFN alpha 2a plus cryotherapy or cryotherapy alone did not appear to augment localised cellular immune responses against any presumed subclinical HPV infection in epithelium surrounding AG warts'\nInfo about spaCy\nspaCy version: 2.1.0a13\nPlatform: Linux-2.6.32-279.el6.x86_64-x86_64-with-redhat-6.7-Santiago\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-25T12:20:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "392": {"issue_url": "https://github.com/explosion/spaCy/issues/4190", "issue_id": "#4190", "issue_summary": "Tokenizer does not properly serialize to disk", "issue_description": "mikerossgithub commented on 24 Aug 2019\nHow to reproduce the behaviour\nI am using spacy's default Tokenizer, with a slightly modified set of exceptions (no exceptions for single letters with periods). The customized Language properly tokenizes. But after saving and reloading from disk, the tokenizer is no longer customized:\nCode Output:\nOriginal Tokenizer:\n[Test, c.]\nCustomized Tokenizer:\n[Test, c, .]\nSaved and reloaded Tokenizer, should be the same as customized:\n[Test, c.]\nCode to reproduce:\nimport spacy\nfrom spacy.tokenizer import Tokenizer\n\ndef customize_tokenizer(nlp):\n    prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n    suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)\n    infix_re = spacy.util.compile_infix_regex(nlp.Defaults.infixes)\n\n    # remove all exceptions where a single letter is followed by a period (e.g. 'h.')\n    exceptions = {k: v for k,v in dict(nlp.Defaults.tokenizer_exceptions).items() if not (len(k) == 2 and k[1] == '.')}\n    new_tokenizer = Tokenizer(nlp.vocab, exceptions,\n                              prefix_search=prefix_re.search,\n                              suffix_search=suffix_re.search,\n                              infix_finditer=infix_re.finditer,\n                              token_match=nlp.tokenizer.token_match)\n\n    nlp.tokenizer = new_tokenizer\n\n# Load default Language\nnlp = spacy.load('en_core_web_sm')\nprint(\"Original Tokenizer:\")\nprint(list(nlp(\"Test c.\")))\n\n# Modify Tokenizer\ncustomize_tokenizer(nlp)\nprint(\"Customized Tokenizer:\")\nprint(list(nlp(\"Test c.\")))\n\n# Save and Reload\nnlp.to_disk('x')\nnlp = spacy.load('x')\nprint(\"Saved and reloaded Tokenizer, should be the same as customized:\")\nprint(list(nlp(\"Test c.\")))\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Linux-4.15.0-58-generic-x86_64-with-Ubuntu-16.04-xenial\nPython version: 3.6.8\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-23T22:41:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "393": {"issue_url": "https://github.com/explosion/spaCy/issues/4185", "issue_id": "#4185", "issue_summary": "RuntimeError Error: Numpy 1.15.0 requires Python version >= 3.5.", "issue_description": "PoojaPradeep commented on 23 Aug 2019\nHow to reproduce the problem\n# copy-paste the error message here\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-08-23T10:45:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "394": {"issue_url": "https://github.com/explosion/spaCy/issues/4184", "issue_id": "#4184", "issue_summary": "File \"/tmp/pip-build-gyfVq7/numpy/setup.py\", line 31, in <module> raise RuntimeError(\"Python version >= 3.5 required.\")", "issue_description": "PoojaPradeep commented on 23 Aug 2019\nHow to reproduce the problem\n# copy-paste the error message here\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-08-23T10:45:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "395": {"issue_url": "https://github.com/explosion/spaCy/issues/4183", "issue_id": "#4183", "issue_summary": "File \"/tmp/pip-build-gyfVq7/numpy/setup.py\", line 31, in <module> raise RuntimeError(\"Python version >= 3.5 required.\")", "issue_description": "PoojaPradeep commented on 23 Aug 2019\nHow to reproduce the problem\n# copy-paste the error message here\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-08-23T10:45:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "396": {"issue_url": "https://github.com/explosion/spaCy/issues/4182", "issue_id": "#4182", "issue_summary": "Custom NER Model performs poorly after updating from v2.0.18 to v2.1.8", "issue_description": "chunyat commented on 23 Aug 2019 \u2022\nedited\nHow to reproduce the behaviour\nI have been using spaCy v2.0.18 (which was downloaded as a package when I installed Fastai) to train a custom NER model to detect a set of custom entity types for my own purposes.\nI was able to get really good results with v2.0.18:\nnlp.entity_tagger|train_entity_tagger|INFO| Training model...\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.15672576857688614}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.12927774898755265}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.10086585149922489}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.11106154029696236}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.08540124149758303}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.0679294809260213}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.08455457622118247}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.11117098736776775}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.09019838208963762}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 0.08187232735047828}\n(This evaluation script was written myself, making use of the seqeval package for sequence evaluation tasks)\nnlp.spacy_tools|evaluate_spacy|INFO|\n                  precision    recall  f1-score   support\n\n        PRODUCT    0.95686   0.93487   0.94574       783\nATTRIBUTE_VALUE    0.97315   0.99315   0.98305       146\n       HOSPITAL    0.96644   0.98630   0.97627       146\n  ATTRIBUTE_KEY    0.88889   0.88889   0.88889        18\n\n      micro avg    0.95930   0.94876   0.95400      1093\n      macro avg    0.95920   0.94876   0.95386      1093\nThis result was achieved using the \"en_core_web_md\" model as the base that was then trained on a dataset containing only the custom entity types that I wish to detect. Recently, I have been trying to add in the EntityRuler pipeline to augment the custom NER model. Since it is only available from v2.1 onward, I tried upgrading to the latest version (v2.1.8) and updated all 3 pre-trained \"en_core_web\" models to the corresponding v2.1.0. I was able to run the training again on a fresh \"en_core_web_md\" model, but the results became significantly poorer:\nnlp.entity_tagger|train_entity_tagger|INFO| Training model...\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 38476.7237200737}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 30793.436396598816}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 29513.01201248169}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 28433.613090991974}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 28227.99483013153}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 27747.527606010437}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 27771.533205986023}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 27399.654321193695}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 27462.22469186783}\nnlp.entity_tagger|train_entity_tagger|INFO| Losses {'ner': 27126.22325849533}\n\nnlp.spacy_tools|evaluate_spacy|INFO|\n                  precision    recall  f1-score   support\n\n        PRODUCT    0.64179   0.65900   0.65028       783\n  ATTRIBUTE_KEY    0.00000   0.00000   0.00000        18\n       HOSPITAL    0.00000   0.00000   0.00000       146\nATTRIBUTE_VALUE    0.16667   0.02055   0.03659       146\n\n      micro avg    0.63139   0.47484   0.54204      1093\n      macro avg    0.48203   0.47484   0.47074      1093\nThe model seemed to become entirely incapable of picking out some of the custom entities even though the only difference was the spaCy version and the pre-trained model's version. Does anyone have a clue as to what may be wrong or if I have missed something out?\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-4.4.0-18362-Microsoft-x86_64-with-debian-stretch-sid\nPython version: 3.7.3\nModels: en_core_web_lg, en_core_web_md, en_core_web_sm", "issue_status": "Closed", "issue_reporting_time": "2019-08-23T10:29:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "397": {"issue_url": "https://github.com/explosion/spaCy/issues/4181", "issue_id": "#4181", "issue_summary": "Spacy NER Evaluation is being incorrect", "issue_description": "mmanishh commented on 23 Aug 2019 \u2022\nedited\nHere is my test data and NER parsed data from Spacy NER. I have been training with my own NER for address parsing. Since the test data and parsed data are 100% accurate why is the score i.. Precision , Recall and F-score only 60 ?\nTest Data:\n[('ACEH', [(0, 4, 'province')]), ('SUMATERA UTARA', [(0, 14, 'province')]), ('SUMATERA BARAT', [(0, 14, 'province')]), ('RIAU', [(0, 4, 'province')]), ('JAMBI', [(0, 5, 'province')])]\nParsed Data\n[{'province': (0, 4)}, {'province': (0, 14)}, {'province': (0, 14)}, {'province': (0, 4)}, {'province': (0, 5)}]\nEvaluation Result:\n{'uas': 0.0, 'las': 0.0, 'ents_p': 60.0, 'ents_r': 60.0, 'ents_f': 60.0, 'tags_acc': 0.0, 'token_acc': 100.0}\nIs there any other way to evaluate Spacy NER model?\nInfo about spaCy\nspaCy version: 2.1.4\nPlatform: Linux-5.0.0-25-generic-x86_64-with-debian-buster-sid\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-23T09:29:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "398": {"issue_url": "https://github.com/explosion/spaCy/issues/4180", "issue_id": "#4180", "issue_summary": "Dataflow ValueError: spacy.syntax.nn_parser.Parser size changed, may indicate binary incompatibility. Expected 72 from C header, got 64 from PyObject [while running 'ParDo(CollectQuestions)']", "issue_description": "yasminalounis commented on 22 Aug 2019\nHello, I'm using SpaCy to do some preprocessing in a dataflow pipeline (based on apache beam),\nI installed SpaCy using the beam setup.py as a specific requirement,\nWhen I launch the pipeline I got on \"spacy.load('en')\"\nValueError: spacy.syntax.nn_parser.Parser size changed, may indicate binary incompatibility. Expected 72 from C header, got 64 from PyObject [while running 'ParDo(CollectQuestions)']\nIs it a version issue, or just the dataflow environment?\nI have already downgraded SpaCy, but no result, I have also tried to install with --no-binary option\nPython Version Used:\n3.7\nspaCy Version Used:\n2.1.8\nEnvironment Information:\nGCP", "issue_status": "Closed", "issue_reporting_time": "2019-08-22T13:11:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "399": {"issue_url": "https://github.com/explosion/spaCy/issues/4177", "issue_id": "#4177", "issue_summary": "if I give a text with html tags Spacy NER will ignore html tags and recognize named entity between raw text only.", "issue_description": "Anticsss commented on 22 Aug 2019\nFeature description\nSometimes it is difficult to retain the original structure of text from which we recognize named entities. For example I have text between standard html tags(ex. web page source) and while recognizing named entity from text I remove all the html tags and give raw text to Spacy NER engine but after prediction if I want to display text in original format with surrounded html tags it is not possible. it will break the originality of data.\nif there is feature in spacy that can ignore specific pattern while recognizing named entities from text it will helpful to retain the originality of text.\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.\nIt will be a custom component so other users can specify the pattern which they want to ignore.", "issue_status": "Closed", "issue_reporting_time": "2019-08-22T11:24:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "400": {"issue_url": "https://github.com/explosion/spaCy/issues/4176", "issue_id": "#4176", "issue_summary": "Spacy 2.1 is not as effective as 2.0 under the same conditions for multi-classification problems.", "issue_description": "jameszlj commented on 22 Aug 2019 \u2022\nedited\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:\nspacy 2.0.18\ntext = \"\"\"Start traveling like a local! Book Trip Guru with Mastercard and get 20% off all tour experiences and adventures in many South East Asian destinations.\"\"\"\ndoc = nlp(text)\ntaxonomys_list = OrderedDict(doc.cats)\ntaxonomys_list = OrderedDict(sorted(taxonomys_list.items(), key=lambda t: t[1], reverse=True))\nprint(taxonomys_list)\nOrderedDict([('Services', 0.991990864276886), ('Travel Services', 0.9888597726821899), ('Holiday', 0.9870848059654236), ('Booking services', 0.9758071303367615), ('Travel', 0.9735677242279053), ('Vacations', 0.9703308343887329), ('Adventure Activities', 0.9638486504554749), ('Entertainment', 0.9459176659584045), ('Holidays', 0.8224777579307556), ('Vacation', 0.7244226932525635), ('Booking Service', 0.08504372835159302), ('Booking', 0.06343142688274384), ('Attractions', 0.046378690749406815), ('Car Rentals', 0.03734173625707626), ('Experiences', 0.03500397875905037), ('Family Vacations', 0.033907972276210785), ('Tickets', 0.02211538888514042), ('Hotels', 0.02100328728556633), ('Movies', 0.019198477268218994), ('Vacation Rentals', 0.018203062936663628), ('Personal Care', 0.0152616947889328), ('Weekend Getaways', 0.015231739729642868), ('Museums', 0.013910885900259018), ('Jewish', 0.01270236074924469), ('Arts and Crafts', 0.011866671964526176), ('Sports and Fitness', 0.010706927627325058), ('Teas', 0.009054446592926979), ('Meats', 0.009014514274895191), ('Restaurants', 0.008879509754478931), ('Champagne', 0.008256825618445873), ('Buffets', 0.00780327757820487), ('Desserts', 0.007737184874713421), ('Cooking', 0.007576660718768835), ('Sports Fan', 0.005898750387132168), ('Cuisine', 0.005667830351740122), ('Dining', 0.00557311624288559), ('Health Care', 0.005544014275074005), ('Barbecue', 0.005424442235380411), ('Food & Beverage', 0.005325845908373594), ('Golf', 0.004812314175069332), ('Healthcare', 0.0047915480099618435), ('Beauty', 0.0039006059523671865), ('Luxury', 0.00378055521287024), ('Spas', 0.0037371967919170856), ('Games', 0.003526924178004265), ('Airports', 0.0034586999099701643), ('Resorts', 0.003405642230063677), ('Bakeries', 0.0033209563698619604), ('Seafood', 0.003297787392511964), ('Travel Insurance', 0.0026215368416160345), ('Exhibitions', 0.0022796255070716143), ('Islands', 0.002163636265322566), ('Souvenir', 0.0021531537640839815), ('Shopping', 0.002134014619514346), ('Muslim', 0.0018916900735348463), ('Cruises', 0.0018594496650621295), ('Wineries', 0.0017661957535892725), ('Fruit', 0.0016967753181234002), ('Michelin', 0.0016406797803938389), ('Celebrity Chef', 0.001576813287101686), ('Parks', 0.0015764206182211637), ('Gambling', 0.0015523636247962713), ('Accessories', 0.0015489469515159726), ('Coffee', 0.0015304710250347853), ('Household', 0.001439731102436781), ('Grocery', 0.0013601784594357014), ('Theme', 0.0012071426026523113), ('Concerts and Music', 0.001160294166766107), ('Department Stores', 0.001099713845178485), ('Hindu', 0.0010434803552925587), ('Clubs/Nightclubs', 0.0009954808047041297), ('Buddhist', 0.0009882946033030748), ('Sushi', 0.0009507693466730416), ('Clothes', 0.0009361709235236049), ('Bars', 0.0008493054774589837), ('Fishing', 0.0008431627647951245), ('Market Places', 0.0007646719459444284), ('Weddings', 0.0006306252907961607), ('Honeymoons', 0.0005952748470008373), ('Fast Food', 0.0005368471029214561), ('Fashion Trends', 0.00039127955096773803), ('Breakfast', 0.0003885205078404397), ('Gold Beach', 0.00038597179809585214), ('Electronics', 0.0003595442685764283), ('Health Foods', 0.0003307793813291937), ('Nightlife', 0.0002483692951500416), ('Beverages', 0.00024224855587817729), ('Airlines', 0.00021940836450085044), ('Alcoholic Beverages', 0.00017127700266428292)])\nbut spacy 2.1.8\nOrderedDict([('Experiences', 0.2224183976650238), ('Vacations', 0.18968799710273743), ('Holiday', 0.18574681878089905), ('Vacation', 0.15565261244773865), ('Holidays', 0.1445891559123993), ('Food & Beverage', 0.06648927927017212), ('Travel', 0.03541579842567444), ('Services', 0.0), ('Fashion Trends', 0.0), ('Entertainment', 0.0), ('Luxury', 0.0), ('Hotels', 0.0), ('Resorts', 0.0), ('Family Vacations', 0.0), ('Islands', 0.0), ('Honeymoons', 0.0), ('Weekend Getaways', 0.0), ('Personal Care', 0.0), ('Spas', 0.0), ('Sports and Fitness', 0.0), ('Accessories', 0.0), ('Adventure Activities', 0.0), ('Airlines', 0.0), ('Airports', 0.0), ('Arts and Crafts', 0.0), ('Attractions', 0.0), ('Beauty', 0.0), ('Booking services', 0.0), ('Car Rentals', 0.0), ('Clothes', 0.0), ('Clubs/Nightclubs', 0.0), ('Concerts and Music', 0.0), ('Cruises', 0.0), ('Department Stores', 0.0), ('Electronics', 0.0), ('Exhibitions', 0.0), ('Fishing', 0.0), ('Gambling', 0.0), ('Games', 0.0), ('Gold Beach', 0.0), ('Golf', 0.0), ('Grocery', 0.0), ('Health Care', 0.0), ('Household', 0.0), ('Market Places', 0.0), ('Movies', 0.0), ('Museums', 0.0), ('Nightlife', 0.0), ('Parks', 0.0), ('Shopping', 0.0), ('Souvenir', 0.0), ('Sports Fan', 0.0), ('Theme', 0.0), ('Tickets', 0.0), ('Travel Insurance', 0.0), ('Travel Services', 0.0), ('Vacation Rentals', 0.0), ('Weddings', 0.0), ('Michelin', 0.0), ('Celebrity Chef', 0.0), ('Restaurants', 0.0), ('Cuisine', 0.0), ('Cooking', 0.0), ('Dining', 0.0), ('Beverages', 0.0), ('Alcoholic Beverages', 0.0), ('Wineries', 0.0), ('Champagne', 0.0), ('Bars', 0.0), ('Teas', 0.0), ('Coffee', 0.0), ('Bakeries', 0.0), ('Desserts', 0.0), ('Barbecue', 0.0), ('Breakfast', 0.0), ('Buffets', 0.0), ('Fast Food', 0.0), ('Fruit', 0.0), ('Health Foods', 0.0), ('Meats', 0.0), ('Seafood', 0.0), ('Sushi', 0.0), ('Hindu', 0.0), ('Muslim', 0.0), ('Buddhist', 0.0), ('Jewish', 0.0), ('Booking', 0.0), ('Booking Service', 0.0), ('Healthcare', 0.0)])\ncode\n`def auto_training(model=\"en_core_web_lg\", output_dir=None, split=0.8, n_iter=20, init_tok2vec=None, get_data=get_data_from_database):\nnlp = spacy.load(model)\nif \"textcat\" not in nlp.pipe_names:\n    # architecture in [\"ensemble\", \"simple_cnn\", \"bow\"]\n    textcat = nlp.create_pipe(\n        \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"})\n    nlp.add_pipe(textcat, last=True)\nelse:\n    textcat = nlp.get_pipe(\"textcat\")\n\ntexts, annotations, labels = get_data()\n\n# add label to text classifier\nfor label in labels:\n    textcat.add_label(label)\n\nlogger.info(\"Loading training data...\")\nn_texts = int(len(texts) * split)\ntrain_texts = texts[:n_texts]\ntrain_cats = annotations[:n_texts]\ndev_texts = texts[n_texts:]\ndev_cats = annotations[n_texts:]\nlogger.info(\"Using {} examples ({} training, {} evaluation)\".format(len(texts), len(train_texts), len(dev_texts)))\n\ntrain_data = list(zip(train_texts, train_cats))\n\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\nwith nlp.disable_pipes(*other_pipes):  # only train textcat\n    optimizer = nlp.begin_training()\n    if init_tok2vec is not None:\n        with init_tok2vec.open(\"rb\") as file_:\n            textcat.model.tok2vec.from_bytes(file_.read())\n    logger.info(\"Training the model...\")\n    logger.info(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n    batch_sizes = compounding(4.0, 32.0, 1.001)\n    for _ in range(n_iter):\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        random.shuffle(train_data)\n        batches = minibatch(train_data, size=batch_sizes)\n        # batches_len = len(batches)\n        batch_p = 0\n        for batch in batches:\n            batch_p += 1\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n        with textcat.model.use_params(optimizer.averages):\n            # evaluate on the dev data split off in load_data()\n            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n        logger.info(\"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(losses[\"textcat\"], scores[\"textcat_p\"], scores[\"textcat_r\"], scores[\"textcat_f\"]))\n\nif output_dir is not None:\n    output_dir = Path(output_dir)\n    if not output_dir.exists():\n        output_dir.mkdir()\n    with nlp.use_params(optimizer.averages):\n        nlp.to_disk(output_dir)\n    logger.info(\"Saved model to %s\" % output_dir)`", "issue_status": "Closed", "issue_reporting_time": "2019-08-22T10:09:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "401": {"issue_url": "https://github.com/explosion/spaCy/issues/4175", "issue_id": "#4175", "issue_summary": "Get Index of an Entity in a Sentence", "issue_description": "iCHAIT commented on 22 Aug 2019 \u2022\nedited\nHi,\nI want to know if there is an elegant way to get the index of an Entity with respect to a Sentence. I know I can get the index of an Entity in a string using ent.start_char and ent.end_char, but that value is with respect to the entire string.\nFor example in the following code -\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion. Apple just launched a new Credit Card.\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\nI want the Entity Apple in both the sentences to point to start and end indexes 0 and 5 respectively.\nBasically, I am trying to achieve similar to this SO Question but for Entities rather than tokens.", "issue_status": "Closed", "issue_reporting_time": "2019-08-22T09:48:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "402": {"issue_url": "https://github.com/explosion/spaCy/issues/4172", "issue_id": "#4172", "issue_summary": "Train NER with Custom Entities", "issue_description": "AbhayGodbole commented on 22 Aug 2019 \u2022\nedited\nHi\nI have one use case where I want to extract ACTION (Update, Add, Remove etc.) and there are other Entities like Hire-Date, Trem-Date, Status etc. This is to be extracted from the Comments. e.g. \"Please update hire date to 12/3/2017. Also add status as Active\". I have approx 12000 such comments. I have just started working on it. My approach is...\nI have taken 2500 random comments for training.\nI have tokenize these comments and prepare a tsv file with Word and default Tag as \"O\".\nManually annotating each word by adding the entity under Tag replacing \"O\" like\nWord Tag\nPlease O\nupdate Action\nhire O\ndate O\nto O\n12/3/2017 Hire-Date\nthen I am converting the tsv to json and json to spacy required format.\nCreating a Blank en model, adding these new labels and updating the model.\nMy doubts are:\nIs this the right approach?\nShould I use all the 12000 comments for training?\nthis Annotating the entities, have to be done Manually?\nHow should I evaluate the model? which would give me Entity wise precision, recall, accuracy etc.\nEnvironment\nSpacY Version: 2.0\nPython: 3.7\nOS: Windows 10\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-08-22T02:36:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "403": {"issue_url": "https://github.com/explosion/spaCy/issues/4170", "issue_id": "#4170", "issue_summary": "Unexpected behavior during convert .iob to json file", "issue_description": "Contributor\nF0rge1cE commented on 22 Aug 2019\nHow to reproduce the behaviour\nI try to call python -m spacy convert <path> ./ -t json -c iob with this labeled NER training corpus:\nhttps://github.com/EuropeanaNewspapers/ner-corpora/blob/master/enp_DE.onb.bio/enp_DE.onb.bio\nAfter the conversion, you can get a json file with content:\n[\n  {\n    \"id\":0,\n    \"paragraphs\":[\n      {\n        \"sentences\":[\n          {\n            \"tokens\":[\n              {\n                \"orth\":\"November\",\n                \"tag\":\"-\",\n                \"ner\":\"O\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"id\":0,\n    \"paragraphs\":[\n      {\n        \"sentences\":[\n          {\n            \"tokens\":[\n              {\n                \"orth\":\"Heute\",\n                \"tag\":\"-\",\n                \"ner\":\"O\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n......\nSuch behavior is really weired. Seems that every separate token is considered as a whole sentence and a paragraph.\nAnother issue has already mentioned here: #4111 , which will cause exception when the token contains any non-word char (\"[^\\w-]\" in the regex scope).\nYour Environment\nOperating System: OSX 10.13.6\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.8\nEnvironment Information: N/A", "issue_status": "Closed", "issue_reporting_time": "2019-08-22T01:01:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "404": {"issue_url": "https://github.com/explosion/spaCy/issues/4165", "issue_id": "#4165", "issue_summary": "Token pattern schema doesn't include all token attributes", "issue_description": "Collaborator\nadrianeboyd commented on 22 Aug 2019\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load('en')\nmatcher = Matcher(nlp.vocab)\npattern = [{\"IS_SENT_START\": True}]\nmatcher.add(\"TEST\", None, pattern)\nOutput:\nValueError: [E152] The attribute IS_SENT_START is not supported for token patterns. Please use the option validate=True with Matcher, PhraseMatcher, or EntityRuler for more details.\nWhat's the best way to get a list of all the supported attributes to add to the schema?\nYour Environment\nCurrent master with #4105.", "issue_status": "Closed", "issue_reporting_time": "2019-08-21T19:07:55Z", "fixed_by": "#4210", "pull_request_summary": "Add more token attributes to token pattern schema", "pull_request_description": "Collaborator\nadrianeboyd commented on 29 Aug 2019\nDescription\nAdd token attributes with tests to token pattern schema.\nFixes #4165.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-08-29T10:02:27Z", "files_changed": [["29", "spacy/matcher/_schemas.py"], ["30", "spacy/tests/matcher/test_matcher_api.py"]]}, "405": {"issue_url": "https://github.com/explosion/spaCy/issues/4164", "issue_id": "#4164", "issue_summary": "Creating a Croatian lemmatizer - licence issue", "issue_description": "Contributor\nisaric commented on 22 Aug 2019\nHello all,\nI am looking to extend the croatian language model in spaCy with a look-up lemmatizer.\nI found a great source of lemmas (over 100 000 lemmas in more than a million forms) here : http://meta-share.ffzg.hr/repository/browse/croatian-morphological-lexicon-v50/2d429672703d11e28a985ef2e4e6c59e27b37c59b92d42a5be839f7daff7ecfb/\nI have already adaptet the lexicon into a lemmatizer and tested it out locally. It seems to work well.\nThe issue I`m having with submitting this lemmatizer to the spaCy codebase is the lexicons licence:\nThe lexicon is licensed under CC BY NC SA licence and spaCy is released under an MIT licence.\nTL;DR; Is it okay to adapt the Croatian morphological lexicon into a lemmatizer given the different licenses that the two projects use?", "issue_status": "Closed", "issue_reporting_time": "2019-08-21T18:44:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "406": {"issue_url": "https://github.com/explosion/spaCy/issues/4161", "issue_id": "#4161", "issue_summary": "NER Model results are not satisfying", "issue_description": "Farnod-glitch commented on 21 Aug 2019\nYour Environment\nOperating System: Windows/Linux\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1\nEnvironment Information: CPU: Intel Core i5-7300U with 16 GB Ram\nAm am trying to figure out, how to improve the results of my Spacy NER model. My trainings data is a text document with sentences in every line. I am using the PhraseMatcher to tokenize a this text document. I am also using your \"Training an additional entity type\" on the Spacy page.\nI don't know how to train my own Model with one specific Label (My label consists of 12 vocabularies\nI don't know what the loss function has to say and how to build in the eval function.", "issue_status": "Closed", "issue_reporting_time": "2019-08-21T12:28:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "407": {"issue_url": "https://github.com/explosion/spaCy/issues/4158", "issue_id": "#4158", "issue_summary": "How to efficiently create a span with char_span() when a regex match does not perfectly coincide with a token (or multiple tokens)", "issue_description": "H20Watermelon commented on 21 Aug 2019 \u2022\nedited\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.18\nEnvironment Information:\nI am trying to use regular expressions to find certain phrases in a set of data. Because of the way the text data is generated and extracted, oftentimes the phrases that I am trying to extract might be preceded and/or followed immediately by some symbols (with no white space in between).\nLet me use a couple of made-up examples to illustrate what I mean: Say what I want to find is the phrase home address. In the example below, the regex match perfectly coincides with the two tokens produced by spaCy tokenization. As a result, I am able to use the start and end values from re.search to create a span with char_span,\nnlp = spacy.load(\"en_core_web_lg\")\ndoc = nlp(\"Home address / Phone number\")\nprint(list(doc))\n#[Home, address, /, Phone, Number]\nstart, end = re.search(r\"\\b[Hh]ome address\\b\", doc.text).span()\nspan = doc.char_span(doc, start, end, label=\"HomeAddressLabel\")\nprint(span)\n#Home address\nIn the example below however, because the slash / immediately follows \"address\", the slash is grouped with \"address\" as a single token \"address/\". As a result, the start and end values from re.search cannot be used to create a span (at least with the default tokenization scheme):\ndoc = nlp(\"Home address/ Phone number\")\nprint(list(doc))\n#[Home, address/, Phone, Number]\nstart, end = re.search(r\"\\b[Hh]ome address\\b\", doc.text).span()\nspan = doc.char_span(doc, start, end, label=\"HomeAddressLabel\")\nprint(span)\nSo my question:\nIn my actual data, the trailing and/or leading symbols, and the number of them (e.g. \"Home address///\", \"Home address--\", could vary quite a bit, so ideally I don't want to create different regex patterns to deal with different scenarios, so I wonder if there is a more streamlined way to create a span when a regex match does not fully encompass all the corresponding tokens. For example, is there a way to, say, efficiently determine if a start value and an end value from re.search are, respectively, not at the start and the end of some tokens, and adjust the start and end values accordingly (e.g., move the start value to the left if not aligned, and move the end value to the right if not aligned).\nOr does it make sense to simply change the tokenization rules. My preference is definitely not to change the tokenization rules (e.g., split on all slashes, all hyphens, or simply split on all symbols that are not underscore.)\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-08-21T03:25:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "408": {"issue_url": "https://github.com/explosion/spaCy/issues/4157", "issue_id": "#4157", "issue_summary": "Lowercasing causes different tokenization result", "issue_description": "Halvani commented on 21 Aug 2019\nIf I lowercase a specific text, the resulting token sequence differs from the one of the original text. This happens in particular in cases where fullstops are attached to short strings. For example, \"St.\" results in \"st\", while the fullstop \".\" dissapears. The same problem occurs when a text contains a fullstop without a separating space. For example, \"similarity.In\" results in \"similarity\" and then \"in\" such that the fullstop \".\" again dissapears.\nHere is a shoprt example:\nimport spacy\ndef read_text(path):\nwith open(path, encoding=\"utf-8\") as f:\nreturn f.read()\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\np = r\"Some_Text_File.txt\"\ntext_normal = read_text(p)\ntext_lower = read_text(p).lower()\ndoc_normal = nlp(text_normal)\ndoc_lower = nlp(text_lower.lower())\n// At some point you can notice what caused the wrong tokenization...\nfor i, token in enumerate(doc_normal):\nif doc_lower[i] != doc_normal[i]:\nprint(token, doc_lower[i])", "issue_status": "Closed", "issue_reporting_time": "2019-08-20T23:38:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "409": {"issue_url": "https://github.com/explosion/spaCy/issues/4156", "issue_id": "#4156", "issue_summary": "Unexpected result matching on LEMMA with Token matcher", "issue_description": "Contributor\nmr-bjerre commented on 21 Aug 2019 \u2022\nedited\nLooking at the example here I'd expect both tests here to pass. They do not.\nfrom spacy.lang.en import English\nfrom spacy.matcher import Matcher\n\nnlp = English()\ndoc = nlp('Fourteen acquisition have been complete since the start of the financial year.')\n\nmatcher = Matcher(nlp.vocab)\nmatcher.add('IGNORED', None, [{'LOWER': {'IN': ['acquisition']}}])\nassert len(matcher(doc)) == 1  # ok\n\nmatcher = Matcher(nlp.vocab)\nmatcher.add('IGNORED', None, [{'LEMMA': {'IN': ['acquisition']}}])\nassert len(matcher(doc)) == 1  # fails\nFYI: print(doc[1].lemma_) yields acquisition.\nspaCy version: 2.1.8\nPlatform: Linux-5.0.0-25-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-20T18:47:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "410": {"issue_url": "https://github.com/explosion/spaCy/issues/4155", "issue_id": "#4155", "issue_summary": "Wrong language code for the Serbian language", "issue_description": "BLKSerene commented on 20 Aug 2019 \u2022\nedited\nHi, I'm wondering that shouldn't the language code for Serbian be sr rather than rs (according to ISO 639-1)?\nAnd since Serbian uses either the Cyrillic alphabet or the Latin alphabet, is there any need to distinguish between the two writing systems (e.g. sr-Cyrl for Serbian Cyrillic and sr-Latn for Serbian Latin according to ISO 15924) in spaCy to make things more clear (the newly-added stop words for Serbian in #4078 uses the Cyrillic alphabet)?\nSimilar issues: #2339 (Norwegian Bokm\u00e5l and Norwegian Nynorsk), #1308, Simplified Chinese and Tradition Chinese\nOperating System: Windows 10 64-bit\nPython Version Used: 3.7.4\nspaCy Version Used: 2.1.8\n2", "issue_status": "Closed", "issue_reporting_time": "2019-08-20T16:12:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "411": {"issue_url": "https://github.com/explosion/spaCy/issues/4154", "issue_id": "#4154", "issue_summary": "\ud83d\udcab Matcher issues with 'OP': '?'", "issue_description": "Member\nines commented on 20 Aug 2019\nCreating a master issue for some of the reported Matcher bugs, especially those related to 'OP': '?' (optional tokens) to keep things in one place. We believe that they're all likely related to the same root cause and should be fixed at the same time. Each reported test case is already reflected in a regression test.\n#3879: False positive matches on subsequent OPs in the Matcher patterns\nRegression test: test_issue3879.py\n#3951: Issue with several optional rule in Token Matcher\nRegression test: test_issue3951.py\n#4120: Matches without a final {OP: ?} token are not returned\nRegression test: test_issue4120.py\n2", "issue_status": "Closed", "issue_reporting_time": "2019-08-20T14:42:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "412": {"issue_url": "https://github.com/explosion/spaCy/issues/4152", "issue_id": "#4152", "issue_summary": "Matcher won't match pattern with Negate operator", "issue_description": "Contributor\nyash1994 commented on 20 Aug 2019\nHow to reproduce the behavior\nimport spacy\nfrom spacy.matcher import Matcher\nnlp = spacy.load('en_core_web_sm')\nmatcher = Matcher(nlp.vocab)\n\npattern_1 = [{\"LOWER\": \"hello\"},{\"IS_PUNCT\": True, \"OP\": \"!\"}]\npattern_2 = [{\"LOWER\": \"hello\"},{\"IS_PUNCT\": True, \"OP\": \"?\"}]\npattern_3 = [{\"LOWER\": \"hello\"},{\"IS_PUNCT\": True}]\n\nmatcher.add(\"pattern_1\", None, pattern_1)\nmatcher.add(\"pattern_2\", None, pattern_2)\nmatcher.add(\"pattern_3\", None, pattern_3)\n\ndoc = nlp(u'Hello! Hello')\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n string_id = nlp.vocab.strings[match_id]  # Get string representation\n span = doc[start:end]  # The matched span\n print(match_id, string_id, start, end, span.text)\nActual Output\n15719126060659784350 pattern_2 0 2 Hello!\n13375652922493068680 pattern_3 0 2 Hello!\n15719126060659784350 pattern_2 2 3 Hello\nExpected Ouput\n-------------------- pattern_1 2 3 Hello\n15719126060659784350 pattern_2 0 2 Hello!\n13375652922493068680 pattern_3 0 2 Hello!\n15719126060659784350 pattern_2 2 3 Hello\nYour Environment\nOperating System: Ubuntu 18.04.1 LTS\nPython Version Used: Python 3.6.7\nspaCy Version Used: 2.1.8", "issue_status": "Closed", "issue_reporting_time": "2019-08-20T11:43:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "413": {"issue_url": "https://github.com/explosion/spaCy/issues/4149", "issue_id": "#4149", "issue_summary": "Converting data for spacy train CLI", "issue_description": "kormilitzin commented on 19 Aug 2019\nWhile I'm following the examples #3813 , I'm stuck with this problem:\nValueError Traceback (most recent call last)\nin\n9 print(doc.ents)\n10 #json_docs.append(doc.to_json())\n---> 11 json_docs.append(docs_to_json(doc, id=i))\n12\n13 with open(os.path.join(path_to_data, 'train_set.json'), 'w') as fh:\ngold.pyx in spacy.gold.docs_to_json()\ndoc.pyx in sents()\nValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start.\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.gold import docs_to_json\n\n\nTRAINING_DATA = [{\"text\": \"Horses are too tall and they pretend to care about your feelings\", \"entities\": [(0, 6, 'ANIMAL')]},\n{\"text\": \"Do they bite?\", \"entities\": []},\n{\"text\": \"horses are too tall and they pretend to care about your feelings\", \"entities\": [(0, 6, 'ANIMAL')]},\n{\"text\": \"horses pretend to care about your feelings\", \"entities\": [(0, 6, 'ANIMAL')]},\n{\"text\": \"they pretend to care about your feelings, those horses\", \"entities\": [(48, 54, 'ANIMAL')]},\n{\"text\": \"horses?\", \"entities\": [(0, 6, 'ANIMAL')]}]\n\nnlp = spacy.load('en_core_web_lg')\n\njson_docs = []\nfor i, training_data_i in enumerate(TRAINING_DATA ):\n    doc = nlp.make_doc(training_data_i['text'])\n    doc[0].is_sent_start = True\n    doc.ents = [doc.char_span(s, e, label=L) for s, e, L in training_data_i[\"entities\"]]\n    print(doc.ents)\n    json_docs.append(docs_to_json(doc, id=i))\n    \nwith open(os.path.join(path_to_data, 'train_set.json'), 'w') as fh:\n    fh.write(json.dumps(json_docs))\nI'm getting the aforementioned error.\nHOWEVER (!!!), If I change the line:\njson_docs.append(docs_to_json(doc, id=i)) -> json_docs.append(doc.to_json())\nthen it works. BUT (!!!), If I use the resulted train_set.json file, the CLI spacy training fails with:\nFile \"gold.pyx\", line 130, in spacy.gold.GoldCorpus.init\nFile \"gold.pyx\", line 141, in spacy.gold.GoldCorpus.write_msgpack\nFile \"gold.pyx\", line 181, in read_tuples\nFile \"gold.pyx\", line 393, in _json_iterate\nFile \"gold.pyx\", line 307, in json_to_tuple\nKeyError: 'paragraphs'\nAny ideas how to properly convert TRAINING_DATA into a format that is compatible with spacy CLI train? Thanks.\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.7\nPlatform: Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-08-19T17:10:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "414": {"issue_url": "https://github.com/explosion/spaCy/issues/4148", "issue_id": "#4148", "issue_summary": "Token.set_extension parameter force is not documented", "issue_description": "johann-petrak commented on 19 Aug 2019\nThe function Token.set_extension has a kw parameter force which can be set to True, but the documentation here https://spacy.io/api/token\ndoes not document it. The python documentation string does include the info though.\nI am not sure why the information on the web \"API\" documentation pages differs from what is actually present in the Python code?", "issue_status": "Closed", "issue_reporting_time": "2019-08-19T17:09:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "415": {"issue_url": "https://github.com/explosion/spaCy/issues/4146", "issue_id": "#4146", "issue_summary": "Setting number of layers hyperparameter for Simple CNN", "issue_description": "miskolc commented on 19 Aug 2019 \u2022\nedited\nI'm looking into the Spacy documentation for a way to set the number of layers for the simple_cnn (or ensamble) architecture for a TextClassifier.\nI've found the depth parameter in the documentation for the pretrained models https://spacy.io/api/cli#pretrain\nIs there a similar parameter for the TextClassifier based models?\nIf not what is the default number of layers for a Simple CNN based model?", "issue_status": "Closed", "issue_reporting_time": "2019-08-19T11:37:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "416": {"issue_url": "https://github.com/explosion/spaCy/issues/4145", "issue_id": "#4145", "issue_summary": "How can I exactly import the en_core_web_lg?", "issue_description": "alperendurmus commented on 19 Aug 2019\nI am not sure my code is using the which model.\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(u\"Access to documents independently of time and space\")\nspacy.displacy.serve(doc, style='dep')\nand\nimport spacy\nnlp = spacy.load('en_core_web_lg')\ndoc = nlp(u\"Access to documents independently of time and space\")\nspacy.displacy.serve(doc, style='dep')\nare giving the same result(arcs). They should be different.", "issue_status": "Closed", "issue_reporting_time": "2019-08-18T21:50:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "417": {"issue_url": "https://github.com/explosion/spaCy/issues/4138", "issue_id": "#4138", "issue_summary": "Displacy no support for utf-8", "issue_description": "Contributor\navramandrei commented on 17 Aug 2019\nI get the following error when I try to use displacy to evaluate a Romanian corpus with entities.\nTraceback (most recent call last):\n  File \"C:\\Users\\avramus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\avramus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\avramus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\spacy\\__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"C:\\Users\\avramus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"C:\\Users\\avramus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"C:\\Users\\avramus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\spacy\\cli\\evaluate.py\", line 77, in evaluate\n    ents=render_ents,\n  File \"C:\\Users\\avramus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\spacy\\cli\\evaluate.py\", line 89, in render_parses\n    file_.write(html)\n  File \"C:\\Users\\avramus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\encodings\\cp1252.py\", line 19, in encode\n    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\nUnicodeEncodeError: 'charmap' codec can't encode character '\\u0103' in position 469: character maps to <undefined>\nIt looks like you don't open the html files for the entities and parses in UTF-8. I modified the code to open them in UTF-8 and it worked.", "issue_status": "Closed", "issue_reporting_time": "2019-08-17T14:52:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "418": {"issue_url": "https://github.com/explosion/spaCy/issues/4137", "issue_id": "#4137", "issue_summary": "Vocab for init-model", "issue_description": "kennyjoseph commented on 17 Aug 2019\nI think its worth noting in the init-model documentation that if you just start with a word2vec vector file, the vocabulary will still depend on spacy's vocabulary (or at least, thats what it looks like in the code). It took me a while to figure out why the word \"cop\", which was in my file, didn't have a vector loaded in (because it wasn't ever created by init-model)\nWhich page or section is this issue related to?\nhttps://spacy.io/api/cli#init-model", "issue_status": "Closed", "issue_reporting_time": "2019-08-16T19:47:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "419": {"issue_url": "https://github.com/explosion/spaCy/issues/4136", "issue_id": "#4136", "issue_summary": "Interactive Course - Incorrect Code", "issue_description": "yhscherber commented on 17 Aug 2019\nWhich page or section is this issue related to?\nIn Chapter 2, section \"Data Structures Best Practices\", Part 2, the solution is not a general solution and would not work for strings which end in a Proper Noun. To fix this, I used this code instead:\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Berlin is a nice city\")\n\nlength = len(doc)\n\nfor token in doc:\n  if token.i + 1 < length:\n    nextToken = doc[token.i + 1]\n    if token.pos_ == 'PROPN':\n      if nextToken.pos_ == 'VERB':\n        print(\"Found proper noun before a verb:\", token.text)", "issue_status": "Closed", "issue_reporting_time": "2019-08-16T19:21:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "420": {"issue_url": "https://github.com/explosion/spaCy/issues/4135", "issue_id": "#4135", "issue_summary": "unable to download spacy models by cli commands", "issue_description": "adalisan commented on 16 Aug 2019\nHow to reproduce the behaviour\ninstall conda package\nconda install -c conda-forge spacy=2.0\nDownload models\n> conda activate base\n> python -m spacy download en\n> \nTraceback (most recent call last):\n  File \"/nfs/mercury-12/u15/sadali/anaconda3/envs/tensorflow-1.5/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/nfs/mercury-12/u15/sadali/anaconda3/envs/tensorflow-1.5/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/nfs/mercury-12/u15/sadali/anaconda3/envs/tensorflow-1.5/lib/python3.6/site-packages/spacy/__main__.py\", line 31, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/nfs/mercury-12/u15/sadali/anaconda3/envs/tensorflow-1.5/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/nfs/mercury-12/u15/sadali/anaconda3/envs/tensorflow-1.5/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/nfs/mercury-12/u15/sadali/anaconda3/envs/tensorflow-1.5/lib/python3.6/site-packages/spacy/cli/download.py\", line 31, in download\n    shortcuts = get_json(about.__shortcuts__, \"available shortcuts\")\n  File \"/nfs/mercury-12/u15/sadali/anaconda3/envs/tensorflow-1.5/lib/python3.6/site-packages/spacy/cli/download.py\", line 58, in get_json\n    return r.json()\n  File \"/nfs/mercury-12/u15/sadali/anaconda3/envs/tensorflow-1.5/lib/python3.6/site-packages/requests/models.py\", line 897, in json\n    return complexjson.loads(self.text, **kwargs)\nAttributeError: module 'simplejson' has no attribute 'loads'\nInfo about spaCy\nspaCy version: 2.0.18\nPlatform: Linux-2.6.32-504.8.1.el6_lustre.x86_64-x86_64-with-redhat-6.6-Carbon\nPython version: 3.6.8\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-08-16T16:30:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "421": {"issue_url": "https://github.com/explosion/spaCy/issues/4134", "issue_id": "#4134", "issue_summary": "Unable to build spacy", "issue_description": "StKi2 commented on 16 Aug 2019\nI am trying to setup Spacy on Mojave 10.14.6. I get this persistent build error.\npip install spacy\nwarning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\nspacy/_align.cpp:833:14: fatal error: 'complex' file not found\n#include\n^~~~~~~~~\n2 warnings and 1 error generated.\nerror: command 'gcc' failed with exit status 1\nI see the header files in /usr/include and yet it will not build. Does spacy work with the latest version of MacOs?\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-08-16T16:15:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "422": {"issue_url": "https://github.com/explosion/spaCy/issues/4133", "issue_id": "#4133", "issue_summary": "Vocab serialization/deserialization leads to incomplete document", "issue_description": "Criffle12 commented on 16 Aug 2019\nIt seems that Spacy has an issue with transforming a document to a byte-array and the other way around - because when I do so, some information e.g. part-of-speech data is missing.\nI already figured out that it works when I load the document directly with the model vocab - which means that the bug is most likely happening during the serialization resp. deserialization of vocab.\ndoc = Doc(nlp.vocab).from_bytes(doc_bytes)\nHow to reproduce the behaviour\nfrom spacy import load\nfrom spacy.tokens import Doc\nfrom spacy.vocab import Vocab\n\nclass SpacySaveLoadTest(unittest.TestCase):\n\n    def test_foo(self):\n        nlp = load('en_core_web_sm')\n        vocab_bytes = nlp.vocab.to_bytes()\n        doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n        doc_bytes = doc.to_bytes()\n\n        expected = []\n        for token in doc:\n            expected.append(token.pos_)\n\n        vocab = Vocab()\n        vocab.from_bytes(vocab_bytes)\n        doc = Doc(vocab).from_bytes(doc_bytes)\n\n        actual = []\n        for token in doc:\n            actual.append(token.pos_)\n\n        print(actual)\n        print(expected)\n        self.assertEqual(actual, expected)\nYour Environment\nspaCy version: 2.1.8\nPlatform: Windows-10-10.0.17134-SP0\nPython version: 3.7.4\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-16T15:56:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "423": {"issue_url": "https://github.com/explosion/spaCy/issues/4128", "issue_id": "#4128", "issue_summary": "other actions for Retokenizer?", "issue_description": "bdewilde commented on 16 Aug 2019\nFeature description\nI'm looking for an efficient way to add data augmentation into my TextCategorizer training workflow \u2014 namely, by applying a variety of transformations to a given Doc and thereby producing a new, similar-but-distinct document. There's a variety of transformations in the literature, but many come down to either swapping two tokens present within a given document/sentence or replacing tokens outright.\nIs there any way that the existing Retokenizer could incorporate .replace() and .swap() actions? I appreciate that the resulting sequence of tokens could become ungrammatical, but I don't know what the implications would be for spaCy.\nCurrently, I'm taking a Doc and using its annotations and vectors and such to produce a transformed text string, which I then use to create a new Doc, then repeat for the next transformation. It's inefficient. Would be awesome to have some built-in functionality to assist.\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.\nProbably! tbh I'd like to incorporate the higher-level data augmentation functionality into textacy, I'm just hoping for a less hacky / more efficient solution.", "issue_status": "Closed", "issue_reporting_time": "2019-08-16T03:59:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "424": {"issue_url": "https://github.com/explosion/spaCy/issues/4127", "issue_id": "#4127", "issue_summary": "Export SVG graphics of dependency parses", "issue_description": "Fourthought commented on 16 Aug 2019 \u2022\nedited\nHi Ines,\nI'm using the code below from your documentation to export images of dependency parses, however, it throws up 'TypeError: write() argument must be str, not None'.\nprint(type(svg)) shows svg to be a <class 'NoneType'>\nI don't seem to be able to find a solution online, do you have any suggestions please. Hopefully a quick solution which should take too much of your time!\n`import spacy\nfrom spacy import displacy\nfrom pathlib import Path\nnlp = spacy.load(\"en_core_web_sm\")\nsentences = [u\"This is an example.\", u\"This is another one.\"]\nfor sent in sentences:\ndoc = nlp(sent)\nsvg = displacy.render(doc, style=\"dep\")\nfile_name = '-'.join([w.text for w in doc if not w.is_punct]) + \".svg\"\noutput_path = Path(\"/images/\" + file_name)\noutput_path.open(\"w\", encoding=\"utf-8\").write(svg)`", "issue_status": "Closed", "issue_reporting_time": "2019-08-15T22:28:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "425": {"issue_url": "https://github.com/explosion/spaCy/issues/4126", "issue_id": "#4126", "issue_summary": "Empty files in Norwegian lemmatizer", "issue_description": "Contributor\npolm commented on 15 Aug 2019\nNot sure if this is an issue or not, but there are several files in the Norwegian (nb) lemmatizer directory that are basically empty. Here's an example - this is the whole file:\n# coding: utf8\nfrom __future__ import unicode_literals\n\n\nNOUNS = set(\n    \"\"\"\n\"\"\".split()\n)\nThe NOUNS object is imported and used elsewhere so maybe it's a placeholder for something but this just seems pointless.\nIf this actually serves no purpose, I guess the files should be deleted. On the other hand, if it's there for a reason, it's probably best to add a comment explaining what that is.", "issue_status": "Closed", "issue_reporting_time": "2019-08-15T15:07:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "426": {"issue_url": "https://github.com/explosion/spaCy/issues/4125", "issue_id": "#4125", "issue_summary": "Help with REGEX", "issue_description": "hanymorcos commented on 15 Aug 2019\nI'm having a hard time getting a REGEX to work. Can I get some input?\nHow to reproduce the behaviour\n    patterns = [{\"TEXT\"   : {\"REGEX\": r\"^\\d{3}[ -]?\\d{2}[ -]?\\d{4}$\"}} ]\n    nlp = spacy.load(\"en_core_web_sm\")\n    matcher = Matcher(nlp.vocab)\n    matcher.add(\"SSN\", None, patterns)\n    doc =nlp(u\"343-22-2322\")\n    entities = []\n    matches = matcher(doc)\n    print (matches)\nYour Environment\nspaCy version: 2.1.8\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.4", "issue_status": "Closed", "issue_reporting_time": "2019-08-15T12:16:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "427": {"issue_url": "https://github.com/explosion/spaCy/issues/4124", "issue_id": "#4124", "issue_summary": "Failed to install Spacy", "issue_description": "PanderBoy18 commented on 15 Aug 2019\nHi all, I am trying to install Spacy using this command:\npip3 install spacy\nBut that gives me the following output:\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\nCollecting spacy\n  Using cached https://files.pythonhosted.org/packages/58/f2/5a23bb7251988da474eec844b692760cb0a317912291afc77b516f399cff/spacy-2.1.8.tar.gz\n  Installing build dependencies ... error\n  Complete output from command /usr/bin/python3 -m pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-bh6kutyx --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple --extra-index-url https://www.piwheels.org/simple -- setuptools wheel>0.32.0,<0.33.0 Cython cymem>=2.0.2,<2.1.0 preshed>=2.0.1,<2.1.0 murmurhash>=0.28.0,<1.1.0 thinc>=7.0.8,<7.1.0:\n  Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple, https://www.piwheels.org/simple\n  Building wheels for collected packages: blis\n    Running setup.py bdist_wheel for blis: started\n    Running setup.py bdist_wheel for blis: finished with status 'error'\n    Complete output from command /usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-4vv5o4lr/blis/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-vy53b2u6 --python-tag cp37:\n    BLIS_COMPILER? None\n    running bdist_wheel\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-armv7l-3.7\n    creating build/lib.linux-armv7l-3.7/blis\n    copying blis/__init__.py -> build/lib.linux-armv7l-3.7/blis\n    copying blis/about.py -> build/lib.linux-armv7l-3.7/blis\n    copying blis/benchmark.py -> build/lib.linux-armv7l-3.7/blis\n    creating build/lib.linux-armv7l-3.7/blis/tests\n    copying blis/tests/__init__.py -> build/lib.linux-armv7l-3.7/blis/tests\n    copying blis/tests/common.py -> build/lib.linux-armv7l-3.7/blis/tests\n    copying blis/tests/test_dotv.py -> build/lib.linux-armv7l-3.7/blis/tests\n    copying blis/tests/test_gemm.py -> build/lib.linux-armv7l-3.7/blis/tests\n    copying blis/cy.pyx -> build/lib.linux-armv7l-3.7/blis\n    copying blis/py.pyx -> build/lib.linux-armv7l-3.7/blis\n    copying blis/__init__.pxd -> build/lib.linux-armv7l-3.7/blis\n    copying blis/cy.pxd -> build/lib.linux-armv7l-3.7/blis\n    running build_ext\n    unix\n    py_compiler gcc\n    {'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'HOSTTYPE': 'x86_64', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'LANG': 'C.UTF-8', 'OLDPWD': '/home/matt/repos/flame-blis', 'VIRTUAL_ENV': '/home/matt/repos/wheelwright/env3.6', 'USER': 'matt', 'PWD': '/home/matt/repos/cython-blis', 'HOME': '/home/matt', 'NAME': 'LAPTOP-OMKOB3VM', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SHELL': '/bin/bash', 'TERM': 'xterm-256color', 'SHLVL': '1', 'LOGNAME': 'matt', 'PATH': '/home/matt/repos/wheelwright/env3.6/bin:/tmp/google-cloud-sdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/Python37/Scripts:/mnt/c/Python37:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/WiFi/bin:/mnt/c/Program Files/Common Files/Intel/WirelessCommon:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/LLVM/bin:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/nodejs:/mnt/c/Users/matt/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/matt/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/matt/AppData/Roaming/npm:/snap/bin:/mnt/c/Program Files/Oracle/VirtualBox', 'PS1': '(env3.6) \\\\[\\\\e]0;\\\\u@\\\\h: \\\\w\\\\a\\\\]${debian_chroot:+($debian_chroot)}\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\]:\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ ', 'VAGRANT_HOME': '/home/matt/.vagrant.d/', 'OMP_NUM_THREADS': '1', 'LESSOPEN': '| /usr/bin/lesspipe %s', '_': '/home/matt/repos/wheelwright/env3.6/bin/python'}\n    gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/generic/bli_cntx_init_generic.c -o /tmp/tmpmvvjj4f1/bli_cntx_init_generic.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/haswell/bli_cntx_init_haswell.c -o /tmp/tmpmvvjj4f1/bli_cntx_init_haswell.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/penryn/bli_cntx_init_penryn.c -o /tmp/tmpmvvjj4f1/bli_cntx_init_penryn.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/piledriver/bli_cntx_init_piledriver.c -o /tmp/tmpmvvjj4f1/bli_cntx_init_piledriver.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/sandybridge/bli_cntx_init_sandybridge.c -o /tmp/tmpmvvjj4f1/bli_cntx_init_sandybridge.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/steamroller/bli_cntx_init_steamroller.c -o /tmp/tmpmvvjj4f1/bli_cntx_init_steamroller.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n    gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c -o /tmp/tmpmvvjj4f1/bli_amaxv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n    gcc: error: unrecognized -march target: core-avx2\n    gcc: note: valid arguments are: armv2 armv2a armv3 armv3m armv4 armv4t armv5 armv5t armv5e armv5te armv5tej armv6 armv6j armv6k armv6z armv6kz armv6zk armv6t2 armv6-m armv6s-m armv7 armv7-a armv7ve armv7-r armv7-m armv7e-m armv8-a armv8.1-a armv8.2-a armv8.3-a armv8.4-a armv8-m.base armv8-m.main armv8-r iwmmxt iwmmxt2 native\n    gcc: error: missing argument to \u2018-march=\u2019\n    gcc: error: unrecognized command line option \u2018-mavx2\u2019\n    gcc: error: unrecognized command line option \u2018-mfma\u2019\n    gcc: error: unrecognized command line option \u2018-mfpmath=sse\u2019\n    Traceback (most recent call last):\n      File \"<string>\", line 1, in <module>\n      File \"/tmp/pip-install-4vv5o4lr/blis/setup.py\", line 266, in <module>\n        'Topic :: Scientific/Engineering'\n      File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 145, in setup\n        return distutils.core.setup(**attrs)\n      File \"/usr/lib/python3.7/distutils/core.py\", line 148, in setup\n        dist.run_commands()\n      File \"/usr/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n        self.run_command(cmd)\n      File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/usr/lib/python3/dist-packages/wheel/bdist_wheel.py\", line 188, in run\n        self.run_command('build')\n      File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n        self.distribution.run_command(command)\n      File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/usr/lib/python3.7/distutils/command/build.py\", line 135, in run\n        self.run_command(cmd_name)\n      File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n        self.distribution.run_command(command)\n      File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n        cmd_obj.run()\n      File \"/usr/lib/python3.7/distutils/command/build_ext.py\", line 340, in run\n        self.build_extensions()\n      File \"/tmp/pip-install-4vv5o4lr/blis/setup.py\", line 105, in build_extensions\n        objects = self.compile_objects(compiler.split('-')[0], arch, OBJ_DIR)\n      File \"/tmp/pip-install-4vv5o4lr/blis/setup.py\", line 184, in compile_objects\n        objects.append(self.build_object(env=env, **spec))\n      File \"/tmp/pip-install-4vv5o4lr/blis/setup.py\", line 198, in build_object\n        subprocess.check_call(command, cwd=BLIS_DIR)\n      File \"/usr/lib/python3.7/subprocess.py\", line 347, in check_call\n        raise CalledProcessError(retcode, cmd)\n    subprocess.CalledProcessError: Command '['gcc', '-c', '/tmp/pip-install-4vv5o4lr/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c', '-o', '/tmp/tmpmvvjj4f1/bli_amaxv_zen_int.o', '-O3', '-mavx2', '-mfma', '-mfpmath=sse', '-march=core-avx2', '-fPIC', '-std=c99', '-D_POSIX_C_SOURCE=200112L', '-DBLIS_VERSION_STRING=\"0.5.0-6\"', '-DBLIS_IS_BUILDING_LIBRARY', '-Iinclude/linux-x86_64', '-I./frame/3/', '-I./frame/ind/ukernels/', '-I./frame/1m/', '-I./frame/1f/', '-I./frame/1/', '-I./frame/include', '-I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64']' returned non-zero exit status 1.\n\n    ----------------------------------------\n    Failed building wheel for blis\n    Running setup.py clean for blis\n  Failed to build blis\n  Installing collected packages: setuptools, wheel, Cython, cymem, preshed, murmurhash, numpy, wasabi, srsly, tqdm, plac, blis, thinc\n    Running setup.py install for blis: started\n      Running setup.py install for blis: finished with status 'error'\n      Complete output from command /usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-4vv5o4lr/blis/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-1633r1jr/install-record.txt --single-version-externally-managed --prefix /tmp/pip-build-env-bh6kutyx --compile:\n      BLIS_COMPILER? None\n      running install\n      running build\n      running build_py\n      creating build\n      creating build/lib.linux-armv7l-3.7\n      creating build/lib.linux-armv7l-3.7/blis\n      copying blis/__init__.py -> build/lib.linux-armv7l-3.7/blis\n      copying blis/about.py -> build/lib.linux-armv7l-3.7/blis\n      copying blis/benchmark.py -> build/lib.linux-armv7l-3.7/blis\n      creating build/lib.linux-armv7l-3.7/blis/tests\n      copying blis/tests/__init__.py -> build/lib.linux-armv7l-3.7/blis/tests\n      copying blis/tests/common.py -> build/lib.linux-armv7l-3.7/blis/tests\n      copying blis/tests/test_dotv.py -> build/lib.linux-armv7l-3.7/blis/tests\n      copying blis/tests/test_gemm.py -> build/lib.linux-armv7l-3.7/blis/tests\n      copying blis/cy.pyx -> build/lib.linux-armv7l-3.7/blis\n      copying blis/py.pyx -> build/lib.linux-armv7l-3.7/blis\n      copying blis/__init__.pxd -> build/lib.linux-armv7l-3.7/blis\n      copying blis/cy.pxd -> build/lib.linux-armv7l-3.7/blis\n      running build_ext\n      unix\n      py_compiler gcc\n      {'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'HOSTTYPE': 'x86_64', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'LANG': 'C.UTF-8', 'OLDPWD': '/home/matt/repos/flame-blis', 'VIRTUAL_ENV': '/home/matt/repos/wheelwright/env3.6', 'USER': 'matt', 'PWD': '/home/matt/repos/cython-blis', 'HOME': '/home/matt', 'NAME': 'LAPTOP-OMKOB3VM', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SHELL': '/bin/bash', 'TERM': 'xterm-256color', 'SHLVL': '1', 'LOGNAME': 'matt', 'PATH': '/home/matt/repos/wheelwright/env3.6/bin:/tmp/google-cloud-sdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/Python37/Scripts:/mnt/c/Python37:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/WiFi/bin:/mnt/c/Program Files/Common Files/Intel/WirelessCommon:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/LLVM/bin:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/nodejs:/mnt/c/Users/matt/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/matt/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/matt/AppData/Roaming/npm:/snap/bin:/mnt/c/Program Files/Oracle/VirtualBox', 'PS1': '(env3.6) \\\\[\\\\e]0;\\\\u@\\\\h: \\\\w\\\\a\\\\]${debian_chroot:+($debian_chroot)}\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\]:\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ ', 'VAGRANT_HOME': '/home/matt/.vagrant.d/', 'OMP_NUM_THREADS': '1', 'LESSOPEN': '| /usr/bin/lesspipe %s', '_': '/home/matt/repos/wheelwright/env3.6/bin/python'}\n      gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/generic/bli_cntx_init_generic.c -o /tmp/tmpe9ie6965/bli_cntx_init_generic.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/haswell/bli_cntx_init_haswell.c -o /tmp/tmpe9ie6965/bli_cntx_init_haswell.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/penryn/bli_cntx_init_penryn.c -o /tmp/tmpe9ie6965/bli_cntx_init_penryn.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/piledriver/bli_cntx_init_piledriver.c -o /tmp/tmpe9ie6965/bli_cntx_init_piledriver.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/sandybridge/bli_cntx_init_sandybridge.c -o /tmp/tmpe9ie6965/bli_cntx_init_sandybridge.o -O3 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/config/steamroller/bli_cntx_init_steamroller.c -o /tmp/tmpe9ie6965/bli_cntx_init_steamroller.o -O2 -fomit-frame-pointer -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n      gcc -c /tmp/pip-install-4vv5o4lr/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c -o /tmp/tmpe9ie6965/bli_amaxv_zen_int.o -O3 -mavx2 -mfma -mfpmath=sse -march=core-avx2 -fPIC -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude/linux-x86_64 -I./frame/3/ -I./frame/ind/ukernels/ -I./frame/1m/ -I./frame/1f/ -I./frame/1/ -I./frame/include -I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64\n      gcc: error: unrecognized -march target: core-avx2\n      gcc: note: valid arguments are: armv2 armv2a armv3 armv3m armv4 armv4t armv5 armv5t armv5e armv5te armv5tej armv6 armv6j armv6k armv6z armv6kz armv6zk armv6t2 armv6-m armv6s-m armv7 armv7-a armv7ve armv7-r armv7-m armv7e-m armv8-a armv8.1-a armv8.2-a armv8.3-a armv8.4-a armv8-m.base armv8-m.main armv8-r iwmmxt iwmmxt2 native\n      gcc: error: missing argument to \u2018-march=\u2019\n      gcc: error: unrecognized command line option \u2018-mavx2\u2019\n      gcc: error: unrecognized command line option \u2018-mfma\u2019\n      gcc: error: unrecognized command line option \u2018-mfpmath=sse\u2019\n      Traceback (most recent call last):\n        File \"<string>\", line 1, in <module>\n        File \"/tmp/pip-install-4vv5o4lr/blis/setup.py\", line 266, in <module>\n          'Topic :: Scientific/Engineering'\n        File \"/usr/lib/python3/dist-packages/setuptools/__init__.py\", line 145, in setup\n          return distutils.core.setup(**attrs)\n        File \"/usr/lib/python3.7/distutils/core.py\", line 148, in setup\n          dist.run_commands()\n        File \"/usr/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n          self.run_command(cmd)\n        File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n          cmd_obj.run()\n        File \"/usr/lib/python3/dist-packages/setuptools/command/install.py\", line 61, in run\n          return orig.install.run(self)\n        File \"/usr/lib/python3.7/distutils/command/install.py\", line 589, in run\n          self.run_command('build')\n        File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n          self.distribution.run_command(command)\n        File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n          cmd_obj.run()\n        File \"/usr/lib/python3.7/distutils/command/build.py\", line 135, in run\n          self.run_command(cmd_name)\n        File \"/usr/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n          self.distribution.run_command(command)\n        File \"/usr/lib/python3.7/distutils/dist.py\", line 985, in run_command\n          cmd_obj.run()\n        File \"/usr/lib/python3.7/distutils/command/build_ext.py\", line 340, in run\n          self.build_extensions()\n        File \"/tmp/pip-install-4vv5o4lr/blis/setup.py\", line 105, in build_extensions\n          objects = self.compile_objects(compiler.split('-')[0], arch, OBJ_DIR)\n        File \"/tmp/pip-install-4vv5o4lr/blis/setup.py\", line 184, in compile_objects\n          objects.append(self.build_object(env=env, **spec))\n        File \"/tmp/pip-install-4vv5o4lr/blis/setup.py\", line 198, in build_object\n          subprocess.check_call(command, cwd=BLIS_DIR)\n        File \"/usr/lib/python3.7/subprocess.py\", line 347, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['gcc', '-c', '/tmp/pip-install-4vv5o4lr/blis/blis/_src/kernels/zen/1/bli_amaxv_zen_int.c', '-o', '/tmp/tmpe9ie6965/bli_amaxv_zen_int.o', '-O3', '-mavx2', '-mfma', '-mfpmath=sse', '-march=core-avx2', '-fPIC', '-std=c99', '-D_POSIX_C_SOURCE=200112L', '-DBLIS_VERSION_STRING=\"0.5.0-6\"', '-DBLIS_IS_BUILDING_LIBRARY', '-Iinclude/linux-x86_64', '-I./frame/3/', '-I./frame/ind/ukernels/', '-I./frame/1m/', '-I./frame/1f/', '-I./frame/1/', '-I./frame/include', '-I/tmp/pip-install-4vv5o4lr/blis/blis/_src/include/linux-x86_64']' returned non-zero exit status 1.\n\n      ----------------------------------------\n  Command \"/usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-4vv5o4lr/blis/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-1633r1jr/install-record.txt --single-version-externally-managed --prefix /tmp/pip-build-env-bh6kutyx --compile\" failed with error code 1 in /tmp/pip-install-4vv5o4lr/blis/\n\n  ----------------------------------------\nCommand \"/usr/bin/python3 -m pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-bh6kutyx --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple --extra-index-url https://www.piwheels.org/simple -- setuptools wheel>0.32.0,<0.33.0 Cython cymem>=2.0.2,<2.1.0 preshed>=2.0.1,<2.1.0 murmurhash>=0.28.0,<1.1.0 thinc>=7.0.8,<7.1.0\" failed with error code 1 in None\nOperating System: Linux/Raspbian Latest\nPython Version Used: 3.7\nspaCy Version Used: Latest\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-15T10:14:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "428": {"issue_url": "https://github.com/explosion/spaCy/issues/4122", "issue_id": "#4122", "issue_summary": "Inferring GPE type while doing NER", "issue_description": "phaterpekar commented on 15 Aug 2019 \u2022\nedited\nWhen using Spacy for NER, is there a way to extract information if the entity type is city, country or state when entity type returned is GPE ?", "issue_status": "Closed", "issue_reporting_time": "2019-08-15T04:13:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "429": {"issue_url": "https://github.com/explosion/spaCy/issues/4121", "issue_id": "#4121", "issue_summary": "spacy==2.1.6 with neuralcoref", "issue_description": "tnmcneil commented on 15 Aug 2019\nTrying to use huggingface neuralcoref with spacy version 2.1.6 and receive the following error when I try to add coref to the pipe:\nRuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\nHave already tried this:\npip uninstall neuralcoref\npip install neuralcoref --no-binary neuralcoref\nRight now the only solution seems to be using spacy 2.1.3 or lower but I need version 2.1.6 - anyone else running into this?\nOperating System: Mac OS\nPython Version Used: 3.5.1\nspaCy Version Used: 2.1.6\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-08-14T22:41:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "430": {"issue_url": "https://github.com/explosion/spaCy/issues/4120", "issue_id": "#4120", "issue_summary": "Matches without a final {OP: ?} token are not returned", "issue_description": "Collaborator\nadrianeboyd commented on 15 Aug 2019\nA match ending with {\"OP\": \"?\"} only returns the match with this token, not without, unless the preceding token is the last token in the document.\nIf you change ? to * all matches are returned as expected.\nIf there is another matched token after {\"OP\": \"?\"} it also works as expected.\nI suspect this is related to some of the other OP Matcher bugs, but I hadn't seen this test case yet. The test suite includes * but not ?.\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Doc\n\nnlp = spacy.load('en')\n\nmatcher = Matcher(nlp.vocab)\n\npattern = [\n    {\"ORTH\": \"a\"},\n    {\"OP\": \"?\"},\n]\n\nmatcher.add(\"TEST\", None, pattern)\n\ndoc = Doc(nlp.vocab, words=[\"a\", \"b\", \"c\"])\nprint(matcher(doc))\n# [(1046765068364475028, 0, 2)] # expected match \"a\" from (0, 1) is missing\n\ndoc = Doc(nlp.vocab, words=[\"a\"])\nprint(matcher(doc))\n# [(1046765068364475028, 0, 1)] # but it works at the end of a document\n\nmatcher = Matcher(nlp.vocab)\n\npattern = [\n    {\"ORTH\": \"a\"},\n    {\"OP\": \"?\"},\n    {\"ORTH\": \"b\"},\n]\n\nmatcher.add(\"TEST\", None, pattern)\ndoc = Doc(nlp.vocab, words=[\"a\", \"b\", \"b\", \"c\"])\nprint(matcher(doc))\n# [(1046765068364475028, 0, 2), (1046765068364475028, 0, 3)] # good\n\nmatcher = Matcher(nlp.vocab)\n\npattern = [\n    {\"ORTH\": \"a\"},\n    {\"OP\": \"?\"},\n    {\"ORTH\": \"b\", \"OP\": \"?\"},\n]\n\nmatcher.add(\"TEST\", None, pattern)\ndoc = Doc(nlp.vocab, words=[\"a\", \"b\", \"b\", \"c\"])\nprint(matcher(doc))\n# [(1046765068364475028, 0, 2), (1046765068364475028, 0, 3)] # \"a\" is missing\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-4.19.0-5-amd64-x86_64-with-debian-10.0\nPython version: 3.7.3\nModels: xx, en", "issue_status": "Closed", "issue_reporting_time": "2019-08-14T19:36:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "431": {"issue_url": "https://github.com/explosion/spaCy/issues/4118", "issue_id": "#4118", "issue_summary": "Generate generic sentences and then train the model on top of that. Is it a good idea?", "issue_description": "AbhishekKargawal commented on 14 Aug 2019 \u2022\nedited\nI am training a model from scratch to predict food items from the text. I have tagged around 500 sentences to train my model and the accuracy is pretty good. But, I am a bit worried about the unseen real-world data so I have come up with an interesting idea. So I wanted to know some experienced person thought in this interesting idea.\nSo the idea is to convert the 500 sentences into maybe 10000 sentences. For that, I have first I replaced the actual entity with tag and then filled with possible entities. Example of this as follows:\nOriginal training Sentences:\n\"Tesco sold fifty thousand pizza last year. \" --- Food = pizza\n\"He loves to eat pudding when he is alone.\" --- Food = pudding\nGeneric Sentences:\n\"Tesco sold fifty thousand last year. \"\n\"He loves to eat when he is alone.\"\nFood List:\npizza\npudding\nNew training sentences:\n\"Tesco sold fifty thousand pizza last year. \" --- Food = pizza\n\"Tesco sold fifty thousand pudding last year. \" --- Food = pudding\n\"He loves to eat pizza when he is alone.\" --- Food = pizza\n\"He loves to eat pudding when he is alone.\" --- Food = pudding\nSo is this a good to generate training sentences like this.\nBenefits which I think:\nMore sentences.\nThe single entity will have more example instead of one or two.\nMay be high accuracy.\nIssues could be:\nTraining data full of similar sentence pattern.\nThanks, Please let me know thoughts on this approach.", "issue_status": "Closed", "issue_reporting_time": "2019-08-14T11:54:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "432": {"issue_url": "https://github.com/explosion/spaCy/issues/4116", "issue_id": "#4116", "issue_summary": "Raising: [E084] Error assigning label ID even with string added to vocab", "issue_description": "Gonzalo933 commented on 14 Aug 2019\nI am trying to add a new match to doc.ents but Spacy raises an error after executing span = Span(doc, start, end, label=match_id):\n*** ValueError: [E084] Error assigning label ID 10305103465695748238 to span: not in StringStore.\nI tried to add the string doc[start:end] to the vocab with nlp.vocab.strings.add(str(doc[start:end]) but first I get returned a different hash\n1395163539440239885\nand second I still get the same error. So it does seems that the match text is different from doc[start:end] (which is: \"No fiebre\").\nThis is extra weird as the same match was added yesterday without raising any errors\nOperating System: Windows 10\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.4\nEnvironment Information: N/A", "issue_status": "Closed", "issue_reporting_time": "2019-08-14T07:39:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "433": {"issue_url": "https://github.com/explosion/spaCy/issues/4115", "issue_id": "#4115", "issue_summary": "Why can i not tag \\n as entity?", "issue_description": "damianoporta commented on 13 Aug 2019\nHello,\ni am building a custom sentence boundaries classifier using the NER.\nBasically i would like to tag the \\n with EOS label (End Of Sentence).\nIt seems not possible because during the training I get:\nFile \"/home/damiano/lavoro/python/.virtualenvs/parser/lib/python3.6/site-packages/spacy/language.py\", line 475, in update\nproc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\nFile \"nn_parser.pyx\", line 413, in spacy.syntax.nn_parser.Parser.update\nFile \"nn_parser.pyx\", line 519, in spacy.syntax.nn_parser.Parser._init_gold_batch\nFile \"transition_system.pyx\", line 86, in spacy.syntax.transition_system.TransitionSystem.get_oracle_sequence\nFile \"transition_system.pyx\", line 148, in spacy.syntax.transition_system.TransitionSystem.set_costs\nValueError: [E024] Could not find an optimal move to supervise the parser. Usually, this means that the model can't be updated in a way that's valid and satisfies the correct annotations specified in the GoldParse. For example, are all labels added to the model? If you're training a named entity recognizer, also make sure that none of your annotated entity spans have leading or trailing whitespace. You can also use the experimental debug-data command to validate your JSON-formatted training data. For details, run:\npython -m spacy debug-data --help\nI must not tag all the \\n characters as EOS, because of this reason i would like to use the NER to take context decisions.\nA workaround could be replacing \\n with a specific code/word like NL or [NL] or whatever... is this the only solution?\nYour Environment\nspaCy version 2.1.8\nLocation /home/damiano/lavoro/python/.virtualenvs/parser/lib/python3.6/site-packages/spacy\nPlatform Linux-5.0.0-23-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version 3.6.8", "issue_status": "Closed", "issue_reporting_time": "2019-08-13T12:44:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "434": {"issue_url": "https://github.com/explosion/spaCy/issues/4112", "issue_id": "#4112", "issue_summary": "Is it possible to train NER with custom data labeled for named entities while keeping the base model's ability to do DEP and POS taggin?", "issue_description": "H20Watermelon commented on 13 Aug 2019\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.4\nEnvironment Information:\nThis is not a bug or feature request per se, but I didn't post this question on Stackoverflow because it's not specifically about programming.\nI have a set of data labeled for named entities. Some of these named entities overlap with the entity categories in the pre-trained English models, and some are new entity types.\nWhile my dataset is labeled for named entities, it is not annotated for either DEP or POS because I do not have the resources to tag them exhaustively in my data.. Both DEP and POS are things I need for my work.\nSo my question is: Is it possible to train (finetune) on a pretrained model with a data set that is only labeled for NER and still retain the DEP and POS tagging? Thanks!", "issue_status": "Closed", "issue_reporting_time": "2019-08-13T01:16:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "435": {"issue_url": "https://github.com/explosion/spaCy/issues/4111", "issue_id": "#4111", "issue_summary": "spacy convert .iob fix.", "issue_description": "vsraptor commented on 13 Aug 2019 \u2022\nedited\nHow to reproduce the behaviour\nspacy convert test.iob ./\nIt fails if the file have tokens that contain \"? , $\" f.e. PRP$ i.e. explain('PRP$') => 'pronoun, possessive'\nHere is the fix. Modify the regex in iob2json.py::read_iob() , from :\ntokens = [re.split(\"[^\\w-]\", line.strip())]\nto:\ntokens = [re.split(\"[^\\w-$,?.]\", line.strip())]\nor it will be probably better to split on white-space, rather than enumerating all the exception cases !!\nExample :\nLily NNP B-SRC\nhas VBZ O\nsung VBN B-PRED\nthe DT O\nblues NNS B-DTGT\nall PDT O\nher PRP$ O\nlife NN O", "issue_status": "Closed", "issue_reporting_time": "2019-08-12T21:54:00Z", "fixed_by": "#4186", "pull_request_summary": "Updates/bugfixes for NER/IOB converters", "pull_request_description": "Collaborator\nadrianeboyd commented on 23 Aug 2019\nDescription\nUpdates/bugfixes for NER/IOB converters:\nConverter formats ner and iob use autodetect to choose a converter if possible\niob2json() is reverted to handle sentence-per-line data likeword1|pos1|ent1 word2|pos2|ent2\nFix bug in merge_sentences() so the second sentence in each batch isn't skipped\nconll_ner2json() is made more general so it can handle more formats with whitespace-separated columns\nSupports all formats where the first column is the token and the final column is the IOB tag; if present, the second column is the POS tag\nAs in CoNLL 2003 NER, blank lines separate sentences, -DOCSTART- -X- O O separates documents\nAdd option for segmenting sentences (new flag -s)\nParser-based sentence segmentation with a provided model, otherwise with sentencizer (new option -b to specify model)\nCan group sentences into documents with n_sents as long as sentence segmentation is available\nOnly applies automatic segmentation when there are no existing delimiters of this type in the data\nProvide info about settings applied during conversion with warnings and suggestions if settings conflict or might not be not optimal.\nAdd tests for common formats\nFixes #4111 and #4170.\nTypes of change\nBugfixes and enhancements.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.", "pull_request_status": "Merged", "issue_fixed_time": "2019-08-29T10:04:02Z", "files_changed": [["7", "examples/training/ner_example_data/README.md"], ["2", "examples/training/ner_example_data/ner-sent-per-line.iob"], ["349", "examples/training/ner_example_data/ner-sent-per-line.json"], ["70", "examples/training/ner_example_data/ner-token-per-line-conll2003.iob"], ["349", "examples/training/ner_example_data/ner-token-per-line-conll2003.json"], ["66", "examples/training/ner_example_data/ner-token-per-line-with-pos.iob"], ["353", "examples/training/ner_example_data/ner-token-per-line-with-pos.json"], ["66", "examples/training/ner_example_data/ner-token-per-line.iob"], ["353", "examples/training/ner_example_data/ner-token-per-line.json"], ["45", "spacy/cli/convert.py"], ["120", "spacy/cli/converters/conll_ner2json.py"], ["27", "spacy/cli/converters/iob2json.py"], ["87", "spacy/tests/test_cli.py"], ["8", "website/docs/api/cli.md"]]}, "436": {"issue_url": "https://github.com/explosion/spaCy/issues/4109", "issue_id": "#4109", "issue_summary": "PhraseMatcher slower than Matcher when using attr=\"LOWER\"?", "issue_description": "Contributor\nfizban99 commented on 12 Aug 2019\nAccording to the PhraseMatcher usage guide\nIf you need to match large terminology lists, you can also use the PhraseMatcher\nand in the Rule-based matching section\nThe PhraseMatcher is useful if you already have a large terminology list or gazetteer consisting of single or multi-token phrases that you want to find exact instances of in your data.\nand\nThe Matcher isn\u2019t as blazing fast as the PhraseMatcher\nI was suprised at finding out that the Matcher seems to be significantly faster for single-term terminology lists when using \"LOWER\". Here is a sample code using a sample text from project Gutenberg and the results with %timeit:\nimport nltk\nfrom nltk.corpus import gutenberg\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.matcher import Matcher\nfrom spacy.lang.en import English\n\nnltk.download(\"gutenberg\")\nnlp = English()\ndoc = nlp(gutenberg.raw(\"carroll-alice.txt\"))\nwords = [\"alice\", \"she\", \"tired\", \"sitting\", \"sister\"]\n\n# with PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\nterminology = list(nlp.tokenizer.pipe(words))\nmatcher.add(\"Test1\", None, *terminology)\n%timeit matches = matcher(doc)\n# Result: 359 ms \u00b1 55.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n# with Matcher\nmatcher = Matcher(nlp.vocab)\nterminology = [{\"LOWER\": {\"IN\": words}}]\nmatcher.add(\"Test1\", None, terminology)\n%timeit matches = matcher(doc)\n#Result: 65.1 ms \u00b1 5.01 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\nOn the other hand, when not using \"LOWER\", the difference favours the PhraseMatcher:\nimport nltk\nfrom nltk.corpus import gutenberg\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.matcher import Matcher\nfrom spacy.lang.en import English\n\nnltk.download(\"gutenberg\")\nnlp = English()\ndoc = nlp(gutenberg.raw(\"carroll-alice.txt\"))\nwords = [\"alice\", \"she\", \"tired\", \"sitting\", \"sister\"]\n\n# with PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab)\nterminology = list(nlp.tokenizer.pipe(words))\nmatcher.add(\"Test1\", None, *terminology)\n%timeit matches = matcher(doc)\n# Result: 39.1 ms \u00b1 979 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# with Matcher\nmatcher = Matcher(nlp.vocab)\nterminology = [{\"ORTH\": {\"IN\": words}}]\nmatcher.add(\"Test2\", None, terminology)\n%timeit matches = matcher(doc)\n#Result: 54.8 ms \u00b1 1.97 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\nI would suggest reviewing the usage guide about the benefits of the PhraseMatcher limitting it to multi-term terminology or ORTH values. Alternatively, review the code to see why the PhraseMather performs poorly when using LOWER...", "issue_status": "Closed", "issue_reporting_time": "2019-08-12T13:44:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "437": {"issue_url": "https://github.com/explosion/spaCy/issues/4108", "issue_id": "#4108", "issue_summary": "Inaccurate lemmas and POS tags for Greek ('el' model)", "issue_description": "petasis commented on 12 Aug 2019\nHi all,\nI am facing problems with the model for the Greek language. Mainly for part of speech tags (failure on verbs is quite high) and lemmas. For example:\n'\u0388\u03c7\u03b5\u03b9\u03c2 \u03b1\u03b4\u03ad\u03c1\u03c6\u03b9\u03b1;' - Here the verb is tagged as noun.\nSpacy: \"\u0388\u03c7\u03b5\u03b9\u03c2|\u03ad\u03c7\u03b5\u03b9|NOUN \u03b1\u03b4\u03ad\u03c1\u03c6\u03b9\u03b1|\u03b1\u03b4\u03ad\u03c1\u03c6\u03b9|NOUN ;|;|PUNCT\"\nEllogon: \"\u0388\u03c7\u03b5\u03b9\u03c2|\u03ad\u03c7\u03c9|VERB \u03b1\u03b4\u03ad\u03c1\u03c6\u03b9\u03b1|\u03b1\u03b4\u03ad\u03c1\u03c6\u03b9|NOUN ;|;|PUNCT\"\n\"\u03a4\u03b9 \u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2 \u03c0\u03b1\u03c0\u03bf\u03c5\u03c4\u03c3\u03b9\u03ce\u03bd \u03c6\u03bf\u03c1\u03ac\u03c2;\" - Here the lemma of the 3rd word is not a Greek word, and the verb is tagged as a noun.\nSpacy: \"\u03a4\u03b9|\u03c4\u03b9|PRON \u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2|\u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2|NOUN \u03c0\u03b1\u03c0\u03bf\u03c5\u03c4\u03c3\u03b9\u03ce\u03bd|\u03c0\u03b1\u03c0\u03bf\u03c5\u03c4\u03c3\u03af|NOUN \u03c6\u03bf\u03c1\u03ac\u03c2|\u03c6\u03bf\u03c1\u03ac|NOUN ;|;|PUNCT\"\nEllogon: \"\u03a4\u03b9|\u03c4\u03b9|PRON \u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2|\u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2|NOUN \u03c0\u03b1\u03c0\u03bf\u03c5\u03c4\u03c3\u03b9\u03ce\u03bd|\u03c0\u03b1\u03c0\u03bf\u03cd\u03c4\u03c3\u03b9|NOUN \u03c6\u03bf\u03c1\u03ac\u03c2|\u03c6\u03bf\u03c1\u03ce|VERB ;|;|PUNCT\"\n\"\u03a4\u03b9 \u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2 \u03c0\u03b1\u03c0\u03bf\u03cd\u03c4\u03c3\u03b9\u03b1 \u03c6\u03bf\u03c1\u03ac\u03c2;\" - Here the lemma of the 3rd word is correct, the verb tagged as noun.\nSpacy: \"\u03a4\u03b9|\u03c4\u03b9|PRON \u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2|\u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2|NOUN \u03c0\u03b1\u03c0\u03bf\u03cd\u03c4\u03c3\u03b9\u03b1|\u03c0\u03b1\u03c0\u03bf\u03cd\u03c4\u03c3\u03b9|NOUN \u03c6\u03bf\u03c1\u03ac\u03c2|\u03c6\u03bf\u03c1\u03ac|NOUN ;|;|PUNCT\"\nEllogon: \"\u03a4\u03b9|\u03c4\u03b9|PRON \u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2|\u03bc\u03ad\u03b3\u03b5\u03b8\u03bf\u03c2|NOUN \u03c0\u03b1\u03c0\u03bf\u03cd\u03c4\u03c3\u03b9\u03b1|\u03c0\u03b1\u03c0\u03bf\u03cd\u03c4\u03c3\u03b9|NOUN \u03c6\u03bf\u03c1\u03ac\u03c2|\u03c6\u03bf\u03c1\u03ac-\u03c6\u03bf\u03c1\u03ce-\u03c6\u03bf\u03c1\u03ac\u03b4\u03b1|VERB ;|;|PUNCT\"\n\"\u03a0\u03ce\u03c2 \u03b5\u03af\u03bd\u03b1\u03b9 \u03c4\u03bf \u03bd\u03b1 \u03b5\u03af\u03c3\u03b1\u03b9 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ae\u03c2;\" - Here the pronoun is tagged as verb, and the first verb as AUX.\nSpacy: \"\u03a0\u03ce\u03c2|\u03c0\u03ce\u03c2|VERB \u03b5\u03af\u03bd\u03b1\u03b9|\u03b5\u03af\u03bd\u03b1\u03b9|AUX \u03c4\u03bf|\u03c4\u03bf|DET \u03bd\u03b1|\u03bd\u03b1|PART \u03b5\u03af\u03c3\u03b1\u03b9|\u03b5\u03af\u03bc\u03b1\u03b9|VERB \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ae\u03c2|\u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ae\u03c2|NOUN ;|;|PUNCT\"\nEllogon: \"\u03a0\u03ce\u03c2|\u03c0\u03ce\u03c2|ADV \u03b5\u03af\u03bd\u03b1\u03b9|\u03b5\u03af\u03bc\u03b1\u03b9|VERB \u03c4\u03bf|\u03bf|DET \u03bd\u03b1|\u03bd\u03b1|ADP \u03b5\u03af\u03c3\u03b1\u03b9|\u03b5\u03af\u03bc\u03b1\u03b9|VERB \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ae\u03c2|\u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03ae\u03c2|NOUN ;|;|PUNCT\"\n\"\u03a0\u03bf\u03b9\u03b1 \u03b5\u03af\u03bd\u03b1\u03b9 \u03b7 \u03b1\u03b3\u03b1\u03c0\u03b7\u03bc\u03ad\u03bd\u03b7 \u03c3\u03bf\u03c5 \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1;\" - Here again the VERB/AUX mismatch, and the participle is tagged as a verb. Also, leemas from Ellogon are more correct.\nSpacy: \"\u03a0\u03bf\u03b9\u03b1|\u03c0\u03bf\u03b9\u03b1|PRON \u03b5\u03af\u03bd\u03b1\u03b9|\u03b5\u03af\u03bd\u03b1\u03b9|AUX \u03b7|\u03b7|DET \u03b1\u03b3\u03b1\u03c0\u03b7\u03bc\u03ad\u03bd\u03b7|\u03b1\u03b3\u03b1\u03c0\u03b7\u03bc\u03ad\u03bd\u03b7|VERB \u03c3\u03bf\u03c5|\u03c3\u03bf\u03c5|PRON \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1|\u03b3\u03bb\u03ce\u03c3\u03c3\u03b1|NOUN ;|;|PUNCT\"\nEllogon: \"\u03a0\u03bf\u03b9\u03b1|\u03c0\u03bf\u03b9\u03bf\u03c2|PRON \u03b5\u03af\u03bd\u03b1\u03b9|\u03b5\u03af\u03bc\u03b1\u03b9|VERB \u03b7|\u03bf|DET \u03b1\u03b3\u03b1\u03c0\u03b7\u03bc\u03ad\u03bd\u03b7|\u03b1\u03b3\u03b1\u03c0\u03ce|PART \u03c3\u03bf\u03c5|\u03b5\u03b3\u03ce|PRON \u03b3\u03bb\u03ce\u03c3\u03c3\u03b1|\u03b3\u03bb\u03ce\u03c3\u03c3\u03b1|NOUN ;|;|PUNCT\"\n\"\u03a3\u03c5\u03bc\u03c6\u03c9\u03bd\u03b5\u03af\u03c2 \u03bc\u03b5 \u03c4\u03bf\u03bd \u03cc\u03c1\u03bf \u0392\u03cc\u03c1\u03b5\u03b9\u03b1 \u039c\u03b1\u03ba\u03b5\u03b4\u03bf\u03bd\u03af\u03b1 \u03b3\u03b9\u03b1 \u03c4\u03bf\u03c5\u03c2 \u03b2\u03cc\u03c1\u03b5\u03b9\u03bf\u03c5\u03c2 \u03b3\u03b5\u03af\u03c4\u03bf\u03bd\u03ad\u03c2 \u03bc\u03b1\u03c2;\" - Here the verb is tagged as ADJ, and again the lemmas are better from Ellogon.\nSpacy: \"\u03a3\u03c5\u03bc\u03c6\u03c9\u03bd\u03b5\u03af\u03c2|\u03c3\u03c5\u03bc\u03c6\u03c9\u03bd\u03b5\u03af\u03c2|ADJ \u03bc\u03b5|\u03bc\u03b5|ADP \u03c4\u03bf\u03bd|\u03c4\u03bf\u03bd|DET \u03cc\u03c1\u03bf|\u03cc\u03c1\u03bf|NOUN \u0392\u03cc\u03c1\u03b5\u03b9\u03b1|\u03b2\u03cc\u03c1\u03b5\u03b9\u03bf\u03c2|ADJ \u039c\u03b1\u03ba\u03b5\u03b4\u03bf\u03bd\u03af\u03b1|\u03bc\u03b1\u03ba\u03b5\u03b4\u03bf\u03bd\u03af\u03b1|PROPN \u03b3\u03b9\u03b1|\u03b3\u03b9\u03b1|ADP \u03c4\u03bf\u03c5\u03c2|\u03c4\u03bf\u03c5\u03c2|DET \u03b2\u03cc\u03c1\u03b5\u03b9\u03bf\u03c5\u03c2|\u03b2\u03cc\u03c1\u03b5\u03b9\u03bf\u03c5\u03c2|ADJ \u03b3\u03b5\u03af\u03c4\u03bf\u03bd\u03ad\u03c2|\u03b3\u03b5\u03af\u03c4\u03bf\u03bd\u03ad|NOUN \u03bc\u03b1\u03c2|\u03bc\u03b1\u03c2|PRON ;|;|PUNCT\"\nEllogon: \"\u03a3\u03c5\u03bc\u03c6\u03c9\u03bd\u03b5\u03af\u03c2|\u03c3\u03c5\u03bc\u03c6\u03c9\u03bd\u03ce|VERB \u03bc\u03b5|\u03bc\u03b5|ADP \u03c4\u03bf\u03bd|\u03bf|DET \u03cc\u03c1\u03bf|\u03cc\u03c1\u03bf\u03c2|NOUN \u0392\u03cc\u03c1\u03b5\u03b9\u03b1|\u03b2\u03cc\u03c1\u03b5\u03b9\u03bf\u03c2|ADJ \u039c\u03b1\u03ba\u03b5\u03b4\u03bf\u03bd\u03af\u03b1|\u039c\u03b1\u03ba\u03b5\u03b4\u03bf\u03bd\u03af\u03b1|PROPN \u03b3\u03b9\u03b1|\u03b3\u03b9\u03b1|ADP \u03c4\u03bf\u03c5\u03c2|\u03bf|DET \u03b2\u03cc\u03c1\u03b5\u03b9\u03bf\u03c5\u03c2|\u03b2\u03cc\u03c1\u03b5\u03b9\u03bf\u03c2|ADJ \u03b3\u03b5\u03af\u03c4\u03bf\u03bd\u03ad\u03c2|\u03b3\u03b5\u03af\u03c4\u03bf\u03bd\u03b1\u03c2|NOUN \u03bc\u03b1\u03c2|\u03bc\u03bf\u03c5|PRON ;|;|PUNCT\"\nDo you know if a better model for Greek will be released soon?\nIn the meantime, is it possible to replace the part-of-speech tagger and lemmatiser of the 'el' model with others that I have access to?", "issue_status": "Closed", "issue_reporting_time": "2019-08-12T13:25:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "438": {"issue_url": "https://github.com/explosion/spaCy/issues/4107", "issue_id": "#4107", "issue_summary": "Improvements on setting attributes in merge_noun_chunks function", "issue_description": "alaponin commented on 12 Aug 2019\nFeature description\nIt would be nice if during the merging of noun chunks the lemma would also be properly set. Currently, the lemma of the merged token is the lemma of the first word in the noun chunk sequence, which I don't think is the desired behavior in most cases.\nAdditionally, it would be nice to have more flexibility in setting the attributes of the merged token. For example, I would like to keep the entity type of the root token as the entity type of the whole merged token.", "issue_status": "Closed", "issue_reporting_time": "2019-08-12T12:18:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "439": {"issue_url": "https://github.com/explosion/spaCy/issues/4104", "issue_id": "#4104", "issue_summary": "Really minor documentation issue - custom_ner_model not available", "issue_description": "sharonwoo commented on 12 Aug 2019\nHello, I'm new to Spacy and trying it out. Really enjoyable stuff, thanks. Spotted one small thing working through the docs:\nWhich page or section is this issue related to?\nURL: https://spacy.io/usage/linguistic-features\nIn the NAMED ENTITY EXAMPLE, this line\nnlp = spacy.load(\"custom_ner_model\")\ndoesn't work for me. Replacing it with the standard model \"en_core_web_sm\" works perfectly fine.\nIf I try to install it with python -m spacy download custom_ner_model, I get the error message \u2718 No compatible model found for 'custom_ner_model' (spaCy v2.1.4).", "issue_status": "Closed", "issue_reporting_time": "2019-08-12T03:32:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "440": {"issue_url": "https://github.com/explosion/spaCy/issues/4102", "issue_id": "#4102", "issue_summary": "Wrong lemmatization using only tokenization (en language)", "issue_description": "Contributor\najrader commented on 10 Aug 2019\nObservation\nI noticed if I tried to use spacy for tokenization only (disabling 'ner', 'parser' and 'tagger') that i was getting an erroneous mapping for the english word 'spun'. As long as tagger is not disabled, this past tense verb is correctly mapped to 'spin'. But when 'tagger' is disabled then it gets mapped to 'spin-dry' which in my estimation is wrong.\nI think the source of this error is due to line 35297 in https://github.com/explosion/spaCy/tree/master/spacy/lang/en/lemmatizer/lookup.py\n\"spun\": \"spin-dry\"\nHow to reproduce the behavior\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp('spun', disable=['tagger'])\nfor tok in doc0:\nprint(tok.text, tok.lemma_)\nThe output of this is\nspun spin-dry\nI noticed the same behavior when using 'en_core_web_lg' and 'en_core_web_md' because I think this is the default lemmatizer mapping in all 'en' dictionaries.\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.7\nEnvironment Information: installed with conda\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-09T21:30:33Z", "fixed_by": "#4110", "pull_request_summary": "Correction of default lemmatizer lookup in English (Issue # 4104)", "pull_request_description": "Contributor\najrader commented on 13 Aug 2019\nResolves issue 4102.\nDescription\nmade the following changes to lookup.py:\n'dry' : 'dry'\n'spun': 'spin'\n'spun-dry': 'spin-dry'\ncreated new test (test_issue4104.py) and verified it passed after the above changes.\nTypes of change\nmainly feat / lemmatizer bug fix.\nNote that this change will become obsolete once using a default lookup for a language is implemented.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-08-15T09:39:11Z", "files_changed": [["106", ".github/contributors/ajrader.md"], ["5", "spacy/lang/en/lemmatizer/lookup.py"], ["14", "spacy/tests/regression/test_issue4104.py"]]}, "441": {"issue_url": "https://github.com/explosion/spaCy/issues/4100", "issue_id": "#4100", "issue_summary": "PhraseMatcher does not match LEMMA", "issue_description": "Contributor\nfizban99 commented on 9 Aug 2019\nAccording to the documentation, the PhraseMatcher allows setting a different attribute other than ORTH. It explicitely mentions LOWER, POS and DEP, but leaves the door open for other attributes.\nThe LEMMA attribute does not seem to work with the English model. This was mentioned in the gitter chat room.\nHow to reproduce the behaviour\nIf we take the exact example in the usage guide and just add attr=\"LEMMA\" to the constructor, it does not find any match:\nimport spacy\nfrom spacy.matcher import PhraseMatcher\n\nnlp = spacy.load('en_core_web_sm')\nmatcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\nterms = [u\"Barack Obama\", u\"Angela Merkel\", u\"Washington, D.C.\"]\n# Only run nlp.make_doc to speed things up\npatterns = [nlp.make_doc(text) for text in terms]\nmatcher.add(\"TerminologyList\", None, *patterns)\n\ndoc = nlp(u\"German Chancellor Angela Merkel and US President Barack Obama \"\n          u\"converse in the Oval Office inside the White House in Washington, D.C.\")\nmatches = matcher(doc)\nfor match_id, start, end in matches:\n    span = doc[start:end]\n    print(span.text)\nwe can verify that in this case the lemmas and the text should be the same for the matching terms and I would expect to get the same results with ORTH or with LEMMA in this specific example:\nfor tok in doc:\n    print(tok.lemma_, tok.text)\nIf instead of the en_core_web_sm model we use an empty English model:\nfrom spacy.lang.en import English\nnlp = English()\nWe get that the terms match everything, which is also incorrect.\nYour Environment\nspaCy version: 2.1.8\nPlatform: Linux-4.15.0-52-generic-x86_64-with-debian-buster-sid\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-09T08:41:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "442": {"issue_url": "https://github.com/explosion/spaCy/issues/4099", "issue_id": "#4099", "issue_summary": "Prevent base English models from producing \"subtok\" parse?", "issue_description": "dxiao2003 commented on 9 Aug 2019 \u2022\nedited\nHow to reproduce the behaviour\nUsing the base English en_core_web_sm model, it's possible for the dependency parser to produce tokens with the \"subtok\" label. This seems like unexpected behavior and it's definitely problematic since training the parser on examples with \"subtok\" labels causes crashes.\nTo reproduce:\n> nlp = spacy.load(\"en_core_web_sm\")\n> d = nlp(\"I'd rather have one Sharks with no crunch mustard instead of 205S\")\n> d[11].dep_\n'subtok'\nGranted the above isn't a particularly well-formed English sentence, but nevertheless I would not expect the \"subtok\" label. Is there a flag we can set that prevents the model from generating this label?\nAlso, running merge_subtokens afterwards isn't a solution because I need \"of\" to be a separate token.\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.8\nPlatform: Linux-4.15.0-1040-aws-x86_64-with-debian-9.9\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-08T20:23:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "443": {"issue_url": "https://github.com/explosion/spaCy/issues/4098", "issue_id": "#4098", "issue_summary": "Flaky matcher test and ghost IDs", "issue_description": "Member\nsvlandeg commented on 8 Aug 2019 \u2022\nedited\nWhen merging PR #4097 we ran into an error for test_issue_1971_4 which was resolved after restarting. What happened is that test, which is supposed to produce 1 match like [(1046765068364475028, 0, 3)], produced 1 for each token instead: [(9223420625083070725, 0, 1), (9223420625083070725, 1, 2), (9223420625083070725, 2, 3)]. I can't reproduce the error on my own system, even though I also run Windows and Python 3.6.\nNote that the unit test has a comment:\n# Uncommenting this caused a segmentation fault\nAfter discussing with @ines, this seems related to previous issues #3839 and #3291, where the matcher was producing \"ghost matches\" with IDs that weren't in the string store.\nPossibly, the fix that was supposed to resolve the original issue (matcher segfault) didn't fully fix it...\nNot very clear on how to proceed here because it's so difficult to test, but we wanted to record this for future reference.\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-08T14:09:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "444": {"issue_url": "https://github.com/explosion/spaCy/issues/4096", "issue_id": "#4096", "issue_summary": "Add check for cycles in dependency trees to debug-data CLI", "issue_description": "Collaborator\nadrianeboyd commented on 8 Aug 2019\nFeature description\nThe debug-data CLI should also check for cycles in dependency trees.\nCycles should be identified elsewhere, too, to prevent the kind of error from #4083, but a check in debug-data would also be helpful.\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-08T07:54:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "445": {"issue_url": "https://github.com/explosion/spaCy/issues/4095", "issue_id": "#4095", "issue_summary": "Getting KeyError", "issue_description": "AbhayGodbole commented on 7 Aug 2019 \u2022\nedited\nI am getting error while executing \u201cspacy_ner_custom_entities.py\u201d\nLoaded model 'en'\nKeyError Traceback (most recent call last)\nin\n----> 1 main(model=\"en\",output_dir=\"../Data/Out\")\nC:\\Abhay\\AI\\Spacy-NER\\Notebooks\\spacy_ner_custom_entities.py in main(model, new_model_name, output_dir, n_iter)\n68 texts, annotations = zip(*batch)\n69 nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n---> 70 losses=losses)\n71 print('Losses', losses)\n72\nc:\\abhay\\ai\\spacy-ner\\vir-spacy\\lib\\site-packages\\spacy\\language.py in update(self, docs, golds, drop, sgd, losses, component_cfg)\n450 kwargs = component_cfg.get(name, {})\n451 kwargs.setdefault(\"drop\", drop)\n--> 452 proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n453 for key, (W, dW) in grads.items():\n454 sgd(W, dW, key=key)\nnn_parser.pyx in spacy.syntax.nn_parser.Parser.update()\nnn_parser.pyx in spacy.syntax.nn_parser.Parser._init_gold_batch()\nner.pyx in spacy.syntax.ner.BiluoPushDown.preprocess_gold()\nner.pyx in spacy.syntax.ner.BiluoPushDown.lookup_transition()\nKeyError: \"[E022] Could not find a transition with the name 'U-Tag' in the NER model.\"\nI am using following custom lables:\nLABEL = [\u2018B-NewNom\u2019,\u2019I-NewNom\u2019,\u2019B-OldNom\u2019,\u2019I-OldNom\u2019]\nNewNom = New Nominee\nOldNom = Old Nominee\nI am using latest Sapcy:\n===== Info about spaCy =========\nspaCy version 2.1.0\nPlatform Windows-10\u201310.0.16299-SP0\nPython version 3.7.1\nModels en\nEnvironment\nOperating System: Windows-10\u201310.0.16299-SP0\nPython Version Used: 3.7.1\nspaCy Version Used: 2.1.0\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-08-07T08:38:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "446": {"issue_url": "https://github.com/explosion/spaCy/issues/4094", "issue_id": "#4094", "issue_summary": "How is the overall F Score of an NER Model calculated?", "issue_description": "iCHAIT commented on 7 Aug 2019\nI trained a custom NER model with 6 Entities and later tested it on an unseen sample.\nThis is the code I am using to evaluate my model on some unseen examples -\ndef evaluate(ner_model, examples):\n\n    scorer = Scorer()\n    for input_, annot in examples:\n        doc_gold_text = ner_model.make_doc(input_)\n        gold = GoldParse(doc_gold_text, entities=annot.get('entities'))\n        pred_value = ner_model(input_)\n        scorer.score(pred_value, gold)\n    return scorer.scores\nAnd here is the result I get -\n{\n    \"uas\": 0.0, \"las\": 0.0,\n    \"ents_p\": 90.66397423978736,\n    \"ents_r\": 90.26112736858471,\n    \"ents_f\": 90.46210231583312,\n    \"ents_per_type\": {\n                        \"ENTITY1\": {\n                                        \"p\": 90.15041861869855,\n                                        \"r\": 88.68202816927871,\n                                        \"f\": 89.4101949283755\n                                   },\n                        \"ENTITY2\":    {\n                                        \"p\": 82.64149214221386,\n                                        \"r\": 85.28465990316725,\n                                        \"f\": 83.94227421270608\n                                   },\n                        \"ENTITY3\": {\n                                        \"p\": 98.7231590403657,\n                                        \"r\": 98.44665796656699,\n                                        \"f\": 98.5847146278393\n                                    }, \n                        \"ENTITY4\": {\n                                        \"p\": 89.59285008653282,\n                                        \"r\": 88.74727538604445,\n                                        \"f\": 89.16805814934497\n                                    }, \n                        \"ENTITY5\": {\n                                        \"p\": 72.83978334689753,\n                                        \"r\": 75.61103504529963,\n                                        \"f\": 74.199542595769\n                                    }, \n                        \"ENTITY6\":{\n                                        \"p\": 88.18408901672848,\n                                        \"r\": 88.07075899606895,\n                                        \"f\": 88.12738757139074}\n                                    },\n    \"tags_acc\": 0.0,\n    \"token_acc\": 100.0\n}\nI want to understand how is the overall F Score of the model calculated? I thought that the overall F score may be the average of all individual entity F Score, but that is not the case. See screenshot below -\nCan someone help me better understand how is the overall F Score getting calculated?\nInfo about spaCy\nspaCy version: 2.1.6\nPlatform: Darwin-18.5.0-x86_64-i386-64bit\nPython version: 3.7.3\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-08-07T02:42:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "447": {"issue_url": "https://github.com/explosion/spaCy/issues/4088", "issue_id": "#4088", "issue_summary": "Move Thinc custom CUDA kernels into cupy to dump thinc-gpu-ops", "issue_description": "Member\nhonnibal commented on 6 Aug 2019\nThis is really a Thinc change, but posting it here for visibility.\nThinc currently contains some kernels implemented in .cu files, which live in the thinc-gpu-ops library. This library needs compilation, as we can't ship a wheel for it that works across both CPU and GPU.\nIt should be possible to instead move these kernels to be compiled by Cupy, by specifying the source as strings. At the same time, I'm sure the efficiency of the kernels can be improved (I did a pretty poor job of these, as I'm not an expert in CUDA). The only ones that might be difficult are the Murmurhash kernels.\nMoving these kernels back into Thinc and using the Cupy kernel functionality should allow us to remove the dependency on the thinc-gpu-ops sublibrary. This would greatly improve ease of GPU utilisation for Windows users, as currently the thinc-gpu-ops sublibrary requires a compiler to be installed.", "issue_status": "Closed", "issue_reporting_time": "2019-08-06T10:20:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "448": {"issue_url": "https://github.com/explosion/spaCy/issues/4087", "issue_id": "#4087", "issue_summary": "endless loop when Analyzing corpus...", "issue_description": "Contributor\nphiedulxp commented on 6 Aug 2019\nHow to reproduce the behaviour\nendless loop when Analyzing corpus...\n=========================== Data format validation ===========================\n\u2714 Loaded train.json\n\u2714 Loaded dev.json\n\u2714 Training data JSON format is valid\n\u2714 Development data JSON format is valid\n\u280f Analyzing corpus...\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.7\nPlatform: Windows-10-10.0.17134-SP0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-06T00:56:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "449": {"issue_url": "https://github.com/explosion/spaCy/issues/4086", "issue_id": "#4086", "issue_summary": "Quality of models in lower case", "issue_description": "Contributor\nBramVanroy commented on 5 Aug 2019\nI was doing some tests and found that the quality of spaCy's output drastically decreases when the input is not cased. Especially for named entity recognition that is true. For instance, when you try the sentence:\nI saw Obama and Trump shaking hands.\nCased, there aren't issues (even though Trump is marked NORP). But lower-cased, the model doesn't recognize any named entities. See here.\nThis begs the question: is spaCy only trained on cased data? If so, it might be useful to make this more explicit in the documentation.", "issue_status": "Closed", "issue_reporting_time": "2019-08-05T14:55:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "450": {"issue_url": "https://github.com/explosion/spaCy/issues/4083", "issue_id": "#4083", "issue_summary": "Converting data for Chinese dependency parsing", "issue_description": "Contributor\nphiedulxp commented on 5 Aug 2019 \u2022\nedited\nHow to reproduce the behaviour\npython -m spacy train en train.json dev.json\n    with skip some ill GoldParse:\n            spacy\\syntax\\nn_parser.pyx\n                begin_training\n                    try:\n                        gold_sample.append(GoldParse(doc_sample[-1], words=words, tags=tags,\n                                                    heads=heads, deps=deps, ents=ents))\n                    except Exception as e:\n                        doc_sample.pop()\nstill error:\nTraceback (most recent call last):\n  File \"spacy\\cli\\train.py\", line 248, in train\n    for batch in util.minibatch_by_words(train_docs, size=batch_sizes):\n  File \"spacy\\util.py\", line 532, in minibatch_by_words\n    doc, gold = next(items)\n  File \"gold.pyx\", line 217, in train_docs\n  File \"gold.pyx\", line 233, in iter_gold_docs\n  File \"gold.pyx\", line 253, in spacy.gold.GoldCorpus._make_golds\n  File \"gold.pyx\", line 451, in spacy.gold.GoldParse.from_annot_tuples\n  File \"gold.pyx\", line 599, in spacy.gold.GoldParse.__init__\nValueError: [E069] Invalid gold-standard parse tree. Found cycle between word IDs: {1362, 1363}\nIt seams that gold_sample handle sentence level, as Found cycle between word IDs: {0, 1} etc.\nand spacy.gold.GoldParse.from_annot_tuples handle doc level, as Found cycle between word IDs: {1362, 1363}\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.7\nPlatform: Windows-10-10.0.17134-SP0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-05T08:43:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "451": {"issue_url": "https://github.com/explosion/spaCy/issues/4076", "issue_id": "#4076", "issue_summary": "Cannot load BERT uncased", "issue_description": "kernelmachine commented on 3 Aug 2019\nHow to reproduce the behaviour\n$ python -m spacy download en_pytt_bertbaseuncased_lg\n\u2714 Download and installation successful\nYou can now load the model via spacy.load('en_pytt_bertbaseuncased_lg')\n$ python\nPython 3.7.3 (default, Mar 27 2019, 22:11:17) \n[GCC 7.3.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import spacy\n>>> spacy.load('en_pytt_bertbaseuncased_lg')\nTraceback (most recent call last):\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/site-packages/spacy/util.py\", line 70, in get_lang_class\n    module = importlib.import_module(\".lang.%s\" % lang, \"spacy\")\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'spacy.lang.pytt'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/site-packages/spacy/util.py\", line 134, in load_model\n    return load_model_from_package(name, **overrides)\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/site-packages/spacy/util.py\", line 155, in load_model_from_package\n    return cls.load(**overrides)\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/site-packages/en_pytt_bertbaseuncased_lg/__init__.py\", line 12, in load\n    return load_model_from_init_py(__file__, **overrides)\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/site-packages/spacy/util.py\", line 196, in load_model_from_init_py\n    return load_model_from_path(data_path, meta, **overrides)\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/site-packages/spacy/util.py\", line 166, in load_model_from_path\n    cls = get_lang_class(lang)\n  File \"/home/suching/miniconda/envs/allennlp/lib/python3.7/site-packages/spacy/util.py\", line 72, in get_lang_class\n    raise ImportError(Errors.E048.format(lang=lang, err=err))\nImportError: [E048] Can't import language pytt from spacy.lang: No module named 'spacy.lang.pytt'\nYour Environment\nspaCy version: 2.1.7\nPlatform: Linux-4.15.0-45-generic-x86_64-with-debian-buster-sid\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-03T14:43:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "452": {"issue_url": "https://github.com/explosion/spaCy/issues/4074", "issue_id": "#4074", "issue_summary": "\ud83d\udcab Raise error if annotation dict in simple training style has unexpected keys", "issue_description": "Member\nines commented on 3 Aug 2019\nThis is a really easy mistake to make and can lead to very frustrating debugging experiences. It just happened to me again and I've debugged people's training code in the past where this issue was the root cause of the model not learning anything.\nConsider the following example:\ntexts = [\"hello world\", \"this is a text\"]\ncats = [{\"LABEL\": True}, {\"LABEL\": False}]\nnlp.update(texts, cats)\nSpot the problem? It's here:\ntexts = [\"hello world\", \"this is a text\"]\n- cats = [{\"LABEL\": True}, {\"LABEL\": False}]\n+ cats = [{\"cats\": {\"LABEL\": True}}, {\"cats\": {\"LABEL\": False}}]\nnlp.update(texts, cats)\nRequiring a dict like that makes sense because it allows you to train multiple things at once (entities and text categories for instance). However, we currently do not raise an error if no expected top-level keys are present and instead just quietly ignore the additional keys. Which means that in the first example, the model would have been updated with nothing.\nInstead, spaCy should raise an error like \"Trying to call nlp.update with annotation type 'LABEL'. Expected top-level keys 'words', 'tags', 'heads', 'deps', 'entities' or 'cats'. Got: {\"LABEL\": True}.\"", "issue_status": "Closed", "issue_reporting_time": "2019-08-03T10:36:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "453": {"issue_url": "https://github.com/explosion/spaCy/issues/4073", "issue_id": "#4073", "issue_summary": "Missing sentencizer when loading custom word vectors", "issue_description": "ilyaivensky commented on 2 Aug 2019 \u2022\nedited\nI am trying to load nlp with fasttext vectors. I have initialized Spacy as it is suggested on https://spacy.io/usage/vectors-similarity and loaded nlp with the same path which I provided to spacy init-model. I successfully loaded vectors, but I was not able to iterate sentences since sentence boundaries were not set. The same code with Spacy native model works fine.\nIs there missing information in instructions on how to load custom vectors or an issue with the code?\nYour Environment\nOperating System: osx 10.13.6\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.6\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-08-02T17:16:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "454": {"issue_url": "https://github.com/explosion/spaCy/issues/4071", "issue_id": "#4071", "issue_summary": "`AttributeError: type object 'Callable' has no attribute '_abc_registry'` during spacy 2.1.7 update on python 3.7", "issue_description": "bastbnl commented on 2 Aug 2019\nHow to reproduce the problem\nTried updating to spacy 2.1.7 from a working installation containing spacy 2.1.6 on python 3.7.3.\n(env) bastb@bastb-vps:/var/www/brownpapersession/dev/brownpapersession$ TMPDIR=/var/tmp pip install -U --no-binary :all: spacy\n...\nCollecting spacy\n  Downloading https://files.pythonhosted.org/packages/f1/04/f25cdc3cb6d143ef397c23718026aff606c3e558cbd4939e9e4cb0a4b515/spacy-2.1.7.tar.gz (30.7MB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30.7MB 6.0MB/s\n  Installing build dependencies ... error\n  ERROR: Command errored out with exit status 1:\n   command: /var/www/brownpapersession/dev/env/bin/python3.7 /var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /var/tmp/pip-build-env-wkhtgtnf/overlay --no-warn-script-location --no-binary :all: --only-binary :none: -i https://pypi.org/simple -- setuptools 'wheel>0.32.0.<0.33.0' Cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=7.0.8,<7.1.0'\n       cwd: None\n  Complete output (24 lines):\n  Traceback (most recent call last):\n    File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip/__main__.py\", line 16, in <module>\n      from pip._internal import main as _main  # isort:skip # noqa\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip/_internal/__init__.py\", line 40, in <module>\n      from pip._internal.cli.autocompletion import autocomplete\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip/_internal/cli/autocompletion.py\", line 8, in <module>\n      from pip._internal.cli.main_parser import create_main_parser\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip/_internal/cli/main_parser.py\", line 7, in <module>\n      from pip._internal.cli import cmdoptions\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip/_internal/cli/cmdoptions.py\", line 24, in <module>\n      from pip._internal.models.search_scope import SearchScope\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip/_internal/models/search_scope.py\", line 11, in <module>\n      from pip._internal.utils.misc import normalize_path, redact_password_from_url\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip/_internal/utils/misc.py\", line 58, in <module>\n      from typing import cast, Tuple\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/typing.py\", line 1356, in <module>\n      class Callable(extra=collections_abc.Callable, metaclass=CallableMeta):\n    File \"/var/www/brownpapersession/dev/env/lib/python3.7/site-packages/typing.py\", line 1004, in __new__\n      self._abc_registry = extra._abc_registry\n  AttributeError: type object 'Callable' has no attribute '_abc_registry'\n  ----------------------------------------\nERROR: Command errored out with exit status 1: /var/www/brownpapersession/dev/env/bin/python3.7 /var/www/brownpapersession/dev/env/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /var/tmp/pip-build-env-wkhtgtnf/overlay --no-warn-script-location --no-binary :all: --only-binary :none: -i https://pypi.org/simple -- setuptools 'wheel>0.32.0.<0.33.0' Cython 'cymem>=2.0.2,<2.1.0' 'preshed>=2.0.1,<2.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=7.0.8,<7.1.0' Check the logs for full command output.\nYour Environment\n(env) bastb@bastb-vps:/var/www/brownpapersession/dev/brownpapersession$ python -m spacy info\n\n============================== Info about spaCy ==============================\n\nspaCy version    2.1.6\nLocation         /var/www/brownpapersession/dev/env/lib/python3.7/site-packages/spacy\nPlatform         Linux-4.9.0-7-amd64-x86_64-with-debian-9.9\nPython version   3.7.3\nModels           en, nl", "issue_status": "Closed", "issue_reporting_time": "2019-08-02T08:11:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "455": {"issue_url": "https://github.com/explosion/spaCy/issues/4070", "issue_id": "#4070", "issue_summary": "Improve handling of unsupported Matcher attributes", "issue_description": "Collaborator\nadrianeboyd commented on 2 Aug 2019\nFeature description\nI decided to separate this from #4069 (see also #4063 ) since it's not quite the same issue.\nWith Matcher, it's very confusing that \"bad\" parts of patterns (e.g., with attributes that aren't supported) are silently discarded and end up matching every token rather than no token.\n{'ASDF': True} shouldn't be equivalent to {}.\nHow slow is validation? Would it make sense to make validate=True the default? Are there simpler checks that could be added for when attributes are discarded that don't require full validation so that you could provide warnings or errors in these cases?\nI think that less confusing default behavior would be:\n{'ASDF': True} matches nothing\nthere are always warnings or errors when it is known that patterns will match nothing due to this kind of problem\n1", "issue_status": "Closed", "issue_reporting_time": "2019-08-02T08:11:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "456": {"issue_url": "https://github.com/explosion/spaCy/issues/4069", "issue_id": "#4069", "issue_summary": "Add pattern validation to EntityRuler", "issue_description": "Collaborator\nadrianeboyd commented on 2 Aug 2019\nFeature description\nI think EntityRuler should have a validate option (or something like validate_patterns, if you want to keep validate for something more local) that's passed to Matcher and PhraseMatcher. Otherwise it's easy to create buggy patterns with EntityRuler and there's no simple way to check.\nRelated: #4063\n3", "issue_status": "Closed", "issue_reporting_time": "2019-08-02T07:55:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "457": {"issue_url": "https://github.com/explosion/spaCy/issues/4068", "issue_id": "#4068", "issue_summary": "Invalid syntax in \"/spacy/lang/ko/__init__.py\" when calling spacy.blank(\"ko\") for Korean", "issue_description": "Contributor\nveer-bains commented on 2 Aug 2019\nHow to reproduce the behaviour\ncalling\nspacy.blank(\"ko\")\nproduces the following traceback:\n`File \"/usr/lib64/python2.7/site-packages/spacy/init.py\", line 31, in blank\nLangClass = util.get_lang_class(name)\nFile \"/usr/lib64/python2.7/site-packages/spacy/util.py\", line 70, in get_lang_class\nmodule = importlib.import_module(\".lang.%s\" % lang, \"spacy\")\nFile \"/usr/lib64/python2.7/importlib/init.py\", line 37, in import_module\nimport(name)\nFile \"/usr/lib64/python2.7/site-packages/spacy/lang/ko/init.py\", line 28\nsurface: str\n\n       ^\nSyntaxError: invalid syntax`\nThe same script has worked fine for me with other languages, and the same format of data(english, german, chinese, spanish, and a few others), so perhaps this may not be an error on my side.\nYour Environment\nOperating System: CentOS (\"Linux-3.10.0-957.21.3.el7.x86_64-x86_64-with-centos-7.6.1810-Core\")\nPython Version Used: 2.7\nspaCy Version Used: 2.1.6\nEnvironment Information: NIL (running script directly from terminal)", "issue_status": "Closed", "issue_reporting_time": "2019-08-02T06:15:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "458": {"issue_url": "https://github.com/explosion/spaCy/issues/4063", "issue_id": "#4063", "issue_summary": "Matcher and EntityRuler token patterns should use same language and use original Token attribute names", "issue_description": "johann-petrak commented on 1 Aug 2019\nThis is one of the most puzzling and not very well documented details about Spacy:\nthe Matcher apparently only allows to match a subset of token attributes and requires that instead of the original names of those attributes, special all-uppercase identifiers have to be used. Why? Why not allow to use all Token attributes and by their original name?\nThe entity ruler examples seem to indicate that lower case, original name attributes can be used. But the documentation does not really say much about this or if other features of the matcher like value ranges, operators and quantifiers can be used.\nIdeally the token matching in the entity ruler would work identically to the matcher and both would allow to use all Token attributes with their original name.", "issue_status": "Closed", "issue_reporting_time": "2019-08-01T11:40:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "459": {"issue_url": "https://github.com/explosion/spaCy/issues/4061", "issue_id": "#4061", "issue_summary": "Misidentifies the ner using goldparse", "issue_description": "bellamkondaprakash commented on 1 Aug 2019\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2019-08-01T10:11:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "460": {"issue_url": "https://github.com/explosion/spaCy/issues/4060", "issue_id": "#4060", "issue_summary": "Active Learning using uncertainty", "issue_description": "m-michalek commented on 1 Aug 2019\nIs it possible to get a certainty score for each token or sentence? I want to use spaCy in my bachelor thesis about active learning for named entity recognition. My first approach would be active learning with uncertainty sampling, but therefore I need my model to support me an indication about how certain it is with its predictions.\nIt looks like prodigy also uses uncertainty sampling so I guess technically it should be possible for spaCy to support some kind of certainty score.\nI couldn't find that kind of information in the official spaCy documentation.\nThank you for your great tool and any help!", "issue_status": "Closed", "issue_reporting_time": "2019-08-01T09:19:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "461": {"issue_url": "https://github.com/explosion/spaCy/issues/4059", "issue_id": "#4059", "issue_summary": "Cross Validation with Spacy for Named Entity Recognition", "issue_description": "iCHAIT commented on 1 Aug 2019\nHi,\nThis is not a bug, but I wanted to understand something that I could not find anywhere in the docs. Pardon me if this is stupid question.\nI am trying to train a custom NER Model on 50,000 million samples. I am using minibatch with 20 iterations for modeling. I want to understand if I should be using use Cross-Validation for more accurate out of sample accuracy. If yes then where should the cross-validation step take place? If no, then how do I split/distribute my training and testing data, since I am using annotations and 6 custom entities and it is hard to keep track of the percentages of annotated labels in each of training and test data since and evenly distribute it.\nHere is the code I am using for training -\ndef train_spacy(data, iterations):\n    TRAIN_DATA = data\n\n    # create blank Language class\n    nlp = spacy.blank('en')  \n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        nlp.add_pipe(ner, last=True)\n\n    # Add LABELS\n    for _, annotations in TRAIN_DATA:\n         for ent in annotations.get('entities'):\n            ner.add_label(ent[2])\n\n    # Get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n\n    # only train NER\n    with nlp.disable_pipes(*other_pipes):  \n        optimizer = nlp.begin_training()\n        for itn in range(iterations):\n            print(\"Starting iteration \" + str(itn))\n\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts, annotations, sgd=optimizer, \n                           drop=0.20,losses=losses)\n            print('Losses', losses)\n\n    return nlp\n\n\nif __name__ == \"__main__\":\n\n    # Train formatted data\n    model = train_spacy(data, 10)\nI think cross-validation step should take place somewhere inside the for loop for iterations but I am not sure. Can someone throw some light on how to use cross-validation with Spacy NER or whether is it not needed at all?\nMy Environment\nspaCy version: 2.1.6\nPlatform: Darwin-18.5.0-x86_64-i386-64bit\nPython version: 3.7.3\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-08-01T09:09:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "462": {"issue_url": "https://github.com/explosion/spaCy/issues/4058", "issue_id": "#4058", "issue_summary": "AttributeError: 'HashEmbed' object has no attribute 'G'", "issue_description": "damianoporta commented on 1 Aug 2019 \u2022\nedited\nHello,\ni am training a new NER model with the code:\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    # reset and initialize the weights randomly \u2013 but only if we're\n    # training a new model\n    nlp.begin_training(component_cfg={\"ner\": {\"conv_depth\": 8}})\n\n    for itn in range(N_ITER):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(\n                texts,  # batch of texts\n                annotations,  # batch of annotations\n                drop=0.2,  # dropout - make it harder to memorise data\n                losses=losses,\n            )\ni only have changed the conv_depth setting as said by @honnibal here: #3798\nThen, loading the model i get:\n>>> nlp = spacy.load('/home/nlp/0')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py\", line 133, in load_model\n    return load_model_from_path(Path(name), **overrides)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py\", line 173, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/language.py\", line 791, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py\", line 630, in from_disk\n    reader(path / key)\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/language.py\", line 787, in <lambda>\n    deserializers[name] = lambda p, proc=proc: proc.from_disk(p, exclude=[\"vocab\"])\n  File \"nn_parser.pyx\", line 634, in spacy.syntax.nn_parser.Parser.from_disk\n  File \"/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/thinc/neural/_classes/model.py\", line 371, in from_bytes\n    dest = getattr(layer, name)\nAttributeError: 'HashEmbed' object has no attribute 'G'\nWhat is this problem?\nYour Environment\n(.prodigy-1.8.3) root@damiano:/home/nlp# python -m spacy info\n\n============================== Info about spaCy ==============================\n\nspaCy version    2.1.4                         \nLocation         /home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy\nPlatform         Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic\nPython version   3.6.7                         \nModels", "issue_status": "Closed", "issue_reporting_time": "2019-08-01T08:53:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "463": {"issue_url": "https://github.com/explosion/spaCy/issues/4056", "issue_id": "#4056", "issue_summary": "Support for merging noun chunks excluding entities", "issue_description": "prashantbudania commented on 1 Aug 2019\nHow to reproduce the behaviour\nI am using the filter_spans util function to take care of the overlapping spans. But the filter_spans prefers longer spans and sometimes entity information gets destroyed as a result of this. For ex:\nimport spacy \nfrom spacy import displacy\nfrom spacy.util import filter_spans\n\nnlp = spacy.load('en_2.1.0_md')\ndoc = nlp(\"This market improvement drove a $4 billion decline in Asset Backed Finance.\")\n\nprint(doc.ents)\n# ($4 billion, Asset Backed Finance)\n\nspans = list(doc.ents) + list(doc.noun_chunks)\nspans = filter_spans(spans)\n\nwith doc.retokenize() as retokenizer:\n    for span in spans:\n        retokenizer.merge(span)\n\nprint(doc.ents)\n# (Asset Backed Finance,)\nManually editing the filter_spans function and setting reverse=False doesn't work for this example.\nUsing merge_ents and merge_noun_chunks pipelines doesn't work either.\nFor relation extraction, it's important to merge noun chunks but also not destroy entity information. Can we add support for merging noun chunks only when there are no entity tokens present in the noun chunk (while still keeping the default behavior i.e. merge all noun chunks)? For ex:\nnlp = spacy.load(\"en_2.1.0_md\")\n\n# first merge all entities\nmerge_ents = nlp.create_pipe(\"merge_entities\")\nnlp.add_pipe(merge_ents)\n\ndoc = nlp(\"This market improvement drove a $4 billion decline in Asset Backed Finance.\")\n\ndef merge_noun_chunks(doc, exclude_entities=False):\n    with doc.retokenize() as re:\n        for span in doc.noun_chunks:\n            if not (exclude_entities and any([t for t in span if t.ent_type_])):\n                re.merge(span, attrs={\"POS\": span.root.pos_, \"TAG\": span.root.tag_})\n    return doc\n\n# now merge all noun phrases which do not overlap with entities:\ndoc = merge_noun_chunks(doc, exclude_entities=True)\n\nprint(doc.ents)\nYour Environment\nOperating System: MacOS\nPython Version Used: 3.6\nspaCy Version Used: 2.1", "issue_status": "Closed", "issue_reporting_time": "2019-07-31T22:22:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "464": {"issue_url": "https://github.com/explosion/spaCy/issues/4055", "issue_id": "#4055", "issue_summary": "Textcat reproducibility across platform", "issue_description": "easonla commented on 1 Aug 2019 \u2022\nedited\nI am trying to exercise intent classifier data with spacy example code with minimal modification. Similar to previous solved issue, I've set\nrandom.seed(0)\nnp.random.seed(0)\nand did not use gpu training. From my experiments, I was able to recreate identical model with the the same machine. But failed to reproduce results across different platform even with exact same code, same environment, same glove vector and random number seed. I've test it on Ubuntu 18.04, Mac 10.14.5 and docker ubuntu:latest with python 3.6, spacy 2.0.11, thinc 6.10.1. Also tested on spacy 2.1.6 and thinc 7.0.8 with other pretrained vector (en-web-small) but nothing different, the model cannot be reproduce across platform/ machine. The model performance is comparable just I expect to get exact the same model with everything controlled.\n# Load the data (training data for utterance to intent mappings) - internal count\nimport sys\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport spacy\nfrom spacy.util import minibatch\nfrom pathlib import Path\nrandom.seed(0)\nnp.random.seed(0)\n\ntraining = pd.read_csv('data/train.csv', keep_default_na=False, na_values=[\"\"]\n                       , usecols = ['utterance', 'intent', 'weight'], encoding=\"latin-1\")\n#Sort the input by intent alphabetically then index\ntraining = training.reset_index().sort_values(['intent','index']).drop(columns='index').reset_index(drop=True)\n\n#testing_new = pd.read_csv('data/test.csv', usecols = ['utterance', 'intent'])\n\n# Create duplicate rows based on the weight column \ntraining = pd.DataFrame(np.repeat(training.values, training['weight'].values, axis=0)\n                        , columns=['utterance', 'intent', 'weight'])[['utterance', 'intent']]\n\n\nprint(\"Data read in\")\nsys.stdout.flush()\n\n# Load NLP model\nnlp1 = spacy.load('en')\n# Add text classification to pipeline\ntextcat = nlp1.create_pipe('textcat')\nnlp1.add_pipe(textcat, last=True)\n\nprint(\"Spacy and model loaded\")\nsys.stdout.flush()\n\n# Get unique intents to add to possible classes\nintents_unique = training['intent'].unique()\nintents_unique = [str(\"\".join(i for i in a if ord(i)<128)) for a in intents_unique]\nfor intent in intents_unique:\n    textcat.add_label(intent)\n\n\n# Process data for modeling\ntraining_raw = [tuple(x) for x in training.to_records(index=False)]\ntraining_raw = [(str(\"\".join(i for i in a if ord(i)<128)), \n                 str(\"\".join(j for j in b if ord(j)<128))) \n                for a,b in training_raw]\n\n\n# Construct training data and convert to unicode for input into spaCy\ntexts, labels = zip(*training_raw)\ncats = []\nintent_cat_dict = {}\nfor intent in intents_unique:\n    intent_cat_dict[intent] = {}\n    for intent1 in intents_unique:\n        if intent == intent1:\n            intent_cat_dict[intent][intent1] = 1\n        else:\n            intent_cat_dict[intent][intent1] = 0\nfor label in labels:\n    cats.append({'cats': intent_cat_dict[label]})\ntrain_data = list(zip(texts, cats))\n\nrandom.shuffle(train_data)\ntexts, _ = zip(*train_data)\n\n# Import spaCy utilities needed and get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp1.pipe_names if pipe != 'textcat']\n\nprint(\"Data processed and ready for modeling\")\nsys.stdout.flush()\n\nstart = time.time()\n# Train categorization model\nn_iter = 5\nwith nlp1.disable_pipes(*other_pipes):\n    optimizer = nlp1.begin_training()\n    for i in range(n_iter):\n        losses = {}\n        batches = minibatch(train_data, size=256)\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp1.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n        current_time = time.time()\n        time_esclape = current_time - start\n        print(i, losses['textcat'], time_esclape)\n        sys.stdout.flush()\n\n\nprint(\"Modeling finished\")\nsys.stdout.flush()\n\n# Output model to disk\noutput_dir = 'model/output_intent_clf_docker'\noutput_dir = Path(output_dir)\nif not output_dir.exists():\n    output_dir.mkdir()\nnlp1.to_disk(output_dir)\nprint(\"Saved model to\", output_dir)\n\nprint(\"Output to disk finished\")\nsys.stdout.flush()\nYour Environment\nOperating System: Mac 10.14.5/ Ubuntu 18.04 / Docker ubuntu:latest runs on Mac\nPython Version Used: 3.6\nspaCy Version Used: 2.0.11\nEnvironment Information:\nabsl-py==0.7.1\naniso8601==6.0.0\nastor==0.7.1\nClick==7.0\ncymem==1.31.2\ncytoolz==0.8.2\ndill==0.2.9\nduckling==1.8.0\nFlask==1.0.2\nFlask-RESTful==0.3.6\ngast==0.2.2\ngevent==1.2.2\ngreenlet==0.4.15\ngrpcio==1.19.0\ngunicorn==19.8.1\nh5py==2.9.0\nitsdangerous==1.1.0\nJinja2==2.10.1\nJPype1==0.6.3\njsonschema==2.6.0\nKeras-Applications==1.0.7\nKeras-Preprocessing==1.0.9\nMarkdown==3.1\nMarkupSafe==1.1.1\nmock==2.0.0\nmsgpack==0.6.1\nmsgpack-numpy==0.4.3.2\nmsgpack-python==0.5.6\nmurmurhash==0.28.0\nnltk==3.3\nnumpy==1.16.2\npandas==0.23.4\npathlib==1.0.1\npbr==5.1.3\nplac==0.9.6\npreshed==1.0.1\nprotobuf==3.7.1\npython-dateutil==2.8.0\npytz==2019.1\nregex==2017.4.5\nsix==1.12.0\nspacy==2.0.11\ntensorboard==1.13.1\ntensorflow==1.13.1\ntensorflow-estimator==1.13.0\ntermcolor==1.1.0\nthinc==6.10.1\ntoolz==0.9.0\ntqdm==4.31.1\nujson==1.35\nWerkzeug==0.15.1\nwrapt==1.11.1\n1", "issue_status": "Closed", "issue_reporting_time": "2019-07-31T19:53:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "465": {"issue_url": "https://github.com/explosion/spaCy/issues/4054", "issue_id": "#4054", "issue_summary": "No valid 'lang' when creating blank model with vocab from file", "issue_description": "Member\nsvlandeg commented on 1 Aug 2019\nHow to reproduce the behaviour\nPreviously, I saved en_core_web_lg to file, creating a vocab subdirectory, and its meta.json which reads (among other things)\n\"lang\":\"en\",\n\"name\":\"core_web_lg\"\nNow, I'm attempting to use the vocab subdir as source for a blank model:\nvocab = Vocab().from_disk(vocab_dir)\nnlp = spacy.blank(\"en\", vocab=vocab)\nprint(nlp(\"This is a test sentence\"))\nnlp.to_disk(output_dir)\nnlp2 = spacy.load(output_dir)\nprint(nlp2(\"This is another test sentence\"))\nWhich fails, giving the error ValueError: [E054] No valid 'lang' setting found in model meta.json.\nAnd indeed, the meta.json of the new nlp object reads\n\"lang\":\"\"\nIs this expected behaviour? When I run this in a unit test and replace the second line with\nnlp = spacy.blank(\"en\", vocab=en_vocab)\nit does work correctly.\nYour Environment\nspaCy version: 2.1.6\nPlatform: Windows-10-10.0.17763-SP0\nPython version: 3.6.7", "issue_status": "Closed", "issue_reporting_time": "2019-07-31T19:01:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "466": {"issue_url": "https://github.com/explosion/spaCy/issues/4053", "issue_id": "#4053", "issue_summary": "Using model en-1.1.0 in Spacy 2", "issue_description": "yuvalkry commented on 31 Jul 2019\nHello,\nI am migrating to Python 3 and would like to use the latest spacy with model en-1.1.0 .\nAs explained in https://pypi.org/project/spacy/ (\"Support for older versions\")\nI unpacked en-1.1.0.tar.gz into spacy/data . I then used\nspacy.blank(\"en\").from_disk('.../lib/python3.7/site-packages/spacy/data/en-1.1.0')\n(This is a conda environment)\nand got error messages - that \"init.py\" was missing.\nI this possible at all to use this model?\nIf not, which of the new models supercedes it?\nThanks,\nYuval\nspacy validate\nInfo about spaCy\nspaCy version: 2.1.6\nPlatform: Linux-3.10.0-327.3.1.el7.x86_64-x86_64-with-centos-7.2.1511-Core\nPython version: 3.7.3\nModels: en-1.1.0\n(", "issue_status": "Closed", "issue_reporting_time": "2019-07-31T14:18:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "467": {"issue_url": "https://github.com/explosion/spaCy/issues/4051", "issue_id": "#4051", "issue_summary": "Retokenizer not setting POS tags correctly", "issue_description": "prashantbudania commented on 31 Jul 2019\nHow to reproduce the behaviour\nI am trying to merge entities and noun chunks for a custom application without having to use merge_entities or merge_noun_chunks pipeline. But the pos tag information is inaccurate. Here's one example:\nimport spacy \nfrom spacy import displacy\nfrom spacy.util import filter_spans\n\nnlp = spacy.load('en_2.1.0_md')\ndoc = nlp(\"While our net charge-off rate improved from a year ago, our provision expense increased due to a $150 million reserve build in the first quarter of 2019 compared with a $550 million reserve release a year ago.\")\n\nspans = list(doc.ents) + list(doc.noun_chunks)\nspans = filter_spans(spans)\n\nwith doc.retokenize() as retokenizer:\n    for span in spans:\n        retokenizer.merge(span)\n\nfor i, tkn in enumerate(doc):\n    print(\"{:<2}{:<2}{:<32}{:<2}{:<8}{:<2}{:<8}{:<2}{:<2}\".format(i, '|', tkn.text, '|', tkn.pos_, '|', tkn.dep_, '|', tkn.ent_type_))\nBut if I specifically set the pos attribute to be the same as the root's attribute, it works fine:\nwith doc.retokenize() as retokenizer:\n    for span in spans:\n        retokenizer.merge(span, attrs={\"POS\": span.root.pos_})\nI remember this being the default behaviour in 2.0. And also, even after the fix, displacy would show the incorrect pos tag information:\ndisplacy.render(doc, style='dep')\nYour Environment\nOperating System: MacOS\nPython Version Used: 3.6\nspaCy Version Used: 2.1.4", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T22:46:02Z", "fixed_by": "#4219", "pull_request_summary": "Modify retokenizer to use span root attributes", "pull_request_description": "Collaborator\nadrianeboyd commented on 30 Aug 2019 \u2022\nedited\nDescription\ntag/pos/morph are set to root tag/pos/morph initially (can be overridden by attrs later)\nlemma and norm are reset and end up as orth (not ideal, but better than orth of first token)\nFixes #4051.\nTypes of change\nBugfix.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-08T11:04:50Z", "files_changed": [["100", "spacy/tests/doc/test_retokenize_merge.py"], ["140", "spacy/tokens/_retokenize.pyx"]]}, "468": {"issue_url": "https://github.com/explosion/spaCy/issues/4048", "issue_id": "#4048", "issue_summary": "Init-model doeesn't work for some languages", "issue_description": "Xargonus commented on 30 Jul 2019 \u2022\nedited\nInit-model doesn't work for some languages with less features. So far I have found that cs and bg don't work, but there are probably more. The problem is I think init-model expecting non-empty vocab.\nError\npython -m spacy init-model bg test\n\ufffd[2K\u2714 Counted frequencies\n\ufffd[2K\u2714 Read clusters\n0it [00:00, ?it/s]\nTraceback (most recent call last):\nFile \"C:\\Users\\xargo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"C:\\Users\\xargo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\runpy.py\", line 85, in run_code\nexec(code, run_globals)\nFile \"C:\\Users\\xargo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\spacy_main.py\", line 35, in\nplac.call(commands[command], sys.argv[1:])\nFile \"C:\\Users\\xargo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\plac_core.py\", line 328, in call\ncmd, result = parser.consume(arglist)\nFile \"C:\\Users\\xargo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\plac_core.py\", line 207, in consume\nreturn cmd, self.func(*(args + varargs + extraopts), **kwargs)\nFile \"C:\\Users\\xargo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\spacy\\cli\\init_model.py\", line 78, in init_model\nnlp = create_model(lang, lex_attrs)\nFile \"C:\\Users\\xargo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\spacy\\cli\\init_model.py\", line 146, in create_model\noov_prob = min(lex.prob for lex in nlp.vocab)\nValueError: min() arg is an empty sequence\nHow to reproduce the behaviour\npython -m spacy init-model cs test\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.6.7\nspaCy Version Used: 2.1.6\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T11:04:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "469": {"issue_url": "https://github.com/explosion/spaCy/issues/4047", "issue_id": "#4047", "issue_summary": "Tokens with ',\"' in between are being mistaken as one token", "issue_description": "zeeshanalipnhwr commented on 30 Jul 2019\nHow to reproduce the behaviour\nsentence = \"\"\"John said,\"There's an elephant outside the window.\".\"\"\"\nfor token in nlp(sentence): print (token.text, token.tag_)\n...\nJohn NNP\nsaid,\"There NNP\n's VBZ\nan DT\nelephant NN\noutside IN\nthe DT\nwindow NN\n. .\n\" ''\n. .\nYour Environment\nOperating System:\nPython Version Used: 3.xx\nspaCy Version Used: 2.1.6\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T10:12:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "470": {"issue_url": "https://github.com/explosion/spaCy/issues/4046", "issue_id": "#4046", "issue_summary": "Installation error for \"pip install -U spacy[cuda]\" on Ubuntu 18.04, CUDA 10.1, cuDNN 7.5, cupy for CUDA 10.1", "issue_description": "tomtel commented on 30 Jul 2019\nHow to reproduce the problem\n# copy-paste the error message here\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T08:30:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "471": {"issue_url": "https://github.com/explosion/spaCy/issues/4045", "issue_id": "#4045", "issue_summary": "spaCy2.0.12 doesn't have span.ents", "issue_description": "wli-chegg commented on 30 Jul 2019\nHow to reproduce the behaviour\nspaCy 2.0.12 didn't have span.ent as you claimed, it's a bit inconvenient because you have to go back to the doc object to match ents.\nimport spaCy nlp = spacy.load(\"en_core_web_md\" doc = nlp('some random text. test sentences 2.') for sent in doc.sents: print(sent.ents)\nYour Environment\nOperating System: mac os\nPython Version Used: Python3.6\nspaCy Version Used: 2.0.12\nEnvironment Information: virtual environment", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T01:22:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "472": {"issue_url": "https://github.com/explosion/spaCy/issues/4044", "issue_id": "#4044", "issue_summary": "Using Matcher() in conjunction with regex returns strings longer than what the regex is supposed to match", "issue_description": "mrxiaohe commented on 30 Jul 2019\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.4\nEnvironment Information:\nI am trying to capture some number sequences. Below is the code I use:\nfrom spacy.lang.en import English\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Span\n\nnlp = English()\nmatcher = Matcher(nlp.vocab)\n\ndef add_event_ent(matcher, doc, i, matches):\n    match_id, start, end = matches[i]\n    entity = Span(doc, start, end, label=\"EVENT\")\n    ents = [ent for ent in doc.ents if ent.label_ != 'NumberSequence']\n    doc.ents += (entity,)\n    print(entity.text)\n\n#pattern = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\npattern = [{'ORTH': {\"REGEX\": r\"\\d{3}\"}},\n           {\"ORTH\": \"-\"}, \n           {'ORTH': {\"REGEX\": r\"\\d{4}\"}}, \n           {\"ORTH\": \"-\"}, \n           {'ORTH': {\"REGEX\": r\"\\d{4}\"}}]\n\nmatcher.add(\"NumberSequence\", add_event_ent, pattern)\ndoc = nlp(u\"123-3433-2234; -123-3433-2234; 123-3433-2234-\")\nmatches = matcher(doc)\nThe actual regex pattern I use is more complicated than what's shown above, but i am using a simplified regex for the sake of simplicity. The regex above returns the following matches., Crucially, I only want the first one to be matched. I wonder if there is a way to achieve that. Thanks.\n123-3433-2234\n-123-3433-2234\n123-3433-2234-", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T00:12:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "473": {"issue_url": "https://github.com/explosion/spaCy/issues/4043", "issue_id": "#4043", "issue_summary": "Problem training a new model TypeError: Could not determine the signature of None", "issue_description": "DanielFLopez commented on 30 Jul 2019\nProblem when I'm trying to train a model\nI have this error when trying to run the algorithm https://spacy.io/usage/examples#training-ner to train a new model.\nTraceback (most recent call last):\n  File \"train.py\", line 120, in <module>\n    plac.call(main('en_core_web_sm', Path.cwd(), 100))\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plac_core.py\", line 324, in call\n    parser = parser_from(obj)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plac_core.py\", line 133, in parser_from\n    parser.populate_from(obj)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plac_core.py\", line 248, in populate_from\n    self._set_func_argspec(func)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plac_core.py\", line 240, in _set_func_argspec\n    self.argspec = getargspec(obj)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\plac_core.py\", line 38, in getargspec\n    str(callableobj))\nTypeError: Could not determine the signature of None\nJust add the base model and the path in the main()\nif __name__ == \"__main__\":\n    plac.call(main('en_core_web_sm', Path.cwd(), 100))\nEnvironment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.6", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T20:27:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "474": {"issue_url": "https://github.com/explosion/spaCy/issues/4042", "issue_id": "#4042", "issue_summary": "Adding EntityRuler before ner and saving model to disk crashes loading the model", "issue_description": "psychosis448 commented on 29 Jul 2019\nHow to reproduce the behaviour\nI got a strange problem when loading a modified model, adding an EnityRuler to the pipeline with the before='ner' flag.\nSo I create my patterns, add them to my entity_ruler and the ruler to my pipe:\nnlp = spacy.load('de_core_news_sm')\nruler = EntityRuler(nlp)\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler, before='ner')\nThen I check for the dir, create it if necessary and save the model to disk:\noutput_dir = Path('custom_model')\nif not output_dir.exists():\n  output_dir.mkdir()\nnlp.to_disk(output_dir)\nWhen I now load the model with\nspacy.load(Path('custom_model'))\nIt throws the following error:\nValueError: [E109] Model for component 'ner' not initialized.\nDid you forget to load a model, or forget to call begin_training()?\nThere is no problem doing this without before='ner'...\nExtended Error:\nTraceback (most recent call last):\n  File \"/spacy/__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"/spacy/util.py\", line 135, in load_model\n    return load_model_from_path(name, **overrides)\n  File \"/spacy/util.py\", line 173, in load_model_from_path\n    return nlp.from_disk(model_path)\n  File \"/spacy/language.py\", line 791, in from_disk\n    util.from_disk(path, deserializers, exclude)\n  File \"/spacy/util.py\", line 630, in from_disk\n    reader(path / key)\n  File \"/spacy/language.py\", line 787, in <lambda>\n    deserializers[name] = lambda p, proc=proc: proc.from_disk(p, exclude=[\"vocab\"])\n  File \"/spacy/pipeline/entityruler.py\", line 183, in from_disk\n    self.add_patterns(patterns)\n  File \"/spacy/pipeline/entityruler.py\", line 138, in add_patterns\n    self.phrase_patterns[label].append(self.nlp(pattern))\n  File \"/spacy/language.py\", line 390, in __call__\n    doc = proc(doc, **component_cfg.get(name, {}))\n  File \"nn_parser.pyx\", line 205, in spacy.syntax.nn_parser.Parser.__call__\n  File \"nn_parser.pyx\", line 238, in spacy.syntax.nn_parser.Parser.predict\n  File \"nn_parser.pyx\", line 235, in spacy.syntax.nn_parser.Parser.require_model\nValueError: [E109] Model for component 'ner' not initialized.\nDid you forget to load a model, or forget to call begin_training()?\nYour Environment\nspaCy version: 2.1.6\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.3\n2", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T17:57:52Z", "fixed_by": "#4330", "pull_request_summary": "Ensure the NER remains consistent after resizing", "pull_request_description": "Member\nsvlandeg commented on 27 Sep 2019 \u2022\nedited\nDescription\nWhen the ner got resized through an add_action(), the nn_parser would have an outdated field nr_class sitting in its cfg. So then when you write to disk this resized NER, it would throw an error about invalid dimensions when reading it back in, because it was using the outdated nr_class value.\nThe solution is to ensure that the resizing is done consistently by always calling the self._resize() method that updates both the model(s) as well as the nr_class variable. (it's still not a very clean solution ...)\nI encountered this behaviour first in Issue #4042, which actually harboured two bugs. The one described above was the second bug and can be tested in isolation with the unit test test_issue4042_bug2().\nI took the liberty of fixing the first bug of that issue too, which is a workaround for applying nlp() within the EntityLinker by disabling components later on in the pipeline (as suggested by @ines). I know that ideally we shouldn't use this pattern, but at least this is a quick fix, and the combination effectively closes #4042.\nI started to look into the internal behaviour of the NER because of Issue #4313 , which I feel should be related. I wrote a unit test for this issue, but it unfortunately still crashes so not quite there yet. I put it on skip for now so we could merge this at least, and continue the search. Removed that unit test, because it uses an external lib.\n[UPDATE:] also fixes #4313 with Matt's suggestion.\nTypes of change\nbug fix\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-27T18:57:14Z", "files_changed": [["37", "spacy/pipeline/entityruler.py"], ["21", "spacy/syntax/nn_parser.pyx"], ["7", "spacy/syntax/transition_system.pyx"], ["83", "spacy/tests/regression/test_issue4042.py"], ["39", "spacy/tests/regression/test_issue4313.py"]]}, "475": {"issue_url": "https://github.com/explosion/spaCy/issues/4041", "issue_id": "#4041", "issue_summary": "Got lower scores using custom word embeddings - what does this mean?", "issue_description": "erotavlas commented on 29 Jul 2019 \u2022\nedited\nusing genism I created custom word vectors Word2Vec(vocab=28964, size=100, alpha=0.025) using a larger corpus from which my NER training dataset was derived. (In case it's relevant, I lower cased all the words and removed words with numerics and special characters before training the word2vec model )\nThen I trained my NER model using an empty English model en and the vectors I created and I got the following scores\nFinal model average score... (blank en model - with custom word vectors)\nRecall: 69.63350785340315\nPrecision: 77.32558139534885\nFScore: 73.27823691460055\nTraining an NER model with no word embeddings - just the blank English model en and I got the following scores\nFinal model average score... (Blank en model)\nRecall: 73.82198952879581\nPrecision: 83.67952522255193\nFScore: 78.44228094575799\nWhat does this mean? I didn't expect the score to drop so much after introducing my custom word embeddings.", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T17:23:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "476": {"issue_url": "https://github.com/explosion/spaCy/issues/4040", "issue_id": "#4040", "issue_summary": "Can't find model 'en_model.vectors'.", "issue_description": "erotavlas commented on 29 Jul 2019 \u2022\nedited\nspacy version 2.1.6\nI trained multiple spacy models which completed without error. But when I tried to reload the models I get the following error\nOSError: [E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nWhen training I used the en_core_web_lg model to start and added the en_vectors_web_lg on top of that using\nspacy.load(voc, vocab=nlp.vocab)\nwhere voc is en_vectors_web_lg", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T14:02:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "477": {"issue_url": "https://github.com/explosion/spaCy/issues/4039", "issue_id": "#4039", "issue_summary": "Support custom features for NER training", "issue_description": "bratao commented on 29 Jul 2019\nFeature description\nMy task is to extract/segment information from a semi structured text. But not only the text is important some \"external features\" are also important.\nFor example, imagine that I want to segment this text about GitHub projects in Category, Project name, URl and description.\nI utilize an BIO scheme to tag each html token as a category.\nWhere\ntoken=NLP start_of_p=True bold=True center=True B-Category\ntoken=Projects start_of_p=False bold=True center=True I-Category\ntoken=Project start_of_p=True bold=True center=False B-Project-name\ntoken=Name start_of_p=False bold=True center=False I-Project-name\ntoken=: start_of_p=False bold=False center=False I-Project-name\nThe final result is something like:\nPay attention that some features are important such as: Text formatting (italic, bold, centered), position in text and more...\nThis would be a scenario where Spacy can work with?\nI can see a lot of moviment around Tok2Vec. But looking at\nspaCy/spacy/_ml.py\nLine 322 in 87fcf31\n def Tok2Vec(width, embed_size, **kwargs): \n, apparently it still glove | norm | prefix | suffix | shape only.\nThere is anyway of using those custom external features for training Spacy?\nThank you!", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T12:30:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "478": {"issue_url": "https://github.com/explosion/spaCy/issues/4038", "issue_id": "#4038", "issue_summary": "Proposal: Add textcat to spacy train CLI", "issue_description": "Collaborator\nadrianeboyd commented on 29 Jul 2019\nIt would be useful for spacy train to support textcat components. Some of the main questions are:\nHow to extend the current JSON training format\nHow to provide model settings to spacy train\nHow to score textcat results\nHow to provide textcat results to users (especially for multilabel tasks)\nI think it would make sense to adopt slightly different terminology for mutually exclusive vs. non-mutually exclusive classes (as in https://spacy.io/api/textcategorizer#init) and instead call them:\nmutually exclusive -> multiclass\nnon-mutually exclusive -> multilabel\nJSON Training Format\nThe format currently looks like this:\n[{\n    \"id\": int,                      # ID of the document within the corpus\n    \"paragraphs\": [{                # list of paragraphs in the corpus\n        \"raw\": string,              # raw text of the paragraph\n        \"sentences\": [{             # list of sentences in the paragraph\n            \"tokens\": [{            # list of tokens in the sentence\n                \"id\": int,          # index of the token in the document\n                \"dep\": string,      # dependency label\n                \"head\": int,        # offset of token head relative to token index\n                \"tag\": string,      # part-of-speech tag\n                \"orth\": string,     # verbatim text of the token\n                \"ner\": string       # BILUO label, e.g. \"O\" or \"B-ORG\"\n            }],\n            \"brackets\": [{          # phrase structure (NOT USED by current models)\n                \"first\": int,       # index of first token\n                \"last\": int,        # index of last token\n                \"label\": string     # phrase label\n            }]\n        }]\n    }]\n}]\ncats could be added at the document level like this:\n[{\n    \"id\": int,                      # ID of the document within the corpus\n    \"paragraphs\": [{                # list of paragraphs in the corpus\n        \"raw\": string,              # raw text of the paragraph\n        \"sentences\": [{             # list of sentences in the paragraph\n            \"tokens\": [{            # list of tokens in the sentence\n                \"id\": int,          # index of the token in the document\n                \"dep\": string,      # dependency label\n                \"head\": int,        # offset of token head relative to token index\n                \"tag\": string,      # part-of-speech tag\n                \"orth\": string,     # verbatim text of the token\n                \"ner\": string       # BILUO label, e.g. \"O\" or \"B-ORG\"\n            }],\n            \"brackets\": [{          # phrase structure (NOT USED by current models)\n                \"first\": int,       # index of first token\n                \"last\": int,        # index of last token\n                \"label\": string     # phrase label\n            }]\n        }]\n    }],\n    \"cats\": [{\n        \"label\": string,\n        \"value\": number\n    }]\n}]\nWith the data spread across paragraphs, I don't think it makes sense to try to support subdocument textcats with character offsets (the (start, end, label) keys supported in GoldParse, see https://spacy.io/api/goldparse#attributes), but if you wanted to, it could easily be extended like this:\n\"cats\": [{\n    \"label\": string,\n    \"value\": number,\n    \"start\": int,\n    \"end\": int\n}]\nIf you were extremely sure that you did not want to support subdocument textcats, then a slightly simpler version would be with the labels as keys:\n\"cats\": {\n    string: number,\n    string: number,\n    ...\n}\nThis simple version is what is proposed in #2928, but I think it would be better for it to be a list so that it is more like the other types of annotation and so that it is more extensible.\nJoining Paragraphs\nWith the current paragraph-based format, you would need to decide how to join paragraphs for training purposes. Something like \\n\\n? Should this be a language-specific setting?\nModel/Task Information\nThe following information is needed in order to initialize the model. It could be included directly as command-line options to spacy train or in a separate JSON file (e.g., --meta meta.json):\n{\n    \"labels\": [string, ... ]    # list of all labels\n    \"type\": string,             # multiclass vs. multilabel (default: multiclass)\n    \"sparse\": boolean,          # true: missing labels are 0.0, false: missing labels are None\n                                #    (default: false)\n}\nI think for most typical use cases with spacy train, this information could be automatically detected. This isn't true for general-purpose textcat training where you might not able to make an initial pass through all your data , but I think it might be okay to simplify this for spacy train and have spacy train autodetect these settings. Users should be able to override the autodetected settings with command-line options if needed.\nAutodetection of Model Settings\nIf training a new model, autodetect would examine the training data:\nlabels: all labels present in the training data\ntype: multiclass if each text has exactly one positive label\nsparse: True if all labels are not present on all texts in the data\nIf extending an existing model:\nlabels: union of all labels in the model and training data\ntype: multiclass vs. multilabel would be detected from the existing model\nsparse: True if all labels are not present on all texts in the training data (?)\nBinary Tasks\nbinary can be represented as either one-class multilabel or two-class multiclass. With one-class multilabel the positive label would be the one provided label, but with two-class multiclass you'd need to know the positive label to provide a better evaluation. This could potentially be added to the info in meta.json or as a command-line option.\nGoldParse / GoldCorpus\nGoldParse supports the textcat annotations as .cats.\ngold.json_to_tuple() would need to be updated to read in the cats information for GoldParse/GoldCorpus.\n(Are the jsonl and msg file input options in spacy.gold just a sketch at this point?)\nScorer\nThere are multiple options for scoring. I think some kind of precision/recall/f-scores are probably okay for most use cases, but feedback/suggestions are welcome.\nThe main question with f-scores is how to average across labels/instances, especially for multilabel tasks. If I had to pick one option, I might pick the weighted macro average as described here as weighted: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html, but weighted scores get tricky and possibly a macro average would be more straightforward to interpret for a typical user, especially if the per-label scores are also easy to inspect.\nmicro and macro averaging would be supported by PRFScore and the only additional functionality would be the weighted average.\nScorer could be extended with the properties cats_p/r/f and cats_per_cat similar to NER.\nAlternative metrics:\nAUC ROC (also decide how to average across labels)\na misclassification cost matrix\naccuracy for multiclass tasks\n???\ndoc.cats and Multilabel Thresholds\nIn order to optimize results for a multilabel classification task given a particular evaluation metric (e.g., f0.5-score), you might want to find/store probability thresholds on a per-label basis. I'm not sure I know enough about where/how to do this sensibly, but my initial suggestion would be to use a supplied evaluation metric along with the dev set in spacy train to find thresholds and store them in the model as a default. (As something like cfg['default_thresholds']?)\nI think an alternative to doc.cats that just provides a set of positive labels could be useful. In the multiclass case, argmax provides the positive label. In the multilabel case, the stored thresholds or provided thresholds could be applied to doc.cats to provide a set of positive labels. I'm not sure whether this should be stored in Doc or provided as a separate utility function like util.get_positive_cats(nlp, doc, thresholds=thresholds), where nlp provides the thresholds unless alternate thresholds are specified. (Preferably with a better name than get_positive_cats, but I can't think of anything better right now.)\nTasks\nAdd cats to JSON training format\nExport cats in JSON training format for Doc (see discussion in: #4013)\nImport cats from JSON training format into GoldParse/GoldCorpus\nAdd threshold logic for multilabel tasks (in TextCategorizer? in Scorer?)\nAdd textcat scoring to Scorer\nAdd positive_cats-type set output?\nAdd textcat to the pipeline options in spacy train including autodetection of model settings\nRelated/Future Tasks\nAdd data debugging / warnings as described in #2928\n2\n2", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T12:01:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "479": {"issue_url": "https://github.com/explosion/spaCy/issues/4037", "issue_id": "#4037", "issue_summary": "Get the pattern of the matched sentence", "issue_description": "nadachaabani1 commented on 29 Jul 2019\nFeature description\nAs i understand from the documentation, we can match sentence using rules with adding patterns,\nexample :\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\n\ndoc = nlp(\n    \"i want to buy an iPhone X\"\n)\npattern1 = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\npattern2 = [{\"TEXT\": \"Samsung\"}, {\"TEXT\": \"S 10\"}]\n\nmatcher.add(\"Phone\", None, pattern1,pattern2)\nmatches = matcher(doc)\nprint(\"Total matches found:\", len(matches))\n\nfor match_id, start, end in matches:\n    print(\"Match found:\", doc[start:end].text)\nthe program will return :\nTotal matches found: 1\nMatch found: iPhone X\nin this step, the program has successful extracted the iPhone X, and it was due to the match.add method call.\nin our implementation, we need to identify witch pattern was successful ( in this example [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}] was the pattern chosen to return this result).", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T10:28:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "480": {"issue_url": "https://github.com/explosion/spaCy/issues/4036", "issue_id": "#4036", "issue_summary": "Spacy predict no result for the large dataset?", "issue_description": "imsaiful commented on 29 Jul 2019\nI have tried the following code to retrain the spacy model:\nimport spacy\nimport random\nfrom sklearn.externals import joblib\nnlp = spacy.load('en')\nnlp.entity.add_label('Brand')\nnlp.entity.add_label('Celebrity')\nnlp.entity.add_label('Community')\nnlp.entity.add_label('GPE')\nnlp.entity.add_label('Publisher')\nnlp.entity.add_label('Show')\nTRAIN_DATA = l[0:1800]\nprint(len(TRAIN_DATA))\ntry:\n    optimizer = nlp.begin_training()\n    for text, annotations in TRAIN_DATA:\n        nlp.update([text], [annotations],drop=0.3, sgd=optimizer)\n    nlp.to_disk(\"./model\")\nexcept Exception as e:\n    print(e)\nnlp = spacy.load('./model')\ntext = \"Google is a Company.Elon Musk is world number one innovator.WordPress is good for SEO website. I live in America\"\ndoc = nlp(text)\nfor ent in doc.ents:\n    print(ent.text,ent.label_)\nI have 1817 sentance in my test data. There is no prediction after training for the test data.However when I try to retrain the model using slicing i.e. first I train the model for the slice l[0:200] and then l[200:500],l[500:800],l[800:1000],l[1000:1300],l[1200:1600],l[1600:1817] for all slice then I got the prediction output. It means my dataset is correct. However when I take data size l[0:1800] I am not getting any prediction result. What is the reason of this issue?", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T09:23:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "481": {"issue_url": "https://github.com/explosion/spaCy/issues/4034", "issue_id": "#4034", "issue_summary": "Upper/Lower case changes from `pobj` to `dobj`", "issue_description": "fersarr commented on 28 Jul 2019\nIn the sentence 'He used to like icecream', if we change 'he' to 'He' the pobj becomes dobj as shown below:\nUpper case 'He' -> pobj\nLower case 'he' -> dobj\nUsing en_core_web_lg.\nExpected behaviour: both sentences should use dobj, not pobj\nThis is probably another one for the perf/accuracy tag", "issue_status": "Closed", "issue_reporting_time": "2019-07-28T13:57:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "482": {"issue_url": "https://github.com/explosion/spaCy/issues/4032", "issue_id": "#4032", "issue_summary": "Getting vectors from FastText using user_hooks", "issue_description": "Contributor\nFallakAsad commented on 27 Jul 2019 \u2022\nedited\nI want to train NER with FastText vectors, I tried 2 approaches:\n1st Approach:\nLoad blank 'en' model\nLoad fasttext vectors for 2M vocabulary using nlp.vocab.set_vector() function\nCall begin_training() followed by the code that iterate over batches and call update function.\n2nd Approach:\nLoad blank 'en' model\nAdd a custom component i.e 'FastTextModel' in pipeline which sets the user_token_hooks with key 'vector' as shown in following code\nCall begin_training() followed by the code that iterate over batches and call update function.\nLABEL = ['label_1', 'label_2']\n\ndef train_model(model):\n    nlp = spacy.load('en_core_web_sm')  # load existing spacy model\n\n    # Load Fasttext component\n    fasttext_component = FastTextModel(fasttext.load_model(\"cc.en.300.bin\"))\n    nlp.add_pipe(fasttext_component, first=True) \n\n    if 'ner' not in nlp.pipe_names:           \n        ner = nlp.create_pipe('ner')\n        nlp.add_pipe(ner)\n    else:\n        ner = nlp.get_pipe('ner')\n\n    # Add new entity labels to entity recognizer\n    for i in LABEL:\n        ner.add_label(i)   \n\n    optimizer = nlp.resume_training()\n    \n    #Get names of other pipes to disable them during training to train only NER\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner' and pipe != 'FastTextModel']\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # Training code here\n    save_model(nlp, 'model_name', 'output_dir', 0)\n\nclass FastTextModel(object):\n    def __init__(self, model):\n        self._model = model\n        \n    def __call__(self, doc):\n        doc.user_token_hooks[\"vector\"] = self.vector\n        return doc;\n\n    def vector(self, obj1):\n        return self._model.get_word_vector(obj1.text)\n        \nif __name__ == '__main__':\n    train_model()\nMy questions are:\nIn first approach, is it correct way to load vectors using nlp.vocab.set_vector() function? All the vectors loaded by this function will be utilized by NER while training?\nIn second approach, will NER utilize 'vector' function of FastTextModel to get word vectors for feeding it to CNN while training? During training, will NER learn new representation of words? if so, If I load word vector using following code after training is finished, will model return new learned word representations or the once generated by FastTextModel's 'vector' function\ndoc = nlp('payment')\nfor d in doc:\n    print(d.vector) \nIn second approach, will NER be able to generate word representation of OOV words using user_token_hooks since FastTextModel's vector() function generates representation of OOV words as well.\nIf NER does not use user_token_hooks during training, then only way to use FastText embeddings it to load it as a static embedding table using npl.vocab.set_vector() function?\nIs there is some way to log what vector are being fed to embed part of NER while training or testing?\nYour Environment\nPlatform: Linux-4.15.0-55-generic-x86_64-with-Ubuntu-16.04-xenial\nspaCy version: 2.1.6\nPython version: 3.5.2", "issue_status": "Closed", "issue_reporting_time": "2019-07-27T12:27:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "483": {"issue_url": "https://github.com/explosion/spaCy/issues/4030", "issue_id": "#4030", "issue_summary": "TextCategorizer crashes on an empty text", "issue_description": "Collaborator\nadrianeboyd commented on 26 Jul 2019\nThe textcat component doesn't seem to handle empty texts well.\nHow to reproduce the behaviour\nnlp = spacy.load(\"/path/to/textcat-model\")\nnlp(\"\")\nThe errors are different depending on the type of model, but both bow and simple_cnn crash with an IndexError in thinc.\nFor bow:\n  File \"/home/adriane/spacy/venv/spacy21/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n  File \"linear.pyx\", line 45, in thinc.linear.linear.LinearModel.begin_update\n  File \"linear.pyx\", line 67, in thinc.linear.linear.LinearModel._begin_cpu_update\nIndexError: Out of bounds on buffer access (axis 0)\nFor simple_cnn:\n  File \"/home/adriane/spacy/venv/spacy21/lib/python3.7/site-packages/thinc/api.py\", line 310, in predict\n    X = layer(layer.ops.flatten(seqs_in, pad=pad))\n  File \"ops.pyx\", line 113, in thinc.neural.ops.Ops.flatten\nIndexError: list index out of range\nYour Environment\nspaCy version: 2.1.6\nPlatform: Linux-4.19.0-5-amd64-x86_64-with-debian-10.0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-07-26T07:46:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "484": {"issue_url": "https://github.com/explosion/spaCy/issues/4029", "issue_id": "#4029", "issue_summary": "Is there a way to train on custom data to recognize number sequences in specific context?", "issue_description": "mrxiaohe commented on 26 Jul 2019\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.4\nEnvironment Information:\nSuppose that I have a lot of emails in which people mention certain account numbers associated with an online service. It could be something along the line of the following (the sequence of #'s represents account numbers):\n\"My account # is #########\"\n\"######### is my account number\"\n\"our account #'s are the following: #########, #########, #########\"\n\"The number you provided to us (#########) does not match the your account number in our system.\"\nThese account numbers follow certain rules that can be captured by regex (not just any sequence of 9 digits). However, there are also many other number sequences that can be captured by the same regex -- notably, some conference call services' access codes follow the same patterns, resulting many false positives.\nSo this leads me to wonder if it is possible to use spaCy to train on custom data like the sentences show above to identify and extract these account numbers. Crucially, I don't want the model to learn specific sequences of number combinations -- that is, if I train on 2000 sentences, I don't want the model to simply learn the specific sequences of numbers present in the 2000 sentences.\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-07-26T03:28:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "485": {"issue_url": "https://github.com/explosion/spaCy/issues/4028", "issue_id": "#4028", "issue_summary": "What is the lemma for noun_chunk \"two pets\"?", "issue_description": "colingoldberg commented on 26 Jul 2019\nShould the lemma for noun chunk \"two pets\" be \"two pets\" or \"two pet\"?\n`\nimport spacy\nnlp = spacy.load('en_core_web_md')\ndoc = nlp(\"John has two pets\")\nfor chunk in doc.noun_chunks:\nprint(chunk.lemma_)\n`\nproduces: \"two pet\"\nMethinks it should be \"two pets\"\nYour Environment\nspaCy version: 2.1.6\nPlatform: Linux-4.4.0-154-generic-x86_64-with-debian-stretch-sid\nPython version: 3.7\nAlso on macosx (with en_core_web_sm)\nspaCy version: 2.1.4\nPlatform: Darwin-18.7.0-x86_64-i386-64bit\nPython version: 3.6.7", "issue_status": "Closed", "issue_reporting_time": "2019-07-25T21:13:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "486": {"issue_url": "https://github.com/explosion/spaCy/issues/4027", "issue_id": "#4027", "issue_summary": "How to use gensim word2vec output for training NER?", "issue_description": "erotavlas commented on 26 Jul 2019 \u2022\nedited\nA few questions regarding using word vectors for NER\n1 - After training the word2vec model using Gensim, what are we saving to file how do we import the Gensim output to spacy when training an NER model? (using the train_ner.py script provided, are we supposed to use this spacy.load(vocabulary, vocab=nlp.vocab) where vocabulary is something like en_vectors_web_lg\n2 - I'm using the same corpus of text for both steps - training the NER model and creating word2vec model. Is this the correct approach?\n3 - When building the word2vec model should we preserve word shape? or make everything lowercase? Also should we remove numerics?\n4- CBOW vs Skip Gram for NER?", "issue_status": "Closed", "issue_reporting_time": "2019-07-25T20:05:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "487": {"issue_url": "https://github.com/explosion/spaCy/issues/4026", "issue_id": "#4026", "issue_summary": "Cannot allocate memory while training with cli on GPU", "issue_description": "pratapaprasanna commented on 26 Jul 2019 \u2022\nedited\nHi all ,\nI have tried my traning data with spacy with the following cli command\npython -m spacy train en spacy_exam_clean_tag train.json valid.json --pipeline ner -G -v <vectors_path> -g 1 -n 400\nMy training data set has around 130K records\nand while saving the best model i get this weird error can any one suggest me as to what might be the possible issue?\n398       0.000      15.070    0.000    1.247    1.943    1.519  100.000  100.000    45155    66339\n399       0.000      14.969    0.000    1.248    1.943    1.520  100.000  100.000    45008    66389\n\u2714 Saved model to output directory\nspacy_exam_clean_tag/model-final\nTraceback (most recent call last):\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/site-packages/spacy/cli/train.py\", line 367, in train\n    with msg.loading(\"Creating best model...\"):\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/contextlib.py\", line 81, in __enter__\n    return next(self.gen)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/site-packages/wasabi/printer.py\", line 192, in loading\n    t.start()\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/multiprocessing/process.py\", line 105, in start\n    self._popen = self._Popen(self)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n    return Popen(process_obj)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/home/vz/miniconda3/envs/gp/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n    self.pid = os.fork()\nOSError: [Errno 12] Cannot allocate memory\nSegmentation fault (core dumped)\nis there anything wrong about my method of starting the training?\nI have ample amount of resources in my machine. But donno why im landing into this error\nAny way forward will be of great use.\nThanks in advance\nYour Environment\nOperating System:\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-25T19:07:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "488": {"issue_url": "https://github.com/explosion/spaCy/issues/4025", "issue_id": "#4025", "issue_summary": "Altering model architecture for textcat training", "issue_description": "Matt52 commented on 25 Jul 2019\nHi everyone!\nWhen training a text classification model, I was trying to alter model architecture. However, once I use anything else than simple_cnn, I will get error message:\nAttributeError: '_finish_linear_update' object has no attribute 'nO'\nHow to reproduce the behaviour\nimport random\nimport spacy\nfrom spacy.util import minibatch, compounding\n\ntrain_data = [\n('Augmenting semantic web service descriptions with compositional specification.', {'cats': {'VLDB': False, 'ISCAS': False, 'SIGGRAPH': False, 'INFOCOM': False, 'WWW': True}}),\n('VRML: Prelude and Future (Panel).', {'cats': {'VLDB': False, 'ISCAS': False, 'SIGGRAPH': True, 'INFOCOM': False, 'WWW': False}}),\n('Finding all modes of nonlinear oscillations by the Krawczyk-Moore-Jones algorithm.', {'cats': {'VLDB': False, 'ISCAS': True, 'SIGGRAPH': False, 'INFOCOM': False, 'WWW': False}}),\n('Special properties of the modified DFT to achieve algorithmic fault tolerance in Adaptive Filters.', {'cats': {'VLDB': False, 'ISCAS': True, 'SIGGRAPH': False, 'INFOCOM': False, 'WWW': False}}),\n('Data summaries for on-demand queries over linked data.', {'cats': {'VLDB': False, 'ISCAS': False, 'SIGGRAPH': False, 'INFOCOM': False, 'WWW': True}})\n]\n\nnlp = spacy.load('en_core_web_md')\n\nconfiguration = {\"exclusive_classes\": True, \"architecture\": \"ensemble\"}\ntextcat = nlp.create_pipe(\"textcat\", config = configuration)\nnlp.add_pipe(textcat, last=True)\n\n# add label to text classifier\nlabels = ['VLDB', 'ISCAS', 'SIGGRAPH', 'INFOCOM', 'WWW']\nfor l in labels:\n    textcat.add_label(l)\n\nbatch_sizes = compounding(4.0, 32.0, 1.001)\n    \nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\nwith nlp.disable_pipes(*other_pipes):  # only train textcat\n    optimizer = nlp.begin_training()\n\n    print('*************** BEGINING OF THE TRAINING ***************')\n    for i in range(10):\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        random.shuffle(train_data)\n        batches = minibatch(train_data, size=batch_sizes)\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.5, losses=losses)\n\n        print(\"LOSS\", losses[\"textcat\"])\n    print('*************** END OF THE TRAINING ***************')\nEverything works when I change ensemble to simple_cnn. I even tried using blank en model instead of en_core_web_md but that is not affecting it.\nHow exactly can be used different architecture than simple_cnn?\nThank you in advance.\nMy Environment\nOperating System: osx Mojave 10.14.5\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.6", "issue_status": "Closed", "issue_reporting_time": "2019-07-25T16:24:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "489": {"issue_url": "https://github.com/explosion/spaCy/issues/4024", "issue_id": "#4024", "issue_summary": "Editing spacy NER result before evaluation", "issue_description": "erotavlas commented on 25 Jul 2019 \u2022\nedited\nIs it possible to make changes to the result spacy provides before passing it to evaluation method?\nWhat I want to do is iterate over the found entities and if one is a false positive (that I know for sure by other means) I want to remove it from the spacy result. Then I want to evaluate the modified result against my gold standard after having removed the false positive items. I know the evaluation would no longer be a true representation of spacy performance, and I'm doing this because I'm combining spacy with another process to produce a final result and I want to evaluate the combined performance.\nIs this possible?", "issue_status": "Closed", "issue_reporting_time": "2019-07-25T16:01:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "490": {"issue_url": "https://github.com/explosion/spaCy/issues/4020", "issue_id": "#4020", "issue_summary": "gold.biluo_tags_from_offsets should throw E103 if two entity labels are overlapping", "issue_description": "Contributor\nRyanZHe commented on 25 Jul 2019 \u2022\nedited\nFeature description\nspacy.gold.biluo_tags_from_offsets should throw Error.E103 when the collection of entites passed in have overlapping tokens.\nFor example,\nfrom spacy.gold import biluo_tags_from_offsets\n\ndoc = nlp(u\"I like California Pizza Kitchen.\")\nentities = [(7, 17, \"LOC\"), (7, 31, \"BRAND\") ]\ntags = biluo_tags_from_offsets(doc, entities) # should throw Error.E103\nRight now tags is being assigned as [\"O\", \"O\", \"U-LOC\", \"I-BRAND\", \"L-BRAND\"], which violates the biluo tagging schema.", "issue_status": "Closed", "issue_reporting_time": "2019-07-25T02:02:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "491": {"issue_url": "https://github.com/explosion/spaCy/issues/4019", "issue_id": "#4019", "issue_summary": "GPU spacy objects to CPU", "issue_description": "Contributor\nyanaiela commented on 24 Jul 2019\nI'm running spacy on large amounts of text, and wanted to persist the annotations to the disk.\nTo speed things up, I started using gpus, but then got the following error:\nTypeError: can not serialize 'cupy.core.core.ndarray' object\nI'm persisting the annotations using pickle.\nWhen running on the cpus it works fine.\nIs there a method to transfer all the cupy objects into numpy or something similar?\nHow to reproduce the behaviour\nimport spacy\nimport pickle\n\nspacy.prefer_gpu()\nnlp = spacy.load('en_core_web_sm')\ns = nlp(\"Hello World\")\npickle.dump(s, open('temp.pickle', \"wb\"))\nYour Environment\nOperating System: CentOS Linux v.7\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.6\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-24T14:02:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "492": {"issue_url": "https://github.com/explosion/spaCy/issues/4018", "issue_id": "#4018", "issue_summary": "Include API documentation in package", "issue_description": "donpellegrino commented on 24 Jul 2019\nFor the use case of Python development in an air gapped environment, it would be useful if the API documentation was accessible via the built-in Python help() system. For example, running help(spacy.tokens.Span) in the interpreter gives the following:\nHelp on class Span in module spacy.tokens.span:\n\nclass Span(builtins.object)\n |  A slice from a Doc object.\n |  \n |  DOCS: https://spacy.io/api/span\n |  \n |  Methods defined here:\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n...\nThe online API documentation at https://spacy.io/api/span contains additional information and a number of useful examples. However, if connectivity to https://spacy.io is not available then the information cannot be used. Placing the information in the class documentation would make it easier to include with the packaged module. It would also make access to it more consistent with other Python packages that use the help() system.\nAlternatively, the package might deploy the HTML API documentation within the /share directory for virtual environments. This would be less ideal since it would be separate from the help() system, however it would help ensure it is included with local installations of the package.\nWhich page or section is this issue related to?\nhttps://spacy.io/api/span\nTest System\nspacy PyPI package Version: 2.1.6 with Python 3.6.8.", "issue_status": "Closed", "issue_reporting_time": "2019-07-24T13:31:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "493": {"issue_url": "https://github.com/explosion/spaCy/issues/4017", "issue_id": "#4017", "issue_summary": "Connection refused when downloading English models for spacy-2.1.4 and spacy-2.1.6", "issue_description": "vostrosablin commented on 24 Jul 2019\nHow to reproduce the problem\nWhen trying to install spacy english models with python -m spacy download en getting a connection refused error. Probably the server is down.\nTried with versions 2.1.4 and 2.1.6\nTraceback (most recent call last):\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/connection.py\", line 160, in _new_conn\n    (self._dns_host, self.port), self.timeout, **extra_kw)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/util/connection.py\", line 80, in create_connection\n    raise err\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/util/connection.py\", line 70, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 61] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/connectionpool.py\", line 603, in urlopen\n    chunked=chunked)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/connectionpool.py\", line 344, in _make_request\n    self._validate_conn(conn)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/connectionpool.py\", line 843, in _validate_conn\n    conn.connect()\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/connection.py\", line 316, in connect\n    conn = self._new_conn()\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/connection.py\", line 169, in _new_conn\n    self, \"Failed to establish a new connection: %s\" % e)\nurllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x11df5dd50>: Failed to establish a new connection: [Errno 61] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/requests/adapters.py\", line 449, in send\n    timeout=timeout\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/connectionpool.py\", line 641, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/urllib3/util/retry.py\", line 399, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x11df5dd50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/cli/download.py\", line 37, in download\n    shortcuts = get_json(about.__shortcuts__, \"available shortcuts\")\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/cli/download.py\", line 73, in get_json\n    r = requests.get(url)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/requests/api.py\", line 75, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/requests/api.py\", line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/requests/sessions.py\", line 533, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/requests/sessions.py\", line 646, in send\n    r = adapter.send(request, **kwargs)\n  File \"/Users/vostrosa/Library/Python/3.7/lib/python/site-packages/requests/adapters.py\", line 516, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /explosion/spacy-models/master/shortcuts-v2.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x11df5dd50>: Failed to establish a new connection: [Errno 61] Connection refused'))\nEnvironment\nspaCy version: 2.1.6\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.4\nOperating system: Mac OS Mojave 10.14.5", "issue_status": "Closed", "issue_reporting_time": "2019-07-24T08:54:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "494": {"issue_url": "https://github.com/explosion/spaCy/issues/4016", "issue_id": "#4016", "issue_summary": "PhraseMatcher only uses the last label when more than one PhraseMatcher is defined", "issue_description": "ChrisPalmerNZ commented on 24 Jul 2019 \u2022\nedited\nThis is the same as other people have reported, however I am trying to make it clear that the error seems to be that only the last label is used for any entity that is identified. It's clear that the entities are correctly identified if you examine matcher._docs, but the last label is the one used - this should clarify that in this case its not a situation of multiple labels for an entity not being handled, its simply that multiple matchers always get the label of the last matcher\nimport spacy\nfrom spacy.matcher import PhraseMatcher\nfrom spacy.tokens import Doc, Span\n\nnlp = spacy.load('en')\nmatcher = PhraseMatcher(nlp.vocab)\nlogin_name_pattern = list(nlp.pipe(['jimbrown','jackblack']))\nmatcher.add('LOGIN_NAME', None, *login_name_pattern )\n\ndef login_name_component(doc):\n    # Apply the matcher to the doc\n    matches = matcher(doc)\n    # Create a Span for each match and assign the label 'LOGIN_NAME'\n    spans = [Span(doc, start, end, label=\"LOGIN_NAME\") for match_id, start, end in matches]\n    # Overwrite the doc.ents with the matched spans\n    doc.ents = spans\n    return doc\n\n# Add the component to the pipeline before the 'ner' component\nnlp.add_pipe(login_name_component, before=\"ner\")\nprint(nlp.pipe_names)\n# ['tagger', 'parser', 'login_name_component', 'ner']\n\ntxt = 'jimbrown: was it in ACT or NSW we saw jackblack in the Casino? Or maybe with MOMA?'\ndoc = nlp(txt)\nprint([(ent.text, ent.label_) for ent in doc.ents])\n# [('jimbrown', 'LOGIN_NAME'), ('ACT', 'ORG'), ('NSW', 'ORG'), ('jackblack', 'LOGIN_NAME'), ('Casino', 'LOC'), ('MOMA', 'ORG')]\n\nstate_terr_pattern = list(nlp.pipe(['ACT', 'NSW', 'NT', 'QLD', 'SA', 'TAS', 'VIC', 'WA']))\nmatcher.add('STATE_TERR', None, *state_terr_pattern)\n\ndef state_terr_component(doc):\n    # Apply the matcher to the doc\n    matches = matcher(doc)\n    # Create a Span for each match and assign the label 'STATE_TERR'\n    spans = [Span(doc, start, end, label=\"STATE_TERR\") for match_id, start, end in matches]\n    # Overwrite the doc.ents with the matched spans\n    doc.ents = spans\n    return doc\n\n# Add the component to the pipeline before the 'login_name_component'\nnlp.add_pipe(state_terr_component, before=\"login_name_component\")\nprint(nlp.pipe_names)\n# ['tagger', 'parser', 'state_terr_component', 'login_name_component', 'ner']\n\ntest_doc = nlp(txt)\nprint([(ent.text, ent.label_) for ent in test_doc.ents])\n# [('jimbrown', 'LOGIN_NAME'), ('ACT', 'LOGIN_NAME'), ('NSW', 'LOGIN_NAME'), ('jackblack', 'LOGIN_NAME'), ('Casino', 'LOC'), ('MOMA', 'ORG')]\n\nparents_pattern = list(nlp.pipe(['MOMA', 'POPA']))\nmatcher.add('PARENT', None, *parents_pattern)\n\ndef parent_component(doc):\n    # Apply the matcher to the doc\n    matches = matcher(doc)\n    # Create a Span for each match and assign the label 'PARENT'\n    spans = [Span(doc, start, end, label=\"PARENT\") for match_id, start, end in matches]\n    # Overwrite the doc.ents with the matched spans\n    doc.ents = spans\n    return doc\n\n# Add the component to the pipeline **after** the 'login_name_component'\nnlp.add_pipe(parent_component, after=\"login_name_component\")\nprint(nlp.pipe_names)\n# ['tagger', 'parser', 'state_terr_component', 'login_name_component', 'parent_component', 'ner']\n\ntest_doc = nlp(txt)\nprint([(ent.text, ent.label_) for ent in test_doc.ents])\n# [('jimbrown', 'PARENT'), ('ACT', 'PARENT'), ('NSW', 'PARENT'), ('jackblack', 'PARENT'), ('Casino', 'LOC'), ('MOMA', 'PARENT')]\n\nmatcher._docs\n# {7038724010617191: (jimbrown, jackblack),\n# 5513256235797150270: (ACT, NSW, NT, QLD, SA, TAS, VIC, WA),\n# 11213764556427237819: (MOMA, POPA)}\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.6.5\nspaCy Version Used: 2.1.6\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-24T07:39:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "495": {"issue_url": "https://github.com/explosion/spaCy/issues/4015", "issue_id": "#4015", "issue_summary": "How to convert OntoNotes 5 dataset to spaCy\u2019s JSON format?", "issue_description": "Contributor\nphiedulxp commented on 24 Jul 2019\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2019-07-24T02:06:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "496": {"issue_url": "https://github.com/explosion/spaCy/issues/4011", "issue_id": "#4011", "issue_summary": "Make extensions local to a pipeline", "issue_description": "johann-petrak commented on 23 Jul 2019\nCurrently, extensions are something that gets defined on a global, system wide basis.\nI think this is a serious design flaw since it severely restricts how spacy can be used.\nIt is not hard to find situations where people would want to use several pipelines, several languages etc. within the same system and clearly, extensions would ideally be specific to a pipeline. This would allow to have several pipelines which all may use different extensions.\nOf course, there is nothing that would prevent us from declaring shared callbacks for several pipelines.", "issue_status": "Closed", "issue_reporting_time": "2019-07-23T11:55:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "497": {"issue_url": "https://github.com/explosion/spaCy/issues/4010", "issue_id": "#4010", "issue_summary": "Training on GPU -> Inference on CPU", "issue_description": "alejandrojcastaneira commented on 23 Jul 2019 \u2022\nedited\nHello.\nI would like to know how to make the inference on a CPU configuration, from a GPU trained model.\nBest regards", "issue_status": "Closed", "issue_reporting_time": "2019-07-23T11:30:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "498": {"issue_url": "https://github.com/explosion/spaCy/issues/4008", "issue_id": "#4008", "issue_summary": "How the model is retrain by spacy?", "issue_description": "imsaiful commented on 23 Jul 2019 \u2022\nedited\nEarlier token 'Modi' is recognised as an Org by spacy to I retrain it with the following code:\nimport spacy \nimport random\nnlp = spacy.load('en')\nnlp.entity.add_label('CELEBRITY')\nTRAIN_DATA = [\n        (u\"Modi\", {\"entities\": [(0, 4, \"PERSON\")]}),\n        (u\"India\", {\"entities\": [(0, 5, \"GPE\")]})]\n\noptimizer = nlp.begin_training()\nfor i in range(20):\n    random.shuffle(TRAIN_DATA)\n    for text, annotations in TRAIN_DATA:\n        nlp.update([text], [annotations],drop=0.3, sgd=optimizer)\n\n\ntext = \"But Modi is starting India. The company made a late push\\ninto hardware, and Apple\u2019s Siri and Google available on iPhones, and Amazon\u2019s Alexa\\nsoftware, which runs on its Echo and Dot devices, have clear leads in\\nconsumer adoption.\"\ndoc = nlp(text)\nfor ent in doc.ents:\n    print(ent.text,ent.label_)\nAnd I got the following answer:\nModi PERSON\nIndia GPE\nApple\u2019s Siri ORG\nGoogle ORG\niPhones ORG\nAmazon GPE\nEcho PERSON\nDot PERSON\nIt changes the Modi to the person at the same time it doing incorrect NER as compare to the previous mode. In the previous model, Amazon was recognized as ORG but now change to GPE.\nNow I add the extra-label CELEBRITY and categorize Modi to CELEBRITY with this following code\nimport spacy \nimport random\nnlp = spacy.load('en')\nnlp.entity.add_label('CELEBRITY')\nTRAIN_DATA = [\n        (u\"Modi\", {\"entities\": [(0, 4, \"CELEBRITY\")]})]\n\noptimizer = nlp.begin_training()\nfor i in range(20):\n    random.shuffle(TRAIN_DATA)\n    for text, annotations in TRAIN_DATA:\n        nlp.update([text], [annotations],drop=0.3, sgd=optimizer)\n\n\ntext = \"But Modi is starting India. The company made a late push\\ninto hardware, and Apple\u2019s Siri and Google available on iPhones, and Amazon\u2019s Alexa\\nsoftware, which runs on its Echo and Dot devices, have clear leads in\\nconsumer adoption.\"\ndoc = nlp(text)\nfor ent in doc.ents:\n    print(ent.text,ent.label_)\nBut looks like it crashes my model and getting the following result:\nBut CELEBRITY\nModi CELEBRITY\nis CELEBRITY\nstarting CELEBRITY\nIndia GPE\n. CELEBRITY\nThe CELEBRITY\ncompany CELEBRITY\nmade CELEBRITY\na CELEBRITY\nlate CELEBRITY\npush CELEBRITY\ninto CELEBRITY\nhardware CELEBRITY\n, CELEBRITY\nand CELEBRITY\nApple CELEBRITY\nPlease let me know the behind the seen reason and also how can I achieve that only entity which I label should change while all other should be according to spacy.", "issue_status": "Closed", "issue_reporting_time": "2019-07-23T08:21:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "499": {"issue_url": "https://github.com/explosion/spaCy/issues/4007", "issue_id": "#4007", "issue_summary": "doc.ents returns spans instead of tokens after merge_entities", "issue_description": "nyejon commented on 23 Jul 2019 \u2022\nedited\nHi,\nThe merge entities doesn't seem to work as expected. Entities remain Span objects instead of token objects.\nHow to reproduce the behaviour\nimport spacy\nfrom spacy.pipeline import merge_entities\n\n\ntext = \"\"\"\n                100 sqm\n                100 square meters\n                100 sq meters\n                \"\"\"\n_nlp = spacy.load(\"en_core_web_sm\")\n_nlp.add_pipe(merge_entities)\n\ndoc = _nlp(text)\n\nprint(doc.ents)\n\ntexts = [t.ent_type_ for t in doc.ents]\nprint(texts)\nThe output looks like this:\n'spacy.tokens.span.Span' object has no attribute 'ent_type_'\nYour Environment\nOperating System: macOS\nPython Version Used: 3.7.4\nspaCy Version Used: 2.1.6\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-23T07:12:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "500": {"issue_url": "https://github.com/explosion/spaCy/issues/4006", "issue_id": "#4006", "issue_summary": "tag every token from the matched sentence", "issue_description": "nadachaabani1 commented on 23 Jul 2019\nFeature description\nAs i understand from the documentation, we can match sentence using rules with adding patterns,\nexample :\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\n\ndoc = nlp(\n    \"i want to buy an iPhone X\"\n)\npattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n\nmatcher.add(\"Phone\", None, pattern)\nmatches = matcher(doc)\nprint(\"Total matches found:\", len(matches))\n\nfor match_id, start, end in matches:\n    print(\"Match found:\", doc[start:end].text)\nthe program will return :\nTotal matches found: 1\nMatch found: iPhone X\nnow we want to tag each token from the matched sentence:\nwe want a result as:\nPhone Product: iPhone\nVersion: X\nPhone Product and Version are two variables tags provided by the user\nis there a way to achieve this result ?", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T20:11:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "501": {"issue_url": "https://github.com/explosion/spaCy/issues/4005", "issue_id": "#4005", "issue_summary": "Failure for proper noun ending in \"ly\"", "issue_description": "colingoldberg commented on 23 Jul 2019\nHow to reproduce the behaviour\n`\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ntext = \"Carly has a large family\"\ndoc = nlp(text)\nprint(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n`\nproduces:\nNoun phrases: ['a large family']\nWhere's Carly?\nYour Environment\nspaCy version: 2.1.4\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.6.7", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T19:07:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "502": {"issue_url": "https://github.com/explosion/spaCy/issues/4004", "issue_id": "#4004", "issue_summary": "DLL load failing when importing spacy", "issue_description": "anetschka commented on 22 Jul 2019\nHow to reproduce the problem\nI have been fighting with various python-related installation issues. After having cleaned my system, I am trying now to reinstall spacy, but I cannot even get the models installed or load the namespace. Any hints?", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T14:27:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "503": {"issue_url": "https://github.com/explosion/spaCy/issues/4002", "issue_id": "#4002", "issue_summary": "PhraseMatcher to match in a different token attribute", "issue_description": "jesusfbes commented on 22 Jul 2019 \u2022\nedited\nAccording to the documentation of PhraseMatcher it\u2019s also possible to make it match on different token attributes, for instance the POS tag of the dependency. Is it posible also to match other attibutes like norm? We are trying to modify the norm token attibute to remove the accents of spanish words and it is correctly changed (we checked the token.norm_ attribute) but it is not able to match in those cases:\nclass Deaccentuate(object):\n\n    def __init__(self, nlp):\n        self._nlp = nlp\n\n    def __call__(self, doc):\n        for token in doc:\n            token.norm_ = deaccent(token.lower_)\n           \n        return doc\n\nruler = EntityRuler(nlp, phrase_matcher_attr=\"NORM\")\nruler.add_patterns(patterns_)\nnlp.add_pipe(ruler)\n\ncustom_component = Deaccentuate(nlp)\nnlp.add_pipe(custom_component, first=True)\nYour Environment\nspaCy version: 2.1.6\nPlatform: Windows-10-10.0.16299-SP0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T11:00:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "504": {"issue_url": "https://github.com/explosion/spaCy/issues/4000", "issue_id": "#4000", "issue_summary": "Having an error with Entity Linking", "issue_description": "almoslmi commented on 22 Jul 2019\nI have run NEL (wikidata_entity_linking.py) and was working well after fixing some issues, but when it successfully finished processing the two dumps of Wikipedia and Wikidata, it failed to write the KB and got an AssertionError. All necessary libs are installed and added to the environment too. Any idea how to solve this issue, please?\nBy the way, I tried to debug the file that causes the problem (kb.pyx) but didn't work.\nI am using Pycharm, and Here is the last lines of the log\n4 1907 0.07248597717285156\n4 1908 0.07176494598388672\nTrained on 9540645 entities across 5 epochs\nFinal loss: 0.07176494598388672\nget entity embeddings 2019-07-20 18:33:18.329715\nadding 1908129 entities 2019-07-20 19:30:44.652900\nadding aliases 2019-07-20 19:31:01.172724\nkb size: 1908129 1908129 2830524\ndone with kb 2019-07-20 19:32:18.446255\nkb entities: 1908129\nkb aliases: 2830524\nSTEP 3b: write KB and NLP 2019-07-20 19:32:36.823641\nTraceback (most recent call last):\nFile \"C:/Users/tareq/PycharmProjects/spaCy/examples/pipeline/wikidata_entity_linking.py\", line 441, in\nrun_pipeline()\nFile \"C:/Users/tareq/PycharmProjects/spaCy/examples/pipeline/wikidata_entity_linking.py\", line 111, in run_pipeline\nkb_1.dump(KB_FILE)\nFile \"kb.pyx\", line 208, in spacy.kb.KnowledgeBase.dump\nFile \"kb.pyx\", line 346, in spacy.kb.Writer.init\nAssertionError\nProcess finished with exit code 1\n========================\nLater, I tried to debug the file that causes the error (kb.pyx) and got this error:\nC:\\Users\\tareq\\Anaconda3\\python.exe \"C:\\Program Files\\JetBrains\\PyCharm 2019.1.3\\helpers\\pydev\\pydevconsole.py\" --mode=client --port=52781\nimport sys; print('Python %s on %s' % (sys.version, sys.platform))\nsys.path.extend(['C:\\Users\\tareq\\PycharmProjects\\spaCy', 'C:\\Users\\tareq\\PycharmProjects\\spaCy\\spacy', 'C:/Users/tareq/PycharmProjects/spaCy'])\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.6.1 -- An enhanced Interactive Python. Type '?' for help.\nPyDev console: using IPython 7.6.1\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] on win32\nrunfile('C:/Users/tareq/PycharmProjects/spaCy/spacy/kb.pyx', wdir='C:/Users/tareq/PycharmProjects/spaCy/spacy')\nTraceback (most recent call last):\nFile \"C:\\Users\\tareq\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3325, in run_code\nexec(code_obj, self.user_global_ns, self.user_ns)\nFile \"\", line 1, in\nrunfile('C:/Users/tareq/PycharmProjects/spaCy/spacy/kb.pyx', wdir='C:/Users/tareq/PycharmProjects/spaCy/spacy')\nFile \"C:\\Program Files\\JetBrains\\PyCharm 2019.1.3\\helpers\\pydev_pydev_bundle\\pydev_umd.py\", line 197, in runfile\npydev_imports.execfile(filename, global_vars, local_vars) # execute the script\nFile \"C:\\Program Files\\JetBrains\\PyCharm 2019.1.3\\helpers\\pydev_pydev_imps_pydev_execfile.py\", line 18, in execfile\nexec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\nFile \"C:/Users/tareq/PycharmProjects/spaCy/spacy/kb.pyx\", line 7\nfrom cymem.cymem cimport Pool\n^\nSyntaxError: invalid syntax\n==============Environment\nspaCy version: 2.1.6\nPlatform: Windows-10-10.0.17763-SP0\nPython version: 3.7.1", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T09:32:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "505": {"issue_url": "https://github.com/explosion/spaCy/issues/3998", "issue_id": "#3998", "issue_summary": "python -m spacy download en too slow", "issue_description": "db12138 commented on 22 Jul 2019\nthe speed is less 10KB/s when I run python -m spacy download en\nfor example:\nLooking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\nCollecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\nDownloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n29% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 3.2MB 8.0kB/s eta 0:16:16\nand then the connection will break because of reading time out.\nbut I can download en_core_web_sm-2.1.0.tar.gz in my browser quickly.\nhow can I use en_core_web_sm-2.1.0.tar.gz directly by using a command line such as\npython -m spacy install ./en_core_web_sm-2.1.0.tar.gz", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T08:58:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "506": {"issue_url": "https://github.com/explosion/spaCy/issues/3997", "issue_id": "#3997", "issue_summary": "Getting a RuntimeError: Running cythonize failed when compiling Spacy", "issue_description": "almoslmi commented on 22 Jul 2019 \u2022\nedited by ines\nI am trying to build SpaCy from the source but I got \"RuntimeError: Running cythonize failed\". I tried to do \"pip install cython\" and found that it has been installed already\n(base) C:\\Users\\tareq\\PycharmProjects\\spaCy>pip install cython\nRequirement already satisfied: cython in c:\\users\\tareq\\anaconda3\\lib\\site-packages (0.29.12)\nAny idea how to solve this issue, please?\n============\n(base) C:\\Users\\tareq\\PycharmProjects\\spaCy>python setup.py build_ext --inplace\nCythonizing sources\nProcessing kb.pyx\n\nError compiling Cython file:\n------------------------------------------------------------\n...\n    cdef hash_t alias_hash\n    cdef float prior_prob\n\n\ncdef class KnowledgeBase:\n    cdef Pool mem\n        ^\n------------------------------------------------------------\n\nkb.pxd:30:9: 'Pool' is not a type identifier\nTraceback (most recent call last):\n  File \"C:\\Users\\tareq\\PycharmProjects\\spaCy\\bin\\cythonize.py\", line 169, in <module>\n    run(args.root)\n  File \"C:\\Users\\tareq\\PycharmProjects\\spaCy\\bin\\cythonize.py\", line 158, in run\n    process(base, filename, db)\n  File \"C:\\Users\\tareq\\PycharmProjects\\spaCy\\bin\\cythonize.py\", line 124, in process\n    preserve_cwd(base, process_pyx, root + \".pyx\", root + \".cpp\")\n  File \"C:\\Users\\tareq\\PycharmProjects\\spaCy\\bin\\cythonize.py\", line 87, in preserve_cwd\n    func(*args)\n  File \"C:\\Users\\tareq\\PycharmProjects\\spaCy\\bin\\cythonize.py\", line 63, in process_pyx\n    raise Exception(\"Cython failed\")\nException: Cython failed\nTraceback (most recent call last):\n  File \"setup.py\", line 276, in <module>\n    setup_package()\n  File \"setup.py\", line 209, in setup_package\n    generate_cython(root, \"spacy\")\n  File \"setup.py\", line 132, in generate_cython\n    raise RuntimeError(\"Running cythonize failed\")\nRuntimeError: Running cythonize failed\n==============\nEnvironment\nspaCy version: 2.1.6\nPlatform: Windows-10-10.0.17763-SP0\nPython version: 3.7.1", "issue_status": "Closed", "issue_reporting_time": "2019-07-21T23:25:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "507": {"issue_url": "https://github.com/explosion/spaCy/issues/3996", "issue_id": "#3996", "issue_summary": "Getting scores as probabilities or confidence from the output of the Spacy TextCategorizer", "issue_description": "haroonhassan commented on 20 Jul 2019 \u2022\nedited\nYour Environment\nOperating System: Linux\nPython Version Used: 3.7\nspaCy Version Used: 2.1\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-20T13:29:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "508": {"issue_url": "https://github.com/explosion/spaCy/issues/3995", "issue_id": "#3995", "issue_summary": "Getting scores as probabilities or confidence from the output of the Spacy TextCategorizer", "issue_description": "haroonhassan commented on 20 Jul 2019\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-20T13:29:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "509": {"issue_url": "https://github.com/explosion/spaCy/issues/3994", "issue_id": "#3994", "issue_summary": "How should `is_oov` be used?", "issue_description": "Contributor\nDeNeutoy commented on 20 Jul 2019\nI had a question (allenai/scispacy#136) in scispacy regarding the usage of is_oov, which then confused me:\nHow should the is_oov flag be used? Initially, I thought it would correspond to tokens which do not have a vector, but it seems like it should correspond to existence in the nlp.vocab, given this line: https://github.com/explosion/spaCy/blob/master/spacy/cli/init_model.py#L142\nIn [28]: x = spacy.load(\"en_core_sci_sm\")\n\nIn [29]: doc = x(\"hello this word smelling is oov.\")\n\nIn [30]: [t.is_oov for t in doc]\nOut[30]: [True, False, False, True, False, True, False]\n\nIn [31]: x = spacy.load(\"en_core_web_sm\")\n\nIn [32]: doc = x(\"hello this word smelling is oov.\")\n\nIn [33]: [t.is_oov for t in doc]\nOut[33]: [True, True, True, True, True, True, True]\nI've seen previously in #3986 that this was an issue from v2.0, so I double checked that the model is fresh:\nIn [36]: x.meta\nOut[36]:\n{'accuracy': {'ents_f': 85.8587845242,\n  'ents_p': 86.3317889027,\n  'ents_r': 85.3909350025,\n  'las': 89.6616629074,\n  'tags_acc': 96.7783856079,\n  'token_acc': 99.0697323163,\n  'uas': 91.5287392082},\n 'author': 'Explosion AI',\n 'description': 'English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.',\n 'email': 'contact@explosion.ai',\n 'lang': 'en',\n 'license': 'MIT',\n 'name': 'core_web_sm',\n 'parent_package': 'spacy',\n 'pipeline': ['tagger', 'parser', 'ner'],\n 'sources': ['OntoNotes 5'],\n 'spacy_version': '>=2.1.0',\n 'speed': {'cpu': 6684.8046553827, 'gpu': None, 'nwords': 291314},\n 'url': 'https://explosion.ai',\n 'version': '2.1.0',\n 'vectors': {'width': 0, 'vectors': 0, 'keys': 0, 'name': None}}\nAdditionally, i'm not quite sure how i've managed to get this behaviour in one of the scispacy models:\nIn [51]: x = spacy.load(\"en_core_sci_sm\")\n\nIn [52]: doc = x(\"hello this word smelling is oov.\")\n\nIn [53]: for t in doc:\n    ...:     print(t.is_oov, t.text in x.vocab)\n    ...:\nTrue True\nFalse True\nFalse True\nTrue True\nFalse True\nTrue False\nFalse True\n\nIn [54]: x = spacy.load(\"en_core_web_sm\")\n\nIn [55]: doc = x(\"hello this word smelling is oov.\")\n\nIn [56]: for t in doc:\n    ...:     print(t.is_oov, t.text in x.vocab)\n    ...:\nTrue True\nTrue True\nTrue True\nTrue True\nTrue True\nTrue True\nTrue True\nSo basically, i'm just wondering what the correct interpretation is of Token.is_oov is\nThanks!\nWhich page or section is this issue related to?\nhttps://spacy.io/api/token", "issue_status": "Closed", "issue_reporting_time": "2019-07-20T13:01:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "510": {"issue_url": "https://github.com/explosion/spaCy/issues/3993", "issue_id": "#3993", "issue_summary": "Results per entity type missing entities", "issue_description": "erotavlas commented on 19 Jul 2019 \u2022\nedited\nspacy version = 2.1.6\nI trained my data to recognize 8 different entity types using 10 fold cross validation\nWhen I step through my code and evaluate each of the 10 models, the results['ents_per_type'] variable contains different number of entities for each fold. Is this normal?\nI was expecting there to be consistently 8 entities for each result with some entities having precision recall and fscore of 0 if they didn't exist in that particular test set.\nIt was causing me some problem calculating the final averages which is why I noticed it.\nFor example I can get a result like this from the overall results object containing results['ents_f'], results['ents_p'] and results['ents_r']\n10-fold cross validation average score...\nAverage Recall: 85.15209283181693\nAverage Precision: 88.09937683644647\nAverage FScore: 86.56832860827618\nBut my per entity results from results['ents_per_type'] are\n10-fold cross validation average scores per entity type...\nPERSON p:74.085 r:78.916 f:76.415\nDATE p:94.367 r:99.136 f:96.671\nPHONENUMBER p:81.55 r:92.067 f:86.16\nORG p:71.474 r:77.25 f:74.095\nSPECIMENID p:84.682 r:87.2 f:85.799\nADDRESS p:85.798 r:85.798 f:85.798\nEMAIL p:100.0 r:100.0 f:100.0\nMRN p:43.75 r:43.75 f:43.75\nAnd say I average up the precisions for all entities I get a value of approximately 79\nThis doesn't match with the overall average precision of 88\nThe way I calculated the per entity type values is\n        result_per_ent = []\n        \n        #accumulate results for each fold into an array\n        # for each model in the folds, load model and evaluate on the test set\n        # append the results to the array\n        # result_per_ent.append(results['ents_per_type'])\n\n        # average result for k-fold cross validation - per entity type\n        # first accumulate all the values into arrays\n        ent_dict = {}\n        for item in result_per_ent:\n            for ent in item:\n                if (ent not in ent_dict):\n                    ent_dict[ent] = {'p': [], 'r': [], 'f': []}\n                ent_dict[ent]['p'].append(item[ent]['p'])\n                ent_dict[ent]['f'].append(item[ent]['f'])\n                ent_dict[ent]['r'].append(item[ent]['r'])\n\n        #average the results\n        file.write(\"\" + \"\\n\")\n        file.write(str(len(result_f)) + \"-fold \" + \"cross validation average scores per entity type...\" + \"\\n\")\n        for ent in ent_dict:\n            file.write(str(ent) + \" p: \" + str(average(ent_dict[ent]['p'])) + \" r: \" + str(\n                average(ent_dict[ent]['r'])) + \" f: \" + str(average(ent_dict[ent]['f'])) + \"\\n\")", "issue_status": "Closed", "issue_reporting_time": "2019-07-19T15:49:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "511": {"issue_url": "https://github.com/explosion/spaCy/issues/3991", "issue_id": "#3991", "issue_summary": "Error in evaluation after training NER en pretrain spaCy model", "issue_description": "swicaksono commented on 19 Jul 2019 \u2022\nedited\nHi, recently I'm trying to train en spacy model on the ner model by some new dataset. But when it comes to the evaluation method:\nscorer_train = Scorer()\nfor text, _ann_ in train_data:\n      doc = model(text)\n      doc_gold_text = model.make_doc(text)\n      gold = GoldParse(doc_gold_text, entities=_ann_['entities'])\n      try:\n             scorer_train.score(doc, gold)\n      except:\n              continue\nprint(scorer_train.scores)\nIt prints an error in following:\nFile \"_retokenize.pyx\", line 56, in spacy.tokens._retokenize.Retokenizer.merge\nValueError: [E102] Can't merge non-disjoint spans. 'co' is already part of tokens to merge.\nEnvironment\nOperating System: Ubuntu 19\nPython Version Used: Python 3.6\nspaCy Version Used: 2.1\nAny help?\nThank you in advance.", "issue_status": "Closed", "issue_reporting_time": "2019-07-19T03:30:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "512": {"issue_url": "https://github.com/explosion/spaCy/issues/3990", "issue_id": "#3990", "issue_summary": "Sentence boundary fails with sentence containing parentheses", "issue_description": "colingoldberg commented on 19 Jul 2019 \u2022\nedited\nHi,\nI found unexpected behavior in the following sentence.\nHow to reproduce the behaviour\n`\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ntext = \"The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America\"\ndoc = nlp(text)\nfor sentence in doc.sents:\nprint(sentence)\nprint(sentence.root)\n`\nproduces three sentences:\nThe Amazon rainforest rainforest\n(Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; amazonienne\nDutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America is\nYour Environment\n$ python3 -m spacy info --markdown\nInfo about spaCy\nspaCy version: 2.1.4\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.6.7\nIs this a bug? How to fix it?\nColin Goldberg", "issue_status": "Closed", "issue_reporting_time": "2019-07-18T18:34:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "513": {"issue_url": "https://github.com/explosion/spaCy/issues/3989", "issue_id": "#3989", "issue_summary": "Weighted Hash Embeddings", "issue_description": "YannDubs commented on 18 Jul 2019 \u2022\nedited\nTL;DR: Improving HashEmbed class by using a weighted second matrix.\nI was recently looking at the HashEmbed class. I was wondering if there was any reason not to use a weighted second matrix as in Hash Embeddings for Efficient Word Representations (NIPS 2017)?\nThe second matrix would be long but narrow (so not many parameters, usually same order of magnitude than first matric) and contains for each word the weights it should give for different hashes. E.g. \"dog: [0.7, 0.2, 0.9]\" such that word dog is not 0.7*hash_1(dog) + 0.2 * hash_2(dog) + 0.9 * has_3(dog). I've implemented it for the NIPS implementation challenge in 2017 and it worked very well (https://github.com/YannDubs/Hash-Embeddings). Small drawing I made back then to explain the method:\nI was thus wondering if there is any reason not to use that instead ? Have you tried it?", "issue_status": "Closed", "issue_reporting_time": "2019-07-18T12:57:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "514": {"issue_url": "https://github.com/explosion/spaCy/issues/3988", "issue_id": "#3988", "issue_summary": "Vocab.__contains__ not working as expected", "issue_description": "Contributor\nBreakBB commented on 18 Jul 2019\nHow to reproduce the behaviour\nWhile testing for #3986 I encountered some unexpected behaviour. It seems that Vocab.contains might be broken or maybe the docs are outdated?\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\nif u\"September\" in nlp.vocab.strings:\n    print(\"September in StringStore\")  # Working as expected\n\ndog = nlp.vocab.strings[u\"dog\"]\nassert dog not in nlp.vocab  # Working as expected\n\nseptember = nlp.vocab.strings[u\"September\"]\nassert september in nlp.vocab  # AssertionError\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.6", "issue_status": "Closed", "issue_reporting_time": "2019-07-18T12:39:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "515": {"issue_url": "https://github.com/explosion/spaCy/issues/3986", "issue_id": "#3986", "issue_summary": "is_oov and prob does not work for en_core_web_sm model", "issue_description": "lkluo commented on 18 Jul 2019\nHow to reproduce the behaviour\nlex = nlp.vocab[u\"dog\"] print(lex.is_oov, lex.prob)\nproducing\nTrue -20.0\nThis bug had been reported #1204.\nYour Environment\nOperating System: masOS\nPython Version Used: 3.6\nspaCy Version Used: 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-18T09:01:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "516": {"issue_url": "https://github.com/explosion/spaCy/issues/3985", "issue_id": "#3985", "issue_summary": "Use the same pipeline multiple times", "issue_description": "RonRademaker commented on 18 Jul 2019\nHi,\nIs it possible to use the same pipeline multiple times, I got a few different text classification tasks so I'd like to train different textcat pipelines for each task and merge them into one model.", "issue_status": "Closed", "issue_reporting_time": "2019-07-18T08:49:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "517": {"issue_url": "https://github.com/explosion/spaCy/issues/3984", "issue_id": "#3984", "issue_summary": "Pretraining gives AttributeError: 'FunctionLayer' object has no attribute 'G'", "issue_description": "DeltaSierra4 commented on 18 Jul 2019 \u2022\nedited\nThe full error log I get is below:\nTraceback (most recent call last):\n  File \"classify.py\", line 461, in <module>\n    plac.call(main)\n  File \"/usr/local/lib/python3.7/dist-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/usr/local/lib/python3.7/dist-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"classify.py\", line 193, in main\n    textcat.model.tok2vec.from_bytes(file_.read())\n  File \"/usr/local/lib/python3.7/dist-packages/thinc/neural/_classes/model.py\", line 375, in from_bytes\n    dest = getattr(layer, name)\nAttributeError: 'FunctionLayer' object has no attribute 'G'\nHow to reproduce the behaviour\nI obtained several .bin files from running the following line:\npython3 -m spacy pretrain data.jsonl en_vectors_web_lg binaries\nthen I used the .bin file for a text classifier with the -t2v command line argument:\npython3 classify.py -t2v binaries/model100.bin [...other optional arguments...]\n(classify.py is basically the sample code that is presented in the textcat section of the documentation) It loads the en model and initializes the pipelines correctly, but when it reaches the line where it is trying to read in the .bin file it crashes with the error log above.\nRetraining with a different version (2.1.4) or using python 2.7 or Python 3.6 instead of python 3.7.3 didn't work. Following the suggestions on issue #3668 (i.e. setting --use-vectors during pretraining) didn't work either.\nYour Environment\nOperating System: Linux Ubuntu 18.04.2 LTS\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.6", "issue_status": "Closed", "issue_reporting_time": "2019-07-17T22:21:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "518": {"issue_url": "https://github.com/explosion/spaCy/issues/3983", "issue_id": "#3983", "issue_summary": "Installing spacy with fewer languages to save disk space", "issue_description": "martinpeck commented on 18 Jul 2019\nI'm building a docker container that contains a the Python library spacy. I'm now trying to reduce the size of this container, and spacy appears to be the main contributor to the disk image size.\nWithout any models installed, and without any other code/dependencies etc, spacy consumes around 500MB of disk when installed. It looks like the lang folder within this package is the main cause of this disk usage.\nI'd like to be able to install spacy with a smaller set of languages, or to be able to remove them in a supported manner post-install, but I've not seen any docs that explain how this might be done.\nMy repro steps are:\nmkdir foo1                  # create a folder \ncd foo1                     # change directory\npython3 -m venv .venv       # create virtual environment\nsource .venv/bin/activate   # activate virtual environment\npip install --upgrade pip   # upgrade pip\npip install spacy           # install spacy\nAfter doing this, I then navigate into the following folder...\nfoo1/.venv/lib/python3.7/site-packages\n... and can see that the spacy folder is very large:\n$ du -sh spacy\n425M spacy\nSpecifically, it's the language folder that's large:\n$ du -sh spacy/lang\n401M spacy/lang\nThere are 52 languages in that folder, and for many situations I only want one or two languages. For the specific app I'm working on now, I'm only interested in English.\nWhen I look at the sizes, English is the 14th largest (only showing the top 14 in this list)...\n$ du -sH spacy/lang/* | sort -n -r \n\n142024 spacy/lang/tr\n86608 spacy/lang/pt\n78368 spacy/lang/nb\n76592 spacy/lang/da\n74840 spacy/lang/sv\n60672 spacy/lang/ca\n50880 spacy/lang/es\n48296 spacy/lang/fr\n41688 spacy/lang/de\n36960 spacy/lang/nl\n34008 spacy/lang/it\n32632 spacy/lang/ro\n24160 spacy/lang/lt\n8712 spacy/lang/en  <--- THE ONE I WANT\nIs there a spacy-specifc way of installing spacy without all of these languages?\nI can hack around post-install, but is there a safer way to install fewer languages?\nMy environment, and details about the versions installed in the above...\n$ pip freeze\nblis==0.2.4\ncertifi==2019.6.16\nchardet==3.0.4\ncymem==2.0.2\nidna==2.8\nmurmurhash==1.0.2\nnumpy==1.16.4\nplac==0.9.6\npreshed==2.0.1\nrequests==2.22.0\nspacy==2.1.6\nsrsly==0.0.7\nthinc==7.0.8\ntqdm==4.32.2\nurllib3==1.25.3\nwasabi==0.2.2\n$ python --version\nPython 3.7.4\nInstalled on MacOS", "issue_status": "Closed", "issue_reporting_time": "2019-07-17T21:56:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "519": {"issue_url": "https://github.com/explosion/spaCy/issues/3978", "issue_id": "#3978", "issue_summary": "Fasttext vectors usage in spacy", "issue_description": "van51 commented on 17 Jul 2019\nSpacy recently added support for fasttext vectors, but it is not clear to me how to package them, along with the info about subword features. init-model does not look like it knows to work with both the word vectors and the ngram vectors.\nThe approach I have been using so far is to create a custom Spacy component which sets the\nvector user_hooks to a wrapper function which knows how to properly embed even OOV words.\nHowever, later on I realised that these vectors hooks are not used throughout training, since the Tok2Vec expects to find the vectors pre-loaded in the vocab.\nCould you tell me if I am missing something or if there is an easy work-around?\nEnvironment\nspaCy version: 2.1.4\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.3\nModels: en\n2", "issue_status": "Closed", "issue_reporting_time": "2019-07-17T08:41:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "520": {"issue_url": "https://github.com/explosion/spaCy/issues/3977", "issue_id": "#3977", "issue_summary": "Weird error when using regex in EntityRuler", "issue_description": "talbaumel commented on 17 Jul 2019\nHow to reproduce the behaviour\nimport spacy \nfrom spacy.pipeline import EntityRuler\n\n\nnlp = spacy.load(\"en_core_web_sm\")\nruler = EntityRuler(nlp)\npatterns = [{\"label\": \"EMAIL\", \n             \"pattern\":[{\"REGEX\": r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\"}]},\n            {\"label\": \"PHONE\", \n             \"pattern\":[{\"REGEX\": r\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d*|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d*|\\d{3}[-\\.\\s]??\\d*)\"}]}]\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler)\n\ndoc = nlp(u\"mymail@mail.com abcd abcd 111111111\")\nprint([(ent.text, ent.label_) for ent in doc.ents])\nReturns:\n[('mymail@mail.com', 'PHONE'), ('abcd', 'PHONE'), ('abcd', 'PHONE'), ('111111111', 'EMAIL')]\nThe regexs work fine when using re.findall\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7.1\nspaCy Version Used: 2.1.3\nEnvironment Information:\n1", "issue_status": "Closed", "issue_reporting_time": "2019-07-17T08:20:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "521": {"issue_url": "https://github.com/explosion/spaCy/issues/3976", "issue_id": "#3976", "issue_summary": "incorrect pos tag", "issue_description": "dlemke01 commented on 17 Jul 2019\n\"quit\" is being tagged as a noun, when it should be a verb.\nHow to reproduce the behaviour\ndoc = nlp(u\"There were 100 contestants on the gameshow and then 20 quit.\")\n[(token, token.pos_) for token in doc]\n[(There, u'ADV'), (were, u'VERB'), (100, u'NUM'), (contestants, u'NOUN'), (on, u'ADP'), (the, u'DET'), (gameshow, u'NOUN'), (and, u'CCONJ'), (then, u'ADV'), (20, u'NUM'), (quit, u'NOUN'), (., u'PUNCT')]\nas opposed to:\ndoc = nlp(u\"There were 100 contestants on the gameshow and then 20 decided to quit.\")\n[(token, token.pos_) for token in doc]\n[(There, u'ADV'), (were, u'VERB'), (100, u'NUM'), (contestants, u'NOUN'), (on, u'ADP'), (the, u'DET'), (gameshow, u'NOUN'), (and, u'CCONJ'), (then, u'ADV'), (20, u'NUM'), (decided, u'VERB'), (to, u'PART'), (quit, u'VERB'), (., u'PUNCT')]\nYour Environment\nInfo about spaCy\nPython version: 2.7.16\nPlatform: Linux-5.1.12-arch1-1-ARCH-x86_64-with-glibc2.2.5\nspaCy version: 2.1.3\nModels: en_core_web_sm", "issue_status": "Closed", "issue_reporting_time": "2019-07-17T04:20:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "522": {"issue_url": "https://github.com/explosion/spaCy/issues/3975", "issue_id": "#3975", "issue_summary": "Unable to process multiple jsonlines files as input", "issue_description": "Contributor\nakornilo commented on 17 Jul 2019 \u2022\nedited\nHow to reproduce the behaviour\nI am trying to train a new spacy NER model, my training data is spread across multiple jsonlines files in one directory. I created each example using from spacy.gold import docs_to_json.\nUsing this data, I tried to run python -m spacy train en testtest train_docs/ test_docs/ -b en\nand got the error:\nMy train_docs and test_docs directories look like:\n0000.jsonl\n0001.jsonl\nYour Environment\nOperating System: Ubuntu 16.04.5 LTS (Xenial Xerus)\nPython Version Used: Python 3.6.5 :: Anaconda, Inc.\nspaCy Version Used: 2.1.5\nEnvironment Information:\nA fix?\nI made a custom fix - but I would like to know if there is a better solution.\nI was able to fix this issue locally, by modifying the gold.pyx file as follows:\n@@ -142,7 +142,7 @@ class GoldCorpus(object):\n                 continue\n             elif path.is_dir():\n                 paths.extend(path.iterdir())\n-            elif path.parts[-1].endswith(\".json\"):\n+            elif path.parts[-1].endswith(\".json\") or path.parts[-1].endswith(\".jsonl\"):\n                 locs.append(path)\n         return locs\n\n@@ -154,7 +154,7 @@ class GoldCorpus(object):\n             if loc.parts[-1].endswith(\"json\"):\n                 gold_tuples = read_json_file(loc)\n             elif loc.parts[-1].endswith(\"jsonl\"):\n-                gold_tuples = srsly.read_jsonl(loc)\n+                gold_tuples = jsonl_to_tuples(srsly.read_jsonl(loc))\n             elif loc.parts[-1].endswith(\"msg\"):\n                 gold_tuples = srsly.read_msgpack(loc)\n             else:\n@@ -311,6 +311,10 @@ def json_to_tuple(doc):\n         if sents:\n             yield [paragraph.get(\"raw\", None), sents]\n\n+def jsonl_to_tuples(docs):\n+    for doc in docs:\n+        for json_tuple in json_to_tuple(doc):\n+            yield json_tuple", "issue_status": "Closed", "issue_reporting_time": "2019-07-16T21:57:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "523": {"issue_url": "https://github.com/explosion/spaCy/issues/3974", "issue_id": "#3974", "issue_summary": "Transient segmentation fault in noun_chunks and as_doc()", "issue_description": "tedgoddard commented on 16 Jul 2019 \u2022\nedited\nThis crash was observed on macOS but also occurs on Ubuntu 18.04 on docker. The error reported on linux is slightly more informative than \"segmentation fault\", it's \"corrupted size vs. prev_size\" or \"corrupted double-linked list\". The crash is being called transient because it only occurs after many invocations, hence the while loop. It also seems to occur less often (never?) for short text input.\nAs a workaround, I am using nlp(chunk.text) rather than as_doc(), but this is significantly slower.\nHow to reproduce the behaviour\nimport spacy\n\nnlp = spacy.load('en_core_web_sm')\n\nwhile True:\n    text = \"Goku is introduced in the Dragon Ball manga and anime at 12 years of age[23] (initially, he claims to be 14,[22] but it is later clarified during the Tournament Saga that this is because Goku had trouble counting), as a young boy living in obscurity on Mount Paozu. Goku owns the Power Pole and the four-star Dragon Ball, inheritances from his grandfather.\"\n    doc = nlp(text)\n    result = [ [token.text for token in chunk.as_doc()] for chunk in doc.noun_chunks ]\n    print(result)\nYour Environment\nspaCy version: 2.1.6\nPlatform: Linux-4.9.125-linuxkit-x86_64-with-Ubuntu-18.04-bionic\nPython version: 3.6.8\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-07-16T16:15:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "524": {"issue_url": "https://github.com/explosion/spaCy/issues/3973", "issue_id": "#3973", "issue_summary": "Create your own tok2vec", "issue_description": "RonRademaker commented on 16 Jul 2019\nIf I understand correctly, the tok2vec property that was created for the bert-like pretraining is a model that creates an embedding vector for a token instead of just getting one from a word2vec model. Is there an API or something I could implement to create my own tok2vec model?\nWhich page or section is this issue related to?\n#3448", "issue_status": "Closed", "issue_reporting_time": "2019-07-16T14:13:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "525": {"issue_url": "https://github.com/explosion/spaCy/issues/3972", "issue_id": "#3972", "issue_summary": "PhraseMatcher returns only 1 match while more than 1 rules are verified", "issue_description": "cverluise commented on 16 Jul 2019 \u2022\nedited\nHello,\nWe are working on a project (PatentCity) to collect inventors location directly from early ages patent documents. spaCy is undoubtedly our library. So, please, let me start by a big \"Thank You\" to the spaCy developing community for your awesome work .\nIssue\nWe try to complement statistical GPE recognition with administrative entity lists look-up. To do so, we use PhraseMatcher (from spacy.matcher). We also want to track which kind of entity we match (e.g CITY, COUNTY, STATE, etc). Hence, we have a different rule for each.\nNow, some entities have common ngrams. E.g, 'New York' is a STATE, a COUNTY and a CITY. In these case, the PhraseMatcher only returns 1 match. E.g, 'New York' is matched only by a single rule, let say the CITY rule. It seems that the last rule has the priority.\nWe find this behavior potentially misleading. Is it in the spirit of the PhraseMatcher or should it be changed ?\nThanks in advance for help !\nHow to reproduce the behaviour\nfrom spacy.matcher import PhraseMatcher\n\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nmatcher = PhraseMatcher(nlp.vocab)\n\nmatcher.add('COUNTY', None, *[nlp('New York')])\nmatcher.add('CITY', None, *[nlp('New York')])\n\nmatcher._docs\n# Check that the PhraseMatcher was properly populated\n# {14532842148348552135: (New York,), 13852145969607952771: (New York,)}  # ok\n\nmatcher(nlp('I live in New York'))\n# [(13852145969607952771, 3, 5)]  # Only 1 match\n1", "issue_status": "Closed", "issue_reporting_time": "2019-07-16T10:37:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "526": {"issue_url": "https://github.com/explosion/spaCy/issues/3971", "issue_id": "#3971", "issue_summary": "\ud83d\udcab Proposal: API for efficient serializable dictionaries and lookup tables", "issue_description": "Member\nines commented on 16 Jul 2019\nThis topic came up while talking to @polm at spaCy IRL about how to best incorporate the Unidic dictionary for Japanese. Since the licensing allows it, we ideally want to be shipping it within a model package \u2013 and we realised that we'd probably be able to provide an efficient API for this within the core library that could also be useful for many other tasks.\nProblem\nSome languages and use cases may need to depend on large dictionaries or lookup tables for tokenization, lemmatization or other custom features. Including the linguistic resources with the spaCy core library itself is problematic because it creates a lot of bloat (see #3258). It's also not an option for very custom resources. For instance, a user may want to build a biomedical model and include a large dictionary to use in a custom pipeline component.\nImporting large dictionaries and serializing them with the model is currently possible out-of-the-box using the EntityRuler, or a custom pipeline component with custom serialization methods. However, this is something a user would have to implement themselves \u2013 including how to make the lookup as fast and as efficient as possible.\nProposed solution\nspaCy should provide an API for adding, accessing and serializing large dictionaries and lookup tables.\nThe lookup methods and data should live within the Vocab object to make it all accessible beyond pipeline components (e.g. in a custom tokenizer). It also seems like a much better fit conceptually: the vocab already holds the string store and vectors, so it makes sense for it to also hold the lookup tables. When the nlp object is serialized, the data will be written to the /vocab directory saved to disk.\nAdvantages\nUser lookups and dictionaries can take advantage of spaCy's built-in hashing.\nSerialization out-of-the-box to make it easy to ship dictionaries and lookups with custom models.\nSince the API isn't tied to any specific internal use case, the lookups can be implemented anywhere \u2013 in a custom component, the tokenizer, the lemmatizer etc.\nWe can finally consider moving the lemmatizer lookup tables and extensive tokenizer exception lists out of the library and provide them as separate files that can be loaded in, or as part of the statistical models we ship.\nImplementation details: set membership with Bloom filters\nSome of the large language data lists are used for simple set-membership checks. For instance, the lemmatizer needs to know whether some word-form is a valid lemma. Certain segmenters (e.g. for Japanese) also need to make frequent set-membership tests. It would be good to move these lookups into Bloom filters, to save space and improve efficiency. It's likely we can find a good library with a Bloom filter implementation, however we'd prefer to avoid introducing another dependency for this sort of thing. It would be best to have an implementation ourselves in the preshed library, even if it means forking someone else's code and pasting it in.\nUser-facing API\nThis is just a rough sketch and all subject to change.\nSome of the basic principles are:\nLookups can only be performed using integer IDs, not strings.\nLookups can be nested, i.e. one lookup can return another lookup table.\n# Add a new table\nnlp.vocab.lookups.add_table(\"my_lookup\")\n# Get a table\nlookup_table = nlp.vocab.lookups.get_table(\"my_lookup\")\n# Look up an ID in a table\nvalue = lookup_table.get(nlp.vocab.strings[\"cat\"])\n# Set an value in a table\nlookup_table.set(nlp.vocab.strings[\"cat\"], nlp.vocab.strings[\"meow\"])\nUsage example: in a custom pipeline component\ndef custom_lemmatizer(doc):\n    # Set the lemmas for each token in a doc given its POS tag and text\n    lookup_table = doc.vocab.lookups.get_table(\"custom_lemmatizer\")\n    for token in doc:\n        lemmas = lookup_table.get_table(token.pos)\n        token.lemma = lemmas.get(token.orth)\n    return doc\nUsage example: in an extension attribute getter\ndef resolve_abbreviation(token):\n    # Look up a token in an abbreviation dictionary\n    lookup_table = token.doc.vocab.lookups.get_table(\"abbreviations\")\n    return lookup_table.get(token.orth)\n\nToken.set_extension(\"abbrev\", getter=resolve_abbreviation)\n2\n1", "issue_status": "Closed", "issue_reporting_time": "2019-07-16T10:16:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "527": {"issue_url": "https://github.com/explosion/spaCy/issues/3970", "issue_id": "#3970", "issue_summary": "\u0421/\u0421++ free(): invalid next size (fast)", "issue_description": "gorqkop commented on 16 Jul 2019\nSpacy 2.1.1 - 2.1.6\nPython 3.6\nUbuntu 18.04.1 LTS\nIn Spacy since 2.1 randomly occurs memory errors from \u0421/\u0421++ like \"free(): invalid next size (fast)\" no any additional information are provided in logs. Has anybody meet the same problems?\n1", "issue_status": "Closed", "issue_reporting_time": "2019-07-16T09:16:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "528": {"issue_url": "https://github.com/explosion/spaCy/issues/3968", "issue_id": "#3968", "issue_summary": "Invalid per entity NER accuracy", "issue_description": "Contributor\nFallakAsad commented on 15 Jul 2019 \u2022\nedited\nI trained a NER model with some new entity types, My model accuracy is as follows on a test set which shows per entity accuracy as well as total accuracy.\n{\n\"ents_f\": 84.65116279069768,\n\"ents_p\": 95.28795811518324,\n\"ents_per_type\": {\n\"label_1\": {\n\"f\": 39.47368421052631,\n\"p\": 24.59016393442623,\n\"r\": 100.0\n},\n\"label_2\": {\n\"f\": 90.47619047619048,\n\"p\": 90.47619047619048,\n\"r\": 90.47619047619048\n},\n\"label_3\": {\n\"f\": 80.00000000000001,\n\"p\": 80.0,\n\"r\": 80.0\n},\n\"label_4\": {\n\"f\": 80.0,\n\"p\": 72.72727272727273,\n\"r\": 88.88888888888889\n},\n\"label_5\": {\n\"f\": 14.14141414141414,\n\"p\": 8.045977011494253,\n\"r\": 58.333333333333336\n},\n\"label_6\": {\n\"f\": 100.0,\n\"p\": 100.0,\n\"r\": 100.0\n},\n\"label_7\": {\n\"f\": 50.0,\n\"p\": 50.0,\n\"r\": 50.0\n}\n},\n\"ents_r\": 76.15062761506276,\n\"las\": 0.0,\n\"tags_acc\": 0.0,\n\"token_acc\": 100.0,\n\"uas\": 0.0\n}\nHowever, If I evaluate same model on the slightly different set which contains same docs as in previous test set but have few more annotation for label-4 and label-5 which were missing for some docs in previous test set, the model gives '0' f-score for label-3 for which no annotation were changed in new test set. The evaluation result on new test set is:\n{\n\"ents_f\": 84.65346534653466,\n\"ents_p\": 95.53072625698324,\n\"ents_per_type\": {\n\"label_1\": {\n\"f\": 32.35294117647059,\n\"p\": 19.298245614035086,\n\"r\": 100.0\n},\n\"label_2\": {\n\"f\": 94.44444444444444,\n\"p\": 94.44444444444444,\n\"r\": 94.44444444444444\n},\n\"label_3\": {\n\"f\": 0.0,\n\"p\": 0.0,\n\"r\": 0.0\n},\n\"label_4\": {\n\"f\": 80.0,\n\"p\": 72.72727272727273,\n\"r\": 88.88888888888889\n},\n\"label_5\": {\n\"f\": 14.432989690721651,\n\"p\": 8.13953488372093,\n\"r\": 63.63636363636363\n},\n\"label_6\": {\n\"f\": 100.0,\n\"p\": 100.0,\n\"r\": 100.0\n},\n\"label_7\": {\n\"f\": 50.0,\n\"p\": 50.0,\n\"r\": 50.0\n}\n},\n\"ents_r\": 76.0,\n\"las\": 0.0,\n\"tags_acc\": 0.0,\n\"token_acc\": 100.0,\n\"uas\": 0.0\n}\nAlso, model is able to identify label_3 in many docs on new test set even though the f-score of label_3 is zero.\nEnvironment Information:\nOperating System: Linux-4.15.0-54-generic-x86_64-with-Ubuntu-16.04-xenial\nPython Version Used: 3.5.2\nspaCy Version Used: 2.1.6\nThanks in adv for help.", "issue_status": "Closed", "issue_reporting_time": "2019-07-15T11:04:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "529": {"issue_url": "https://github.com/explosion/spaCy/issues/3967", "issue_id": "#3967", "issue_summary": "How can I add words to model?", "issue_description": "sepidehamiri commented on 15 Jul 2019 \u2022\nedited\nFeature description\nI downloaded the en_core_web_lg package and I want to use it for similarity, but I have some special words, which the vectors are zero and I want to add the vector. What is the order?\nfor example:\n    nlp = spacy.load('en_core_web_lg')\n\n    vector_data = {u\"maestro\": np.random.uniform(-1, 1, (300,)),\n                   u\"bot\": np.random.uniform(-1, 1, (300,)),\n                   u\"nugget\": np.random.uniform(-1, 1, (300,))}\n    vocab = Vocab()\n    for word, vector in vector_data.items():\n        vocab.set_vector(word, vector)\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.", "issue_status": "Closed", "issue_reporting_time": "2019-07-15T10:55:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "530": {"issue_url": "https://github.com/explosion/spaCy/issues/3966", "issue_id": "#3966", "issue_summary": "Can't use Gpu under k8s", "issue_description": "jmvizcainoio commented on 15 Jul 2019\nHow to reproduce the problem\nWe are trying to use spacy using gpu under kubernetes environment. We have already nvidia drivers and docker installed and i can see the gpu.\nnvidia-smi\nMon Jul 15 07:40:10 2019\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n| N/A   52C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nBut when we try to use we got and error.\nValueError: GPU is not accessible. Was the library installed correctly?\nI think that the problem is related with thinc_gpu_ops library but i'm not sure.\npip install cupy-cuda90\npip install -U spacy[cuda90]\nPython 3.6.7 | packaged by conda-forge | (default, Jul  2 2019, 02:18:42)\n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import spacy\n>>> spacy.prefer_gpu()\nFalse\n>>> spacy.require_gpu()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/opt/conda/envs/myownenv/lib/python3.6/site-packages/thinc/neural/util.py\", line 74, in require_gpu\n    raise ValueError(\"GPU is not accessible. Was the library installed correctly?\")\nValueError: GPU is not accessible. Was the library installed correctly?\nYour Environment\nOperating System: Debian buster/sid\nPython Version Used: 3.6.7\nspaCy Version Used: 2.1.6\nEnvironment Information: Kubernetes", "issue_status": "Closed", "issue_reporting_time": "2019-07-15T07:44:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "531": {"issue_url": "https://github.com/explosion/spaCy/issues/3965", "issue_id": "#3965", "issue_summary": "How to save best models for Spacy", "issue_description": "pratapaprasanna commented on 15 Jul 2019\nHi all,\nCurrently i am going through this document and see that i can save the model. Is there any facility to save the best model as-in the model which doesn't overfit. I didnt find any relevant part in the link saving and loading\nOperating System: Ubuntu 18.04\nPython Version Used: python 3.6.8\nspaCy Version Used: 2.1.4\nEnvironment Information:\nThanks in advance", "issue_status": "Closed", "issue_reporting_time": "2019-07-15T06:00:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "532": {"issue_url": "https://github.com/explosion/spaCy/issues/3963", "issue_id": "#3963", "issue_summary": "comparison to magpie", "issue_description": "bytearchive commented on 15 Jul 2019\ncan you provide comparison to magpie", "issue_status": "Closed", "issue_reporting_time": "2019-07-14T23:11:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "533": {"issue_url": "https://github.com/explosion/spaCy/issues/3962", "issue_id": "#3962", "issue_summary": "Doc.to_json() method fails when called on doc generated via `Span.as_doc()`", "issue_description": "JohnStuartRutledge commented on 14 Jul 2019 \u2022\nedited\nHow to reproduce the behaviour\nIt would appear that calling to_json() on a Doc that was created via the as_doc() method is prone to failure. This may very well be connected to other issues with with as_doc like the that mentioned in issue #3669\nimport spacy\n\nnlp = spacy.load('en')\ndoc = nlp('He jests at scars, that never felt a wound.')\nspan = doc[0:3]      # \"He jests at\"\ndoc2 = span.as_doc()\ndoc2.to_json()\n# IndexError: [E040] Attempt to access token at 7, max length 3.\nThe above error seems to trigger when second token of the span (jests), calls spacy.tokens.Token.head.__get__() and hits the self.c.head\nEnvironment\nspaCy version: 2.1.6\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.6.4\nModels: en", "issue_status": "Closed", "issue_reporting_time": "2019-07-13T22:43:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "534": {"issue_url": "https://github.com/explosion/spaCy/issues/3960", "issue_id": "#3960", "issue_summary": "Regex does not pick up the token that contains a word", "issue_description": "SaulML commented on 13 Jul 2019\nHi,\nI am running below simple code to obtain all tokens that contains the word ( for example, words containing acompared, notcompared, thiscompared).\nBut, the spaCy regrex does not return anything. The regular expression word fine on python re.\nCould you let me know if this is an spaCy issue or how resolve the issue?\nimport plac\nfrom spacy.lang.en import English\nfrom spacy.matcher import PhraseMatcher, Matcher\nfrom spacy.tokens import Doc, Span, Token\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ntext = \"\"\"\n\"Net income was $9.4 million acompared to the prior year of $2.7 million.\",\n\"Revenue exceeded twelve billion dollars, with a loss of $1b. run\",\n\"\"\"\ndoc = nlp(text)\npattern = [{\"LOWER\": {\"REGEX\": \"\\b\\wcompared\\w\\b\"}}]\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"item\", None, pattern )\nmatches = matcher(doc)\nprint(matches)\nprint(matcher)\nThanks,\nSaul\nYour Environment\nOperating System:\nUbuntu 18\nPython Version Used:\n3.7.3\nspaCy Version Used:\n2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-13T02:06:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "535": {"issue_url": "https://github.com/explosion/spaCy/issues/3959", "issue_id": "#3959", "issue_summary": "Modified pos attribute not being serialized", "issue_description": "aabbi commented on 12 Jul 2019\nHow to reproduce the behaviour\nSince pos attribute of tokens is now writable (related comment ), I ran into the issue that I changed the pos tag and serialized the doc but on deserializing, it retained the original pos tag.\nI believe this is happening because only the tag attributes are pickled\nimport spacy\nnlp = spacy.load('en')\nd = nlp(\"displaCy uses JavaScript, SVG and CSS to show you how computers understand language\")\nprint(d[0].pos_) # prints ADJ\nd[0].pos_ = \"NOUN\"\nd.to_disk(\"t.txt\")\n\nd2 = nlp(\"\")\nd2.from_disk(\"t.txt\")\nprint(d2[0].pos_) # prints ADJ\nWasn't entirely sure if this should be a bug or a feature request to make pos tags picklable or maybe documentation could be added specifying how serializing would not save changes in coarse grained attributes with pos tag as examples\nYour Environment\nspaCy version: 2.1.3\nPlatform: Ubuntu-14.04-trusty\nPython version: 3.6.8\nModels: en_core_web_lg, en, en_core_web_md\n1", "issue_status": "Closed", "issue_reporting_time": "2019-07-12T16:53:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "536": {"issue_url": "https://github.com/explosion/spaCy/issues/3958", "issue_id": "#3958", "issue_summary": "POS tag of \"LANG\" assigned to tokens, causing a KeyError", "issue_description": "bdewilde commented on 12 Jul 2019 \u2022\nedited\nHow to reproduce the behaviour\nHello! I upgraded to v2.1.5 and ran into an issue POS-tagging a text that wasn't present yesterday in v2.1.4. Specifically, the en_core_web_sm model assigns \"LANG\" as a POS tag for some tokens, which afaik isn't a valid value. This, in turn, raises a KeyError when calling tok.pos_ on the offending tokens, since parts_of_speech.IDS doesn't have \"LANG\" as a key.\nDiving in, I see that the POS tags are nonsensical:\n>>> [(tok, doc.vocab[tok.pos].text) for tok in doc]\n...\n (., 'PROPN'),\n (, 'EOL'),\n (In, 'ADJ'),\n (this, 'CCONJ'),\n (case, 'INTJ'),\n (it, 'PART'),\n (was, 'SYM'),\n (several, 'LANG'),\n (feet, 'INTJ'),\n (below, 'ADJ'),\n (it, 'PART'),\n (., 'PROPN'),\n (But, 'CONJ'),\n (a, 'CCONJ'),\n (section, 'INTJ'),\n...\nI have no idea what's gone wrong. Here's a full example, using a fresh install of both spacy and the model:\nIn [1]: import spacy\n\nIn [2]: nlp = spacy.load(\"en\")\n\nIn [3]: doc = nlp(\"This is an example sentence.\")\n\nIn [4]: [(tok, doc.vocab[tok.pos].text) for tok in doc]\nOut[4]:\n[(This, 'CCONJ'),\n (is, 'SYM'),\n (an, 'CCONJ'),\n (example, 'INTJ'),\n (sentence, 'INTJ'),\n (., 'PROPN')]\nYour Environment\nspaCy version: 2.1.5\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.0\nModels: es, en, xx\n2", "issue_status": "Closed", "issue_reporting_time": "2019-07-12T14:48:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "537": {"issue_url": "https://github.com/explosion/spaCy/issues/3956", "issue_id": "#3956", "issue_summary": "Unexpected behaviour about noun_chunks in specific examples", "issue_description": "DiamondI commented on 12 Jul 2019\nHow to reproduce the behaviour\nCode:\nimport spacy\n\nnlp = spacy.load(\"en\")\n\nsentences = [\n    \"Fruits, for example bananas, oranges and peaches.\",\n    \"Fruits, for example, bananas, oranges and peaches.\",\n    \"Fruits, for example apples, bananas, oranages and peaches.\",\n    \"Fruits, for example, apples, bananas, oranges and peaches.\",\n    \"Fruits, for example apples, bananas, watermelons, oranages and peaches.\",\n    \"Fruits, for example, apples, bananas, watermelons, oranages and peaches.\"\n]\n\ncnt = 6\n\nfor sentence in sentences:\n    doc = nlp(sentence)\n    results = list([i for i in doc.sents][0].noun_chunks)\n    if cnt & 1:\n        print(\"'for example,' with comma\")\n    else:\n        print(\"'for example' without comma\")\n    print(\"Sentence: {sentence}\\nReal Examples: {coe}\\nNumber of noun phrases:\"\n          \" {len_of_result}\\nResults: {results}\\n\\n\".\n         format(sentence=sentence, coe=cnt//2,\n                len_of_result=len(results), results=results))\n    cnt += 1\nOutputs:\n'for example' without comma\nSentence: Fruits, for example bananas, oranges and peaches.\nReal Examples: 3\nNumber of noun phrases: 5\nResults: [Fruits, example, bananas, oranges, peaches]\n\n\n'for example,' with comma\nSentence: Fruits, for example, bananas, oranges and peaches.\nReal Examples: 3\nNumber of noun phrases: 5\nResults: [Fruits, example, bananas, oranges, peaches]\n\n\n'for example' without comma\nSentence: Fruits, for example apples, bananas, oranages and peaches.\nReal Examples: 4\nNumber of noun phrases: 6\nResults: [Fruits, example, apples, bananas, oranages, peaches]\n\n\n'for example,' with comma\nSentence: Fruits, for example, apples, bananas, oranges and peaches.\nReal Examples: 4\nNumber of noun phrases: 5\nResults: [Fruits, example, bananas, oranges, peaches]\n\n\n'for example' without comma\nSentence: Fruits, for example apples, bananas, watermelons, oranages and peaches.\nReal Examples: 5\nNumber of noun phrases: 7\nResults: [Fruits, example, apples, bananas, watermelons, oranages, peaches]\n\n\n'for example,' with comma\nSentence: Fruits, for example, apples, bananas, watermelons, oranages and peaches.\nReal Examples: 5\nNumber of noun phrases: 6\nResults: [Fruits, example, bananas, watermelons, oranages, peaches]\nFrom the example above, we shall see that the noun phrase, 'apple' in this example, right behind \"for example, \" will be ignored by spacy when the number of example noun phrases are greater than 3. However, there's no such problem when using \"for example\" without a comma.\nYour Environment\nOperating System: Mac OS X 10.14.5 18F132\nPython Version Used: Python 3.6.3\nspaCy Version Used: 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-12T07:29:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "538": {"issue_url": "https://github.com/explosion/spaCy/issues/3955", "issue_id": "#3955", "issue_summary": "install fails PEP 517 , thinc --- need fix quickly for project deadline --- switching back to NLTK for now", "issue_description": "SumeetSandhu commented on 11 Jul 2019\nI have a 2015 MacBook Pro running Mojave 10.14.5\nPython 2.7.16\npip 19.1.1\ngcc version 5.1.0\nI tried upgrading to the latest spaCy with different options eg --no-use-pep517 , --no-binary etc. They didn't work and removed old spaCy - installing with no options didn't work.\nError message excerpts look like:\nBuilding wheels for collected packages: spacy\nBuilding wheel for spacy (PEP 517) ... error\n<-----long list of errors------------------------------------------>\ngcc: error: unrecognized command line option \u2018-stdlib=libc++\u2019\nerror: command 'gcc' failed with exit status 1\nERROR: Failed building wheel for spacy\nRunning setup.py clean for spacy\nFailed to build spacy\nBuilding wheels for collected packages: thinc\nBuilding wheel for thinc (setup.py) ... error\n<-----another long list of errors------------------------------------------>\ngcc: error: unrecognized command line option \u2018-stdlib=libc++\u2019\nerror: command 'gcc' failed with exit status 1\nERROR: Failed building wheel for thinc\nRunning setup.py clean for thinc\nFailed to build thinc\nERROR: Could not build wheels for spacy which use PEP 517 and cannot be installed directly", "issue_status": "Closed", "issue_reporting_time": "2019-07-11T15:44:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "539": {"issue_url": "https://github.com/explosion/spaCy/issues/3954", "issue_id": "#3954", "issue_summary": "Should we pass sentences that contain no entities when training NER model?", "issue_description": "erotavlas commented on 11 Jul 2019\nIs it ok to pass sentences that contain no entities when training the NER model? Will spacy even accept these sentences and learn from them, or will it discard them?", "issue_status": "Closed", "issue_reporting_time": "2019-07-11T15:36:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "540": {"issue_url": "https://github.com/explosion/spaCy/issues/3952", "issue_id": "#3952", "issue_summary": "How to make NER more consistent? It catches and misses the same entities within the same text.", "issue_description": "MrEricL commented on 11 Jul 2019\nI trained a NER model and I found the recall has some minor issues.\nCompanies are usually only recognized when it has a \"Inc\" or \"Co\". \"Apple Inc.\" works fine, but \"Apple\" is fairly inconsistent\nThere are struggles with last names. \"Bob Smith\" will do fine, but \"Smith\" is more of a hit or miss\nCertain entities like \"Company X\" will be tagged and not tagged\nMy guess is that my data is more user comments, so it's not as uniform and structured as well as a news article. With a bit more randomness, what's the best way to approach it? I've already taken into considerationEntityRuler but it won't help cover cases I don't expect.", "issue_status": "Closed", "issue_reporting_time": "2019-07-11T14:20:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "541": {"issue_url": "https://github.com/explosion/spaCy/issues/3951", "issue_id": "#3951", "issue_summary": "Issue with several optional rule in Token Matcher", "issue_description": "alteest commented on 11 Jul 2019 \u2022\nedited\nIn my Token Matcher I use OP '?' and if I use it once in rule it works, but is I use it several times it doesn't.\nWith pattern like : pattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"my\", \"OP\": \"?\"}, {'LENGTH': {'>': 1}, 'OP': '?'}, {\"LOWER\": \"world\"}]\nI expect to match only : \"Hello world\", \"Hello my world\" or \"Hello my world\" (or \"Hello world\")\nBut it also match phrase \"Hello this small world\", so this is an issue because I want to have any token only once between \"hello\" and \"world\" with or without token \"my\".\nBut, for example if I use rule: pattern = [{\"LOWER\": \"hello\"}, {'LENGTH': {'>': 1}, 'OP': '?'}, {\"LOWER\": \"world\"}]\nIt works well with phrase like: \"Hello world\"\nAnd rule : pattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"my\", \"OP\": \"?\"}, {\"LOWER\": \"world\"}] also properly match both phrases : \"Hello world\" and \"Hello my world\"\nSo, I see only issue when we use rule {\"OP\": \"?\"} one by one\nimport spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\npattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"my\", \"OP\": \"?\"}, {'LENGTH': {'>': 1}, 'OP': '?'},  {\"LOWER\": \"world\"}]\n#pattern = [{\"LOWER\": \"hello\"}, {'LENGTH': {'>': 1}, 'OP': '?'},  {\"LOWER\": \"world\"}]\n#pattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"my\", \"OP\": \"?\"}, {\"LOWER\": \"world\"}]\nmatcher.add(\"HelloWorld\", None, pattern)\n\n\nfor text in (\"Hello world\", \"Hello my world\", \"Hello big world\", \"Hello this small world\"):\n    doc = nlp(text)\n    matches = matcher(doc)\n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]  # Get string representation\n        span = doc[start:end]  # The matched span\n        print(text, match_id, string_id, start, end, span.text)\nspaCy version: 2.1.3\nPlatform: Linux-4.15.0-1037-azure-x86_64-with-debian-stretch-sid\nPython version: 3.6.0\nModels: de, en, fr", "issue_status": "Closed", "issue_reporting_time": "2019-07-11T11:49:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "542": {"issue_url": "https://github.com/explosion/spaCy/issues/3947", "issue_id": "#3947", "issue_summary": "Anaconda Unsatisfiable Error", "issue_description": "mertyertugrul commented on 11 Jul 2019\nHow to reproduce the problem\nI am trying to install Spacy package using Anaconda Navigator but I get this error:\nUnsatisfiableError: The following specifications were found to be incompatible with each other:\n\n  - anaconda==2019.03=py37_0 -> importlib_metadata==0.8=py37_0 -> zipp[version='>=0.3.2']\n  - pkgs/main/osx-64::importlib_metadata==0.8=py37_0 -> zipp[version='>=0.3.2']\n  - pkgs/main/osx-64::path.py==11.5.0=py37_0 -> importlib_metadata[version='>=0.5']\n  - pkgs/main/osx-64::zipp==0.3.3=py37_1\nThis probably happens not because Spacy but my Anaconda environment but I thought you might know how to resolve the issue.\nPlus if I use terminal with conda command I get this error message:\nconda install -c conda-forge spacy\nWARNING conda.base.context:use_only_tar_bz2(632): Conda is constrained to only using the old .tar.bz2 file format because you have conda-build installed, and it is <3.18.3. Update or remove conda-build to get smaller downloads and faster extractions.\n\nCollecting package metadata (repodata.json): done\n\nSolving environment: failed\n\n\n\nUnsatisfiableError: The following specifications were found to be incompatible with each other:\n\n- anaconda==2019.03=py37_0 -> anaconda-client==1.7.2=py37_0 -> nbformat -> jsonschema[version='>=2.4,!=2.5.0']\n\n- anaconda==2019.03=py37_0 -> importlib_metadata==0.8=py37_0\n\n- jsonschema - pkgs/main/osx-64::_ipyw_jlab_nb_ext_conf==0.1.0=py37_0 -> ipywidgets -> nbformat[version='>=4.2.0'] -> jsonschema[version='>=2.4,!=2.5.0'] - pkgs/main/osx-64::anaconda-client==1.7.2=py37_0 -> nbformat -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::anaconda-navigator==1.9.7=py37_0 -> anaconda-client[version='>=1.6.14'] -> nbformat[version='>=4.4.0'] -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::importlib_metadata==0.8=py37_0 - pkgs/main/osx-64::ipywidgets==7.4.2=py37_0 -> nbformat[version='>=4.2.0'] -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::jupyter==1.0.0=py37_7 -> ipywidgets -> nbformat[version='>=4.2.0'] -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::jupyterlab==0.35.4=py37hf63ae98_0 -> jupyterlab_server[version='>=0.2.0,<0.3.0'] -> notebook -> nbconvert -> nbformat[version='>=4.4'] -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::jupyterlab_server==0.2.0=py37_0 -> notebook -> nbconvert -> nbformat[version='>=4.4'] -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::nbconvert==5.4.1=py37_3 -> nbformat[version='>=4.4'] -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::nbformat==4.4.0=py37_0 -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::notebook==5.7.8=py37_0 -> nbconvert -> nbformat[version='>=4.4'] -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::path.py==11.5.0=py37_0 -> importlib_metadata[version='>=0.5']\n\n- pkgs/main/osx-64::spyder==3.3.3=py37_0 -> nbconvert -> nbformat[version='>=4.4'] -> jsonschema[version='>=2.4,!=2.5.0']\n\n- pkgs/main/osx-64::widgetsnbextension==3.4.2=py37_0 -> notebook[version='>=4.4.1'] -> nbconvert -> nbformat[version='>=4.4'] -> jsonschema[version='>=2.4,!=2.5.0']\nYour Environment\nOperating System: MacOS Mojave\nPython Version Used: 3.7.3\nspaCy Version Used: 2.0.9\nEnvironment Information: Anaconda Navigator 1.9.7", "issue_status": "Closed", "issue_reporting_time": "2019-07-10T21:19:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "543": {"issue_url": "https://github.com/explosion/spaCy/issues/3944", "issue_id": "#3944", "issue_summary": "tokenizer.add_special_case not working", "issue_description": "bklippstein commented on 10 Jul 2019\ntokenizer.add_special_case is not working for me.\nimport spacy\n\nfrom spacy.symbols import ORTH, POS, X, LEMMA\n\nnlp = spacy.load('en')\n\nnlp.tokenizer.add_special_case('\u00bb', [{ORTH: '\u00bb', POS: X, LEMMA:'Test'  }])\nnlp.tokenizer.add_special_case('\u00ab', [{ORTH: '\u00ab', POS: X, LEMMA:'Test' }])\n\ndoc = nlp('He said: \u00bbI am lying.\u00ab')\n\nfor token in doc:\n    print('{:10}{:10}{:10}'.format(token.text, token.pos_, token.lemma_))\nreturns\nHe        PRON      -PRON-    \nsaid      VERB      say       \n:         PUNCT     :         \n\u00bb         PUNCT     \u00bb         \nI         PRON      -PRON-    \nam        VERB      be        \nlying     VERB      lie       \n.         PUNCT     .         \n\u00ab         PUNCT     \u00ab   \nThe reason why I try to change the POS tags is that they are wrong for '\u00bb' and '\u00ab' in the german model.\nEven the code from here is not working:\nimport spacy\n\nfrom spacy.symbols import ORTH, POS, NOUN, VERB\n\nnlp = spacy.load('en')\n\nnlp.tokenizer.add_special_case('{G}', [{ORTH: '{G}', POS: NOUN}])\nnlp.tokenizer.add_special_case('{T}', [{ORTH: '{T}', POS: VERB}])\n\ndoc = nlp('This {G} a noun and this is a {T}')\n\nfor token in doc:\n    print('{:10}{:10}'.format(token.text, token.pos_))\n  \n# Wanted output:\n# This      DET       \n# {G}       NOUN      \n# a         DET       \n# noun      NOUN      \n# and       CCONJ     \n# this      DET       \n# is        VERB      \n# a         DET       \n# {T}       VERB\n\n# My output:\n# This      DET       \n# {G}       NOUN      \n# a         DET       \n# noun      NOUN      \n# and       CCONJ     \n# this      DET       \n# is        VERB      \n# a         DET       \n# {T}       NOUN   <<<<<<<<<<<<<<<<<<<\nYour Environment\nOperating System: Linux-5.0.0-20-generic-x86_64-with-debian-buster-sid\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.4 [cuda100]\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-10T12:56:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "544": {"issue_url": "https://github.com/explosion/spaCy/issues/3942", "issue_id": "#3942", "issue_summary": "Token after is_left_punct has is_sent_start == True", "issue_description": "Contributor\nBreakBB commented on 10 Jul 2019\nI am currently working with texts that include a lot of quotes and direct speech. The problem I am facing is that the quote tokens sometimes are registered as single token sentences instead of part of the sentence. The thing is that the token after an opening quote \u201c can be considered to be is_sent_start as well so the quote will be a sentence with just one token.\nWith the custom sentence segmentation I found a solution but I am wondering if this is intended or just a problem with the models.\nimport spacy\nnlp = spacy.load(\"en\")\ntext = u\"\u201cI want this to work,\u201d says a man. \u201cThat would be really great.\u201d\"\n\nprint(\"Before\\n\", list(nlp(text).sents))\n# [\u201c, I want this to work,\u201d says a man., \u201c, That would be really great., \u201d]\n\n\ndef custom_sentencizer(doc):\n    for t in doc:\n        # First token is always sent_start AND\n        # Token after is_left_punct should not be sent_start OR\n        # Last token should not be sent_start\n        if t.i != 0 and (t.nbor(-1).is_left_punct or t.i == len(doc) - 1):\n            t.is_sent_start = False\n    return doc\n\n\nnlp.add_pipe(custom_sentencizer, before=\"parser\")\n\ndoc = nlp(text)\nprint(\"\\nAfter:\\n\", list(doc.sents))\n# [\u201cI want this to work,\u201d says a man., \u201cThat would be really great.\u201d]\nYour Environment\nOperating System: Windows 10\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.4\n1", "issue_status": "Closed", "issue_reporting_time": "2019-07-10T11:48:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "545": {"issue_url": "https://github.com/explosion/spaCy/issues/3938", "issue_id": "#3938", "issue_summary": "Create proper pypi packages for downloadable models and data sets", "issue_description": "spearsem commented on 10 Jul 2019\nFeature description\nThere was an issue reported in #1143 but the stated solution (to modify egg-info) is not adequate for many use cases. For example, in a lot of Python projects it is customary to automatically parse the entries of requirements.txt and populate the install_requires field in setup.py when building a package.\nIf some of the dependencies need to be specified as git URLs, it can make this process needless complicated to deal with dependency links and updating egg-info tags.\nWhy not build spaCy downloadable artifacts as proper installable packages? This would be especially nice because the artifact can specify the range of versions of spacy that are compatible, and then users can specify everything with pinned versions like usual in a single place, and ensure their combination of spacy and downloaded artifacts will 'just work' regardless of what type of packaging / environment situation they are working in.\nPlease consider changing the existing data set release procedure to build proper pypi-distributed packages (and also conda-forge to be extra awesome).\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T20:59:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "546": {"issue_url": "https://github.com/explosion/spaCy/issues/3937", "issue_id": "#3937", "issue_summary": "No loss decrease in spacy-GPU training", "issue_description": "pratapaprasanna commented on 10 Jul 2019 \u2022\nedited\nHi all,\nI have trained a NER model with CPU and i could see that the NER loss decreasing but when i try to use the same code but add add spacy.require_gpu() and try using my GPU the NER loss was stagnate after few epochs\nBut my loss was decreasing when i ran my training on CPU\n    spacy.require_gpu()\n    nlp = spacy.blank('en')\n    ner = nlp.create_pipe('ner')\n    sbd = nlp.create_pipe('sentencizer')\n    nlp.add_pipe(sbd)\n    nlp.add_pipe(ner, last=True)\n    output_dir = config['TRAIN']['model_save_path']\n    for _, annotations in train_data:\n        for ent in annotations.get('entities'):\n            ner.add_label(ent[2])\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n    with nlp.disable_pipes(*other_pipes):\n        n_iter = int(config['TRAIN']['num_epochs'])\n        optimizer = nlp.begin_training(n_threads=8, device=0)\n        for itn in range(n_iter):\n            if config['TRAIN'][\"shuffle_data\"]:\n                random.shuffle(train_data)\n            losses = {}\n            batches = minibatch(train_data, size=2048)\nDid i miss anything and is there any approach to speed-up my training\nAny help would be off great use\nThanks in advance.\nYour Environment\nOperating System: 18.04\nPython Version Used: 3.6.8\nspaCy Version Used: spacy 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T19:17:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "547": {"issue_url": "https://github.com/explosion/spaCy/issues/3936", "issue_id": "#3936", "issue_summary": "Question: Can dependency parser use existing tokens and tags to increases accuracy?", "issue_description": "zhunyoung commented on 10 Jul 2019\nI've got a large amount of domain specific text. Can I use pre-defined tokens and tags to increase the accuracy of the dependency parser?", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T19:01:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "548": {"issue_url": "https://github.com/explosion/spaCy/issues/3935", "issue_id": "#3935", "issue_summary": "PCF python app deployment SpaCy install error code -9", "issue_description": "zhangyilun commented on 9 Jul 2019\nI'm deploying an app on PCF (Pivotal Cloud Foundry) that uses SpaCy and I'm having error when install SpaCy.\nThe manifest file is:\napplications:\n- name: xxxxx\n  instances: 1\n  memory: 2048MB\n  disk_quota: 2048MB\n  command: python app.py\n  buildpack: python_buildpack\n  stack: cflinuxfs3\n  timeout: 180\nThe error I'm getting is:\nCommand \"/tmp/contents587794960/deps/0/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-install-ralx3p8i/spacy/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-record-sx4g4727/install-record.txt --single-version-externally-managed --compile\" failed with error code -9 in /tmp/pip-install-ralx3p8i/spacy/\nI have seen others reporting this error but it was due to Python version, but mine's different (unless PCF is messing things up).\nI was able to deploy exactly the same app on Heroku with no above error.\nYour Environment\nOperating System: Ubuntu 18.04 (on PCF)\nPython Version Used: 3.6.8\nspaCy Version Used: 2.0.12", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T18:26:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "549": {"issue_url": "https://github.com/explosion/spaCy/issues/3934", "issue_id": "#3934", "issue_summary": "Doc.is_sentenced is False for single-token Docs", "issue_description": "Contributor\nmr-bjerre commented on 9 Jul 2019\nThe following code should pass imo.\nfrom spacy.lang.en import English\n\nnlp = English()\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\n\nassert len([s for s in nlp('The sentencizer is working fine. Right').sents]) == 2\nassert len([s for s in nlp('a').sents]) == 1\nbut it produces the following error due to the last line\nValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start.\nInfo about spaCy\nspaCy version: 2.1.4\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T17:07:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "550": {"issue_url": "https://github.com/explosion/spaCy/issues/3933", "issue_id": "#3933", "issue_summary": "Multiple NER components in one pipeline", "issue_description": "romlatron commented on 9 Jul 2019\nI know this issue was already posted in #1752, but I'm actually having the exact same problem, even though it was supposed to be fixed in #2159. I am loading different NER models which are contained in the ner_models folder in the NLP pipeline, as following:\nHow to reproduce the behaviour\nfor i, model in enumerate(os.listdir(path+ \"/ner_models\")):\n    ner = EntityRecognizer(nlp.vocab)\n    ner.from_disk(path+ \"/ner_models/\" + model, vocab=False)\n    nlp.add_pipe(ner, \"ner\" + str(i+1))\nI then get a segmentation fault when I apply the model to a sentence in which entities are detected by one of the NER (code 0xC0000005 with PyCharm), am I missing something?\nYour Environment\nspaCy version: 2.0.18 * Platform: Windows-10-10.0.17134-SP0 * Python version: 3.6.6", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T15:05:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "551": {"issue_url": "https://github.com/explosion/spaCy/issues/3932", "issue_id": "#3932", "issue_summary": "Dutch language model no longer loading after upgrade to spaCy 2.1.4", "issue_description": "Ruler26 commented on 9 Jul 2019\nAfter upgrading to spaCy 2.1.4 nlp=spacy.load('nl_core_news_sm') no longer works giving the error \"error: bad escape \\p at position 257\"\nloading an english model nlp=spacy.load('en_core_web_sm') does work.\nThe full error code is as follows:\nerror Traceback (most recent call last)\nin\n----> 1 nlp=spacy.load('nl_core_news_sm')\n2\nD:\\Anaconda\\lib\\site-packages\\spacy_init_.py in load(name, **overrides)\n25 if depr_path not in (True, False, None):\n26 deprecation_warning(Warnings.W001.format(path=depr_path))\n---> 27 return util.load_model(name, **overrides)\n28\n29\nD:\\Anaconda\\lib\\site-packages\\spacy\\util.py in load_model(name, **overrides)\n129 return load_model_from_link(name, **overrides)\n130 if is_package(name): # installed as package\n--> 131 return load_model_from_package(name, **overrides)\n132 if Path(name).exists(): # path to model data directory\n133 return load_model_from_path(Path(name), **overrides)\nD:\\Anaconda\\lib\\site-packages\\spacy\\util.py in load_model_from_package(name, **overrides)\n150 \"\"\"Load a model from an installed package.\"\"\"\n151 cls = importlib.import_module(name)\n--> 152 return cls.load(**overrides)\n153\n154\nD:\\Anaconda\\lib\\site-packages\\nl_core_news_sm_init_.py in load(**overrides)\n10\n11 def load(**overrides):\n---> 12 return load_model_from_init_py(file, **overrides)\nD:\\Anaconda\\lib\\site-packages\\spacy\\util.py in load_model_from_init_py(init_file, **overrides)\n188 if not model_path.exists():\n189 raise IOError(Errors.E052.format(path=path2str(data_path)))\n--> 190 return load_model_from_path(data_path, meta, **overrides)\n191\n192\nD:\\Anaconda\\lib\\site-packages\\spacy\\util.py in load_model_from_path(model_path, meta, **overrides)\n171 component = nlp.create_pipe(name, config=config)\n172 nlp.add_pipe(component, name=name)\n--> 173 return nlp.from_disk(model_path)\n174\n175\nD:\\Anaconda\\lib\\site-packages\\spacy\\language.py in from_disk(self, path, exclude, disable)\n789 # Convert to list here in case exclude is (default) tuple\n790 exclude = list(exclude) + [\"vocab\"]\n--> 791 util.from_disk(path, deserializers, exclude)\n792 self._path = path\n793 return self\nD:\\Anaconda\\lib\\site-packages\\spacy\\util.py in from_disk(path, readers, exclude)\n628 # Split to support file names like meta.json\n629 if key.split(\".\")[0] not in exclude:\n--> 630 reader(path / key)\n631 return path\n632\nD:\\Anaconda\\lib\\site-packages\\spacy\\language.py in (p)\n779 deserializers[\"meta.json\"] = lambda p: self.meta.update(srsly.read_json(p))\n780 deserializers[\"vocab\"] = lambda p: self.vocab.from_disk(p) and _fix_pretrained_vectors_name(self)\n--> 781 deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(p, exclude=[\"vocab\"])\n782 for name, proc in self.pipeline:\n783 if name in exclude:\ntokenizer.pyx in spacy.tokenizer.Tokenizer.from_disk()\ntokenizer.pyx in spacy.tokenizer.Tokenizer.from_bytes()\nD:\\Anaconda\\lib\\re.py in compile(pattern, flags)\n232 def compile(pattern, flags=0):\n233 \"Compile a regular expression pattern, returning a Pattern object.\"\n--> 234 return _compile(pattern, flags)\n235\n236 def purge():\nD:\\Anaconda\\lib\\re.py in _compile(pattern, flags)\n284 if not sre_compile.isstring(pattern):\n285 raise TypeError(\"first argument must be string or compiled pattern\")\n--> 286 p = sre_compile.compile(pattern, flags)\n287 if not (flags & DEBUG):\n288 if len(_cache) >= _MAXCACHE:\nD:\\Anaconda\\lib\\sre_compile.py in compile(p, flags)\n762 if isstring(p):\n763 pattern = p\n--> 764 p = sre_parse.parse(p, flags)\n765 else:\n766 pattern = None\nD:\\Anaconda\\lib\\sre_parse.py in parse(str, flags, pattern)\n928\n929 try:\n--> 930 p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)\n931 except Verbose:\n932 # the VERBOSE flag was switched on inside the pattern. to be\nD:\\Anaconda\\lib\\sre_parse.py in _parse_sub(source, state, verbose, nested)\n424 while True:\n425 itemsappend(_parse(source, state, verbose, nested + 1,\n--> 426 not nested and not items))\n427 if not sourcematch(\"|\"):\n428 break\nD:\\Anaconda\\lib\\sre_parse.py in _parse(source, state, verbose, nested, first)\n534 break\n535 elif this[0] == \"\\\":\n--> 536 code1 = _class_escape(source, this)\n537 else:\n538 if set and this in '-&~|' and source.next == this:\nD:\\Anaconda\\lib\\sre_parse.py in _class_escape(source, escape)\n335 if len(escape) == 2:\n336 if c in ASCIILETTERS:\n--> 337 raise source.error('bad escape %s' % escape, len(escape))\n338 return LITERAL, ord(escape[1])\n339 except ValueError:\nerror: bad escape \\p at position 257\nInfo about spaCy\nspaCy version: 2.1.4\nPlatform: Windows-2012ServerR2-6.3.9600-SP0\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T13:30:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "552": {"issue_url": "https://github.com/explosion/spaCy/issues/3931", "issue_id": "#3931", "issue_summary": "Cant dump utf-8 text strings in the EntityRuler", "issue_description": "shaked571 commented on 9 Jul 2019 \u2022\nedited\nCan't dump a ruler with utf-8 pattern:\n from spacy.pipeline import EntityRuler\n        import en_core_web_md\n        nlp = en_core_web_md.load()\n        ruler = EntityRuler(nlp, overwrite_ents=True)\n        ruler.add_patterns( {\"label\": \"CALENDAREVENT\", \"pattern\": [{\"lower\": \"day\"}, {\"lower\": \"of\"}, {\"lower\": \"andaluc\u00eda\"}]})\n       ruler.to_disk('my/path/config') # also tried with ('my/path/config', ensure_ascii=False, encoding='utf-8' \n==>my/path/config.jsonl:\n{\"label\":\"CALENDAREVENT\",\"pattern\":[{\"lower\":\"day\"},{\"lower\":\"of\"},{\"lower\":\"andaluc\\u00eda\"}]}\nI manage to solve it using this patch:\n def dump_ruler_nonascii(data_path, ruler):\n        path = Path(os.path.join(data_path, 'config.jsonl'))\n        pattern = ruler.patterns\n        with open(path, \"a\", encoding=\"utf-8\") as f:\n            for line in pattern:\n                f.write(json.dumps(line, ensure_ascii=False) + \"\\n\")\n\n\nimport en_core_web_md\nnlp = en_core_web_md.load()\nruler = EntityRuler(nlp, overwrite_ents=True)\nruler.add_patterns( {\"label\": \"CALENDAREVENT\", \"pattern\": [{\"lower\": \"day\"}, {\"lower\": \"of\"}, {\"lower\": \"andaluc\u00eda\"}]})\ndump_ruler_nonascii('my/path/config', ruler)\n==>my/path/config.jsonl:\n{\"label\":\"CALENDAREVENT\",\"pattern\":[{\"lower\":\"day\"},{\"lower\":\"of\"},{\"lower\":\"andaluc\u00eda\"}]}\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.4\nPlatform: Windows-7-6.1.7601-SP1\nPython version: 3.6.2\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T12:41:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "553": {"issue_url": "https://github.com/explosion/spaCy/issues/3930", "issue_id": "#3930", "issue_summary": "Incorrect segmentation of a sentence into two due to capitalized tokens", "issue_description": "zeeshanalipnhwr commented on 9 Jul 2019\nHow to reproduce the behaviour\nfor sentence in nlp(\"There Are Many capitalization Mistakes In This Sentence.\").sents: print (sentence.text)\n...\nThere Are Many capitalization Mistakes\nIn This Sentence.\nYour Environment\nOperating System: Windows 10 Pro 64-Bit\nPython Version Used: 3.6\nspaCy Version Used: 2.0.18\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-09T11:17:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "554": {"issue_url": "https://github.com/explosion/spaCy/issues/3926", "issue_id": "#3926", "issue_summary": "Error exporting html output from displacy render", "issue_description": "mansiganatra commented on 9 Jul 2019 \u2022\nedited\nHi,\nI'm trying to render the output from displacy ent in the jupyter notebook and also save the html form to the disk. I have the following code:\n    displacy.render(document, style='ent', jupyter=True)\n    html =  displacy.render(document, style='ent', page=True)\n    print(\"HTML markup: \", html)\n    with open(\"./1.html\", 'w+', encoding=\"utf-8\") as fp:\n        fp.write(html)\n        fp.close()\nBut this doesn't work for exporting the html. On looking through the code in init.py I found that it is due to the following lines of code:\n    _html[\"parsed\"] = renderer.render(parsed, page=page, minify=minify).strip()\n    html = _html[\"parsed\"]\n    if RENDER_WRAPPER is not None:\n        html = RENDER_WRAPPER(html)\n    **if jupyter or is_in_jupyter():  # return HTML rendered by IPython display()**\n        from IPython.core.display import display, HTML\n\n        return display(HTML(html))\n    return html\nThe is_in_jupyter() property is set to true if the code is in a jupyter notebook. But in this case, I still want to export it as an html along with displaying in the notebook.\nI'm able to have a workaround by changing the above line of code to :\n**if (jupyter or is_in_jupyter()) and not page:**\nThere maybe other cases where it may break due to it. Need to resolve this in the correct manner.", "issue_status": "Closed", "issue_reporting_time": "2019-07-08T21:29:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "555": {"issue_url": "https://github.com/explosion/spaCy/issues/3924", "issue_id": "#3924", "issue_summary": "Model Missing in GitHub", "issue_description": "tgerstner commented on 8 Jul 2019 \u2022\nedited\nI am on closed network with no access to internet.\nI need to manually install the Spacy models.\nTrying to download the model en_core_web_sm-2.1.0 from GitHub from https://github.com/explosion/spacy-models/releases//tag/en_core_web_sm-2.1.0\nWhen I click on en_core_web_sm-2.1.0.tar.gz in the Assets section, I get the following error:\nThis site can\u2019t be reached\nThe webpage at https://github-production-release-asset-2e65be.s3.amazonaws.com/84940268/85ec3200-48aa-11e9-9878-337412e36828?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190708%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190708T142717Z&X-Amz-Expires=300&X-Amz-Signature=279c5a11cd010d868690d32c358e5c29684535a0cd746f6d91efa50b6d745502&X-Amz-SignedHeaders=host&actor_id=30666098&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-2.1.0.tar.gz&response-content-type=application%2Foctet-stream might be temporarily down or it may have moved permanently to a new web address.\nERR_TUNNEL_CONNECTION_FAILED", "issue_status": "Closed", "issue_reporting_time": "2019-07-08T14:31:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "556": {"issue_url": "https://github.com/explosion/spaCy/issues/3923", "issue_id": "#3923", "issue_summary": "Trying to load a model in the same script that downloaded it raises OSError", "issue_description": "louismartin commented on 8 Jul 2019 \u2022\nedited\nHi,\nWhen downloading a model using spacy.cli.download and then directly using spacy.load (in the same interpreter), it raises an OSError because spacy.utils.is_package returns False:\nspaCy/spacy/util.py\nLines 133 to 134 in a7fd42d\n if is_package(name):  # installed as package \n     return load_model_from_package(name, **overrides) \n\nWe currently need to start a new interpreter for the list of packages to be updated with the newly installed model.\nIs this the expected behaviour?\nIs there a way to download a model at run time and using it directly? (I'd like my package to download the model lazily only when needed and without having to execute the script twice).\nThanks,\nHow to reproduce the behaviour\n$ pip uninstall -y en_core_web_sm; python -c \"import spacy; spacy.cli.download('en_core_web_sm'); spacy.load('en_core_web_sm')\"\n\nOSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\nYour Environment\nOperating System: Ubuntu 18.04\nPython Version Used: 3.7.3\nspaCy Version Used: 2.1.3\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-08T14:02:41Z", "fixed_by": "#4090", "pull_request_summary": "\ud83d\udcab Require downloaded model in pkg_resources so it can be used in same session", "pull_request_description": "Member\nines commented on 6 Aug 2019 \u2022\nedited\nResolves #3923.\nDescription\nThis PR attempts to resolve an inconvenience of the download command. If a model is downloaded and then loaded within the same process, our is_package check currently fails, because pkg_resources.working_set is not refreshed automatically (see #3923). This can commonly happen in Jupyter notebooks if you do something like:\nIn [1]: !python -m spacy download de_core_news_sm\nIn [2]: import spacy\n        nlp = spacy.load(\"de_core_news_sm\")\nTo work around this, we can try and require the newly installed package explicitly using pkg_resources.working_set.require.\nThe following test case works without the code proposed in this PR (if you want to try it, make sure the model isn't installed):\nfrom spacy.cli import download\nfrom spacy.util import is_package\nimport pkg_resources\n\nmodel = \"de_core_news_sm\"\n\ndownload(model)\nprint(is_package(model))  # False\npkg_resources.working_set.require(model)\nprint(is_package(model))  # True\nTypes of change\nenhancement\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-08-07T11:18:12Z", "files_changed": [["11", "spacy/cli/download.py"]]}, "557": {"issue_url": "https://github.com/explosion/spaCy/issues/3922", "issue_id": "#3922", "issue_summary": "Missing remove method in PhraseMatcher", "issue_description": "bayethiernodiop commented on 8 Jul 2019\nFeature description\nIn Matcher it is possible to remove an added pattern by calling the remove method but that's not possible with the PhraseMatcher. Any tips", "issue_status": "Closed", "issue_reporting_time": "2019-07-08T13:51:09Z", "fixed_by": "#4309", "pull_request_summary": "Replace PhraseMatcher with trie-based search", "pull_request_description": "Collaborator\nadrianeboyd commented on 19 Sep 2019 \u2022\nedited\nDescription\nReplace PhraseMatcher with a trie-based search algorithm over numpy arrays of the hash values for the relevant attribute. The implementation is based on FlashText (https://github.com/vi3k6i5/flashtext).\nThe speed should be similar to the previous PhraseMatcher. It is now possible to easily remove match IDs and matches don't go missing with large keyword lists / vocabularies.\nFixes #3922, fixes #4308.\nTypes of change\nBugfix? Enhancement? One of the two.\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n2", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-27T14:22:35Z", "files_changed": [["2", "spacy/errors.py"], ["26", "spacy/matcher/phrasematcher.pxd"], ["324", "spacy/matcher/phrasematcher.pyx"], ["89", "spacy/tests/matcher/test_phrase_matcher.py"]]}, "558": {"issue_url": "https://github.com/explosion/spaCy/issues/3921", "issue_id": "#3921", "issue_summary": "How to use different NER models loading only one vocabulary", "issue_description": "romlatron commented on 8 Jul 2019\nI have a model with different NER categories, which I want to separate to optimize training results. To do so, I thought about having one Spacy model with different custom components, one for each category. The problem is, I then have to load the same vocabulary for each one of my NER components, which is pretty heavy because we use custom word vectors which take a lot of memory space (> 1Gb). I tried to load the NER models without the vectors, which raises an error. I also tried to load them with very few vocabulary, and then assign them the vocab of the original NLP model:\nner = spacy.load(\u201cpath/to/ner\u201d)\nnlp = spacy.load(\u201cpath/to/nlp\u201d)\nner.vocab = nlp.vocab\nBut it didn\u2019t seem to work so well as the last line did not affect the NER results.\nIs there a way to assign the same vocabulary to every component, without loading it multiple times which takes considerably too much space in memory? I thought about redefining the EntityRecognizer object, but is it a good approach, as it won\u2019t let me load the model without the vectors?\nYour Environment\nspaCy version: 2.0.18\nPlatform: Windows-10-10.0.17134-SP0\nPython version: 3.6.6\n1", "issue_status": "Closed", "issue_reporting_time": "2019-07-08T12:41:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "559": {"issue_url": "https://github.com/explosion/spaCy/issues/3920", "issue_id": "#3920", "issue_summary": "Cannot increase the speed of training with cpu_count", "issue_description": "pratapaprasanna commented on 8 Jul 2019 \u2022\nedited\nHi all,\nI have been going using spacy and i like it but when i try to boost the speed of my training as per the suggestion in here #1530-comment and i see the performace is not difference the code i used is as follows\n    nlp.add_pipe(sbd)\n    nlp.add_pipe(ner, last=True)\n    evaluation_steps = int(config['evaluation']['evaluation_interval'])\n    output_dir = config['TRAIN']['model_save_path']\n    for _, annotations in train_data:\n        for ent in annotations.get('entities'):\n            ner.add_label(ent[2])\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n    with nlp.disable_pipes(*other_pipes):\n        n_iter = int(config['TRAIN']['num_epochs'])\n        optimizer = nlp.begin_training(cpu_count=96)\n        for itn in range(n_iter):\nPlease correct me if there is any mistake in the code or approach.\nYour Environment\nOperating System: ubuntu 18.04\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-08T09:26:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "560": {"issue_url": "https://github.com/explosion/spaCy/issues/3917", "issue_id": "#3917", "issue_summary": "Dependency graph labels appear upside down", "issue_description": "urigoren commented on 7 Jul 2019\nHow to reproduce the behaviour\nThe following code generates a dependecy graph visualisation, with some of the arc labels shown upside down.\nSee screenshot\nfrom spacy import displacy\nviz = {'words': [{'text': 'Balmer', 'tag': ''},\n{'text': 'replaces', 'tag': ''},\n{'text': 'Gates', 'tag': ''},\n{'text': 'at', 'tag': ''},\n{'text': 'Microsoft', 'tag': ''}],\n'arcs': [{'start': 0, 'end': 1, 'dir': 'left', 'label': 'nsubj'},\n{'start': 1, 'end': -1, 'dir': 'right', 'label': 'root'},\n{'start': 2, 'end': 1, 'dir': 'right', 'label': 'obj'},\n{'start': 3, 'end': 4, 'dir': 'left', 'label': 'case'},\n{'start': 4, 'end': 1, 'dir': 'right', 'label': 'obl'}]}\nhtml = displacy.render(viz,\njupyter=False,\nstyle=\"dep\",\nmanual=True,\noptions={\"compact\": True,\"color\": \"blue\",}\n)\nwith open(\"bug.html\", 'w') as f:\nf.write(html)\nYour Environment\nOperating System: Win10\nPython Version Used: 3.6.7\nspaCy Version Used: spacy==2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-07T08:47:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "561": {"issue_url": "https://github.com/explosion/spaCy/issues/3912", "issue_id": "#3912", "issue_summary": "Improve textcat failure upon wrong initialization", "issue_description": "Member\nsvlandeg commented on 5 Jul 2019\nWhen creating a textcat pipe without a config, it doesn't throw a warning but fails ungracefully later on when training, with an error because textcat.model is a Bool", "issue_status": "Closed", "issue_reporting_time": "2019-07-05T13:00:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "562": {"issue_url": "https://github.com/explosion/spaCy/issues/3910", "issue_id": "#3910", "issue_summary": "Using spacy download command via pipenv", "issue_description": "marinswk commented on 5 Jul 2019\nHello,\nI'm trying to run spacy on a jenkins instance via a pipenv and i'm having troubles doing that both locally and on the jenkins installation.\nI need to be able to run the command:\nspacy download xx\nvia pipenv cause that's the way in which i'm installing the virtual environment.\nIn both cases (jenkins and local, with the same exact pipfile) i get the following error when running:\npipenv run spacy download xx\nTraceback (most recent call last):\n  File \"/usr/local/bin/pipenv\", line 10, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/vendor/click/decorators.py\", line 64, in new_func\n    return ctx.invoke(f, obj, *args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/vendor/click/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/cli/command.py\", line 408, in run\n    command=command, args=args, three=state.three, python=state.python, pypi_mirror=state.pypi_mirror\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/core.py\", line 2328, in do_run\n    do_run_posix(script, command=command)\n  File \"/usr/local/lib/python3.7/site-packages/pipenv/core.py\", line 2297, in do_run_posix\n    command_path, command_path, *[os.path.expandvars(arg) for arg in script.args]\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/os.py\", line 539, in execl\n    execv(file, args)\nOSError: [Errno 8] Exec format error\nis there any solution to that?\nYour Environment\nInfo about spaCy\nspaCy version: 2.0.12\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.3\nModels: xx", "issue_status": "Closed", "issue_reporting_time": "2019-07-05T07:10:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "563": {"issue_url": "https://github.com/explosion/spaCy/issues/3909", "issue_id": "#3909", "issue_summary": "Evaluate NER wrong results", "issue_description": "pika97 commented on 5 Jul 2019 \u2022\nedited\nHow to reproduce the behaviour\nI have annotated about 600 sentences for NER in the spacy format.\nWhen I run the evaluate function, on the data it shows 0 for all values except token_acc.\nI tried to run it for individual sentences, it shows 100 for some ad 0 for the others.\nFunction I'm using:\nimport spacy \nfrom spacy.gold import GoldParse\nfrom spacy.scorer import Scorer \n\ndef evaluate(ner_model,examples):\n scorer = Scorer()\n for input_,annot in examples:\n  doc_gold_text = ner_model.make_doc(input_)\n  gold = GoldParse(doc_gold_text,entities = annot['entities'])\n  pred_value = ner_model(input_)\n  scorer.score(pred_value,gold)\n  return scorer.scores,gold\nIndividual results (ie per sentence)\nCorrect (ie P =100 R=100 F=100):\n('with the assistance of the canadian red cross, city staff quickly mobilized to provide reception centres for those torontonians who were without power.', {'entities': [(115, 127, 'NORP')]})\nIncorrect(ie P=0 R=0 F= 0)\n('b province of new brunswick preparedness, response and recovery december 2013 ice storm after action review report 5 august 2014 final version presented to: mr. greg maccallum director new brunswick emergency measures organization po box 6000 364 argyle street, room 3-92 fredericton, new brunswick e3b 5h1 prepared by: jim bruce atlantic security group inc 33 paul thomas drive dartmouth, nova scotia, b2w 6a1 0c table of contents executive summary .', {'entities': [(17, 30, 'GPE'), (66, 90, 'EVENT'), (120, 134, 'DATE'), (168, 187, 'PERSON'), (197, 242, 'ORG'), (259, 272, 'LOC'), (17, 30, 'GPE'), (337, 346, 'PERSON'), (348, 370, 'ORG'), (407, 418, 'GPE'), (396, 405, 'GPE')]})\nGrouped results\nWhen I include even one of the incorrect sentences with several correct ones as part of the goldparse, it gives 0 for all values except token_acc.\nEven if the goldparse if off by one character the result changes to 0 for P,F & R.\nEg: Predicted = (8,16,'PERSON')\nGoldParse = (7,16,'PERSON')\nCan someone please clarify why this is happening? I have gone through the code yet I don't understand why partially correct entities are being scored as 0 for P, R and F-Score.\nYour Environment\nOperating System: Ubuntu 18.04 (Using Colab)\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.4\nEnvironment Information: Running on Colab", "issue_status": "Closed", "issue_reporting_time": "2019-07-04T22:21:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "564": {"issue_url": "https://github.com/explosion/spaCy/issues/3908", "issue_id": "#3908", "issue_summary": "Training German NER model", "issue_description": "jonasadigi commented on 4 Jul 2019\nHow to reproduce the behaviour\nSimilar to Issue #3279, I tried to improve NER accuracy of the german model by training the model further on some data. For that I first tried to get a working test implementation inspired by https://medium.com/@manivannan_data/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6\nI first tried it with English model and data:\nTRAIN_DATA = [('what is the price of polo?', {'entities': [(21, 25, 'PrdName')]}), \n              ('what is the price of ball?', {'entities': [(21, 25, 'PrdName')]}), \n              ('what is the price of jegging?', {'entities': [(21, 28, 'PrdName')]}), \n              ('what is the price of t-shirt?', {'entities': [(21, 28, 'PrdName')]}), \n              ('what is the price of jeans?', {'entities': [(21, 26, 'PrdName')]}), \n              ('what is the price of bat?', {'entities': [(21, 24, 'PrdName')]}), \n              ('what is the price of shirt?', {'entities': [(21, 26, 'PrdName')]}), \n              ('what is the price of bag?', {'entities': [(21, 24, 'PrdName')]}), \n              ('what is the price of cup?', {'entities': [(21, 24, 'PrdName')]}), \n              ('what is the price of jug?', {'entities': [(21, 24, 'PrdName')]}), \n              ('what is the price of plate?', {'entities': [(21, 26, 'PrdName')]}), \n              ('what is the price of glass?', {'entities': [(21, 26, 'PrdName')]})]\n\ndef main(model=None, output_dir=None, n_iter=100):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # reset and initialize the weights randomly \u2013 but only if we're\n        # training a new model\n        if model is None:\n            nlp.begin_training()\n        for itn in range(n_iter):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.2,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n            if itn % 5 == 0:\n                print(\"Losses\", losses)\n\n    # test the trained model\n    for text, _ in TRAIN_DATA:\n        doc = nlp(text)\n        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\nmain()\nThis works well and when testing, all the entities from the training data are recognized.\nHowever, if I do the same with some roughly equivalent german data:\nTRAIN_DATA = [('Was kostet das Auto?', {'entities': [(15, 18, 'PrdName')]}), \n              ('Was kostet der Ball?', {'entities': [(15, 18, 'PrdName')]}), \n              ('Was kostet die Hose?', {'entities': [(15, 18, 'PrdName')]}), \n              ('Was kostet das T-shirt?', {'entities': [(15, 21, 'PrdName')]}), \n              ('Was kostet die Jeans?', {'entities': [(15, 19, 'PrdName')]}), \n              ('Was kostet der Rechner?', {'entities': [(15, 21, 'PrdName')]}), \n              ('Was kostet die Karte?', {'entities': [(15, 19, 'PrdName')]}), \n              ('Was kostet die T\u00fcr?', {'entities': [(15, 17, 'PrdName')]}), \n              ('Was kostet der Schrank?', {'entities': [(15, 21, 'PrdName')]}), \n              ('Was kostet der Tisch?', {'entities': [(15, 19, 'PrdName')]}), \n              ('Was kostet das Bild?', {'entities': [(15, 18, 'PrdName')]}), \n              ('Was kostet der Stuhl?', {'entities': [(15, 19, 'PrdName')]})]\nand set the model to german\nif model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"de\")  # create blank Language class\n        print(\"Created blank 'de' model\")\nNo entities are recognized:\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('die', '', 2), ('Karte', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('der', '', 2), ('Stuhl', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('das', '', 2), ('T-shirt', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('der', '', 2), ('Rechner', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('der', '', 2), ('Ball', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('das', '', 2), ('Auto', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('der', '', 2), ('Tisch', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('die', '', 2), ('T\u00fcr', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('die', '', 2), ('Hose', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('das', '', 2), ('Bild', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('der', '', 2), ('Schrank', '', 2), ('?', '', 2)]\nEntities []\nTokens [('Was', '', 2), ('kostet', '', 2), ('die', '', 2), ('Jeans', '', 2), ('?', '', 2)]\nAlthough loss is going down very fast. The same happens if I do not use a blank model but load a german model instead. Also I tried some other german test data which also did not work. Am I doing something wrong or is there some kind of issue?\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.4\nPlatform: Darwin-17.7.0-x86_64-i386-64bit\nPython version: 3.7.3\nModels: de, en\nOperating System: macOS high sierra 10.13.6", "issue_status": "Closed", "issue_reporting_time": "2019-07-04T14:06:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "565": {"issue_url": "https://github.com/explosion/spaCy/issues/3907", "issue_id": "#3907", "issue_summary": "Span.sent is None for single-token Docs", "issue_description": "Contributor\njuliamakogon commented on 4 Jul 2019 \u2022\nedited\nHow to reproduce the behaviour\nThe code below works with longer text like \"John Doe goes,\" but fails when the entity and the sentence coincide.\n    nlp_en = spacy.blank(language)\n    sentencizer = nlp_en.create_pipe(\"sentencizer\")\n    nlp_en.add_pipe(sentencizer)\n\n    doc = nlp_en(\"John Doe\")\n    doc_ents = []\n    taghash = nlp_en.vocab.strings.add('PERSON')\n    doc_ents.append(Span(doc, 0, 2, label=taghash))\n    merge_ents = nlp_en.create_pipe(\"merge_entities\")\n    doc.ents = doc_ents\n    for ent in doc.ents:\n        print(ent, '*', ent.sent) # John Doe * John Doe\n        assert ent is not None\n        assert ent.sent is not None\n    doc = merge_ents(doc)\n    for ent in doc.ents:\n        print(ent, '*', ent.sent) # John Doe * None\n        assert ent is not None\n        assert ent.sent is not None\nYour Environment\nspaCy version: 2.1.4\nPlatform: Linux-4.15.0-54-generic-x86_64-with-debian-buster-sid\nPython version: 3.6.7\nModels: en, en_core_web_md, xx", "issue_status": "Closed", "issue_reporting_time": "2019-07-04T09:30:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "566": {"issue_url": "https://github.com/explosion/spaCy/issues/3906", "issue_id": "#3906", "issue_summary": "German large/pretrained model", "issue_description": "hermes-z commented on 4 Jul 2019\nFeature description\nWould it be possible to have a de_core_news_lg model and/or a pertained German model?", "issue_status": "Closed", "issue_reporting_time": "2019-07-04T08:45:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "567": {"issue_url": "https://github.com/explosion/spaCy/issues/3905", "issue_id": "#3905", "issue_summary": "Model download extremely slow and interrupted in China", "issue_description": "piofel commented on 4 Jul 2019 \u2022\nedited\nI live in China. When I want to download the en_core_web_sm model, the download speed becomes below 1KB/s and then timed-out. When added --default-timeout=3600 option, what surprisingly works, I receive pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer')), after some time passes.", "issue_status": "Closed", "issue_reporting_time": "2019-07-04T08:18:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "568": {"issue_url": "https://github.com/explosion/spaCy/issues/3904", "issue_id": "#3904", "issue_summary": "German part-of-speech tagging", "issue_description": "hermes-z commented on 3 Jul 2019\nHi, I am encountering some problems with the German POS tagger. Even though the tokenizer works correctly on \\u201e and \\u201c, the part-of-speech does not work.\nHow to reproduce the behaviour\nnlp = space.load('de_core_news_md')\ntext = \"\\u201eDas ist nicht gut.\\u201c\"\ndoc = nlp(text)\nfor tk in doc:\n  print(tk.text, tk.pos_)\nIn this case, is assigned VERB and \\u201c PROPN.\nHowever, the following works as expected:\nnlp = space.load('de_core_news_md')\ntext = \"'Das ist nicht gut.'\"\ndoc = nlp(text)\nfor tk in doc:\n  print(tk.text, tk.pos_)\nYour Environment\nOperating System: macOS 10.14.5\nPython Version Used: 3.6.6\nspaCy Version Used: 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-07-03T14:21:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "569": {"issue_url": "https://github.com/explosion/spaCy/issues/3903", "issue_id": "#3903", "issue_summary": "I want to extract information from a receipt but after testing custom trained NER model no output produced", "issue_description": "jmkumbo commented on 3 Jul 2019 \u2022\nedited\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2019-07-03T13:14:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "570": {"issue_url": "https://github.com/explosion/spaCy/issues/3898", "issue_id": "#3898", "issue_summary": "Training NER model fails with KeyError: 'paragraphs'", "issue_description": "niedakh commented on 2 Jul 2019\nHow to reproduce the behaviour\nI'm using the following approach to generate a data set from my data frame (from the jsonl to json converter):\n    model = get_lang_class('en')()\n    json_data = []\n    for raw_text, entities in tqdm(generate_spacy_entity_training_data(df context)):\n        try:\n            doc = model.make_doc(raw_text)\n            doc[0].is_sent_start = True\n            doc.ents = [doc.char_span(s, e, label=L) for s, e, L in entities]\n        except TypeError:\n            print(raw_text, entities)\n            break\n\n        json_data.append(doc.to_json())\n\n    if split:\n        output_path = Path(output_path)\n        train, test = train_test_split(json_data, test_size=0.33)\n        train_path = output_path.stem+'_train.json'\n        test_path = output_path.stem+'_test.json'\n        print('Writing {} training data to: {}'.format(len(train), train_path))\n        srsly.write_json(train_path, train)\n        print('Writing {} evaluation data to: {}'.format(len(test), test_path))\n        srsly.write_json(test_path, test)\nSpacy train then fails with KeyError 'paragraph' error. Running debug-data shows:\n=========================== Data format validation ===========================\n\u2714 Loaded ml_intent_dump_20190624_spacy_context_3_train.json\n\u2714 Loaded ml_intent_dump_20190624_spacy_context_3_test.json\n\u2714 Training data JSON format is valid\n\u2714 Development data JSON format is valid\n\u2839 Analyzing corpus...\nTraceback (most recent call last):\n  File \"/usr/lib64/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib64/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/venv36/lib64/python3.6/site-packages/spacy/__main__.py\", line 35, in <module>\n    plac.call(commands[command], sys.argv[1:])\n  File \"/opt/venv36/lib64/python3.6/site-packages/plac_core.py\", line 328, in call\n    cmd, result = parser.consume(arglist)\n  File \"/opt/venv36/lib64/python3.6/site-packages/plac_core.py\", line 207, in consume\n    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n  File \"/opt/venv36/lib64/python3.6/site-packages/spacy/cli/debug_data.py\", line 94, in debug_data\n    corpus = GoldCorpus(train_data, dev_data)\n  File \"gold.pyx\", line 112, in spacy.gold.GoldCorpus.__init__\n  File \"gold.pyx\", line 123, in spacy.gold.GoldCorpus.write_msgpack\n  File \"gold.pyx\", line 277, in read_json_object\n  File \"gold.pyx\", line 289, in json_to_tuple\nKeyError: 'paragraphs'\nYour Environment\nspaCy version: 2.1.4\nPlatform: Linux-3.10.0-957.21.3.el7.x86_64-x86_64-with-centos-7.6.1810-Core\nPython version: 3.6.6\nModels: en\n\u2139 spaCy installation:\nTYPE NAME MODEL VERSION\npackage en-vectors-web-lg en_vectors_web_lg 2.1.0 \u2714\npackage en-core-web-sm en_core_web_sm 2.1.0 \u2714\npackage en-core-web-md en_core_web_md 2.1.0 \u2714\npackage en-core-web-lg en_core_web_lg 2.1.0 \u2714\nlink en en_core_web_sm 2.1.0 \u2714", "issue_status": "Closed", "issue_reporting_time": "2019-07-02T16:30:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "571": {"issue_url": "https://github.com/explosion/spaCy/issues/3897", "issue_id": "#3897", "issue_summary": "Coreference resolution for german", "issue_description": "snape6666 commented on 2 Jul 2019\nIs there a Coreference resolution feature for german in spacy now?\nI saw #820 but there only solutions for english. Is there something for german too? I am also happy about other tool suggestions.", "issue_status": "Closed", "issue_reporting_time": "2019-07-02T15:32:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "572": {"issue_url": "https://github.com/explosion/spaCy/issues/3896", "issue_id": "#3896", "issue_summary": "How to label entity positions in custom data (in paragraph form) for NER training", "issue_description": "mrxiaohe commented on 2 Jul 2019\nYour Environment\nOperating System: Window 10\nPython Version Used: 3.7\nspaCy Version Used: 2.1.4\nEnvironment Information:\nBased on #3676, it is OK to use paragraphs as training data (each piece of training data is a paragraph), as opposed to sentences. In the example from your website, the training data can be annotated in the format below, where the indices indicate the start and end positions of a named entity.\ntrain_data = [\n    (\"Uber blew through $1 million a week\", [(0, 4, 'ORG')]),\n    (\"Android Pay expands to Canada\", [(0, 11, 'PRODUCT'), (23, 30, 'GPE')]),\n    (\"Spotify steps up Asia expansion\", [(0, 8, \"ORG\"), (17, 21, \"LOC\")]),\n    (\"Google Maps launches location sharing\", [(0, 11, \"PRODUCT\")]),\n    (\"Google rebrands its business apps\", [(0, 6, \"ORG\")]),\n    (\"look what i found on google! \ud83d\ude02\", [(21, 27, \"PRODUCT\")])]\nIf each piece of training data is a paragraph, should I annotate the positions the same way, only now the start and end positions are with respect to the full paragraph? So for the sample data below which is a short paragraph consisting of two sentences, if I want to annotate \"San Diego\", the positions would be the positions in the paragraph?\ntrain_data = [\n    (\"It's going to rain tomorrow. I think you should bring an umbrella with you to San Diego.\", [(78, 87, 'GPE')])]\n\n\n\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-07-02T15:11:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "573": {"issue_url": "https://github.com/explosion/spaCy/issues/3893", "issue_id": "#3893", "issue_summary": "Lemmatization case sensitive", "issue_description": "mvaz commented on 2 Jul 2019\nHow to reproduce the behaviour\nI'm using the same code snippet as with #3572 and as far as I understood should have been resolved with #3551\nimport spacy\nfrom spacy.matcher import Matcher\nnlp = spacy.load(\"en_core_web_sm\")\nmatcher = Matcher(nlp.vocab)\npattern = [{\"LEMMA\": \"bike\"}]\nmatcher.add(\"BasinTest\", None, pattern)\ndoc = nlp(\"The Bike is a great tools\")\nmatches = matcher(doc)\nfor match_id, start, end in matches:\nstring_id = nlp.vocab.strings[match_id] # Get string representation\nspan = doc[start:end] # The matched span\nprint(match_id, string_id, start, end, span.text)\nThe code produces no output, meaning that the lemmatisation is case sensitive.\n[t.lemma_ for t in doc]\nthe output is ['the', 'Bike', 'be', 'a', 'great', 'tool']\nThanks\nYour Environment\nInfo about spaCy\nspaCy version: 2.1.4\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-07-02T09:26:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "574": {"issue_url": "https://github.com/explosion/spaCy/issues/3892", "issue_id": "#3892", "issue_summary": "Tokenizer fails to initilize correctly (or different initialization behaviors)", "issue_description": "aryehgigi commented on 2 Jul 2019 \u2022\nedited\nHow to reproduce the behaviour\nThis is according to the following tokenizer initialization example.\nimport spacy\nfrom spacy.tokenizer import Tokenizer\nnlp = spacy.load(\"en_core_web_lg\") # for example\ntest_case = \"I don't know.\"\ntokenizer_lame = Tokenizer(nlp.vocab)\n[token for token in tokenizer_lame(test_case)] # will produce first output\ntokenizer_awesome = nlp.Defaults.create_tokenizer(nlp)\n[token for token in tokenizer_awesome(test_case)] # will produce second output\nFirst output:\n[I, don't, know.] # BAD\nSecond output:\n[I, do, n't, know, .] # GOOD\nNotes:\nNot sure if this is the expected behavior (?), but even so, presenting both initializations as the same is misleading (IMHO).\nI came across this when I used spacy_conll{ @BramVanroy } (and passed the flag is_tokenized=False), as they use the first method I posted as their tokenizer initialization step.\nYour Environment\nspaCy version: 2.1.4\nPlatform: Windows-10-10.0.17134-SP0\nPython version: 3.6.5", "issue_status": "Closed", "issue_reporting_time": "2019-07-02T08:43:37Z", "fixed_by": "#3939", "pull_request_summary": "Update tokenizer and doc init example", "pull_request_description": "Contributor\nBreakBB commented on 10 Jul 2019\nDescription\nThis PR:\nupdates the documentation examples of the tokenizer creation to fix #3892\nupdates the Doc.__init__ example in the methods docstring\nfixes the hyperlink to Doc.to_json\nTypes of change\nDocumentation changes\nChecklist\nI have submitted the spaCy Contributor Agreement.\nI ran the tests, and all new and existing tests passed.\nMy changes don't require a change to the documentation, or if they do, I've added all required information.\n1\n1", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-10T08:16:49Z", "files_changed": [["5", "spacy/tokens/doc.pyx"], ["2", "website/docs/api/doc.md"], ["8", "website/docs/api/tokenizer.md"]]}, "575": {"issue_url": "https://github.com/explosion/spaCy/issues/3891", "issue_id": "#3891", "issue_summary": "Hi can we get a accuracy of Every Named Entity in Spacy", "issue_description": "chowdary-git commented on 1 Jul 2019\nFeature description\nCould the feature be a custom component or spaCy plugin?\nIf so, we will tag it as project idea so other users can take it on.", "issue_status": "Closed", "issue_reporting_time": "2019-07-01T06:37:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "576": {"issue_url": "https://github.com/explosion/spaCy/issues/3889", "issue_id": "#3889", "issue_summary": "Spacy NER: Training existing model on new entities reducing the accuracy.", "issue_description": "AbhishekKargawal commented on 30 Jun 2019\nI'm using Spacy NER to get organisation names from the text.\nExample:\nText: AG Barr is recalling 750ml glass bottles due to a manufacturing fault which may cause bottle caps to pop off unexpectedly\nEntities I want: AG Barr: ORG\nUsing \"en_core_web_lg\" as base model.\nAccuracy using the base model is: 76%\nFrom here I have filtered the incorrectly predicted sentence and created a test example of them to train new entity to improve overall accuracy. I have not created a blank model, using the \"en_core_web_lg\" as a base model.\nAfter training a new entity using the new training sentence, my overall model accuracy got reduced to 60%.", "issue_status": "Closed", "issue_reporting_time": "2019-06-30T08:29:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "577": {"issue_url": "https://github.com/explosion/spaCy/issues/3888", "issue_id": "#3888", "issue_summary": "tracking Language.pipe progress. Is batch_size mandatory?", "issue_description": "jcrousse commented on 28 Jun 2019\nDescription:\nWhen trying to track the progress of the Language.pipe function with tqdm as suggested in #2318 (also tried progressbar2) it appears that the progress tracking is updated only at the end of the processing.\nThe below code takes about 13 seconds to run on my machine, and the tqdm progress bar \"jumps\" from 0 to 500 at the end of the 13 seconds.\nI can \"fix\" the issue by setting the option batch_size to 1, but then I lose in efficiency.\nIf I add a custom component to track the progress (by example by printing something) the print output appears all at once at the end (if last component of the pipeline) or at the very beginning before processing the text (if first component of the pipeline).\nMaybe this is the intended behaviour, but the answer to #2318 makes me think it is not.\nHow to reproduce the behaviour\nTook a random dataset from here\nfrom tqdm import tqdm\nimport pandas as pd\nimport spacy\n\nDATASET = pd.read_csv(\"imdb_master.csv\", \n                      encoding=\"latin-1\",  nrows=500)[\"review\"].values\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nprocessed_docs = []\nfor doc in tqdm(nlp.pipe(DATASET)):\n    processed_docs.append(doc)\nEnvironment\nOperating System: Ubuntu 18.04\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.4\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-06-28T13:16:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "578": {"issue_url": "https://github.com/explosion/spaCy/issues/3887", "issue_id": "#3887", "issue_summary": "EntityRuler returns unexpected entity", "issue_description": "ccoppa commented on 27 Jun 2019 \u2022\nedited\nHow to reproduce the behaviour\nI tried different ways to define the patterns, but none gave me the expected output. I'm not sure how the output ended up combining both defined entities together with an extra token \"for\".\nimport spacy\nfrom spacy.pipeline import EntityRuler\n\npatterns1 = [ {\"label\": \"ORG\", \"pattern\": \"XYZ Company\"}, \n{\"label\": \"XYZ_PRODUCT\", \"pattern\": \"Fitness Services\"}\n ]\n\npatterns2 = [ {\"label\": \"ORG\", \"pattern\": [{\"ORTH\": \"XYZ\"},{\"ORTH\": \"Company\"}]}, \n{\"label\": \"XYZ_PRODUCT\", \"pattern\": [{\"ORTH\": \"Fitness\"},{\"ORTH\":\"Services\"}]}\n ]\n\npatterns3 = [ {\"label\": \"ORG\", \"pattern\": [{\"lower\": \"xyz company\"}]}, \n{\"label\": \"XYZ_PRODUCT\", \"pattern\": [{\"lower\": \"fitness services\"}]}\n ]\n\npatterns4 = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"xyz\"},{\"lower\": \"company\"}]}, \n{\"label\": \"XYZ_PRODUCT\", \"pattern\": [{\"lower\": \"fitness\"},{\"lower\":\"services\"}]}\n ]\n\npatterns5 = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": {\"IN\": [\"xyz company\"]}}]}, \n{\"label\": \"XYZ_PRODUCT\", \"pattern\": [{\"lower\":{\"IN\": [\"fitness services\"]}}]}\n ]\n\n\npatterns = [patterns1,patterns2,patterns3,patterns4,patterns5]\n\nfor i in range(0,len(patterns)):\n  nlp = spacy.load(\"en_core_web_sm\")\n  ruler = EntityRuler(nlp, overwrite=True)\n  ruler.add_patterns(patterns[i])\n  nlp.add_pipe(ruler, after=\"ner\")\n  doc = nlp(u\"I joined XYZ Company for Fitness Services\")\n  print([(ent.text, ent.label_) for ent in doc.ents])\nOutput\n[('XYZ Company for Fitness Services', 'ORG')]\n[('XYZ Company for Fitness Services', 'ORG')]\n[('XYZ Company for Fitness Services', 'ORG')]\n[('XYZ Company for Fitness Services', 'ORG')]\n[('XYZ Company for Fitness Services', 'ORG')]\nAlso tried using overwrite=False got different output but still not solving the issue completely. Some got the XYZ_PRODUCT CORRECT but all got the ORG entity span wrong.\nIdeally, I'd hope format like pattern5 to work as I want to use lower and IN for data processing and scalibility.\nfor i in range(0,len(patterns)):\n  nlp = spacy.load(\"en_core_web_sm\")\n  ruler = EntityRuler(nlp, overwrite=False)\n  ruler.add_patterns(patterns[i])\n  nlp.add_pipe(ruler, before=\"ner\")\n  doc = nlp(u\"I joined XYZ Company for Fitness Services\")\n  print([(ent.text, ent.label_) for ent in doc.ents])\nOutput\n[('XYZ Company for', 'ORG'), ('Fitness Services', 'XYZ_PRODUCT')]\n[('XYZ Company for', 'ORG'), ('Fitness Services', 'XYZ_PRODUCT')]\n[('XYZ Company for Fitness Services', 'ORG')]\n[('XYZ Company for', 'ORG'), ('Fitness Services', 'XYZ_PRODUCT')]\n[('XYZ Company for Fitness Services', 'ORG')]\nYour Environment\nOperating System: macOS Mojave 10.14.5\nPython Version Used: Python 3.7.1\nspaCy Version Used: spaCy 2.1.4", "issue_status": "Closed", "issue_reporting_time": "2019-06-27T17:55:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "579": {"issue_url": "https://github.com/explosion/spaCy/issues/3886", "issue_id": "#3886", "issue_summary": "Invalid noun_chunk in a specific case", "issue_description": "zeeshanalipnhwr commented on 27 Jun 2019\nHow to reproduce the behaviour\nsentence = \"The Cleveland Chamber of Commerce is more a good-government organization than a commercial body.\"\nfor chunk in nlp(sentence).noun_chunks: print (len(chunk), chunk)\n...\n3 The Cleveland Chamber\n1 Commerce\n6 more a good-government organization\n3 a commercial body\nThe third noun_chunk should have been just \"a good-government organization\"; \"more\" before it is extraneous and/or not part of it.\nYour Environment\nOperating System:\nPython Version Used:\nspaCy Version Used:\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-06-27T12:11:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "580": {"issue_url": "https://github.com/explosion/spaCy/issues/3885", "issue_id": "#3885", "issue_summary": "Retokenizer.split old documentation?", "issue_description": "gorqkop commented on 27 Jun 2019\nI'm trying to implement Retokenizer.split in my project as noticed here, but noticed that even given example not works. It gives 2 errors:\nsplit() has no attribute heads;\n'spacy.tokens.token.Token' object has no attribute 'start_char'.\nCould you please provide working example in documentation.", "issue_status": "Closed", "issue_reporting_time": "2019-06-27T09:09:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "581": {"issue_url": "https://github.com/explosion/spaCy/issues/3884", "issue_id": "#3884", "issue_summary": "Spanish POS incorrectly tagging verbs, trouble training model", "issue_description": "neongreen13 commented on 27 Jun 2019\nThis is my first time trying to train a SpaCy POS model to update some incorrectly tagged verbs in the Spanish model. I can't seem to get it to work. Specifically, compound verbs in the command (imperative) form with an attached indirect object are being tagged as nouns or adjectives. Example: 'Dimelo!' is 'Give me it, or Give it to me'. Any help would be appreciated. Here is my code:\nimport spacy\nnlp = spacy.load('es')\n\nfrom __future__ import unicode_literals, print_function\n\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\n\nfrom spacy.lang.es.tag_map import TAG_MAP\nfrom spacy.symbols import POS, PUNCT, SYM, ADJ, NUM, DET, ADV, ADP, X, VERB, NOUN\nfrom spacy.symbols import PROPN, PART, INTJ, PRON, SCONJ, AUX, CCONJ\n\nTAG_MAP = {\"V\": {\"pos\": \"VERB\"}}\nTRAIN_DATA = [\n    (\"almuerza\", {\"tags\": [\"V\"]}),\n    (\"no almuerces\", {\"tags\": [\"V\"]}),\n    (\"come\", {\"tags\": [\"V\"]}),\n    (\"no comas\", {\"tags\": [\"V\"]}),\n    (\"escribe\", {\"tags\": [\"V\"]}),\n    (\"no escribas\", {\"tags\": [\"V\"]}),\n    (\"habla\", {\"tags\": [\"V\"]}),\n    (\"no hables\", {\"tags\": [\"V\"]}),\n    (\"juega\", {\"tags\": [\"V\"]}),\n    (\"no juegues\", {\"tags\": [\"V\"]}),\n    (\"miente\", {\"tags\": [\"V\"]}),\n    (\"no mientas\", {\"tags\": [\"V\"]}),\n    (\"pide\", {\"tags\": [\"V\"]}),\n    (\"no pidas\", {\"tags\": [\"V\"]}),\n    (\"toca\", {\"tags\": [\"V\"]}),\n    (\"no toques\", {\"tags\": [\"V\"]}),\n    (\"l\u00e1vate\", {\"tags\": [\"V\"]}),\n    (\"no te laves\", {\"tags\": [\"V\"]}),\n    (\"c\u00e1ete\", {\"tags\": [\"V\"]}),\n    (\"no te caigas\", {\"tags\": [\"V\"]}),\n    (\"si\u00e9ntate\", {\"tags\": [\"V\"]}),\n    (\"no te sientes\", {\"tags\": [\"V\"]}),\n    (\"du\u00e9rmete\", {\"tags\": [\"V\"]}),\n    (\"no te duermas\", {\"tags\": [\"V\"]}),\n    (\"l\u00e9alo\", {\"tags\": [\"V\"]}),\n    (\"s\u00e1cala\", {\"tags\": [\"V\"]}),\n    (\"est\u00fadialo\", {\"tags\": [\"V\"]}),\n    (\"compre\", {\"tags\": [\"V\"]}),\n    (\"c\u00f3mprelo\", {\"tags\": [\"V\"]}),\n    (\"c\u00f3mpremelo\", {\"tags\": [\"V\"]}),\n    (\"c\u00f3mprelos\", {\"tags\": [\"V\"]}),\n    (\"c\u00f3mpremelos\", {\"tags\": [\"V\"]}),\n    (\"hazlo\", {\"tags\": [\"V\"]}),\n    (\"hazmelo\", {\"tags\": [\"V\"]}),\n    (\"hazlos\", {\"tags\": [\"V\"]}), \n]\n\nimport os\nos.getcwd()\noutput_dir = os.getcwd() \n\ndef main(lang=\"es\", output_dir=output_dir, n_iter=84):\n    \"\"\"Create a new model, set up the pipeline and train the tagger. In order to\n    train the tagger with a custom tag map, we're creating a new Language\n    instance with a custom vocab.\n    \"\"\"\n    nlp = spacy.blank(lang)\n    # add the tagger to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    tagger = nlp.create_pipe(\"tagger\")\n    # Add the tags. This needs to be done before you start training.\n    for tag, values in TAG_MAP.items():\n        tagger.add_label(tag, values)\n    nlp.add_pipe(tagger)\n    \n    optimizer = nlp.begin_training()\n    for i in range(n_iter):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n        print(\"Losses\", losses)\n\ntest_2 = \"Pr\u00e9ndelo y ap\u00e1galo Ven d\u00edmelo ay yo ya vi Es obvio que t\u00fa mil cosas quieres hacer\"\ndoc = nlp(test_2)\nprint(\"Tags\", [(t.text, t.pos_) for t in doc])\nOutput: \nTags [('Pr\u00e9ndelo', 'NOUN'), ('y', 'CONJ'), ('ap\u00e1galo', 'ADJ'), ('Ven', 'PROPN'), ('d\u00edmelo', 'ADJ'), ('ay', 'PROPN'), ('yo', 'PRON'), ('ya', 'ADV'), ('vi', 'VERB'), ('Es', 'AUX'), ('obvio', 'ADJ'), ('que', 'SCONJ'), ('t\u00fa', 'PRON'), ('mil', 'NUM'), ('cosas', 'NOUN'), ('quieres', 'ADJ'), ('hacer', 'VERB')]\n\nif output_dir is not None:\n    output_dir = Path(output_dir)\n    if not output_dir.exists():\n        output_dir.mkdir()\n    nlp.to_disk(output_dir)\n    print(\"Saved model to\", output_dir)\n\n    # test the save model\n    print(\"Loading from\", output_dir)\n    nlp2 = spacy.load(output_dir)\n    doc = nlp2(test_2)\n    print(\"Tags\", [(t.text, t.pos_) for t in doc])\n\nSaved model to /home/ariggs/JupyterNotebooks/Spanish\nLoading from /home/ariggs/JupyterNotebooks/Spanish\nTags [('Pr\u00e9ndelo', 'NOUN'), ('y', 'CONJ'), ('ap\u00e1galo', 'ADJ'), ('Ven', 'PROPN'), ('d\u00edmelo', 'ADJ'), ('ay', 'PROPN'), ('yo', 'PRON'), ('ya', 'ADV'), ('vi', 'VERB'), ('Es', 'AUX'), ('obvio', 'ADJ'), ('que', 'SCONJ'), ('t\u00fa', 'PRON'), ('mil', 'NUM'), ('cosas', 'NOUN'), ('quieres', 'ADJ'), ('hacer', 'VERB')]\nThe output for the test_2 is largely incorrect:\nPr\u00e9ndelo, ap\u00e1galo, d\u00edmelo, and quieres are all verbs but tagged as other parts of speech.\nThanks!\nOperating System: Windows X\nPython Version Used: 3.5.2\nspaCy Version Used: 2.1.3, model es\nEnvironment Information:", "issue_status": "Closed", "issue_reporting_time": "2019-06-26T20:23:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "582": {"issue_url": "https://github.com/explosion/spaCy/issues/3883", "issue_id": "#3883", "issue_summary": "Train the existing POS tagger with my own training examples", "issue_description": "royakabiri commented on 27 Jun 2019\nI am trying to train the existing POS tagger on my own lexicon, not starting off from scratch (I do not want to create an \"empty model\").\nIn the documentation, it says \"Load the model you want to stat with\", and the next step is \"Add the tag map to the tagger using add_label method\". However, when I try to load the English small model, and add the tag map, it throws this error:\nValueError: [T003] Resizing pre-trained Tagger models is not currently supported.\nI have also seen #2635 but I am using a simple training style as suggested, and it gives such an error. I was wondering how it can be fixed.\nAlso, it is not very clear in the documentation if we need to have a mapping dictionary (TAG_MAP) even if our training examples tags are the same as the universal dependency tags.\nfrom __future__ import unicode_literals, print_function\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\n\nTAG_MAP = {\"noun\": {\"pos\": \"NOUN\"}, \"verb\": {\"pos\": \"VERB\"}, \"adj\": {\"pos\": \"ADJ\"}, \"adv\": {\"pos\": \"ADV\"}}\n\nTRAIN_DATA = [\n    ('Afrotropical', {'tags': ['adj']}), ('Afrocentricity', {'tags': ['noun']}),\n    ('Afrocentric', {'tags': ['adj']}), ('Afrocentrism', {'tags': ['noun']}),\n    ('Anglomania', {'tags': ['noun']}), ('Anglocentric', {'tags': ['adj']}),\n    ('apraxic', {'tags': ['adj']}), ('aglycosuric', {'tags': ['adj']}),\n    ('asecretory', {'tags': ['adj']}), ('aleukaemic', {'tags': ['adj']}),\n    ('agrin', {'tags': ['adj']}), ('Eurotransplant', {'tags': ['noun']}),\n    ('Euromarket', {'tags': ['noun']}), ('Eurocentrism', {'tags': ['noun']}),\n    ('adendritic', {'tags': ['adj']}), ('asynaptic', {'tags': ['adj']}),\n    ('Asynapsis', {'tags': ['noun']}), ('ametabolic', {'tags': ['adj']})\n]\n@plac.annotations(\n    lang=(\"ISO Code of language to use\", \"option\", \"l\", str),\n    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n)\ndef main(lang=\"en\", output_dir=None, n_iter=25):\n    nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n    tagger = nlp.get_pipe('tagger')\n    for tag, values in TAG_MAP.items():\n        tagger.add_label(tag, values)\n    nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n    optimizer = nlp.begin_training()\n    for i in range(n_iter):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n        print(\"Losses\", losses)\n\n    # test the trained model\n    test_text = \"I like Afrotropical apraxic blue eggs and Afrocentricity. A Eurotransplant is cool too. The agnathostomatous Euromarket and asypnapsis is even cooler. What about Eurocentrism?\"\n    doc = nlp(test_text)\n    print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])\n\n    # save model to output directory\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        # test the save model\n        print(\"Loading from\", output_dir)\n        nlp2 = spacy.load(output_dir)\n        doc = nlp2(test_text)\n        print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])\n\n\nif __name__ == \"__main__\":\n    plac.call(main)\nYour Environment\nspaCy version:** 2.1.3\nPlatform:** Darwin-18.6.0-x86_64-i386-64bit\nPython version:** 3.6.8\nModel: en_core_web_sm\n1", "issue_status": "Closed", "issue_reporting_time": "2019-06-26T18:31:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "583": {"issue_url": "https://github.com/explosion/spaCy/issues/3882", "issue_id": "#3882", "issue_summary": "Displacy.serve() serialising doc.user_data attribute fails if it contains a set as a value", "issue_description": "michaeljohnclancy commented on 26 Jun 2019\nHow to reproduce the behaviour\nimport spacy\n\nnlp = spacy.load('en_core_web_sm')\ntest_doc = nlp('This is a test doc!')\n\n#Set a value in user_data to a set\ntest_doc.user_data['test_data'] = set()\n\nspacy.displacy.serve(test_doc, style='dep')\nThe serialisation of the user_data attribute fails when attempting to dump a set. I assume this is because the set datatype is not part of standard JSON notation.\nAre only simple datatypes allowed in the user_data attribute?\nThanks, Michael\nError\nTypeError: can not serialise 'set' object\nYour Environment\nspaCy version: 2.1.4\nPlatform: Darwin-18.6.0-x86_64-i386-64bit\nPython version: 3.6.8", "issue_status": "Closed", "issue_reporting_time": "2019-06-26T08:06:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "584": {"issue_url": "https://github.com/explosion/spaCy/issues/3881", "issue_id": "#3881", "issue_summary": "Spacy Token parent issue", "issue_description": "asarmohd commented on 26 Jun 2019 \u2022\nedited by honnibal\nHow to reproduce the behaviour\nRun Spacy dependency parsing for the above sentence using the large module.\nWhenever we try to find token which has simillar to some other token in the sentence wrong parent is getting fetched.\ntext = \"The increase in non-cash adjustments was primarily due to a \" \\\n\"$26.0 million increase in stock-based compensation as a result of increase in headcount, $15.9 million **amortization** of issuance cost \"\\\n\"related to convertible notes, and $7.1 million increase in **amortization** of deferred sales commission and depreciation and **amortization**.\"\nnlp = spacy.load(\"en_core_web_lg\")\ndoc=nlp(str(text))\n\nprint(doc[55].text)\n\nprint([x.text for x in doc[55].ancestors])\namortization\n['depreciation', 'commission', 'of', '**amortization**', 'in', 'increase', '**amortization**', 'increase', 'of', 'result', 'as', 'increase', 'due', 'was']\nIn the above output\ninstead of taking parent of 2nd amortization it took parent of 1 st amortization\nprint(doc[47].text)\n\nprint([x.text for x in doc[47].ancestors])\namortization\n['in', 'increase', 'amortization', 'increase', 'of', 'result', 'as', 'increase', 'due', 'was']", "issue_status": "Closed", "issue_reporting_time": "2019-06-25T19:12:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "585": {"issue_url": "https://github.com/explosion/spaCy/issues/3880", "issue_id": "#3880", "issue_summary": "nlp.pipe() crashes after second empty string", "issue_description": "theudas commented on 25 Jun 2019 \u2022\nedited\nHow to reproduce the behaviour\nit is very easy to reproduce:\nimport spacy\nnlp = spacy.load(\"de_core_news_sm\")\ntexts = [\"ich bin ein Satz\", \"\", \" \", \"meow\", \" \", \"\"]\nfor doc in nlp.pipe(texts):\nprint(doc)\nit works if the array contains only one empty string.\nYour Environment\nspaCy version: 2.1.4\nPlatform: Windows-10-10.0.17763-SP0\nPython version: 3.6.4", "issue_status": "Closed", "issue_reporting_time": "2019-06-25T15:12:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "586": {"issue_url": "https://github.com/explosion/spaCy/issues/3879", "issue_id": "#3879", "issue_summary": "False positive matches on subsequent OPs in the Matcher patterns", "issue_description": "marina-sp commented on 25 Jun 2019\nHello, I'm working with spacy a lot, and I especially like the spacy.matchers! But I think I found something that is not working properly (yet).\nIf a pattern contains a pattern element with a specified OP followed by another optional pattern element with OP=?, then all other restrictions one the first element (like ORTH or REGEX) are ignored.\nHow to reproduce\nimport spacy\nimport de_core_news_sm\nfrom spacy.matcher import Matcher\n\nnlp = de_core_news_sm.load()\n\ntext = 'Das ist ein Test.'\ndoc = nlp(text)\nassert(len(doc) == 5)\n\npattern1 = [\n    {'ORTH': 'Das', 'OP': '?'}, \n    {'OP': '?'},\n    {'ORTH': 'Test'}\n]\nmatcher = Matcher(nlp.vocab)\nmatcher.add('rule1', None, pattern1)\nmatches = matcher(doc) \nassert(len(matches) == 2)   # fails because of a FP match 'ist ein Test'\nFalse positives appear on other OPs as well:\nMy Environment\nspaCy version: 2.1.4\nPlatform: Windows-10-10.0.17763-SP0\nPython version: 3.7.3\nI hope this is helpful and you will be able to look into it. Thank you in advance!", "issue_status": "Closed", "issue_reporting_time": "2019-06-25T12:14:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "587": {"issue_url": "https://github.com/explosion/spaCy/issues/3878", "issue_id": "#3878", "issue_summary": "Matcher to validate two tokens", "issue_description": "alteest commented on 25 Jun 2019 \u2022\nedited\nIt seems that this is not a bug, may be feature request.\nI have an issue with usage Match to validate 'want' or 'would like' in phrase.\nWith single token I can just use 'ORTH' : {'IN': ['word1', 'word2']}, but in my case it's impossible because 'would like' is two tokens. I understand it.\nIs there a possibility to describe somehow rule to validate my example.\nOperating System: Ubuntu 16.04\nPython Version Used: 3.6.8\nspaCy Version Used: 2.1.3", "issue_status": "Closed", "issue_reporting_time": "2019-06-25T11:03:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "588": {"issue_url": "https://github.com/explosion/spaCy/issues/3877", "issue_id": "#3877", "issue_summary": "NER consistently missing several organizations", "issue_description": "MrEricL commented on 24 Jun 2019\nI'm updating the NER with new data (about 2k more sentences) using existing tags. However when examining what the model missed with the manual annotation, there are a few ORGs that the model consistently seems to miss despite multiple examples in those sentences. Similar looking ORGs don't seem to suffer from the same issue.\nIs there a way for me to fix this without going into the over fitting territory?", "issue_status": "Closed", "issue_reporting_time": "2019-06-24T15:05:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}}}