{"open_issues": {"1": {"issue_url": "https://github.com/scrapy/scrapy/issues/4308", "issue_id": "#4308", "issue_summary": "Mac OS X, OS X \u2192 macOS", "issue_description": "Member\nGallaecio commented 13 hours ago\nWe have a few references in the documentation where we use the old name of that OS. We should update them.", "issue_status": "Open", "issue_reporting_time": "2020-02-06T08:23:42Z"}, "2": {"issue_url": "https://github.com/scrapy/scrapy/issues/4307", "issue_id": "#4307", "issue_summary": "Use f-strings", "issue_description": "Member\nGallaecio commented 13 hours ago\nThey offer a simpler syntax and speed, so once we drop Python 3.5 support I guess it makes sense to look into replacing % and format usages with f-strings where possible.\n\ud83d\ude80 1", "issue_status": "Open", "issue_reporting_time": "2020-02-06T08:01:52Z"}, "3": {"issue_url": "https://github.com/scrapy/scrapy/issues/4306", "issue_id": "#4306", "issue_summary": "Add a setting to choose a custom AsyncIO loop", "issue_description": "Member\nGallaecio commented 14 hours ago\nJust as #4294 allows to easily choose a custom Twister reactor, I think we should consider implementing a setting that allows choosing a custom AsyncIO loop (when using and AsyncIO reactor.\nI\u2019m thinking of https://github.com/MagicStack/uvloop, which I\u2019ve used in web development, and can\u2019t help by wonder if it could improve Scrapy performance in some scenarios.\n\ud83d\ude80 1", "issue_status": "Open", "issue_reporting_time": "2020-02-06T07:55:03Z"}, "4": {"issue_url": "https://github.com/scrapy/scrapy/issues/4302", "issue_id": "#4302", "issue_summary": "Allow checking that settings are named correctly", "issue_description": "Member\nGallaecio commented yesterday\nI would like a way to be able to detect typos on spider settings.\nSome approaches that I can think of:\nMaking this a core feature of Scrapy, having extensions report the settings they use somehow and having an opt-in setting to enable this kind of check.\nWriting a static analyzer for Scrapy projects that detects such things. It could be an extension for Pylint.\n\u2764\ufe0f 1\n\ud83d\udc40 1", "issue_status": "Open", "issue_reporting_time": "2020-02-05T10:03:16Z"}, "5": {"issue_url": "https://github.com/scrapy/scrapy/issues/4299", "issue_id": "#4299", "issue_summary": "Make it easier to use Scrapy in Jupyter Notebook", "issue_description": "Member\nGallaecio commented 2 days ago\nSee http://gsoc2015.scrapinghub.com/ideas/#iphyton-ide for more information.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2020-02-04T12:35:06Z"}, "6": {"issue_url": "https://github.com/scrapy/scrapy/issues/4296", "issue_id": "#4296", "issue_summary": "Merge Slot classes back?", "issue_description": "Contributor\nwRAR commented 6 days ago \u2022\nedited\nIn 409aaad scrapy.core.engine.Slot was extracted from scrapy.core.engine.ExecutionEngine and they had a many-to-one rel. In a84e5f8 that rel was changed to one-to-one, and I'm not sure what does this split achieve now.", "issue_status": "Open", "issue_reporting_time": "2020-01-31T18:00:17Z"}, "7": {"issue_url": "https://github.com/scrapy/scrapy/issues/4295", "issue_id": "#4295", "issue_summary": "Document async signal handlers", "issue_description": "Contributor\nwRAR commented 6 days ago\nRight now there are 7 signals whose handlers can return Deferreds: https://docs.scrapy.org/en/latest/topics/signals.html?highlight=deferred\nWith #4271 these signal handlers can also be async def coroutines.\nAFAIK nowhere in the docs it is explained what does this mean and how can it be used. I looked at the code and the situation seems to be this:\nitem_scraped, item_dropped, item_error: I'm not 100% sure as I couldn't fully untangle the web of callbacks and Deferreds inside Scraper but it looks like until the handlers for items produced by a Request haven't finished, that Request isn't counted as processed, which means spider_idle isn't called and so the spider isn't closed, and may also mean some queue-related things. I traced the callback chain until Scraper.enqueue_scrape.finish_scraping which also calls some _scrape_next method.\nengine_started, engine_stopped, spider_opened, spider_closed: these are included in the spider/engine startup/shutdown processes and the process waits for the handler to finish before proceeding to the next line.", "issue_status": "Open", "issue_reporting_time": "2020-01-31T17:54:17Z"}, "8": {"issue_url": "https://github.com/scrapy/scrapy/issues/4292", "issue_id": "#4292", "issue_summary": "Exceptions in middleware don't return exit code 1 in `scrapy crawl` & `scrapy check`", "issue_description": "dpfeif commented 9 days ago \u2022\nedited\nDescription\nIf a middleware raises an exception, running scrapy crawl or scrapy check raises the exception to the shell but returns with exit code 0, instead of the expected 1.\nSteps to Reproduce\nSet up the tutorial up to here\nCreate a minimal middleware raising an issue in middlewares.py\nclass BreakingMiddleware:\n    def __init__(self):\n        raise Exception(\"uhoh\")\nAdd the middleware to the quotes spider and a contract for the parse function\nimport scrapy\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    custom_settings = {\n        \"SPIDER_MIDDLEWARES\" : {\n            \"tutorial.middlewares.BreakingMiddleware\": 100,\n        }\n    }\n\n    def start_requests(self):\n        urls = [\n            \"http://quotes.toscrape.com/page/1/\",\n            \"http://quotes.toscrape.com/page/2/\",\n        ]\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n\n    def parse(self, response):\n        \"\"\"\n        @url http://quotes.toscrape.com/page/1/\n        @returns items 10 10\n        @returns requests 10 10\n        \"\"\"\n\n        page = response.url.split(\"/\")[-2]\n        filename = \"quotes-%s.html\" % page\n        with open(filename, \"wb\") as f:\n            f.write(response.body)\n        self.log(\"Saved file %s\" % filename)\nExecute scrapy check or scrapy crawl quotes\nExecute echo $?\nExpected behavior:\nExit code 1\nActual behavior:\nExit code 0\nReproduces how often: 100%\nVersions\nScrapy       : 1.8.0\nlxml         : 4.4.2.0\nlibxml2      : 2.9.4\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.10.0\nPython       : 3.8.0 (default, Nov 26 2019, 14:40:47) - [Clang 10.0.1 (clang-1001.0.46.4)]\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019)\ncryptography : 2.8\nPlatform     : macOS-10.15.2-x86_64-i386-64bit\nAdditional context\nscrapy check logs:\n----------------------------------------------------------------------\nRan 0 contracts in 0.000s\n\nOK\nUnhandled error in Deferred:\n\nTraceback (most recent call last):\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 184, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 188, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1613, in unwindGenerator\n    return _cancellableInlineCallbacks(gen)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1529, in _cancellableInlineCallbacks\n    _inlineCallbacks(None, g, status)\n--- <exception caught here> ---\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\n    result = g.send(result)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 86, in crawl\n    self.engine = self._create_engine()\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 111, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/core/engine.py\", line 70, in __init__\n    self.scraper = Scraper(crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/core/scraper.py\", line 69, in __init__\n    self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/middleware.py\", line 53, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/middleware.py\", line 35, in from_settings\n    mw = create_instance(mwcls, settings, crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/utils/misc.py\", line 146, in create_instance\n    return objcls(*args, **kwargs)\n  File \"/Users/dpf/Public/break-scrapy-check/tutorial/tutorial/middlewares.py\", line 10, in __init__\n    raise Exception(\"uhoh\")\nbuiltins.Exception: uhoh\nscrapy crawl quotes logs:\n2020-01-28 11:29:40 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: tutorial)\n2020-01-28 11:29:40 [scrapy.utils.log] INFO: Versions: lxml 4.4.2.0, libxml2 2.9.4, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.8.0 (default, Nov 26 2019, 14:40:47) - [Clang 10.0.1 (clang-1001.0.46.4)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform macOS-10.15.2-x86_64-i386-64bit\n2020-01-28 11:29:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders']}\n2020-01-28 11:29:40 [scrapy.extensions.telnet] INFO: Telnet Password: c7073899ef38fd40\n2020-01-28 11:29:40 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats']\n2020-01-28 11:29:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\nUnhandled error in Deferred:\n2020-01-28 11:29:40 [twisted] CRITICAL: Unhandled error in Deferred:\n\nTraceback (most recent call last):\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 184, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 188, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1613, in unwindGenerator\n    return _cancellableInlineCallbacks(gen)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1529, in _cancellableInlineCallbacks\n    _inlineCallbacks(None, g, status)\n--- <exception caught here> ---\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\n    result = g.send(result)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 86, in crawl\n    self.engine = self._create_engine()\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 111, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/core/engine.py\", line 70, in __init__\n    self.scraper = Scraper(crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/core/scraper.py\", line 69, in __init__\n    self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/middleware.py\", line 53, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/middleware.py\", line 35, in from_settings\n    mw = create_instance(mwcls, settings, crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/utils/misc.py\", line 146, in create_instance\n    return objcls(*args, **kwargs)\n  File \"/Users/dpf/Public/break-scrapy-check/tutorial/tutorial/middlewares.py\", line 10, in __init__\n    raise Exception(\"uhoh\")\nbuiltins.Exception: uhoh\n\n2020-01-28 11:29:40 [twisted] CRITICAL:\nTraceback (most recent call last):\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\n    result = g.send(result)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 86, in crawl\n    self.engine = self._create_engine()\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/crawler.py\", line 111, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/core/engine.py\", line 70, in __init__\n    self.scraper = Scraper(crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/core/scraper.py\", line 69, in __init__\n    self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/middleware.py\", line 53, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/middleware.py\", line 35, in from_settings\n    mw = create_instance(mwcls, settings, crawler)\n  File \"/Users/dpf/Public/break-scrapy-check/py3/lib/python3.8/site-packages/scrapy/utils/misc.py\", line 146, in create_instance\n    return objcls(*args, **kwargs)\n  File \"/Users/dpf/Public/break-scrapy-check/tutorial/tutorial/middlewares.py\", line 10, in __init__\n    raise Exception(\"uhoh\")\nException: uhoh", "issue_status": "Open", "issue_reporting_time": "2020-01-28T10:33:29Z"}, "9": {"issue_url": "https://github.com/scrapy/scrapy/issues/4289", "issue_id": "#4289", "issue_summary": "Fatal error launching scrapy>1.6.0 from Anaconda Prompt", "issue_description": "pwinzer commented 11 days ago \u2022\nedited\nDescription\nI'm having the same issue reported in issue 4075, but I'd like to provide some more information.\nSteps to Reproduce\nFresh Anaconda install, installed scrapy with conda install -c conda-forge scrapy, as specified in official Windows installation instructions. Scrapy 1.8.0 was successfully installed.\nEnter a scrapy command: scrapy shell, scrapy version, scrapy startproject spam, etc.\nExpected behavior:\nScrapy commands work as documented, as they have in the past.\nActual behavior:\nScrapy commands yield\nFatal error in launcher: Unable to create process using '\"d:\\bld\\scrapy_1572360424769\\_h_env\\python.exe\" \"C:\\Users\\path\\to\\Continuum\\anaconda3\\Scripts\\scrapy.exe\" version'\nThere is no d:\\ on my machine. I have no idea where this path is coming from.\nReproduces how often:\n100% in my current installation\nWork around:\npython -m scrapy <command> rather than scrapy <command>\nVersions\npython -m scrapy version --verbose yields.\nScrapy : 1.8.0\nlxml : 4.4.1.0\nlibxml2 : 2.9.9\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.21.0\nTwisted : 19.10.0\nPython : 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1d 10 Sep 2019)\ncryptography : 2.7\nPlatform : Windows-10-10.0.18362-SP0\nAdditional context\nIt appears any version after 1.6.0 has this issue. Downgrading to 1.6.0 with conda install -c conda-forge scrapy=1.6.0 resolves the launching issue. I installed a few versions of 1.7 with conda install -c conda-forge scrapy=1.7.x and the launcher issue was present there.", "issue_status": "Open", "issue_reporting_time": "2020-01-26T14:30:25Z"}, "10": {"issue_url": "https://github.com/scrapy/scrapy/issues/4285", "issue_id": "#4285", "issue_summary": "Fix \"Debugging memory leaks with Guppy\" section (docs)", "issue_description": "Contributor\nnoviluni commented 13 days ago \u2022\nedited\nAfter removing the Python 2.7 support, this section:\nhttps://docs.scrapy.org/en/latest/topics/leaks.html#debugging-memory-leaks-with-guppy\nshould be removed or merged with this:\nhttps://docs.scrapy.org/en/latest/topics/leaks.html#topics-leaks-muppy\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2020-01-24T10:37:54Z"}, "11": {"issue_url": "https://github.com/scrapy/scrapy/issues/4284", "issue_id": "#4284", "issue_summary": "Uses ipdb instead pdb when package installed", "issue_description": "jeanmask commented 14 days ago\nSummary\nAdd support to uses ipdb instead pdb when ipdb is installed, if you use parameter --pdb.\nMotivation\nImproves debugging\nDescribe alternatives you've considered\nAn alternative is implement a new cmd parameter --ipdb to handle it.", "issue_status": "Open", "issue_reporting_time": "2020-01-23T20:26:53Z"}, "12": {"issue_url": "https://github.com/scrapy/scrapy/issues/4280", "issue_id": "#4280", "issue_summary": "scrapy installation error in visual code", "issue_description": "Haroon-afk commented 18 days ago\nDescription\n[Description of the issue]\nSteps to Reproduce\n[First Step]\n[Second Step]\n[and so on...]\nExpected behavior: [What you expect to happen]\nActual behavior: [What actually happens]\nReproduces how often: [What percentage of the time does it reproduce?]\nVersions\nPlease paste here the output of executing scrapy version --verbose in the command line.\nAdditional context\nAny additional information, configuration, data or output from commands that might be necessary to reproduce or understand the issue. Please try not to include screenshots of code or the command line, paste the contents as text instead. You can use GitHub Flavored Markdown to make the text look better.", "issue_status": "Open", "issue_reporting_time": "2020-01-20T08:06:54Z"}, "13": {"issue_url": "https://github.com/scrapy/scrapy/issues/4277", "issue_id": "#4277", "issue_summary": "Response.follow() method not consistent with Request.__init__(). Missing `flags`.", "issue_description": "Contributor\nLanetheGreat commented 20 days ago\nVersions Affected:\nAll versions after PR #2082 when flags were added to Request. (>1.4.0)\nResponse.follow() method not consistent with Request.init(). Missing flags.\nNo sure how this got missed since PR #2082 was merged, but looking at the parameters for Request.__init__ and Response.follow it appears flags wasn't added to Response.follow to keep it in line with how creating new Request instances works. I'm not sure if this was just overlooked or was intentional, when flags support was added to requests. It's also missing for the subclasses of Response as well (TextResponse will need updated, the rest currently use inheritance).\nI was looking to be able to add custom flags to certain Response.follow() calls but realized looking at the source they wouldn't be carried over to the new responses, but this can easily be worked around by manually added the flag after Request creation. This seems like an easy fix to include in 1.8.1 or later but doesn't seem too high priority and I just thought you guys should know or that it should at least be noted somewhere on this repo since the docs still say, \"It accepts the same arguments as Request.__init__ method...\".", "issue_status": "Open", "issue_reporting_time": "2020-01-17T13:28:19Z"}, "14": {"issue_url": "https://github.com/scrapy/scrapy/issues/4276", "issue_id": "#4276", "issue_summary": "Fields not registered in Items.fields if set as static variable", "issue_description": "scratchmex commented 21 days ago \u2022\nedited\nDescription\nThe static variables set in scrapy.Items class are not registered in the internal _values dict and therefore I cannot access them via self['field_name'], i.e., the __getattr__ function.\nSteps to Reproduce\nIn my items_error.py file I have this. (the actual code is larger but I isolate the code causing the error)\nfrom scrapy import Item, Field\n\nclass BaseItem(Item):\n    url=Field()\n    id=Field()\n    type=Field()\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if not self['type']: #<--- error here. Raises KeyError\n            raise NotImplementedError('type of ad must be set as static class variable')\n        #use url hash as id to avoid duplicates\n        self['id']=hash(self.get('url'))\n\nclass ChildItem(BaseItem):\n    type='estate'\n    other_fields=Field()\nWhen I ran it this raises:\nC:\\Users\\ivangonzalez\\Documents\\Code\\scrady>python\nPython 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 21:48:41) [MSC v.1916 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from scrady.items_error import *\n>>> ChildItem()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Users\\ivangonzalez\\Documents\\Code\\scrady\\scrady\\items_error.py\", line 10, in __init__\n    if not self['type']: #<--- error here. Raises KeyError\n  File \"C:\\Users\\ivangonzalez\\Anaconda3\\lib\\site-packages\\scrapy\\item.py\", line 91, in __getitem__\n    return self._values[key]\nKeyError: 'type'\n>>> BaseItem()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Users\\ivangonzalez\\Documents\\Code\\scrady\\scrady\\items_error.py\", line 10, in __init__\n    if not self['type']: #<--- error here. Raises KeyError\n  File \"C:\\Users\\ivangonzalez\\.virtualenvs\\scrady-d4yjsu4b\\lib\\site-packages\\scrapy\\item.py\", line 91, in __getitem__       return self._values[key]\nKeyError: 'type'\n>>>\nExpected behavior: self['field_name'] should return the Field() attribute.\nActual behavior: static fields are not registered by scrapy.Items in scrapy.Items._values[key].\nReproduces how often: Ran in pipenv, so 100%.\nVersions\nPlease paste here the output of executing scrapy version --verbose in the command line:\nC:\\Users\\ivangonzalez\\Documents\\Code\\scrady>scrapy version --verbose\nScrapy       : 1.8.0\nlxml         : 4.4.2.0\nlibxml2      : 2.9.5\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.10.0\nPython       : 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 21:48:41) [MSC v.1916 64 bit (AMD64)]\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019)\ncryptography : 2.8\nPlatform     : Windows-10-10.0.18362-SP0\nAdditional context\nIn scrapy/item.py : ItemMeta class the attributes should be registered by this code:\nclass ItemMeta(ABCMeta):\n    \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n\n    .. _metaclass: https://realpython.com/python-metaclasses\n    \"\"\"\n\n    def __new__(mcs, class_name, bases, attrs):\n        classcell = attrs.pop('__classcell__', None)\n        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n\n        fields = getattr(_class, 'fields', {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n] #<--- here\n\n        new_attrs['fields'] = fields\n        new_attrs['_class'] = _class\n        if classcell is not None:\n            new_attrs['__classcell__'] = classcell\n        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)\nBut they aren't. I don't know why.", "issue_status": "Open", "issue_reporting_time": "2020-01-17T01:22:01Z"}, "15": {"issue_url": "https://github.com/scrapy/scrapy/issues/4268", "issue_id": "#4268", "issue_summary": "Random failures in CrawlTestCase.test_fixed_delay", "issue_description": "Contributor\nwRAR commented 28 days ago\nOn some test runs CrawlTestCase.test_fixed_delay fails with\n>       yield self._test_delay(total=3, delay=0.1)\nE   twisted.trial.unittest.FailTest: twisted.trial.unittest.FailTest: True is not false : test total or delay values are too small", "issue_status": "Open", "issue_reporting_time": "2020-01-09T11:21:43Z"}, "16": {"issue_url": "https://github.com/scrapy/scrapy/issues/4266", "issue_id": "#4266", "issue_summary": "process_spider_output called twice when exception occurs", "issue_description": "Contributor\nStasDeep commented on Jan 3\nDescription\nDon't know if this should be considered a bug or not, but it looks very unintuitive.\nWhen raising an exception in a generator callback, the process_spider_output method of a spider middleware, that goes after a middleware that catches an exception, is invoked twice. Once for the result of the callback that was yielded before exception occurred and one more time for the iterable returned by the process_spider_exception method of the middleware that caught an exception.\nI think it may make more sense to pass objects yielded by the callback to the process_spider_exception so that middleware can have control over what to do with objects yielded by a callback that raised an exception. Or, we should have a way of checking if the object passed to process_spider_output was generated by a callback that eventually raised an exception. For example, I want to modify an item generated by a callback if there was an exception after it was generated.\nAt the very least, this should be documented to not mislead.\nCode to Reproduce\nimport scrapy\n\n\nclass FirstExceptionHandlerMiddleware:\n    def process_spider_exception(self, response, exception, spider):\n        print('Exception caught with First:', exception)\n        return []\n\n    def process_spider_output(self, response, result, spider):\n        result = list(result)\n        print('Processing spider output with First:', result)\n        yield from result\n\n\nclass SecondExceptionHandlerMiddleware:\n    def process_spider_output(self, response, result, spider):\n        result = list(result)\n        print('Processing spider output with Second:', result)\n        yield from result\n\n\nclass ExceptionSpider(scrapy.Spider):\n    name = 'exception_spider'\n    start_urls = ['https://example.org']\n    custom_settings = {\n        'SPIDER_MIDDLEWARES': {\n            __name__ + '.FirstExceptionHandlerMiddleware': 870,\n            __name__ + '.SecondExceptionHandlerMiddleware': 860,\n        }\n    }\n\n    def parse(self, response):\n        yield {'an': 'item'}\n        raise Exception('foo')\nOutput\nException caught with First: foo\nProcessing spider output with First: [{'an': 'item'}]\nProcessing spider output with Second: [{'an': 'item'}]\nProcessing spider output with Second: []\nVersions\nScrapy       : 1.7.1\nlxml         : 4.2.5.0\nlibxml2      : 2.9.8\ncssselect    : 1.0.3\nparsel       : 1.5.1\nw3lib        : 1.19.0\nTwisted      : 18.9.0\nPython       : 3.6.8 (default, Sep  5 2019, 08:52:26) - [GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.10.44.4)]\npyOpenSSL    : 18.0.0 (OpenSSL 1.1.0j  20 Nov 2018)\ncryptography : 2.4.2\nPlatform     : Darwin-19.2.0-x86_64-i386-64bit", "issue_status": "Open", "issue_reporting_time": "2020-01-03T06:55:02Z"}, "17": {"issue_url": "https://github.com/scrapy/scrapy/issues/4263", "issue_id": "#4263", "issue_summary": "defer.inlineCallbacks in spider ?", "issue_description": "HelloEdit commented on Jan 1 \u2022\nedited\nHi,\nI'm trying to use this feature of Twisted to run the following code:\n# -*- coding: utf-8 -*-\nimport scrapy\nimport re\nfrom twisted.internet.defer import inlineCallbacks\n\nfrom sherlock import utils, items, regex\n\n\nclass PagesSpider(scrapy.spiders.SitemapSpider):\n    name = 'pages'\n    allowed_domains = ['thing.com']\n    sitemap_follow = [r'sitemap_page']\n\n    def __init__(self, site=None, *args, **kwargs):\n        super(PagesSpider, self).__init__(*args, **kwargs)\n\n    @inlineCallbacks\n    def parse(self, response):\n        # things\n        response = yield scrapy.Request(\"https://google.com\")\n        # Twisted execute the request and resume the generator here with the response\n        print(response.text)\nIs this possible ? I'm trying to use this to dispense with the inline-request module.\nThanks", "issue_status": "Open", "issue_reporting_time": "2020-01-01T14:23:26Z"}, "18": {"issue_url": "https://github.com/scrapy/scrapy/issues/4260", "issue_id": "#4260", "issue_summary": "First Spider Middleware does not process exception for generator callback", "issue_description": "Contributor\nStasDeep commented on Dec 30, 2019 \u2022\nedited\nDescription\nprocess_spider_exception method of a spider middleware is ignored when spider middleware is first and callback is a generator.\nSteps to Reproduce\nCreate spider middleware with defined process_spider_exception\nPut it into SPIDER_MIDDLEWARES with number more than 900 (to make it first)\nRaise an exception in a spider callback\nUPD. Yield an item from the callback\nExpected behavior: process_spider_exception is called for this exception\nActual behavior: process_spider_exception is not called\nVersions\nScrapy       : 1.7.1\nlxml         : 4.2.5.0\nlibxml2      : 2.9.8\ncssselect    : 1.0.3\nparsel       : 1.5.1\nw3lib        : 1.19.0\nTwisted      : 18.9.0\nPython       : 3.6.8 (default, May  8 2019, 05:35:00) - [GCC 6.3.0 20170516]\npyOpenSSL    : 18.0.0 (OpenSSL 1.1.0j  20 Nov 2018)\ncryptography : 2.4.2\nPlatform     : Linux-4.9.184-linuxkit-x86_64-with-debian-9.9", "issue_status": "Open", "issue_reporting_time": "2019-12-30T10:02:52Z"}, "19": {"issue_url": "https://github.com/scrapy/scrapy/issues/4255", "issue_id": "#4255", "issue_summary": "Settings.getlist default value", "issue_description": "mredaelli commented on Dec 24, 2019\nNot sure if I'm reading this wrong, but I don't understand why this function has a default of None, but can actually never return it, because of the or at this line.\nIn my use case it would be handy to distinguish between being passed nothing (getting a None) and an empty list.", "issue_status": "Open", "issue_reporting_time": "2019-12-24T08:37:10Z"}, "20": {"issue_url": "https://github.com/scrapy/scrapy/issues/4253", "issue_id": "#4253", "issue_summary": "Settings, multiple concurrent spiders, and middlewares", "issue_description": "mredaelli commented on Dec 23, 2019\nThis is more of a question than a feature request, but I guess I can translate it to a request for an enhancement of the documentation.\nThis is a question I posted on StackOverflow, where I didn't get any answers:\n=========================\nI'm used to running spiders one at a time, because we mostly work with scrapy crawl and on scrapinghub, but I know that one can run multiple spiders concurrently, and I have seen that middlewares often have a spider parameter in their callbacks.\nWhat I'd like to understand is:\nthe relationship between Crawler and Spider. If I run one spider at a time, I'm assuming there's one of each. But if you run more spiders together, like in the example linked above, do you have one crawler for multiple spiders, or are they still 1:1?\nis there in any case only one instance of a middleware of a certain class, or do we get one per-spider or per-crawler?\nAssuming there's one, what are the crawler.settings in the middleware creation (for example, here)? In the documentation it says that those take into account the settings overridden in the spider, but if there are multiple spiders with conflicting settings, what happens?\nI'm asking because I'd like to know how to handle spider-specific settings. Take again the DeltaFetch middleware as an example:\nenabling it seems to be a global matter, because DELTAFETCH_ENABLED is read from the crawler.settings\nhowever, the sqlite db is opened in spider_opened and is a unique instance variable (i.e., not depending on the spider); so if you have more than one spider and the instance is shared, when the second spider is opened, the old db is lost. And if you have only one instance of the middleware per spider, why bother passing the spider as a parameter?\nIs that a correct way of handling it, or should you rather have a dict spider_dbs indexed by spider name?", "issue_status": "Open", "issue_reporting_time": "2019-12-23T09:36:25Z"}, "21": {"issue_url": "https://github.com/scrapy/scrapy/issues/4250", "issue_id": "#4250", "issue_summary": "Batch deliveries for long running crawlers", "issue_description": "Contributor\nejulio commented on Dec 19, 2019\nSummary\nAdd a new setting FEED_STORAGE_BATCH that will deliver a file whenever item_scraped_count reaches a multiple of that number.\nMotivation\nFor long running jobs (say we are consuming inputs from a working queue) we may want partial results instead of waiting for a long batch to finish.\nDescribe alternatives you've considered\nOf course we can stop and restart a spider every now and then.\nHowever, a simpler approach is to have it running as long as required, but delivering partial results.", "issue_status": "Open", "issue_reporting_time": "2019-12-19T12:40:20Z"}, "22": {"issue_url": "https://github.com/scrapy/scrapy/issues/4240", "issue_id": "#4240", "issue_summary": "Wrong type(response) for binary responses", "issue_description": "Contributor\nejulio commented on Dec 17, 2019\nDescription\nBinary (file) responses are identified as TextResponse instead of a plain Response in spider.parse.\nSteps to Reproduce\nFrom this URL https://www.ecb.europa.eu/press/pr/date/2004/html/pr040702.en.html\nWe can get this link https://www.ecb.europa.eu/pub/redirect/pub_5874_en.html (text is pdf 1692 kB).\nIt redirects to https://www.ecb.europa.eu/pub/pdf/other/developmentstatisticsemu200406en.pdf\nExpected behavior: [What you expect to happen]\nIn the spider, isinstance(response, TextResponse) should be False.\nActual behavior: [What actually happens]\nIn the spider, isinstance(response, TextResponse) is True, even though the Content-Type header is application/pdf.\nVersions\nScrapy       : 1.7.3\nlxml         : 4.4.2.0\nlibxml2      : 2.9.9\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.10.0\nPython       : 3.6.8 (default, Jan 14 2019, 11:02:34) - [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019)\ncryptography : 2.8\nPlatform     : Linux-4.15.0-72-generic-x86_64-with-Ubuntu-18.04-bionic\nExtra info\nProbably we should handle known binary types in Content-Type here https://github.com/scrapy/scrapy/blob/master/scrapy/responsetypes.py\nAt least, application/pdf should be in the list.\nIt would be nice to add some mechanism to allow developers to extend the mapping on the fly, as we can get new types in a project basis and it would be easier update the desired behavior.\n\ud83d\udc4d 2\n\ud83d\ude80 1", "issue_status": "Open", "issue_reporting_time": "2019-12-17T14:51:59Z"}, "23": {"issue_url": "https://github.com/scrapy/scrapy/issues/4234", "issue_id": "#4234", "issue_summary": "The line length is not consistent through the whole project", "issue_description": "Contributor\nnoviluni commented on Dec 16, 2019 \u2022\nedited\nBefore adding the flake8 checker, we were allowing any line length, recommending people to follow pep8 (79 characters) but allowing any length \u201cif it improves readability\u201d.\nWhen adding the flake8 checker we needed to add the E501 rule exception to most of the files (https://github.com/scrapy/scrapy/blob/master/pytest.ini), so technically, now we are allowing any line length in those files and a line length of 79 characters in the other files.\nAs this is bad for the code readability and maintenance (and self-contradictory), I think we should decide a clearer position of the line length for the entire project.\nSome possibilities are:\nAllow any length in all the project (if that\u2019s the case we can just ignore the E501 rule for the whole project).\nRestrict to 79 characters as PEP8 stands for.\nRestrict to another agreed length.\nIn the 2nd and 3rd case it could be possible to allow longer lines individually (and not per file) by adding the # noqa: E501 to the line.\nLooking at the project\nTo get a big picture of the issue I decided to check the current status of the project and I graphed this (please, obviate the y axis scale), that represents the line length frequency (for those lines with length > 79).\nWhen looking at the graph, it is curious to see that when arriving to the 110 characters it decreases a lot and that there is a \u201cnatural\u201d end in about the 140 characters.\nYou can find here the full file list: https://gist.githubusercontent.com/noviluni/0993ac9d2cbdad12e84b8f5ba2a18d17/raw/d84c298645bd5d6b10545726c2292a417432888c/line_too_long_ordered.txt\nSolutions\n1. Allow any length\nI think it\u2019s obvious that this (and the current) option is the worst, as it allows some lines to be excessively long. Take this example:\nscrapy/tests/test_utils_sitemap.py\nLine 26 in c841a1f\n [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'}, {'priority': '0.8', 'loc': 'http://www.example.com/Special-Offers.html', 'lastmod': '2009-08-16', 'changefreq': 'weekly'}]) \nthat is 237 characters long without improving the readability (in fact, it gets worse).\n2. Restrict to 79 characters (PEP8)\nSome maintainers have declared in the past that this is the prefered option, but that if the code readability improves we should allow longer lines. If we decide to follow this, we should first decide or at least try to describe what does \u201cimproves the readability\u201d mean and allow only those cases to get a longer length by adding a # noqa: E501 to those lines, avoiding the current status, where we allow all the file to have any line length.\n3. Restrict to another agreed length.\nThere are several projects where the maintainers have decided to follow another line length.\nDjango: \u201cDon\u2019t limit lines of code to 79 characters if it means the code looks significantly uglier or is harder to read. We allow up to 119 characters as this is the width of GitHub code review; anything longer requires horizontal scrolling which makes review more difficult.\u201d\nFlask: \u201c79 characters with a soft limit for 84 if absolutely necessary\u201d\nRequests: \u201cLine-length can exceed 79 characters, to 100, when convenient. Line-length can exceed 100 characters, when doing otherwise would be terribly inconvenient.\u201d\nPandas: \u201cWe restrict line-length to 80 characters to promote readability\u201d\nBlack: \u201cBlack defaults to 88 characters per line, which happens to be 10% over 80. This number was found to produce significantly shorter files than sticking with 80 (the most popular), or even 79 (used by the standard library). In general, 90-ish seems like the wise choice.\u201d\nOn the other hand, some editors/IDEs (like Pycharm) default to 120 characters.\nMy opinion\nI personally like the Django\u2019s argument, as it isn\u2019t subjective, it exposes a good and objective reason to limit the line length to 119 characters. Moreover, after looking the graph it seems that the \u201cnatural end\u201d of the current codebase is about that number.\nI can understand that PEP8 purists could consider a sacrilege to allow all the lines to be longer than 79 characters by default, but I consider that the modern screens shouldn't have any problem to show them completely.\nYour opinion\nWhat do you think? Is there any of the three proposals that fits the project? Do you have more arguments (pros and cons) or another point of view?\nAll feedback is welcomed :)", "issue_status": "Open", "issue_reporting_time": "2019-12-15T18:33:54Z"}, "24": {"issue_url": "https://github.com/scrapy/scrapy/issues/4233", "issue_id": "#4233", "issue_summary": "Missing the Scrapy entry in Wikipedia in many languages", "issue_description": "Contributor\nnoviluni commented on Dec 15, 2019 \u2022\nedited\nThis is not an issue related with the code itself but with Scrapy.\nI've seen that the only Wikipedias with the Scrapy entry are:\nEnglish: https://en.m.wikipedia.org/wiki/Scrapy\nFrench: https://fr.m.wikipedia.org/wiki/Scrapy\nPolish: https://pl.m.wikipedia.org/wiki/Scrapy\nChinese: https://zh.m.wikipedia.org/wiki/Scrapy\nI think it could be a good idea to create this issue and tag it as \"easy\" or \"good first issue\" and maybe even pin this issue some time to encourgare those who want to contribute to Scrapy to create an entry in Wikipedia in their native language.\nIt could be a good/nice goal to try to get 17 (just to put a number, it is the number of wikipedias with +1.000.000 articles) for june 2020 (6 months).\nFeel free to give your opinion or even close the issue if you don't like the idea.\nExpected goal:\nEnglish\nCebuano\nSwedish\nGerman\nFrench\nDutch\nRussian\nItalian\nSpanish\nPolish\nWaray\nVietnamese\nJapanese\nChinese\nArabic\nPortuguese\nOther:\nKorean", "issue_status": "Open", "issue_reporting_time": "2019-12-15T10:12:18Z"}, "25": {"issue_url": "https://github.com/scrapy/scrapy/issues/4225", "issue_id": "#4225", "issue_summary": "File extension not extracted from URLs with query parameters", "issue_description": "dankeil commented on Dec 10, 2019\nDescription\nWhen using the built-in FilesPipeline, files downloaded from URLs which include query parameters will often be saves without an extension, causing potential problems for downstream processing.\nSteps to Reproduce\nApply the patch below\nRun pytest tests/test_pipeline_files.py -k test_file_path from the root scrapy directory\nExpected behavior: file_path() will return 'full/a2b4913a62f65445aeae2bac08cd8c3b41d7195e.txt' (with the .txt extension)\nActual behavior: file_path() returns 'full/a2b4913a62f65445aeae2bac08cd8c3b41d7195e'\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex 52f2b554..62f93c6a 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -58,6 +58,8 @@ class FilesPipelineTestCase(unittest.TestCase):\n         self.assertEqual(file_path(Request(\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\\\n                                     //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\")),\n                          'full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png')\n+        self.assertEqual(file_path(Request(\"http://foo.bar/baz.txt?fizz\")),\n+                          'full/a2b4913a62f65445aeae2bac08cd8c3b41d7195e.txt')\n                          \n \n     def test_fs_store(self):\nVersions\nScrapy       : 1.8.0\nlxml         : 4.4.2.0\nlibxml2      : 2.9.10\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.10.0\nPython       : 3.7.4 (default, Aug 13 2019, 15:17:50) - [Clang 4.0.1 (tags/RELEASE_401/final)]\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019)\ncryptography : 2.8\nPlatform     : Darwin-19.0.0-x86_64-i386-64bit\nAdditional context\nThe example URL in the diff above comes from a previous issue related to the handling of file paths, #1287. At the time that that issue was opened, the resulting path returned from Scrapy v1.6.0 would have been 'full/a2b4913a62f65445aeae2bac08cd8c3b41d7195e.txt?fizz'. Having no extension at all is an improvement over that, but the best possible result would be to correctly extract the .txt extension from the URL path.\nThere are several existing pull requests for solving this issue which could be added to the v1.8.0 code.\nBoth #1548 and #2809 simply look at the extension of the path component of the URL if the extension of the full URL contains non-alphanumeric characters, but #1548 contains the added optimization of using Scrapy's urlparse_cached() functionality so it should be preferred.\nPull request #3817 prefers extracting the file extension from the response headers over the URL. It is debatable whether this is the best order of precedence, since it does not preserve existing test case behavior, but checking the response headers is generally a good idea.", "issue_status": "Open", "issue_reporting_time": "2019-12-09T19:36:50Z"}, "26": {"issue_url": "https://github.com/scrapy/scrapy/issues/4222", "issue_id": "#4222", "issue_summary": "SCRAPY_CHECK is not set while running contract", "issue_description": "Lightjohn commented on Dec 8, 2019 \u2022\nedited\nDescription\nHi, it seems that #3739 is not doing what the documentation describe:\nos.environ.get('SCRAPY_CHECK') is returning None in my contract check.\nSteps to Reproduce\nCreate a project from scratch\nAdd a random spider\nContract code is as follow\n    def parse(self, response):\n        \"\"\"\n        @url http://www.amazon.com/s?field-keywords=selfish+gene\n        @returns requests 1 1\n        \"\"\"\n        print(\"test\", os.environ.get('SCRAPY_CHECK'))\n        if os.environ.get('SCRAPY_CHECK'):\n            yield scrapy.Request(url=\"next_url\")\nExpected behavior: Request should be yielded as per the documentation\nActual behavior: Nothing happen\nReproduces how often: In my local project and with fresh project\nVersions\nWindows\n(globenv) C:\\Users\\johnl>scrapy version --verbose\nScrapy       : 1.8.0\nlxml         : 4.4.1.0\nlibxml2      : 2.9.5\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.10.0\nPython       : 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)]\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1c  28 May 2019)\ncryptography : 2.7\nPlatform     : Windows-10-10.0.18362-SP0\nLinux\nscrapy version --verbose\nScrapy       : 1.8.0\nlxml         : 4.4.1.0\nlibxml2      : 2.9.9\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.7.0\nPython       : 3.6.8 (default, Oct  7 2019, 12:59:55) - [GCC 8.3.0]\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019)\ncryptography : 2.8\nPlatform     : Linux-4.4.0-18362-Microsoft-x86_64-with-Ubuntu-18.04-bionic", "issue_status": "Open", "issue_reporting_time": "2019-12-08T14:27:26Z"}, "27": {"issue_url": "https://github.com/scrapy/scrapy/issues/4217", "issue_id": "#4217", "issue_summary": "DOWNLOAD_FAIL_ON_DATALOSS logged for all requests", "issue_description": "Member\nGallaecio commented on Dec 6, 2019 \u2022\nedited\n[scrapy.core.downloader.handlers.http11] Got data loss in . If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests\nThis message is logged every time you get data loss on a request.\nI\u2019ve had a quick look at the code, and it figures, since the flag to avoid repeating the message is stored on a _ResponseReader instance, and for each response a new _ResponseReader instance is created.\nRelates to #2590 (original implementation).\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-12-05T19:38:32Z"}, "28": {"issue_url": "https://github.com/scrapy/scrapy/issues/4216", "issue_id": "#4216", "issue_summary": "Cover arbitrary filtering in the Scrapy logging documentation", "issue_description": "Member\nGallaecio commented on Dec 5, 2019\nIt should be clear, from reading the documentation, how to filter out a specific log message that we wish to ignore.\nThis is specially important for warnings that depend on input, like the one introduced in #4214. Since you seldom have the power to fix the issue that triggers the warning message, caused by the content or behavior of the website you are scraping, you may need to simply ignore those warning messages.\nExposing a setting or a LogFormatter method for each of those warnings does not seem scalable to me, specially when such warnings can come from third-party Scrapy extensions.", "issue_status": "Open", "issue_reporting_time": "2019-12-05T13:38:14Z"}, "29": {"issue_url": "https://github.com/scrapy/scrapy/issues/4211", "issue_id": "#4211", "issue_summary": "SitemapSpider throws lxml.etree.XMLSyntaxError when trying to scrape a blank sitemap page", "issue_description": "Endi1 commented on Dec 4, 2019 \u2022\nedited\nDescription\nSitemapSpider throws a lxml.etree.XMLSyntaxError when hitting a blank sitemap page while crawling a sitemap.\nExample sitemap with blank pages: https://bikeradar.com/sitemap.xml\nStack trace\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"/usr/local/lib/python3.8/site-packages/sh_scrapy/middlewares.py\", line 30, in process_spider_output\n    for x in result:\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/spiders/sitemap.py\", line 53, in _parse_sitemap\n    s = Sitemap(body)\n  File \"/usr/local/lib/python3.8/site-packages/scrapy/utils/sitemap.py\", line 18, in __init__\n    self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n  File \"src/lxml/etree.pyx\", line 3234, in lxml.etree.fromstring\n  File \"src/lxml/parser.pxi\", line 1876, in lxml.etree._parseMemoryDocument\n  File \"src/lxml/parser.pxi\", line 1764, in lxml.etree._parseDoc\n  File \"src/lxml/parser.pxi\", line 1127, in lxml.etree._BaseParser._parseDoc\n  File \"src/lxml/parser.pxi\", line 601, in lxml.etree._ParserContext._handleParseResultDoc\n  File \"src/lxml/parser.pxi\", line 711, in lxml.etree._handleParseResult\n  File \"src/lxml/parser.pxi\", line 640, in lxml.etree._raiseParseError\n  File \"<string>\", line 1\nlxml.etree.XMLSyntaxError: Document is empty, line 1, column 1\nSteps to Reproduce\nCrawl sitemap with at least a blank .xml page\nExpected behavior:\nHandle the XMLSyntaxError by logging a warning for an invalid sitemap page and returning.\nActual behavior:\nThrows XMLSyntaxError\nVersions\nScrapy version 1.7.2", "issue_status": "Open", "issue_reporting_time": "2019-12-04T15:11:36Z"}, "30": {"issue_url": "https://github.com/scrapy/scrapy/issues/4206", "issue_id": "#4206", "issue_summary": "Cover https://michael-shub.github.io/curl2scrapy/ in the documentation", "issue_description": "Member\nGallaecio commented on Dec 3, 2019\nSee #3991 (comment)\nIn the parts of the documentation where we currently cover Request.from_curl we may want to mention this online tool as well.", "issue_status": "Open", "issue_reporting_time": "2019-12-03T15:25:42Z"}, "31": {"issue_url": "https://github.com/scrapy/scrapy/issues/4196", "issue_id": "#4196", "issue_summary": "Update spider settings during runtime", "issue_description": "VMRuiz commented on Nov 27, 2019\nSummary\nIt should be possible to update some of the jobs settings while they are running. This would be specially useful for the settings related to crawling speed.\nMotivation\nI have experienced several cases where the crawl speed was too slow because CONCURRENT_REQUEST was set too low initially, or for other external reasons like changes on the website availability or in proxies response times. When this happens after more than 200 hours of runtime, restarting the job in order to change the settings is not always an option.\nFor this reason, it should be possible to update settings dynamically while the spider is still running.\nDescribe alternatives you've considered\nA current approach I have found so far, is to move the functionally you need to update into your own custom middlewares or extensions. There, the initial configuration is based on the spider settings and during the execution, it can be updated to new values based on:\nThe spider status itself\nThe responses from the website\nExternal updates from the Telnet shell.\nA better approach would be to let Scrapy Core itself handle this updating process in some way, without having to duplicate this functionally in custom middlewares tailored for every setting you need to change.\nOne way to do that would be having signals that indicates certain setting must be updated. I see 2 different approaches here:\nUpdate any spider settings and then triggering a reload all settings signal that would make the crawler engine to reload every single setting where required.\nTrigger an update {setting_name} value signal, that would make the crawler engine to reload only that settings in the part of the code where it's needed.\nIn both cases, it should be possible to trigger this signals from both the spider code itself (parse methods, middlewares, extensions) and from the telnet interface or any other interface that allow access to the job internal objects.\nAdditional context\nIs it possible that for some settings it would not be possible or it wouldn't make sense.\nWhen several spiders are running under a single crawler engine, updating the settings in one spider may have side-effects on others. Implementation needs to be aware of this to prevent introducing bugs.\n\ud83d\ude80 2", "issue_status": "Open", "issue_reporting_time": "2019-11-27T11:00:27Z"}, "32": {"issue_url": "https://github.com/scrapy/scrapy/issues/4182", "issue_id": "#4182", "issue_summary": "Spider finshed normally despite an error in start_requests", "issue_description": "Member\nlopuhin commented on Nov 20, 2019\nThis is a usability issue, although I'm not sure it's a good one. This is based on a real case we discovered with @whalebot-helmsman .\nConsider a spider which crawls a large list of URLs, and in it's start_requests method it will loop through urls and yield scrapy.Request objects. If one URL is invalid, it will fail to create request object, and we'll get ERROR: Error while obtaining start requests caused by ValueError: Missing scheme in request url:, and the rest of the URLs won't be scheduled, because start_requests crashed. So far this is ok, but the problem is that this issue is not trivial to diagnose, because spider finish reason would be 'finish_reason': 'finished',, and this particular error in the log won't be the last one very often, because requests already scheduled would be still executed. So we will not schedule all URLs after the bad one, but it would be hard to find out.\nI'm not sure it's a good solution, but maybe if errors in start_requests would cause a different finish reason or some message at the end of the log or in stats, it would be easier to debug such an issue.\nOn the other hand, it's possible that a similar error could happen in the parse method - and we won't be able to do anything with it.\nAnother potential solution is to delay URL validation, so that one faulty URL does not cause other URLs to not be scheduled (while still giving an error in the log) - but this probably has it's own drawbacks.", "issue_status": "Open", "issue_reporting_time": "2019-11-20T08:49:35Z"}, "33": {"issue_url": "https://github.com/scrapy/scrapy/issues/4180", "issue_id": "#4180", "issue_summary": "Provide built-in SFTP support, discourage FTP", "issue_description": "Member\nGallaecio commented on Nov 20, 2019 \u2022\nedited\nScrapy provides built-in support for exporting data through FTP. FTP is considered insecure.\nWe should consider:\nAdding built-in support for SFTP; see https://github.com/scrapy-plugins/scrapy-feedexporter-sftp\nDiscourage FTP usage (through documentation and a warning?)\nDeprecate FTP support? If we decide to eventually remove FTP support, that should include stop ignoring bandit\u2019s 402 check.", "issue_status": "Open", "issue_reporting_time": "2019-11-20T07:57:48Z"}, "34": {"issue_url": "https://github.com/scrapy/scrapy/issues/4175", "issue_id": "#4175", "issue_summary": "Scrapy does not use a non-zero exit code when pipeline's open_spider throws the exception", "issue_description": "gunblues commented on Nov 19, 2019 \u2022\nedited\nDescription\nIn our case, we execute command scrapy crawl in airflow task and the exit code would be used to judge this task success or failure. I agree that scrapy crawl ignores spider exceptions because it's unpredictable in the crawling process.\nBack to our case, we export data to file or database in the pipeline and we create the directory or database connection in open_spider(self, spider). I think if there is an exception happens during this function, it's reasonable to propagate a non-zero exit code. it because we normally do some initialization in this function.\nSteps to Reproduce\nscrapy startproject test_spider\ncd test_spider\nscrapy genspider example example.com\nmodify spiders/example.py to\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = 'example'\n    allowed_domains = ['example.com']\n    start_urls = ['http://example.com/']\n\n    custom_settings = {\n        'ITEM_PIPELINES': {\n            'test_spider.pipelines.TestSpiderPipeline': 300\n        }\n    }\n\n    def parse(self, response):\n        pass\nmodify pipelines.py to\n# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n\n\nclass TestSpiderPipeline(object):\n\n    def open_spider(self, spider):\n        raise Exception('error')\n\n    def process_item(self, item, spider):\n        return item\nscrapy crawl example\necho $?\nExpected behavior: [What you expect to happen]\nreturn non-zero exit code\nActual behavior: [What actually happens]\nreturn zero exit code\nReproduces how often: [What percentage of the time does it reproduce?]\n100%\nVersions\nScrapy : 1.8.0\nlxml : 4.3.3.0\nlibxml2 : 2.9.9\ncssselect : 1.0.3\nparsel : 1.5.1\nw3lib : 1.20.0\nTwisted : 19.2.0\nPython : 3.7.3 (default, Mar 27 2019, 09:23:39) - [Clang 10.0.0 (clang-1000.11.45.5)]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1b 26 Feb 2019)\ncryptography : 2.6.1\nPlatform : Darwin-18.5.0-x86_64-i386-64bit\nAdditional context\nI could get the expected behavior if I change def run(self, args, opts) in scrapy/commands/crawl.py to\n    def run(self, args, opts):\n        if len(args) < 1:\n            raise UsageError()\n        elif len(args) > 1:\n            raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n        spname = args[0]\n\n        res = self.crawler_process.crawl(spname, **opts.spargs)\n\n        if hasattr(res, 'result') and res.result is not None and issubclass(res.result.type, Exception):\n            self.exitcode = 1\n        else:\n            self.crawler_process.start()\n\n            if self.crawler_process.bootstrap_failed:\n                self.exitcode = 1\noriginal def run(self, args, opts)\n    def run(self, args, opts):\n        if len(args) < 1:\n            raise UsageError()\n        elif len(args) > 1:\n            raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n        spname = args[0]\n\n        self.crawler_process.crawl(spname, **opts.spargs)\n        self.crawler_process.start()\n\n        if self.crawler_process.bootstrap_failed:\n            self.exitcode = 1\nIs it the proper way to modify the code for achieving this purpose? if it is, could I create a PR request for this issue?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-11-19T08:10:32Z"}, "35": {"issue_url": "https://github.com/scrapy/scrapy/issues/4156", "issue_id": "#4156", "issue_summary": "Importance of Requests", "issue_description": "oscarrobertson commented on Nov 13, 2019 \u2022\nedited\nSummary\nIt would be nice to have the concept of request importance to affect the logging level of download failures, i.e. a way to yield a request but say, if this request has a download failure then only log a warning. A feature to keep track of how many requests of low importance have failed so far would also be useful, as there could be a threshold on the number of failures where error logging starts to kick back in.\nMotivation\nWe use scrapy extensively and we track log files to help us monitor scraping processes.\nImagine a scrape that yields one request (R1) to start, then in the first callback it yields 10000 requests (R2s). From a monitoring perspective if R1 fails that is a huge problem, but if one of the R2s fails I don't really care. If lots of the R2s fail though that is a big deal.\nIf I see an error in the log files for code I did not write, it's not immediately clear to me if the failure is an R1 or R2 request, I have to go and read the code to find out.\nAdditional context\nIt looks like all we need to do is override the Scraper class slightly, and maybe write new Request types. It's kind of hard to plug in a custom Scraper class currently, this line could be changed to pull a class from settings like the settings above it though\nscrapy/scrapy/core/engine.py\nLine 70 in c911e80\n self.scraper = Scraper(crawler) \n\nI thought I would reach out to see if this behavior is something that might be useful to others as part of the default Scraper, we're happy to contribute a PR if so.", "issue_status": "Open", "issue_reporting_time": "2019-11-13T09:25:15Z"}, "36": {"issue_url": "https://github.com/scrapy/scrapy/issues/4135", "issue_id": "#4135", "issue_summary": "Switch pickle to protocol 4", "issue_description": "Member\nGallaecio commented on Nov 6, 2019\nSince we no longer support Python 2 or Python < 3.4, it looks like protocol 4 of pickle is our best choice.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-11-06T14:03:00Z"}, "37": {"issue_url": "https://github.com/scrapy/scrapy/issues/4125", "issue_id": "#4125", "issue_summary": "Expose _find_method and _get_method from scrapy.utils.reqser", "issue_description": "Member\nGallaecio commented on Nov 5, 2019\nThere are two functions in scrapy.utils.reqser, _find_method and _get_method, that are used to serialize and unserialize spider callbacks, useful e.g. when using JOBDIR.\nIt would be nice if these functions were given a non-private name, and were covered in the documentation about JOBDIR as functions that can be used to serialize and unserialize store spider callbacks in the request meta.\nI found them handy on a spider that needs to temporarily change the callback of a request, and call the original callback from the new callback.\nRelated to #4124.", "issue_status": "Open", "issue_reporting_time": "2019-11-04T20:25:49Z"}, "38": {"issue_url": "https://github.com/scrapy/scrapy/issues/4117", "issue_id": "#4117", "issue_summary": "MemoryError: Cannot allocate write+execute memory for ffi.callback()", "issue_description": "askaliuk commented on Nov 1, 2019 \u2022\nedited\nDescription\nI have simple Scrapy script which fails on Ubuntu 18 with weird memory error.\nWorks fine on local Mac, but fails on remote host.\nLooks like a openSSL issue. Any advice is appreciated.\nSteps to Reproduce\nSimply run scrapy script\nExpected behavior:\nRun normally\nActual behavior:\n2019-10-31 20:24:51 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://xxx.yyy/robots.txt>: Cannot allocate write+execute memory for ffi.callback(). You might be running on a system that prevents this. For more information, see https://cffi.readthedocs.io/en/latest/using.html#callbacks\nTraceback (most recent call last):\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1416, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/twisted/python/failure.py\", line 512, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\n    result = f(*args, **kw)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 71, in download_request\n    return handler.download_request(request, spider)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py\", line 68, in download_request\n    return agent.download_request(request)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py\", line 332, in download_request\n    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/twisted/web/client.py\", line 1732, in request\n    endpoint = self._getEndpoint(parsedURI)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/twisted/web/client.py\", line 1715, in _getEndpoint\n    return self._endpointFactory.endpointForURI(uri)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/twisted/web/client.py\", line 1590, in endpointForURI\n    uri.port)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/scrapy/core/downloader/contextfactory.py\", line 59, in creatorForNetloc\n    return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext())\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/scrapy/core/downloader/contextfactory.py\", line 56, in getContext\n    return self.getCertificateOptions().getContext()\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1678, in getContext\n    self._context = self._makeContext()\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1709, in _makeContext\n    ctx.set_verify(verifyFlags, _verifyCallback)\n  File \"/home/scrapy/env/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1103, in set_verify\n    self._verify_helper = _VerifyHelper(callback)\nReproduces how often:\n100%\nVersions\n$ scrapy version --verbose\nScrapy       : 1.6.0\nlxml         : 4.4.1.0\nlibxml2      : 2.9.9\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.7.0\nPython       : 2.7.15+ (default, Oct  7 2019, 17:39:04) - [GCC 7.4.0]\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019)\ncryptography : 2.8\nPlatform     : Linux-4.14.117-grsec-grsec+-x86_64-with-Ubuntu-18.04-bionic\nAdditional context\n$ cat /etc/os-release\nNAME=\"Ubuntu\"\nVERSION=\"18.04.2 LTS (Bionic Beaver)\"\nPRETTY_NAME=\"Ubuntu 18.04.2 LTS\"\nVERSION_ID=\"18.04\"", "issue_status": "Open", "issue_reporting_time": "2019-11-01T03:37:52Z"}, "39": {"issue_url": "https://github.com/scrapy/scrapy/issues/4110", "issue_id": "#4110", "issue_summary": "LogStats Extension should log the IPM and RPM to the stats on spider_closed signal", "issue_description": "Contributor\nBurnzZ commented on Oct 30, 2019 \u2022\nedited\nSummary\nBy default, the scrapy.extensions.logstats.LogStats log the details below to the standard logger every 60 seconds:\nIPM (Items per Minute)\nItems scraped\nRPM (Requests per Minute)\nRequests produced\nMotivation\nWe often find the need to retrieve the RPM and IPM after a job is finished. We can calculate this manually or we can check the logs for the last update of LogStats. However, this is tedious and inefficient as LogStats already has this data but isn't putting them somewhere that is readily available after the job is finished.\nDescribe alternatives you've considered\nWe can pull out the last update of LogStats from the logs but pulling it from the API and filtering it out is expensive.\nAdditional context\nNone", "issue_status": "Open", "issue_reporting_time": "2019-10-30T07:37:15Z"}, "40": {"issue_url": "https://github.com/scrapy/scrapy/issues/4106", "issue_id": "#4106", "issue_summary": "Persistence is not fully persistent when using JOBDIR setting", "issue_description": "JSPromisel commented on Oct 26, 2019 \u2022\nedited by elacuesta\nDescription\nI have been experimenting with keeping a persistent state for a spider by using the JOBDIR setting, so if it crashes or I stop it, I can start it up again from the same point it left off. I notice that the seen request fingerprints are persisted, as well as an active.json file holding information about the queue. However, in my testing, I find that the existence of the queue is inconsistent and when I restart a spider, the queue has not persisted, it just starts from the beginning and filters out all the seen requests.\nSteps to Reproduce\nCreate a spider with settings:\ncustom_settings = {\n    'JOBDIR': f'/tmp/requests',\n    'LOG_LEVEL': 'DEBUG\n}\nUse the following start_requests:\ndef start_requests(self):\n    # Get a sampling of different links\n    for key in range(40):\n        url = f'https://httpbin.org/links/{key}/0'\n        cb_kwargs = {'key': key}\n        yield scrapy.Request(url, cb_kwargs=cb_kwargs)\nand parse functions:\ndef parse(self, response, key):\n    self.logger.info(f'Requested {response.url}')\n\n    # Close spider w/o finishing at 4, retry should\n    # pick up at the next request\n    if key == 4:\n        raise CloseSpider('cancelled')\nRun the spider, then run it again after it closes.\nExpected behavior: [What you expect to happen]\nFor true persistency, I would expect the spider to pick up the queue and begin making requests that hadn't already been made when the spider closed, then continue until the spider is finished.\nActual behavior: [What actually happens]\nThe output of the spider on the second run is shown below:\n2019-10-25 16:24:01 [scrapy.core.engine] INFO: Spider opened\n2019-10-25 16:24:01 [scrapy.core.scheduler] INFO: Resuming crawl (1 requests scheduled)\n2019-10-25 16:24:01 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://httpbin.org/links/0/0> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/22/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/23/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/30/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/24/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/25/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/31/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/26/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/27/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/32/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/33/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/28/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/34/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/35/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/29/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/36/0> (referer: None)\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/37/0> (referer: None)\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/22/0\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/38/0> (referer: None)\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/23/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/30/0\n2019-10-25 16:24:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://httpbin.org/links/39/0> (referer: None)\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/24/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/25/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/31/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/26/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/27/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/32/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/33/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/28/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/34/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/35/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/29/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/36/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/37/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/38/0\n2019-10-25 16:24:02 [root] INFO: Requested https://httpbin.org/links/39/0\n2019-10-25 16:24:02 [scrapy.core.engine] INFO: Closing spider (finished)\n2019-10-25 16:24:02 [scrapy.core.engine] INFO: Spider closed (finished)\nAs the debug logging shows, the spider picks up the queue from the disk and continues the crawl:\n[scrapy.core.scheduler] INFO: Resuming crawl (1 requests scheduled)\nHowever, after that, it redoes the first request and filters it:\n[scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://httpbin.org/links/0/0> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\nI understand that keeping the entire queue for a spider is probably not feasible, I just want to make sure I understand this correctly, as the documentation claims:\nSometimes, for big sites, it\u2019s desirable to pause crawls and be able to resume them later.\n\nScrapy supports this functionality out of the box by providing the following facilities:\n\na scheduler that persists scheduled requests on disk\na duplicates filter that persists visited requests on disk\nan extension that keeps some spider state (key/value pairs) persistent between batches\nI feel I am either misunderstanding the persistence functionality or the persistence is not complete.\nAny feedback is greatly appreciated.\nReproduces how often: [What percentage of the time does it reproduce?]\nEverytime\nVersions\nScrapy       : 1.7.3\nlxml         : 4.3.3.0\nlibxml2      : 2.9.9\ncssselect    : 1.0.3\nparsel       : 1.5.1\nw3lib        : 1.20.0\nTwisted      : 19.2.0\nPython       : 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 16:52:21) - [Clang 6.0 (clang-600.0.57)]\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1c  28 May 2019)\ncryptography : 2.7\nPlatform     : Darwin-19.0.0-x86_64-i386-64bit", "issue_status": "Open", "issue_reporting_time": "2019-10-25T20:36:14Z"}, "41": {"issue_url": "https://github.com/scrapy/scrapy/issues/4073", "issue_id": "#4073", "issue_summary": "Middleware - unused arguements", "issue_description": "mohmad-null commented on Oct 11, 2019\nLets say I have a simple piece of middleware that discards urls that have a bad extension:\nclass IgnoreTheseLinkTypes(object):\n @staticmethod\n def process_request(request, spider):\n\n  if not acceptable_extension(request):\n   raise exceptions.IgnoreRequest\nThe middleware works fine, but my IDE is correctly complaining at inspection time that I have an unused argument (spider).\nSo I change spider to be _ to represent the fact it is unused per Python best-practice. But now the middleware never gets invoked.\nIt would be nice if Scrapy could handle this scenario so I could have cleaner code.", "issue_status": "Open", "issue_reporting_time": "2019-10-11T14:53:13Z"}, "42": {"issue_url": "https://github.com/scrapy/scrapy/issues/4070", "issue_id": "#4070", "issue_summary": "MEMUSAGE_ENABLED - default", "issue_description": "mohmad-null commented on Oct 11, 2019\nThe default for this is True, except the extension doesn't work on Windows apparently. I would suggest the default should thus be False.\nAnd/or there should be a comment in the logs about how the extension doesn't work on windows if it is enabled on said OS.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-10-11T08:57:10Z"}, "43": {"issue_url": "https://github.com/scrapy/scrapy/issues/4043", "issue_id": "#4043", "issue_summary": "Scrapy download middelware should pause on Connection loss and Resume when internet connection is available.", "issue_description": "royahsan commented on Sep 29, 2019 \u2022\nedited\nSummary\nWe should make Scrapy downlaoder middleware pause when Internet connection is lost, and wait until it is back to resume the downloader middleware.\nMotivation\nCurrently, on a connection lost, retry middleware keeps trying to download the Request MAX_RETRIES time and then throws ConnectError.\nWe can provide this feature out of the box, though keep in mind the same error does occur with a working internet connection if host website intentionally refuses the connection.\nIf I intentionally close the internet connection while a request is being processed this is the log generated.\nDEBUG: scrapy.downloadermiddlewares.retry:  Gave up retrying <GET https://mblsportal.sos.state.mn.us//Business/SearchDetails?filingGuid=6f38440e-98d4-e011-a886-001ec94ffe7f> (failed 3 times): An error occurred while connecting: 10065: A socket operation was attempted to an unreachable host..\nTraceback (most recent call last):\n  File \"C:\\Users\\royah\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 44, in process_request\n    defer.returnValue((yield download_func(request=request, spider=spider)))\ntwisted.internet.error.ConnectError: An error occurred while connecting: 10065: A socket operation was attempted to an unreachable host..\nAlternatives I have considered\nAdding a custom downloader middleware to check the internet connection is possible but quite hefty addition considering connection drops are somewhat rare and it will check internet connection (a hefty function itself) on each exception.\nA working such middleware is below.\nimport logging\nimport time\nimport requests\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nRETRY_DELAY = 30\n\n\ndef internet_access():\n    while True:\n        try:\n            requests.head(\"https://www.example.com/\")\n            return\n        except Exception as exc:\n            del exc\n            logger.warning(\"Internet connection Lost. . .\")\n            time.sleep(RETRY_DELAY)\n\nclass InternetConnectionRobustDownloaderMiddleware(object):\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls()\n\n    def process_exception(self, request, exception, spider):\n        logger.error(exception)\n        internet_access()\n        return None", "issue_status": "Open", "issue_reporting_time": "2019-09-28T23:35:14Z"}, "44": {"issue_url": "https://github.com/scrapy/scrapy/issues/4041", "issue_id": "#4041", "issue_summary": "Adding Type Hints to Scrapy and its Modules", "issue_description": "royahsan commented on Sep 27, 2019 \u2022\nedited\nSummary\nWe should add Variable Annotations/ Type hints as supported in PEP 526 , Python 3.6 to Scrapy to help out existing and new contributors and developers in understanding scrapy code.\nMotivation\nIntellisense enabled IDES like PyCharm need Type hints to provide better experience.\nFor new contributors to understand Scrapy comprehensively, type hints are vital.\nConsider someone not that familiar with scrapy, stumbling upon scheduler's constructor.\n    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None,\n                 logunser=False, stats=None, pqclass=None, crawler=None):\n        self.df = dupefilter\n        self.dqdir = self._dqdir(jobdir)\n        self.pqclass = pqclass\n        self.dqclass = dqclass\n        self.mqclass = mqclass\n        self.logunser = logunser\n        self.stats = stats\n        self.crawler = crawler", "issue_status": "Open", "issue_reporting_time": "2019-09-27T09:56:43Z"}, "45": {"issue_url": "https://github.com/scrapy/scrapy/issues/4029", "issue_id": "#4029", "issue_summary": "httpproxy remove creds of proxy after processing request", "issue_description": "Contributor\nNewUserHa commented on Sep 20, 2019\nit's hard to add different creds back to different proxy URL for reusing in next Request.\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/downloadermiddlewares/httpproxy.py#L53\nthis line should be removed.", "issue_status": "Open", "issue_reporting_time": "2019-09-20T02:49:13Z"}, "46": {"issue_url": "https://github.com/scrapy/scrapy/issues/4027", "issue_id": "#4027", "issue_summary": "Upgrading from 1.4 to 1.5 results in many 400 responses", "issue_description": "Member\nGallaecio commented on Sep 19, 2019 \u2022\nedited\n@stav has a project where upgrading from Scrapy 1.4 to Scrapy 1.5 resulted in many 400 responses, and going back to Scrapy 1.4 solved the issue.\nSomeone else (Alejandro Marti) saw the same in a different project, so it seems confirmed. Now we need to figure out why.\n@kmike mentioned it may be related to #2743 and #2767.", "issue_status": "Open", "issue_reporting_time": "2019-09-19T06:30:27Z"}, "47": {"issue_url": "https://github.com/scrapy/scrapy/issues/4009", "issue_id": "#4009", "issue_summary": "Allow some redirects and deny some others", "issue_description": "casertap commented on Sep 12, 2019 \u2022\nedited by elacuesta\nSummary\nLets take the example of a e-commerce where all product's urls contain /product/\nSome website redirect you to a collection page when a product is not available, the url will contain: /collection/\nWe would be able to stop redirecting (and downloading the page) to the collection page because we will not find our product anyway but we would like to follow some \"legit\" redirect to /product/ because the product may have moved to another location (seo, website change in the url, ...)\nMotivation\ndont_redirect: True or handle_httpstatus_list are too broad. Sometime you would like to say:\nallow_redirect_locations: (\".*/product/.*\", )\nthe conditional could be tested at this location in the source code:\nscrapy/scrapy/downloadermiddlewares/redirect.py\nLine 70 in 534de73\n if 'Location' not in response.headers or response.status not in allowed_status: \nDescribe alternatives you've considered\nI can not get around it. I have to follow the redirect which is costly.\nMost of the time the redirects are all because of availability but once is a while I will get all redirects pointing to another \"legit\" /product/ location. At that point if I used the dont_follow setting I will think all the products are unavailable.", "issue_status": "Open", "issue_reporting_time": "2019-09-12T06:12:59Z"}, "48": {"issue_url": "https://github.com/scrapy/scrapy/issues/4005", "issue_id": "#4005", "issue_summary": "move item loaders to a separate library", "issue_description": "Member\nkmike commented on Sep 8, 2019 \u2022\nedited\nSummary\nMove item loaders to a separate repository, similar to parsel.\nMotivation\nThere are two main reasons for doing so:\nIf item loaders are decoupled from Scrapy, they can be used without Scrapy. They would still support CSS/XPath selectors, but use parsel to populate dicts. For example, this would allow to use item loaders in PageObjects from https://github.com/scrapinghub/scrapy-po, and allow reusing them without Scrapy.\nIt would allow to develop item loaders separately, make it easier to have a different set of maintainers, allow a separate release cycle. Currently they are under-maintained in Scrapy.\nOn a first sight, most of the item loaders code is not tied to Scrapy already.\nPrevious attempts\nWe tried to move itemloaders in past, by copying the current implementation. Decision was not to go with this, as Scrapy will have to depend on scrapy-itemloaders, and scrapy-itemloaders will have to depend on Scrapy. In this proposal Scrapy depends on a new package, but package can be used without Scrapy - it depends on parsel only.\nHow to do it\nre-create https://github.com/scrapy/scrapy-itemloader repo - import current code (with git history). Or maybe use a different name for a repository - just \"itemloaders\"? We can rename the repo.\nremove Scrapy-specific parts:\n\"response\" argument, response object in loader context\nscrapy Item support. It can be re-added for attr.s or dataclass-based items, when Scrapy gets support for them.\nimplement scrapy.loaders on top of a new library - inherit, add response and item support.\nProvide backwards compatibility shims for scrapy.loader.processors\nOther considerations\nOptional scrapy.Item support in the library itself can be fine.\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2019-09-07T23:17:35Z"}, "49": {"issue_url": "https://github.com/scrapy/scrapy/issues/4002", "issue_id": "#4002", "issue_summary": "Warning messages to add in the CSV exporter when there's any field dropped", "issue_description": "Contributor\nstarrify commented on Sep 5, 2019\nSummary\nA warning message shall be generated when there're some fields dropped within CsvItemExporter (or also in other exporters?) during exporting.\nWe may have this kind of warning messages emitted only for the first occurrence to avoid generating too much noise, similar to the warning messages when there's possible data loss (related code).\nMotivation\nDue to nature of the CSV format, the CsvItemExporter needs to determine the list of fields to write before writing the 1st record (related code), which would in some occasions cause unintentional loss of data.\nUnfortunately I've yet observed this kind of mistakenly introduced data loss issue from different developers, where the common cause may be:\nReturning a dict as the 1st item that doesn't have all fields included.\nReturning more than one type of items that have different fields.\n(both when not having FEED_EXPORT_FIELDS properly configured)\nDescribe alternatives you've considered\nProperly configuring the FEED_EXPORT_FIELDS setting entry would surely have the issue resolved. However that's not always guaranteed (a developer may just forget to, etc.).\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-09-05T16:02:37Z"}, "50": {"issue_url": "https://github.com/scrapy/scrapy/issues/4001", "issue_id": "#4001", "issue_summary": "Rewrite the virtual environment documentation section for Python-3-only Scrapy", "issue_description": "Member\nGallaecio commented on Sep 5, 2019\nI think https://docs.scrapy.org/en/latest/intro/install.html#using-a-virtual-environment-recommended can be made simpler once Scrapy only supports Python 3.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-09-05T06:18:35Z"}, "51": {"issue_url": "https://github.com/scrapy/scrapy/issues/3991", "issue_id": "#3991", "issue_summary": "Create scrapy command to get the equivalent Request from a cURL command", "issue_description": "Contributor\nnoviluni commented on Aug 30, 2019 \u2022\nedited\nHi!\nSince #3862 was accepted I was suggested to create a scrapy command that receive a cURL command and return the equivalent Request __repr__.\nSomething like:\nscrapy requestfromcurl \"curl 'http://example.org/post' -X POST -H 'Cookie: _gauges_unique_year=1; _gauges_unique=1'\"\nwould print:\nRequest(method='POST', url='http://example.org/post', cookies={'_gauges_unique_year': '1', '_gauges_unique': '1'})\nI just did a PR (#3990) to show you the idea, but I would like to ask you:\n1st. Do you like the idea? Is it useful? (I\u2019m not sure about it)\nIn the affirmative case:\n2nd. how would you name that command? (ideas: requestfromcurl, curltorequest...)\n3rd. where would you mention that command in the docs?\nThanks in advance", "issue_status": "Open", "issue_reporting_time": "2019-08-30T08:28:43Z"}, "52": {"issue_url": "https://github.com/scrapy/scrapy/issues/3979", "issue_id": "#3979", "issue_summary": "Have StatsCollector interface pass in an optional Request/Response", "issue_description": "rgruener commented on Aug 28, 2019\nSummary\nStatsCollector defines an interface for collecting stats while your Spider is running. We are using it to publish stats through OpenCensus. However we would like more information about the stats based on the request/response objects in order to better slice/dice our collected stats.\nIt would be extremely helpful to have set_value, inc_value, etc also have a request and an response argument which would optionally be the request/response the value is from. This would allow extraction of headers, url parsing, etc to make useful metric tags.", "issue_status": "Open", "issue_reporting_time": "2019-08-27T20:34:48Z"}, "53": {"issue_url": "https://github.com/scrapy/scrapy/issues/3971", "issue_id": "#3971", "issue_summary": "Establish a policy on the use of Sphinx directives for deprecations and version changes", "issue_description": "Member\nGallaecio commented on Aug 21, 2019 \u2022\nedited\nI think we should make the following changes to make a better use of Sphinx\u2019s deprecated, versionadded, and versionchanged directives:\nChanges should include these tags when appropriate.\nWe need to update the contribution documentation accordingly, and remember to enforce this during code reviews.\nThese tags should be removed in major version changes.\nBecause of this, Scrapy 2.0 is probably the best time to implement this policy.", "issue_status": "Open", "issue_reporting_time": "2019-08-21T15:17:23Z"}, "54": {"issue_url": "https://github.com/scrapy/scrapy/issues/3947", "issue_id": "#3947", "issue_summary": "set/inc stat values when exporter extension exports the data successfully", "issue_description": "HassanQamar07 commented on Aug 8, 2019\nThere are no stat values set when a file is uploaded to FEED_STORAGE or not. If there are some stats values set then it would be helpful to implement scrapinghub/spidermon#196", "issue_status": "Open", "issue_reporting_time": "2019-08-07T18:54:01Z"}, "55": {"issue_url": "https://github.com/scrapy/scrapy/issues/3938", "issue_id": "#3938", "issue_summary": "Incorrect deprecation warning on S3FeedStorage", "issue_description": "hermit-crab commented on Aug 5, 2019\nGood day. Recently upon switching from 1.5 to 1.6 I started getting this warning:\nInitialising `scrapy.extensions.feedexport.S3FeedStorage` without AWS keys is deprecated. Please supply credentials or use the `from_crawler()` constructor.\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/extensions/feedexport.py#L106\nProject does have AWS credentials properly set in settings. This warning happens due to S3FeedStorage being initialized twice:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/extensions/feedexport.py#L209 this check initializes the storage just to see if it's supported. At that point crawler is not yet set on the extension instance so the the storage is instantiated as S3FeedStorage(uri) and not through S3FeedStorage.from_crawler(crawler, uri) which causes this warning to appear.\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/extensions/feedexport.py#L233 at that point the storage is properly initialize via from_crawler and all goes well.\nOne solution is to switch from from_crawler to from_settings (or just use both for backward compatibility) on S3FeedStorage so that it wont require crawler instance, but the fact that any storage class is instantiated twice is kind of not obvious to the developer? As in the class might be written to do some initial set up with potential side effects inside storage.__init__() and not in storage.open(). Is that alright?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-08-05T13:44:42Z"}, "56": {"issue_url": "https://github.com/scrapy/scrapy/issues/3936", "issue_id": "#3936", "issue_summary": "Why is Scrapy constantly increasing memory usage?", "issue_description": "Colafroth commented on Aug 5, 2019\nI have a scraping project used scrapy and it will continuously run, meaning when it's finished, I have a script to make it run again.\nIt takes about 10 hours to finish a round and the memory increases by 100mb and not released after it.\nI used JOBDIR it helps with the speed but no luck for solving the memory issue.\nHere's what I have got for the spider:\nwith open(suburbLinkFileName) as f:\n        data = json.load(f)\n        for link in data:\n            all_suburb_links.append(link['result'])\n    \n    def parse(self, response):\n        for suburb_link in self.all_suburb_links:\n            absolute_next_suburb_link = 'http://www.xxxx.com.au/buy/' + suburb_link + \"?includeSurrounding=false\"\n            yield Request(url=absolute_next_suburb_link, callback=self.parse_suburb)\n\n    def parse_suburb(self, response):\n        properties_urls = response.xpath(\"//*[@class='details-link ']/@href\").extract()\n\n    for property_url in properties_urls:\n        absolute_property_url = self.base_url + property_url\n        yield Request(absolute_property_url, callback=self.parse_property)\n\n        next_page_url = response.xpath('//a[@class=\"rui-button-brand pagination__link-next\"]/@href').extract_first()\n        if next_page_url is None:\n            return None\n\n        absolute_next_page_url = self.base_url + next_page_url\n        yield Request(url=absolute_next_page_url, callback=self.parse_suburb)\n\n    def parse_property(self, response):\n        if not response.xpath('//title'):\n            yield Request(url=response.url, dont_filter=True)\nI couldn't see I got anything leaking the memory.. Took me couple of days already but no luck..", "issue_status": "Open", "issue_reporting_time": "2019-08-04T23:27:02Z"}, "57": {"issue_url": "https://github.com/scrapy/scrapy/issues/3903", "issue_id": "#3903", "issue_summary": "Can I get remote server's ip address via response?", "issue_description": "imfht commented on Jul 25, 2019\nCan I get remote server's ip address via response?\nFor some reason. I'll need get remote site's ip address when parsing response. I looked the document but found nothing.\nAny one know that?\nThanks!", "issue_status": "Open", "issue_reporting_time": "2019-07-25T06:18:54Z"}, "58": {"issue_url": "https://github.com/scrapy/scrapy/issues/3902", "issue_id": "#3902", "issue_summary": "Can't view pdf response in browser", "issue_description": "Contributor\nOmarFarrag commented on Jul 25, 2019\nTo reproduce:\nscrapy shell http://www.ivoa.net/documents/IVOAIdentifiers/20160523/REC-Identifiers-2.0.pdf\nview(response)\nThe result:\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"E:\\Python\\scrapy_dev\\lib\\site-packages\\scrapy-1.7.0-py3.7.egg\\scrapy\\utils\\response.py\", line 60, in open_in_browser\n    ext = response.extension\n  File \"E:\\Python\\scrapy_dev\\lib\\site-packages\\scrapy-1.7.0-py3.7.egg\\scrapy\\http\\response\\__init__.py\", line 142, in extension\n    self.__class__.__name__)\nTypeError: Unsupported response type: Response\nThis issue can be fixed in open_in_browser in scrapy\\utils\\response by either:\nchecking the content type in the headers to be application/pdf, if so then the extension is .pdf and the process continues\nCreating a pdf response class just like HtmlResponse\nBut, there is a comment stating that the implementation is a bit dirty and could be imroved, so I have two approaches to refactor this implementation:\nAdd extension property to response classes and then just use response.extension\nCreate a dictionary with classes as keys and the extension as value\nWaiting for comment, and then will work on it", "issue_status": "Open", "issue_reporting_time": "2019-07-25T00:59:58Z"}, "59": {"issue_url": "https://github.com/scrapy/scrapy/issues/3888", "issue_id": "#3888", "issue_summary": "Exporters should support nested scrapy.Item objects", "issue_description": "mjpieters commented on Jul 19, 2019 \u2022\nedited\nIf you were to build a nested scrapy.Item structure, such as:\nimport scrapy\n\nclass TopLevel(scrapy.Item):\n    foo = scrapy.Field()\n    bars = scrapy.Field()\n\nclass Bar(scrapy.Item):\n    spam = scrapy.Field(serializer=str.upper)\n\nitem = TopLevel(foo=42, bars=[Bar(spam='hello'), Bar(spam='world')])\none might expect the field configuration for Bar to be honoured when exporting. Unfortunately, that's not the case (note the lack of uppercased strings in the <spam> tags below).\nimport sys\nfrom scrapy.exporters import XmlItemExporter\nexporter = XmlItemExporter(sys.stdout, indent=2)\nexporter.start_exporting()\nexporter.export_item(item)\nexporter.finish_exporting()\nproduces\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <foo>42</foo>\n    <bars>\n      <value>\n        <spam>hello</spam>\n      </value>\n      <value>\n        <spam>world</spam>\n      </value>\n    </bars>\n  </item>\n</items>\nI worked around this for this Stack Overflow question by subclassing the exporter and passing along enough information to the serializer to recurse in the serializer itself:\nclass RecursingXmlItemExporter(XmlItemExporter):\n    def _recursive_serialized_fields(self, item):\n        if isinstance(item, scrapy.Item):\n            return dict(self._get_serialized_fields(item, default_value=''))\n        return item\n\n    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', lambda x: x)\n        try:\n            return serializer(value, self._recursive_serialized_fields)\n        except TypeError:\n            return serializer(value)\nThe above hides the private BaseItemExporter._get_serialized_fields() API, but still adaptable to how the different exporter classes call this method to meet their specific needs.\nHowever, this does require every single nested field item to use a custom serializer. Ideally this is handled natively, ideally as fields are traversed over in export_item().", "issue_status": "Open", "issue_reporting_time": "2019-07-19T12:52:40Z"}, "60": {"issue_url": "https://github.com/scrapy/scrapy/issues/3880", "issue_id": "#3880", "issue_summary": "Does scrapy.Requests has streaming method?", "issue_description": "lycanthropes commented on Jul 14, 2019\nI want download many PDF files, and I want to use scrapy to complete this task. But I find scrapy couldn't download them completely, there are always 1KB files in the result. So I think it is necessary to use a method such as requests.streaming in requests module.", "issue_status": "Open", "issue_reporting_time": "2019-07-14T08:44:11Z"}, "61": {"issue_url": "https://github.com/scrapy/scrapy/issues/3874", "issue_id": "#3874", "issue_summary": "DOWNLOAD_MAXSIZE logger level shouldn't be Error", "issue_description": "mohmad-null commented on Jul 13, 2019\nWhen triggered, the DOWNLOAD_MAXSIZE setting raises an ERROR level logging event. I don't think it should be at this level. Error is \"something went wrong\"; I've explicitly told Scrapy I don't want files of this size, so it's not an error to not download them. WARN or even just INFO makes more sense.\nI'm logging Error levels and above to stderr so I can keep track of them easily and it's being flooded by these. In fact, apart from a couple of genuine errors, of the ~100,000 requests my spider has made so far, these are the only Errors that are showing up.\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2019-07-13T08:44:36Z"}, "62": {"issue_url": "https://github.com/scrapy/scrapy/issues/3871", "issue_id": "#3871", "issue_summary": "Deprecate hacky code from get_project_settings()", "issue_description": "Member\nGallaecio commented on Jul 12, 2019\nReported by @nyov:\n@kmike, would you or someone perhaps also find time to correctly deprecate this (or just rip it > out)?:\nscrapy/scrapy/utils/project.py\nLines 70 to 79 in 9c90d95\n # XXX: remove this hack \n pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\") \n if pickled_settings: \n     settings.setdict(pickle.loads(pickled_settings), priority='project') \n    # XXX: deprecate and remove this functionality \n env_overrides = {k[7:]: v for k, v in os.environ.items() if \n                  k.startswith('SCRAPY_')} \n if env_overrides: \n     settings.setdict(env_overrides, priority='project') \n\nOr is it still needed.\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2019-07-12T11:56:35Z"}, "63": {"issue_url": "https://github.com/scrapy/scrapy/issues/3870", "issue_id": "#3870", "issue_summary": "Allow to pass objects in Settings?", "issue_description": "Contributor\nnyov commented on Jul 12, 2019\nSee code example; why can I not reference plain objects into Settings(), but need to let Scrapy handle the import magic?\nWould it make sense to have this? it seems \"unclean\" to do this in the usual settings.py environment, but in a single-script setup it looks less convoluted than to refer scrapy to import from current module?\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\nimport logging\nimport scrapy\n\nlogger = logging.getLogger(__name__)\n\n\nclass Spider(scrapy.Spider):\n\n    name = 'Spidy'\n\n    def start_requests(self):\n        yield scrapy.Request('https://scrapy.org/')\n\n    def parse(self, response):\n        logger.info('Here I fetched %s for you. [%s]' % (response.url, response.status))\n        return {\n            'status': response.status,\n            'url': response.url,\n            'test': 'item',\n        }\n\n\nclass LogPipeline(object):\n\n    def process_item(self, item, spider):\n        logger.warning('HIT ME PLEASE')\n        logger.info('Got hit by:\\n %r' % item)\n        return item\n\n\nif __name__ == \"__main__\":\n    from scrapy.settings import Settings\n    from scrapy.crawler import CrawlerProcess\n\n    settings = Settings(values={\n        'TELNETCONSOLE_ENABLED': False, # necessary evil :(\n        'EXTENSIONS': {\n            'scrapy.extensions.telnet.TelnetConsole': None,\n        },\n        'ITEM_PIPELINES': {\n            #'myproject.pipelines.LogPipeline': 800, # << as resolved by `scrapy.utils.project.get_project_settings()`\n            #'__main__.LogPipeline': 800, # << works, but still resolved and imported by scrapy magic\n\n            LogPipeline: 800, # << WHY CAN'T I DO THIS?\n        },\n    })\n\n    spider = Spider()\n\n    process = CrawlerProcess(settings=settings)\n    process.crawl(spider)\n    process.start()", "issue_status": "Open", "issue_reporting_time": "2019-07-12T10:59:13Z"}, "64": {"issue_url": "https://github.com/scrapy/scrapy/issues/3867", "issue_id": "#3867", "issue_summary": "CONCURRENT_REQUESTS_PER_IP doesn't work properly", "issue_description": "Member\nkmike commented on Jul 11, 2019 \u2022\nedited\nCONCURRENT_REQUESTS_PER_IP doesn't work properly:\nThe first seen IP for a domain is remembered forever. It means that by using CONCURRENT_REQUESTS_PER_IP Scrapy can handle virtual hosts properly (many websites on a single ip), but not DNS load balancing (a single website uses several IP addresses).\nIt doesn't work properly for first few requests - while IP is not resolved, requests use domain name as a download slot. It means e.g. that for first few(?) requests concurrency limits are effectively doubled.\nIt is not implemented for DownloaderAwarePriorityQueue.\nOur options:\ndocument these limitations explicitly;\ndeprecate CONCURRENT_REQUESTS_PER_IP, as it never worked as intended;\nfix these issues (can be tricky, but possible).", "issue_status": "Open", "issue_reporting_time": "2019-07-11T07:33:56Z"}, "65": {"issue_url": "https://github.com/scrapy/scrapy/issues/3855", "issue_id": "#3855", "issue_summary": "Scrapy finding lots of invalid links - could we have functionality to filter them out?", "issue_description": "ddebernardy commented on Jul 5, 2019\nUsing this selector:\nlinks = response.xpath('//a[@href]')\nI picked up a lot of spaces, as well as random schemes like mailto:, tel:, and javascript:, which makes me presume there might be a better function. If Link extractors merely are wrappers around the above though, it might be worth adding some kind of option to sanitize link urls. (Perhaps that functionality is buried somewhere in the source code or in a section of the docs I skimmed over?)\nAnyway, more importantly than the above, I found the following nuggets in my data's href fields, and methinks Scrapy should definitely provide some kind of built-in filtering mechanism for those:\nImproperly closed tag (href is missing its closing bracket; likely an lxml bug):\n\nhttps://so.dajie.com/job/search?positionFunction=120610&positionName=\u89c6\u9891\u7b97\u6cd5>\n                                        \u89c6\u9891\u7b97\u6cd5\n                                    </a>\n                                </li>\n                                <li>\n                                    <a id=\n\n---\nSame issue with a pretty quote:\n\nhttps://asg.to/contentsPage.html?mcd=dGQRAGplZCkLCfbw\u201d>\u30b5\u30f3\u30d7\u30eb\u52d5\u753b\u306f\u30b3\u30c1\u30e9</a><br />\n</dd>\n <dt>2013.7.16</dt>\n <dd><h3>\u30d7\u30ec\u30df\u30a2\u30e0\u4f1a\u54e1\u9650\u5b9a\u30aa\u30ea\u30b8\u30ca\u30eb\u52d5\u753b\u65b0\u4f5c\u914d\u4fe1\uff01</h3>\n \u30a2\u30b2\u30b5\u30b2\u306e\u30d7\u30ec\u30df\u30a2\u30e0\u30e1\u30f3\u30d0\u30fc\u306b\u5411\u3051\u3066\u5b8c\u5168\u30aa\u30ea\u30b8\u30ca\u30eb\u52d5\u753b\u304c\u66f4\u65b0\u3055\u308c\u307e\u3057\u305f\uff01<br />\u4eca\u56de\u306f\u30d5\u30ea\u30fc\u30bf\u30fc\u306e\u308a\u304a\u3061\u3083\u3093\uff01\u30b9\u30ec\u30f3\u30c0\u30fc\u306a\u611f\u3058\u3084\u3059\u3044\u4f53\u304c\u305d\u305d\u308a\u307e\u3059\uff01\n <a href=\n\n---\nPHP error:\n\n<br />\n<b>Warning</b>:  Use of undefined constant url - assumed 'url' (this will throw an Error in a future version of PHP) in <b>/home/forge/jaidefinichon.com/public/wp-content/themes/jaidefinichon/header.php</b> on line <b>116</b><br />\nhttps://jaidefinichon.com\nAlso, it've been finding urls with carriage returns, line feeds, tabs, and spaces at random location. Sometimes they'd be valid had they been url encoded. Other times, it's basically a miracle that browsers accept the links as is. I've seen the latter characters before and after each of ., ?, =, &, and #.\nIt might be an upstream problem in urlparse(). For instance:\n>>> url = '''https://www.booking.com/country\n... .html?label=gen173nr-1FCAEoggI46AdIM1gEaGeIAQGYATG4AQfIAQ3YAQHoAQH4AQKIAgGoAgO4Av2s9OgFwAIB;sid=514ad6c12110c2103c6a2618a429af6d'''\n>>> url\n'https://www.booking.com/country\\n.html?label=gen173nr-1FCAEoggI46AdIM1gEaGeIAQGYATG4AQfIAQ3YAQHoAQH4AQKIAgGoAgO4Av2s9OgFwAIB;sid=514ad6c12110c2103c6a2618a429af6d'\n>>> from urllib.parse import urlparse\n>>> urlparse(url)\nParseResult(scheme='https', netloc='www.booking.com', path='/country\\n.html', params='', query='label=gen173nr-1FCAEoggI46AdIM1gEaGeIAQGYATG4AQfIAQ3YAQHoAQH4AQKIAgGoAgO4Av2s9OgFwAIB;sid=514ad6c12110c2103c6a2618a429af6d', fragment='')", "issue_status": "Open", "issue_reporting_time": "2019-07-05T15:46:28Z"}, "66": {"issue_url": "https://github.com/scrapy/scrapy/issues/3854", "issue_id": "#3854", "issue_summary": "strip trailing backslashes from domain when redirecting to avoid invalid domains", "issue_description": "ddebernardy commented on Jul 4, 2019 \u2022\nedited\nTo reproduce:\n$ scrapy shell 'http://mlaib.com'\n[...]\n2019-07-04 12:04:30 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.belgoal.com\\> from <GET http://mlaib.com>\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 10, in <module>\n    sys.exit(execute())\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 150, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\n    func(*a, **kw)\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 157, in _run_command\n    cmd.run(args, opts)\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 74, in run\n    shell.start(url=url, redirect=not opts.no_redirect)\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/shell.py\", line 48, in start\n    self.fetch(url, spider, redirect=redirect)\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/shell.py\", line 115, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/failure.py\", line 488, in raiseException\n    raise self.value.with_traceback(self.tb)\nValueError: invalid hostname: www.belgoal.com\\\n\n$ curl -I http://mlaib.com\nHTTP/1.1 301 Moved Permanently\n[...]\nLocation: http://www.belgoal.com\\\n[...]\nArguably server related, and I presume I can fix it in a downloader middleware, but it should probably get handled by Scrapy.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-07-04T10:06:48Z"}, "67": {"issue_url": "https://github.com/scrapy/scrapy/issues/3853", "issue_id": "#3853", "issue_summary": "Improve exception logging while using telnet", "issue_description": "ddebernardy commented on Jul 4, 2019\nI had two scary looking Unhandled Error messages in my logs (see below), which after investigation seem to be related to stuff I did while using telnet to check on my crawler.\nThe first stack trace is likely due to a failed login attempt. The second is likely due to me logging out of telnet (I can't remember if I used exit() or ^C or something else).\nI'll know to simply ignore this unhandled error in the future when I've been playing with telnet. Still, Scrapy could try to be a tiny bit more helpful for new users in this specific case.\n(Or you could ignore this entirely, other new users will google 'unhandled error scrapy' and find this.)\nUnhandled Error\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/log.py\", line 103, in callWithLogger\n    return callWithContext({\"system\": lp}, func, *args, **kw)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/log.py\", line 86, in callWithContext\n    return context.call({ILogContext: newCtx}, func, *args, **kw)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/context.py\", line 122, in callWithContext\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/context.py\", line 85, in callWithContext\n    return func(*args,**kw)\n--- <exception caught here> ---\n  File \"/usr/local/lib/python3.7/site-packages/twisted/internet/selectreactor.py\", line 149, in _doReadOrWrite\n    why = getattr(selectable, method)()\n  File \"/usr/local/lib/python3.7/site-packages/twisted/internet/tcp.py\", line 243, in doRead\n    return self._dataReceived(data)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/internet/tcp.py\", line 249, in _dataReceived\n    rval = self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/telnet.py\", line 636, in dataReceived\n    self.applicationDataReceived(b''.join(appDataBuffer))\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/telnet.py\", line 988, in applicationDataReceived\n    self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/telnet.py\", line 1035, in dataReceived\n    self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/insults/insults.py\", line 537, in dataReceived\n    self.terminalProtocol.keystrokeReceived(ch, None)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/recvline.py\", line 225, in keystrokeReceived\n    m()\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/recvline.py\", line 374, in handle_RETURN\n    return RecvLine.handle_RETURN(self)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/recvline.py\", line 292, in handle_RETURN\n    self.lineReceived(line)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 267, in lineReceived\n    more = self.interpreter.push(line)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 106, in push\n    more = self.runsource(source, self.filename)\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/code.py\", line 74, in runsource\n    self.runcode(code)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 117, in runcode\n    code.InteractiveInterpreter.runcode(self, *a, **kw)\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/code.py\", line 90, in runcode\n    exec(code, self.locals)\n  File \"<console>\", line 1, in <module>\n    \n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_sitebuiltins.py\", line 26, in __call__\n    raise SystemExit(code)\nbuiltins.SystemExit: None\n\n2019-07-04 05:06:45 [twisted] CRITICAL: Unhandled Error\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/log.py\", line 103, in callWithLogger\n    return callWithContext({\"system\": lp}, func, *args, **kw)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/log.py\", line 86, in callWithContext\n    return context.call({ILogContext: newCtx}, func, *args, **kw)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/context.py\", line 122, in callWithContext\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/context.py\", line 85, in callWithContext\n    return func(*args,**kw)\n--- <exception caught here> ---\n  File \"/usr/local/lib/python3.7/site-packages/twisted/internet/selectreactor.py\", line 149, in _doReadOrWrite\n    why = getattr(selectable, method)()\n  File \"/usr/local/lib/python3.7/site-packages/twisted/internet/tcp.py\", line 243, in doRead\n    return self._dataReceived(data)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/internet/tcp.py\", line 249, in _dataReceived\n    rval = self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/telnet.py\", line 636, in dataReceived\n    self.applicationDataReceived(b''.join(appDataBuffer))\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/telnet.py\", line 988, in applicationDataReceived\n    self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/telnet.py\", line 1035, in dataReceived\n    self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/insults/insults.py\", line 537, in dataReceived\n    self.terminalProtocol.keystrokeReceived(ch, None)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/recvline.py\", line 225, in keystrokeReceived\n    m()\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/recvline.py\", line 374, in handle_RETURN\n    return RecvLine.handle_RETURN(self)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/recvline.py\", line 292, in handle_RETURN\n    self.lineReceived(line)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 267, in lineReceived\n    more = self.interpreter.push(line)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 106, in push\n    more = self.runsource(source, self.filename)\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/code.py\", line 74, in runsource\n    self.runcode(code)\n  File \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 117, in runcode\n    code.InteractiveInterpreter.runcode(self, *a, **kw)\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/code.py\", line 90, in runcode\n    exec(code, self.locals)\n  File \"<console>\", line 1, in <module>\n    \n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/_sitebuiltins.py\", line 26, in __call__\n    raise SystemExit(code)\nbuiltins.SystemExit: None", "issue_status": "Open", "issue_reporting_time": "2019-07-04T09:59:37Z"}, "68": {"issue_url": "https://github.com/scrapy/scrapy/issues/3852", "issue_id": "#3852", "issue_summary": "Got no cookie support when using builtin FilesPipeline", "issue_description": "das-rob3rt commented on Jul 4, 2019\nThis makes things complicated when the target site requires cookie to download files and I have to implement a parser for this.", "issue_status": "Open", "issue_reporting_time": "2019-07-04T04:27:58Z"}, "69": {"issue_url": "https://github.com/scrapy/scrapy/issues/3851", "issue_id": "#3851", "issue_summary": "HttpErrorMiddleware not honoring handle_httpstatus_all meta as documented", "issue_description": "ddebernardy commented on Jul 4, 2019\n    def process_spider_input(self, response, spider):\n        if 200 <= response.status < 300:  # common case\n            return\n        meta = response.meta\n        if 'handle_httpstatus_all' in meta:\n            return\nShouldn't that be more like:\n        if 'handle_httpstatus_all' in meta and meta['handle_httpstatus_all']:\n            return\nAs I read the code, setting meta['handle_httpstatus_all'] = False would likely be treated as if it was set to True.", "issue_status": "Open", "issue_reporting_time": "2019-07-03T18:58:45Z"}, "70": {"issue_url": "https://github.com/scrapy/scrapy/issues/3850", "issue_id": "#3850", "issue_summary": "[DOC] Making it a documented (\"officially supported\") feature to return deferred from request callbacks.", "issue_description": "Contributor\nstarrify commented on Jul 2, 2019\n1.\nIt's been a long time since Scrapy could properly handle request callbacks that return a Deferred (e.g. in version 1.0.0).\nSuch feature is believed to be much helpful in specific use cases, e.g. when the task is CPU-intensive due to massive lxml queries performed within a request callback and a developer wants to spread the workload to different threads using twisted.internet.threads.deferToThread.\n2.\nHere's the related piece of code as of the current release: https://github.com/scrapy/scrapy/blob/1.6.0/scrapy/core/scraper.py#L146\nHowever the feature remains undocumented for a few years least. Besides, the document currently suggests:\nThis method, as well as any other Request callback, must return an iterable of Request and/or dicts or Item objects.\nhttps://github.com/scrapy/scrapy/blob/1.6.0/docs/topics/spiders.rst#L183\n3.\nTo-dos if we're to make it a supported/documented feature:\nTo add documentations for it.\nTo add tests for it (optional but good-to-have).\nTo try keeping it backward-compatible in future versions.\n4.\nThe described change may be potential beneficial to:\nPeople that knew of this undocumented feature, and didn't choose to use it since it's undocumented and potentially unstable.\nPeople that don't yet know of such feature.\n5.\nRelated issue(s): #2230\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-07-02T17:20:05Z"}, "71": {"issue_url": "https://github.com/scrapy/scrapy/issues/3849", "issue_id": "#3849", "issue_summary": "Scrapy not honoring the Retry-After header when given a 429", "issue_description": "ddebernardy commented on Jul 2, 2019\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After\nTo reproduce:\n$ curl -L -I http://reddit.com\nIt should yield a 429 at some point, when trying to hit https://www.reddit.com/:\nHTTP/2 429 \n[...]\nretry-after: 7\n[...]\nWhile looking at the Retry middleware it doesn't seem to even try to honor the retry-after header at any point. I suppose I'll just extend the class to do so but it would be sweet if Scrapy did that out of the box.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-07-02T12:06:13Z"}, "72": {"issue_url": "https://github.com/scrapy/scrapy/issues/3845", "issue_id": "#3845", "issue_summary": "S3FeedStorage should use async IO (like txaws)", "issue_description": "diemacht commented on Jun 27, 2019\nCurrently, S3FeedStorage uses io blocking boto library through deferToThread that converts it to deferred.\nHowever, this is not a real non-blocking execution, as the blocking code runs in a separate python thread. Overhead of thread is still significant. Taking into consideration there might be much more than 1 such thread. More about disadvantages of python threads: http://alexeyvishnevsky.com/2013/10/why-you-should-consider-asynchronous-programming-model-apm-when-writing-web-server-in-python/\nInstead, S3FeedStorage should use a real non-blocking AWS library, such as txaws, for example.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-06-27T11:13:54Z"}, "73": {"issue_url": "https://github.com/scrapy/scrapy/issues/3803", "issue_id": "#3803", "issue_summary": "ImportError: No module named _util", "issue_description": "merrisco commented on Jun 2, 2019\nFollowing the tutorial, I found an error when I run the command: scrapy crawl spider\nand get the error message:\npyOpenSSLlib ImportError: No module named _util\nIt seems to me that the pyOpenSSL was not updated. I have looked around and found some others who have had a similar problem: https://stackoverflow.com/questions/50324329/running-scrapy-but-it-error-no-module-named-util\nThe solution seems to be to upgrade pyOpenSSL, but if this is a regular issues (one that is usually encountered during tutorial) might it be a good idea to add a note on the webpage.\nScrapy : 1.6.0\nlxml : 3.2.1.0\nlibxml2 : 2.9.1\ncssselect : 1.0.3\nparsel : 1.5.1\nw3lib : 1.20.0\nTwisted : 19.2.0\nPython : 2.7.5 (default, Jul 13 2018, 13:06:57) - [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1c 28 May 2019)\ncryptography : 2.7\nPlatform : Linux-3.10.0-862.14.4.el7.x86_64-x86_64-with-centos-7.5.1804-Core", "issue_status": "Open", "issue_reporting_time": "2019-06-01T19:14:19Z"}, "74": {"issue_url": "https://github.com/scrapy/scrapy/issues/3798", "issue_id": "#3798", "issue_summary": "#3798 LinkExtractor with Unique = False doesn't extract fully identical Links", "issue_description": "Ksianka commented on May 27, 2019 \u2022\nedited\nCase:\nunexpected behavior identified for LinkExctractor with Unique = False if the page contains fully identical links (the same URL and text).\nThe current result returns one unique link instead of two or more identical ones.\nFor example:\nLocal http file\n<html>\n<body>\n<a href='sample3.html'>sample 3 repetition</a>\n<a href='sample3.html'>sample 3 repetition</a>\n</body>\n</html>```\n\n2) Scrapy test code:\nignore the error about <GET file:///robots.txt>  as we use local file\n\n`import scrapy\nfrom scrapy.linkextractors import LinkExtractor\n\nclass QuotesSpider(scrapy.Spider):\n    name = \u201ctest\u201d\n    start_urls = [\n        'file:///<insert_your_local_path>/Test_file.html',\n    ]\n\ndef __init__(self, *args, **kwargs):\n    super(QuotesSpider, self).__init__(*args, **kwargs)\n    self.le = LinkExtractor(unique=False)\n\ndef parse(self, response):\n    links = self.le.extract_links(response)\n    yield {'extract_link': links}`\n\n3) run the code: scrapy crawl test, and get the result with one link:\n\n{'extract_link': [Link(url='file:///<your_local_path>/sample3.html', text='sample 3 repetition', fragment='', nofollow=False)]}\n\nInstead of the result with two links from HTML:\n\n{'extract_link': [Link(url='file:///<your_local_path>/sample3.html', text='sample 3 repetition', fragment='', nofollow=False), \nLink(url='<your_local_path>/sample3.html', text='sample 3 repetition', fragment='', nofollow=False)]}", "issue_status": "Open", "issue_reporting_time": "2019-05-27T18:21:49Z"}, "75": {"issue_url": "https://github.com/scrapy/scrapy/issues/3793", "issue_id": "#3793", "issue_summary": "Progress bar for large downloads", "issue_description": "nemec commented on May 25, 2019 \u2022\nedited by elacuesta\nWhat are your thoughts on adding a progress bar to the scrapy HTTP handler? I recently wrote a crawler that would scrape a site and throw any files into a FilesPipeline for download. Some of these files were 100+ MB in size which made the terminal seem to \"freeze\" while they downloaded in the background. I know scrapy isn't really designed to be an efficient file downloader like aria2 or jdownloader, but it's a handy tool and I was already using it to scrape the file list.\nI wrote a proof of concept using the Python library tqdm and it went even better than expected - tqdm automatically handles multiple progress bars at a time (scrapy's queue) so I got a clean section at the bottom of the console showing individual progress for each pending file over 5MB in size.\nSince I leaned so heavily on tqdm, the change to the scrapy source was only ~15 lines of code to fully implement (the POC patch is at the bottom of this post). If this feature is worth including, I'd expect other changes too since I'm sure you don't want scrapy to take a hard dependency on tqdm and the progress bar should have some configuration options, too.\nConsiderations\nDisable the progress bar in noninteractive mode (does scrapy have this? how does scrapinghub behave?)\nOptional dependency on tqdm (or code the feature from scratch within scrapy? - this may be a lot of work)\nConfigurable minimum size threshold for triggering the progress bar.\nIf tqdm is allowed as an optional dependency, the http11 handler should set a warning if devs try to set the minimum threshold but do not have tqdm installed\nWhat to do when txresponse.length is UNKNOWN_LENGTH? This can happen if the server does not return a Content-Length header. Should it be disabled entirely? Or monitor _bytes_received and lazily create a progress bar if it crosses the threshold?\nPatch\nI am using scrapy 1.5.0 in my POC but it looks like the source for http11 in master is unchanged except for the addition of one line disabling lazy, so the patch line numbers are mostly off by one.\n--- ~/scrapy-1.5.0/scrapy/core/downloader/handlers/http11.py\n+++ ~/.local/lib/python3.6/site-packages/scrapy/core/downloader/handlers/http11.py\n@@ -28,6 +28,9 @@\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n from scrapy import twisted_version\n+\n+from tqdm import tqdm\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -432,6 +435,15 @@\n         self._reached_warnsize = False\n         self._bytes_received = 0\n \n+        self.progress = None\n+        try:\n+            length = int(txresponse.length)\n+            # show progress if > 5MB\n+            if length > 5242880:\n+                self.progress = tqdm(total=length, unit='B', unit_scale=True)\n+        except ValueError:\n+            pass \n+\n     def dataReceived(self, bodyBytes):\n         # This maybe called several times after cancel was called with buffered\n         # data.\n@@ -439,7 +451,10 @@\n             return\n \n         self._bodybuf.write(bodyBytes)\n-        self._bytes_received += len(bodyBytes)\n+        new_bytes = len(bodyBytes)\n+        self._bytes_received += new_bytes\n+        if self.progress is not None:\n+            self.progress.update(new_bytes)\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n             logger.error(\"Received (%(bytes)s) bytes larger than download \"\n@@ -460,6 +475,9 @@\n                             'request': self._request})\n \n     def connectionLost(self, reason):\n+        if self.progress is not None:\n+            self.progress.close()\n+\n         if self._finished.called:\n             return\n ", "issue_status": "Open", "issue_reporting_time": "2019-05-24T21:37:17Z"}, "76": {"issue_url": "https://github.com/scrapy/scrapy/issues/3778", "issue_id": "#3778", "issue_summary": "RFC2616 cache policy checks max-age against wrong date after revalidation", "issue_description": "Contributor\njameysharp commented on May 15, 2019\nExpected behavior\nA Cache-Control: max-age=60 request header should prevent requests to the same URL from being sent more frequently than once per minute. If spiders generate requests for that URL more often, then they should be satisfied from the cache without contacting the origin server.\n(For simplicity, I'm assuming that either the origin server hasn't set a shorter max-age and hasn't made the response uncacheable, or HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS and HTTPCACHE_ALWAYS_STORE are set to ignore those restrictions.)\nActual behavior\nThe header prevents sending requests for the first minute, but after that every request results in contacting the origin server for revalidation.\nRoot cause\nRFC 7234 says:\nA response's age is the time that has passed since it was generated by, or successfully validated with, the origin server.\nRFC2616Policy._compute_current_age computes the wrong age when an Age header is present or after at least one 304 response has been received.\nAs a result, scrapy does not correctly interpret the Cache-Control: max-age=<seconds> request header.\nProposed fixes\nFix handling of Age response header\nRFC2616Policy._compute_current_age is computing results in one of two different time-bases, depending on whether the Age header is present. Without the Age header, it returns age relative to now. However the Age header only provides age relative to when the response was received, which will usually be quite different.\nFixing this requires finding out when the response was stored in the cache. Since the existing cache storage implementations maintain a timestamp or <key>_time metadata field, existing caches already have that information; it just isn't currently exposed to the cache policy.\nFreshen cached response on successful validation\nThe RFC2616Policy implementations of should_cache_response and is_cached_response_valid ensure that a 304 response never affects what the spider sees or the cache contents. However, that violates the requirements of RFC 7234 4.3.4:\nWhen a cache receives a 304 (Not Modified) response and already has one or more stored 200 (OK) responses for the same cache key, the cache needs to identify which of the stored responses are updated by this new response and then update the stored response(s) with the new information provided in the 304 response.\nSince scrapy only keeps one version of the response, and used that version to generate its conditional GET request, identifying which one to update is trivial. Then,\n...the cache MUST:\ndelete any Warning header fields in the stored response with warn-code 1xx (see Section 5.5);\nretain any Warning header fields in the stored response with warn-code 2xx; and,\nuse other header fields provided in the 304 (Not Modified) response to replace all instances of the corresponding header fields in the stored response.\nIn addition, since we need to use the response timestamp to compute the age of the response, that has to get updated to the current time as well. (This has the nice side benefit that the DBM and LevelDB storage backends won't expire responses which have been successfully validated recently.)\nInappropriate use of now\nRFC2616Policy._compute_current_age shouldn't use now as a default date (in ... .get(b'Date')) or now) because that method is invoked long after the initial request was submitted, when the cached response is being checked for usability for a later request. now just isn't a meaningful substitute for the timestamp when the response was generated; the age computation can only be correct if there's a Date header in the response.\nFortunately, HttpCacheMiddleware.process_response already adds a Date header if one is not present, and does so before calling self.storage.store_response, so we can rely on it being present in all cached responses.\nSo the or now should just get dropped because it's unreachable, and even if it was reachable, it's also wrong.\n(However, I'd think setting a default Date header shouldn't rely on enabling the cache middleware. If this were done somewhere globally, I think that would resolve #2221.)", "issue_status": "Open", "issue_reporting_time": "2019-05-15T02:19:10Z"}, "77": {"issue_url": "https://github.com/scrapy/scrapy/issues/3775", "issue_id": "#3775", "issue_summary": "Small improvement to the docs for setting ITEM_PIPELINES", "issue_description": "intotecho commented on May 14, 2019 \u2022\nedited\nIn the docs\nhttps://github.com/scrapy/scrapy/blob/65d631329a1434ec013f24341e4b8520241aec70/scrapy/templates/project/module/pipelines.py.tmpl\nIt says, in the comments:\nDefine your item pipelines here\nDon't forget to add your pipeline to the ITEM_PIPELINES setting\nSee: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\nPlease change the instruction to:\nDon't forget to add your pipeline to the ITEM_PIPELINES setting in settings.py\nI added the setting to my spider's init, and it was hard to find out what was going wrong.\nMentioning settings.py would help others who make the same mistake.", "issue_status": "Open", "issue_reporting_time": "2019-05-14T06:02:45Z"}, "78": {"issue_url": "https://github.com/scrapy/scrapy/issues/3773", "issue_id": "#3773", "issue_summary": "CONCURRENT_REQUESTS_PER_* per XXX (multiple queues)", "issue_description": "ddofborg commented on May 11, 2019 \u2022\nedited\nThe CONCURRENT_REQUESTS_PER_IP or CONCURRENT_REQUESTS_PER_DOMAIN options are not always useful. For example when you use 10 proxy servers, you would want to set this up per proxy.\nI would suggest adding CONCURRENT_REQUESTS_PER_QUEUE_NAME. For each request you should be able to set a queue name, basically it's a hashable type in which each request can be put. This way you could manually group items for concurrency.\nIf there is a workaround for the proxy problem, I would like to hear it :)", "issue_status": "Open", "issue_reporting_time": "2019-05-11T10:03:57Z"}, "79": {"issue_url": "https://github.com/scrapy/scrapy/issues/3771", "issue_id": "#3771", "issue_summary": "Add fallback selectors to ItemLoader", "issue_description": "Contributor\nejulio commented on May 10, 2019 \u2022\nedited\nIn some cases it is common to have fallback selectors for certain fields.\nThis way, we end up writing a piece of code like\nloader = MyLoader(response=response)\nloader.add_css('my_field', 'selector1')\nloader.add_css('my_field', 'selector2') # fallback 1\nloader.add_css('my_field', 'selector3') # fallback 2\nHowever, a, maybe, better way would be\nloader = MyLoader(response=response)\nloader.add_css('my_field', [\n    'selector1',\n    'selector2', # fallback 1\n    'selector3', # fallback 2\n])\nThe API above would be the equivalent of the first example.\nHowever, @cathalgarvey also shared a nice idea to stop in the first matching selector.\nloader = MyLoader(response=response)\nloader.add_css('my_field', [\n    'selector1',\n    'selector2', # fallback 1\n    'selector3', # fallback 2\n], selectors_as_preferences=True)\nThen, if selector1 yields a result, the other ones are not attempted, otherwise we fallback to selector2 and so on.\nThe same API should be applied to loader.add_xpath.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-05-10T16:46:15Z"}, "80": {"issue_url": "https://github.com/scrapy/scrapy/issues/3769", "issue_id": "#3769", "issue_summary": "CookiesMiddleware distinguish between response and request merge cookies", "issue_description": "tomstelk commented on May 10, 2019\ndont_merge_cookies setting applies to merging both previous request's cookies and the response's Set-Cookie.\nWould be useful to have a separate setting so that you can persist the previous request's Cookies, but ignore the response's Set-Cookies.\nSomething like:\ndont_merge_request_cookies\ndont_merge_response_cookies", "issue_status": "Open", "issue_reporting_time": "2019-05-10T13:25:15Z"}, "81": {"issue_url": "https://github.com/scrapy/scrapy/issues/3765", "issue_id": "#3765", "issue_summary": "Do you think about supporting Scrapy cheatsheet", "issue_description": "MarcSteven commented on May 5, 2019\nI think if you can support cheatsheet on Dash ,that's cool ,so do you plan to support it", "issue_status": "Open", "issue_reporting_time": "2019-05-05T04:00:09Z"}, "82": {"issue_url": "https://github.com/scrapy/scrapy/issues/3761", "issue_id": "#3761", "issue_summary": "Items as Dataclasses", "issue_description": "mredaelli commented on May 2, 2019\nIs there any plan to support dataclasses as yield-able results from crawlers (and possibly meta in Request/Result)?\nIt would be a very welcome addition for us, mostly for typing reasons, IDE completion, additional logic.\nAnd it should also be simple to implement (just call dataclasses.asdict())?\n\ud83d\udc4d 5", "issue_status": "Open", "issue_reporting_time": "2019-05-02T08:39:43Z"}, "83": {"issue_url": "https://github.com/scrapy/scrapy/issues/3755", "issue_id": "#3755", "issue_summary": "LinkExtractor does not extract relative links", "issue_description": "zach-watrhub commented on Apr 24, 2019 \u2022\nedited\nIs this the intended behaviour of LinkExtractor? I seem to not be able to extract relative URLs when using it. Alternatively, if I use a selector for a elements, I can capture everything.\nFor reproduction:\n<!-- assume this is from domain zip.com -->\n<html>\n<body>\n    <a href=\"/zap.html\" />\n    <a href=\"http://zip.com/zop.html\" />\n</body>\n</html>\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor \n\nclass LinkSpider(scrapy.Spider):\n    name = 'linkspider'\n    start_urls = [\"zip.com\"]\n\n    def __init__(self, *args, **kwargs):\n        super(LinkSpider, self).__init__(*args, **kwargs)\n        self.le = LinkExtractor()\n\n    def parse(self, response):\n        links = self.le.extract_links(response)\n        print(len(response.css('a')), len(links)) #2, 1", "issue_status": "Open", "issue_reporting_time": "2019-04-23T21:30:56Z"}, "84": {"issue_url": "https://github.com/scrapy/scrapy/issues/3751", "issue_id": "#3751", "issue_summary": "On distributing some blocking tasks to threads", "issue_description": "Contributor\nstarrify commented on Apr 18, 2019\nBy default, Scrapy launches much of its tasks in the reactor thread (\"main thread\"). In some cases such operations may become the bottleneck due to blocking operations (usually CPU or I/O bounded. A few examples (as of v1.6.0):\nIn scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware._decode when decompressing the HTTP responses.\nIn scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware when saving and loading cached responses. Contributes more if HTTPCACHE_GZIP is enabled.\nIn scrapy.http.response.text.TextResponse.selector when the selector (or to be precise, the underlying lxml.etree) is initially built.\nIn all these sample cases, we may expect it to be better if the task is distributed to multiple cores, since the GIL shall be unlocked when performing the specific CPU or I/O intensive task.\nThere're a few options I can think of for now:\nKeep it as-is. A user would need to customize the corresponding code, should the need arise.\nMake it switchable in Scrapy (e.g. a new setting entry COMPRESSION_ENABLED_IN_THREADS), and make such switches off by default, for minimal changes comparing to the current version.\nIntroduce the changes as non-optional ones, or make them enabled by default, as they seem not harmful.\nIMHO option 3 looks good to me.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-04-18T10:51:07Z"}, "85": {"issue_url": "https://github.com/scrapy/scrapy/issues/3745", "issue_id": "#3745", "issue_summary": "Do not name spider's logger after spider name. Respect loggers tree instead.", "issue_description": "dmugtasimov commented on Apr 14, 2019\nNo description provided.", "issue_status": "Open", "issue_reporting_time": "2019-04-13T20:12:16Z"}, "86": {"issue_url": "https://github.com/scrapy/scrapy/issues/3723", "issue_id": "#3723", "issue_summary": "GSOC2019-about scrapy-redis", "issue_description": "MikeoPerfect commented on Apr 4, 2019\nI'm a student from china, recently I want to use scrapy to support my data project, the project need fast and efficient work to crawl data, however when I use scrapy to run my code, find it's not work efficiently, so I just find the code from scrapy-master https://github.com/scrapy/queuelib/tree/master/queuelib\nby using python's own collection.deque (although it has been modified, its performance is still not high ) to store the request to be crawled, so I want to use redis to receive the scrapy-request, you know the redis usually fast. Also I'm applying for http://gsoc2019.scrapinghub.com/ideas/ , what should I do if I want to contribute the scrapy-redis and GSOC2019 for scrapinghub, sincere expectation @kmike @dangra @ejulio and so on...", "issue_status": "Open", "issue_reporting_time": "2019-04-04T15:26:13Z"}, "87": {"issue_url": "https://github.com/scrapy/scrapy/issues/3716", "issue_id": "#3716", "issue_summary": "Return more detailed error message in scrapy check", "issue_description": "Gravellent commented on Apr 2, 2019\nWhen I run scrapy check myspider, it returns an error message of\nUnhandled error in Deferred:\n\n\n----------------------------------------------------------------------\nRan 0 contracts in 0.000s\n\nOK\nHowever, if I run scrapy crawl myspider, it returns an error message of\n2019-04-02 16:37:21 [twisted] CRITICAL: Unhandled error in Deferred:\n\n2019-04-02 16:37:21 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\n    result = g.send(result)\n  File \"/usr/local/lib/python3.6/site-packages/scrapy/crawler.py\", line 82, in crawl\n    yield self.engine.open_spider(self.spider, start_requests)\npymongo.errors.ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused\nIn the end, it was the database connection that is causing the issue, but in scrapy check it doesn't return anything useful for me to debug. I think we should have more details in the error message.\nIf you guys think this is a good improvement, I will work on this as my first issue in this project. Thanks!", "issue_status": "Open", "issue_reporting_time": "2019-04-02T08:42:53Z"}, "88": {"issue_url": "https://github.com/scrapy/scrapy/issues/3710", "issue_id": "#3710", "issue_summary": "Got \"UnicodeEncodeError: 'ascii' codec can't encode characters ...\" using MailSender", "issue_description": "runbing commented on Mar 31, 2019 \u2022\nedited\nI want to send a message with MailSender when the spider closed. Here is the code snip:\n# -*- coding:utf-8 -*-\n\nimport scrapy\nfrom scrapy.mail import MailSender\n\nclass UpdateSpider(scrapy.Spider):\n    name = 'update'\n\n    # ...\n\n    def close(self):\n        mailer = MailSender.from_settings(self.settings)\n        body = u'\u6d4b\u8bd5\u5185\u5bb9\u3002'\n        return mailer.send(to=[\"runbing@example.com\"], subject=u\"\u6d4b\u8bd5\u6807\u9898\",\n               body=body, mimetype=\"text/html\")\nbut if the parameter body is non-ascii string, I will get a error like blow.\n2019-03-31 22:03:59 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method ?.close of <UpdateSpider 'update' at 0x7f4932496650>>\nTraceback (most recent call last):\n  File \"/home/runbing/Code/Scrapy/venv/lib/python2.7/site-packages/twisted/internet/defer.py\", line 151, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/home/runbing/Code/Scrapy/venv/lib/python2.7/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/home/runbing/Code/Scrapy/amazon/amazon/spiders/update.py\", line 81, in close\n    mimetype=\"text/html\")\n  File \"/home/runbing/Code/Scrapy/venv/lib/python2.7/site-packages/scrapy/mail.py\", line 101, in send\n    dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))\n  File \"/usr/lib64/python2.7/email/message.py\", line 137, in as_string\n    g.flatten(self, unixfrom=unixfrom)\n  File \"/usr/lib64/python2.7/email/generator.py\", line 83, in flatten\n    self._write(msg)\n  File \"/usr/lib64/python2.7/email/generator.py\", line 108, in _write\n    self._dispatch(msg)\n  File \"/usr/lib64/python2.7/email/generator.py\", line 134, in _dispatch\n    meth(msg)\n  File \"/usr/lib64/python2.7/email/generator.py\", line 180, in _handle_text\n    self._fp.write(payload)\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)\nI spend all my day to solve this problem, but get no luck, I just find a way like below to hack the problem, but I know this is not recommended.\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\nIs there any help for this problem?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-03-31T14:19:33Z"}, "89": {"issue_url": "https://github.com/scrapy/scrapy/issues/3693", "issue_id": "#3693", "issue_summary": "CONCURRENT_REQUESTS not being honoured", "issue_description": "mohmad-null commented on Mar 17, 2019 \u2022\nedited\nI'm seeing far too many connections being created for my spider.\nI am only using ONE single crawler process.\nThese are the settings in the spider when it starts.\nCONCURRENT_REQUESTS = 60\nCONCURRENT_REQUESTS_PER_DOMAIN = 8\nCONCURRENT_REQUESTS_PER_IP = 5\nDOWNLOAD_DELAY = 0\nDOWNLOAD_TIMEOUT = 300\nRANDOMIZE_DOWNLOAD_DELAY = True\nREACTOR_THREADPOOL_MAXSIZE = 40\n\nAUTOTHROTTLE_ENABLED = True\nAUTOTHROTTLE_START_DELAY = 1\nAUTOTHROTTLE_MAX_DELAY = 6\nAUTOTHROTTLE_TARGET_CONCURRENCY = 3\nI then feed in a list of about 100,000 urls which are split between about 4000 different domains (probably as many IP addresses too) and inserted in a random() order.\nWhat I'm seeing in Process Explorer (A Windows application) on the TCP/IP tab, is about 180 open connections, of which one IP address features about 70+ times(1), and two other IP addresses feature 20+ times.\nOn the off-chance that I was misreading/misunderstanding Process Explorer, I look at my software Firewall's open connections and it's showing the same numbers.\nScrapy 1.5.0", "issue_status": "Open", "issue_reporting_time": "2019-03-17T17:21:08Z"}, "90": {"issue_url": "https://github.com/scrapy/scrapy/issues/3690", "issue_id": "#3690", "issue_summary": "Separate Attribute to allow offsite requests", "issue_description": "Contributor\nkasun commented on Mar 15, 2019\nRight now Request.dont_filter attribute is used for two intentions. To allow duplicated requests and to allow offsite requests. This behavior is not desirable since, you could want to allow a duplicate request but still want to filter out offsite requests and vice versa.", "issue_status": "Open", "issue_reporting_time": "2019-03-15T12:46:13Z"}, "91": {"issue_url": "https://github.com/scrapy/scrapy/issues/3686", "issue_id": "#3686", "issue_summary": "Cannot skip retries for status codes from handle_httpstatus_list on specific requests if they are in RETRY_HTTP_CODES", "issue_description": "Member\nGallaecio commented on Mar 15, 2019\nGiven a request with 'handle_httpstatus_list': [404] in a project with RETRY_HTTP_CODES = [\u2026, 404, \u2026], there is currently no way to prevent retries of that specific request only for 404 responses.\n#3675 shows a potential fix, but I think alternative approaches should be discussed.", "issue_status": "Open", "issue_reporting_time": "2019-03-15T08:10:44Z"}, "92": {"issue_url": "https://github.com/scrapy/scrapy/issues/3680", "issue_id": "#3680", "issue_summary": "Feature Request: Improve template lookup for genspider", "issue_description": "ttilberg commented on Mar 12, 2019\nI've found the process of specifying custom templates to be challenging for the following two reasons:\nI added a custom templates folder to an existing project, but in doing so, I lose access to the included default Scrapy templates, since Scrapy will only search the defined directory.\nWhen specifying the template file to use, .tmpl is appended to the string regardless of what you pass in. This is obscured by the error message, as it doesn't show that Scrapy appended .tmpl.\nGiven a file my_template.tmpl exists in the correct directory: scrapy genspider example example.com -t my_template.tmpl => Unable to find template: my_template.tmpl\nI created a small patch which feels much better to use (attached in PR), I'm wondering if you'd consider this, or something similar.\nRelated: #2046", "issue_status": "Open", "issue_reporting_time": "2019-03-12T14:21:21Z"}, "93": {"issue_url": "https://github.com/scrapy/scrapy/issues/3666", "issue_id": "#3666", "issue_summary": "Request priority not guaranteed with the default scheduler configurations due to different queues (memory/disk) used.", "issue_description": "Contributor\nstarrify commented on Mar 8, 2019\nDescription\nRequest priority not guaranteed with the default scheduler configurations due to different queues (memory/disk) used.\nReproduction of the issue\n$ scrapy shell -s JOBDIR=/tmp/scrapytmp -s LOG_UNSERIALIZABLE_REQUESTS=0\n(MULTIPLE LINES OMITTED HERE)\nIn [1]: schdlr = scrapy.core.scheduler.Scheduler.from_crawler(crawler)                                                                                                                        \n\nIn [2]: schdlr.open(spider)                                                                                                                                                                   \n\nIn [3]: schdlr.enqueue_request(scrapy.Request('http://a.com/mqs-0-unserializable', meta={'foo': lambda: 'bar'}, priority=0))                                                                  \nOut[3]: True\n\nIn [4]: schdlr.enqueue_request(scrapy.Request('http://a.com/dqs-100', priority=100))                                                                                                          \nOut[4]: True\n\nIn [5]: schdlr.next_request()                                                                                                                                                                 \nOut[5]: <GET http://a.com/mqs-0-unserializable>\n\nIn [6]: schdlr.next_request()                                                                                                                                                                 \nOut[6]: <GET http://a.com/dqs-100>\nScrapy version involved: c72ab1d (the current HEAD).\nExpected behavior\nThe request with higher priority shall have come out first.\nCause of the issue\nThe scheduler maintains two queues (memory / disk) and the memory queue always have a higher priority when generating the next request.\nRelated code: https://github.com/scrapy/scrapy/blob/c72ab1d/scrapy/core/scheduler.py#L66\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-03-08T12:18:34Z"}, "94": {"issue_url": "https://github.com/scrapy/scrapy/issues/3665", "issue_id": "#3665", "issue_summary": "Allow specifying CrawlerProcess class for custom ScrapyCommands", "issue_description": "tyerq commented on Mar 8, 2019 \u2022\nedited\nCurrently there is no proper way of defining the ScrapyCommand's crawler_process attribute as a custom subclass of CrawlerProcess, since it's hardcoded in scrapy.cmdline:execute\nI need to add custom deferreds during start and stop of the crawler process, that need to be waited for before start of any crawler and stop of the process.\nUsing reactor system event triggers isn't an option because there fire time isn't right for me, so subclassing CrawlerProcess looks like the best solution.\nThe problem is that if simply redefining cmd.crawler_process would result in configure for logging, printing of start info and setting signal handlers twice, which is undesirable in production.\nI wouldn't like having to rewrite execute since it does lots of useful things I want to keep, using CrawlerRunner instead is also undesirable.\nI've thought of two possible simple solutions:\nSpecifying optional crawler_process_class for execute(...), same as settings:\ndef execute(argv=None, settings=None):\n    ...\nAdding CRAWLER_PROCESS_CLASS to settings and importing it with load_object:\ncrawler_process_class = load_object(settings.get(\"CRAWLER_PROCESS_CLASS\"))\ncmd.crawler_process = crawler_process_class(settings)\nA more robust solution would be to rethink ScrapyCommand initialisation, since the process isn't the clearest.\nI'm willing to implement a PR after discussion.\n\ud83d\udc4d 1\n\u2764\ufe0f 1", "issue_status": "Open", "issue_reporting_time": "2019-03-08T11:17:46Z"}, "95": {"issue_url": "https://github.com/scrapy/scrapy/issues/3663", "issue_id": "#3663", "issue_summary": "Make it possible to update settings in `__init__` or `from_crawler`", "issue_description": "Contributor\nejulio commented on Mar 7, 2019\nThis issue might be related to #1305\nI noticed that settings are frozen in https://github.com/scrapy/scrapy/blob/master/scrapy/crawler.py#L57\nHowever, in a given project I had a requirement to change some settings based on some spider arguments. An alternative would be to write this spider as a base class and extend it from specific spiders setting the proper settings.\nHowever, I think it would make sense to only freeze settings after the spider and other components were initialized. Or, provide some other entry point to configure settings based on arguments.\nThe other option is to use -s arguments, but in my case I was changing the FEED_EXPORT_FIELDS setting (https://docs.scrapy.org/en/latest/topics/feed-exports.html#std:setting-FEED_EXPORT_FIELDS).\nAny thoughts here?", "issue_status": "Open", "issue_reporting_time": "2019-03-07T15:07:34Z"}, "96": {"issue_url": "https://github.com/scrapy/scrapy/issues/3653", "issue_id": "#3653", "issue_summary": "exit code propagation in case of CLOSESPIDER_ERRORCOUNT extension?", "issue_description": "HyunTruth commented on Mar 5, 2019 \u2022\nedited\nWhen I run scrapy with CLOSESPIDER_ERRORCOUNT=1 and a spider raises unhandled exception, the command exits with successful code 0.\nHowever, in case of the errors, it would make sense to propagate the non-zero exit code to indicate that there were enough error occurred thus the job finished prematurely.\nI think this will help prevent the next batch job scheduled to not run on the wrong data.\nscrapy crawl $SPIDER_NAME\n...some stuff...\n$ERROR_NAME: $ERROR_DETAIL\n...Traceback...\n[scrapy.core.engine] INFO: Closing spider (closespider_errorcount)\n[scrapy.utils.signal] ERROR: Error caught on signal handler: <...>\n[scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{ ...\n'spider_exceptions/$ERROR_NAME': 1\n....}\n$ echo $?\n0\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-03-05T01:53:34Z"}, "97": {"issue_url": "https://github.com/scrapy/scrapy/issues/3645", "issue_id": "#3645", "issue_summary": "Sticky request/response meta", "issue_description": "Contributor\nGranitosaurus commented on Mar 1, 2019 \u2022\nedited\nProblem\nThere should be a default behaviour for enabling sticky meta for all spider requests.\nSticky meta means transfer Response.meta to Request.meta automatically without having to define that explicitly in every Request object.\nProblem Example\nExample for meta carry-over:\nclass MySpider(Spider):\n    order = [1,2,3]\n    def start_request(self):\n        for order in self.orders:\n            yield Request('url',\n                          self.parse_order1,\n                          meta={'order': order}\n                          )\n\n    def parse_order1(self, response):\n        yield Request(\n            'url',\n            self.parse_order2,\n            meta={'order': response.meta['order']}\n        #     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        )\n\n    def parse_order2(self, response):\n        yield {'item': 'foo', 'order': response.meta['order']}\nThe intermediate parse_order1 callback has to explicitly state that meta attribute order should be carried over. This gets very awkward if the request chain is several callbacks long or even worse - several callbacks and several keys long.\nSolution\nOne solution would be enable sticky carry over spider middleware that manages that and automatically carries over any predefined keys:\nclass StickyMeta:\n    def process_spider_output(self, response, result, spider):\n        sticky_meta = getattr(spider, 'sticky_meta', [])\n        if not sticky_meta:\n            yield from result\n            return\n        for value in result:\n            if not issubclass(type(value), Request):\n                yield value\n                continue\n            for k, v in response.meta.items():\n                if k in sticky_meta and k not in value.meta:\n                    value.meta[k] = v\n            yield value\n\nclass MySpider(Spider):\n    order = [1, 2, 3]\n    sticky_meta = ['order']\nedit: \u261d\ufe0f more detailed implementation in referenced PR \ud83d\udc47\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2019-03-01T08:47:19Z"}, "98": {"issue_url": "https://github.com/scrapy/scrapy/issues/3628", "issue_id": "#3628", "issue_summary": "Crawler logs are cut by day", "issue_description": "Lingshuirong commented on Feb 15, 2019\nI hope scrapy's log can provide more convenience, such as crawlers running for a long time, log can be cut by day, so that the log file is not too large.", "issue_status": "Open", "issue_reporting_time": "2019-02-15T07:49:27Z"}, "99": {"issue_url": "https://github.com/scrapy/scrapy/issues/3618", "issue_id": "#3618", "issue_summary": "Type hints and Response.selector", "issue_description": "mohmad-null commented on Feb 8, 2019\nPython 3.5 introduced Type Hints: https://docs.python.org/3/library/typing.html\nI'm using them with Scrapy's response object, but it seems that however you've declared selectors in scrapy it makes PyCharm unable to realise there's a selector method/attribute:\nfrom scrapy.http import Response\n\ndef my_function(response: Response):\n    response.selector.remove_namespaces()\nFor which PyCharm reports: \"Unresolved attribute reference 'selector' for class 'Response'\"\nObviously the selector works, but it's not being exposed sufficiently well for the IDE to be able to spot it which obviates the point of type hints.", "issue_status": "Open", "issue_reporting_time": "2019-02-07T19:40:23Z"}, "100": {"issue_url": "https://github.com/scrapy/scrapy/issues/3614", "issue_id": "#3614", "issue_summary": "_tunnelReadyDeferred need canceller to close connection like twisted do", "issue_description": "baby5 commented on Feb 3, 2019 \u2022\nedited\nclass TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n    ...\n    self._tunnelReadyDeferred = defer.Deferred(canceller=self._canceller)\n    ...\n\n    def _canceller(self, deferred):\n        self._protocol.tansport.loseConnection()\ntwisted do\nthere must be some things i do not know\nwelcome to discuss", "issue_status": "Open", "issue_reporting_time": "2019-02-03T09:13:25Z"}, "101": {"issue_url": "https://github.com/scrapy/scrapy/issues/3613", "issue_id": "#3613", "issue_summary": "Apply priority in Rule", "issue_description": "phongtnit commented on Feb 2, 2019 \u2022\nedited\nHello,\nI think it is useful to add priority in Rule, so developers can use CrawlSpider with priority property and the property automatically pass to Spider object.\nThe expected Rule would be some thing likes:\n        # Category link: https://abc.com/cat1/page=2\n        Rule(LinkExtractor(allow=r'cat1/page=([0-9])+'), kwargs={'priority': 20}, follow=True),\n        Rule(LinkExtractor(allow=r'cat2/page=([0-9])+'), kwargs={'priority': 19}, follow=True),\n        Rule(LinkExtractor(allow=r'cat3/page=([0-9])+'), kwargs={'priority': 18}, follow=True),\n\n        # Item link: https://abc.com/items/54460547/<title>\n        Rule(LinkExtractor(allow=r'items\\/([0-9])+\\/.*'), kwargs={'priority': 10}, callback='parse_item', follow=True),\nI can use Spider to resolve the issue in that case. CrawlSpider object is awesome, however, I need priority property here. Any ideas to do that?\nThanks for your support,", "issue_status": "Open", "issue_reporting_time": "2019-02-02T11:00:23Z"}, "102": {"issue_url": "https://github.com/scrapy/scrapy/issues/3590", "issue_id": "#3590", "issue_summary": "Ability to retry a request from inside a spider callback", "issue_description": "Contributor\nkasun commented on Jan 19, 2019\nThere are situations where websites return 200 responses but the content is not available due to bans or temporal issues which can be fixed by retrying requests.\nThere should be an easier way to retry requests inside spider callbacks, which should ideally reuse the code in Retry downloader middleware.\nI see two approaches for this.\nIntroduce new exception called RetryRequest which can be raised inside a spider callback to indicate a retry. I personally prefer this but the implementation of this is a little untidy due to this bug #220\nfrom scrapy.exceptions import RetryRequest\n\ndef parse(self, response):\n    if response.xpath('//title[text()=\"Content not found\"]'):\n        raise RetryRequest('Missing content')\nIntroduce a new class RetryRequest which wraps a request that needs to be retried. A RetryRequest can be yielded from a spider callback to indicate a retry\nfrom scrapy.http import RetryRequest\n\ndef parse(self, response):\n    if response.xpath('//title[text()=\"Content not found\"]'):\n        yield RetryRequest(response.request, reason='Missing content')\nWill be sending two PRs for the two approaches. Happy to hear about any other alternatives too.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2019-01-19T17:44:24Z"}, "103": {"issue_url": "https://github.com/scrapy/scrapy/issues/3585", "issue_id": "#3585", "issue_summary": "request.meta['download_slot'] is not documented", "issue_description": "Member\nkmike commented on Jan 16, 2019\nrequest.meta['download_slot'] is not documented in https://doc.scrapy.org/en/latest/topics/request-response.html#request-meta-special-keys.", "issue_status": "Open", "issue_reporting_time": "2019-01-16T11:36:37Z"}, "104": {"issue_url": "https://github.com/scrapy/scrapy/issues/3582", "issue_id": "#3582", "issue_summary": "Do not recommend Google Cache in the documentation", "issue_description": "Member\nGallaecio commented on Jan 14, 2019\nFrom the feedback at Using Scrapy on a Google cache of a website, I think we should stop recommending Google Cache in the Scrapy documentation.\nWe should review the terms of use of the Wayback Machine from archive.org, and see if we can recommend that instead.\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2019-01-14T11:28:12Z"}, "105": {"issue_url": "https://github.com/scrapy/scrapy/issues/3580", "issue_id": "#3580", "issue_summary": "Redirects don't go through middleware", "issue_description": "mohmad-null commented on Jan 14, 2019\nI have some simple middleware that blocks certain domains from being crawled and raises an IgnoreRequest for those domains. It works well.\nHowever, I've just discovered it gets ignored for requests that go through the RedirectMiddleware.\nMy normal priority for the middleware is 110, but upon looking up https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-SPIDER_MIDDLEWARES_BASE - I see the redirect middleware is later than that, so I tried changing the number to 800, but to no avail.\nI tried looking in https://github.com/scrapy/scrapy/blob/1fd1702a11a56ecbe9851ba4f9d3c10797e262dd/scrapy/downloadermiddlewares/redirect.py - to see what was going on, but there are exactly 2 lines of comment in 102 lines of code so....", "issue_status": "Open", "issue_reporting_time": "2019-01-13T22:12:39Z"}, "106": {"issue_url": "https://github.com/scrapy/scrapy/issues/3576", "issue_id": "#3576", "issue_summary": "Extend ItemLoader processors", "issue_description": "Contributor\nMatthijsy commented on Jan 12, 2019\nCurrently there are three methods to add ItemLoader processor:\nThe default_input/output_processor on the ItemLoader class\nThe field_name_in/out on the ItemLoader class\nThe input/output_processor on the scrapy.Field\nPersonally I use the input/output_processor on the scrapy.Field combined with the default_input/output_processor a lot. But I use those in combination. Often I just want to add one more processor after the default processors. Since input/output_processor on scrapy.Field does a override of the defaults this is quite hard to do.\nSo I would propose to add another method to add a input/output processors. I would like to have something like add_input/output on the scrapy.Field, which would add the specified processor to the default processor.\nI did implement this on my own ItemLoader class but think that it would be usefull for the scrapy core. My implementation is as follows (original source: https://github.com/scrapy/scrapy/blob/master/scrapy/loader/__init__.py#L69). Ofcourse this can be added to get_output_processor in the same way.\ndef get_input_processor(self, field_name):\n        proc = getattr(self, '%s_in' % field_name, None)\n        if not proc:\n            override_proc = self._get_item_field_attr(field_name, 'input_processor')\n            extend_proc = self._get_item_field_attr(field_name, 'add_input')\n            if override_proc and extend_proc:\n                raise ValueError(f'Not allowed to define input_processor and add_input to {field_name}')\n            if override_proc:\n                return override_proc\n            elif extend_proc:\n                return Compose(self.default_input_processor, extend_proc)\n            return self.default_input_processor\n        return proc\nI am not sure if add_input is a good name, probably extend_input_processor is more clear but this quite a long name. I would like to hear if more people are wanting this feature and what you all think about what the naming should be.\n\ud83d\udc4e 1", "issue_status": "Open", "issue_reporting_time": "2019-01-11T21:39:32Z"}, "107": {"issue_url": "https://github.com/scrapy/scrapy/issues/3575", "issue_id": "#3575", "issue_summary": "Double-encoded cookies", "issue_description": "Member\nelacuesta commented on Jan 10, 2019\nWhen cookies are passed as UTF8 encoded bytes to the Request constructor, they end up being encoded twice and escaped in the Cookie header.\n$ scrapy shell\n(...)\nIn [1]: fetch(scrapy.Request('https://httpbin.org/cookies', cookies={'a': u'\u00e1'.encode('utf8')}))\n\nIn [2]: request.headers['Cookie']\nOut[2]: b\"a=b'\\\\xc3\\\\xa1'\"\n\nIn [3]: print(response.text)\n{\n  \"cookies\": {\n    \"a\": \"b'\\\\xc3\\\\xa1'\"\n  }\n}\nThis seems to happen only in Python 3.\n$ scrapy version -v\nScrapy       : 1.5.0\nlxml         : 4.2.6.0\nlibxml2      : 2.9.8\ncssselect    : 1.0.3\nparsel       : 1.5.1\nw3lib        : 1.19.0\nTwisted      : 18.9.0\nPython       : 3.6.0 (default, Sep  1 2017, 10:59:37) - [GCC 4.8.4]\npyOpenSSL    : 18.0.0 (OpenSSL 1.1.0j  20 Nov 2018)\ncryptography : 2.4.2\nPlatform     : Linux-4.4.0-134-generic-x86_64-with-debian-jessie-sid", "issue_status": "Open", "issue_reporting_time": "2019-01-10T16:38:01Z"}, "108": {"issue_url": "https://github.com/scrapy/scrapy/issues/3573", "issue_id": "#3573", "issue_summary": "Catching VerificationError warning in scrapy.core.downloader.tls", "issue_description": "dkajtoch commented on Jan 10, 2019\nIn the file: https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/tls.py line 77. When scraping some website I am getting VerificationError (which is true - this website has broken certificate) which instead of being passed further is caught there and logged as a warning. Is there a chance to redesign the class somehow so that any warning (in general) is passed further, for example in response.meta? I don't want to close the connection if that error happens but I need to ascribe this info to the url.", "issue_status": "Open", "issue_reporting_time": "2019-01-10T09:11:59Z"}, "109": {"issue_url": "https://github.com/scrapy/scrapy/issues/3565", "issue_id": "#3565", "issue_summary": "bindaddress of meta never work right.", "issue_description": "Contributor\nNewUserHa commented on Jan 4, 2019\nI have multi local IPs with different outgoing port to internet.\nsimply:\n        yield scrapy.Request('http://httpbin.org/ip', self.parse3, meta={'bindaddress': ('192.168.0.2', 0)}, dont_filter=True)\n        yield scrapy.Request('http://httpbin.org/ip', self.parse3, meta={'bindaddress': ('192.168.0.3', 0)}, dont_filter=True)\nresponsed same result.\nps: already verified the two bind-address works using curl.\ndoes anyone know how to fix and be willing to fix it?", "issue_status": "Open", "issue_reporting_time": "2019-01-04T03:11:35Z"}, "110": {"issue_url": "https://github.com/scrapy/scrapy/issues/3553", "issue_id": "#3553", "issue_summary": "Genspider prepend `http://` without checking it in the `domain`", "issue_description": "ThunderMind2019 commented on Dec 28, 2018\ngenspider prepend http:// But when i enter address like https://example.com it becomes http://https://example.com that, when run scrapy crawl throws an error.\nWhat it should do, it should first check the receiving domain than take decision according to the passing domain whether it needs a http:// or nothing.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-12-28T07:18:19Z"}, "111": {"issue_url": "https://github.com/scrapy/scrapy/issues/3552", "issue_id": "#3552", "issue_summary": "OpenSSL error when fetching an HTTPS page", "issue_description": "sstalle commented on Dec 27, 2018\nHello,\nI'm getting an error when trying to scrape HTTPS pages. Let's look at www.google.com as an example:\nscrapy fetch --pdb 'https://www.google.com'\n2018-12-27 13:45:04 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n2018-12-27 13:45:04 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.7 (default, Oct 22 2018, 11:32:17) - [GCC 8.2.0], pyOpenSSL 18.0.0 (OpenSSL 1.1.0j  20 Nov 2018), cryptography 2.4.2, Platform Linux-4.15.0-36-generic-x86_64-with-Ubuntu-18.04-bionic\n2018-12-27 13:45:04 [scrapy.crawler] INFO: Overridden settings: {}\n2018-12-27 13:45:04 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats']\n2018-12-27 13:45:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2018-12-27 13:45:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2018-12-27 13:45:04 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2018-12-27 13:45:04 [scrapy.core.engine] INFO: Spider opened\n2018-12-27 13:45:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2018-12-27 13:45:04 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2018-12-27 13:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.google.com> (referer: None)\n<!doctype html><html dir=\"rtl\" itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ar-AE\"><head><meta content=\"text/html; charset=UTF-8\" http-equiv=\"Content-Type\"><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><title>Google</title><script nonce=\"y8SVUeCln6pIIL+tT3bpDA==\">(function(){window.google={kEI:'MK0kXLv5Ne-mrgTclpzYDg',kEXPI:'0,1353747,57,1958,1016,1406,697,528,730,326,293,1180,30,524,27,144,532,895,546,6,2335661,186,32,68,329226,1294,12383,4855,32691,15248,867,6057,6106,12919,1410,2192,364,3319,5505,1112,129,1201,260,1028,4079,575,155,680,284,2,578,728,2431,1362,283,4040,3390,8,1569,774,2115,132,4352,395,525,626,2,1965,528,2067,182,283,3136,669,1050,464,1344,386,743,268,81,7,28,463,620,29,994,385,474,3204,644,3394,896,313,876,412,2,554,33,2332,2,267,381,3,283,948,1220,38,363,259,298,573,145,28,127,499,718,1361,106,381,47,589,491,1284,1085,323,44,49,741,9,2,258,443,56,1196,940,217,2045,5,136,159,2,145,531,44,18,31,285,600,823,22,408,252,97,249,247,1116,42,368,540,177,8,22,478,593,166,850,630,448,16,87,100,484,279,6,38,331,135,2,358,61,6,99,132,302,18,438,136,2,83,340,357,274,587,359,273,5970803,2554,235,5997366,90,2800095,4,1572,549,332,445,1,2,80,1,900,583,9,304,1,8,1,2,2132,1,1,1,1,1,414,1,889,59,351,2,373,3,7,2,360,81,3,6,71,1,9,9,2,1,18,1,2,4,1,1,2,1,1,100,1,29,8,22,1,1,1,1,1,1,1,1,1,1,1,1,2,28,1,1,3,1,2,1,1,1,1,1,1,68,8,6,1,3,1,2,1,1,3,1,3,31,10,6,2,4,2,2,2,4,12,4,3',authuser:0,kscs:'c9c918f0_MK0kXLv5Ne-mrgTclpzYDg',kGL:'AE'};google.kHL='ar-AE';})();google.time=function(){return(new Date).getTime()};(function(){google.lc=[];google.li=0;google.getEI=function(a){for(var b;a&&(!a.getAttribute||!(b=a.getAttribute(\"eid\")));)a=a.parentNode;return b||google.kEI};google.getLEI=function(a){for(var b=null;a&&(!a.getAttribute||!(b=a.getAttribute(\"leid\")));)a=a.parentNode;return b};google.https=function(){return\"https:\"==window.location.protocol};google.ml=function(){return null};google.log=function(a,b,e,c,g){if(a=google.logUrl(a,b,e,c,g)){b=new Image;var d=google.lc,f=google.li;d[f]=b;b.onerror=b.onload=b.onabort=function(){delete d[f]};google.vel&&google.vel.lu&&google.vel.lu(a);b.src=a;google.li=f+1}};google.logUrl=function(a,b,e,c,g){var d=\"\",f=google.ls||\"\";e||-1!=b.search(\"&ei=\")||(d=\"&ei=\"+google.getEI(c),-1==b.search(\"&lei=\")&&(c=google.getLEI(c))&&(d+=\"&lei=\"+c));c=\"\";!e&&google.cshid&&-1==b.search(\"&cshid=\")&&\"slh\"!=a&&(c=\"&cshid=\"+google.cshid);a=e||\"/\"+(g||\"gen_204\")+\"?atyp=i&ct=\"+a+\"&cad=\"+b+d+f+\"&zx=\"+google.time()+c;/^http:/i.test(a)&&google.https()&&(google.ml(Error(\"a\"),!1,{src:a,glmm:1}),a=\"\");return a};}).call(this);(function(){google.y={};google.x=function(a,b){if(a)var c=a.id;else{do c=Math.random();while(google.y[c])}google.y[c]=[a,b];return!1};google.lm=[];google.plm=function(a){google.lm.push.apply(google.lm,a)};google.lq=[];google.load=function(a,b,c){google.lq.push([[a],b,c])};google.loadAll=function(a,b){google.lq.push([a,b])};}).call(this);google.f={};</script><script nonce=\"y8SVUeCln6pIIL+tT3bpDA==\">var a=window.location,b=a.href.indexOf(\"#\");if(0<=b){var c=a.href.substring(b+1);/(^|&)q=/.test(c)&&-1==c.indexOf(\"#\")&&a.replace(\"/search?\"+c.replace(/(^|&)fp=[^&]*/g,\"\")+\"&cad=h\")};</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:left}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-left:.5em;vertical-align:top}#gbar{float:right}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}\n</style><style>body,td,a,p,.h{font-family:arial,sans-serif}body{margin:0;overflow-y:scroll}#gog{padding:3px 8px 0}td{line-height:.8em}.gac_m td{line-height:17px}form{margin-bottom:20px}.h{color:#36c}.q{color:#00c}.ts td{padding:0}.ts{border-collapse:collapse}em{font-weight:bold;font-style:normal}.lst{height:25px;width:496px}.gsfi,.lst{font:18px arial,sans-serif}.gsfs{font:17px arial,sans-serif}.ds{display:inline-box;display:inline-block;margin:3px 0 4px;margin-right:4px}input{font-family:inherit}a.gb1,a.gb2,a.gb3,a.gb4{color:#11c !important}body{background:#fff;color:black}a{color:#11c;text-decoration:none}a:hover,a:active{text-decoration:underline}.fl a{color:#36c}a:visited{color:#551a8b}a.gb1,a.gb4{text-decoration:underline}a.gb3:hover{text-decoration:none}#ghead a.gb2:hover{color:#fff !important}.sblc{padding-top:5px}.sblc a{display:block;margin:2px 0;margin-right:13px;font-size:11px}.lsbb{background:#eee;border:solid 1px;border-color:#ccc #ccc #999 #999;height:30px}.lsbb{display:block}.ftl,#fll a{display:inline-block;margin:0 12px}.lsb{background:url(/images/nav_logo229.png) 0 -261px repeat-x;border:none;color:#000;cursor:pointer;height:30px;margin:0;outline:0;font:15px arial,sans-serif;vertical-align:top}.lsb:active{background:#ccc}.lst:focus{outline:none}.tiah{width:458px}</style><script nonce=\"y8SVUeCln6pIIL+tT3bpDA==\"></script></head><body bgcolor=\"#fff\"><script nonce=\"y8SVUeCln6pIIL+tT3bpDA==\">(function(){var src='/images/nav_logo229.png';var iesg=false;document.body.onload = function(){window.n && window.n();if (document.images){new Image().src=src;}\nif (!iesg){document.f&&document.f.q.focus();document.gbqf&&document.gbqf.q.focus();}\n}\n})();</script><div id=\"mngb\"> <div id=gbar><nobr><b class=gb1>&#1576;&#1581;&#1579;</b> <a class=gb1 href=\"https://www.google.ae/imghp?hl=ar&tab=wi\">&#1589;&#1608;&#1585;</a> <a class=gb1 href=\"https://maps.google.ae/maps?hl=ar&tab=wl\">&#1582;&#1585;&#1575;&#1574;&#1591;</a> <a class=gb1 href=\"https://www.youtube.com/?gl=AE&tab=w1\">YouTube</a> <a class=gb1 href=\"https://news.google.ae/nwshp?hl=ar&ned=ar_me&tab=wn\">&#1575;&#1604;&#1571;&#1582;&#1576;&#1575;&#1585;</a> <a class=gb1 href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a> <a class=gb1 href=\"https://drive.google.com/?tab=wo\">Drive</a> <a class=gb1 href=\"https://www.google.com/calendar?tab=wc\">&#1578;&#1602;&#1608;&#1610;&#1605;</a> <a class=gb1 style=\"text-decoration:none\" href=\"https://www.google.ae/intl/ar/about/products?tab=wh\"><u>&#1575;&#1604;&#1605;&#1586;&#1610;&#1583;</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a href=\"http://www.google.ae/history/optout?hl=ar\" class=gb4>&#1587;&#1616;&#1580;&#1604; &#1576;&#1581;&#1579; &#1575;&#1604;&#1608;&#1610;&#1576;</a> | <a  href=\"/preferences?hl=ar\" class=gb4>&#1575;&#1604;&#1573;&#1593;&#1583;&#1575;&#1583;&#1575;&#1578;</a> | <a target=_top id=gb_70 href=\"https://accounts.google.com/ServiceLogin?hl=ar&passive=true&continue=https://www.google.com/\" class=gb4>&#1578;&#1587;&#1580;&#1610;&#1604; &#1575;&#1604;&#1583;&#1582;&#1608;&#1604;</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div> </div><center><br clear=\"all\" id=\"lgpd\"><div id=\"lga\"><img alt=\"Google\" height=\"92\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\" id=\"hplogo\" onload=\"window.lol&&lol()\"><br><br></div><form action=\"/search\" name=\"f\"><table cellpadding=\"0\" cellspacing=\"0\"><tr valign=\"top\"><td width=\"25%\">&nbsp;</td><td align=\"center\" nowrap=\"\"><input name=\"ie\" value=\"ISO-8859-1\" type=\"hidden\"><input value=\"ar-AE\" name=\"hl\" type=\"hidden\"><input name=\"source\" type=\"hidden\" value=\"hp\"><input name=\"biw\" type=\"hidden\"><input name=\"bih\" type=\"hidden\"><div class=\"ds\" style=\"height:32px;margin:4px 0\"><div style=\"position:relative;zoom:1\"><input style=\"color:#000;margin:0;padding:5px 6px 0 8px;vertical-align:top;padding-left:38px\" autocomplete=\"off\" class=\"lst tiah\" value=\"\" title=\"&#1576;&#1581;&#1579; Google\" maxlength=\"2048\" name=\"q\" size=\"57\"><img src=\"/textinputassistant/tia.png\" style=\"position:absolute;cursor:pointer;left:5px;top:4px;z-index:300\" onclick=\"(function(){var src='/textinputassistant/11/ar_tia.js';var s=document.createElement('script');s.src=src;(document.getElementById('xjsc')||document.body).appendChild(s);})();\" alt=\"\" height=\"23\" width=\"27\"></div></div><br style=\"line-height:0\"><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" value=\"&#1576;&#1581;&#1579; Google\" name=\"btnG\" type=\"submit\"></span></span><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" value=\"&#1590;&#1585;&#1576;&#1577; &#1581;&#1592;\" name=\"btnI\" onclick=\"if(this.form.q.value)this.checked=1; else top.location='/doodles/'\" type=\"submit\"></span></span></td><td class=\"fl sblc\" align=\"right\" nowrap=\"\" width=\"25%\"><a href=\"/advanced_search?hl=ar-AE&amp;authuser=0\">&#1576;&#1581;&#1579; &#1605;&#1578;&#1602;&#1583;&#1605;</a><a href=\"/language_tools?hl=ar-AE&amp;authuser=0\">&#1571;&#1583;&#1608;&#1575;&#1578; &#1575;&#1604;&#1604;&#1594;&#1577;</a></td></tr></table><input id=\"gbv\" name=\"gbv\" type=\"hidden\" value=\"1\"><script nonce=\"y8SVUeCln6pIIL+tT3bpDA==\">(function(){var a,b=\"1\";if(document&&document.getElementById)if(\"undefined\"!=typeof XMLHttpRequest)b=\"2\";else if(\"undefined\"!=typeof ActiveXObject){var c,d,e=[\"MSXML2.XMLHTTP.6.0\",\"MSXML2.XMLHTTP.3.0\",\"MSXML2.XMLHTTP\",\"Microsoft.XMLHTTP\"];for(c=0;d=e[c++];)try{new ActiveXObject(d),b=\"2\"}catch(h){}}a=b;if(\"2\"==a&&-1==location.search.indexOf(\"&gbv=2\")){var f=google.gbvu,g=document.getElementById(\"gbv\");g&&(g.value=a);f&&window.setTimeout(function(){location.href=f},0)};}).call(this);</script></form><div id=\"gac_scont\"></div><div style=\"font-size:83%;min-height:3.5em\"><br><div id=\"gws-output-pages-elements-homepage_additional_languages__als\"><style>#gws-output-pages-elements-homepage_additional_languages__als{font-size:small;margin-bottom:24px}#SIvCob{display:inline-block;line-height:28px;}#SIvCob a{padding:0 3px;}.H6sW5{display:inline-block;margin:0 2px;white-space:nowrap}.z4hgWe{display:inline-block;margin:0 2px}</style><div id=\"SIvCob\">&#1605;&#1581;&#1585;&#1617;&#1603; &#1576;&#1581;&#1579; Google &#1605;&#1578;&#1608;&#1601;&#1617;&#1585; &#1576;&#1575;&#1604;&#1604;&#1594;&#1577;:  <a href=\"https://www.google.com/setprefs?sig=0_Op0m1OssGjK0jPuLZxgPPuzH9Ek%3D&amp;hl=fa&amp;source=homepage&amp;sa=X&amp;ved=0ahUKEwi70byd6r_fAhVvk4sKHVwLB-sQ2ZgBCAU\">&#1601;&#1575;&#1585;&#1587;&#1740;</a>    <a dir=\"ltr\" href=\"https://www.google.com/setprefs?sig=0_Op0m1OssGjK0jPuLZxgPPuzH9Ek%3D&amp;hl=en&amp;source=homepage&amp;sa=X&amp;ved=0ahUKEwi70byd6r_fAhVvk4sKHVwLB-sQ2ZgBCAY\">English</a>    <a dir=\"ltr\" href=\"https://www.google.com/setprefs?sig=0_Op0m1OssGjK0jPuLZxgPPuzH9Ek%3D&amp;hl=hi&amp;source=homepage&amp;sa=X&amp;ved=0ahUKEwi70byd6r_fAhVvk4sKHVwLB-sQ2ZgBCAc\">&#2361;&#2367;&#2344;&#2381;&#2342;&#2368;</a>    <a href=\"https://www.google.com/setprefs?sig=0_Op0m1OssGjK0jPuLZxgPPuzH9Ek%3D&amp;hl=ur&amp;source=homepage&amp;sa=X&amp;ved=0ahUKEwi70byd6r_fAhVvk4sKHVwLB-sQ2ZgBCAg\">&#1575;&#1585;&#1583;&#1608;</a>  </div></div></div><span id=\"footer\"><div style=\"font-size:10pt\"><div style=\"margin:19px auto;text-align:center\" id=\"fll\"><a href=\"/intl/ar/ads/\">&#1575;&#1604;&#1576;&#1585;&#1606;&#1575;&#1605;&#1580; &#1575;&#1604;&#1573;&#1593;&#1604;&#1575;&#1606;&#1610;</a><a href=\"http://www.google.com/intl/ar/services/\">&#1581;&#1604;&#1608;&#1604; &#1575;&#1604;&#1588;&#1585;&#1603;&#1575;&#1578;</a><a href=\"https://plus.google.com/101532581614261957891\" rel=\"publisher\">+Google</a><a href=\"/intl/ar/about.html\">&#1603;&#1604; &#1605;&#1575; &#1578;&#1581;&#1576; &#1605;&#1593;&#1585;&#1601;&#1578;&#1607; &#1593;&#1606; Google &#1607;&#1606;&#1575;</a><a dir=\"ltr\" href=\"https://www.google.com/setprefdomain?prefdom=AE&amp;prev=https://www.google.ae/&amp;sig=K_LVsvsOYgNgpu3bZkCt1rVK7Ltrs%3D\">Google.ae</a></div></div><p style=\"color:#767676;font-size:8pt\">&copy; 2018 - <a href=\"/intl/ar/policies/privacy/\">&#1575;&#1604;&#1582;&#1589;&#1608;&#1589;&#1610;&#1577;</a> - <a href=\"/intl/ar/policies/terms/\">&#1575;&#1604;&#1576;&#1606;&#1608;&#1583;</a></p></span></center><script nonce=\"y8SVUeCln6pIIL+tT3bpDA==\">(function(){window.google.cdo={height:0,width:0};(function(){var a=window.innerWidth,b=window.innerHeight;if(!a||!b){var c=window.document,d=\"CSS1Compat\"==c.compatMode?c.documentElement:c.body;a=d.clientWidth;b=d.clientHeight}a&&b&&(a!=google.cdo.width||b!=google.cdo.height)&&google.log(\"\",\"\",\"/client_204?&atyp=i&biw=\"+a+\"&bih=\"+b+\"&ei=\"+google.kEI);}).call(this);})();(function(){var u='/xjs/_/js/k\\x3dxjs.hp.en.-PpIeOYp7rw.O/m\\x3dsb_he,d/am\\x3dYsAs/rt\\x3dj/d\\x3d1/rs\\x3dACT90oHak6Vea8_d5yZnUPgd2x9xqhMspg';var b={gen204:\"xjsls\",clearcut:31};setTimeout(function(){var a=document.createElement(\"script\");a.src=u;google.timers&&google.timers.load&&google.tick&&google.tick(\"load\",b);document.body.appendChild(a)},0);})();(function(){window.google.xjsu='/xjs/_/js/k\\x3dxjs.hp.en.-PpIeOYp7rw.O/m\\x3dsb_he,d/am\\x3dYsAs/rt\\x3dj/d\\x3d1/rs\\x3dACT90oHak6Vea8_d5yZnUPgd2x9xqhMspg';})();function _DumpException(e){throw e;}\n(function(){var pmc='{\\x22Qnk92g\\x22:{},\\x22U5B21g\\x22:{},\\x22YFCs/g\\x22:{},\\x22ZI/YVQ\\x22:{},\\x22d\\x22:{},\\x22sb_he\\x22:{\\x22agen\\x22:true,\\x22cgen\\x22:true,\\x22client\\x22:\\x22heirloom-hp\\x22,\\x22dh\\x22:true,\\x22dhqt\\x22:true,\\x22ds\\x22:\\x22\\x22,\\x22ffql\\x22:\\x22en\\x22,\\x22fl\\x22:true,\\x22host\\x22:\\x22google.com\\x22,\\x22isbh\\x22:28,\\x22jsonp\\x22:true,\\x22msgs\\x22:{\\x22cibl\\x22:\\x22&#1605;&#1581;&#1608; &#1575;&#1604;&#1576;&#1581;&#1579;\\x22,\\x22dym\\x22:\\x22&#1607;&#1604; &#1578;&#1602;&#1589;&#1583; :\\x22,\\x22lcky\\x22:\\x22&#1590;&#1585;&#1576;&#1577; &#1581;&#1592;\\x22,\\x22lml\\x22:\\x22&#1605;&#1586;&#1610;&#1583; &#1605;&#1606; &#1575;&#1604;&#1605;&#1593;&#1604;&#1608;&#1605;&#1575;&#1578;\\x22,\\x22oskt\\x22:\\x22&#1571;&#1583;&#1608;&#1575;&#1578; &#1575;&#1604;&#1573;&#1583;&#1582;&#1575;&#1604;\\x22,\\x22psrc\\x22:\\x22&#1578;&#1605;&#1578; &#1573;&#1586;&#1575;&#1604;&#1577; &#1607;&#1584;&#1575; &#1575;&#1604;&#1576;&#1581;&#1579; &#1605;&#1606; \\\\u003Ca href\\x3d\\\\\\x22/history\\\\\\x22\\\\u003E&#1587;&#1616;&#1580;&#1604; &#1575;&#1604;&#1576;&#1581;&#1579;\\\\u003C/a\\\\u003E.\\x22,\\x22psrl\\x22:\\x22&#1573;&#1586;&#1575;&#1604;&#1577;\\x22,\\x22sbit\\x22:\\x22&#1575;&#1604;&#1576;&#1581;&#1579; &#1576;&#1581;&#1587;&#1576; &#1575;&#1604;&#1589;&#1608;&#1585;\\x22,\\x22srch\\x22:\\x22&#1576;&#1581;&#1579; Google\\u200f\\x22},\\x22ovr\\x22:{},\\x22pq\\x22:\\x22\\x22,\\x22refpd\\x22:true,\\x22rfs\\x22:[],\\x22sbpl\\x22:24,\\x22sbpr\\x22:24,\\x22scd\\x22:10,\\x22sce\\x22:5,\\x22stok\\x22:\\x22w5imgy_Y5x5UzxatpHEl2smmvPE\\x22,\\x22uhde\\x22:false}}';google.pmc=JSON.parse(pmc);})();(function(){var r=['aa','async','ipv6','mu','sf'];google.plm(r);})();</script>     </body></html>\n2018-12-27 13:45:05 [scrapy.core.engine] INFO: Closing spider (finished)\n2018-12-27 13:45:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 213,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 6323,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2018, 12, 27, 10, 45, 5, 11903),\n 'log_count/DEBUG': 2,\n 'log_count/INFO': 7,\n 'memusage/max': 51888128,\n 'memusage/startup': 51888128,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2018, 12, 27, 10, 45, 4, 773777)}\n2018-12-27 13:45:05 [scrapy.core.engine] INFO: Spider closed (finished)\nJumping into debugger for post-mortem of exception '(-1, 'Unexpected EOF')':\n> /root/scraper/lib/python3.6/site-packages/OpenSSL/SSL.py(1632)_raise_ssl_error()\n-> raise SysCallError(-1, \"Unexpected EOF\")\n(Pdb) where\n  /root/scraper/lib/python3.6/site-packages/twisted/protocols/tls.py(274)_flushReceiveBIO()\n-> bytes = self._tlsConnection.recv(2 ** 15)\n  /root/scraper/lib/python3.6/site-packages/OpenSSL/SSL.py(1783)recv()\n-> self._raise_ssl_error(self._ssl, result)\n> /root/scraper/lib/python3.6/site-packages/OpenSSL/SSL.py(1632)_raise_ssl_error()\n-> raise SysCallError(-1, \"Unexpected EOF\")\n(Pdb) exit\nHere is some info about my environment:\nscrapy version -v\nScrapy       : 1.5.1\nlxml         : 4.2.5.0\nlibxml2      : 2.9.8\ncssselect    : 1.0.3\nparsel       : 1.5.1\nw3lib        : 1.19.0\nTwisted      : 18.9.0\nPython       : 3.6.7 (default, Oct 22 2018, 11:32:17) - [GCC 8.2.0]\npyOpenSSL    : 18.0.0 (OpenSSL 1.1.0j  20 Nov 2018)\ncryptography : 2.4.2\nPlatform     : Linux-4.15.0-36-generic-x86_64-with-Ubuntu-18.04-bionic\npip freeze\nasn1crypto==0.24.0\nattrs==18.2.0\nAutomat==0.7.0\ncffi==1.11.5\nconstantly==15.1.0\ncryptography==2.4.2\ncssselect==1.0.3\nhyperlink==18.0.0\nidna==2.8\nincremental==17.5.0\nlxml==4.2.5\nparsel==1.5.1\npkg-resources==0.0.0\npyasn1==0.4.4\npyasn1-modules==0.2.2\npycparser==2.19\nPyDispatcher==2.0.5\nPyHamcrest==1.9.0\npyOpenSSL==18.0.0\nqueuelib==1.5.0\nScrapy==1.5.1\nservice-identity==18.1.0\nsix==1.12.0\nTwisted==18.9.0\nw3lib==1.19.0\nNo error is reported when I run the scrapy fetch command without the --pdb option. Also, fetching that page works fine with requests 2.21.0 and curl.", "issue_status": "Open", "issue_reporting_time": "2018-12-27T11:23:33Z"}, "112": {"issue_url": "https://github.com/scrapy/scrapy/issues/3537", "issue_id": "#3537", "issue_summary": "Document Scheduler API", "issue_description": "Member\nelacuesta commented on Dec 20, 2018\nThe Scheduler component can be overridden with the SCHEDULER setting, but its interface is not documented.\n\ud83d\udc4d 2\n\ud83c\udf89 2", "issue_status": "Open", "issue_reporting_time": "2018-12-20T14:43:07Z"}, "113": {"issue_url": "https://github.com/scrapy/scrapy/issues/3529", "issue_id": "#3529", "issue_summary": "SitemapSpider memory issues", "issue_description": "altunyurt commented on Dec 11, 2018\nI'm using SitemapSpider on a sitemapindex consisting of 20-30 sitemaps each having 50k urls.\nEven trying each sitemap alone ends up eating all the memory on a 6gb machine, let alone the millions of urls in total of all the sitemaps in the index.\nIIRC, the parser keeps the documents in memory during the operation. So I've monkey patched the scrapy.utils.Sitemap with the following snippet. The original code is at https://stackoverflow.com/a/12161078\n# patching scrapy/utils/sitemap.py\nimport lxml.etree\n\n\nclass Sitemap(object):\n    \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n    (type=sitemapindex) files\"\"\"\n\n    def __init__(self, xmltext):\n        self.type = \"urlset\" if \"urlset\" in xmltext[:100] else \"sitemapindex\"\n\n        tag = {\"urlset\": \"url\", \"sitemapindex\": \"sitemap\"}[self.type]\n        self._root = lxml.etree.iterparse(\n            xmltext,\n            tag=tag,\n            event=(\"end\",),\n            recover=True,\n            remove_comments=True,\n            resolve_entities=False,\n        )\n\n    def __iter__(self):\n\n        for event, elem in self._root:\n            loc = elem.find(\"{*}loc\")\n            if loc is None:\n                continue\n            yield {\"loc\": loc.text}\n            # It's safe to call clear() here because no descendants will be\n            # accessed\n            elem.clear()\n            # Also eliminate now-empty references from the root node to elem\n            for ancestor in elem.xpath(\"ancestor-or-self::*\"):\n                while ancestor.getprevious() is not None:\n                    del ancestor.getparent()[0]\n        del self._root\nand using it as follows allowed me to run the task much smoothly, utilizing at most 3gb at any time.\nworker.py \n-------------------------\nimport scrapy\nfrom core.patches.sitemap import Sitemap\n\n__all__ = [\"Spider\"]\n\nscrapy.spiders.Sitemap = Sitemap\n\n\nclass Spider(scrapy.spiders.SitemapSpider):\n    sitemap_urls = [ sitemap1, sitemap2 ...  ]\n\n    def parse(self, response):\n    ...\nI can create a pull request if this is ok.", "issue_status": "Open", "issue_reporting_time": "2018-12-11T17:28:37Z"}, "114": {"issue_url": "https://github.com/scrapy/scrapy/issues/3511", "issue_id": "#3511", "issue_summary": "Translating the docs", "issue_description": "Maransatto commented on Nov 29, 2018\nHi folks,\nI've been talking to @lopuhin and we have decide to start a translation project for the docs to Portuguese so that we can increase the community here in Brazil.\nAt first he suggested me using the RTD itself (Read the docs), and he's also considering using transifex (https://www.transifex.com ).\nI've read a little about both scenarios and I think we could start by creating multiple languages with ReadtheDocs once this is the first localization project.\nhttps://docs.readthedocs.io/en/latest/localization.html\nWell, I'd like to know your opinion.\nRight now I'm forking the project to my own github account to start to understand how it works (RTD).\nhttps://github.com/Maransatto/scrapy\nI don't actually know how to merge it from my project to this one, or...\nif the admins here decide to give me some permission I can create a branch for this.\nObs: I also have 3 friends here in Brazil that will help me to make the translation.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-11-28T22:38:26Z"}, "115": {"issue_url": "https://github.com/scrapy/scrapy/issues/3510", "issue_id": "#3510", "issue_summary": "should scrapy catch the exception that timeout raised by itself to \"ignore response\" or something instead Traceback (most recent call last):?", "issue_description": "Contributor\nNewUserHa commented on Nov 28, 2018\nsetting:\n'DOWNLOAD_TIMEOUT': 6,\nspider:\n        yield scrapy.Request('https://httpbin.org/delay/20', self.parse, priority=1, dont_filter=True)\n2018-11-28 11:11:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://httpbin.org/delay/20> (failed 1 times): User timeout caused connection failure: Getting https://httpbin.org/delay/20 took longer than 6.0 seconds..\n2018-11-28 11:11:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://httpbin.org/delay/20> (failed 2 times): User timeout caused connection failure: Getting https://httpbin.org/delay/20 took longer than 6.0 seconds..\n2018-11-28 11:11:52 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://httpbin.org/delay/20> (failed 3 times): User timeout caused connection failure: Getting https://httpbin.org/delay/20 took longer than 6.0 seconds..\n.......................................no\n2018-11-28 11:11:52 [scrapy.core.scraper] ERROR: Error downloading <GET https://httpbin.org/delay/20>\nTraceback (most recent call last):\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1416, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\twisted\\python\\failure.py\", line 491, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\twisted\\internet\\defer.py\", line 654, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 351, in _cb_timeout\n    raise TimeoutError(\"Getting %s took longer than %s seconds.\" % (url, timeout))\ntwisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://httpbin.org/delay/20 took longer than 6.0 seconds..", "issue_status": "Open", "issue_reporting_time": "2018-11-28T03:15:53Z"}, "116": {"issue_url": "https://github.com/scrapy/scrapy/issues/3506", "issue_id": "#3506", "issue_summary": "retry middleware add EOFError", "issue_description": "baby5 commented on Nov 26, 2018\n[Failure instance: Traceback: <class 'EOFError'>: Compressed file ended before the end-of-stream marker was reached\ni got this log from scrapy, i deal with it by adding EOFError to retry middleware EXCEPTIONS_TO_RETRY\nmanually\nin deep, provide a setting for user to custom retry error maybe better", "issue_status": "Open", "issue_reporting_time": "2018-11-26T03:14:55Z"}, "117": {"issue_url": "https://github.com/scrapy/scrapy/issues/3487", "issue_id": "#3487", "issue_summary": "scrapy should have own base exception class", "issue_description": "baby5 commented on Nov 12, 2018\ni saw the code in exceptions.py\nthe exceptions all base on bulid-in Exception class\nwhen i need separate scrapy\u2018s exceptions from others, there is not a simple way", "issue_status": "Open", "issue_reporting_time": "2018-11-12T07:57:33Z"}, "118": {"issue_url": "https://github.com/scrapy/scrapy/issues/3486", "issue_id": "#3486", "issue_summary": "Adding delimiter, quotechar, encoding, lineterminator options to the CSV exporter", "issue_description": "maaaaz commented on Nov 11, 2018\nHello there,\nCould you add the support of delimiter, quotechar, encoding, lineterminator options for the csvexporter feed exporter ?\nThese are common CSV format options.\nBest regards.\nThomas.", "issue_status": "Open", "issue_reporting_time": "2018-11-11T13:41:00Z"}, "119": {"issue_url": "https://github.com/scrapy/scrapy/issues/3478", "issue_id": "#3478", "issue_summary": "Scrapy mail.send error", "issue_description": "Kevinsss commented on Oct 30, 2018 \u2022\nedited\nHi, I'm new to scrapy and I want to send some emails after the spider closed. But I got some errors, anyone know ? I'm using python2.7 and scrapy 1.5.1.\nHere are my codes:\nclass AlertSpider(scrapy.Spider):\n    name = \"alert\"\n    start_urls = ['http://www.test.com']\n    mails = []\n\n    def parse(self, response):\n        # Do something work\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        spider = cls()\n        crawler.signals.connect(spider.spider_closed, signals.spider_closed)\n        return spider\n\n    def spider_closed(self, spider):\n        settings = get_project_settings()\n        mailer = MailSender.from_settings(settings)\n       # first e-mail\n        mailer.send(to=[\"xxxx@gmail.com\"], subject='subject1', body='body1')\n       # second e-mail\n        return mailer.send(to=[\"xxxx@gmail.com\"], subject='subject2', body='body2')\nI want to send two e-mails after the spider close, but I get below errors:\n(By the way, there is no problem if I just send one e-mail)\nFile \"C:\\Software\\Python27\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 149, in _doReadOrWrite why = getattr(selectable, method)() File \"C:\\Software\\Python27\\lib\\site-packages\\twisted\\internet\\tcp.py\", line 243, in doRead return self._dataReceived(data) File \"C:\\Software\\Python27\\lib\\site-packages\\twisted\\internet\\tcp.py\", line 249, in _dataReceived rval = self.protocol.dataReceived(data) File \"C:\\Software\\Python27\\lib\\site-packages\\twisted\\protocols\\tls.py\", line 330, in dataReceived self._flushReceiveBIO() File \"C:\\Software\\Python27\\lib\\site-packages\\twisted\\protocols\\tls.py\", line 300, in _flushReceiveBIO self._flushSendBIO() File \"C:\\Software\\Python27\\lib\\site-packages\\twisted\\protocols\\tls.py\", line 252, in _flushSendBIO bytes = self._tlsConnection.bio_read(2 ** 15) exceptions.AttributeError: 'NoneType' object has no attribute 'bio_read'\nIt seems to the twisted doesn't close the io, but I don't find any close method in MailSender class,\nso anyone have met this error?", "issue_status": "Open", "issue_reporting_time": "2018-10-30T00:48:42Z"}, "120": {"issue_url": "https://github.com/scrapy/scrapy/issues/3477", "issue_id": "#3477", "issue_summary": "Make requests via message queues", "issue_description": "octohedron commented on Oct 30, 2018 \u2022\nedited\nI'm trying to pass requests to the spider externally, via message queues, and keep it running forever.\nI found some projects made by others but none of them work for the current version of scrapy, so I'm trying to fix the issues or just find a way to do it myself.\nSo far, I found that others got a reference to the scheduler from the spider in the middleware, like\nclass MyMiddleware(object):\n    # [...]\n    def ensure_init(self, spider):\n        self.spider = spider\n        self.scheduler = spider.crawler.engine.slot.scheduler\n    # [...]\n    def process_response(self, request, response, spider):\n        self.ensure_init(spider)\n        return response\nThen, in another, custom \"Scheduler\" class\nclass MyScheduler(object):\n    # [...]\n    def open(self, spider):\n        self.spider = spider\n    # [...]\n    def next_request(self):\n        return self.spider.make_request(page)\nAnd then, in the spider\nclass TestSpider(scrapy.Spider):\n    # [...]\n    def make_request(self, page):\n        # This works, i.e. prints \"Making request\"\n        logging.info(\"Making request\")\n        yield scrapy.Request(url=page, callback=self.parse)\n    # [...]\n    def parse(self, response):\n        # This never gets printed\n        logging.info(\"Got response\")\nThis is the simplest example I could put together, but you get the idea, the code in reality is very messy, which makes it harder to fix.\nThe issue is, that in theory it should work, although I don't know when is that next_request method called, but it does because it calls the make_request method in the spider, the only problem is that it never gets to the parse or callback method in the spider, I don't know why.\nI also tried connecting the spider to the message queue directly in the spider, which should work, but it doesn't, for example\nimport pika\n\nclass TestSpider(scrapy.Spider):\n        rbmqrk = 'test'\n        rmq = pika.BlockingConnection(\n            pika.ConnectionParameters(host='localhost'))\n        # Init channel\n        rmqc = rmq.channel()\n\n    def __init__(self, *args, **kwargs):\n        super(TestSpider, self).__init__(*args, **kwargs)\n        self.rmqc.queue_declare(queue=self.rbmqrk)\n\n    def start_requests(self):\n        self.rmqc.basic_consume(self.callback, self.rbmqrk)\n\n    def callback(channel, method_frame, header_frame, body):\n        # This gets printed! It works up to here.\n        logging.info(body)\nUp to there, everything is working fine, the body of the message received from the queue gets printed.\nBut, if we try to make or yield a request from the callback method in the spider, it won't work, for example\nclass TestSpider(scrapy.Spider):\n       # [...] same as above...\n\n    def start_requests(self):\n        # Same as above\n        self.rmqc.basic_consume(self.callback, self.rbmqrk)\n\n    def callback(channel, method_frame, header_frame, body):\n        # This DOESN'T get printed\n        logging.info(body)\n        yield scrapy.Request(url=body.decode(), callback=self.parse)\n        # This wouldn't work either (already tried as well)\n        # return scrapy.Request(url=body.decode(), callback=self.parse)\n\n    def prase(self, response):\n        # We never get here, this doesn't get printer either\n        logging.info(\"Got response\")\nUnfortunately, yielding a generator or a request from the callback is not making a spider request.\nAs you can see, I've tried several things without luck, but all I need to do is to be able to make requests with the messages from the message queue, I'm not sure if there's a bug in scrapy or there's something I can fix in the code but I would love to have some input on this before I start digging deeper into the scrapy code myself.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-10-29T20:54:44Z"}, "121": {"issue_url": "https://github.com/scrapy/scrapy/issues/3463", "issue_id": "#3463", "issue_summary": "raise CloseSpider(\"Error Message\") is not saving error message", "issue_description": "parlays commented on Oct 16, 2018 \u2022\nedited\nWhen I run the code raise CloseSpider(\"Error Message\") in the Spider parse function, the reason gets saved in the finish_reason stats string.\nWhen it is run in the start_requests function, the spider properly closes but the finish_reason does not have the message it in. The finish_reason value is \"finished\" instead of \"Error Message\" as set in the CloseSpider exception.\nIs this intentional or a bug? thanks!\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-10-15T23:42:08Z"}, "122": {"issue_url": "https://github.com/scrapy/scrapy/issues/3439", "issue_id": "#3439", "issue_summary": "mailer.send(mimetype='text/html') does not work along with \u201cattachs\u201d", "issue_description": "mani619cash commented on Oct 1, 2018\nMy code is creating CSV files and attaching it in email\nI am using Gmail SMTP\nHere is related code\nmailer = MailSender(smtphost='smtp.gmail.com', \n    mailfrom='noreply@domain.io', \n    smtpuser='email@gmail.com', \n    smtppass='PASS', smtpport=587)\n\n\nattachs.append((\"file.csv\", \"text/csv\", csv_buffer))\n\nself.mailer.send(\n    to=self.mailto,\n    mimetype='text/html',\n    subject=\"Crawler to scrape RELATED domains finished\",\n    body=\"Please <b>download</b> attached files<br /><br />Crawler Stats<br />\" ,\n    attachs=attachs\n   )\nI simply get raw HTML instead of rendered.\nIf I remove attachs param from send then I get rendered email\nI also tried\nattachs.append((\"file.csv\", \"text/html\", csv_buffer))\nbut still I get raw html instead of rendered one.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-10-01T11:01:51Z"}, "123": {"issue_url": "https://github.com/scrapy/scrapy/issues/3416", "issue_id": "#3416", "issue_summary": "How to create JOBDIR setting from Spider __init__ method ? or dynamically", "issue_description": "lxmn commented on Sep 7, 2018 \u2022\nedited\nI want to create JOBDIR setting from Spider __init__ or dynamically when I call that spider .\nI want to create different JOBDIR for different spiders , like FEED_URI in the below example\nclass QtsSpider(scrapy.Spider):\n     \n         name = 'qts'\n     \n         custom_settings = {\n      'FEED_URI': 'data_files/' + '%(site_name)s.csv',\n      'FEED_FORMAT': \"csv\",\n      # 'JOBDIR' : 'resume/' + '%(site_name)s'\n     }\n     \n         allowed_domains = ['quotes.toscrape.com']\n     start_urls = ['http://quotes.toscrape.com']\n\n\n     def __init__(self, **kw):\n      super(QtsSpider, self).__init__(**kw)\n      self.site_name = kw.get('site_name')\n\n     def parse(self, response):\n      for quote in response.css(\"div.quote\"):\nand I am calling that spider from this script\n    from scrapy.crawler import CrawlerProcess\n    from scrapy.utils.project import get_project_settings\n    \n    def main_function():\n        all_spiders = ['spider1','spider2']\n        process = CrawlerProcess(get_project_settings())\n        for spider_name in all_spiders:\n            process.crawl('qts', site_name = spider_name )\n\n        process.start()\n\n    main_function()\nHow to achieve that dynamic creation of JOBDIR for different Spider like FEED_URI ?? Help will be appreciated.", "issue_status": "Open", "issue_reporting_time": "2018-09-07T07:31:52Z"}, "124": {"issue_url": "https://github.com/scrapy/scrapy/issues/3412", "issue_id": "#3412", "issue_summary": "allowed_domains: Allow only root domain and no subdomains", "issue_description": "ZakariaAhmed commented on Sep 3, 2018\nWe are trying to configure the allowed_domains list to only include the root domain and not any subdomains. As of now it doesn't seem possible.\nDesired behavior\nOK to crawl:\nhttp://example.com\nShouldn't be crawled:\nhttp://www.example.com\nhttp://ww2.example.com\nhttp://subdomain1.example.com\nThe following configuration allows root domain and ALL subdomains:\nallowed_domains = ['example.com']\nA solution that only allows the root domains should be added :)\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2018-09-03T11:42:58Z"}, "125": {"issue_url": "https://github.com/scrapy/scrapy/issues/3407", "issue_id": "#3407", "issue_summary": "Can't get partial response body after timeout", "issue_description": "c3pmark commented on Aug 29, 2018\nI'm working on scraping some pages that hold the HTTP connection open indefinitely, trickling data to the client little by little. I had hoped to use a short timeout to grab the page up to a certain point, but after the timeout is triggered, I can't get at the data that was received. I thought that setting DOWNLOAD_FAIL_ON_DATALOSS=False would allow partial responses to be processed, but I still get twisted.internet.error.TimeoutError: User timeout caused connection failure. If I use errback= on the request, the errback just gets the TimeoutError object and not the response.\nShould setting DOWNLOAD_FAIL_ON_DATALOSS=True allow timed out responses to be processed? Or is there another way to deal with partial responses after a timeout?", "issue_status": "Open", "issue_reporting_time": "2018-08-28T20:05:04Z"}, "126": {"issue_url": "https://github.com/scrapy/scrapy/issues/3404", "issue_id": "#3404", "issue_summary": "Support kwargs for loader.*_xpath()", "issue_description": "alexander-matsievsky commented on Aug 23, 2018 \u2022\nedited\nAllow passing named variables and namespaces dict arguments on ItemLoader.*_xpath() family of methods (add_xpath, etc.).\nRelated #2457.", "issue_status": "Open", "issue_reporting_time": "2018-08-23T14:41:10Z"}, "127": {"issue_url": "https://github.com/scrapy/scrapy/issues/3396", "issue_id": "#3396", "issue_summary": "Set DNS Servers to use", "issue_description": "mohmad-null commented on Aug 19, 2018\nI'm not sure if this is a docs issue (as in, the setting exists but doesn't seem to be documented) or a request for a new setting.\nBasically - I'd like to be able to set what DNS Servers Scrapy should use. It currently uses the system servers, but I'd like it to use a different set and not have to change my system DNS whenever I do a scrape.\nThere is a section of the docs which implies it's possible to set the DNS server to use, but it doesn't say how - https://doc.scrapy.org/en/latest/topics/broad-crawls.html#setup-your-own-dns (I have no particular interest in setting up my own DNS. 8.8.8.8 or its ilk will be sufficient for a scrape).", "issue_status": "Open", "issue_reporting_time": "2018-08-19T16:51:35Z"}, "128": {"issue_url": "https://github.com/scrapy/scrapy/issues/3386", "issue_id": "#3386", "issue_summary": "Python zipapp .pyz archive - problem with loading scrapy", "issue_description": "Sonique commented on Aug 14, 2018 \u2022\nedited\nGet error when trying to run python package *.pyz builded with zipapp.\nFile \"app.pyz/scrapy/__init__.py\", line 10, in <module>\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/pkgutil.py\", line 619, in get_data\n    spec = importlib.util.find_spec(package)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/util.py\", line 90, in find_spec\n    fullname = resolve_name(name, package) if name.startswith('.') else name\nAttributeError: 'NoneType' object has no attribute 'startswith'\nLooks like relates to: https://stackoverflow.com/questions/25557693/pyinstaller-scrapy-error", "issue_status": "Open", "issue_reporting_time": "2018-08-14T08:02:29Z"}, "129": {"issue_url": "https://github.com/scrapy/scrapy/issues/3385", "issue_id": "#3385", "issue_summary": "scrapy check command exports output", "issue_description": "Contributor\nStasDeep commented on Aug 13, 2018\nWhen you run scrapy check <spider>, the command will create an empty output file.\nShould pipelines be disabled by default for contracts?", "issue_status": "Open", "issue_reporting_time": "2018-08-13T14:37:11Z"}, "130": {"issue_url": "https://github.com/scrapy/scrapy/issues/3354", "issue_id": "#3354", "issue_summary": "Customize schemes for download handlers.", "issue_description": "dingld commented on Jul 26, 2018 \u2022\nedited\nscrapy.core.downloader.handlers.DownloadHandlers gets the handler via scheme parsed from request.url.\ndef download_request(self, request, spider):\n    scheme = urlparse_cached(request).scheme\n    handler = self._get_handler(scheme)\nFor schemes that are present directly in request.url, it is quite convenient to implement and direct to understand, say file, http(s), s3.\nHowever, things become obscure when you access request.url via a customized way, say, render service from splash via scrapy_splash's middlewares\nintercept original request, and post to splash\nintercept splash's response, link it to the original request\nadd new cookiejar\nadd customized download slot\nadd new fingerprint algorithm\nadd new cache storage\nThe first two steps work just like a customized download handler of a non-standard scheme - splash.\nIt does make sense to regard splash as a transparent downloader of its own scheme other than a rendering service. It might be more friendly if an interface is provided in the first place. That would save much work.\nI have noticed that similar logic already exists in scrapy.core.downloader.Downloader, to retrive download_slot other than parse directly\n    def _get_slot_key(self, request, spider):\n        if 'download_slot' in request.meta:\n            return request.meta['download_slot']\n\n        key = urlparse_cached(request).hostname or ''\n        if self.ip_concurrency:\n            key = dnscache.get(key, key)\n\n        return key\nI guess the same goes for download scheme.", "issue_status": "Open", "issue_reporting_time": "2018-07-26T03:30:46Z"}, "131": {"issue_url": "https://github.com/scrapy/scrapy/issues/3345", "issue_id": "#3345", "issue_summary": "Allow Scrapy to send e-mails using Amazon SES", "issue_description": "Contributor\nrennerocha commented on Jul 20, 2018\nI need to create an e-mail report and send it using Amazon SES. Scrapy only has scrapy.email.MailSender class that only allows to send e-mails using a SMTP server.\nConsidering that Scrapy already has integration for some Amazon services (like S3 storage), I would like to suggest the creation of scrapy.email.SESMailSender with the same interface of MailSender (with the expected differences, like providing AWS keys instead of SMTP host).", "issue_status": "Open", "issue_reporting_time": "2018-07-19T21:41:57Z"}, "132": {"issue_url": "https://github.com/scrapy/scrapy/issues/3343", "issue_id": "#3343", "issue_summary": "Memusage extension AssertionError", "issue_description": "nadzimo commented on Jul 19, 2018 \u2022\nedited\nUsing scrapy 1.5.0. I was testing the default configuration of the scrapy.extensions.memusage.MemoryUsage extension which is enabled by default. In settings.py I set MEMUSAGE_LIMIT_MB = 8, a low number to test it out so it gets triggered.\nThen I run scrapy crawl myspider and the process gives an AssertionError. The extension keeps checking every minute the memory usage and repeats the message ERROR: Memory usage exceeded 8M. Shutting down Scrapy...\n2018-07-19 19:05:53 [scrapy.extensions.memusage] ERROR: Memory usage exceeded 8M. Shutting down Scrapy...\n2018-07-19 19:05:53 [scrapy.core.engine] INFO: Closing spider (memusage_exceeded)\n2018-07-19 19:05:53 [myspider] INFO: Spider closed: myspider because memusage_exceeded\n2018-07-19 19:05:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'finish_reason': 'memusage_exceeded',\n 'finish_time': datetime.datetime(2018, 7, 19, 15, 5, 53, 236830),\n 'log_count/DEBUG': 1,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 10,\n 'memusage/limit_reached': 1,\n 'memusage/max': 78512128,\n 'memusage/startup': 78512128,\n 'start_time': datetime.datetime(2018, 7, 19, 15, 5, 53, 235177)}\n2018-07-19 19:05:53 [scrapy.core.engine] INFO: Spider closed (memusage_exceeded)\nUnhandled error in Deferred:\n2018-07-19 19:05:53 [twisted] CRITICAL: Unhandled error in Deferred:\n\n2018-07-19 19:05:53 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"/Users/myuser/Dropbox/Local-backup/my-app/app/venv-app/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/Users/myuser/Dropbox/Local-backup/my-app/app/venv-app/lib/python3.6/site-packages/scrapy/core/engine.py\", line 85, in stop\n    assert self.running, \"Engine not running\"\nAssertionError: Engine not running\n2018-07-19 19:06:53 [scrapy.extensions.memusage] ERROR: Memory usage exceeded 8M. Shutting down Scrapy...\n2018-07-19 19:07:53 [scrapy.extensions.memusage] ERROR: Memory usage exceeded 8M. Shutting down Scrapy...\n2018-07-19 19:08:53 [scrapy.extensions.memusage] ERROR: Memory usage exceeded 8M. Shutting down Scrapy...", "issue_status": "Open", "issue_reporting_time": "2018-07-19T15:10:06Z"}, "133": {"issue_url": "https://github.com/scrapy/scrapy/issues/3329", "issue_id": "#3329", "issue_summary": "Whether or not Scrapy supports headers in CONNECT method?", "issue_description": "shadow-ru commented on Jul 10, 2018\nHi. I read the docs and checked here, but I haven't found the answer yet.\nConnect headers are useful to modify proxy behavior with HTTPS requests:\nhttps://docs.proxymesh.com/article/7-request-response-headers#https", "issue_status": "Open", "issue_reporting_time": "2018-07-10T11:22:01Z"}, "134": {"issue_url": "https://github.com/scrapy/scrapy/issues/3321", "issue_id": "#3321", "issue_summary": "Scrapy fails to fetch request with invalid hostname", "issue_description": "Contributor\npawelmhm commented on Jul 6, 2018\nI have url with invalid hostname - it does not match IDNA standards. Scrapy fails with that.\nscrapy fetch \"https://mediaworld_it_api2.frosmo.com/?method=products&products=[%22747190%22]\"\n\n2018-07-06 11:53:09 [scrapy.core.scraper] ERROR: Error downloading <GET https://mediaworld_it_api2.frosmo.com/?method=products&products=[%22747190%22]>\nTraceback (most recent call last):\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1384, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/twisted/python/failure.py\", line 408, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/home/pawel/scrapy/scrapy/core/downloader/middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\n  File \"/home/pawel//scrapy/scrapy/utils/defer.py\", line 45, in mustbe_deferred\n    result = f(*args, **kw)\n  File \"/home/pawel/scrapy/scrapy/core/downloader/handlers/__init__.py\", line 65, in download_request\n    return handler.download_request(request, spider)\n  File \"/home/pawel/scrapy/scrapy/core/downloader/handlers/http11.py\", line 67, in download_request\n    return agent.download_request(request)\n  File \"/home/pawelscrapy/scrapy/core/downloader/handlers/http11.py\", line 331, in download_request\n    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/twisted/web/client.py\", line 1649, in request\n    endpoint = self._getEndpoint(parsedURI)\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/twisted/web/client.py\", line 1633, in _getEndpoint\n    return self._endpointFactory.endpointForURI(uri)\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/twisted/web/client.py\", line 1510, in endpointForURI\n    uri.port)\n  File \"/home/pawel/scrapy/scrapy/core/downloader/contextfactory.py\", line 59, in creatorForNetloc\n    return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext())\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1152, in __init__\n    self._hostnameBytes = _idnaBytes(hostname)\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/twisted/internet/_idna.py\", line 30, in _idnaBytes\n    return idna.encode(text)\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/idna/core.py\", line 355, in encode\n    result.append(alabel(label))\n  File \"/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/idna/core.py\", line 265, in alabel\n    raise IDNAError('The label {0} is not a valid A-label'.format(label))\nIDNAError: The label mediaworld_it_api2 is not a valid A-label\nIDNA error is legitmate. This url https://mediaworld_it_api2.frosmo.com/?method=products&products=[%22747190%22] is not valid according to IDNA standard.\nIn [1]: x = \"https://mediaworld_it_api2.frosmo.com/?method=products&products=[%22747190%22]\"\n\nIn [2]: import idna\n\nIn [3]: idna.encode(x)\n---------------------------------------------------------------------------\nIDNAError                                 Traceback (most recent call last)\n<ipython-input-3-c97070e17b57> in <module>()\n----> 1 idna.encode(x)\n\n/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/idna/core.pyc in encode(s, strict, uts46, std3_rules, transitional)\n    353         trailing_dot = True\n    354     for label in labels:\n--> 355         result.append(alabel(label))\n    356     if trailing_dot:\n    357         result.append(b'')\n\n/home/pawel/.virtualenvs/scrapy/local/lib/python2.7/site-packages/idna/core.pyc in alabel(label)\n    263             ulabel(label)\n    264         except IDNAError:\n--> 265             raise IDNAError('The label {0} is not a valid A-label'.format(label))\n    266         if not valid_label_length(label):\n    267             raise IDNAError('Label too long')\n\nIDNAError: The label https://mediaworld_it_api2 is not a valid A-label\nHow should scrapy handle this url? Should we download it regardless of validity?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-07-06T09:58:06Z"}, "135": {"issue_url": "https://github.com/scrapy/scrapy/issues/3306", "issue_id": "#3306", "issue_summary": "[RFE] FilesPipeline checksum algorithm should be configurable", "issue_description": "danielkza commented on Jun 24, 2018\nAm I scraping a website that provides downloads with their corresponding SHA256 checksums. Unfortunately, to properly check them I need to manually calculate the file checksums, even though the FilePipeline already calculates a (useless, in this case) MD5 checksum.\nIt would be nice to be able to define which checksum algorithm is used, so that it can be compared to known correct checksums provided externally.", "issue_status": "Open", "issue_reporting_time": "2018-06-24T07:05:35Z"}, "136": {"issue_url": "https://github.com/scrapy/scrapy/issues/3297", "issue_id": "#3297", "issue_summary": "Crawled (200) then returns Error downloading the page that was just crawled?", "issue_description": "daffychuy commented on Jun 17, 2018 \u2022\nedited by kmike\nWhile I was writing a script to get links from the page when I run it, it seemed fine at first.\nIt crawled the page given with response (200) so that means page worked, but after 1 second, it returns Error downloading the page and keyError gives a <GET URL>.\nPage that scrapy said gave an error: https://www.masterani.me/api/anime/search?search=M%C3%A4rchen%20M%C3%A4dchen%20Specials\nbut actually has an json return and keyError doesn't make sense to me\nHere's the log:\nhttps://gist.github.com/daffychuy/aa0a6420d36cde138dbfcae64fd85cc9\nIt then keeps repeating the same error and gives the same for the next couple of pages as well.\nEDIT: I've moved the log to a gist page", "issue_status": "Open", "issue_reporting_time": "2018-06-16T18:41:51Z"}, "137": {"issue_url": "https://github.com/scrapy/scrapy/issues/3295", "issue_id": "#3295", "issue_summary": "Download Middleware HTTPCache pickle error", "issue_description": "yvmarques commented on Jun 15, 2018\nRecently I found that my crawler is generating some errors with the HTTPCache extension.\nHere is the config of HTTPCache:\nHTTPCACHE_ENABLED = True\nHTTPCACHE_EXPIRATION_SECS = 0\nHTTPCACHE_DIR = 'httpcache'\nHTTPCACHE_IGNORE_HTTP_CODES = []\nHTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.DbmCacheStorage'\nHTTPCACHE_POLICY = 'scrapy.extensions.httpcache.RFC2616Policy'\nAnd here is one error I got\nTraceback (most recent call last):\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\n    result = g.send(result)\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 37, in process_request\n    response = yield method(request=request, spider=spider)\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/scrapy/downloadermiddlewares/httpcache.py\", line 50, in process_request\n    cachedresponse = self.storage.retrieve_response(spider, request)\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/scrapy/extensions/httpcache.py\", line 233, in retrieve_response\n    data = self._read_data(spider, request)\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/scrapy/extensions/httpcache.py\", line 266, in _read_data\n    return pickle.loads(db['%s_data' % key])\n_pickle.UnpicklingError: pickle data was truncated\nTraceback (most recent call last):\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\n    result = g.send(result)\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 37, in process_request\n    response = yield method(request=request, spider=spider)\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/scrapy/downloadermiddlewares/httpcache.py\", line 50, in process_request\n    cachedresponse = self.storage.retrieve_response(spider, request)\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/scrapy/extensions/httpcache.py\", line 233, in retrieve_response\n    data = self._read_data(spider, request)\n  File \"/root/.local/share/virtualenvs/lib/python3.6/site-packages/scrapy/extensions/httpcache.py\", line 266, in _read_data\n    return pickle.loads(db['%s_data' % key])\nEOFError: Ran out of input\nMy feeling is that my cache database got corrupted somehow or the fact that I have my crawler working for a month and I never cleaned the cache database.", "issue_status": "Open", "issue_reporting_time": "2018-06-14T20:39:13Z"}, "138": {"issue_url": "https://github.com/scrapy/scrapy/issues/3288", "issue_id": "#3288", "issue_summary": "Deprecation Warning of Twisted", "issue_description": "Aqua-Dream commented on Jun 11, 2018\nMay you do a little modification to avoid these two warnings?\n[py.warnings] WARNING: /.../scrapy/core/downloader/webclient.py:4: DeprecationWarning: twisted.web.client.HTTPClientFactory was deprecated in Twisted 16.7.0: please use https://pypi.org/project/treq/ or twisted.web.client.Agent instead\nfrom twisted.web.client import HTTPClientFactory\n[py.warnings] WARNING: /.../scrapy/core/downloader/contextfactory.py:51: DeprecationWarning: Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\nacceptableCiphers=DEFAULT_CIPHERS)", "issue_status": "Open", "issue_reporting_time": "2018-06-11T10:11:24Z"}, "139": {"issue_url": "https://github.com/scrapy/scrapy/issues/3285", "issue_id": "#3285", "issue_summary": "about setting depth", "issue_description": "MichaelEvil commented on Jun 9, 2018\nIs the depth information written in response.meta['depth']?\nI set DEPTH_LIMIT to 5, and it works. Now I want to make specific Requests ignore this limit(for example, requests about turning page)\uff0cso I try it in this way(assuming this response depth is 4):\nyield scrapy.Request(url=\u201cnext_page_url\u201d, headers=self.headers, callback=self.parse, meta={'depth' : response.meta['depth']-1})\nbut it doesn't work, and this request would be ignored by scrapy engine normally.\nIs there any way to realize my idea? Or is it necessary to use a overwritten depthmidderware? thanks.", "issue_status": "Open", "issue_reporting_time": "2018-06-09T10:04:08Z"}, "140": {"issue_url": "https://github.com/scrapy/scrapy/issues/3276", "issue_id": "#3276", "issue_summary": "[question] Why adding `dont_filter=True` for requests made from `start_urls`", "issue_description": "Contributor\nstarrify commented on May 28, 2018\nIt's observed that currently (as of b364d27) in scrapy.Spider.start_requests the generated requests have dont_filter=True. (related line of code: link)\nAs I've had a quick look through the history, this behavior of adding dont_filter=True to initial requests seems to have been introduced in the very initial commit of 83dcf8a (related line of code: link).\nAt least from my personal perspective of view, there seems to be no reasonable purpose for this specific behavior.\nCould anyone help explaining the design and resolving my confusion? Or, if there's no such reason (or the previous reasons do not hold meaningful now), what about removing the dont_filter=True attribute for initial requests?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-05-28T14:20:51Z"}, "141": {"issue_url": "https://github.com/scrapy/scrapy/issues/3273", "issue_id": "#3273", "issue_summary": "LxmlLinkExtractor unique_list missing key", "issue_description": "nikan1996 commented on May 25, 2018\nscrapy/scrapy/linkextractors/lxmlhtml.py\nLine 130 in b364d27\n return unique_list(all_links) \n\nI think it should behave like\nscrapy/scrapy/linkextractors/lxmlhtml.py\nLine 91 in b364d27\n return unique_list(links, key=self.link_key) \n\nunique the link by its url.", "issue_status": "Open", "issue_reporting_time": "2018-05-24T19:00:49Z"}, "142": {"issue_url": "https://github.com/scrapy/scrapy/issues/3272", "issue_id": "#3272", "issue_summary": "KeyError with the initialization of an Item Field defined with None using ItemLoader", "issue_description": "NadirRisc commented on May 23, 2018\nUsing Scrapy 1.5.0\nI took a look at the FAQ section and nothing was relevant about it.\nSame for issues with keyword KeyError on github, Reddit, or GoogleGroups.\nAs you can see below, it seems to me that here is an inconsistency when we load an Item or initialize it with a values as None or an empty string. First we add a value to our field (here title) through a ItemLoader. Then the loader creates an item with the load_item() method. Once it's done we can't access the field if the value was None or an empty string. The inconsistency is that the other method (initializing an Item directly with a field set to None or empty string) doesn't raise a KeyError (field is set).\nThe class TakeFirst don't return any value when they're set with None or an empty string. Which prevents the method load_item() in ItemLoader class to add an entry to the field.\nHere is a minimal source code that represents the inconsistency.\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import TakeFirst, MapCompose\n\nclass MyItem(scrapy.Item):\n    title = scrapy.Field()\n\nclass MyItemLoader(ItemLoader):\n    default_output_processor = TakeFirst()\n    title_in = MapCompose()\n\nclass MySpider(scrapy.Spider):\n    name = \"My\"\n    start_urls = ['https://blog.scrapinghub.com']\n\n    def parse(self, response):\n        titles = ['fake title', '', None]\n\n        for title in titles:\n            # First case: with ItemLoader\n            loader = MyItemLoader(item=MyItem())\n            loader.add_value('title', title)\n            loaded_item = loader.load_item()\n\n            # Second case: without ItemLoader\n            item = MyItem(title=title)\n\n            if title in ('', None):\n                # inconsistency!\n                assert not 'title' in loaded_item\n                assert 'title' in item\nWe're using Python 3.5 for our project and found the following workaround to prevent this error.\nWe introduce a new class (DefaultAwareItem) which fulfills unset fields were default metadata has been set previously.\nimport scrapy\n\nclass DefaultAwareItem(scrapy.Item):\n    \"\"\"Item class aware of 'default' metadata of its fields.\n\n    For instance to work, each field, which must have a default value, must\n    have a new `default` parameter set in field constructor, e.g.::\n\n        class MyItem(DefaultAwareItem):\n            my_defaulted_field = scrapy.Field()\n            # Identical to:\n            #my_defaulted_field = scrapy.Field(default=None)\n            my_other_defaulted_field = scrapy.Field(default='a value')\n\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        for field_name, field_metadata in self.fields.items():\n            self.setdefault(field_name, field_metadata.get('default'))\n\nclass MyItem(DefaultAwareItem):\n    title = scrapy.Field()\n    title_explicitely_set = scrapy.Field(default=\"empty title\")\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-05-23T14:47:17Z"}, "143": {"issue_url": "https://github.com/scrapy/scrapy/issues/3266", "issue_id": "#3266", "issue_summary": "Allow defining field names that don't follow the restrictions of python variables", "issue_description": "dyeray commented on May 18, 2018\nSometimes we may require defining field names that are not valid Python variables (for example containing spaces). I believe the correct way of doing this right now is that the developer needs to add a Pipeline that maps the internal name to the name we wan't to export. Scrapy could ship with a Pipeline (maybe enabled by default) that does this mapping for fields that define a field parameter. For example:\nclass MyItem(Item):\n   date_from = Field(export_name='Initial date')\nThis is a very simple pipeline. However, since this is a very common use case, I think it is something that could be shipped by default. I'm happy to provide a PR for this.\n\ud83d\udc4d 5", "issue_status": "Open", "issue_reporting_time": "2018-05-18T17:08:45Z"}, "144": {"issue_url": "https://github.com/scrapy/scrapy/issues/3259", "issue_id": "#3259", "issue_summary": "[bug?] while True in start_requests(self): make scrapy is unable to consume the yields.", "issue_description": "Contributor\nNewUserHa commented on May 13, 2018 \u2022\nedited\nI'm doing\n    def start_requests(self):\n        while 1:\n            words = read_a_list_wanna_crawl()\n            ips = get_a_ip_list()\n            if words.count() > 0:\n                for _, __ in zip(words, ips):\n                    print('do while')\n                    yield scrapy.Request(processed_url, self.html_parse, meta={'proxy': ip, ...})\nbut when len(zip(words, ips)) == 1, scrapy print do while forever(Infinite loop) and never download any requests.\nbut if len(zip(words, ips)) > 1, scrapy will not go in to infinite loop.\nis this a bug? can scrapy handle this?\nps: (another way to solve this)\nIs it able to create a fake scrapy.Request() that don't do request but do the callback to finish this kind control flow in scrapy?", "issue_status": "Open", "issue_reporting_time": "2018-05-13T15:12:25Z"}, "145": {"issue_url": "https://github.com/scrapy/scrapy/issues/3258", "issue_id": "#3258", "issue_summary": "Scrapy \"session\" extension", "issue_description": "dmsolow commented on May 11, 2018 \u2022\nedited\nI'm interested in modifying Scrapy spider behavior slightly to add some custom functionality and avoid messing around with the meta dictionary so much. Basically, the implementation I'm thinking of will be an abstract subclass of scrapy.Spider which I will call SessionSpider. The primary differences will be:\nInstead of the normal spider parse callback signature (self, response), SessionSpider will have (self, session, response) callbacks. The session argument will be some kind of Session object that at least keeps track of cookies (and possibly proxies and certain headers).\nThis will require a change in how the cookie middleware works. Instead of passing a cookie jar ID, the session will keep track of cookies directly. As a side note: does the default cookie middleware ever drop cookiejars? I could be missing something, but it looks to me like they stay around forever. This would be a problem for my spiders because I want them to run \"forever\" on an unbounded list of URLs.\nA SessionSpider callback that wants to create requests with the same session will generate requests using a session.Request factory method that returns a scrapy.Request. This method will take care of merging session variables with the new request.\nI'm hoping to implement most of the features I want by having the Session object do the meta manipulation behind the scenes so that SessionSpider subclasses don't have to touch meta as much. However, I will also have to modify/add middleware, since I want to change how cookiejars are passed around.\nI thought I would post this here just to see what thoughts people have. Is this is a bad idea? Has it been tried before? Any issues I might run into? I see that this kind of thing has been discussed before: #1878\n\ud83d\udc4d 3\n\u2764\ufe0f 2", "issue_status": "Open", "issue_reporting_time": "2018-05-11T15:13:58Z"}, "146": {"issue_url": "https://github.com/scrapy/scrapy/issues/3257", "issue_id": "#3257", "issue_summary": "[offsite middleware] allow to dynamically add new entries to allowed domains", "issue_description": "Contributor\npawelmhm commented on May 11, 2018 \u2022\nedited\nCurrently offsite middleware reads allowed domains from spider attribute on spider opened and uses that to decide whether request should be followed or not.\nscrapy/scrapy/spidermiddlewares/offsite.py\nLine 58 in 129421c\n def spider_opened(self, spider): \nI have use case where I'm making some initial request and then need to decide which domains to crawl. So ideally I'd make start_requests and after that set allowed_domains.\nDoes it make sense to add some way to add allowed domains dynamically? E.g. I could set something like this in spider.\n self.add_allowed_domains('http://foo.com')\nand after making this call spider will not follow foo.com.", "issue_status": "Open", "issue_reporting_time": "2018-05-11T11:54:01Z"}, "147": {"issue_url": "https://github.com/scrapy/scrapy/issues/3252", "issue_id": "#3252", "issue_summary": "CsvItemExporter delimiter should be changeable through settings.py", "issue_description": "Contributor\nIAlwaysBeCoding commented on May 9, 2018 \u2022\nedited\nTo me it seems pointless and annoying that you have to create a base class ItemExporter just so you can implement your own delimeter.\nIf we already have FEED_EXPORT_FIELDS to specify the fields order, why can't we have something like CSV_EXPORTER_DELIMETER as a setting that will be passed to the csv.writer inse the CsvItemExporter class?\nThis would be a really easy PR to do, and it will allow me to use just the default Scrapy's csv exporter instead of implementing my own.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-05-09T13:53:25Z"}, "148": {"issue_url": "https://github.com/scrapy/scrapy/issues/3250", "issue_id": "#3250", "issue_summary": "improve documents: global variables and noblock ways", "issue_description": "Contributor\nNewUserHa commented on May 8, 2018 \u2022\nedited\nI have considered the two for a long time, they're useful and common needs information for scrapy. can anyone answer the two questions and add the answer into scrapy documents to improve it?\nglobal variables:\nsay I have a list of some items to crawl and the failed request with any item should be inserted back to the list. is it possible to achieve within spider without another queue process?\nuse class variable/variable in main namespace? yield start_requests again after yield the final_result_item?\nnoblock ways:\nsay I want to requests an ip-pool via HTTP. using requests module in spider will make the spider block. is it that implement it in download/spider middlewares will not block anymore?\nthanks", "issue_status": "Open", "issue_reporting_time": "2018-05-08T14:30:16Z"}, "149": {"issue_url": "https://github.com/scrapy/scrapy/issues/3246", "issue_id": "#3246", "issue_summary": "OffsiteMiddleware silently filtering (invalid) URLs", "issue_description": "Contributor\njesuslosada commented on May 4, 2018 \u2022\nedited\nWe've recently run into a weird situation where the requests were created reading the URL values from a sitemap and as a result they contained leading and trailing newline characters (\\nhttp://example.com\\n).\nConsider this base example:\nclass ToscrapeSpider(scrapy.Spider):\n    name = 'toscrape'\n    allowed_domains = ['toscrape.com']\n    start_urls = ['http://books.toscrape.com/']\n\n    def parse(self, response):\n        yield scrapy.Request(\n            'http://quotes.toscrape.com/',\n            callback=self.parse2,\n        )\n\n    def parse2(self, response):\n        pass\nwhich sends 2 requests as expected:\n2018-05-04 09:17:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://books.toscrape.com/> (referer: None)\n2018-05-04 09:17:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/> (referer: http://books.toscrape.com/)\n2018-05-04 09:17:49 [scrapy.core.engine] INFO: Closing spider (finished)\nAnd now check what happens when you replace http://quotes.toscrape.com/ with \\nhttp://quotes.toscrape.com/:\nclass ToscrapeSpider(scrapy.Spider):\n    name = 'toscrape'\n    allowed_domains = ['toscrape.com']\n    start_urls = ['http://books.toscrape.com/']\n\n    def parse(self, response):\n        yield scrapy.Request(\n            '\\nhttp://quotes.toscrape.com/',\n            callback=self.parse2,\n        )\n\n    def parse2(self, response):\n        pass\nResults:\n2018-05-04 09:19:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://books.toscrape.com/> (referer: None)\n2018-05-04 09:19:49 [scrapy.core.engine] INFO: Closing spider (finished)\nSo Scrapy (the OffsiteMiddleware to be more specific) is silently filtering the second request and the only way to know this is by checking the stats: 'offsite/filtered': 1. In this situation I would expect Scrapy to show some kind of warning (or even an error) when creating the Request or when processing the request in the OffsiteMiddleware.\nFor reference:\nscrapy/scrapy/spidermiddlewares/offsite.py\nLine 36 in c4f096d\n if domain and domain not in self.domains_seen: \n\ndomain is None in this case and the OffsiteMiddleware doesn't add any messages to the log.\nI think the OffsiteMiddleware should log the filtered requests even if it can't properly parse the domain.", "issue_status": "Open", "issue_reporting_time": "2018-05-04T10:18:55Z"}, "150": {"issue_url": "https://github.com/scrapy/scrapy/issues/3240", "issue_id": "#3240", "issue_summary": "Possibility of retrying requests by any response characteristic (using callback).", "issue_description": "dyeray commented on Apr 27, 2018 \u2022\nedited\nThe retry middleware allows to retry requests depending on the response status. However, some websites return a 200 code on error, so we may want to retry depending on a response header, or even the response body.\nInstead of implementing this on a new middleware or on the spider itself I suggest a small backwards compatible change on the retry middleware. The user could define a list of callbacks on a settings variable, like:\nRETRY_CALLBACKS = [\n    \"myproject.retry_callbacks.woocommerce_retry_callback\",\n    \"myproject.retry_callbacks.google_retry_callback\",\n]\nThese callbacks would receive both the request and response object, so the developer can implement the retry conditions. In case that the callback returns True, the middleware would call self._retry.\nThis is something I have used in a couple of projects (inheriting from the retry middleware), so I'd be happy to add this feature to Scrapy.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-04-27T14:39:05Z"}, "151": {"issue_url": "https://github.com/scrapy/scrapy/issues/3236", "issue_id": "#3236", "issue_summary": "Using twisted threads inside parse function", "issue_description": "richa-saraf commented on Apr 25, 2018\nI have a blocking and time consuming method which needs to be invoked from parse() function.\nI wanted another thread to handle the above function. Going through the comments, my guess is I need to use twisted thread to perform this. I was wondering if anyone could point me toward the documentation which list examples of how this can be achieved?", "issue_status": "Open", "issue_reporting_time": "2018-04-24T23:46:06Z"}, "152": {"issue_url": "https://github.com/scrapy/scrapy/issues/3217", "issue_id": "#3217", "issue_summary": "allowed_domains bug/undesired behaviour", "issue_description": "Contributor\nsiulkilulki commented on Apr 15, 2018\nAssume crawler have set allowed_domains to below list:\nself.allowed_domains = ['albert.zgora.pl']\nScrapy shouldn't go beyond 'albert.zgora.pl' domain.\nBut it goes to:\nhttps://www.tumblr.com/widgets/share/tool/preview?shareSource=legacy&canonicalUrl=&url=http%3A%2F%2Falbert.zgora.pl%2F2014%2F08%2Fbierzmowanie%2F&title=Bierzmowanie\nThis is just one real life example, but there are many more (and I can give them here if you want) where domain string appears somewhere in url e.g. in &url= parameter.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-04-15T16:34:57Z"}, "153": {"issue_url": "https://github.com/scrapy/scrapy/issues/3212", "issue_id": "#3212", "issue_summary": "Exception in setUp method of TestCase", "issue_description": "Contributor\nwhalebot-helmsman commented on Apr 11, 2018 \u2022\nedited\nIf exception raised in setUp method tearDown method is never called for this TestCase. Allocated resources are not freed. We run mockserver as an external process, we end up with this mockserver process still running after test program finishes.\ne.g for this test case\nfrom twisted.internet import defer\nfrom twisted.trial.unittest import TestCase\nfrom tests.mockserver import MockServer\n\n\nclass TestSomething(TestCase):\n\n    def setUp(self):\n        self.mockserver = MockServer()\n        self.mockserver.__enter__()\n        raise ValueError('Exception in setUp')\n\n    def tearDown(self):\n        self.mockserver.__exit__(None, None, None)\n\n    @defer.inlineCallbacks\n    def test_smth(self):\n        pass\nwe get\n[]$ runinenv.sh ~/ves/scrapy tox -e py27 -- tests/test_exception.py\n...output skipped...\n============================ 1 failed in 1.45 seconds ============================\nERROR: InvocationError for command 'scrapy/.tox/py27/bin/py.test --cov=scrapy --cov-report= tests/test_exception.py' (exited with code 1)\n_____________________________ summary _____________________________\nERROR:   py27: commands failed\n[]$ pgrep --full --list-name mockserver \n15122 python2.7", "issue_status": "Open", "issue_reporting_time": "2018-04-11T13:44:10Z"}, "154": {"issue_url": "https://github.com/scrapy/scrapy/issues/3209", "issue_id": "#3209", "issue_summary": "Some scrapy request don't show up in Wireshark", "issue_description": "maximevdk commented on Apr 10, 2018\nEnvironment info\nPython 2.7.10\nScrapy 1.4.0\nProblem\nI've 200+ spiders that run in parallel and for one spider I'am struggling with the following issue:\n2018-04-10 16:49:08,107 DEBUG  [/Library/Python/2.7/site-packages/scrapy/downloadermiddlewares/retry.py 75] [scrapy.downloadermiddlewares.retry] Retrying <GET <request_url>> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]\nIt doesn't occur when I run the spider alone or together with 10 other spiders.\nI've noticed that when this line is logged, there is no request visible in Wireshark.\nIs there a limit on how many request scrapy can handle at the same time?", "issue_status": "Open", "issue_reporting_time": "2018-04-10T15:02:34Z"}, "155": {"issue_url": "https://github.com/scrapy/scrapy/issues/3205", "issue_id": "#3205", "issue_summary": "\"Easy\" Content type detection", "issue_description": "mohmad-null commented on Apr 8, 2018\nIt would be nice if Scrapy offered a helper that does content-type detection (i.e. JSON, HTML, XML, TXT, etc).\nSimilar to how Scrapy tries to guess the Encoding using a series of different things (i.e. https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-response-subclasses), I think it would help a lot of people if there was a similar thing for detecting content-type.\nNo need to do all of them (there are a lot - http://www.iana.org/assignments/media-types/media-types.xhtml ) - but the common ones would be good, and if none is declared, trying and get start.\nI'm currently using the resultant Selector, but that only distinguishes between xml and html.", "issue_status": "Open", "issue_reporting_time": "2018-04-08T15:49:39Z"}, "156": {"issue_url": "https://github.com/scrapy/scrapy/issues/3204", "issue_id": "#3204", "issue_summary": "Does scrapy do any cache cleanup?", "issue_description": "ghost commented on Apr 7, 2018 \u2022\nedited by ghost\nIn settings.py there is HTTPCACHE_EXPIRATION_SECS = 300 (seconds) .\nHowever, it seems to me that EXPIRATION is only at what point in time Scrapy ignores that cached data; With seemingly nothing that actually deletes/clean it out of the cache after it's expired.\nIn my experience, this makes the cache to simply continuously grow needlessly in size, (risking filling the disk during lengthy jobs); Even though CACHE_EXPIRATION ,in my opinion, would mean the expired data would get dropped/deleted from a cache when it's expired. And re-enter it into cache if coming across it after it's been dropped/\"de-cached\", with then renewed Time To Live\nWhen the cache (eventually) fills the device,\nOSError: [Errno 28] No space left on device\nis thrown.\nIs there even any point in keeping around data that's being ignored, in a cached state ?\nWouldn't dropping it from cache when expired make more sense?\nCould i simply be missing having some scrapy middleware activated?\nMy question regarding the issue, for more details: https://stackoverflow.com/questions/49588891/how-to-best-handle-scrapy-cache-at-oserror-errno-28-no-space-left-on-device\nmy Scrapy version is 1.5.0 and Python is version 3.6\nBTW: I've tried both FileSystem and LevelDB , and my question applies to both\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-04-07T17:02:02Z"}, "157": {"issue_url": "https://github.com/scrapy/scrapy/issues/3197", "issue_id": "#3197", "issue_summary": "Growing download latency of a CPU-heavy spider", "issue_description": "Member\nlopuhin commented on Apr 2, 2018 \u2022\nedited\nThe issue manifests itself as a growing latency when the spider is relatively CPU-intensive and is sending a lot of requests. Here is an example python 3 spider, based on scrapy bench spider:\nfrom urllib.parse import urlencode\nimport time\n\nimport scrapy\nfrom scrapy.linkextractor import LinkExtractor\nimport logging\n\n\nclass _BenchSpider(scrapy.Spider):\n    \"\"\"A spider that follows all links\"\"\"\n    name = 'follow'\n    total = 10000\n    show = 20\n    baseurl = 'http://localhost:8998'\n    link_extractor = LinkExtractor()\n    custom_settings = {\n        'LOG_LEVEL': 'INFO',\n        'LOGSTATS_INTERVAL': 1,\n\n        'CONCURRENT_REQUEST': 32,  # changed\n        'CONCURRENT_REQUEST_PER_DOMAIN': 32,  # changed\n    }\n\n    def start_requests(self):\n        self.t0 = time.time()\n        qargs = {'total': self.total, 'show': self.show}\n        url = '{}?{}'.format(self.baseurl, urlencode(qargs, doseq=1))\n        return [scrapy.Request(url, dont_filter=True)]\n\n    def parse(self, response):\n        # add latency reporting\n        logging.info('latency {:.2f} s after {:.0f} s'.format(\n            response.meta['download_latency'], time.time() - self.t0))\n        for link in self.link_extractor.extract_links(response):\n            for i in range(5):  # added CPU work extracting items\n                time.sleep(0.05)\n                yield {'step': i}\n            yield scrapy.Request(link.url, callback=self.parse)\nRun server in one window: python -m scrapy.utils.benchserver\nRun spider: scrapy runspider cpu_spider.py\nOvserve the output:\n2018-04-02 19:29:50 [scrapy.core.engine] INFO: Spider opened\n2018-04-02 19:29:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2018-04-02 19:29:50 [root] INFO: latency 0.01 s after 0 s\n2018-04-02 19:29:51 [root] INFO: latency 0.11 s after 1 s\n2018-04-02 19:29:51 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 180 pages/min), scraped 17 items (at 1020 items/min)\n...\n2018-04-02 19:30:02 [root] INFO: latency 0.94 s after 12 s\n2018-04-02 19:30:02 [root] INFO: latency 0.94 s after 12 s\n2018-04-02 19:30:03 [scrapy.extensions.logstats] INFO: Crawled 34 pages (at 360 pages/min), scraped 239 items (at 2340 items/min)\n...\n2018-04-02 19:30:57 [root] INFO: latency 8.41 s after 67 s\n2018-04-02 19:31:02 [scrapy.extensions.logstats] INFO: Crawled 101 pages (at 480 pages/min), scraped 1318 items (at 5940 items/min)\n...\n2018-04-02 19:31:56 [root] INFO: latency 12.77 s after 126 s\n2018-04-02 19:32:01 [scrapy.extensions.logstats] INFO: Crawled 141 pages (at 480 pages/min), scraped 2410 items (at 8280 items/min)\n...\n2018-04-02 19:34:36 [root] INFO: latency 20.37 s after 286 s\n2018-04-02 19:34:47 [scrapy.extensions.logstats] INFO: Crawled 213 pages (at 480 pages/min), scraped 5470 items (at 12840 items/min)\n...\n2018-04-02 19:37:02 [root] INFO: latency 25.34 s after 431 s\n2018-04-02 19:37:08 [scrapy.extensions.logstats] INFO: Crawled 261 pages (at 480 pages/min), scraped 8103 items (at 16260 items/min)\n...\n2018-04-02 19:47:57 [root] INFO: latency 40.99 s after 1087 s\n2018-04-02 19:48:11 [scrapy.extensions.logstats] INFO: Crawled 413 pages (at 480 pages/min), scraped 20446 items (at 23820 items/min)\n2018-04-02 19:48:32 [scrapy.extensions.logstats] INFO: Crawled 413 pages (at 0 pages/min), scraped 20835 items (at 23340 items/min)\nSo it seems that download latency (response.meta['download_latency']) is growing linearly, while the actual time the server spends on generating the response is very low.\nTo summarize, I expected download latency to remain reasonable and not to grow, since the spider is taking constant time to process items.\n\ud83d\udc4d 5", "issue_status": "Open", "issue_reporting_time": "2018-04-02T16:37:31Z"}, "158": {"issue_url": "https://github.com/scrapy/scrapy/issues/3196", "issue_id": "#3196", "issue_summary": "Feature: Running multiple spiders in a pool", "issue_description": "mohmad-null commented on Apr 2, 2018\nI'm interested in running multiple spiders simultaneously. I've achieved this using \"Running multiple spiders in the same process\" from - https://doc.scrapy.org/en/latest/topics/practices.html - and this runs great.\nBut one suggestion to extend that - How about the ability to create a \"pool\" of 10 running spiders, and then send in say, 200 spiders (each with a different initial url/list). When one spider finishes crawling, it closes as normal and the next spider in the queue starts, keeping 10 spiders running. Similar to multiprocess, multithreading queues where you can only have x processes/threads running at once.\n(Note: I'm aware of scrapyd, but it's massively overkill for this purpose).\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-04-02T13:21:50Z"}, "159": {"issue_url": "https://github.com/scrapy/scrapy/issues/3195", "issue_id": "#3195", "issue_summary": "Downloader middleware httpproxy enhancement", "issue_description": "Contributor\ngrammy-jiang commented on Mar 31, 2018 \u2022\nedited\nHi,\nThe downloader middleware httpproxy is a simple and clear middleware of scrapy. Until now it works well - all functions provided by it is following the relative RFC:\nRFC 7235 - Hypertext Transfer Protocol (HTTP/1.1): Authentication\nRFC 7617 - The 'Basic' HTTP Authentication Scheme\nAnd also:\nProxy-Authorization - HTTP | MDN\nBut there are still some places can be enhanced:\nThe proxy bypass now is called each time when a request comes into downloader; this could be ignored by lru_cache (decorator or traditional way), now working on PR #3316\nThe data (proxies) and the usage of proxies are coupled in the one middleware; it should be like another middleware - httpcache - decouple the usage and data (backend), and it even has another object to control the caching strategy (HTTPCACHE_POLICY, e.g. DummyPolicy)\nThe data (proxies, backend or storage) could be many, just following the same protocol (interface)\nWith these features, this middleware can provide more powerful ways to use proxies:\nuse more proxies from different backends/storages\nmanage the proxies, e.g. put invalidated proxies into the blacklist and ignore it for a certain period or forever\nI have done all of above in my repo: grammy-jiang/scrapy-proxy-management: A proxy management middleware and extension for scrapy, so you can easily understand what I would like to contribute by reading my repo.\nI will split the code into pieces to pull back to scrapy for easy review.\nAny suggestions or discussions are welcome!", "issue_status": "Open", "issue_reporting_time": "2018-03-31T13:03:48Z"}, "160": {"issue_url": "https://github.com/scrapy/scrapy/issues/3194", "issue_id": "#3194", "issue_summary": "Deep Copy meta on copy() or replace() Response methods", "issue_description": "mohmad-null commented on Mar 31, 2018\nIt would be nice if when using either the replace() or copy() Response methods that they be able to do a recursive deepcopy of the response's meta information.\nPer the docs, they only do shallow currently - https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta", "issue_status": "Open", "issue_reporting_time": "2018-03-31T11:15:52Z"}, "161": {"issue_url": "https://github.com/scrapy/scrapy/issues/3193", "issue_id": "#3193", "issue_summary": "How do I save scraped items to multiple .jl files?", "issue_description": "chrismp commented on Mar 30, 2018\nI can't seem to find this in the docs, so here goes...\nI want to run a scraper that saves different types of items to separate JSON Lines files.\nMy settings.py has this:\nITEM_PIPELINES = {\n   'permits.pipelines.PermitTypePipeline': 300,\n   'permits.pipelines.PermitNumberPipeline': 301,\n}\nMy pipelines.py has this:\nclass PermitTypePipeline(object):\n def process_item(self, item, spider):\n  return item\n\nclass PermitNumberPipeline(object):\n def process_item(self, item, spider):\n  return item\nitems.py has this:\nclass PermitType(scrapy.Item):\n permitWebCode= scrapy.Field()\n\nclass PermitNumber(scrapy.Item):\n permitNumber= scrapy.Field()\nmy_spider.py has this:\ndef parse(self,response):\n  ## Some scraper code here ... \n  yield PermitType(permitWebCode=someScrapedVariable)\n  yield PermitNumber(permitNumber=anotherScrapedVariable)\nHow do I save the yielded PermitType object to permit_types.jl and the PermitNumber object to permit_number.jl?", "issue_status": "Open", "issue_reporting_time": "2018-03-29T19:26:08Z"}, "162": {"issue_url": "https://github.com/scrapy/scrapy/issues/3191", "issue_id": "#3191", "issue_summary": "Better handling (and docs) of multiple spiders", "issue_description": "mohmad-null commented on Mar 28, 2018\nPython 3.6, Scrapy 1.5, Twisted 17.9.0\nI'm running multiple spiders in the same process per:\nhttps://doc.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process\nMy code is basically:\n```process = CrawlerProcess(scrapy_settings)\nfor i in range(1, (num_spiders + 1)):\n process.crawl(MySpiderClass)\n  \nprocess.start()```\nWhen I run this against a test LOCALHOST site I have, on my Windows 7 box, it runs fine with 30 spiders. Negligible CPU usage, low RAM usage, \"network\" IO that's very low, making about 25 requests per second in total.\nHowever, when I scrape (still from my PC) using these settings, but just 15 spiders, I get a huge list of DNS ERROR and TIMEOUT ERROR's returned near-immediately for sites that work perfectly fine from a browser (and quite quickly too).\nWith 15 spiders against localhost, Scrapy is using about:\n5-10KB/s Up\n300-600KB/s Down (bursting to 800)\nOpening(and closing) about 40 TCP Connections per second\nAbout 543 TCP Packets/ per second\nAbout 14 actual url's crawled per second, along with all related background processing.\nMy internet connection is good for about:\n1000KB/s Up\n4400 KB/S Down\nSo I figure hey, maybe it's a latency thing with my home connection, and spin up a VPS service (Centos 7.4) somewhere across the continent in a datacentre that likely has short, fat pipes.... but I get the exact same thing!\nEven just 7 spiders returns quite a lot of those error's for sites that do work fine, it just takes longer and there are fewer of them.\nNow, it's true that I changed the DNS and TIMEOUT values to something low because DNS has a response time measured in milliseconds (and I tested mine extensively yesterday to confirm it), similar for TIMEOUTS. But even if I change these back to something higher, I still get a wall of errors, it just takes longer.\nFor example, in the first 10 seconds of running 15 spiders against live hosts, (with a 2s DNS_TIMEOUT), there are DNS Errors for over 500 urls covering around 100 domains in my logs. Yet at a system level, I can see that only 35 DNS requests were actually made of which four failed, 8 didn't receive a reply, and the others mostly received responses in a 20-800 ms range. Only three were slower than my 2s value.\nI'm not CPU bound, the Scrapy Python process is using far less than a single core, either on my box, or the VPS (which only had one core anyway).\nAt a guess, this is probably a Twisted thing (though I know nothing of Twisted).\nSo this raises a few issues:\nMake multiple-spiders better at sharing Twisted's resources\nBetter document Scrapy's handling of multiple spiders. Suggestions include:\nCurrently it's not even clear from the docs if multiple-spiders is concurrent or consecutive (I've determined it's concurrent through testing, but the docs should be explicit on this point).\nClarification on which Scrapy Settings are a \"shared pool\" across spiders and which are not. I.e. is the CONCURRENT_REQUESTS pool shared across all spiders, or per-spider? Same for CONCURRENT_REQUESTS_PER_IP, AUTOTHROTTLE, etc etc.\nAny notes on scrapy's/twisted limitations when it comes to multiple spiders. On the one hand I can't be the first person to try this, but on the other hand I can't find anything about it. This person seems to be able to run over 30 spiders but is using a different mechanism - https://kirankoduru.github.io/python/multiple-scrapy-spiders.html\nDNS_TIMEOUT does not mean what I'd expect it to mean. It's not the amount of time a DNS Request has to get a response from when it leaves the host (because most of my requests were never sent), it includes Twisted/Scrapy Queuing Time too.\n(Note: My reason for splitting spiders is simply to speed up the crawl. I have no interest in distributed crawling - my target url-base (far less than a million) is small but split across tens of thousands of hosts. I've successfully crawled with one spider over about a week just fine in the past. I'm not interested in scrapyd.)", "issue_status": "Open", "issue_reporting_time": "2018-03-28T08:03:56Z"}, "163": {"issue_url": "https://github.com/scrapy/scrapy/issues/3185", "issue_id": "#3185", "issue_summary": "Using request callback in pipeline does not seem to work", "issue_description": "fabrepe commented on Mar 25, 2018 \u2022\nedited\nI am using a custom FilesPipeline to download pdf files. The input item embed a pdfLink attribute that point to the wrapper of the pdf. The pdf itself is embedded as an iframe in the link given by the pdfLink attribute.\nI then build the following pipeline :\nimport scrapy\nfrom scrapy.pipelines.files import FilesPipeline\n\nclass PdfPipeline(FilesPipeline):\n    def get_media_requests(self, item, spider):\n        yield scrapy.Request(item['pdfLink'],\n            callback=self.get_pdfurl)\n\n    def get_pdfurl(self, response):\n        import logging\n        logging.info('...............')\n        print response.url\n        yield scrapy.Request(response.css('iframe::attr(src)').extract()[0])\nHowever the get_url callback does not seem to be triggered, as\nneither logs or print function are shown\nthe file that is stored is the source code (html) of the wrapper page, located at item.get['pdfLink']\nIs it really possible to use Request callback in pipelines ? I am doing something wrong ?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2018-03-25T15:20:54Z"}, "164": {"issue_url": "https://github.com/scrapy/scrapy/issues/3182", "issue_id": "#3182", "issue_summary": "CrawlSpider - allow disabling of follow for start_urls", "issue_description": "johtso commented on Mar 22, 2018\nIt seems a little inconsistent that you can specify following behaviour in all your rules, but following of links cannot be disabled for the start_urls without overriding parse.\nMy use-case is a site where I need some custom logic to get the initial urls for the first page, so I don't want my crawling rules to be applied to it.", "issue_status": "Open", "issue_reporting_time": "2018-03-22T17:59:24Z"}, "165": {"issue_url": "https://github.com/scrapy/scrapy/issues/3174", "issue_id": "#3174", "issue_summary": "Feature: Report Queue Length", "issue_description": "mohmad-null commented on Mar 16, 2018\nFairly simple feature suggestion - report the current number of items in the queue in the log files. The line that reads like this is the perfect place for it:\n2018-03-15 09:24:43 [scrapy.extensions.logstats] INFO: Crawled 851 pages (at 155 pages/min), scraped 0 items (at 0 items/min)\nIt looks like it's in the logstats.py file - https://github.com/scrapy/scrapy/blob/ea83e67796aaf7b70cce8056592131fead8a460d/scrapy/extensions/logstats.py\nI'm probably not the only person interested in this being part of scrapy's own logging - https://stackoverflow.com/questions/28169756/how-to-get-the-number-of-requests-in-queue-in-scrapy", "issue_status": "Open", "issue_reporting_time": "2018-03-15T23:07:54Z"}, "166": {"issue_url": "https://github.com/scrapy/scrapy/issues/3163", "issue_id": "#3163", "issue_summary": "Spider logging not optimal", "issue_description": "Contributor\nthernstig commented on Mar 10, 2018\nCurrently the output of a spider log looks like this:\n>>> spider.logger.warning(\"test\")\n>>> 2018-03-10 13:42:56 [spider_name_goes_here] WARNING: test\nThe problem with this that the spider's logger has no parent. This is problematic if you have a lot of spiders. It requires you to have a root logger in order to propagate the root loggers configuration down to all spider loggers. And setting a root logger is not always recommended in all projects.\nSo if you have a dictConfig (e.g. via a json or yaml file) to set your config for your entire python project, you are now forced to enable the root logger OR set logging for each spider in the file.\nThe solution to this would be that all spider logger's name are prepended automatically with the name of the scrapy project. That way you can easily set project wide logging handling that propagates to all spiders. Maybe even that they are prepended with project_name.spiders since that is their logical structure. So the name would end up as:\n>>> spider.logger.warning(\"test\")\n>>> 2018-03-10 13:42:56 [project_name.spiders.spider_name_goes_here] WARNING: test", "issue_status": "Open", "issue_reporting_time": "2018-03-10T14:15:00Z"}, "167": {"issue_url": "https://github.com/scrapy/scrapy/issues/3161", "issue_id": "#3161", "issue_summary": "Logging can't format stack trace with non-ascii chars on Python 2", "issue_description": "tlinhart commented on Mar 9, 2018\nHi,\nI experience the same issue as described in #1602. However, I'm not using Django.\nThe stats look like this:\n{'downloader/request_bytes': 47621,\n 'downloader/request_count': 103,\n 'downloader/request_method_count/GET': 103,\n 'downloader/response_bytes': 1162618,\n 'downloader/response_count': 103,\n 'downloader/response_status_count/200': 101,\n 'downloader/response_status_count/302': 2,\n 'dupefilter/filtered': 2,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2018, 3, 9, 2, 3, 15, 748633),\n 'httpcache/firsthand': 72,\n 'httpcache/hit': 31,\n 'httpcache/miss': 72,\n 'httpcache/store': 72,\n 'item_scraped_count': 48,\n 'log_count/DEBUG': 215,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 9,\n 'memusage/max': 121434112,\n 'memusage/startup': 69783552,\n 'mongodb/item_stored_count': 48,\n 'request_depth_max': 3,\n 'response_received_count': 101,\n 'scheduler/dequeued': 102,\n 'scheduler/dequeued/memory': 102,\n 'scheduler/enqueued': 102,\n 'scheduler/enqueued/memory': 102,\n 'spider_exceptions/AttributeError': 1,\n 'start_time': datetime.datetime(2018, 3, 9, 2, 0, 52, 510449)}\nBut there's no mention about AttributeError (or any other error) in the log.\nI'm executing the spiders on Scrapyd instances running in Docker container. This is the first two lines of the log showing components' versions:\n2018-03-09 02:00:52 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: realestate)\n2018-03-09 02:00:52 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 2.7.12 (default, Nov 20 2017, 18:23:56) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Linux-4.4.0-103-generic-x86_64-with-Ubuntu-16.04-xenial\nThere's nothing special in the settings.py, but I can provide it if needed.", "issue_status": "Open", "issue_reporting_time": "2018-03-09T07:28:02Z"}, "168": {"issue_url": "https://github.com/scrapy/scrapy/issues/3159", "issue_id": "#3159", "issue_summary": "Response and other context is not passed to nested loader", "issue_description": "migr1 commented on Mar 9, 2018\nWhen passing the response to an item loader:\nloader = ItemLoader(item=Product(), response=response)\nThe response can be used in an input processor via the loader_context param:\ndef make_absolute_url(url, loader_context):\n    return loader_context['response'].urljoin(url)\nHowever, when using a nested loader:\nloader = ItemLoader(item=Product(), response=response)\nnested_loader = loader.nested_xpath('...')\nThe input processor fails with the exception:\n  File \"C:\\migr\\crawlers\\crawlers\\items.py\", line 16, in make_absolute_url\n    return loader_context['response'].urljoin(url)\nAttributeError: 'NoneType' object has no attribute 'urljoin'\nresponse is not passed to the input processor and I believe the reason is\nscrapy/scrapy/loader/__init__.py\nLine 55 in acd2b8d\n context.update(selector=selector) \n\nwhere a new context is created without reusing any of the current context.\nTo me, this is unexpected behavior. Since the loader is nested, I presumed that all context except the selector would be preserved unless explicitly overwritten in the call to nested_xpath. But maybe there is an explanation for not passing response and/or other context on to the nested loader. I can file a pull request if you agree that the behavior should be changed.", "issue_status": "Open", "issue_reporting_time": "2018-03-08T19:44:56Z"}, "169": {"issue_url": "https://github.com/scrapy/scrapy/issues/3148", "issue_id": "#3148", "issue_summary": "GSoC 2018: Intro & questions about adding async/await support", "issue_description": "Contributor\npatiences commented on Mar 1, 2018\nHi there!\nI've got a couple of questions regarding this project idea, but I'll introduce myself a bit first :-)\nI'm a 4th year computer science student at the University of British Columbia in Vancouver, Canada (though I'm currently studying abroad in Uppsala, Sweden!). My development experience includes working on integration of software development tools at Tasktop (Java/Javascript) and most recently, customisation of data processors at Microsoft (C#/SQL). I've been programming in python for about 2 years. (More information about me here!)\nI've used scrapy both for work and in my personal projects, which is why I'd love to have an opportunity to contribute meaningfully this summer!\nThere's already been some discussion around this project on the subreddit as well as in this issue which I have read. Still curious about a few things:\nI gathered that the project is not about migrating to asyncio. Is the goal to allow users to leverage async/await syntax in their spiders or custom middleware? What might that look like? Could you, for example, extract all the images from multiple urls concurrently?\nCould the project also be about refactoring parts of scrapy itself to use async/await or other asyncio-related frameworks as in this POC?\nI'll get started on picking up an issue and familiarizing myself with the codebase. If there is anything else that might be useful to know while I'm researching this project please let me know!\nThank you!\n\ud83d\udc4d 6", "issue_status": "Open", "issue_reporting_time": "2018-03-01T17:22:03Z"}, "170": {"issue_url": "https://github.com/scrapy/scrapy/issues/3145", "issue_id": "#3145", "issue_summary": "Make public attribute formdata for FormRequest", "issue_description": "kalombos commented on Feb 27, 2018 \u2022\nedited\nI use FormRequest.from_response method with custom formdata parameter but it is not enough flexible for me. Because some websites(.aspx) require special order of formdata parameters. Here is my workaround:\nimport scrapy \n\nclass CustomFormRequest(scrapy.FormRequest):\n    def __init__(self, *args, **kwargs):\n        if 'formdata' in kwargs:\n            self.formdata = kwargs['formdata']\n        super(FormRequest, self).__init__(*args, **kwargs)\n\n\ndef some_request(self, response):\n    form_request = CustomFormRequest.from_response(response)\n    new_formdata = make_new_formdata_with_right_order(form_request.formdata)\n    yield CustomFormRequest(url=form_request.url, formdata=new_formdata)\nSo if formdata would be a public attribute i would not have to create CustomFormRequest class. Also i was thinking about to add method that would change request body(_set_body method) with new formdata of existing request object instead of make a new one as i do in code above. I can make a PR if my proposal would be approved.", "issue_status": "Open", "issue_reporting_time": "2018-02-27T06:36:51Z"}, "171": {"issue_url": "https://github.com/scrapy/scrapy/issues/3135", "issue_id": "#3135", "issue_summary": "Unable to parse shift_jis encoded page that doesn't specify encoding", "issue_description": "nickhall-m3 commented on Feb 22, 2018 \u2022\nedited\nI'm trying to get data from this page: http://www.dis.h.u-tokyo.ac.jp/byomei/icd10_2013/index.html . The markup is ancient and it doesn't properly set its character encoding, but it is shift jis according to Chrome.\ndocument.characterSet\n\"Shift_JIS\"\nI get the page with this scrapy call.\nyield scrapy.Request(url=url, callback=self.parse, encoding='shift-jis')\nI manually set the encoding, but the output is nonetheless garbled:\n2018-02-22 16:14:09 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.dis.h.u-tokyo.ac.jp/byomei/icd10_2013/index.html>\n{'code': 'A00-B99', 'name': '\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd'}\nLikewise from the shell:\nIn [1]: response.css('title::text').extract()\nOut[1]: ['ICD10 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffda\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd10\ufffd\ufffd2013\ufffdN\ufffd\ufffd']\nThe parse runs as follows:\n    def parse(self, response):\n        for row in response.css('table:nth-of-type(2) tr'):\n            yield {\n                'code': row.css('td:nth-child(2) font::text').extract_first(),\n                'name': row.css('td:nth-child(3) b::text').extract_first()\n            }\nHow can I get the proper output?\npython 3.6.4, scrapy 1.5.0", "issue_status": "Open", "issue_reporting_time": "2018-02-22T07:21:07Z"}, "172": {"issue_url": "https://github.com/scrapy/scrapy/issues/3134", "issue_id": "#3134", "issue_summary": "Scrapy under Python 3 is slower than under Python 2", "issue_description": "Member\nlopuhin commented on Feb 21, 2018\nbookworm benchmark from https://github.com/scrapy/scrapy-bench/ (see also https://medium.com/@vermaparth/parth-gsoc-f5556ffa4025) shows about 15% slowdown, while more synthetic scrapy bench shows a 2x slowdown: #3050 (comment)", "issue_status": "Open", "issue_reporting_time": "2018-02-21T07:40:41Z"}, "173": {"issue_url": "https://github.com/scrapy/scrapy/issues/3133", "issue_id": "#3133", "issue_summary": "Improve response.css speed", "issue_description": "Member\nlopuhin commented on Feb 21, 2018\nSee issue scrapy/parsel#98: response.css takes more time than reponse.xpath: function css_to_xpath() takes 5% of the total time. This issue is meant just as a link, to have all performance issues related to scrapy in the scrapy issue tracker.", "issue_status": "Open", "issue_reporting_time": "2018-02-21T07:35:31Z"}, "174": {"issue_url": "https://github.com/scrapy/scrapy/issues/3125", "issue_id": "#3125", "issue_summary": "Speed regression in ItemLoader since 1.0", "issue_description": "juraseg commented on Feb 15, 2018\nHi\nTLDR: ItemLoader does not reuse response.selector when you are passing response to it as argument. And looks like it was reusing it up to Scrapy==1.0\nRecently we were trying to upgrade our setup to use newer version of Scrapy (we are on 1.0.5).\nWe noticed huge slowdown in a lot of our spiders.\nI dag down a bit and found that the bottleneck was ItemLoader, in particular when you create many ItemLoaders (one for each Item) passing response to every one of them and the response size is relatively big.\nThe speed drop down goes back to version 1.1. So on Scrapy==1.1 and above the performance degradation is present.\nTest spider code (testfile is just a random file of 1MB size):\n# -*- coding: utf-8 -*-\nimport os.path\n\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom scrapy.item import Item, Field\n\nHERE = os.path.abspath(os.path.dirname(__file__))\n\n\nclass Product(Item):\n    url = Field()\n    name = Field()\n\n\nclass TestSpeedSpider(scrapy.Spider):\n    name = 'test_speed'\n\n    def start_requests(self):\n        file_path = HERE + '/testfile'\n\n        file_url = 'file://' + file_path\n        yield scrapy.Request(file_url)\n\n    def parse(self, response):\n        for i in xrange(0, 300):\n            loader = ItemLoader(item=Product(), response=response)\n            loader.add_value('name', 'item {}'.format(i))\n            loader.add_value('url', 'http://site.com/item{}'.format(i))\n\n            product = loader.load_item()\n\n            yield product", "issue_status": "Open", "issue_reporting_time": "2018-02-15T11:39:11Z"}, "175": {"issue_url": "https://github.com/scrapy/scrapy/issues/3119", "issue_id": "#3119", "issue_summary": "url_is_from_spider fails if allowed_domains defined with @property decorator", "issue_description": "Contributor\npawelmhm commented on Feb 12, 2018 \u2022\nedited\nI got some unusual spider that was getting allowed domains like this\n@property \ndef allowed_domains(self):\n    # 2nd domain generated dynamically\n     return ['domain1com', self.generate_2nd_domain]\nThis was failing in scrapy shell:\nTraceback (most recent call last):\n  File \"/home/pawel/.virtualenvs/scrapy/bin/scrapy\", line 11, in <module>\n    load_entry_point('Scrapy', 'console_scripts', 'scrapy')()\n  File \"/home/pawel/prog_langs/py/dev_scrapy/scrapy/scrapy/cmdline.py\", line 149, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/pawel/prog_langs/py/dev_scrapy/scrapy/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/home/pawel/prog_langs/py/dev_scrapy/scrapy/scrapy/cmdline.py\", line 156, in _run_command\n    cmd.run(args, opts)\n  File \"/home/pawel/prog_langs/py/dev_scrapy/scrapy/scrapy/commands/shell.py\", line 61, in run\n    spidercls, log_multiple=True)\n  File \"/home/pawel/prog_langs/py/dev_scrapy/scrapy/scrapy/utils/spider.py\", line 43, in spidercls_for_request\n    snames = spider_loader.find_by_request(request)\n  File \"/home/pawel/prog_langs/py/dev_scrapy/scrapy/scrapy/spiderloader.py\", line 78, in find_by_request\n    if cls.handles_request(request)]\n  File \"/home/pawel/prog_langs/py/dev_scrapy/scrapy/scrapy/spiders/__init__.py\", line 98, in handles_request\n    return url_is_from_spider(request.url, cls)\n  File \"/home/pawel/prog_langs/py/dev_scrapy/scrapy/scrapy/utils/url.py\", line 31, in url_is_from_spider\n    [spider.name] + list(getattr(spider, 'allowed_domains', [])))\nTypeError: 'property' object is not iterable\nIt turns out that url_is_from_spider is called with class and not with instance, and in Python 2.7 this works like this\nIn [1]: class alfa(object):\n   ...:     @property\n   ...:     def malta(self):\n   ...:         return \"a\"\n   ...:     \n\nIn [2]: alfa.malta\nOut[2]: <property at 0x7f8fda62c470>\n\nIn [3]: alfa().malta\nOut[3]: 'a'\nso property of class is not iterable.\nOur tests are not detecting this case. They always test with spider instance, and utility is also called with spider class.\nI created following (failing) unit test in tests/test_utils_url.py\n    def test_url_is_from_spider_allowed_domains_property(self):\n        class PropertySpider(Spider):\n            @property\n            def allowed_domains(self):\n                return ['example.com']\n\n        spider = PropertySpider\n        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))\n        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', spider))\n        self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', spider))\nso now question how to solve this.\nFor me this only affected scrapy shell, so it is not urgent, I already fixed that by reworking spider, but potentially it could have some side effects somewhere?", "issue_status": "Open", "issue_reporting_time": "2018-02-12T11:27:21Z"}, "176": {"issue_url": "https://github.com/scrapy/scrapy/issues/3095", "issue_id": "#3095", "issue_summary": "Cannot retry requests in `scrapy parse`", "issue_description": "Contributor\njdemaeyer commented on Jan 26, 2018 \u2022\nedited\nThis spider:\nimport scrapy\n\n\nclass HttpBinSpider(scrapy.Spider):\n\n    name = \"httpbin.org\"\n\n    start_urls = ['https://httpbin.org/'] \n\n    def parse(self, response):\n        if response.meta.get('retried'):\n            self.logger.info(\"done\")\n            return\n        response.meta['retried'] = True\n        yield response.request.replace(dont_filter=True)\nworks fine with scrapy crawl httpbin.org, but scrapy parse \"https://httpbin.org/\" -d 2 fails with the following traceback:\nTraceback (most recent call last):\n  File \"/home/jakob/.local/lib/python3.6/site-packages/twisted/internet/defer.py\", line 653, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/home/jakob/.local/lib/python3.6/site-packages/scrapy/commands/parse.py\", line 195, in callback\n    items, requests = self.run_callback(response, cb)\n  File \"/home/jakob/.local/lib/python3.6/site-packages/scrapy/commands/parse.py\", line 117, in run_callback\n    for x in iterate_spider_output(cb(response)):\n  File \"/home/jakob/.local/lib/python3.6/site-packages/scrapy/commands/parse.py\", line 195, in callback\n    items, requests = self.run_callback(response, cb)\n  File \"/home/jakob/.local/lib/python3.6/site-packages/scrapy/commands/parse.py\", line 117, in run_callback\n    for x in iterate_spider_output(cb(response)):\n\n  [... the four lines above over and over again ...]\n\n  File \"/home/jakob/.local/lib/python3.6/site-packages/scrapy/commands/parse.py\", line 195, in callback\n    items, requests = self.run_callback(response, cb)\n  File \"/home/jakob/.local/lib/python3.6/site-packages/scrapy/commands/parse.py\", line 117, in run_callback\n    for x in iterate_spider_output(cb(response)):\n  File \"/home/jakob/.local/lib/python3.6/site-packages/scrapy/commands/parse.py\", line 169, in callback\n    cb = response.meta['_callback']\n  File \"/home/jakob/.local/lib/python3.6/site-packages/scrapy/http/response/__init__.py\", line 30, in meta\n    return self.request.meta\nRecursionError: maximum recursion depth exceeded while calling a Python object\nExplicitly also replacing the request callback (yield response.request.replace(dont_filter=True, callback=self.parse)) fixes the issue.\nThis is possibly caused by not properly cleaning up this and this line.\nShout-out to @VMRuiz for finding this. ;)", "issue_status": "Open", "issue_reporting_time": "2018-01-26T11:14:33Z"}, "177": {"issue_url": "https://github.com/scrapy/scrapy/issues/3090", "issue_id": "#3090", "issue_summary": "JsonItemExporter puts lone comma in the output if encoder fails", "issue_description": "tlinhart commented on Jan 25, 2018\nIf JsonItemExporter is unable to encode the item, it still writes a delimiter (comma) to the output file. Here is a sample spider:\n# -*- coding: utf-8 -*-\nimport datetime\nimport scrapy\n\nclass DummySpider(scrapy.Spider):\n    name = 'dummy'\n    start_urls = ['http://example.org/']\n\n    def parse(self, response):\n        yield {'date': datetime.date(2018, 1, 1)}\n        yield {'date': datetime.date(1234, 1, 1)}\n        yield {'date': datetime.date(2019, 1, 1)})\nEncoding the second items fails:\n2018-01-25 09:05:57 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7fcbfbd81250>>\nTraceback (most recent call last):\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 224, in item_scraped\n    slot.exporter.export_item(item)\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 130, in export_item\n    data = self.encoder.encode(itemdict)\n  File \"/usr/lib/python2.7/json/encoder.py\", line 207, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File \"/usr/lib/python2.7/json/encoder.py\", line 270, in iterencode\n    return _iterencode(o, 0)\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/utils/serialize.py\", line 22, in default\n    return o.strftime(self.DATE_FORMAT)\nValueError: year=1234 is before 1900; the datetime strftime() methods require year >= 1900\nThe output looks like this:\n[\n{\"date\": \"2018-01-01\"},\n,\n{\"date\": \"2019-01-01\"}\n]\nThis seems not to be a valid JSON file as e.g. json.load() and jq fail to parse it.\nI think the problem is in export_item method of JsonItemExporter class where it outputs the comma before decoding the item. The correct approach would be to try to decode the item (possibly with other needed operations) and perform the write atomically.", "issue_status": "Open", "issue_reporting_time": "2018-01-25T08:32:24Z"}, "178": {"issue_url": "https://github.com/scrapy/scrapy/issues/3086", "issue_id": "#3086", "issue_summary": "Extend `load_object` to work with attributes", "issue_description": "iAnanich commented on Jan 20, 2018 \u2022\nedited\nI propose to add some special character to the utils.misc.load_object function's path argument, for example : to mark attribute(s) to be loaded too.\nBackground\nIssue created as part of this issue in scrapy-splash plugin and inspired by this PR.\nIn the scrapy-splash plugin it is SPLASH_SLOT_POLICY setting that takes values from scrapy_splash.middleware.SlotPolicy attributes (by default - PER_DOMAIN). So the user needs to import something (!) in his/her settings.py to define SPLASH_SLOT_POLICY value. And even using load_object inside plugin won't help because it can't access attributes of the class.\nSpecification\nFirst of all, this change will keep backward compatibility because we still use . (dots) for everything that we can import.\nThen it is some simple options:\nallow only one attribute to load. In this case, scrapy_splash.middleware.SlotPolicy:PER_DOMAIN will be a solution.\nallow multiple attributes to load. In this case path like package.module.klass:constant:attr:var is a bit overkill and looks bad.\nallow multiple attributes to load, but use only one : to split \"importable\" objects from \"accessible\" (by . dot) objects. For example: package.module.klass:constant.attr.var - looks more pythonic.\nAny ideas on how to be?", "issue_status": "Open", "issue_reporting_time": "2018-01-20T07:01:10Z"}, "179": {"issue_url": "https://github.com/scrapy/scrapy/issues/3083", "issue_id": "#3083", "issue_summary": "Why use UserAgentMiddleware when DefaultHeadersMiddleware exists?", "issue_description": "hduyyg commented on Jan 19, 2018 \u2022\nedited\nDefaultHeadersMiddleware can do all UserAgentMiddleware can do? So what's the meaning of UserAgentMiddleware?", "issue_status": "Open", "issue_reporting_time": "2018-01-19T02:53:28Z"}, "180": {"issue_url": "https://github.com/scrapy/scrapy/issues/3077", "issue_id": "#3077", "issue_summary": "scrapy selector fails when large lines are present response", "issue_description": "shanmuga-cv commented on Jan 15, 2018\nOriginally encoutered when scraping Amazon restaurant.\nThis page contains multiple script tag with lines greater then 64,000 character in one line.\nThe selector (xpath and css) does not search beyond these lines.\nDue to this the following xpath '//h1[contains(@class, \"hw-dp-restaurant-name\")]/text()' to extract name of the restaurant returns empty even though there is a matching tag is present.\nPFA the response text at original_response.html.txt.gz", "issue_status": "Open", "issue_reporting_time": "2018-01-15T11:21:59Z"}, "181": {"issue_url": "https://github.com/scrapy/scrapy/issues/3075", "issue_id": "#3075", "issue_summary": "Telnet Console", "issue_description": "crisfan commented on Jan 13, 2018\nhi, @kmike,I use telnetlib to pause scrapy engine, there is some error! although it pause the scrapy engine.\nthis is my poor code:\nimport telnetlib\nhost = '127.0.0.1'\ntn = telnetlib.Telnet(host, 6023)\ntn.write(\"engine.pause()\\n\") \nerror:\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\n\nUnhandled Error\nTraceback (most recent call last):\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\"'\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\nTraceback (most recent call last):\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\n\nUnhandled Error\nTraceback (most recent call last):\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x1f'\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\nTraceback (most recent call last):\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\n\nUnhandled Error\nTraceback (most recent call last):\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x03'\n2018-01-13 16:19:26 [twisted] ERROR: Unhandled Error\nTraceback (most recent call last):\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\n\nUnhandled Error\nTraceback (most recent call last):\nFailure: twisted.conch.telnet.OptionRefused: twisted.conch.telnet.OptionRefused:'\\x01'\ncould you fix my problem, thansk ~~~", "issue_status": "Open", "issue_reporting_time": "2018-01-13T08:41:24Z"}, "182": {"issue_url": "https://github.com/scrapy/scrapy/issues/3065", "issue_id": "#3065", "issue_summary": "`Connection to the other side was lost` for older French site", "issue_description": "autumnjolitz commented on Jan 10, 2018\nHi,\nI'm attempting to run scrapy on https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences.\nDespite upgrading all my brew packages, scrapy installation via pip install -U scrapy, the website disconnects uncleanly. Specifying -s DOWNLOADER_CLIENT_TLS_METHOD=TLSv1.0 makes no difference to the following run. Neither does a user-agent clear things up with -s USER_AGENT=\"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36\"\nI am uncertain as how to debug this further.\nscrapy shell https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n2018-01-09 18:03:33 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Darwin-16.7.0-x86_64-i386-64bit\n2018-01-09 18:03:33 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOADER_CLIENT_TLS_METHOD': 'TLSv1.0', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage']\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2018-01-09 18:03:33 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2018-01-09 18:03:33 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2018-01-09 18:03:33 [scrapy.core.engine] INFO: Spider opened\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-09 18:03:34 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-09 18:03:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nTraceback (most recent call last):\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 150, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/cmdline.py\", line 157, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/commands/shell.py\", line 73, in run\n    shell.start(url=url, redirect=not opts.no_redirect)\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 48, in start\n    self.fetch(url, spider, redirect=redirect)\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/scrapy/shell.py\", line 115, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/Users/BenJolitz/.virtualenvs/cpython36/lib/python3.6/site-packages/twisted/python/failure.py\", line 385, in raiseException\n    raise self.value.with_traceback(self.tb)\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nscrapy version -v:\nScrapy       : 1.5.0\nlxml         : 4.1.1.0\nlibxml2      : 2.9.7\ncssselect    : 1.0.3\nparsel       : 1.3.1\nw3lib        : 1.18.0\nTwisted      : 17.9.0\nPython       : 3.6.2 (default, Jul 17 2017, 16:44:45) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)]\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017)\ncryptography : 2.1.4\nPlatform     : Darwin-16.7.0-x86_64-i386-64bit\ncurl -sLv https://agences.creditfoncier.fr/credit-immobilier/toutes-nos-agences 2>&1 | head -20:\n*   Trying 160.92.134.3...\n* TCP_NODELAY set\n* Connected to agences.creditfoncier.fr (160.92.134.3) port 443 (#0)\n* TLS 1.0 connection using TLS_RSA_WITH_3DES_EDE_CBC_SHA\n* Server certificate: agences.creditfoncier.fr\n* Server certificate: RapidSSL RSA CA 2018\n* Server certificate: DigiCert Global Root CA\n> GET /credit-immobilier/toutes-nos-agences HTTP/1.1\n> Host: agences.creditfoncier.fr\n> User-Agent: curl/7.54.0\n> Accept: */*\n>\n< HTTP/1.1 200 OK\n< Date: Wed, 10 Jan 2018 02:06:54 GMT\n< Server: Microsoft-IIS/6.0\n< X-Powered-By: ASP.NET\n< Content-Length: 35884\n< Content-Type: text/html\n< Set-Cookie: ASPSESSIONIDQQBACDTS=JHOJNNCCJNIFABOPLNCEEAFH; path=/\n< Cache-control: private", "issue_status": "Open", "issue_reporting_time": "2018-01-10T02:10:23Z"}, "183": {"issue_url": "https://github.com/scrapy/scrapy/issues/3064", "issue_id": "#3064", "issue_summary": "I ran into a problem when I used ipython in a scrapy shell", "issue_description": "reaCodes commented on Jan 8, 2018\nI used this command, scrapy shell 'www.baidu.com', and then used response.body.\nWhen I input resp and click Tab\nAs you can see, there was a display error\nUse the command completion function is encountered a display error\n\ud83d\udc4d 4", "issue_status": "Open", "issue_reporting_time": "2018-01-08T06:15:17Z"}, "184": {"issue_url": "https://github.com/scrapy/scrapy/issues/3056", "issue_id": "#3056", "issue_summary": "File / Image Pipelines should have an option to send referer headers from the referring response", "issue_description": "Contributor\nNewUserHa commented on Dec 31, 2017\nit's usual case and it's ugly to override get_media_requests method of pipelines.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2017-12-31T08:05:39Z"}, "185": {"issue_url": "https://github.com/scrapy/scrapy/issues/3055", "issue_id": "#3055", "issue_summary": "[bug] pillow will always recode images in imagepipieline", "issue_description": "Contributor\nNewUserHa commented on Dec 31, 2017\nscrapy/scrapy/pipelines/images.py\nLine 151 in aa83e15\n image.save(buf, 'JPEG') \nthis line will always recode images silently and damage the image quality.\nplease add an option to avoid this.", "issue_status": "Open", "issue_reporting_time": "2017-12-31T08:02:25Z"}, "186": {"issue_url": "https://github.com/scrapy/scrapy/issues/3024", "issue_id": "#3024", "issue_summary": "startproject command does not work for latest pyOpenSSL version", "issue_description": "cisko3000 commented on Dec 1, 2017\nI did an install of scrapy on virtualenv, upgraded all packages and command \"startproject\" did not work.\nI would get his error:\nTraceback (most recent call last):\n  File \"/home/me/sites/cot/env/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 121, in execute\n    cmds = _get_commands_dict(settings, inproject)\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 45, in _get_commands_dict\n    cmds = _get_commands_from_module('scrapy.commands', inproject)\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 28, in _get_commands_from_module\n    for cmd in _iter_command_classes(module):\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 19, in _iter_command_classes\n    for module in walk_modules(module_name):\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 71, in walk_modules\n    submod = import_module(fullpath)\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/scrapy/commands/version.py\", line 6, in <module>\n    import OpenSSL\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/OpenSSL/__init__.py\", line 8, in <module>\n    from OpenSSL import crypto, SSL\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/OpenSSL/crypto.py\", line 12, in <module>\n    from cryptography import x509\n  File \"/home/me/sites/cot/env/local/lib/python2.7/site-packages/cryptography/x509/__init__.py\", line 7, in <module>\n    from cryptography.x509 import certificate_transparency\nImportError: cannot import name certificate_transparency\nSo I went to an environment where I was able to do startproject (over a year ago), and noticed it has pyOpenSSL version 16.0.0. and then installed this version and then I was able to startproject using latest environment and latest scrapy.\nWhat is up with this? Is this something I can fix here in this scrapy repo?", "issue_status": "Open", "issue_reporting_time": "2017-12-01T07:28:03Z"}, "187": {"issue_url": "https://github.com/scrapy/scrapy/issues/3017", "issue_id": "#3017", "issue_summary": "Entire HTML is not checked for finding base tag", "issue_description": "Vineeth-Mohan commented on Nov 23, 2017\nIn the HTML we are using the base tag is set. It also happens that this HTML has huge amount of comment and white space , and base tag is not coming in first 4096 characters.\nIn the code here -\nscrapy/scrapy/utils/response.py\nLine 27 in b8870ee\n text = response.text[0:4096] \ndef get_base_url(response):\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.text[0:4096]\n        _baseurl_cache[response] = html.get_base_url(text, response.url,\n            response.encoding)\n    return _baseurl_cache[response]\nWe could find that , in the code above , we are NOT checking for the base tag beyond first 4096 characters. This has failed our crawl.\nI believe there could not be any hard coding in any code and I request to make this configurable atleast.\nWe are stuck with this , please advice what needs to be done.", "issue_status": "Open", "issue_reporting_time": "2017-11-23T07:57:03Z"}, "188": {"issue_url": "https://github.com/scrapy/scrapy/issues/3001", "issue_id": "#3001", "issue_summary": "Derived spider with overwritten custom_settings ignores custom_settings of the base spider", "issue_description": "Contributor\nwoxcab commented on Nov 11, 2017\nIf the base spider has custom_settings field and the inherited spider from the base spider also has custom_settings field then all settings from base spider are ignored by derived spider.\nFor example:\nimport scrapy \n\nclass BaseSpider(scrapy.Spider):\n    custom_settings = {'ITEM_PIPELINES': {\n        'test_proj.pipelines.FirstPipeline': 300,\n        'test_proj.pipelines.SecondPipeline': 301,\n    }}\n\nclass DerivedSpider(BaseSpider):\n    name = 'derived'\n    custom_settings = {'CONCURRENT_REQUESTS': 2}\nIn this example DerivedSpider will have only {'CONCURRENT_REQUESTS': 2} among overwritten settings whereas item pipelines from the base spider will not be presented in the DerivedSpider settings.\nSo I suggest to merge custom_settings of the all base spider classes of a spider when spider settings are initialized. What do you think about it?\n\ud83d\udc4e 1", "issue_status": "Open", "issue_reporting_time": "2017-11-10T23:55:04Z"}, "189": {"issue_url": "https://github.com/scrapy/scrapy/issues/2977", "issue_id": "#2977", "issue_summary": "Response data is not released after processing?", "issue_description": "netcaf commented on Oct 26, 2017\nHello,\ncode sample:\ndef start_requests(self):\n    for url in url_list:\n            yield scrapy.Request(url=url, headers = xxx, callback=self.parse, errback=None,dont_filter=False)\n\ndef parse(self, response):\n    ...\n    response.xpath('//*[@id=\"feed-main-list\"]/li')\n    ...\nIf we call selector related function like xpath(...), the response data will not be released after parsing of each url.\nIn scrapy/http/response/text.py,\nclass TextResponse(Response):\n...\n    def selector(self):\n        from scrapy.selector import Selector\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector\n\n    def xpath(self, query, **kwargs):\n        return self.selector.xpath(query, **kwargs)\nIt has the cyclic reference like response -> Selector -> response.\nIn my rough test, if i don't use self._cached_selector to keep Selector object, the memory usage will reduce from 6x% -> 4x%.\nSo, am i right? or how to understand it?", "issue_status": "Open", "issue_reporting_time": "2017-10-26T13:15:22Z"}, "190": {"issue_url": "https://github.com/scrapy/scrapy/issues/2973", "issue_id": "#2973", "issue_summary": "bindaddress doesn't bind IP when getting robots.txt", "issue_description": "D-Kalck commented on Oct 22, 2017\nI have several failover IPs that are well configured (they work with wget or curl), and I would like to bind them when I use Scrapy, so I use the bindaddress key to achieve this, but the public IP is still the main IP.\n# -*- coding: utf-8 -*-\nimport scrapy\nimport random\n#https://ipinfo.io/ip\n\nclass SpiderTest(scrapy.Spider):\n    name = \"failover\"\n\n    def start_requests(self):\n        url = 'https://ipinfo.io/ip'\n        bind_addresses = ['BBB.BBB.BBB.BBX', 'BBB.BBB.BBB.BBY', 'BBB.BBB.BBB.BBZ']\n        bind_address = random.choice(bind_addresses)\n        print('Bind :      ' + bind_address)\n        request = scrapy.Request(url=url,\n                             callback=self.test,\n                             meta={\"bindaddress\": (bind_address, 0)})\n        yield request\n\n    def test(self, response):\n        html = response.body\n        print('Public IP : ' + html.decode('ascii'))\nI tried also :\nrequest.meta['bindaddress'] = (bind_address, 0)\nor\nrequest.meta['bindaddress'] = bind_address\nAn excerpt from ip addr :\n2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff\n    inet AAA.AAA.AAA.AAA/32 brd AAA.AAA.AAA.AAA scope global ens3\n       valid_lft forever preferred_lft forever\n    inet BBB.BBB.BBB.BBX/32 brd BBB.BBB.BBB.BBX scope global ens3:0\n       valid_lft forever preferred_lft forever\n    inet BBB.BBB.BBB.BBY/32 brd BBB.BBB.BBB.BBY scope global ens3:1\n       valid_lft forever preferred_lft forever\n    inet BBB.BBB.BBB.BBZ/32 brd BBB.BBB.BBB.BBZ scope global ens3:2\n       valid_lft forever preferred_lft forever\nMy environment :\nDebian 9, Python 3.5.3, Scrapy 1.4.0 and Twisted 17.9.0 (tested with Twisted 17.5.0)", "issue_status": "Open", "issue_reporting_time": "2017-10-22T08:56:02Z"}, "191": {"issue_url": "https://github.com/scrapy/scrapy/issues/2960", "issue_id": "#2960", "issue_summary": "Class Request without the method __eq__ can't be compared directly in the unittest", "issue_description": "Contributor\ngrammy-jiang commented on Oct 11, 2017 \u2022\nedited\nI met a interesting failure when I did a unittest about the method process_spider_exception of the spider middleware:\nIn my project, this method returns a iterable (list) of request objects, which will be sent to the method process_spider_output. Now I want to assert this iterable (list) of request objects with assertEqual or assertSequence, but the test is always failed. Then I debug my code, also with the test case, then I realized that: Class Request has no method __eq__, so python will compare the id instead, and in my middleware the request is modified with replace method, definitely not the previous one, the test will never return True.\nMy solution is to compare __dict__ of each pair requests in a loop, as mentioned in the following code:\nspider middleware\nclass BlockInspector(object):\n    def __init__(self, crawler):\n        self.crawler = crawler\n        self.settings = crawler.settings\n        self.signal_manager = crawler.signals\n        self.stats = crawler.stats\n\n        self.inspect_block = load_object(\n            self.settings.get('BLOCK_INSPECTOR_SPIDERMIDDLEWARE'))\n        if self.settings.get('RECYCLE_BLOCK_REQUEST_SPIDERMIDDLEWARE'):\n            self.recycle_block_request = load_object(\n                self.settings.get('RECYCLE_BLOCK_REQUEST_SPIDERMIDDLEWARE'))\n        else:\n            self.recycle_block_request = lambda x: x\n        self.block_signals = list(map(\n            lambda x: load_object(x),\n            self.settings.get('BLOCK_SIGNALS_SPIDERMIDDLEWARE')))\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        obj = cls(crawler)\n        return obj\n\n    def process_spider_input(self, response, spider):\n        if self.inspect_block(response):\n            self.stats.inc_value('block_inspector/block', spider=spider)\n            raise BlockException(response)\n\n    def process_spider_exception(self, response, exception, spider):\n        if isinstance(exception, BlockException):\n            _ = list(map(\n                lambda x: self.signal_manager.send_catch_log(\n                    x, spider=spider, response=response, exception=exception),\n                self.block_signals))\n\n            r = response.request.replace(dont_filter=True)\n\n            return [self.recycle_block_request(r)]\ntest case\nclass TestBlockInspector(TestCase):\n    def setUp(self):\n        crawler = get_crawler(Spider, settings_dict={\n            'BLOCK_INSPECTOR_SPIDERMIDDLEWARE': 'tests.test_spidermiddleware_block_inspector._block_inspector',\n            # TODO: add signals and test cases\n            'BLOCK_SIGNALS_SPIDERMIDDLEWARE': [],\n            'RECYCLE_BLOCK_REQUEST_SPIDERMIDDLEWARE': 'tests.test_spidermiddleware_block_inspector._recycle_block_request'\n        })\n\n        self.spider = Spider.from_crawler(crawler, name='foo')\n        self.mw = BlockInspector(crawler)\n        self.req = Request('http://scrapytest.org')\n        self.res_succeed, self.res_block = _responses(self.req)\n\n    def test_process_spider_input(self):\n        self.assertEqual(\n            None,\n            self.mw.process_spider_input(self.res_succeed, self.spider))\n        self.assertRaises(\n            BlockException,\n            self.mw.process_spider_input, self.res_block, self.spider)\n\n    def test_process_spider_exception_default(self):\n        crawler = get_crawler(Spider, settings_dict={\n            'BLOCK_INSPECTOR_SPIDERMIDDLEWARE': 'tests.test_spidermiddleware_block_inspector._block_inspector',\n            # TODO: add signals and test cases\n            'BLOCK_SIGNALS_SPIDERMIDDLEWARE': []})\n\n        spider = Spider.from_crawler(crawler, name='foo')\n        mw = BlockInspector(crawler)\n\n        for req, req_after_exception in zip(\n                [self.req.replace(dont_filter=True)],\n                mw.process_spider_exception(\n                    self.res_block, BlockException(self.res_block),\n                    spider)):\n            self.assertEqual(req.__dict__, req_after_exception.__dict__)\n\n    def test_process_spider_exception(self):\n        # this will get test failure:\n        # self.assertEqual(\n        #     [self.req.replace(dont_filter=True)],\n        #     self.mw.process_spider_exception(\n        #         self.res_block, BlockException(self.res_block), self.spider))\n        # self.assertSequenceEqual(\n        #     [self.req.replace(dont_filter=True)],\n        #     self.mw.process_spider_exception(\n        #         self.res_block, BlockException(self.res_block), self.spider))\n\n        # this will work:\n        for req, req_after_exception in zip(\n                [self.req.replace(dont_filter=True)],\n                self.mw.process_spider_exception(\n                    self.res_block, BlockException(self.res_block),\n                    self.spider)):\n            self.assertEqual(req.__dict__, req_after_exception.__dict__)\nI have gone through the test cases in scrapy source code, and there is no one just like my situation.\nIs it a good idea to add the method __eq__ to class request?", "issue_status": "Open", "issue_reporting_time": "2017-10-11T02:33:18Z"}, "192": {"issue_url": "https://github.com/scrapy/scrapy/issues/2951", "issue_id": "#2951", "issue_summary": "SSLv3 method is no longer available", "issue_description": "Member\nkmike commented on Oct 3, 2017\nDOWNLOADER_CLIENT_TLS_METHOD='SSLv3' no longer works with recent Twisted, PyOpenSSL, OpenSSL, etc.:\nscrapy shell 'https://example.com' -s DOWNLOADER_CLIENT_TLS_METHOD=SSLv3\n2017-10-03 13:17:49 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n2017-10-03 13:17:49 [scrapy.utils.log] INFO: Versions: lxml 4.0.0.0, libxml2 2.9.5, cssselect 1.0.1, parsel 1.2.0, w3lib 1.18.0, Twisted 17.9.0, Python 3.6.1 (default, Apr  4 2017, 09:40:21) - [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)], pyOpenSSL 17.3.0 (OpenSSL 1.1.0f  25 May 2017), cryptography 2.0.3, Platform Darwin-16.7.0-x86_64-i386-64bit\n2017-10-03 13:17:49 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOADER_CLIENT_TLS_METHOD': 'SSLv3', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'EDITOR': 'nano', 'LOGSTATS_INTERVAL': 0}\n2017-10-03 13:17:49 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage']\n2017-10-03 13:17:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-10-03 13:17:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-10-03 13:17:49 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-10-03 13:17:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-10-03 13:17:49 [scrapy.core.engine] INFO: Spider opened\nTraceback (most recent call last):\n  File \"/Users/kmike/envs/deepdeep/lib/python3.6/site-packages/OpenSSL/SSL.py\", line 618, in __init__\n    method_func = self._methods[method]\nKeyError: 2\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/kmike/envs/deepdeep/bin/scrapy\", line 11, in <module>\n    load_entry_point('Scrapy', 'console_scripts', 'scrapy')()\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 150, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 90, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 157, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/kmike/svn/scrapy/scrapy/commands/shell.py\", line 73, in run\n    shell.start(url=url, redirect=not opts.no_redirect)\n  File \"/Users/kmike/svn/scrapy/scrapy/shell.py\", line 48, in start\n    self.fetch(url, spider, redirect=redirect)\n  File \"/Users/kmike/svn/scrapy/scrapy/shell.py\", line 115, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/Users/kmike/envs/deepdeep/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/Users/kmike/envs/deepdeep/lib/python3.6/site-packages/twisted/python/failure.py\", line 385, in raiseException\n    raise self.value.with_traceback(self.tb)\nValueError: No such protocol", "issue_status": "Open", "issue_reporting_time": "2017-10-03T11:22:16Z"}, "193": {"issue_url": "https://github.com/scrapy/scrapy/issues/2944", "issue_id": "#2944", "issue_summary": "sslv3 alert handshake failure", "issue_description": "Dainius-P commented on Sep 26, 2017\nI have been getting sslv3 errors in scrapy after updating it.\n2017-09-11 17:33:20 [scrapy.core.engine] INFO: Spider opened 2017-09-11 17:33:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2017-09-11 17:33:20 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6025 2017-09-11 17:33:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://api.citypantry.com/packages/search?page=1&sortBy=relevance> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'sslv3 alert handshake failure')]>]\nVersions:\nScrapy : 1.4.0 lxml : 3.8.0.0 libxml2 : 2.9.3 cssselect : 1.0.1 parsel : 1.2.0 w3lib : 1.18.0 Twisted : 17.5.0 Python : 2.7.12+ (default, Sep 17 2016, 12:08:02) - [GCC 6.2.0 20160914] pyOpenSSL : 17.2.0 (OpenSSL 1.1.0f 25 May 2017) Platform : Linux-4.8.0-59-generic-x86_64-with-Ubuntu-16.10-yakkety\nThe same thing happens while using python 3.", "issue_status": "Open", "issue_reporting_time": "2017-09-26T12:17:56Z"}, "194": {"issue_url": "https://github.com/scrapy/scrapy/issues/2933", "issue_id": "#2933", "issue_summary": "Overwrite settings in image pipeline?", "issue_description": "ghost commented on Sep 20, 2017\nHello,\nfrom the documentation it is not clear to me how to change IMAGES_STORE path inside a pipeline.\nI am scraping two different images which shall be stored in different paths.\nSo my idea was to write for each image an extra pipeline and overwrite the images_store path.\nIs that even possible? Or is there another method to change the path depending on which image i am downloading?", "issue_status": "Open", "issue_reporting_time": "2017-09-20T10:41:06Z"}, "195": {"issue_url": "https://github.com/scrapy/scrapy/issues/2919", "issue_id": "#2919", "issue_summary": "FormRequest.formdata with GET method duplicates same key in query string", "issue_description": "talhashraf commented on Sep 10, 2017\n>>> from scrapy import FormRequest\n>>> FormRequest('http://example.com/?id=9', method='GET', formdata={'id': '8'}).url\n'http://example.com/?id=9&id=8'\nNotice how id is being duplicated in query string. It must replace the value if same key is found. I believe this can be handled in form.py#L36.", "issue_status": "Open", "issue_reporting_time": "2017-09-10T12:45:51Z"}, "196": {"issue_url": "https://github.com/scrapy/scrapy/issues/2916", "issue_id": "#2916", "issue_summary": "SSL website. `twisted.internet.error.ConnectionLost`", "issue_description": "unk2k commented on Sep 7, 2017\nHi everybody!\nI catch this error on both OS. This HTTPS site can't be downloaded via scrapy (twisted). I looked on this issue board and I don't found solution.\nBoth: Debian 9 / Mac OS\n$ scrapy shell \"https://wwwnet1.state.nj.us/\"\n2017-09-07 16:23:02 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n2017-09-07 16:23:02 [scrapy.utils.log] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2017-09-07 16:23:02 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2017-09-07 16:23:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-09-07 16:23:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-09-07 16:23:03 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-09-07 16:23:03 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-09-07 16:23:03 [scrapy.core.engine] INFO: Spider opened\n2017-09-07 16:23:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://wwwnet1.state.nj.us/> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2017-09-07 16:23:03 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://wwwnet1.state.nj.us/> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2017-09-07 16:23:04 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://wwwnet1.state.nj.us/> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nTraceback (most recent call last):\n  File \"scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/lib/python3.5/site-packages/scrapy/cmdline.py\", line 149, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/lib/python3.5/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/lib/python3.5/site-packages/scrapy/cmdline.py\", line 156, in _run_command\n    cmd.run(args, opts)\n  File \"/lib/python3.5/site-packages/scrapy/commands/shell.py\", line 73, in run\n    shell.start(url=url, redirect=not opts.no_redirect)\n  File \"/lib/python3.5/site-packages/scrapy/shell.py\", line 48, in start\n    self.fetch(url, spider, redirect=redirect)\n  File \"/lib/python3.5/site-packages/scrapy/shell.py\", line 115, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/lib/python3.5/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/lib/python3.5/site-packages/twisted/python/failure.py\", line 385, in raiseException\n    raise self.value.with_traceback(self.tb)\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nMac OSx:\n$ scrapy version -v\nScrapy    : 1.4.0\nlxml      : 3.8.0.0\nlibxml2   : 2.9.4\ncssselect : 1.0.1\nparsel    : 1.2.0\nw3lib     : 1.18.0\nTwisted   : 17.9.0rc1\nPython    : 3.5.1 (default, Jan 22 2016, 08:54:32) - [GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)]\npyOpenSSL : 17.2.0 (OpenSSL 1.1.0f  25 May 2017)\nPlatform  : Darwin-16.7.0-x86_64-i386-64bit\nDebian 9:\n$ scrapy version -v\nScrapy    : 1.4.0\nlxml      : 3.8.0.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.2.0\nw3lib     : 1.18.0\nTwisted   : 17.9.0rc1\nPython    : 3.4.2 (default, Oct  8 2014, 10:45:20) - [GCC 4.9.1]\npyOpenSSL : 17.2.0 (OpenSSL 1.1.0f  25 May 2017)\nPlatform  : Linux-3.16.0-4-amd64-x86_64-with-debian-8.7\nMac OSx:\n$ openssl s_client -connect wwwnet1.state.nj.us:443 -servername wwwnet1.state.nj.us\nCONNECTED(00000003)\n140736760988680:error:140790E5:SSL routines:ssl23_write:ssl handshake failure:s23_lib.c:177:\n---\nno peer certificate available\n---\nNo client certificate CA names sent\n---\nSSL handshake has read 0 bytes and written 336 bytes\n---\nNew, (NONE), Cipher is (NONE)\nSecure Renegotiation IS NOT supported\nCompression: NONE\nExpansion: NONE\nNo ALPN negotiated\nSSL-Session:\n    Protocol  : TLSv1.2\n    Cipher    : 0000\n    Session-ID: \n    Session-ID-ctx: \n    Master-Key: \n    Key-Arg   : None\n    PSK identity: None\n    PSK identity hint: None\n    SRP username: None\n    Start Time: 1504790705\n    Timeout   : 300 (sec)\n    Verify return code: 0 (ok)\n---\nDebian 9:\nCONNECTED(00000003)\n---\nCertificate chain\n 0 s:/C=US/ST=New Jersey/L=Trenton/O=New Jersey State Government/OU=E-Gov Services - wwwnet1.state.nj.us/CN=wwwnet1.state.nj.us\n   i:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec Class 3 Secure Server SHA256 SSL CA\n---\nServer certificate\n-----BEGIN CERTIFICATE-----\n<cut out>\n-----END CERTIFICATE-----\n<cut out>\n---\nNo client certificate CA names sent\n---\nSSL handshake has read 1724 bytes and written 635 bytes\n---\nNew, TLSv1/SSLv3, Cipher is DES-CBC3-SHA\nServer public key is 2048 bit\nSecure Renegotiation IS NOT supported\nCompression: NONE\nExpansion: NONE\nSSL-Session:\n    Protocol  : TLSv1\n    Cipher    : DES-CBC3-SHA\n    Session-ID: 930F00007F5944DC3C6010F96E95E7FA63656EF5EA35508B055078CEC249DC38\n    Session-ID-ctx:\n    Master-Key: 27B02D427F006A57B121CCEFEAA7F33B870DE262848BB6F851242F48F051ABB77BA4ED06706766EE8EE55F6643C9FF55\n    Key-Arg   : None\n    PSK identity: None\n    PSK identity hint: None\n    SRP username: None\n    Start Time: 1504790821\n    Timeout   : 300 (sec)\n    Verify return code: 21 (unable to verify the first certificate)\n---\nThanks you for your time.", "issue_status": "Open", "issue_reporting_time": "2017-09-07T13:30:51Z"}, "197": {"issue_url": "https://github.com/scrapy/scrapy/issues/2912", "issue_id": "#2912", "issue_summary": "Download delay and autothrottle do not work when cache is enabled", "issue_description": "ollieglass commented on Sep 4, 2017\nEnabling the cache by setting HTTPCACHE_ENABLED = True causes any download delay or autothrottle settings to be ignored.", "issue_status": "Open", "issue_reporting_time": "2017-09-03T20:03:15Z"}, "198": {"issue_url": "https://github.com/scrapy/scrapy/issues/2900", "issue_id": "#2900", "issue_summary": "_get_sitemap_body() from SitemapSpider is not compatible with all sitemaps", "issue_description": "lolobosse commented on Aug 27, 2017\nHi there,\nHave a look at the following code:\nclass [PlaceHolder]nanzeigen(SitemapSpider):\n    name = '[PlaceHolder]nanzeigen.de '\n\n    sitemap_urls=['https://www.[PlaceHolder]nanzeigen.de/sitemap.xml']\n\n    sitemap_rules = [\n        ('[PlaceHolder]nanzeigen.de', 'parse_page'),\n    ]\nREPLACE [PlaceHolder] BY 'stelle'\nIt will return Ignoring invalid sitemap for all 2nd level sitemaps because this server returns that the content is html (whereas it is XML indeed): Content-Type: ['text/html; charset=utf-8']\nTo me, it sounds reasonable to check if the response can be parsed in XML and if not then return that the body is not a sitemap.\nSomething like this:\ndef _get_sitemap_body(self, response):\n        \"\"\"Return the sitemap body contained in the given response,\n        or None if the response is not a sitemap.\n        \"\"\"\n        if isinstance(response, XmlResponse):\n            return response.body\n        elif is_gzipped(response):\n            return gunzip(response.body)\n        elif response.url.endswith('.xml'):\n            return response.body\n        elif response.url.endswith('.xml.gz'):\n            return gunzip(response.body)\n        else:\n            try:\n              #Parse the sitemap in XML\n              #Return response.body\n            except:\n               #return None\nIs that something which could be considered as doable?", "issue_status": "Open", "issue_reporting_time": "2017-08-27T12:32:00Z"}, "199": {"issue_url": "https://github.com/scrapy/scrapy/issues/2893", "issue_id": "#2893", "issue_summary": "Feature suggest: add downloaded/uptodate status to information about downloaded media", "issue_description": "Contributor\ndjunzu commented on Aug 23, 2017\nCurrently File/Image pipelines populate files/images fields with dicts containing information about the downloaded files (the downloaded path, the original scraped url, and the file checksum). It would be useful to have downloaded/uptodate status in this dict (motivation).\nIt goes along with other features requests such as having width/height of images also in the dict output.", "issue_status": "Open", "issue_reporting_time": "2017-08-22T22:25:26Z"}, "200": {"issue_url": "https://github.com/scrapy/scrapy/issues/2881", "issue_id": "#2881", "issue_summary": "Error when install scrapy in window by using pip install scrapy", "issue_description": "lhkthomas commented on Aug 11, 2017 \u2022\nedited\nErrors exists about the Twisted when installing scrapy in window by using pip install scrapy. Any one know how to fix it.\n copying src\\twisted\\internet\\test\\_yieldfromtests.py.3only -> build\\lib.win32-2.7\\twisted\\internet\\test\n    creating build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\chain.pem -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\not-a-certificate -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing1.pem -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing2-duplicate.pem -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing2.pem -> build\\lib.win32-2.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\mail\\test\\rfc822.message -> build\\lib.win32-2.7\\twisted\\mail\\test\n    copying src\\twisted\\python\\test\\_deprecatetests.py.3only -> build\\lib.win32-2.7\\twisted\\python\\test\n    copying src\\twisted\\words\\im\\instancemessenger.glade -> build\\lib.win32-2.7\\twisted\\words\\im\n    copying src\\twisted\\words\\xish\\xpathparser.g -> build\\lib.win32-2.7\\twisted\\words\\xish\n    running build_ext\n    building 'twisted.test.raiser' extension\n    error: INCLUDE environment variable is empty\n\n    ----------------------------------------\nCommand \"c:\\python27\\python.exe -u -c \"import setuptools, tokenize;__file__='c:\\\\users\\\\thomas~1\\\\appdata\\\\local\\\\temp\\\\pip-build-3oirm6\\\\Twisted\\\\setup.py';\nf=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\"\n install --record c:\\users\\thomas~1\\appdata\\local\\temp\\pip-xvbgd2-record\\install-record.txt --single-version-externally-managed --compile\" \nfailed with error code 1 in c:\\users\\thomas~1\\appdata\\local\\temp\\pip-build-3oirm6\\Twisted\\\n\ud83d\udc4d 5", "issue_status": "Open", "issue_reporting_time": "2017-08-11T07:51:22Z"}, "201": {"issue_url": "https://github.com/scrapy/scrapy/issues/2879", "issue_id": "#2879", "issue_summary": "Is it possible to get time to first byte (TTFB)?", "issue_description": "Kookabura commented on Aug 10, 2017\nHello,\nIs it possible to get time to first byte (TTFB) from request\\response? I can get download_latency but this is a time including downloading. I've looked through middlewares but couldn't understand how to get TTFB.", "issue_status": "Open", "issue_reporting_time": "2017-08-10T15:13:09Z"}, "202": {"issue_url": "https://github.com/scrapy/scrapy/issues/2842", "issue_id": "#2842", "issue_summary": "XMLFeedSpider iternodes iterator does not work on XML document with namespace", "issue_description": "Contributor\nredapple commented on Jul 20, 2017 \u2022\nedited\n(Opening the issue so that we track it, although it is already known.)\nSample input document:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n<url><loc>http://www.argos.ie/static/Product/partNumber/2353030.htm</loc></url>\n<url><loc>http://www.argos.ie/static/Product/partNumber/2717339.htm</loc></url>\n(...)\nSymptom:\nWith Scrapy 1.4.0 (and earlier for sure)\nUsing XMLFeedSpider and the default iternodes iterator, nodes using itertag='loc' cannot be found,\n    (...) site-packages/scrapy/utils/iterators.py\", line 31, in xmliter\n        yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]\n    exceptions.IndexError: list index out of range\nand registering namespaces and using itertag='prefix:loc' does not work either.\nRecently seen (again) on StackOverflow.\nWas already discussed on scrapy-users.\nThere's a WIP PR #861. Last comments were about moving to iterparse-based implementation", "issue_status": "Open", "issue_reporting_time": "2017-07-20T10:16:18Z"}, "203": {"issue_url": "https://github.com/scrapy/scrapy/issues/2825", "issue_id": "#2825", "issue_summary": "Add debug print of config file path (if found and used)", "issue_description": "Contributor\nredapple commented on Jul 13, 2017\nMotivation: https://stackoverflow.com/questions/45076871/scrapy-is-trying-to-find-a-module-that-doesnt-exist\nWe could (DEBUG) log which scrapy.cfg is being used to configure settings.", "issue_status": "Open", "issue_reporting_time": "2017-07-13T14:01:00Z"}, "204": {"issue_url": "https://github.com/scrapy/scrapy/issues/2810", "issue_id": "#2810", "issue_summary": "Issue with Scrapy Install", "issue_description": "tsgoud69 commented on Jul 2, 2017\nHi,\nI am new to python and trying to install scrapy in my virtual env. But its fails with following error. Can any one susggest what is the issue?\npython --version\nPython 2.7.13\nvirtualenv --version\n15.1.0\npip install scrapy\nCollecting scrapy\nUsing cached Scrapy-1.4.0-py2.py3-none-any.whl\nCollecting service-identity (from scrapy)\nUsing cached service_identity-17.0.0-py2.py3-none-any.whl\nCollecting parsel>=1.1 (from scrapy)\nDownloading parsel-1.2.0-py2.py3-none-any.whl\nRequirement already satisfied: six>=1.5.2 in /home/swaraj/E2/VenvP27E1/lib/python2.7/site-packages (from scrapy)\nCollecting w3lib>=1.17.0 (from scrapy)\nDownloading w3lib-1.17.0-py2.py3-none-any.whl\nCollecting lxml (from scrapy)\nDownloading lxml-3.8.0-cp27-cp27m-manylinux1_x86_64.whl (6.8MB)\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.8MB 172kB/s\nCollecting Twisted>=13.1.0 (from scrapy)\nCould not find a version that satisfies the requirement Twisted>=13.1.0 (from scrapy) (from versions: )\nNo matching distribution found for Twisted>=13.1.0 (from scrapy)\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2017-07-02T06:19:24Z"}, "205": {"issue_url": "https://github.com/scrapy/scrapy/issues/2808", "issue_id": "#2808", "issue_summary": "Mac os Python pip3 install Scrapy or Twisted gives\u201cCommand \u201dpython setup.py egg_info\u201c failed with error code 1 in /private/tmp/pip-build-51x6vr4n/Twisted/\u201d", "issue_description": "ShayChris commented on Jul 1, 2017\nsudo pip3 install Scrapy -i https://pypi.douban.com/simple\nCommand \"python setup.py egg_info\" failed with error code 1 in /private/tmp/pip-build-s3504tu6/Twisted/\nso I use sudo pip3 install Scrapy -i https://pypi.douban.com/simple but failed again.\nPlease give me some advices, Thank you so much\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2017-07-01T02:48:45Z"}, "206": {"issue_url": "https://github.com/scrapy/scrapy/issues/2803", "issue_id": "#2803", "issue_summary": "Feature suggestion: Preserve Header Order", "issue_description": "rjbks commented on Jun 23, 2017 \u2022\nedited\nI know it has been brought up some time ago (issue #223 in 2013 I believe). And at that time it was seen as not necessary with the respondent asking for some example sites which relied on header order. While I cannot say for sure that some of the sites I've had issues with are giving me problems because of header order, there have been a few papers (I can only recall this one for now: http://www.letmetrackyou.org/paper.pdf) which mention header construction/order as a way to fingerprint browsers, or in this case determine whether the UA string has been spoofed. I have tried switching CaselessDict to inherit from OrderedDict switching all references in the class from dict to OrderedDict and the Headers class seems to (in this very limited example) construct the headers appropriately.\nWould there be any drawbacks in making this change?\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2017-06-23T00:00:01Z"}, "207": {"issue_url": "https://github.com/scrapy/scrapy/issues/2802", "issue_id": "#2802", "issue_summary": "Support ISO 8601 timestamps in logging", "issue_description": "dalbani commented on Jun 22, 2017\nI haven't been able to configure logging via LOG_DATEFORMAT to output ISO 8601 timestamps.\nGiven that the logging formatter is passed local times apparently (??), I suppose that I need to use the %z qualifier.\nBut %z always returns +0000 in my configuration (Ubuntu 16.04, Linux 2.7) \u2014 whereas in my case %Z correctly produces CEST.\nMy understanding is that Python's logging.Formatter() doesn't support timezone offset: https://stackoverflow.com/questions/27858539/python-logging-module-emits-wrong-timezone-information##answer-27865750\nAm I missing something obvious to be able to produce ISO 8601 timestamps in my Scrapy logs?", "issue_status": "Open", "issue_reporting_time": "2017-06-22T07:40:33Z"}, "208": {"issue_url": "https://github.com/scrapy/scrapy/issues/2792", "issue_id": "#2792", "issue_summary": "[idea] Adding an option to profile the spider using vmprof", "issue_description": "Member\nParth-Vader commented on Jun 15, 2017 \u2022\nedited\nvmprof is a great tool for profiling the spider and the results can be uploaded to it's server (using --web option).\nUsers would be able to submit performance issues using the results obtained, and we could check if some parts are taking unexpectedly long time.\nExamples : Benchmark Spider Result\nThis could be implemented like the --profile option.\ncc @lopuhin", "issue_status": "Open", "issue_reporting_time": "2017-06-15T08:13:45Z"}, "209": {"issue_url": "https://github.com/scrapy/scrapy/issues/2749", "issue_id": "#2749", "issue_summary": "Feature Request: Item class field access and item creation using this", "issue_description": "Manslow commented on May 18, 2017\nItems provide a simple way to structure data. However, as items are currently implemented, you can only populate an item by referencing a field using a string that matches the field's name e.g item['field_name'] = value. This becomes a problem when you want to change the name of a field after creation. It also requires that you remember the name of the fields defined for an item.\nI think it would be nice to be able to populate items using syntax like the following:\nitem = Item()\nitem[Item.field] = value\nIn this way, you can use code completion in an IDE to be sure that you are referencing a field the Item class has defined and can leverage refactoring to change the names of fields across your entire project wherever Item.field is referenced.\nPerhaps there is a good reason why this option was discarded during development but it isn't obvious to me why.", "issue_status": "Open", "issue_reporting_time": "2017-05-18T16:30:35Z"}, "210": {"issue_url": "https://github.com/scrapy/scrapy/issues/2744", "issue_id": "#2744", "issue_summary": "[idea] HttpCacheMiddleware could be further enhanced", "issue_description": "Contributor\nstarrify commented on May 17, 2017\nThe current design of HttpCacheMiddleware:\nChecks whether a request hits the cache in process_request\nStores a response to the cache storage in process_response\nThat makes it possible to download a same resource multiple times within a same job, given that a following request to the same resource comes before the first one returns.\nIt's possible to enhance this middleware by storing the state and returning a deferred when there is a pending request of the same resource.", "issue_status": "Open", "issue_reporting_time": "2017-05-17T05:48:53Z"}, "211": {"issue_url": "https://github.com/scrapy/scrapy/issues/2733", "issue_id": "#2733", "issue_summary": "Import Request in the Template file", "issue_description": "zhongdai commented on May 10, 2017\nI have been creating many spiders recently, and I noticed I had to add below line every time\nfrom scrapy.http import Request\nI think for most cases we need the Request to crawl to other pages, only for very simple spider we don't need that. Is it possible to include that line to the template?", "issue_status": "Open", "issue_reporting_time": "2017-05-09T22:29:12Z"}, "212": {"issue_url": "https://github.com/scrapy/scrapy/issues/2726", "issue_id": "#2726", "issue_summary": "Expose certificate information on HTTPS responses", "issue_description": "alex commented on May 6, 2017\nFor scraper I'm working on, I'd like to have access to the certificate supplied by the server for HTTPS requests. As far as I can tell, this is not currently exposed on either requests or responses.", "issue_status": "Open", "issue_reporting_time": "2017-05-06T00:12:47Z"}, "213": {"issue_url": "https://github.com/scrapy/scrapy/issues/2711", "issue_id": "#2711", "issue_summary": "Scrapy capitalizes headers for request", "issue_description": "kspiridonova commented on Apr 15, 2017\nI'm setting the headers following way\nheaders = {\n    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n    'cache-control': 'no-cache',\n    ...\n}\nAnd calling request like that:\nyield scrapy.Request(url='https:/myurl.com/', callback=self.parse, headers=headers, cookies=cookies, meta={'proxy': 'http://localhost:8888'})\nAnd it makes that scrapy capitalizes all these headers and it looks like that (I'm using Charles proxy for debugging):\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nCache-Control: no-cache\nAnd this is not working correctly for my case.\nIf I'm using curl and set headers lowercase\naccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\ncache-control: no-cache\neverything works like a charm.\nIs there any way how I can disable this capitalizing behavior in Scrapy?\nThanks for any help!\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2017-04-15T15:00:46Z"}, "214": {"issue_url": "https://github.com/scrapy/scrapy/issues/2702", "issue_id": "#2702", "issue_summary": "error at telnet shutdown in Python 3", "issue_description": "Member\nkmike commented on Apr 6, 2017\nScrapy version:\nScrapy    : 1.3.3\nlxml      : 3.7.3.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 3.5.2 (default, Nov 17 2016, 17:05:23) - [GCC 5.4.0 20160609]\npyOpenSSL : 0.15.1 (OpenSSL 1.0.2g  1 Mar 2016)\nPlatform  : Linux-4.4.0-59-generic-x86_64-with-Ubuntu-16.04-xenial\nI've got this exception on shutdown:\n2017-04-05 18:18:10 [scrapy.core.engine] INFO: Spider closed (finished)\n2017-04-05 18:18:10 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method TelnetConsole.stop_listening of <scrapy.extensions.telnet.TelnetConsole object at 0x7fbb993bfeb8>>\nTraceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/scrapy/extensions/telnet.py\", line 63, in stop_listening\n    self.port.stopListening()\nAttributeError: 'TelnetConsole' object has no attribute 'port'\nNot sure if it is a real problem, but it'd be nice to shut down without error messages. No idea how to reproduce it, I've only seen it once.", "issue_status": "Open", "issue_reporting_time": "2017-04-05T18:30:50Z"}, "215": {"issue_url": "https://github.com/scrapy/scrapy/issues/2701", "issue_id": "#2701", "issue_summary": "Make Exceptions Handled by Retry Middleware Configurable", "issue_description": "tad3j commented on Apr 5, 2017\nI was having troubles finding which exceptions are handled by Retry Middleware, so I asked on freenode scrapy irc channel (should obviously check the source code). Paul from scrapinhub helped me there and pointed me to the source code.\nBeside that he also asked me to open an issue here.\nHe gave a great suggestion to make exceptions handled by RetryMiddleware configurable, so if we don't like defaults we can configure our own.\nBeside that he said that the handled exceptions in question are not documented well; I'm just mentioning it here, didn't want to open another issue (I guess by adding them to settings will also require some documentation).", "issue_status": "Open", "issue_reporting_time": "2017-04-05T11:37:24Z"}, "216": {"issue_url": "https://github.com/scrapy/scrapy/issues/2680", "issue_id": "#2680", "issue_summary": "name collision of spider with existing module", "issue_description": "swoertz commented on Mar 23, 2017\nThe problem arises when I generate a spider with the name of an already existing module e.g., csv, urllib,...\nMy guess is that the import mechanism imports the existing module, which most likely does not contain a spider, instead of the generated spider. Hence the no spider found message.\n$ scrapy version\nScrapy 1.3.3\n$ scrapy genspider csv csv.com\nCreated spider 'csv' using template 'basic' \n$ scrapy runspider csv.py \n2017-03-23 16:55:14 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n2017-03-23 16:55:14 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\nUsage\n=====\n  scrapy runspider [options] <spider_file>\n\nrunspider: error: No spider found in file: csv.py", "issue_status": "Open", "issue_reporting_time": "2017-03-23T16:03:13Z"}, "217": {"issue_url": "https://github.com/scrapy/scrapy/issues/2658", "issue_id": "#2658", "issue_summary": "large response causes scrapy to hang", "issue_description": "rjbks commented on Mar 17, 2017\nI have tried stackoverflow but haven't received any answers for this as of yet.\nscrapy hangs on large response\nSummary:\nUsing scrapy to make thousands of API calls, all of them work fine except for 1 which returns a response of around 2GB (takes roughly 4 mins to complete using requests library). I have increased or disabled the related settings which may limit download size or timeout (DOWNLOAD_MAXSIZE DOWNLOAD_TIMEOUT). The logs show a 200 response then the warning for download size (which I left enabled just to indicate some form of progress) I then get 3 consecutive INFO logs from the stat tracker (1 minute apart), then it just hangs. I have left it running for up to 30 mins then I had to force quit (control-c does not work). Based on that, it would seem to hang after roughly 3 minutes after initiating that API call without any more logging whether I leave the download_timeout enable (for either 6.5 or 7 minutes when several attempts with requests package yields results in about 4 minutes) or disabled. Any ideas what could be causing this?", "issue_status": "Open", "issue_reporting_time": "2017-03-16T23:59:41Z"}, "218": {"issue_url": "https://github.com/scrapy/scrapy/issues/2653", "issue_id": "#2653", "issue_summary": "http11 connections are left opened", "issue_description": "Member\nkmike commented on Mar 14, 2017\nI haven't checked it, but there is a #999 (comment) by @adiroiban which suggests HTTP11DownloadHandler.close implementation is not complete.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2017-03-14T10:01:09Z"}, "219": {"issue_url": "https://github.com/scrapy/scrapy/issues/2645", "issue_id": "#2645", "issue_summary": "Order of calling close_spider in pipelines", "issue_description": "debosmit commented on Mar 13, 2017\nprocess_item is called based on the order of the pipeline classes mentioned in ITEM_PIPELINES setting. But, close_spider follows the exact opposite order with the close_spider of the last pipeline getting called first [link].\nI can't think of a good reason for this. Should I submit a PR?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2017-03-12T23:24:33Z"}, "220": {"issue_url": "https://github.com/scrapy/scrapy/issues/2633", "issue_id": "#2633", "issue_summary": "Base classes for the item pipeline and middleware", "issue_description": "Contributor\njorenham commented on Mar 9, 2017 \u2022\nedited\nIn the pipeline docs it says:\nEach item pipeline component is a Python class that must implement the following method\nI believe that it would be easier for the user and for documentation purposes to have an abstract class e.g. ItemPipeline available that raises a NotImplementedError if they forget to implement the required methods. This could also be extended to the optional methods so the user can see directly from the code which methods are supported.\nThis could also be applied to the downloader middleware, spider middleware and extensions.\n\ud83d\udc4d 5", "issue_status": "Open", "issue_reporting_time": "2017-03-08T20:33:56Z"}, "221": {"issue_url": "https://github.com/scrapy/scrapy/issues/2624", "issue_id": "#2624", "issue_summary": "scrapy shell - bug in processing escaped URLs", "issue_description": "Contributor\nharshasrinivas commented on Mar 6, 2017\nFor eg: URL - https://altoona.craigslist.org/search/sss?query=cars&sort=rel&searchNearby=1 when pasted on the command line (shell-specific) creates a few escaping characters - backslashes\nThe following command gives a 404:\n$ scrapy shell 'https://altoona.craigslist.org/search/sss\\?query\\=cars\\&sort\\=rel\\&searchNearby\\=1'\nI saw a similar discussion in #1232 - where urllib.parse.unquote() was used to fix it.\nIn this case even unquote doesn't seem to work.\n>>> unquote('https://altoona.craigslist.org/search/sss\\?query\\=cars\\&sort\\=rel\\&searchNearby\\=1')\n'https://altoona.craigslist.org/search/sss\\\\?query\\\\=cars\\\\&sort\\\\=rel\\\\&searchNearby\\\\=1'\nEven though URL-character escaping is an issue specific to the shell, wouldn't it be awesome if Scrapy processes such URLs automatically?", "issue_status": "Open", "issue_reporting_time": "2017-03-06T06:43:23Z"}, "222": {"issue_url": "https://github.com/scrapy/scrapy/issues/2608", "issue_id": "#2608", "issue_summary": "scrapy runspider in case of several spiders in a file", "issue_description": "Member\nlopuhin commented on Mar 2, 2017\nDocs say: \"run a spider self-contained in a Python file, without having to create a project.\"\nIn case there are multiple spiders in a file, one of them will be selected and run, but spider selection is not deterministic.\nAlthough it's technically not backwards compatible, maybe the best course of action would be to raise an error in this case and do not run any spiders?", "issue_status": "Open", "issue_reporting_time": "2017-03-01T19:37:58Z"}, "223": {"issue_url": "https://github.com/scrapy/scrapy/issues/2607", "issue_id": "#2607", "issue_summary": "add params kwarg to scrapy.Request()", "issue_description": "Contributor\npawelmhm commented on Mar 1, 2017 \u2022\nedited\npython-requests have useful params kwargs, you can do\nurl = 'http://aaa'\nparams = {\n    'expand': 'variations,informationBlocks,customisations',\n}\n\nresponse = requests.request(\n    method='GET',\n    url=url,\n    headers=headers,\n    params=params,\n)\nand it will resut in http://aaa?expand=variations,informationBlocks,customizations\nwhat do you think about adding params kwarg to scrapy,Request()? It would simplify work, there would be no need to urlencode querystring if it's a dict and concatenate strings for url.\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2017-03-01T09:37:41Z"}, "224": {"issue_url": "https://github.com/scrapy/scrapy/issues/2602", "issue_id": "#2602", "issue_summary": "Misleading error message about SPIDER_MODULES", "issue_description": "Contributor\ndjunzu commented on Feb 28, 2017 \u2022\nedited\nThis is a side effect from #2433.\nTraceback (most recent call last):\n  File \"/home/djunzu/.pyenv/versions/2.7.13/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 31, in _load_all_spiders\n    for module in walk_modules(name):\n  File \"/home/djunzu/.pyenv/versions/2.7.13/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 63, in walk_modules\n    mod = import_module(path)\n  File \"/home/djunzu/.pyenv/versions/2.7.13/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/home/djunzu/crawler/crawler/spiders/__init__.py\", line 4, in <module>\n    from crawler.loaders import PersonLoader\n  File \"/home/djunzu/crawler/crawler/loaders.py\", line 4, in <module>\n    from processors.phone import PhoneOutputProcessor, extract_phone\nImportError: cannot import name extract_phone\nCould not load spiders from module 'crawler.spiders.development'. Check SPIDER_MODULES setting\n  warnings.warn(msg, RuntimeWarning)\n\"Check SPIDER_MODULES setting\" is misleading; the error has nothing to do with SPIDER_MODULES.\nI would suggest to remove this sentence.\n(And I guess the last line should not be there, but it shows up in log.)", "issue_status": "Open", "issue_reporting_time": "2017-02-28T00:14:11Z"}, "225": {"issue_url": "https://github.com/scrapy/scrapy/issues/2600", "issue_id": "#2600", "issue_summary": "Add a RequestSet class for grouping requests", "issue_description": "Contributor\nimmerrr commented on Feb 28, 2017\nThis came up in #2582.\nI think scrapy lacks a means of grouping requests and sharing some state between them that would become irrelevant once the last request in the group has been dealt with and should be cleaned up. Tracking groups manually with request callbacks & errorbacks is error-prone and hard to do right. A RequestSet class that allows the user to do something like\ndef parse(self, response):\n    ....\n    yield RequestSet(Request(...) for ...)\nand forget about shared state management would be nice.\nI think of RequestSet as something that:\nhas a DeferredList-like API (asyncio.gather has the drawback of being a function rather than an object)\nhas its own callback to be run when the request set is dealt with, probably an errback too, for symmetry has a Deferred that would fire when the RequestSet is being cleaned up\nhas its own 'meta' dictionary, much like the one shared between Request & Response objects\nknows that it contains Requests as deferreds\nsince it knows that it contains Requests, it can piggyback on the first received value, which should be a response, and do response.meta['request_set'] = self so that the callbacks can access the shared data\n(maybe) it should silently copy fields from request_set.meta to response.meta if they are unset in request.meta, or maybe even make request.meta a ChainDict with fallback to request_set.meta so that middlewares don't have to trace through nested metadata dictionaries to check the fields they are interested in\nit should wrap requests coming from its respective response callbacks unless specifically asked not to do that, e.g. with request.meta['request_set'] = None\n(maybe) it should be possible to return other RequestSet from response callbacks\n(maybe) returned RequestSets should be made nestable, i.e. to keep the parent RequestSet alive during their lifetime if not explicitly asked not to with request_set.meta['request_set'] = None (if nesting is considered, the request_set metadata key seems redundant and we might consider parent_set instead.\nnot sure if it's worth it to make them nestable, i.e. if a certain response callback produces a different RequestSet, should it be owned by the parent request set?\nOne more thing to consider is cross-referencing RequestSets, i.e. when two requests that should belong to one RequestSet are produced by different callbacks and thus have different scopes. Maybe a simple WeakValueDictionary would suffice to lookup the sets and ensure the references are cleaned up as necessary. But then you'd have the usual get-or-create operation, that might be worth creating an etalon implementation for.", "issue_status": "Open", "issue_reporting_time": "2017-02-27T21:33:37Z"}, "226": {"issue_url": "https://github.com/scrapy/scrapy/issues/2594", "issue_id": "#2594", "issue_summary": "Jupyter run error ReactorNotRestartable", "issue_description": "liveresume commented on Feb 24, 2017 \u2022\nedited\nI am able to run Scrapy in a Jupyter notebook. The first time it works fine.\nHowever any subsequent attempts will fail with errors below.\nTo get it working again I must restart the python kernel. It looks like a problem of starting a reactor when one is already up and running.\nfrom scrapy.crawler import CrawlerProcess\n \nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\nprocess.crawl(BlogPostSpider)\nprocess.start() # the script will block here until the crawling is finished\n---------------------------------------------------------------------------\nReactorNotRestartable                     Traceback (most recent call last)\n<ipython-input-8-6d30d0f44f41> in <module>()\n     13 })\n     14 process.crawl(spider)\n---> 15 process.start() # the script will block here until the crawling is finished\n     16 \n     17 \n\n/opt/conda/lib/python3.5/site-packages/scrapy/crawler.py in start(self, stop_after_crawl)\n    278         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n    279         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n--> 280         reactor.run(installSignalHandlers=False)  # blocking call\n    281 \n    282     def _get_dns_resolver(self):\n\n/opt/conda/lib/python3.5/site-packages/twisted/internet/base.py in run(self, installSignalHandlers)\n   1240 \n   1241     def run(self, installSignalHandlers=True):\n-> 1242         self.startRunning(installSignalHandlers=installSignalHandlers)\n   1243         self.mainLoop()\n   1244 \n\n/opt/conda/lib/python3.5/site-packages/twisted/internet/base.py in startRunning(self, installSignalHandlers)\n   1220         \"\"\"\n   1221         self._installSignalHandlers = installSignalHandlers\n-> 1222         ReactorBase.startRunning(self)\n   1223 \n   1224 \n\n/opt/conda/lib/python3.5/site-packages/twisted/internet/base.py in startRunning(self)\n    728             raise error.ReactorAlreadyRunning()\n    729         if self._startedBefore:\n--> 730             raise error.ReactorNotRestartable()\n    731         self._started = True\n    732         self._stopped = False\n\nReactorNotRestartable: \n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2017-02-23T23:16:22Z"}, "227": {"issue_url": "https://github.com/scrapy/scrapy/issues/2592", "issue_id": "#2592", "issue_summary": "Improving headers values decoding (utf-8 vs latin1)", "issue_description": "Contributor\nrmax commented on Feb 24, 2017\nThis raises from #2589 where a server returns a non-UTF-8 header value. According to this RFC:\nHistorically, HTTP has allowed field content with text in the\nISO-8859-1 charset [ISO-8859-1], supporting other charsets only\nthrough use of [RFC2047] encoding. In practice, most HTTP header\nfield values use only a subset of the US-ASCII charset [USASCII].\nNewly defined header fields SHOULD limit their field values to\nUS-ASCII octets. A recipient SHOULD treat other octets in field\ncontent (obs-text) as opaque data.\nSo the latin1 encoding may be preferred over utf-8 which is the Scrapy's default. At the other hand, requests in some places uses utf-8 with a fallback to latin1: https://github.com/kennethreitz/requests/blob/d6f4818c0b40bc6c00433c013b7daaea83b2cd51/requests/models.py#L908\nFollowing @kmike's suggestion, it seems that decoding headers with utf-8 having iso-8859-1 as fallback is the best option.", "issue_status": "Open", "issue_reporting_time": "2017-02-23T20:49:23Z"}, "228": {"issue_url": "https://github.com/scrapy/scrapy/issues/2583", "issue_id": "#2583", "issue_summary": "add response.submit or Selector(List?).submit shortcut", "issue_description": "Member\nkmike commented on Feb 22, 2017 \u2022\nedited\nAs discussed in #1940, what do you think about adding response.submit shortcut for FormRequest.from_response? Proposed API:\ndef submit(self, \n    # Selector or SelectorList, e.g. response.xpath('//form')[1] \n    # or response.css('#myform')\n    sel=None, \n\n    # similar to FormRequest.from_response formcss, a shortcut \n    # for passing response.css in sel argument\n    css=None, \n\n    # similar to FormRequest.from_response formxpath, a shortcut \n    # for passing response.xpath in sel argument\n    xpath=None,  \n\n    formdata=None,  # same as FormRequest.from_response formdata\n    clickdata=None, # same as FormRequest.from_response clickdata\n    dont_click=False, # same as FormRequest.from_response dont_click\n    **kwargs  # same as FormRequest.from_response keyword arguments\n)\nDifferences from FormRequest API:\nSelector/SelectorList support;\nformcss / formxpath are renamed to just css/xpath;\nformid/formname/formnumber arguments are dropped. id is trivial with css. I'm on fence about dropping formnumber.\nOnly one of sel, css and xpath can be passed at the same time, there are no fallbacks.\nThere shouldn't be gotchas like #1163. If no forms are matched, exception is raised.\nI think it makes sense to wrap FormRequest instead of starting from scratch; this way issues fixed for FormRequest (like #667 or #508) will be fixed for response.submit as well, and vice versa.", "issue_status": "Open", "issue_reporting_time": "2017-02-22T14:33:21Z"}, "229": {"issue_url": "https://github.com/scrapy/scrapy/issues/2580", "issue_id": "#2580", "issue_summary": "ImportError: No module named 'twisted.persisted'", "issue_description": "kelein commented on Feb 21, 2017\nI haved install scrapy on Python 3.5.2, but when I exeute scrapy -v on command-line, it occurs:\n>> scrapy -v\nTraceback (most recent call last):\n  File \"/usr/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/usr/local/python3.5.2/lib/python3.5/site-packages/scrapy/__init__.py\", line 27, in <module>\n    from . import _monkeypatches\n  File \"/usr/local/python3.5.2/lib/python3.5/site-packages/scrapy/_monkeypatches.py\", line 20, in <module>\n    import twisted.persisted.styles  # NOQA\nImportError: No module named 'twisted.persisted'\nSo I enter ipython3 to ensure whether it's installed porperly:\nIn [2]: import twisted\nIn [3]: twisted.version\nOut[3]: Version('twisted', 15, 2, 1)\nBut when I import scrapy, the same as command-line:\nIn [4]: import scrapy\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n<ipython-input-4-51c73a18167b> in <module>()\n----> 1 import scrapy\n\n/usr/local/python3.5.2/lib/python3.5/site-packages/scrapy/__init__.py in <module>()\n     25 \n     26 # Apply monkey patches to fix issues in external libraries\n---> 27 from . import _monkeypatches\n     28 del _monkeypatches\n     29 \n\n/usr/local/python3.5.2/lib/python3.5/site-packages/scrapy/_monkeypatches.py in <module>()\n     18 # Undo what Twisted's perspective broker adds to pickle register\n     19 # to prevent bugs like Twisted#7989 while serializing requests\n---> 20 import twisted.persisted.styles  # NOQA\n     21 # Remove only entries with twisted serializers for non-twisted types.\n     22 for k, v in frozenset(copyreg.dispatch_table.items()):\n\nImportError: No module named 'twisted.persisted'\nI'm so confused, I just want to run a spaider, So help me ! \ud83c\udd98\n\u2764\ufe0f 2", "issue_status": "Open", "issue_reporting_time": "2017-02-21T09:26:44Z"}, "230": {"issue_url": "https://github.com/scrapy/scrapy/issues/2578", "issue_id": "#2578", "issue_summary": "Raising CloseSpider from DownloaderMiddleware doesn't work", "issue_description": "philonor commented on Feb 20, 2017\nBehavior:\nRaising a CloseSpider exception from a DownloaderMiddleware's process_response doesn't close the spider.\nInstead, the scraper only outputs CloseSpider to stdout.\nExpected behavior:\nSpider should shut down.\nWorkarounds:\nUse crawler.stop:\nself.crawler.stop()\nreturn None\nwhich requires to make self.crawler available in the DownloaderMiddleware via:\n    def __init__(self, crawler):\n        self.crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\nUse crawler._signal_shutdown() (doesn't work for me)\nReturn None from process_response (doesn't work for me)", "issue_status": "Open", "issue_reporting_time": "2017-02-20T16:32:07Z"}, "231": {"issue_url": "https://github.com/scrapy/scrapy/issues/2546", "issue_id": "#2546", "issue_summary": "DOC Add (short) explanation of return vs. yield in callbacks", "issue_description": "Contributor\nredapple commented on Feb 8, 2017\nFollow up to #2533\nQuite a few Scrapy users are also Python-newbies (and that's great!), and using yield in callbacks (and generators in general) can be something odd at first.\nIt would be great to explain different callback patterns:\nfor single object \"returns\"\nfor multi-item or multi-request \"returns\"\nfor mixing items and requests\n(and why/when return with a list is not necessarily a bad thing)", "issue_status": "Open", "issue_reporting_time": "2017-02-08T12:10:13Z"}, "232": {"issue_url": "https://github.com/scrapy/scrapy/issues/2516", "issue_id": "#2516", "issue_summary": "get_output_value behavior differs from load_item with empty data", "issue_description": "Contributor\nandrewbaxter commented on Jan 27, 2017\nIf an output processor requires a value and a field has no data, doing get_output_value will raise an exception whereas load_item doesn't. I expected get_output_value to return None in this case.", "issue_status": "Open", "issue_reporting_time": "2017-01-27T18:19:33Z"}, "233": {"issue_url": "https://github.com/scrapy/scrapy/issues/2504", "issue_id": "#2504", "issue_summary": "Unable to retreive http return code from ImagesPipeline (or MediaPipeline) in scrapy", "issue_description": "manisoftwartist commented on Jan 20, 2017\nI have a working spider scraping image URLs and placing them in image_urls field of a scrapy.Item. I have a custom pipeline that inherits from ImagesPipeline. When a specific URL returns a non-200 http response code (like say a 401 error). For instance, in the log files, I find\nWARNING:scrapy.pipelines.files:File (code: 404): Error downloading file from <GET http://a.espncdn.com/combiner/i%3Fimg%3D/i/headshots/tennis/players/full/425.png> referred in <None>\nWARNING:scrapy.pipelines.files:File (code: 307): Error downloading file from <GET http://www.fansshare.com/photos/rogerfederer/federer-roger-federer-406468306.jpg> referred in <None>\ndef item_completed(self, results, item, info):\n\nimage_paths = []\nfor download_status, x in results:\n    if download_status:\n        image_paths.append(x['path'])\n        item['images'] = image_paths  # update item image path\n        item['result_download_status'] = 1\n    else:\n        item['result_download_status'] = 0\n        #x.printDetailedTraceback()\n        logging.info(repr(x)) # x is a twisted failure object\n\nreturn item\nHowever, I am unable to capture the error codes 404,307 etc in my custom image pipeline in the item_completed() function:\ndef item_completed(self, results, item, info):\nimage_paths = []\nfor download_status, x in results:\n    if download_status:\n        image_paths.append(x['path'])\n        item['images'] = image_paths  # update item image path\n        item['result_download_status'] = 1\n    else:\n        item['result_download_status'] = 0\n        #x.printDetailedTraceback()\n        logging.info(repr(x)) # x is a twisted failure object\n        \nreturn item\nDigging through the scrapy source code, inside the media_downloaded() function in files.py, I found that for non-200 response codes, a warning is logged (which explains the above WARNING lines) and then a FileException is raised.\n    if response.status != 200:\n            logger.warning(\n                'File (code: %(status)s): Error downloading file from '\n                '%(request)s referred in <%(referer)s>',\n                {'status': response.status,\n                 'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n    \n            raise FileException('download-error')\nHow do I also access this response code so I can handle it in my custom image pipeline in item_completed() function?", "issue_status": "Open", "issue_reporting_time": "2017-01-19T20:02:40Z"}, "234": {"issue_url": "https://github.com/scrapy/scrapy/issues/2499", "issue_id": "#2499", "issue_summary": "exceptions.AttributeError: '_SIGCHLDWaker' object has no attribute 'doWrite'", "issue_description": "cp2587 commented on Jan 16, 2017 \u2022\nedited\nHello,\nThank your for your fantastic project. We are facing a really hard to solve bug while running scapy inside celery task. Sometimes we get this error:\nUnhandled Error\nTraceback (most recent call last):\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/python/log.py\", line 101, in callWithLogger\n    return callWithContext({\"system\": lp}, func, *args, **kw)\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/python/log.py\", line 84, in callWithContext\n    return context.call({ILogContext: newCtx}, func, *args, **kw)\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/python/context.py\", line 118, in callWithContext\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/python/context.py\", line 81, in callWithContext\n    return func(*args,**kw)\n--- <exception caught here> ---\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/internet/posixbase.py\", line 602, in _doReadOrWrite\n    why = selectable.doWrite()\nexceptions.AttributeError: '_SIGCHLDWaker' object has no attribute 'doWrite'\nI think it's related to a threading problem (I am not sure twisted is very thread safe) but maybe you already have an explanation ? Anyway, it seems bloated to run scrapy inside celery tasks because we need to recreate the crawler for each task, do you have any good advice on how to built a system to distribute crawls ? We are open to use scrapinghub but we have some very specific pipelines & download middlewares which prevent us to do so :(", "issue_status": "Open", "issue_reporting_time": "2017-01-16T11:53:50Z"}, "235": {"issue_url": "https://github.com/scrapy/scrapy/issues/2498", "issue_id": "#2498", "issue_summary": "ItemLoader: zeros as field values", "issue_description": "medse commented on Jan 15, 2017\nI'm scraping a eshop, there's a price xpath for an item's price, which is empty if the item is out of stock.\nI use ItemLoader, and add_xpath():\nitem.add_xpath('price', './/span[@class=\"price rub\"]/text()')\nI want to set the price to 0.0 if the item is missing, for this I check if the xpath is empty in the price_in declaration within the ItemLoader-based class:\nprice_in=Compose(TakeFirst(), lambda _: float(_) if _ else 0)\nBut the value of zero isn't stored in the _values dict because of the following code in the scrapy/loaders/init.py:_add_value():\n    def _add_value(self, field_name, value):\n        value = arg_to_iter(value)\n        processed_value = self._process_input_value(field_name, value)\n        if processed_value:\n            self._values[field_name] += arg_to_iter(processed_value)\nI don't know why the logic is like this, but it won't store neither zeros nor empty strings. Is it illegal? I use scrapy for about a week, so don't know the usage practices at all, but this seems strange to me.\nMaybe change the condition to \u201cprocessed_value is not None\u201d?", "issue_status": "Open", "issue_reporting_time": "2017-01-15T16:46:58Z"}, "236": {"issue_url": "https://github.com/scrapy/scrapy/issues/2488", "issue_id": "#2488", "issue_summary": "Feature Request: Adding \"add_jmes\" and \"replace_jmes\" method to ItemLoader", "issue_description": "Contributor\nIAlwaysBeCoding commented on Jan 10, 2017\nSo, currently the ItemLoader class has 6 methods for loading values:\nadd_xpath()\nreplace_xpath()\nadd_css()\nreplace_css()\nadd_value()\nreplace_value()\nCould we add another 2 more methods for loading data through JmesPath selectors. Currently, I have to use the SelectJmes processor to do this. Eventually, it looks really ugly and ends up taking so much line real estate.\nI did a hack where I extended the ItemLoader to include those 2 extra methods that are desperately needed.\nWhen there is json data to parse instead of html, JmesPath selectors are the best way to go for parsing, so there should be support for JmesPath selectors in the ItemLoader class as well.", "issue_status": "Open", "issue_reporting_time": "2017-01-10T09:02:27Z"}, "237": {"issue_url": "https://github.com/scrapy/scrapy/issues/2484", "issue_id": "#2484", "issue_summary": "Scrapy: how to rename project", "issue_description": "astrung commented on Jan 6, 2017\nI have created a new project with this command : scrapy startproject first_scrapy\nBut now i want to change this project name to \"web_crawler\" . After i tried to change project name , i can not start scrapy command . I got this error :\n  File \"c:\\python27\\lib\\runpy.py\", line 174, in _run_module_as_main\n  \"__main__\", fname, loader, pkg_name)\n  File \"c:\\python27\\lib\\runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"C:\\Python27\\Scripts\\scrapy.exe\\__main__.py\", line 9, in <module>\n  File \"c:\\python27\\lib\\site-packages\\scrapy\\cmdline.py\", line 108, in execute\n    settings = get_project_settings()\n  File \"c:\\python27\\lib\\site-packages\\scrapy\\utils\\project.py\", line 68, in get_project_settings\n    settings.setmodule(settings_module_path, priority='project')\n  File \"c:\\python27\\lib\\site-packages\\scrapy\\settings\\__init__.py\", line 282, in setmodule\n    module = import_module(module)\n  File \"c:\\python27\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\n  ImportError: No module named first_scrapy.settings\nSo how can i rename old project ?", "issue_status": "Open", "issue_reporting_time": "2017-01-06T08:03:52Z"}, "238": {"issue_url": "https://github.com/scrapy/scrapy/issues/2481", "issue_id": "#2481", "issue_summary": "Clean bad HTML", "issue_description": "Member\neliasdorneles commented on Jan 5, 2017\nThere are some cases of bad HTML that makes Scrapy (well, lxml really) to choke on the response content, and I was thinking it would make sense to add a CleanBadHtml middleware that could be optionally disabled.\nI've just stumbled on an example case from a real website, where the response had something like this (real phone number edited) in its content:\ntext = u'<a href=\"tel:111\\x00111\\x001111\">111-111-1111</a>'\nThe \\x00 is interpreted as end of input by lxml, so the selector ends up stopping right there:\n>>> parsel.Selector(text)\n<Selector xpath=None data=u'<html><body><a href=\"tel:111\"></a></body'>\n>>> parsel.Selector(text).extract()\nu'<html><body><a href=\"tel:111\"></a></body></html>'\nI'm not sure what's the best place to fix this, but I think we gotta do something about it either in Scrapy or in Parsel, because these HTML pages are accepted by the browsers, who ignore the null characters.\nFor this specific case, simply removing the \\x00 characters found in the body before passing to the selector would avoid the issue.\nWhat do you think?\nWhere do you think it would be the best place to do this?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2017-01-04T20:26:21Z"}, "239": {"issue_url": "https://github.com/scrapy/scrapy/issues/2476", "issue_id": "#2476", "issue_summary": "Consistent DOWNLOAD_WARNSIZE message", "issue_description": "mohmad-null commented on Jan 3, 2017\nMy logs have both of these messages:\nWARNING: Expected response size (135576123) larger than download warn size (33554432).\nWARNING: Received more bytes than download warn size (33554432) in request <GET https://example.com>.\nI'd suggest that both should include the url, or neither should. Ideally both as currently it's not readily possible to tell from the logs what URL triggered the former.", "issue_status": "Open", "issue_reporting_time": "2017-01-03T14:27:57Z"}, "240": {"issue_url": "https://github.com/scrapy/scrapy/issues/2473", "issue_id": "#2473", "issue_summary": "Issue with running scrapy spider from script.", "issue_description": "tituskex commented on Jan 2, 2017 \u2022\nedited\nHi, I'm trying to run scrapy from a script like this:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider(scrapy.Spider):\n    name = \"basic\"\n    allowed_domains = [\"web\"]\n    start_urls = ['http://www.example.com']\n\n    def parse(self, response):\n        l = ItemLoader(item=PropertiesItem(), response = response)\n        l.add_xpath('title', '//h1[1]/text()')\n\n        return l.load_item()\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\n\nprocess.crawl(MySpider)\nprocess.start()\nHowever, when I run this script I get the following error:\nFile \"/Library/Python/2.7/site-packages/Twisted-16.7.0rc1-py2.7-macosx-10.11-\nintel.egg/twisted/internet/_sslverify.py\", line 38, in <module>\nTLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,\nAttributeError: 'module' object has no attribute 'OP_NO_TLSv1_1'\nDoes anyone know how to fix this? Thanks in advance.\n\ud83d\udc4d 25", "issue_status": "Open", "issue_reporting_time": "2017-01-02T10:19:51Z"}, "241": {"issue_url": "https://github.com/scrapy/scrapy/issues/2472", "issue_id": "#2472", "issue_summary": "Scrapy loses trailing ?", "issue_description": "mohmad-null commented on Jan 1, 2017\nScrapy 1.2.1\nIf I populate start_urls with:\nhttp://localhost/myfile.json?\nMy actual request (as well as the contents of response.url) is to:\nhttp://localhost/myfile.json\nNote the trailing ? has been removed.\nI don't know if there are any servers that will give a different response depending on whether the ? is there or not, however I'm recording the URL's as keys, so the fact that my return URL is different from my sending-url is making life difficult.\nI can work around it, but I suspect this isn't meant to be happening, hence the report.", "issue_status": "Open", "issue_reporting_time": "2017-01-01T15:35:47Z"}, "242": {"issue_url": "https://github.com/scrapy/scrapy/issues/2468", "issue_id": "#2468", "issue_summary": "Can not access HTTPS web site with proxy", "issue_description": "haowg commented on Dec 27, 2016 \u2022\nedited\nwhen access a http url with a proxy, it worked\nbut a https url it can't work\nis this a bug?", "issue_status": "Open", "issue_reporting_time": "2016-12-27T07:00:03Z"}, "243": {"issue_url": "https://github.com/scrapy/scrapy/issues/2449", "issue_id": "#2449", "issue_summary": "Retry downloading and delay in media pipeline", "issue_description": "guohengkai commented on Dec 14, 2016 \u2022\nedited\nHow to implement retrying and delay in media pipeline? Seem that the DownloaderMiddleware does not work for pipeline. Thanks.", "issue_status": "Open", "issue_reporting_time": "2016-12-14T12:14:24Z"}, "244": {"issue_url": "https://github.com/scrapy/scrapy/issues/2448", "issue_id": "#2448", "issue_summary": "S3FilesStore: add option for AWS Signature v4 support", "issue_description": "vshlapakov commented on Dec 14, 2016\nNewer AWS locations will only authenticate clients that use AWS Signature Version 4.\nFor example, when using Images addon with a custom S3 location you can encounter with:\nboto.exception.S3ResponseError: S3ResponseError: 400 Bad Request\nThe authorization mechanism you have provided is not supported. Please use AWS4-HMAC-SHA256.\nThe problem is pretty well known: boto/boto#2741.\nBoto library supports v4 if you pass a host explicitly and set S3_USE_SIGV4 envvar. It's a bit different for botocore, but should work fine when providing a region short name.\nMy proposal is to implement it in a similar way to this approach: S3FilesStore could handle new AWS_HOST and AWS_S3_USE_SIGV4 settings, use it to set a corresponding envvar and pass the host to boto.S3Connection(or extract the region from host and pass it to botocore client).\n\ud83d\udc4d 4", "issue_status": "Open", "issue_reporting_time": "2016-12-14T11:23:03Z"}, "245": {"issue_url": "https://github.com/scrapy/scrapy/issues/2447", "issue_id": "#2447", "issue_summary": "memory leaks in image pipeline", "issue_description": "Contributor\npawelmhm commented on Dec 13, 2016\nIt seems to me that image pipeline is leaking memory in a very significant ways. I have spider that downloads lists of images. There were always problems with memory when downloading images, but now my list of images to download got larger and I thought about opening issue here.\nBasically after opening some images memory usage goes up and stays up (it's not reset to previous value). It might be some issue with PIL or it might be something we're doing in pipeline that is causing this. In any case this looks worrying and I think we should reflect on steps to take to limit this problem.\nFollowing code reproduces the problem (I know it's long but this is really shortest I could get), it relies on presence of images.txt file that contains list of urls to images.\nimport resource\nimport shutil\nimport sys\nimport tempfile\n\nimport scrapy\nfrom scrapy.pipelines.images import ImagesPipeline\nfrom scrapy.utils.test import get_crawler\nfrom twisted.internet import reactor\nfrom twisted.python import log\n\n\ndef log_memory(result):\n    mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n    log.msg(\"{} bytes\".format(mem))\n\n\nclass SomeSpider(scrapy.Spider):\n    name = 'foo'\n\n\ndef download_image(url, pipe, spider):\n    # Download image with pipeline.\n    item = {\n        'image_urls': [url]\n    }\n    dfd = pipe.process_item(item, spider)\n    dfd.addBoth(log_memory)\n    return dfd\n\n\nlog.startLogging(sys.stdout)\n# Directory must be removed, otherwise pipeline will not attempt to download.\nsome_dir = tempfile.mkdtemp()\ncrawler = get_crawler(settings_dict={'IMAGES_STORE': some_dir,\n                                     \"IMAGE_EXPIRES\": 0})\n\nspider = SomeSpider()\nspider.crawler = crawler\ncrawler.crawl(spider)\npipeline = ImagesPipeline.from_crawler(crawler)\npipeline.open_spider(spider)\n\n\ndef clean_up():\n    print(\"removing {}\".format(some_dir))\n    log_memory(None)\n    shutil.rmtree(some_dir)\n\n\nwith open('images.txt') as image_list:\n    image_urls = image_list.read().split()\n\nfor url in image_urls[:20]:\n    dfd = download_image(url, pipeline, spider)\n\nreactor.addSystemEventTrigger('before', 'shutdown', clean_up)\nreactor.run()\nSample output on attached image file: images.txt\n2016-12-13 13:13:00+0100 [-] Log opened.\n2016-12-13 13:13:00+0100 [-] TelnetConsole starting on 6023\n2016-12-13 13:13:00+0100 [-] 44516 bytes\n2016-12-13 13:13:00+0100 [-] 44752 bytes\n2016-12-13 13:13:00+0100 [-] 49492 bytes\n2016-12-13 13:13:01+0100 [-] 49680 bytes\n2016-12-13 13:13:01+0100 [-] 49680 bytes\n2016-12-13 13:13:01+0100 [-] 49680 bytes\n2016-12-13 13:13:01+0100 [-] 52312 bytes\n2016-12-13 13:13:01+0100 [-] 52312 bytes\n2016-12-13 13:13:01+0100 [-] 52316 bytes\n2016-12-13 13:13:01+0100 [-] 52316 bytes\n2016-12-13 13:13:01+0100 [-] 52316 bytes\n2016-12-13 13:13:01+0100 [-] 52532 bytes\n2016-12-13 13:13:01+0100 [-] 52532 bytes\n2016-12-13 13:13:01+0100 [-] 52700 bytes\n2016-12-13 13:13:01+0100 [-] 52700 bytes\n2016-12-13 13:13:01+0100 [-] 52700 bytes\n2016-12-13 13:13:01+0100 [-] 52700 bytes\n2016-12-13 13:13:01+0100 [-] 52700 bytes\n2016-12-13 13:13:02+0100 [-] 52700 bytes\n2016-12-13 13:13:03+0100 [-] (TCP Port 6023 Closed)\n2016-12-13 13:13:03+0100 [-] 52700 bytes\n^C2016-12-13 13:13:13+0100 [-] Received SIGINT, shutting down.\n2016-12-13 13:13:13+0100 [-] removing /tmp/tmpD1dRmc\n2016-12-13 13:13:13+0100 [-] 52700 bytes\n2016-12-13 13:13:13+0100 [-] Main loop terminated.\nNotice how memory goes up and stays up. from 44516 to 52700. Notice delay between final request and SIGINT ( 10 seconds). After this delay memory usage still stays at 52700.", "issue_status": "Open", "issue_reporting_time": "2016-12-13T12:14:14Z"}, "246": {"issue_url": "https://github.com/scrapy/scrapy/issues/2444", "issue_id": "#2444", "issue_summary": "response.json()?", "issue_description": "Contributor\npawelmhm commented on Dec 12, 2016\npython-requests have response.json() that decodes json body and returns appropriate Python objects. Does it make sense to have something like this in Scrapy?\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2016-12-12T10:22:28Z"}, "247": {"issue_url": "https://github.com/scrapy/scrapy/issues/2443", "issue_id": "#2443", "issue_summary": "robots.txt requests don't use errback", "issue_description": "mohmad-null commented on Dec 12, 2016\nI'm triggering a timeout error to test my script.\nEven though I have an errback, with basically the same code as here - https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks - the automated request that's made by scrapy to robots.txt for a host isn't using the errback function.\nAll other requests are though.\nScrapy 1.2.1", "issue_status": "Open", "issue_reporting_time": "2016-12-11T21:20:40Z"}, "248": {"issue_url": "https://github.com/scrapy/scrapy/issues/2442", "issue_id": "#2442", "issue_summary": "Timeout raises exception", "issue_description": "mohmad-null commented on Dec 12, 2016 \u2022\nedited\nThis issue has several components, all related to TimeoutErrors.\nScrapy 1.2.1\na) If a TimeoutError is raised, by default it will print the entire exception to the logger. This isn't in keeping with other exceptions (DNSLookupError, TCPTimedOutError, ConnectError) which are all transparent.\nb) The TimeoutError trace without a errback will halt the processing of the spider. This isn't in keeping with other exceptions (DNSLookupError, TCPTimedOutError, ConnectError) which are all transparent.", "issue_status": "Open", "issue_reporting_time": "2016-12-11T21:17:26Z"}, "249": {"issue_url": "https://github.com/scrapy/scrapy/issues/2436", "issue_id": "#2436", "issue_summary": "Access Response object from the Scrapy Pipeline", "issue_description": "jacob1237 commented on Dec 8, 2016 \u2022\nedited\nHello,\nI'd like to implement a pipeline which compares a checksum of the current item with some old values in cache and drops the items that are not modified.\nLooks like this is a standard use-case for change detection systems (e.g. price monitoring).\nRight now I put the cache (old checksums) into the Request.meta field, but there is no way to access it inside the pipeline.\nAs a workaround I did the same trick with the Spider Middleware (process_spider_output), but now I have another problem: when I want to skip processing of some item and signalize other components about that with Exception (like DropItem), to catch it with the spider_error signal, I can't suppress the default exception handler.\nSo every time I get some tracebacks in the log (I don't want them because I have my own spider exception handler).\nThis is because of that code in scrapy/core/scraper.py:\ndef handle_spider_error(self, _failure, request, response, spider):\n    exc = _failure.value\n    if isinstance(exc, CloseSpider):\n        self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n        return\n    referer = request.headers.get('Referer')\n    logger.error(\n        \"Spider error processing %(request)s (referer: %(referer)s)\",\n        {'request': request, 'referer': referer},\n        exc_info=failure_to_exc_info(_failure),\n        extra={'spider': spider}\n    )\n    self.signals.send_catch_log(\n        signal=signals.spider_error,\n        failure=_failure, response=response,\n        spider=spider\n    )\n    ...other code here...\nIt seems there is no way to suppress Spider Exceptions before the signal execution (even if I have a handler).\nI know that I can implement a custom crawler signal (or use the item_dropped) and execute it from my middleware (as a workaround), but won't it be easier to access the Response object from the Pipeline handlers?\nI can try to make a patch which will add the additional parameter to the pipeline handler (without breaking compatibility with an old code), but first I'd like to discuss it with the community.\nIs it conceptually wrong to access the response object from the pipeline?\nP.S. Another question outside the topic - what if we want to suppress spider exceptions that differs from the CloseSpider exception?\n\ud83d\udc4d 1\n\ud83d\udc4e 1", "issue_status": "Open", "issue_reporting_time": "2016-12-08T16:50:49Z"}, "250": {"issue_url": "https://github.com/scrapy/scrapy/issues/2435", "issue_id": "#2435", "issue_summary": "Exposing downloader stats to custom scheduler", "issue_description": "Contributor\nsibiryakov commented on Dec 8, 2016\nIn order to get maximum fetching performance, the queue have to be carefully metered. In order to do this the custom scheduler needs to know:\nthe type of the key in downloader (ip or hostname),\ncount of requests to specific hostname/ip in the queue,\ndelay/concurrency parameters of hostname/ip,\nlist of all hostname/ips in the queue.\nCurrent Scheduler API is designed for storage and resume-from-disk purposes, so I think it's time to re-think it taking into account efficiency of fetching. The most common problem with inefficient fetching is a queue filled with a single domain and polite crawling requirement.", "issue_status": "Open", "issue_reporting_time": "2016-12-08T16:29:31Z"}, "251": {"issue_url": "https://github.com/scrapy/scrapy/issues/2399", "issue_id": "#2399", "issue_summary": "Scrapy resume last batch after crash", "issue_description": "vionemc commented on Nov 18, 2016\nHi,\nI know there is a feature to resume Scrapy job, but it turns out, if I force close the spider by clicking Ctrl+C twice, it fails to resume. To be honest that makes the resume feature not that useful.\nWhat I want to ask is, can Scrapy survive a crash and resume the last batch?\nThanks,\nAmi\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2016-11-17T23:14:44Z"}, "252": {"issue_url": "https://github.com/scrapy/scrapy/issues/2392", "issue_id": "#2392", "issue_summary": "Load settings dynamically on a per-spider basis", "issue_description": "mohmad-null commented on Nov 10, 2016 \u2022\nedited\nFeature request.\nIt would be good if scrapy had an easily accessible means of reading settings on a per-spider basis, and then making them accessible to the spider. From my many attempts to do this so far, in theory all of the components for this appear to be in place. Populating settings is already done:\nhttps://doc.scrapy.org/en/latest/topics/settings.html#populating-the-settings - but then the problem is accessing them.\nIdeally in a fashion that's compatible with scrapyd (so no calling process.crawl(spider, my_settings)).\nIdeally: A project could have a generic project wide settings.py file with both the standard settings and any custom ones added by the developer. Then, using a command-line argument to indicate the settings file to use, the __init__ method of the spider overrides specific settings, (much as custom_settings does), and these settings are then accessible throughout the spider via self.settings in the usual way.\nCurrent Problems\ncustom_settings\nUnfortunately custom_settings doesn't seem to be usable for this because it cannot be declared in __init__, but needs to be declared earlier.\nsettings.py\nCurrently, even if a user is willing to just use a different settings.py file entirely for each spider (thereby duplicating most of it), that's not readily possible either.\nos.environ['SCRAPY_SETTINGS_MODULE'] = 'myproject.settings'\nthese_settings = get_project_settings()\nThe above only gets the settings into the variable these_settings, they're not used by the spider or accessible via self.settings.\nDesire for feature\nBased on StackOverFlow, this is something a lot of people want. The fact there are so many answers that are all so different shows there isn't a particular good way of doing it.\nhttp://stackoverflow.com/questions/9814827/creating-a-generic-scrapy-spider\nhttp://stackoverflow.com/questions/12996910/how-to-setup-and-launch-a-scrapy-spider-programmatically-urls-and-settings\nhttp://stackoverflow.com/questions/35662146/dynamic-spider-generation-with-scrapy-subclass-init-error\nhttp://stackoverflow.com/questions/40510526/how-to-load-different-settings-for-different-scrapy-spiders\nhttp://stackoverflow.com/questions/2396529/using-one-scrapy-spider-for-several-websites\nBeing able to readily get allowed_domains and start_urls within it would also be good.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2016-11-10T10:44:16Z"}, "253": {"issue_url": "https://github.com/scrapy/scrapy/issues/2384", "issue_id": "#2384", "issue_summary": "As soon as I start using import logging my default screen output logging when running spiders is gone.", "issue_description": "yssoe commented on Nov 8, 2016\nHi,\nI start to write a lot of relevant logging to my spiders using the 'import logging' and the guidelines described in the documentation.\nWhat I notice is that when I run a spider by hand 'scrapy crawl spidername' I don't see any of my previous default output anymore. That's a big problem for me. While writing and debugging spiders it's very nice to see what's going on with scrapy. The only way for me to have it back is by removing my logging code in the spiders.\nIs there a setting that I'm missing that I can use my specific code log and still see the verbose output while running a spider?\nCheers.", "issue_status": "Open", "issue_reporting_time": "2016-11-08T14:00:28Z"}, "254": {"issue_url": "https://github.com/scrapy/scrapy/issues/2378", "issue_id": "#2378", "issue_summary": "get_project_settings of doesn't reflect per-spider settings", "issue_description": "mohmad-null commented on Nov 6, 2016\nIt's possible I'm using this wrong. But I'm doing the following:\nclass MySpider (Spider)\n  def __init__(self):\n  custom_settings = {\n   'AUTOTHROTTLE_ENABLED': False\n  }\n\n  a = scrapy.utils.project.get_project_settings()\n  print(a.copy_to_dict())\nMy settings.py has AUTOTHROTTLE_ENABLED: True.\nBased on my reading of https://doc.scrapy.org/en/latest/topics/settings.html#populating-the-settings - the custom_settings value should override the value of settings.py. But the result of the above is:\n....\nAUTOTHROTTLE_ENABLED: True\n...\nIt's using the value from settings.py - am I misunderstanding this or is it a bug? I was hoping to load and alter settings dynamically during the init.", "issue_status": "Open", "issue_reporting_time": "2016-11-05T20:51:59Z"}, "255": {"issue_url": "https://github.com/scrapy/scrapy/issues/2376", "issue_id": "#2376", "issue_summary": "disallow_domains", "issue_description": "mohmad-null commented on Nov 2, 2016\nFeature request\nCurrently there's \"allowed_domains\" to create a whitelist of domains to scrape.\nIt would be good if there was a \"disallowed_domains\" or \"blocked_domains\" as well. I appreciate I could could probably do this in middleware, but I figure it's something quite a few people would want.", "issue_status": "Open", "issue_reporting_time": "2016-11-02T14:36:34Z"}, "256": {"issue_url": "https://github.com/scrapy/scrapy/issues/2370", "issue_id": "#2370", "issue_summary": "Permissions issue when installing attrs during pip install scrapy", "issue_description": "chrisfromthelc commented on Oct 29, 2016 \u2022\nedited by redapple\nOS: Rasbian Jessie 4.4\nWhen using a virtualenv (Python 2.7), there's an error that seems to be permissions-related during the pip install scrapy installation.\nCollecting attrs\n  Using cached attrs-16.2.0-py2.py3-none-any.whl\nInstalling collected packages: attrs\nException:\nTraceback (most recent call last):\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/commands/install.py\", line 317, in run\n    prefix=options.prefix_path,\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 742, in install\n    **kwargs\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 831, in install\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 1032, in move_wheel_files\n    isolated=self.isolated,\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/wheel.py\", line 346, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/wheel.py\", line 317, in clobber\n    ensure_dir(destdir)\n  File \"/home/pi/TEST/local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 83, in ensure_dir\n    os.makedirs(path)\n  File \"/home/pi/TEST/lib/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/home/pi/TEST/lib/python2.7/site-packages/attrs-16.2.0.dist-info'\nUsing chmod on /lib/python2.7/site-packages/ to open up permissions allows installation of attrs.", "issue_status": "Open", "issue_reporting_time": "2016-10-29T05:40:49Z"}, "257": {"issue_url": "https://github.com/scrapy/scrapy/issues/2365", "issue_id": "#2365", "issue_summary": "provide a way to work with scrapy http cache without making requests", "issue_description": "Member\nkmike commented on Oct 27, 2016\nCurrently it is hard to extract information from scrapy cache: cache storages want 'spider' and 'request' objects, one can't just list all files in cache and get Response instances from them. I think it can be a good to refactor FilesystemCacheStorage:\nallow reading individual cache entries by their path;\nallow to disable expiration logic.\n\ud83d\udc4d 3\n\u2764\ufe0f 1", "issue_status": "Open", "issue_reporting_time": "2016-10-27T13:16:47Z"}, "258": {"issue_url": "https://github.com/scrapy/scrapy/issues/2358", "issue_id": "#2358", "issue_summary": "Items Pipeline output is documented to be dict or Item subclass, but can be anything", "issue_description": "Contributor\nchekunkov commented on Oct 24, 2016\nFrom process_item documentation:\nThis method is called for every item pipeline component. process_item() must either: return a dict with data, return an Item (or any descendant class) object, return a Twisted Deferred or raise DropItem exception. Dropped items are no longer processed by further pipeline components.\nThere was a question in the scrapy-users group and it appeared that user wrote his pipeline in such a way that it swallowed the item - so his pipeline component returned None - which broke the following pipeline component in a way that wasn't intuitive enough for the user. Shouldn't Scrapy check pipeline output and stop item processing instead of passing None to the next pipeline component?", "issue_status": "Open", "issue_reporting_time": "2016-10-24T15:04:54Z"}, "259": {"issue_url": "https://github.com/scrapy/scrapy/issues/2350", "issue_id": "#2350", "issue_summary": "Pipelines documentation limited", "issue_description": "mohmad-null commented on Oct 21, 2016\nI've spent quite a while going through the documentation, and while I like the concept off pipelines, no-where can I find documentation which shows how to fully implement them end-to-end.\nThe Pipelines page (https://doc.scrapy.org/en/latest/topics/item-pipeline.html) only shows code for the pipeline itself, not how to use it / plug it in to the main project.\nThe example project quotesbot is no better. While it does contain a pipelines.py, the class within it is never used. Indeed, filling this file with junk that should raise numerous syntax Exceptions doesn't do anything either. As examples go, it is extremely limited.\nitems.py is similarly ignored.\nI would suggest the following:\nInclude documenation for how to glue all of the components of a complete spider together\nDon't include files that are unused in the example quotesbot\nCreate another example project that does use items/pipelines etc.", "issue_status": "Open", "issue_reporting_time": "2016-10-21T13:18:11Z"}, "260": {"issue_url": "https://github.com/scrapy/scrapy/issues/2349", "issue_id": "#2349", "issue_summary": "Invalid commands in example", "issue_description": "mohmad-null commented on Oct 21, 2016\nThe quotesbot:\nhttps://github.com/scrapy/quotesbot\nSays to run:\nscrapy crawl\nand\nscrapy list\nHowever, neither of these are a valid command for me. I only have:\nAvailable commands:\nbench Run quick benchmark test\ncommands\nfetch Fetch a URL using the Scrapy downloader\ngenspider Generate new spider using pre-defined templates\nrunspider Run a self-contained spider (without creating a project)\nsettings Get settings values\nshell Interactive scraping console\nstartproject Create new project\nversion Print Scrapy version\nview Open URL in browser, as seen by Scrapy\n[ more ] More commands available when run from project directory", "issue_status": "Open", "issue_reporting_time": "2016-10-21T13:09:16Z"}, "261": {"issue_url": "https://github.com/scrapy/scrapy/issues/2309", "issue_id": "#2309", "issue_summary": "Allow specifying infinite expire time for files", "issue_description": "dolohow commented on Oct 6, 2016 \u2022\nedited\nDocumentation does not mentioned how to set up the infinite expire time of IMAGES_EXPIRES or FILES_EXPIRES.\nI look into the source code of the function media_to_download and I found this:\n        self.expires = settings.getint(\n            resolve('FILES_EXPIRES'), self.EXPIRES\n        )\nand that\n            if age_days > self.expires:\n                return  # returning None force download\nI concluded that there is no way to specify infinite time. If I am wrong, please correct me. I can also create PR for that.\nThank you.", "issue_status": "Open", "issue_reporting_time": "2016-10-06T09:42:55Z"}, "262": {"issue_url": "https://github.com/scrapy/scrapy/issues/2305", "issue_id": "#2305", "issue_summary": "input_processor called only once", "issue_description": "dolohow commented on Oct 5, 2016 \u2022\nedited\nHi guys,\nI am using input_processor on custom ItemLoader along one inside Field declaration that belongs to Item which is being used by ItemLoader. The problem is that only ItemLoader fires up the input_processor. Is it a desired behavior?\nRelated code:\nclass ProductsItem(scrapy.Item):\n    # input_loader is not called\n    currency = scrapy.Field(input_processor=MapCompose(utils.get_unified_currency_name))\n\nclass CustomProductLoader(ItemLoader):\n    default_output_processor = TakeFirst()\n    # input_loader works fine here\n    currency_in = MapCompose(lambda x: x[0])\n\nclass MySpider(scrapy.Spider):\n    # ....\n    def parse(self, response):\n        for product in response.css('bla bla'):\n            loader = CustomProductLoader(ProductsItem(), product, response=response)\n            loader.add_css('currency', 'bla bla')", "issue_status": "Open", "issue_reporting_time": "2016-10-05T09:00:30Z"}, "263": {"issue_url": "https://github.com/scrapy/scrapy/issues/2303", "issue_id": "#2303", "issue_summary": "RedirectMiddleware does not respect spider's crawling rules", "issue_description": "Contributor\ndjunzu commented on Oct 4, 2016 \u2022\nedited\nAs described in #15 (and #1042), some links to offsite domains may be crawled via redirects. For example:\n# in spider:\nallowed_domains = ['xxx.com']\n# in log, offsite domain crawled:\nFiltered offsite request to 'www.yyy.com': <GET http://www.yyy.com/foo>\nRedirecting (302) to <GET http://www.yyy.com/> from <GET http://www.xxx.com/bar/>\nCrawled (200) <GET http://www.yyy.com/> (referer: http://www.xxx.com/bar/>)\nBut the problem also affects rules from CrawlSpider:\ndeny is not respected.\n(worse) a response may be parsed by the wrong function.\nFor example:\n# in spider:\nrules = (Rule(LinkExtractor(deny='forbidden', allow='drinks'), callback='parse_drinks', follow=True),\n         Rule(LinkExtractor(deny='forbidden', allow='food'), callback='parse_food', follow=True)\n        )\n# in log:\n\n# deny regexp crawled:\nRedirecting (302) to <GET http://www.xxx.com/forbidden> from <GET http://www.xxx.com/bar/>\nCrawled (200) <GET http://www.xxx.com/forbidden> (referer: http://www.xxx.com/bar/>)\n\n# http://www.xxx.com/drink/bar should be parsed by parse_drinks but it will be parsed by parse_food:\nRedirecting (302) to <GET http://www.xxx.com/drink/bar> from <GET http://www.xxx.com/food/bar>\nCrawled (200) <GET http://www.xxx.com/drink/bar> (referer: http://www.xxx.com/food/bar>)\nA somewhat related discussion is going on #2241.\nPS:\nThere are other similar issues. #1744 is almost identical. But they are all closed without a clearly solution.\nAs I see a developer will never expect the current behavior. It seems a bug to me. If it is not a bug, or if it is a \"wont fix\", or if there is any motivation to don't change the behavior, at least a note should be placed on docs so developers are not taken by surprise!\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2016-10-03T21:13:10Z"}, "264": {"issue_url": "https://github.com/scrapy/scrapy/issues/2297", "issue_id": "#2297", "issue_summary": "Order of HttpCacheMiddleware vs. RetryMiddleware", "issue_description": "ilovenwd commented on Sep 30, 2016\nRetryMiddleware should be AFTER HttpCacheMiddleware (thus retry.process_response called before cache)\nbecause a response that should be retried should NOT be cached.\nDOWNLOADER_MIDDLEWARES_BASE = {\n    # Engine side\n    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 400,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 500,\n    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,\n    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\n    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\n    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\n    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\n    # Downloader side\n}", "issue_status": "Open", "issue_reporting_time": "2016-09-30T05:35:01Z"}, "265": {"issue_url": "https://github.com/scrapy/scrapy/issues/2295", "issue_id": "#2295", "issue_summary": "Add item_failed signal", "issue_description": "ilovenwd commented on Sep 29, 2016 \u2022\nedited\nSometimes we need to do something when item pipeline failed for some reason(code bug for example), like recording the corresponding request for later retry.\nCurrently scrapy just print a log for a failed item:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/core/scraper.py#L232\nrelated issue: #2147", "issue_status": "Open", "issue_reporting_time": "2016-09-29T16:30:15Z"}, "266": {"issue_url": "https://github.com/scrapy/scrapy/issues/2263", "issue_id": "#2263", "issue_summary": "Add FAQ entry on how to deal with a very long \"allowed_domains\"", "issue_description": "Contributor\nredapple commented on Sep 19, 2016\nSee #1908 (comment)", "issue_status": "Open", "issue_reporting_time": "2016-09-19T10:40:44Z"}, "267": {"issue_url": "https://github.com/scrapy/scrapy/issues/2241", "issue_id": "#2241", "issue_summary": "make offsite middleware downloader middleware instead of spider middleware", "issue_description": "Contributor\npawelmhm commented on Sep 15, 2016 \u2022\nedited\nCurrently offsite middleware is spider middleware. It works only on spider output, only processes requests generated in spider callback. But requests in Scrapy can be scheduled outside spider callback. This can happen in following cases:\nrequests scheduled on start_requests\nrequests scheduled with crawler.engine.crawl(), crawler.engine.download(), crawler.engine.schedule\nSecond case (downloading with crawler.engine.crawl()) is very common in my experience. Many spiders get lists of urls from some external source and schedule requests on spider_idle signal. Offsite middleware wont work properly for them, allowed_domains attribute will not have effect.\nI think we should make offsite middleware downloader middleware and make it work for all requests or at least discuss and clarify why we want to keep it as spider middleware and document that it does not work on all possible requests.", "issue_status": "Open", "issue_reporting_time": "2016-09-15T06:20:40Z"}, "268": {"issue_url": "https://github.com/scrapy/scrapy/issues/2231", "issue_id": "#2231", "issue_summary": "Make logging configuration (more) customizable", "issue_description": "Contributor\nredapple commented on Sep 13, 2016 \u2022\nedited\nWould it make sense to have DEFAULT_LOGGING be read from settings before going through dictConfig?\nAlso, reading some of the issues with logging (e.g. #1977), it could even be useful to make configure_logging fully customizable too (for the brave), or at least having a way to add handlers.\n\ud83d\udc4d 6", "issue_status": "Open", "issue_reporting_time": "2016-09-13T12:16:18Z"}, "269": {"issue_url": "https://github.com/scrapy/scrapy/issues/2228", "issue_id": "#2228", "issue_summary": "Duplicate log", "issue_description": "gabsn commented on Sep 11, 2016\nDubplicate Log\nThere is an annoying bug when using python multiprocessing lib whith scrapy :\nclass CompaniesCrawler(object):\n\n    def __init__(self):\n        self.crawler = CrawlerProcess(SETTINGS)\n\n    def _crawl(self, spider):\n        self.crawler.crawl(spider)\n        self.crawler.start()\n        self.crawler.stop()\n\n    def crawl(self, spider):\n        p = Process(\n            target=self._crawl,\n            args=[spider]\n        )\n        p.start()\n        p.join()\nIssue : Each time we instantiate CrawlerProcess class, configure_logging function is called and a new handler is created :\nhandler = _get_handler(settings)\nlogging.root.addHandler(handler)\nSo we eventually find ourselves with a lot of duplicate log handlers and the console becomes unreadable...", "issue_status": "Open", "issue_reporting_time": "2016-09-11T17:09:29Z"}, "270": {"issue_url": "https://github.com/scrapy/scrapy/issues/2222", "issue_id": "#2222", "issue_summary": "cache storages or HttpCacheMiddleware should handle retrieve_response errors", "issue_description": "Member\nkmike commented on Sep 7, 2016 \u2022\nedited\nI think it makes sense to handle broken cache records as cache misses, with a proper log message. I had this problem with FileSystemStorage when gzip is enabled, and spider is killed. In this case it is possible to get a broken gzip file. It means spider can miss a request on a re-crawl.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2016-09-07T17:12:46Z"}, "271": {"issue_url": "https://github.com/scrapy/scrapy/issues/2221", "issue_id": "#2221", "issue_summary": "HTTP cache should expose storage time", "issue_description": "Member\nkmike commented on Sep 7, 2016\nFilesystemCacheStorage, DbmCacheStorage and LeveldbCacheStorage store a timestamp to meta, but unlike all other parameters this information is not available to a spider when response is retrieved from cache.\nI think Scrapy should expose this timestamp as a meta key for cached responses - it is useful not only for expiration. For example, it is required to parse relative dates properly - what \"1 hour ago\" means depends on a time page was downloaded.\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2016-09-07T17:06:22Z"}, "272": {"issue_url": "https://github.com/scrapy/scrapy/issues/2194", "issue_id": "#2194", "issue_summary": "Move scrapy.utils.deprecate to separate package", "issue_description": "Member\nDigenis commented on Aug 23, 2016 \u2022\nedited\nAmong the pypi packages with a purpose similar to\nscrapy.utils.deprecate.create_deprecated_class,\nscrapy's implementation stands out\nbeing the only one that warns when subclassing.\nI think it deserves its own package\nwhere further development can continue\nwithout inflating scrapy's codebase.\n@dangra, you have a name in pypi reserved.\nDo you have a plan to create a package?", "issue_status": "Open", "issue_reporting_time": "2016-08-23T17:47:19Z"}, "273": {"issue_url": "https://github.com/scrapy/scrapy/issues/2186", "issue_id": "#2186", "issue_summary": "move more functions from scrapy.utils.url to w3lib", "issue_description": "Member\nkmike commented on Aug 19, 2016\nHey,\nWhat do you think about moving add_http_if_no_scheme and escape_ajax functions to w3lib?", "issue_status": "Open", "issue_reporting_time": "2016-08-19T13:23:06Z"}, "274": {"issue_url": "https://github.com/scrapy/scrapy/issues/2183", "issue_id": "#2183", "issue_summary": "S3FilesStore - Wait for upload", "issue_description": "BjBlazkowicz commented on Aug 18, 2016\nHello,\nIs it somehow possible to wait for an s3 upload to finish before continuing in the pipeline-chain?\nMaybe I am far off in my design, but the issue is that my post-processor(another application) does some further processing on the files that Scrapy uploads to S3. But sometimes the uploads haven't finished before the item is processed which results in 404s in my s3 client.\nI tried to subclass the S3FilesStore in order to make the upload a blocking call by skipping the defereds, but this resulted in memory problems in my spiders...\nThe main problem is that item_completed in the FilesPipeline is called before the uploads has finished. Is there some good way for extending the FilesPipeline in order to achieve what I want? I dont really want to implement polling or something.\nCheers", "issue_status": "Open", "issue_reporting_time": "2016-08-18T10:57:04Z"}, "275": {"issue_url": "https://github.com/scrapy/scrapy/issues/2182", "issue_id": "#2182", "issue_summary": "Generalize Request/Response", "issue_description": "candlejack297 commented on Aug 16, 2016\nIt would be nice if scrapy had a slightly more general abstraction over request/responses.\nMy use case involves a Rest API that has a python sdk. Instead of using scrapy to make the direct http requests/responses, I'd like to make the request using the sdk, but then follow the normal scrapy process (middleware, parse, item pipeline, etc). I can and have made this happen with a few custom classes, but it still feels like there should be an easier way to hook things into the pipeline (maybe there is, and I'm just missing it). I realize it's not exactly crawling to do something like this, but we have some \"normal\" spiders as well, and it's nice to have a single entry point for all our scraping.", "issue_status": "Open", "issue_reporting_time": "2016-08-15T21:23:00Z"}, "276": {"issue_url": "https://github.com/scrapy/scrapy/issues/2174", "issue_id": "#2174", "issue_summary": "gzip-compressed item exports", "issue_description": "Member\nkmike commented on Aug 11, 2016\nI think compressing exported data can be useful in a lot of cases. It'd be good to have a built-in way for compressed exports in Scrapy. I have this implementation, but probably it makes more sense to handle .jl.gz / csv.gz / ... extensions just like .jl / .csv / ... instead of creating a storage, I'm not sure:\n# -*- coding: utf-8 -*-\nimport os\nimport gzip\n\nfrom zope.interface import Interface, implementer\nfrom w3lib.url import file_uri_to_path\nfrom scrapy.extensions.feedexport import IFeedStorage\n\n\n@implementer(IFeedStorage)\nclass GzipFileFeedStorage(object):\n    \"\"\"\n    Storage which exports data to a gzipped file.\n    To use it, add\n\n    ::\n\n        FEED_STORAGES = {\n            'gzip': 'myproject.exports.GzipFileFeedStorage',\n        }\n\n    to settings.py and then run scrapy crawl like this::\n\n        scrapy crawl foo -o gzip:/path/to/items.jl\n\n    The command above will create ``/path/to/items.jl.gz`` file\n    (.gz extension is added automatically).\n\n    Other export formats are also supported, but it is recommended to use .jl.\n    If a spider is killed then gz archive may be partially broken.\n    In this case it user should read the broken archive line-by-line and stop\n    on gzip decoding errors, discarding the tail. It works OK with .jl exports.\n    \"\"\"\n    COMPRESS_LEVEL = 4\n\n    def __init__(self, uri):\n        self.path = file_uri_to_path(uri) + \".gz\"\n\n    def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname, exist_ok=True)\n        return gzip.open(self.path, 'ab', compresslevel=self.COMPRESS_LEVEL)\n\n    def store(self, file):\n        file.close()\nWhat do you think?", "issue_status": "Open", "issue_reporting_time": "2016-08-11T17:30:35Z"}, "277": {"issue_url": "https://github.com/scrapy/scrapy/issues/2173", "issue_id": "#2173", "issue_summary": "Dump stats to log periodically, not only at the end of the crawl", "issue_description": "Member\nkmike commented on Aug 11, 2016 \u2022\nedited\nIt is useful to check Scrapy stats as spider runs, but there is no a built-in way to do that. What do you think about adding DUMP_STATS_INTERVAL option and outputting current stats to logs each DUMP_STATS_INTERVAL seconds?\nAnother related proposal (sorry for putting them all into this ticket) is to add more logging to Downloader and log periodically a number of pages Downloader is currently trying to fetch (MONITOR_DOWNLOADS_INTERVAL option). Checking that helps to understand what is crawler doing - is it busy downloading data or not.\nImplementation draft:\nimport logging\nimport pprint\n\nfrom twisted.internet.task import LoopingCall\nfrom scrapy import signals\n\nlogger = logging.getLogger(__name__)\n\n\nclass _LoopingExtension:\n    def setup_looping_task(self, task, crawler, interval):\n        self._interval = interval\n        self._task = LoopingCall(task)\n        crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n\n    def spider_opened(self):\n        self._task.start(self._interval, now=False)\n\n    def spider_closed(self):\n        if self._task.running:\n            self._task.stop()\n\n\nclass MonitorDownloadsExtension(_LoopingExtension):\n    \"\"\"\n    Enable this extension to periodically log a number of active downloads.\n    \"\"\"\n    def __init__(self, crawler, interval):\n        self.crawler = crawler\n        self.setup_looping_task(self.monitor, crawler, interval)\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # fixme: 0 should mean NotConfigured\n        interval = crawler.settings.getfloat(\"MONITOR_DOWNLOADS_INTERVAL\", 10.0)\n        return cls(crawler, interval)\n\n    def monitor(self):\n        active_downloads = len(self.crawler.engine.downloader.active)\n        logger.info(\"Active downloads: {}\".format(active_downloads))\n\n\nclass DumpStatsExtension(_LoopingExtension):\n    \"\"\"\n    Enable this extension to log Scrapy stats periodically, not only\n    at the end of the crawl.\n    \"\"\"\n    def __init__(self, crawler, interval):\n        self.stats = crawler.stats\n        self.setup_looping_task(self.print_stats, crawler, interval)\n\n    def print_stats(self):\n        stats = self.stats.get_stats()\n        logger.info(\"Scrapy stats:\\n\" + pprint.pformat(stats))\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        interval = crawler.settings.getfloat(\"DUMP_STATS_INTERVAL\", 60.0)\n        # fixme: 0 should mean NotConfigured\n        return cls(crawler, interval)\nTo get a feel on how it works copy-paste the code above to a project (e.g. to myproject/extensions.py) file, then add them to EXTENSIONS in settings.py:\nEXTENSIONS = {\n    'myproject.extensions.MonitorDownloadsExtension': 100,\n    'myproject.extensions.DumpStatsExtension': 101,\n}\n\ud83d\udc4d 10", "issue_status": "Open", "issue_reporting_time": "2016-08-11T17:24:59Z"}, "278": {"issue_url": "https://github.com/scrapy/scrapy/issues/2164", "issue_id": "#2164", "issue_summary": "openssl el capitan", "issue_description": "spenoir commented on Aug 3, 2016\nLatest el capitan update breaks openssl which means Scrapy fails to install like this:\nbuild/temp.macosx-10.11-intel-2.7/_openssl.c:429:10: fatal error: 'openssl/opensslv.h' file not found\n\n#include <openssl/opensslv.h>\n\n         ^\n\n1 error generated.\n\nerror: command 'cc' failed with exit status 1\nNot a Scrapy issue perhaps but worth mentioning. Also problems with homebrew: https://stackoverflow.com/questions/38670295/brew-refusing-to-link-openssl which is perhaps related. Why did they have to mess with openssl?\n\ud83d\udc4d 4", "issue_status": "Open", "issue_reporting_time": "2016-08-03T11:40:14Z"}, "279": {"issue_url": "https://github.com/scrapy/scrapy/issues/2141", "issue_id": "#2141", "issue_summary": "Redirected request uses same download slot when domain changes", "issue_description": "Contributor\nArturGaspar commented on Jul 22, 2016\nRedirected requests keep the request.meta['download_slot'] value set by the downloader in the original request, making subsequent requests use the same download slot even if the domain changes between redirects.\nThis causes redirected requests to be treated as if they were all belonging to the original domain for per-domain concurrency.", "issue_status": "Open", "issue_reporting_time": "2016-07-22T18:19:19Z"}, "280": {"issue_url": "https://github.com/scrapy/scrapy/issues/2127", "issue_id": "#2127", "issue_summary": "Improve traceback when \"proxy\" cannot be parsed correctly (e.g. missing scheme)", "issue_description": "Contributor\nredapple commented on Jul 15, 2016 \u2022\nedited\nSee http://stackoverflow.com/q/38378710 for motivation.\nWhen using a proxy value without scheme, e.g. 'localhost:8080', scrapy breaks with an obscure exception on to_bytes(). Even if it's a wrong parameter value, I believe we can improve the message so that users can fix it swiftly.\nIn [1]: import scrapy\n\nIn [2]: r = scrapy.Request('http://www.example.com', meta={'proxy': 'localhost:8080'})\n\nIn [3]: fetch(r)\n2016-07-15 17:17:21 [scrapy] INFO: Spider opened\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-3-0f7ba0a442fd> in <module>()\n----> 1 fetch(r)\n\n/home/paul/.virtualenvs/scrapy11/lib/python3.5/site-packages/scrapy/shell.py in fetch(self, request_or_url, spider)\n    110         try:\n    111             response, spider = threads.blockingCallFromThread(\n--> 112                 reactor, self._schedule, request, spider)\n    113         except IgnoreRequest:\n    114             pass\n\n/home/paul/.virtualenvs/scrapy11/lib/python3.5/site-packages/twisted/internet/threads.py in blockingCallFromThread(reactor, f, *a, **kw)\n    120     result = queue.get()\n    121     if isinstance(result, failure.Failure):\n--> 122         result.raiseException()\n    123     return result\n    124 \n\n/home/paul/.virtualenvs/scrapy11/lib/python3.5/site-packages/twisted/python/failure.py in raiseException(self)\n    366     if _PY3:\n    367         def raiseException(self):\n--> 368             raise self.value.with_traceback(self.tb)\n    369     else:\n    370         exec(\"\"\"def raiseException(self):\n\nTypeError: to_bytes must receive a unicode, str or bytes object, got NoneType\n\nIn [4]: ", "issue_status": "Open", "issue_reporting_time": "2016-07-15T15:19:08Z"}, "281": {"issue_url": "https://github.com/scrapy/scrapy/issues/2124", "issue_id": "#2124", "issue_summary": "Cookies not set when dont_merge_cookies is True", "issue_description": "LEChaney commented on Jul 14, 2016 \u2022\nedited by redapple\nThis example is straight from the documentation, and does not work because if dont_merge_cookies is set on the request, then the cookie middleware skips all cookie processing and no cookies are ever set.\nRequest(url=\"http://www.example.com\",\n        cookies={'currency': 'USD', 'country': 'UY'},\n        meta={'dont_merge_cookies': True})\nDoes not work, however, this does:\nRequest(url=\"http://www.example.com\",\n        cookies={'currency': 'USD', 'country': 'UY'})", "issue_status": "Open", "issue_reporting_time": "2016-07-13T20:18:14Z"}, "282": {"issue_url": "https://github.com/scrapy/scrapy/issues/2089", "issue_id": "#2089", "issue_summary": "`FeedExporter` extension should throw when misconfigured", "issue_description": "rampage644 commented on Jul 5, 2016\nFeedExporter extension should either throw NotConfigured when spider lacks some properties it should have according to FEED_URI. Right now in that case open_spider initialize fails causing all subsequent signal handlers to fail as well.", "issue_status": "Open", "issue_reporting_time": "2016-07-05T14:25:00Z"}, "283": {"issue_url": "https://github.com/scrapy/scrapy/issues/2087", "issue_id": "#2087", "issue_summary": "Date/Time handling in scrapy", "issue_description": "Contributor\nnyov commented on Jul 4, 2016\nScrapy uses UTC \"timestamps\" (datetime objects) internally. This is good.\n1st\nI would like to propose using RFC3339 (subset of ISO 8601, or ISO 8601 (where time intervals might be represented) time formats just as universally, where timestamps are represented as string-serialized form.\nThis would allow for easier conversion back to usable date objects (e.g. with iso8601.parse_date(), dateutil.parser.parse() or others).\nNotably in ScrapyJSONEncoder, a simple (but backwards-incompatible) change would be:\ndiff --git a/scrapy/utils/serialize.py b/scrapy/utils/serialize.py\nindex 8320be0..d11d167 100644\n--- a/scrapy/utils/serialize.py\n+++ b/scrapy/utils/serialize.py\n@@ -11,11 +11,11 @@ from scrapy.item import BaseItem\n class ScrapyJSONEncoder(json.JSONEncoder):\n\n     DATE_FORMAT = \"%Y-%m-%d\"\n-    TIME_FORMAT = \"%H:%M:%S\"\n+    TIME_FORMAT = \"%H:%M:%SZ\"\n\n     def default(self, o):\n         if isinstance(o, datetime.datetime):\n-            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n+            return o.strftime(\"%sT%s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n         elif isinstance(o, datetime.date):\n             return o.strftime(self.DATE_FORMAT)\n         elif isinstance(o, datetime.time):\nThis would invalidate the use of the DATE_FORMAT and TIME_FORMAT variables, but with these formats being international standard the need for a customizable format is questionable, so a better change should drop these entirely and use datetime.datetime.isoformat().\n2nd\nI'd like to discuss pro/contra of using \"offset-aware\"/timezone-aware datetime objects internally.\nMy belief is that UNIX timestamps/UTC datetimes as internal representations are enough, and localized date-conversions should be kept to frontend code. No change necessary.\nHowever, datetime objects seem to have the same memory footprint with or without timezone info (if that data is correct):\n>>> import sys, time, datetime, pytz\n>>> utime = time.time()\n>>> dtime = datetime.datetime.utcfromtimestamp(utime)\n>>> ztime = datetime.datetime.fromtimestamp(utime, pytz.UTC)\n>>> sys.getsizeof(utime)\n24\n>>> sys.getsizeof(dtime)\n48\n>>> sys.getsizeof(ztime)\n48\n>>> utime\n1467642195.921702\n>>> print (\"%.20f\" % utime)\n1467642195.92170190811157226562\n>>> dtime.isoformat()\n'2016-07-04T14:23:15.921702'\n>>> ztime.isoformat()\n'2016-07-04T14:23:15.921702+00:00\n>>> dtime == ztime\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can't compare offset-naive and offset-aware datetimes\nIn the name of interoperability, adding UTC \"timezone\" info might be positive.\n(See PR #1056)\nAdding a pytz dependency, when there is no real need for timezone support, feels ugly.\nSince Python 3.2, UTC tzinfo was added to datetime, and here is a possible backport for older Python versions.\ndiff --git a/scrapy/utils/python.py b/scrapy/utils/python.py\nindex 42fbbda..8731d1b 100644\n--- a/scrapy/utils/python.py\n+++ b/scrapy/utils/python.py\n@@ -344,3 +344,34 @@ def without_none_values(iterable):\n         return {k: v for k, v in six.iteritems(iterable) if v is not None}\n     except AttributeError:\n         return type(iterable)((v for v in iterable if v is not None))\n+\n+# Backported `datetime.timezone.utc` (Python 3.2+)\n+# for \"timezone-aware\" datetimes (convoluted way to get unix timestamps)\n+# This implements a simple UTC \"timezone\" for datetime objects,\n+# without requiring the whole `pytz` shebang.\n+try:\n+    from datetime import timezone\n+    utc = timezone.utc\n+except ImportError:\n+    from datetime import timedelta, tzinfo\n+\n+    class timezone(tzinfo): pass\n+    class UTC(timezone):\n+        _zero = timedelta(0)\n+        def utcoffset(self, dt):\n+            return self._zero\n+        def tzname(self, dt):\n+            return \"UTC\"\n+        def dst(self, dt):\n+            return self._zero\n+        def __repr__(self):\n+            return 'datetime.timezone.utc'\n+        def __str__(self):\n+            return 'UTC+00:00'\n+    utc = UTC()\n+\n+    # optional monkey-patching datetime\n+    import datetime\n+    datetime.timezone = timezone\n+    datetime.timezone.utc = utc\n+    del datetime\nCurrent datetime.datetime.utcnow() would change to datetime.datetime.now(datetime.timezone.utc) (Py3.2+), or something like this:\nfrom datetime import datetime\ntry:\n    from datetime import timezone\nexcept ImportError:\n    from scrapy.utils.python import timezone\n\nprint (datetime.now(timezone.utc))\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2016-07-04T14:59:07Z"}, "284": {"issue_url": "https://github.com/scrapy/scrapy/issues/2083", "issue_id": "#2083", "issue_summary": "S3FilesStore should not disable SSL", "issue_description": "wearpants commented on Jul 1, 2016 \u2022\nedited by redapple\nPer comment in source, the S3 storage backend for media pipelines silently strips SSL from the connection to S3, ostensibly because of this python core bug, which oddly enough was fixed 3 years before the code in scrapy was written.\nThis issue has been resolved in all modern versions of Python (2.7+/3.0+) for quite some time. Can this workaround be removed?", "issue_status": "Open", "issue_reporting_time": "2016-06-30T18:39:40Z"}, "285": {"issue_url": "https://github.com/scrapy/scrapy/issues/2074", "issue_id": "#2074", "issue_summary": "Add tar.gz to ignored extensions in linkextractor", "issue_description": "alicenara commented on Jun 23, 2016\nI don't know if this goes here e_e but I've had problems when trying to parse a tar.gz as an html (now I check the extension) and I want to propose to include this type of file as an ignored one in scrapy/scrapy/linkextractors/init.py\n(btw I have modified this file and I've added \"tar.gz\" and \"gz\" because I had the feeling \"tar.gz\" didn't work)\nThanks :D", "issue_status": "Open", "issue_reporting_time": "2016-06-23T11:54:38Z"}, "286": {"issue_url": "https://github.com/scrapy/scrapy/issues/2071", "issue_id": "#2071", "issue_summary": "In Scrapy 1.1.0 --output as xml appending to existing file when spider re-executed resulting in invalid xml file with duplicate declaration", "issue_description": "mobcdi commented on Jun 22, 2016\nIssue\nScrapy is smart enough not to create duplicate file store copies when spider is re-run but appends output to xml file if --output myxmlfile.xml specified in command creating invalid xml file with 2nd declaration statement for each spider run after the xml file is created\nPossible Solution:\nIf xml file exists overwrite the contents not append to existing file or add option to allow user specify overwrite or append. (While the logfile is appended when a spider is re-run its structure isn't important to valid interpretation by 3rd party tools as is the case with xml files.\nTo reproduce:\nRun a spider via the command prompt and include option --output myxmlfile.xml and log the output to a file --logfile mylogfile when spider completes validate the xml using http://codebeautify.org/xmlvalidate and notice the xml is valid.\nIf you have set-up your spider to use the files pipeline and store a copy of every page indexed you can count the number of files created in your FILES_STORE and note for later.\nRe-run the spider command and notice the xml file increases in size, re-validate the xml and you should receive a similar validation message.\nError on-> Line :2 Column :155535 (or how ever many characters your line is before it encounters the 2nd declaration statement)\nMessage :XML declaration allowed only at the start of the document\nError on-> Line :3 Column :1\nMessage :Extra content at the end of the document\nCheck the FILES_STORE and note that you don't have any increase in number of files stored on disk. Check the log file for references File (uptodate): and note the crawler is smart enough to know the page was already created on disk so didn't create a duplicate but when exporting the items to xml it appends the duplicates to the existing file.", "issue_status": "Open", "issue_reporting_time": "2016-06-22T11:09:05Z"}, "287": {"issue_url": "https://github.com/scrapy/scrapy/issues/2066", "issue_id": "#2066", "issue_summary": "Enable Azure Storages for files/images pipeline", "issue_description": "fpghost commented on Jun 21, 2016\nAllow the user to use Azure Storage for files or images persistence when using the files or images pipelines, in an analogous way to AWS S3 Storage.\nI issued a PR that achieves this: #2064.\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2016-06-20T20:14:41Z"}, "288": {"issue_url": "https://github.com/scrapy/scrapy/issues/2046", "issue_id": "#2046", "issue_summary": "Allow to create spider with custom templates", "issue_description": "DrJackilD commented on Jun 9, 2016 \u2022\nedited\nI don't know, am I wrong, but is it possible now to create custom templates and create spiders with scrapy genspider -t <custom_template> <name> <domain>? As I see in source code, user can set custom template folder in TEMPLATE_FOLDER variable, but is there any documentation about template language, which using in template?\nActually, I've planned to work on PR with possibilities to create custom templates, but probably it's already implemented. May be just add documentation about this feature?\nUPD. Ah, I see, that _genspider using render_templatefile, which just using string.Template().substitute() to render template and save .py file", "issue_status": "Open", "issue_reporting_time": "2016-06-09T16:48:40Z"}, "289": {"issue_url": "https://github.com/scrapy/scrapy/issues/2016", "issue_id": "#2016", "issue_summary": "Request from Pipeline", "issue_description": "gdomod commented on May 27, 2016\nHi there,\ni want to send the item data to another webservice. how can i use scrapy.Request to send the item to other website\nreq = scrapy.Request(\n                    url, method='POST',\n                    body=json.dumps(dict(item)),\n                    headers={'content-type': 'application/json'},\n                    priority=1000,\n                    callback=self.callback)\nthe request is correct but scrapy.Request didnt fire the request\nwith yield req, or return req", "issue_status": "Open", "issue_reporting_time": "2016-05-27T10:57:05Z"}, "290": {"issue_url": "https://github.com/scrapy/scrapy/issues/2007", "issue_id": "#2007", "issue_summary": "Scrapy websockets downloader", "issue_description": "Contributor\npawelmhm commented on May 24, 2016 \u2022\nedited\nSeems like now Scrapy does not provide any way to crawl website that use websockets. Can we support something like this? There are websocket implementations based on Twisted: http://autobahn.ws/python/ so probably it could be possible to add some downloader for websockets? I know that HTTP is main protocol used by majority of spiders, but we also support FTP or s3 so maybe we could support websockets too?\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2016-05-24T08:19:09Z"}, "291": {"issue_url": "https://github.com/scrapy/scrapy/issues/1992", "issue_id": "#1992", "issue_summary": "DEFAULT_REQUEST_HEADERS not work as expected", "issue_description": "exotfboy commented on May 16, 2016 \u2022\nedited\nI am new in scrapy, and I meet some problems which I can not get answer from google, so I post it here:\n1 Cookie not work even set in DEFAULT_REQUEST_HEADERS:\nDEFAULT_REQUEST_HEADERS = {\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'accept-encoding': 'gzip, deflate, sdch',\n    'cache-control': 'no-cache',\n    'cookie': 'xx=yy',\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.94 Safari/537.36'\n}\nclass MySpider(scrapy.Spider):\n    def make_requests_from_url(self, url):\n        return scrapy.http.Request(url, headers=DEFAULT_REQUEST_HEADERS)\nI know the make_requests_from_url will only called once for the start_urls, and in my opinion, the first request will send the cookie I set in the DEFAULT_REQUEST_HEADERS, however it does not.\n2 Share settings between spiders.\nI have multiple spiders in the project which share most of the settings like RandomAgentMiddleware RandomProxyMiddleware UserAgent DEFAULT_REQUEST_HEADERS and etc, however they are configured inside the settings.py for each spider.\nIs it possible to share these settings?\nThe\nCOOKIES_ENABLED is set to true.", "issue_status": "Open", "issue_reporting_time": "2016-05-16T02:41:06Z"}, "292": {"issue_url": "https://github.com/scrapy/scrapy/issues/1988", "issue_id": "#1988", "issue_summary": "\"Content-Encoding\" header gets stripped from response headers", "issue_description": "mborho commented on May 12, 2016 \u2022\nedited\nSee https://github.com/scrapy/scrapy/blob/master/scrapy/downloadermiddlewares/httpcompression.py#L36\nIMHO the \"Content-Encoding\" header should get preserved, since the spider probably wants to see all the original response headers.", "issue_status": "Open", "issue_reporting_time": "2016-05-12T15:26:02Z"}, "293": {"issue_url": "https://github.com/scrapy/scrapy/issues/1981", "issue_id": "#1981", "issue_summary": "How to catch the log message from the extensions ?", "issue_description": "BruceDone commented on May 11, 2016 \u2022\nedited\nHi all:\nsince the scrapy log can be outputed as file or output to the shell screen , i want to use the extentions to collect all the log message content(not just the count) .I refer the doc and write this code .\nimport logging\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.extensions.logstats import LogStats\nfrom scrapy.extensions.spiderstate import SpiderState\nfrom scrapy.extensions.corestats import CoreStats\nfrom scrapy.extensions.closespider import CloseSpider\nimport scrapy.utils.log\nimport scrapyd.runner\nfrom scrapy import log as log_level\nfrom scrapy.utils import signal as common_signal\n\nfrom scrapy.log import level_names\nfrom twisted.python import log as txlog\n\n\nfrom twisted.python import log as twisted_log\n\nlogger = logging.getLogger(__name__)\n\nclass SpiderOpenCloseLogging(object):\n\n    def __init__(self, item_count,stats):\n        self.item_count = item_count\n        self.items_scraped = 0\n        self.stats = stats\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool('MYEXT_ENABLED'):\n            raise NotConfigured\n\n        # get the number of items from settings\n        item_count = crawler.settings.getint('MYEXT_ITEMCOUNT', 1000)\n\n        # instantiate the extension object\n        ext = cls(item_count,crawler.stats)\n\n        # connect the extension object to signals\n        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(ext.spider_error, signal=signals.spider_error)\n        #crawler.signals.connect(ext.spider_error,signal=common_signal.send_catch_log)\n\n        # return the extension object\n        return ext\n\n    def spider_opened(self, spider):\n        logger.info(\"opened spider %s\", spider.name)\n        logger.info('the start time is %s ' % self.stats)\n\n    def spider_closed(self, spider):\n        logger.info(\"closed spider %s\", spider.name)\n        logger.info('the total count of of info is %s ' % self.stats)\n\n    def item_scraped(self, item, spider):\n        self.items_scraped += 1\n        if self.items_scraped % self.item_count == 0:\n            logger.info(\"scraped %d items\", self.items_scraped)\n\n    def spider_error(self,failure, response, spider):\n        logger.info('the error comes from the %s',spider.name)\n        logger.info('the message is %s',failure.value.message)\nafter i try this ,it just collected the log count ,not the content .all the signals i try can not collect the log message content.\nthere may be two ways to do\n1.edit the source code\n2.just define a log function by myself and write to database.\nbut these two ways are not best practices\nCan you give me some suggestions ,really appreciate for your help:)\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2016-05-11T12:55:07Z"}, "294": {"issue_url": "https://github.com/scrapy/scrapy/issues/1977", "issue_id": "#1977", "issue_summary": "scrapy logging with python logging module", "issue_description": "sergei-sh commented on May 11, 2016 \u2022\nedited\nI need to configure several custom loggers of the following type:\n-writing both to a file and console\n-writing to console only\n-writing to file only\nI use logging.fileConfig() which works fine without scrapy. I set rootLogger to write to /dev/null, so that I can setup every child logger individually.\nI've tried the following options:\n-Running a spider with CrawlerProcess()\n-Running a spider with CrawlerRunner() with:\na) configure_logging(install_root_handler=True)\nb) configure_logging(install_root_handler=False)\nc) configure_logging(install_root_handler=False) and manually calling logging.getLogger(\"scrapy\").addHandler(...)\nI neither case it worked meaning all loggers write to intended location ONLY, AND I see all scrapy messages.\nThe last case c) was the closest to be correct, by I could see only a PART of scrapy messages (not depending on logging level).", "issue_status": "Open", "issue_reporting_time": "2016-05-11T03:51:35Z"}, "295": {"issue_url": "https://github.com/scrapy/scrapy/issues/1964", "issue_id": "#1964", "issue_summary": "twisted error stack trace doesn't output when the spider is running in scrapyd", "issue_description": "ksimple commented on May 2, 2016\nThe outputted log is something like the following text and no stack trace which makes diagnostic super hard.\n2016-05-02 20:25:02 [twisted] CRITICAL: Unhandled error in Deferred:\n2016-05-02 20:25:02 [twisted] CRITICAL:\nI created a from_crawler method on spider and it runs successfully on local machine but failed on scrapyd. Finally I notice that the signature of from_crawler on scrapyd should be from_crawler(cls, crawler, **kwargs) since there is a _job parameter. This is the first hard thing to discover since the reference doesn't mention it.\nBut the diagnostic of this one is toooooooooooo hard since there is no stack trace in twisted log. I have to write my own log observer and use addObeserve to log it by myself and find the first issue.\nSo please check why the stack trace doesn't output to log. I downloaded the code but can't figure out by just reading the code, thanks.", "issue_status": "Open", "issue_reporting_time": "2016-05-02T12:32:46Z"}, "296": {"issue_url": "https://github.com/scrapy/scrapy/issues/1948", "issue_id": "#1948", "issue_summary": "Incorrect traceback in downloader middleware with @inlineCallbacks", "issue_description": "Member\nlopuhin commented on Apr 21, 2016 \u2022\nedited\nHere is a silly example (put it into bad_traceback.py and run with python3.4 bad_traceback.py):\nimport scrapy\nfrom twisted.internet.defer import inlineCallbacks\nfrom scrapy.crawler import CrawlerProcess\n\nclass BuggyMiddleware(object):\n    @inlineCallbacks\n    def process_request(self, request, spider):\n        undefined  # this is the bug for which I expect to see the traceback\n        yield scrapy.Request('http://yandex.ru')\n\nclass Spider(scrapy.Spider):\n    name = 'spider'\n    start_urls = ['http://google.com']\n    def parse(self, response):\n        print(response)\n\nprocess = CrawlerProcess({\n    'DOWNLOADER_MIDDLEWARES': {'bad_traceback.BuggyMiddleware': 584}\n    })\nprocess.crawl(Spider)\nprocess.start()\nThe output is duplicate for some reason (it's not the case when running it in a normal scrapy project), the traceback I get is:\n2016-04-21 16:02:05 [scrapy] ERROR: Error downloading <GET http://google.com>\nTraceback (most recent call last):\n  File \"/Users/kostia/shub/memex/undercrawler/venv/lib/python3.4/site-packages/twisted/internet/defer.py\", line 1126, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/Users/kostia/shub/memex/undercrawler/venv/lib/python3.4/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/Users/kostia/shub/memex/undercrawler/venv/lib/python3.4/site-packages/scrapy/core/downloader/middleware.py\", line 37, in process_request\n    response = yield method(request=request, spider=spider)\nNameError: name 'undefined' is not defined\nHere I expected to see BuggyMiddleware.process_request. I used CPython 3.4.1 with a recent scrapy (288f8c0), using Twisted 15.5.0 and 16.1.1 (output is the same in both cases).", "issue_status": "Open", "issue_reporting_time": "2016-04-21T13:08:13Z"}, "297": {"issue_url": "https://github.com/scrapy/scrapy/issues/1918", "issue_id": "#1918", "issue_summary": "scrapy contracts issues", "issue_description": "Contributor\npawelmhm commented on Apr 11, 2016\nI tried using Scrapy contracts today and found following problems, I'm creating issue for future reference in case others find it helpful (perhaps it could also serve as guide for future improvements).\nIt's very difficult (or even impossible) to customize test case to suit your needs. In my project some class of spiders always expect response with some meta keys. This seems like common use case, but there is no easy way to pass meta to spider contract. I found out the only way is to create custom contract that updates args for request, but this custom contract has to be added to contract docstring as well which is not documented (I thought adding custom contract to settings is enough, but no you also have to add @custom_contract_name to docstring).\nThere is no easy way to customize request being made from contract. I need to test callback that is response to POST with some formdata, headers, cookies and some meta keys. Passing all those values to Request init should do the trick, but there is no simple api to do that.\nThere is no timeouts for some tests, I noticed that one of the spider was hanging for long time, it would be better to close it down rather than wait for it to end.\nThere is no way to pass command line arguments to spider test. It would be very useful but currently the way contracts are designed you cannot do this in any way (you can create custom contract that will pass arguments to Request(), but there's no contract for spider init).\nI think the problems are mostly result of docstring test format. Ideally you should be able to specify meta in dosctring, e.g.\n@url http://example.com\n@meta {\"a\": \"b\"}\n@return Item\nbut with current implementation args for meta will be parsed as strings and they are split on whitespace, so to parse this you'd have to actually write some JSON and this JSON must be without whitespace. Same for specyfing request init args. Ideally I would do something like this\n@request {\"method\": \"POST\", \"headers\": {\"foo\":\"bar\"}}\nbut this is not possible with current implementation.\nHow about switching to yaml? This will allow us to have test description like this\nrequest: \n    url: http://example.com\n    method: POST,\n    meta : \n         variant_request: True\n         item: \n               name: foobar\n    headers:\n          header-one: header-value\n          header-two: another-value\nspider_init_args:\n    zipcode: 14001\nreturns:\n     item: \n         name: \"bar\"\nthis should allow detailed specification of test case. Yaml would be processed into python dict, and from this dict we could create spider test cases that will control spider init, request init and add proper tests on output. Above would translate to following description: \"create POST Request with following meta and following headers; initialize spider with following argument; return item with name bar\".\nAside from switching to yaml we could move contracts out of spider docstrings. Maybe this yaml file with test specs could be stored outside spider code, e.g. you could have folder spider_tests full of yaml files describing each test case. In case your tests grow large you can easily manage that, you have it in yaml so there is nice syntax highlighting and everything is more readable.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2016-04-11T13:24:29Z"}, "298": {"issue_url": "https://github.com/scrapy/scrapy/issues/1900", "issue_id": "#1900", "issue_summary": "RetryMiddleware doesn't actually retry HttpCompressionMiddleware errors", "issue_description": "Contributor\nArturGaspar commented on Mar 31, 2016\nA comment in the code of RetryMiddleware says\n    # IOError is raised by the HttpCompression middleware when trying to\n    # decompress an empty response\nwhen including IOError in the exceptions to retry.\nBut IOError would be raised in HttpCompressionMiddleware's process_response(), errors from which are not handled by process_exception() in downloader middlewares.", "issue_status": "Open", "issue_reporting_time": "2016-03-31T15:31:46Z"}, "299": {"issue_url": "https://github.com/scrapy/scrapy/issues/1897", "issue_id": "#1897", "issue_summary": "support for \"multipart/form-data\" in scrapy.FormRequest", "issue_description": "Contributor\nDharmeshPandav commented on Mar 31, 2016\nNo description provided.", "issue_status": "Open", "issue_reporting_time": "2016-03-31T09:24:50Z"}, "300": {"issue_url": "https://github.com/scrapy/scrapy/issues/1895", "issue_id": "#1895", "issue_summary": "set HttpCompressionMiddleware priority to 810", "issue_description": "Member\nkmike commented on Mar 31, 2016\nCurrently DOWNLOADER_MIDDLEWARE options looks like this:\nDOWNLOADER_MIDDLEWARES_BASE = {\n    # Engine side\n    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 400,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 500,\n    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 550,\n    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,\n    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,\n    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\n    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\n    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\n    'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware': 830,\n    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\n    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\n    # Downloader side\n}\nI encountered a case where this order causes problems: I want to add custom headers to response based on its contents (load cookies from JSON response body and handle them using standard CookiesMiddleware). But because CookiesMiddleware.process_response is called before HttpCompressionMiddleware.process_response this is not possible with a default middleware order - body is not uncompressed yet.\nThis change will move HttpCompressionMiddleware after RedirectMiddleware (i.e. responses will be decompressed prior to redirect handling); it means Scrapy can do a bit more work for redirect responses. I don't think this is a problem because 301, 302, 303, 307 responses shouldn't have bodies.", "issue_status": "Open", "issue_reporting_time": "2016-03-30T23:38:58Z"}, "301": {"issue_url": "https://github.com/scrapy/scrapy/issues/1889", "issue_id": "#1889", "issue_summary": "Per-host cache for connection/fetching errors, invalidating by time", "issue_description": "Contributor\nsibiryakov commented on Mar 29, 2016\nHere is the use-case. When doing broad crawling, there are could be many requests to a single website (along with others too) in downloader queue. This website is broken, and refuses to connect after timeout. I think, it doesn't make sense to continue processing requests to this website after 3 unsuccessful tries without significant pause. Therefore placing such a hostname in error cache, which invalidates say every hour should solve the problem.\nCurrently, Scrapy is blindly processing downloader queue, spending concurrent requests quota and CPU time.", "issue_status": "Open", "issue_reporting_time": "2016-03-29T10:01:47Z"}, "302": {"issue_url": "https://github.com/scrapy/scrapy/issues/1882", "issue_id": "#1882", "issue_summary": "FilesPipeline does not work with S3FilesStore with botocore", "issue_description": "Member\nlopuhin commented on Mar 27, 2016\nFirst, there is a little issue with meta=None (fix lopuhin@5045a4f) which is not tested yet\nscrapy/tests/test_pipeline_files.py\nLine 195 in d61fbcc\n meta = {'foo': 'bar'} \nBut there also might be other issues as well: for some reason I get this error in the logs, although this might be not a scrapy issue (I'll try to debug it):\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error>\n<Code>BadDigest</Code>\n<Message>The Content-MD5 you specified did not match what we received.</Message>\n<ExpectedDigest>1B2M2Y8AsgTpgAmY7PhCfg==</ExpectedDigest>\n<CalculatedDigest>yCkJGdYt+uzVuLExTok++g==</CalculatedDigest>\n<RequestId>15452BC432AD0B59</RequestId>\n<HostId>giQYaIWMSgCLvoQaR+F6jXfe456/wUEG4G86QAtKiLVYSF+UXBX+Tvn7AnJlDGd194VRnbJgDFo=</HostId>\n</Error>", "issue_status": "Open", "issue_reporting_time": "2016-03-27T09:59:51Z"}, "303": {"issue_url": "https://github.com/scrapy/scrapy/issues/1878", "issue_id": "#1878", "issue_summary": "Expose cookiejars", "issue_description": "Member\nkmike commented on Mar 24, 2016\nScrapy cookiejar API is limited:\nmeta key is called cookiejar, but you can't put CookieJar object there, in fact it means cookiejar_id or session_id, not cookiejar; this is confusing. It should have been called session_id IMHO.\nthere is no way to get or set current cookies; it is a popular issue we don't have a solution for (see http://stackoverflow.com/questions/8708346/access-session-cookie-in-scrapy-spiders and #1448).\nI think we should provide a better API for 'sessions'. It should allow to\naccess current session cookies;\n'fork' a session - start separate sessions from the current session.\nCurrently I'm using an ugly hack to access cookies:\nclass ExposeCookiesMiddleware(CookiesMiddleware):\n    \"\"\"\n    This middleware appends CookieJar with current cookies to response flags.\n\n    To use it, disable default CookiesMiddleware and enable\n    this middleware instead::\n\n        DOWNLOADER_MIDDLEWARES = {\n            'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': None,\n            'autologin.middleware.ExposeCookiesMiddleware': 700,\n        }\n\n    \"\"\"\n    def process_response(self, request, response, spider):\n        response = super(ExposeCookiesMiddleware, self).process_response(\n            request, response, spider)\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        response.flags.append(self.jars[cookiejarkey])\n        return response\n\n\ndef get_cookiejar(response):\n    for obj in response.flags:\n        if isinstance(obj, CookieJar):\n            return obj\nI don't have a concrete API proposal, but likely it should use a word 'session' :)\n\ud83d\udc4d 19", "issue_status": "Open", "issue_reporting_time": "2016-03-24T15:56:01Z"}, "304": {"issue_url": "https://github.com/scrapy/scrapy/issues/1877", "issue_id": "#1877", "issue_summary": "Allow to add custom metadata to Response", "issue_description": "Member\nkmike commented on Mar 24, 2016\nCurrently it is not possible to attach a custom attribute to Response in a middleware because replace method won't keep it. There are workarounds (e.g. putting objects to response.flags), but they are ugly.\nWhat do you think about adding an attribute to Response where user can put anything, similar to Request.meta?", "issue_status": "Open", "issue_reporting_time": "2016-03-24T15:26:00Z"}, "305": {"issue_url": "https://github.com/scrapy/scrapy/issues/1876", "issue_id": "#1876", "issue_summary": "Deprecate Request's AJAX escaping", "issue_description": "Contributor\nredapple commented on Mar 23, 2016\nGoogles' AJAX crawling scheme was deprecated in October 2015\nIs it time to disable escape_ajax() when building Requests, or disabling it by default, with a new escape_ajax argument in the contructor for those who need it?", "issue_status": "Open", "issue_reporting_time": "2016-03-23T11:18:19Z"}, "306": {"issue_url": "https://github.com/scrapy/scrapy/issues/1868", "issue_id": "#1868", "issue_summary": "Allow downloader middleware process_response method to yield multiple Responses", "issue_description": "dizlv commented on Mar 17, 2016\nCurrently working with Splash I found out that it might be a good idea to process iframes as different responses in downloader middleware process_response method.\nHowever turned out it's not possible, because Scrapy downloader middleware allows to return only 1 Response.\nAt the moment processing looks like this:\ndef load_splash_responses(response):\n    json_response = json.loads(response.body_as_unicode())\n\n    # load original response (not from iframe).\n    yield response.replace(body=json_response['html'],\n                           url=json_response['requestedUrl'])\n\n    # load iframe responses.\n    for iframe in json_response['childFrames']:\n        iframe_body = iframe['html']\n\n        yield response.replace(body=iframe_body,\n                               url=iframe['requestedUrl'])\nSomewhere in Spider:\ndef parse(self, response):\n    responses = utils.load_splash_responses(response)\n\n    for response in responses:\n        # process responses\nWhat I want to make possible is something like this:\nclass SplashIframeResponsesMiddleware(object):\n    def process_response(self, request, response, spider):\n        unicode_body = response.body_as_unicode()\n        json_response = json.loads(unicode_body)\n\n        # load original response (not from iframe).\n        yield response.replace(body=json_response['html'],\n                               url=json_response['requestedUrl'])\n\n        # load iframe responses.\n        for iframe in json_response['childFrames']:\n            iframe_body = iframe['html']\n\n            yield response.replace(body=iframe_body,\n                                   url=iframe['requestedUrl'])\nSpider:\ndef parse(self, response):\n    # process responses here\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2016-03-17T13:37:49Z"}, "307": {"issue_url": "https://github.com/scrapy/scrapy/issues/1866", "issue_id": "#1866", "issue_summary": "Move from boto2/botocore to boto3", "issue_description": "jersub commented on Mar 16, 2016\nHi all,\nScrapy moved to a hybrid use of both boto2/botocore since PR #1761. I feel maintaining two AWS libraries would be difficult on the long term, and testing would be even more painful. I see no point for keeping boto2 support (apart for some needed backward compatibility) since botocore supports Python 2.7 as well (see https://botocore.readthedocs.org/en/latest/).\nBesides, I see some benefits for using boto3 instead of botocore. It supports Python 2.7 as well, see https://github.com/boto/boto3/blob/develop/tox.ini. Actually it stands at a higher level and would allow us to fix bug #960 in few lines of code (cf. multipart upload, see http://boto3.readthedocs.org/en/latest/reference/customizations/s3.html#module-boto3.s3.transfer).\nAnyway issue #1790 (i.e. S3 testing) would have to be handled at the same time.", "issue_status": "Open", "issue_reporting_time": "2016-03-16T13:53:01Z"}, "308": {"issue_url": "https://github.com/scrapy/scrapy/issues/1854", "issue_id": "#1854", "issue_summary": "HTTP 2 support", "issue_description": "povilasb commented on Mar 9, 2016\nI peeked at the docs, at the issues but couldn't find any info about HTTP 2 support.\nDoes scrapy support it?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2016-03-09T11:00:32Z"}, "309": {"issue_url": "https://github.com/scrapy/scrapy/issues/1845", "issue_id": "#1845", "issue_summary": "run spider from within scrapy shell (when shell is activated from within project folder path)", "issue_description": "rosnk commented on Mar 4, 2016\ni have notice crawler.spiders.list() within scrapy folder path shows list of spider in my project. And now i want to run individual spider and play with response. How can i do this from scrapy shell running from scrapy project folder path?", "issue_status": "Open", "issue_reporting_time": "2016-03-04T17:33:39Z"}, "310": {"issue_url": "https://github.com/scrapy/scrapy/issues/1832", "issue_id": "#1832", "issue_summary": "IPv6 addresses not correctly recognized", "issue_description": "Contributor\nnyov commented on Mar 1, 2016\nIn a follow-up to #1116 scrapy does not recognize IPv6 addresses correctly.\nIPv6 address notation should be written inside brackets as [<ip>].\n(Check browser behavior for http://::1/ and http://[::1]/. But beware of the wrongly urlescaped [] when copying the second link).\nScrapy seems to do the exact opposite:\n$ scrapy-dev shell \"http://[::1]/\"\n2016-03-01 14:31:34 [scrapy] INFO: Scrapy 1.2.0dev2 started (bot: testbot)\n2016-03-01 14:31:34 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'testbot.spiders', 'SPIDER_MODULES': ['testbot.spiders'], 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'BOT_NAME': 'testbot'}\n2016-03-01 14:31:34 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-03-01 14:31:34 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-03-01 14:31:34 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-03-01 14:31:34 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-03-01 14:31:34 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-03-01 14:31:34 [scrapy] INFO: Spider opened\n2016-03-01 14:31:34 [scrapy] DEBUG: Retrying <GET http://%5B::1%5D/> (failed 1 times): 503 Service Unavailable\n2016-03-01 14:31:34 [scrapy] DEBUG: Retrying <GET http://%5B::1%5D/> (failed 2 times): 503 Service Unavailable\n2016-03-01 14:31:34 [scrapy] DEBUG: Gave up retrying <GET http://%5B::1%5D/> (failed 3 times): 503 Service Unavailable\n2016-03-01 14:31:34 [scrapy] DEBUG: Crawled (503) <GET http://%5B::1%5D/> (referer: None)\n2016-03-01 14:31:34 [root] DEBUG: Using default logger\n2016-03-01 14:31:34 [root] DEBUG: Using default logger\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f29e720d590>\n[s]   item       {}\n[s]   request    <GET http://%5B::1%5D/>\n[s]   response   <503 http://%5B::1%5D/>\n[s]   settings   <scrapy.settings.Settings object at 0x7f29e720d610>\n[s]   spider     <DefaultSpider 'default' at 0x7f29e1d0b7d0>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\nIn [1]: \n...without the brackets it seems to work. Where it shouldn't, IMO.\n$ scrapy-dev shell \"http://::1/\"\n# <snip>\nIn [1]: response.status\nOut[1]: 200", "issue_status": "Open", "issue_reporting_time": "2016-03-01T14:41:34Z"}, "311": {"issue_url": "https://github.com/scrapy/scrapy/issues/1790", "issue_id": "#1790", "issue_summary": "Run all S3 tests on travis", "issue_description": "Member\nlopuhin commented on Feb 18, 2016\nThere are some tests S3 that are currently skipped on travis (S3FeedStorageTest, TestS3FilesStore) because they need real S3. It would be nice to run them, here are some options:\nTest against real S3, using https://docs.travis-ci.com/user/environment-variables/ to hide ASW keys. But there are some problems: tests might be unstable if S3 is down, and this depends on someone providing AWS keys and paying the (tiny) bills.\nUse something like https://github.com/jubos/fake-s3 to fake S3 - if this works, than it will be much more convenient.\nMock S3 (did not find anything satisfactory upfront, but maybe there is something).\nFrom discussion here: #1761 (comment)", "issue_status": "Open", "issue_reporting_time": "2016-02-18T15:02:28Z"}, "312": {"issue_url": "https://github.com/scrapy/scrapy/issues/1777", "issue_id": "#1777", "issue_summary": "Scrapy as a library", "issue_description": "Contributor\nnyov commented on Feb 15, 2016\nScrapy currently assumes in a lot of it's functionality to be used as a Framework.\nWork has been done in the past, and is ongoing, to make it more usable as a library as well.\nI would like to see even more change in this regard.\nIs it feasible to consider decoupling scrapy's django-like concept of a \"project\" in the filesystem from \"scrapy core\" -- possibly moving the behavior of creating a 'scrapy project' (and all that entails, such as file templates) into a separate project? Or the opposite of creating a libscrapy project for the bare essentials?\nA lot of scrapy's environment configuration is done through the Settings object.\nAll configurable classes/functions that are defined through settings (anything that reads like scrapy.downloadermiddlewares.redirect.RedirectMiddleware), get loaded through load_object which in turn leverages importlib.import_module to find a module and pull in the code.\nIn a scripted environment we usually just want to load an already defined or imported class into scrapy, such as an ItemPipeline, and I would like to see support for loading objects directly in some manner there.\n/edit: Being discussed in #1215 (thanks @kmike)\nI hope this is not too \"out there\".\n(And if someone has other scrapy-as-a-library-related considerations, they are welcome to add them here as well.\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2016-02-14T18:39:36Z"}, "313": {"issue_url": "https://github.com/scrapy/scrapy/issues/1775", "issue_id": "#1775", "issue_summary": "Feature Request: Uniform MapCompose-like processor", "issue_description": "Chratho commented on Feb 11, 2016\nIt took me some time to figure out that MapCompose flattens iterables returned by the specified functions. Even though I then I realized that this behavior is well documented, I still wonder: Why is that? To be honest, it feels only appropriate for highly-specific use-cases. It works fine for unary values, but if people start using it on multi-valued fields (e.g., tuples) the results might be counter-intuitive.\nThe following code is a slightly adapted version of MapCompose, mostly just using append instead of += for value-concatenation:\nclass ListCompose(object):\ndef __init__(self, *functions, **default_loader_context):\n    self.functions = functions\n    self.default_loader_context = default_loader_context\n\ndef __call__(self, values, loader_context=None):\n    if loader_context:\n        context = MergeDict(loader_context, self.default_loader_context)\n    else:\n        context = self.default_loader_context\n    wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n    for func in wrapped_funcs:\n        next_values = []\n        for v in values:\n            next_values.append(func(v))\n        values = next_values\n    return values\nSince nearly my all fields are initially created by add_xpath (an approach where Compose turns out to be pointless), I ended up using ListCompose a lot (just like MapCompose it works for unary fields as well", "issue_status": "Open", "issue_reporting_time": "2016-02-10T22:02:31Z"}, "314": {"issue_url": "https://github.com/scrapy/scrapy/issues/1772", "issue_id": "#1772", "issue_summary": "More possibilities to cancel downloads inside HTTP downloader handler", "issue_description": "Djayb6 commented on Feb 8, 2016\nHello,\nCurrently, a download is cancelled in the HTTP downloader if the expected size of the response is greater than the DOWNLOAD_MAXSIZE setting. However, there is no way to cancel a download after the headers are received and before the body is downloaded based on other conditions, such as the value of a specific header, and I see some cases where it could be useful.\nFor instance, one cannot rely on LinkExtractor to filter out media links (images, videos, etc...) since a link without a media extension could still be a media. Thus, by having a way to obtain the headers of a response when they are received, one can check the value of the Content-Type header and trigger the cancellation of the download if necessary.\nI thought about an implementation and came up with this . The main idea is when the headers of the response are received, the downloader handler sends a signal headers_received with the txresponse and the request, and cancels the download based on the return value of the first receiver's callback. It is a quick hack but is not very intrusive. The main drawback is that when connecting to this signal in a spider, one must specifies sender=Any as the crawler's signal manager is not available in the downloader handler.\nI'm waiting for your remarks and ideas.\nMany thanks,\nJB.", "issue_status": "Open", "issue_reporting_time": "2016-02-08T11:50:06Z"}, "315": {"issue_url": "https://github.com/scrapy/scrapy/issues/1758", "issue_id": "#1758", "issue_summary": "`trackref` ordering is non-deterministic on Windows", "issue_description": "Contributor\njdemaeyer commented on Feb 4, 2016\nThe order of weak references to class instances in scrapy.utils.trackref.live_refs is not guaranteed to be the order of instantiation on Windows:\n# test_trackref.py\n\nfrom scrapy.utils import trackref\n\n\nclass Foo(trackref.object_ref):\n    pass\n\ncounts = {True: 0, False: 0}\nfor i in range(1000):\n    trackref.live_refs.clear()\n    o1 = Foo()\n    o2 = Foo()\n    counts[trackref.get_oldest(\"Foo\") is o1] += 1\n\nprint counts\nPS D:\\Downloads\\scrapy> python .\\test_trackref.py\n{False: 332, True: 668}\nPS D:\\Downloads\\scrapy> python .\\test_trackref.py\n{False: 331, True: 669}\nPS D:\\Downloads\\scrapy> python .\\test_trackref.py\n{False: 332, True: 668}\nIf I add time.sleep(0.001) between the o1= and o2= lines the result is as expected ({True: 1000}), so I guess its due to some kind of runtime optimization by CPython?\nI am not sure if this poses a problem in real world use cases. Do we rapidly instantiate tracked objects somewhere and then rely on trackref.get_oldest?", "issue_status": "Open", "issue_reporting_time": "2016-02-04T11:15:21Z"}, "316": {"issue_url": "https://github.com/scrapy/scrapy/issues/1734", "issue_id": "#1734", "issue_summary": "setup non-scrapinghub emails for reporting COC problems", "issue_description": "Member\nkmike commented on Jan 27, 2016\nSee discussion in #1681.", "issue_status": "Open", "issue_reporting_time": "2016-01-27T12:57:48Z"}, "317": {"issue_url": "https://github.com/scrapy/scrapy/issues/1705", "issue_id": "#1705", "issue_summary": "Image pipeline should allow to decide whether to auto-convert to jpg or not", "issue_description": "ddebernardy commented on Jan 21, 2016\nNot 100% it's accurate, but the issue got reported here:\nhttps://news.ycombinator.com/item?id=10941213\nImage pipeline converts to jpg automatically. Cannot be disabled without writing all the image code yourself", "issue_status": "Open", "issue_reporting_time": "2016-01-21T09:21:42Z"}, "318": {"issue_url": "https://github.com/scrapy/scrapy/issues/1703", "issue_id": "#1703", "issue_summary": "Add support for establishing a pipeline for a throttled down scraping rate", "issue_description": "bhagyas commented on Jan 20, 2016\nNo description provided.", "issue_status": "Open", "issue_reporting_time": "2016-01-20T13:44:23Z"}, "319": {"issue_url": "https://github.com/scrapy/scrapy/issues/1659", "issue_id": "#1659", "issue_summary": "Download delay does not work as documented when CONCURRNT_REQUESTS_PER_IP > 0", "issue_description": "Contributor\norangain commented on Jan 5, 2016\nEven when CONCURRNT_REQUESTS_PER_IP > 0, requests to the hosts which have the same IP but have different domain names, are occasionally sent without delay. I think this is because a domain name is used as a download-slot's key when the domain name does not exist in the DNS cache.\nSee also the following question and kev's answer:\npython - scrapy : how to test the delay between every requests - Stack Overflow http://stackoverflow.com/questions/12089365/scrapy-how-to-test-the-delay-between-every-requests\nHow to reproduce\n1. Save the following spider as test_spider.py\n# coding: utf-8\n\nfrom __future__ import print_function\nimport scrapy\n\n\nclass TestSpider(scrapy.Spider):\n    name = 'test'\n    allowed_domains = ['capybala.com']\n    start_urls = (\n        'http://a.capybala.com/',\n        'http://b.capybala.com/',\n        'http://c.capybala.com/',\n        'http://d.capybala.com/',\n        'http://e.capybala.com/',\n    )\n    custom_settings = {\n        'DOWNLOAD_DELAY': 1,\n        'RANDOMIZE_DOWNLOAD_DELAY': False,\n        'CONCURRENT_REQUESTS_PER_IP': 1,\n    }\n\n    def parse(self, response):\n        print('Download slot:', response.request.meta['download_slot'])\nAlso available at https://gist.github.com/orangain/c6293a47d4bb9756bc16\n2. Run the spider\n$ scrapy runspider test_spider.py\n2016-01-05 20:19:02 [scrapy] INFO: Scrapy 1.1.0dev1 started (bot: scrapybot)\n2016-01-05 20:19:02 [scrapy] INFO: Overridden settings: {}\n2016-01-05 20:19:02 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.closespider.CloseSpider',\n 'scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.memdebug.MemoryDebugger',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.spiderstate.SpiderState',\n 'scrapy.extensions.throttle.AutoThrottle']\n2016-01-05 20:19:02 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats',\n 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware']\n2016-01-05 20:19:02 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-01-05 20:19:02 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-01-05 20:19:02 [scrapy] INFO: Spider opened\n2016-01-05 20:19:02 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-01-05 20:19:02 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://a.capybala.com/> (referer: None)\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://b.capybala.com/> (referer: None)\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://c.capybala.com/> (referer: None)\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://d.capybala.com/> (referer: None)\nDownload slot: a.capybala.com\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://e.capybala.com/> (referer: None)\nDownload slot: b.capybala.com\nDownload slot: c.capybala.com\nDownload slot: d.capybala.com\nDownload slot: e.capybala.com\n2016-01-05 20:19:03 [scrapy] INFO: Closing spider (finished)\n2016-01-05 20:19:03 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 1080,\n 'downloader/request_count': 5,\n 'downloader/request_method_count/GET': 5,\n 'downloader/response_bytes': 1955,\n 'downloader/response_count': 5,\n 'downloader/response_status_count/200': 5,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 1, 5, 11, 19, 3, 281146),\n 'log_count/DEBUG': 6,\n 'log_count/INFO': 7,\n 'response_received_count': 5,\n 'scheduler/dequeued': 5,\n 'scheduler/dequeued/memory': 5,\n 'scheduler/enqueued': 5,\n 'scheduler/enqueued/memory': 5,\n 'start_time': datetime.datetime(2016, 1, 5, 11, 19, 2, 938055)}\n2016-01-05 20:19:03 [scrapy] INFO: Spider closed (finished)\nYou can see that the five URLs are downloaded without delay and a domain name, not an IP address, is used as a download-slot's key:\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://a.capybala.com/> (referer: None)\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://b.capybala.com/> (referer: None)\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://c.capybala.com/> (referer: None)\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://d.capybala.com/> (referer: None)\nDownload slot: a.capybala.com\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://e.capybala.com/> (referer: None)\nDownload slot: b.capybala.com\nDownload slot: c.capybala.com\nDownload slot: d.capybala.com\nDownload slot: e.capybala.com\nNote that these domains have the same IP:\n$ dig +noall +answer {a,b,c,d,e}.capybala.com\na.capybala.com.     120 IN  A   49.212.148.82\nb.capybala.com.     120 IN  A   49.212.148.82\nc.capybala.com.     120 IN  A   49.212.148.82\nd.capybala.com.     120 IN  A   49.212.148.82\ne.capybala.com.     120 IN  A   49.212.148.82\nExpected behavior\nThere should be 1 second delay between requests and download-slot's key should be an IP address. Log should be like:\n2016-01-05 20:19:03 [scrapy] DEBUG: Crawled (200) <GET http://a.capybala.com/> (referer: None)\nDownload slot: 49.212.148.82\n2016-01-05 20:19:04 [scrapy] DEBUG: Crawled (200) <GET http://b.capybala.com/> (referer: None)\nDownload slot: 49.212.148.82\n2016-01-05 20:19:05 [scrapy] DEBUG: Crawled (200) <GET http://c.capybala.com/> (referer: None)\nDownload slot: 49.212.148.82\n2016-01-05 20:19:06 [scrapy] DEBUG: Crawled (200) <GET http://d.capybala.com/> (referer: None)\nDownload slot: 49.212.148.82\n2016-01-05 20:19:07 [scrapy] DEBUG: Crawled (200) <GET http://e.capybala.com/> (referer: None)\nDownload slot: 49.212.148.82\nEnvironment\nTested with the latest Scrapy 1b435b2:\n$ scrapy version -v\nScrapy    : 1.1.0dev1\nlxml      : 3.5.0.0\nlibxml2   : 2.9.0\nTwisted   : 15.5.0\nPython    : 2.7.11 (default, Dec  5 2015, 14:44:47) - [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)]\npyOpenSSL : 0.15.1 (OpenSSL 1.0.2e 3 Dec 2015)\nPlatform  : Darwin-14.5.0-x86_64-i386-64bit", "issue_status": "Open", "issue_reporting_time": "2016-01-05T11:48:30Z"}, "320": {"issue_url": "https://github.com/scrapy/scrapy/issues/1651", "issue_id": "#1651", "issue_summary": "CLOSESPIDER_PAGECOUNT don't work as should", "issue_description": "aktywnitu commented on Dec 29, 2015\nI use scrapy 1.0.3 and can't discover how works CLOSESPIDER extesnion.\nFor command:\nscrapy crawl domain_links --set=CLOSESPIDER_PAGECOUNT=1\nis correctly one requst, but for two pages count:\nscrapy crawl domain_links --set CLOSESPIDER_PAGECOUNT=2\nis infinity of requests.\nSo please explain me how it works in simple example.\nThis is my spider code:\nclass DomainLinksSpider(CrawlSpider):\n    name = \"domain_links\"\n    #allowed_domains = [\"www.example.org\"]\n    start_urls = [ \"www.example.org/\",]\n\n    rules = (\n\n        # Extract links matching 'item.php' and parse them with the spider's method parse_item\n        Rule(LinkExtractor(allow_domains=\"www.example.org\"), callback='parse_page'),\n    )\n\n    def parse_page(self, response):\n        print '<<<',response.url\n        items = []\n        item = PathsSpiderItem()\n\n        selected_links = response.selector.xpath('//a[@href]')\n\n        for link in LinkExtractor(allow_domains=\"www.example.org\", unique=True).extract_links(response):\n            item = PathsSpiderItem()\n            item['url'] = link.url\n            items.append(item)\n        return items", "issue_status": "Open", "issue_reporting_time": "2015-12-29T12:53:49Z"}, "321": {"issue_url": "https://github.com/scrapy/scrapy/issues/1646", "issue_id": "#1646", "issue_summary": "ipython 4.0.1 / python 2.7.11 \"scrapy shell\" breaks with \"ValueError: fallback required, but not specified\"", "issue_description": "jschilling1 commented on Dec 21, 2015\nscrapy shell \"url\" breaks with the following traceback:\nTraceback (most recent call last):\nFile \"/home/user/.pyenv/versions/master2/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\nfunc(_a, *_kw)\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\ncmd.run(args, opts)\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/scrapy/commands/shell.py\", line 63, in run\nshell.start(url=url)\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/scrapy/shell.py\", line 55, in start\nstart_python_console(self.vars)\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/scrapy/utils/console.py\", line 24, in start_python_console\nbanner1=banner, user_ns=namespace, config=config)\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/IPython/terminal/embed.py\", line 68, in init\nsuper(InteractiveShellEmbed,self).init(**kw)\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 547, in init\nself.init_io()\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 718, in init_io\nio.stdout = io.IOStream(sys.stdout)\nFile \"/home/user/.pyenv/versions/2.7.11/envs/master2/lib/python2.7/site-packages/IPython/utils/io.py\", line 28, in init\nraise ValueError(\"fallback required, but not specified\")\nValueError: fallback required, but not specified", "issue_status": "Open", "issue_reporting_time": "2015-12-21T00:07:25Z"}, "322": {"issue_url": "https://github.com/scrapy/scrapy/issues/1615", "issue_id": "#1615", "issue_summary": "File is not downloading when response.status is 201", "issue_description": "kurkop commented on Nov 25, 2015\nI am crawling a web that when call a image it is created on the fly, it returns 201 (create) in response.status, but this is not saved because response.status != 200.\nscrapy/scrapy/pipelines/files.py\nLine 243 in 75cd056\n if response.status != 200: \nDo you think that it is a bug?", "issue_status": "Open", "issue_reporting_time": "2015-11-24T22:02:37Z"}, "323": {"issue_url": "https://github.com/scrapy/scrapy/issues/1611", "issue_id": "#1611", "issue_summary": "_sent_failed cut the errback chain in MailSender", "issue_description": "aplanas commented on Nov 22, 2015\nMailSender._sent_failed return None, instead of failure. This cut the errback call chain, making impossible to detect in the code fail in the mails in client code.", "issue_status": "Open", "issue_reporting_time": "2015-11-21T21:42:13Z"}, "324": {"issue_url": "https://github.com/scrapy/scrapy/issues/1609", "issue_id": "#1609", "issue_summary": "Tracebacks are wrongfully written to stderr", "issue_description": "Contributor\nbrunsgaard commented on Nov 20, 2015\nHay folks,\nSometimes i see this error while crawling, and that is fine and dandy.\n2015-11-20 17:11:36 [scrapy] ERROR: Error downloading <GET http://reservations.hotline.com/templates/52591/>\nBut is it expected behavior that the Traceback end up in the terminal, or is that a bug that have to be addressed?\n2015-11-20 17:11:36 [scrapy] ERROR: Error downloading <GET http://reservations.hotline.com/templates/52591/>\nTraceback (most recent call last):\n  File \"/Users/brunsgaard/clerk/crawlers/venv2/lib/python2.7/site-packages/twisted/internet/defer.py\", line 588, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/Users/brunsgaard/clerk/crawlers/venv2/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py\", line 226, in _cb_timeout\n    raise TimeoutError(\"Getting %s took longer than %s seconds.\" % (url, timeout))\nTimeoutError: User timeout caused connection failure: Getting http://reservations.hotline.com/templates/52591/ took longer than 6.0 seconds..\nIt seems that\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/handlers/http11.py#L226\nraises the error, but shouldn't the error be caught somewhere?", "issue_status": "Open", "issue_reporting_time": "2015-11-20T16:22:26Z"}, "325": {"issue_url": "https://github.com/scrapy/scrapy/issues/1587", "issue_id": "#1587", "issue_summary": "Multiple crawls with the same crawler", "issue_description": "Contributor\njdemaeyer commented on Nov 9, 2015\nWhile we don't support using the same crawler for multiple spiders, it is currently possible to run the same crawler (with the same spider) twice, possibly with different spider arguments. I.e., this works:\ncrawler = Crawler(myspidercls)\ncrawler.crawl(my_sparg='blah')\ncrawler.crawl(my_sparg='other_blah')\nThis is done in some of our tests (1, 2, 3). Are these tests legacy code or is support for multiple crawls part of the public API (see also this discussion)?", "issue_status": "Open", "issue_reporting_time": "2015-11-09T00:47:08Z"}, "326": {"issue_url": "https://github.com/scrapy/scrapy/issues/1576", "issue_id": "#1576", "issue_summary": "logger missing spider name", "issue_description": "jschilling1 commented on Nov 1, 2015\nwhen i ran multiple spiders in the same process like so\nhttp://doc.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process\nit's hard to tell what's what, i.e.\n2015-11-01 11:45:19 [scrapy] INFO: Crawled 514 pages (at 514 pages/min), scraped 6234 items (at 6234 items/min)\n2015-11-01 11:45:19 [scrapy] INFO: Crawled 154 pages (at 154 pages/min), scraped 11893 items (at 11893 items/min)\nwhat's the best way to incorporate the name of the spider in there? i.e. [newsbot1] instead of [scrapy]\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2015-11-01T16:50:53Z"}, "327": {"issue_url": "https://github.com/scrapy/scrapy/issues/1572", "issue_id": "#1572", "issue_summary": "Disable telnet by default?", "issue_description": "Member\ncurita commented on Oct 30, 2015\nThis topic was discussed in #1524, where some reasons were presented to keep telnet enable by default (as it is now) or to disable it.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2015-10-30T15:16:36Z"}, "328": {"issue_url": "https://github.com/scrapy/scrapy/issues/1569", "issue_id": "#1569", "issue_summary": "Add a page which explains important Scrapy concepts in a single page", "issue_description": "Member\nkmike commented on Oct 29, 2015\nAs @plafl said: \"Scrapy is very extensible but that has a cost too. There are too many concepts: spiders, items, middlewares, pipelines, exporters, extensions, signals, settings. As a newcomer I would like to know which problem they solve.\"\n+1 :) I think we should add a page which will explain all this in a single place - what are these concepts, when and why to use them.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2015-10-29T13:50:29Z"}, "329": {"issue_url": "https://github.com/scrapy/scrapy/issues/1568", "issue_id": "#1568", "issue_summary": "Add a web UI which shows what's going on inside a spider", "issue_description": "Member\nkmike commented on Oct 29, 2015\nThere is a UI in https://github.com/TeamHG-Memex/arachnado (demo: https://www.youtube.com/watch?v=JPyvmW-eOLs); what about adding something similar to Scrapy itself, maybe as an extension in a separate repository?", "issue_status": "Open", "issue_reporting_time": "2015-10-29T13:43:50Z"}, "330": {"issue_url": "https://github.com/scrapy/scrapy/issues/1560", "issue_id": "#1560", "issue_summary": "`canonicalize_url` should remove known session identifiers", "issue_description": "jvanasco commented on Oct 29, 2015\nJetty tends to add ;jsessionid=md5likestring to the local part of the url\nOS Commerce adds osCsid=md5likestring to the query string in the url\nI'm sure there are other popular ones.\nThere also seems to be a few CMS systems that - when misconfigured - do the following:\n\u2022 set a canonical to the current url + a '/'. This ends up in a an endless loop.\n\u2022 set a canonical to contain a timestamp or random number for cache busting (why the F***** would someone do that? WHY? I don't know, but many do. )", "issue_status": "Open", "issue_reporting_time": "2015-10-28T21:25:45Z"}, "331": {"issue_url": "https://github.com/scrapy/scrapy/issues/1551", "issue_id": "#1551", "issue_summary": "Exceptions occuring in the logger do not invoke the debugger.", "issue_description": "Member\nDigenis commented on Oct 20, 2015\nWhen the logger is misused and throws and exception\nthe --pdb flag doesn't make it drop into a pdb shell.\nE.g. if I put this in the init of some spider:\ndef __init__(self, **kwargs):\n    self.logger.info('%s %s', 1)  # fails because it needs another positional arg.\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 850, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 723, in format\n    return fmt.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 464, in format\n    record.message = record.getMessage()\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 328, in getMessage\n    msg = msg % self.args\nTypeError: not enough arguments for format string\nLogged from file so_and_so_spider.py, line 18\nI think errors there deserve a debugger shell\nbecause some can be caused by conditions in runtime.\nHere's an example:\nAn xls file is downloaded and parsed\nso far the spider was expecting an integer on cell A1\nbut a structural update made cell A1 a string.\nself.logger.info('%d', cell.value()) will fail\nwithout a chance to inspect.", "issue_status": "Open", "issue_reporting_time": "2015-10-20T13:18:14Z"}, "332": {"issue_url": "https://github.com/scrapy/scrapy/issues/1543", "issue_id": "#1543", "issue_summary": "scraping https via proxy: remove custom proxy headers after tunnel connection", "issue_description": "hugojanruiter commented on Oct 15, 2015\nWhen using a proxy service for crawling an https site, the Proxy-authorization header gets removed after the initial HTTP CONNECT method to prevent it being forwarded to the target site in https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/handlers/http11.py line 206:\nif isinstance(agent, self._TunnelingAgent):\n            headers.removeHeader('Proxy-Authorization')\nSome proxy-services (eg. proxymesh.com) allow custom headers to be set (proxymesh: http://proxymesh.com/blog/pages/proxy-server-headers.html#request). If these headers will not be removed after the HTTP CONNECT they will be sent encrypted and the proxy service cannot remove them anymore and they are forwarded to the target site. To prevent these headers from being forwarded to the target site, it would be nice to have an option to remove these as well, similar to the Proxy-Authorization header.\nI am not sure what the best way would be, but maybe via request.meta (eg. 'dont_forward_headers_list')? Any suggestions?", "issue_status": "Open", "issue_reporting_time": "2015-10-15T10:25:03Z"}, "333": {"issue_url": "https://github.com/scrapy/scrapy/issues/1523", "issue_id": "#1523", "issue_summary": "engine_started vs spider_opened", "issue_description": "Member\nkmike commented on Oct 3, 2015\nI think we should explain what is the difference between engine_started and spider_opened signals better. There is now one engine per spider, so these signals look very similar. To make things worse it is documented they can fire in any order:\nThis signal may be fired after the spider_opened signal, depending on how the spider was started. So don\u2019t rely on this signal getting fired before spider_opened.\nI don't know what the difference is and when you want to use engine_started instead of spider_opened :) Any ideas?", "issue_status": "Open", "issue_reporting_time": "2015-10-03T11:48:05Z"}, "334": {"issue_url": "https://github.com/scrapy/scrapy/issues/1511", "issue_id": "#1511", "issue_summary": "Implement concurrent request backoff when downloading files over FTP", "issue_description": "ddcc commented on Sep 28, 2015\nCertain FTP servers are configured to limit the number of simultaneous connections from a single client. Using the ItemPipeline, this results in an unknown-error that is caught by the media_failed function, which prints e.g. WARNING: File (unknown-error): Error downloading file from <GET ftp://...> referred in <None>: ('FTP connection lost', <twisted.python.failure.Failure twisted.protocols.ftp.CommandFailed: ['421 There are too many connections from your internet address.']>.\nIdeally, this request should be rescheduled to be retried later, and the number of concurrent connections reduced.", "issue_status": "Open", "issue_reporting_time": "2015-09-28T03:42:39Z"}, "335": {"issue_url": "https://github.com/scrapy/scrapy/issues/1510", "issue_id": "#1510", "issue_summary": "parse command should use init_request(), make_requests_from_url() from CrawlSpider", "issue_description": "chihchun commented on Sep 27, 2015\nI have some cookie/login sesssion in the Spider class to handle cookies/session of target website.\nIn 1.0.3 implementation the parse command (.scrapy/commands/parse.py) just created a new request and pass response to the Spider.parse(), without calling init_request() or make_requests_from_url() from Spider class.", "issue_status": "Open", "issue_reporting_time": "2015-09-27T05:45:45Z"}, "336": {"issue_url": "https://github.com/scrapy/scrapy/issues/1478", "issue_id": "#1478", "issue_summary": "Adding support to FTP file listing", "issue_description": "bernardotorres commented on Sep 4, 2015\nMake possible to scan FTP repositories. If the path of the request ends in / in a FTP URL, calls FTP list.\nPull Request already open in #941\nI hope the pull request gets at least reviewed.", "issue_status": "Open", "issue_reporting_time": "2015-09-03T20:24:27Z"}, "337": {"issue_url": "https://github.com/scrapy/scrapy/issues/1459", "issue_id": "#1459", "issue_summary": "Headers object serializes to json differently depending on the indent value", "issue_description": "dvdbng commented on Aug 26, 2015\nA scrapy.http.headers.Headers instance will serialize to a shallow object if indent=None and wrap the values in a list if indent is a number:\n>>> from scrapy.http.headers import Headers\n>>> h = Headers({ \"X-Foo\": \"bar\" })\n>>> print json.dumps(h)\n{\"X-Foo\": \"bar\"}\n>>> print json.dumps(h, indent=3)\n{\n   \"X-Foo\": [\n      \"bar\"\n   ]\n}\nTested in Scrapy 1.0.3", "issue_status": "Open", "issue_reporting_time": "2015-08-26T10:45:01Z"}, "338": {"issue_url": "https://github.com/scrapy/scrapy/issues/1453", "issue_id": "#1453", "issue_summary": "Allow custom attributes to be added to responses in middlewares", "issue_description": "vincent-ferotin commented on Aug 24, 2015\nIn order to provide direct access to actually passed cookies for both request and response, it appears to me that custom attributes added to response in middlewares are not allowed.\nMy use case, following work presented in #1448, is to allow access to actual cookies passed to both request and response by CookiesMiddleware. Overwritting cookies request attribute is no problem, as implemented for example here. But it seems to not be possible for response -- however it's still feasible to bypass this limitation by expanding request.meta.\nIt seems that setting request.cookies attribute in middleware.process_request method is possible since it is an already existing attribute of scrapy.http.request.Request. However, response seems to be an other beast: because (i) it is possibly subclassed in TextResponse, XmlResponse and HtmlResponse, and (ii) appropriate class is called at a given stage in download middlewares (as far as I understand), and (iii) constructor of subclasses is called with only predefined Response attributes.\n(NB: The same could eventually occurs for request as well, but, for above use case, overriding request.cookies suffices.)\nIt could be convenient to allow setting custom attributes to request/response through middlewares stack -- in previous example, using response.cookies = [...] (instead of request.meta['response_cookies']). This for example could permit an easy implementation of giving access to cookies passed through requests and responses.\n(Please let me know if/where I'm unclear ;-)", "issue_status": "Open", "issue_reporting_time": "2015-08-24T15:18:16Z"}, "339": {"issue_url": "https://github.com/scrapy/scrapy/issues/1450", "issue_id": "#1450", "issue_summary": "Response.to_dict()", "issue_description": "Contributor\nnyov commented on Aug 24, 2015\nI found myself wanting to have a quick overview about some requests/responses, so the first thing that came to mind was trying dict(response), followed by response.as_dict(). It didn't work ;)\nUnless I miss some other built-in way to do this, I think it'd be nice to have a method that returns a dict of all the \"static data\" of a response or request object; e.g. r.status, r.url, r.encoding, r.flags, r.headers, r.meta, r.body (, r.request) -- instead of needing to list them all, phew.\nr.body could potentially something big, maybe a .to_dict() method should probably accept a parameter to exclude some key(s), instead of filtering them out again later in a list comp or such.\nAlso, the simplest way to turn a response into an item could then be\ndef parse(self, response):\n    item = dict(response)\n    # or\n    item = response.to_dict()\n    return item.update(some_other_stuff)", "issue_status": "Open", "issue_reporting_time": "2015-08-24T07:46:22Z"}, "340": {"issue_url": "https://github.com/scrapy/scrapy/issues/1448", "issue_id": "#1448", "issue_summary": "Allow copying existing cookiejar for request.meta['cookiejar']", "issue_description": "vincent-ferotin commented on Aug 21, 2015\nHi, scrapy developers!\nScrapy cookies middleware since 0.15 allows to have multiple cookies sessions per spider. However, as far as I understand, such new sessions are initialized with an empty cookiejar.\nIt could be useful to allow initializing such new session cookiejar with content of a previous one, which would so save cookies between two different sessions. I encounter such a need when, in a given project, I need to start new cookies sessions after main previous one already had some cookies dedicated to identifying user session. Of course in this case, subsequent requests of new cookies sessions need previous identification cookies.\nTo bypass limitation of actual cookies middleware implementation, I created here some very basic wrapping middleware, which allows such copy between old cookies session and new ones. This is set by new and ugly request.meta['copied_cookiejar'] specifying cookiejar key from which copy already stored cookies. I then use this middleware in lieu et place of scrapy.downloadermiddlewares.cookies.CookiesMiddleware; and in my spider, after namming first cookies session, I then initialized new one with something like this:\nyield Request(url, callback=callback, meta={\n    'copied_cookiejar': response.meta['cookiejar'],\n    'cookiejar': new_cookiejar_id\n    })\nWhat you guys think about this need? And what implementation could be possible?\n\ud83d\udc4d 6", "issue_status": "Open", "issue_reporting_time": "2015-08-21T15:21:45Z"}, "341": {"issue_url": "https://github.com/scrapy/scrapy/issues/1428", "issue_id": "#1428", "issue_summary": "XML serialiser corrupts output when encountering None values", "issue_description": "Malvineous commented on Aug 12, 2015\nIf you set a field to the value None then use the XML serialiser, the output XML becomes corrupted (invalid XML, missing tags, etc.) and you get the following exception for each item during the scrapy run:\n2015-08-12 11:01:43 [scrapy] ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7f70e7723ed0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python2.7/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 57, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 193, in item_scraped\n    slot.exporter.export_item(item)\n  File \"/usr/lib/python2.7/site-packages/scrapy/exporters.py\", line 130, in export_item\n    self._export_xml_field(name, value)\n  File \"/usr/lib/python2.7/site-packages/scrapy/exporters.py\", line 146, in _export_xml_field\n    self._xg_characters(serialized_value)\n  File \"/usr/lib/python2.7/site-packages/scrapy/exporters.py\", line 157, in _xg_characters\n    serialized_value = serialized_value.decode(self.encoding)\nAttributeError: 'NoneType' object has no attribute 'decode'\nThis was happening for me because I was using a CSS selector and extract_first(), but the element was optional, so for those items where the element was missing, the field was being set to None, causing this problem.", "issue_status": "Open", "issue_reporting_time": "2015-08-12T01:07:01Z"}, "342": {"issue_url": "https://github.com/scrapy/scrapy/issues/1405", "issue_id": "#1405", "issue_summary": "Exception in LxmLinkExtractor.extract_links 'ascii' codec can't encode character", "issue_description": "aldarund commented on Aug 2, 2015\nStacktrace (most recent call last):\n\n  File \"scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"scrapy/spiders/crawl.py\", line 69, in _parse_response\n    for requests_or_item in iterate_spider_output(cb_res):\n  File \"ex_link_crawl/spiders/external_link_spider.py\", line 45, in parse_obj\n    for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 108, in extract_links\n    links = self._extract_links(doc, response.url, response.encoding, base_url)\n  File \"scrapy/linkextractors/__init__.py\", line 103, in _extract_links\n    return self.link_extractor._extract_links(*args, **kwargs)\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 57, in _extract_links\n    url = url.encode(response_encoding)\nMy use of extractor is following:\ndef parse_obj(self, response):\n        if not isinstance(response, HtmlResponse):\n            return\n        for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n            if not link.nofollow:\n                yield LinkCrawlItem(domain=link.url)", "issue_status": "Open", "issue_reporting_time": "2015-08-02T12:28:46Z"}, "343": {"issue_url": "https://github.com/scrapy/scrapy/issues/1403", "issue_id": "#1403", "issue_summary": "Exception in LxmLinkExtractor.extract_links 'charmap' codec can't encode character", "issue_description": "aldarund commented on Aug 2, 2015\nStacktrace (most recent call last):\n\n  File \"scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"scrapy/spiders/crawl.py\", line 69, in _parse_response\n    for requests_or_item in iterate_spider_output(cb_res):\n  File \"ex_link_crawl/spiders/external_link_spider.py\", line 45, in parse_obj\n    for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 108, in extract_links\n    links = self._extract_links(doc, response.url, response.encoding, base_url)\n  File \"scrapy/linkextractors/__init__.py\", line 103, in _extract_links\n    return self.link_extractor._extract_links(*args, **kwargs)\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 57, in _extract_links\n    url = url.encode(response_encoding)\n  File \"encodings/cp1252.py\", line 12, in encode\n    return codecs.charmap_encode(input,errors,encoding_table)\nMy use of extractor is following:\ndef parse_obj(self, response):\n        if not isinstance(response, HtmlResponse):\n            return\n        for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n            if not link.nofollow:\n                yield LinkCrawlItem(domain=link.url)", "issue_status": "Open", "issue_reporting_time": "2015-08-02T12:23:58Z"}, "344": {"issue_url": "https://github.com/scrapy/scrapy/issues/1401", "issue_id": "#1401", "issue_summary": "TextResponse and BinaryResponse types", "issue_description": "Contributor\nnyov commented on Aug 1, 2015\nThis is from a quick brain dump in another PR, I'm putting it here for some discuss-ability.\nIn effect we only have Response andTextResponse any longer. HTMLResponse and XMLResponse classes are only leftover faces, their functionality merged into TextResponse.\nInstancing Text, Html or XmlResponse is the same thing by a different name.\nIt could make more sense to use a mime property on the response for such type inferral.\nWe could turn Response into BinaryResponse (still being a superclass for TextResponse), and create a new Response class with the logic to build either Text- or BinaryResponse instances.\nThis is essentially what ResponseTypes currently does, and we might rename that (and clean up the types), with it's from_args method as default behavior.\nThen somebody can \"feel lucky\" and instance a Response, probably getting back the correct type, or they can pick either TextResponse or BinaryResponse directly.\nAdditionally we would have Response.from_content_type() and other methods directly in the Response class (maybe rename to Response.type_from_...() ).\nI think that might be nice because the detection logic of ResponseTypes seems under-used and appreciated, it's used by the system but not much advertised for custom Spider development.", "issue_status": "Open", "issue_reporting_time": "2015-08-01T06:36:52Z"}, "345": {"issue_url": "https://github.com/scrapy/scrapy/issues/1395", "issue_id": "#1395", "issue_summary": "API to retrieve items from execution", "issue_description": "Contributor\neltermann commented on Jul 31, 2015\nIn order to execute from script and retrieving individual items, I've used the following snippet.\nIs there a better way to do that? Also, I wondered if it would be incorporated in the library (probably changing scrapy.crawler:CrawlerProcess).\nimport importlib\nimport multiprocessing\n\nfrom scrapy import signals, optional_features\n\n\ndef scrape_items(timeout, settings, spidercls, spiderkwargs):\n    \"\"\"Runs Scrapy on an isolated process.\n\n    Usage:\n    import logging\n    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n    from myproject import MySpider\n\n    timeout = 10\n    settings = {}\n    spiderkwargs = {'domain': 'scrapy.org'}\n    for item in scraper.scrape_items(timeout, settings, MySpider, spiderkwargs):\n        print(repr(item))\n    \"\"\"\n    timeout = int(timeout)\n    if timeout <= 0:\n        raise ValueError('timeout must be greater than zero')\n\n    items_queue = multiprocessing.Queue()\n    p = multiprocessing.Process(target=_scraper_callback, args=(items_queue, timeout, settings, spidercls, spiderkwargs,))\n    p.start()\n\n    while True:\n        try:\n            queue_item = items_queue.get(timeout=5)\n            if not isinstance(queue_item, tuple) or len(queue_item) < 1 or not isinstance(queue_item[0], six.string_types):\n                continue\n            elif queue_item[0] == 'ITEM':\n                yield queue_item[1]\n                continue\n            elif queue_item[0] == 'FINISHED-SUCCESS':\n                logging.info('Finished crawling.')\n                logging.info(repr(queue_item[1])) # stats\n                break\n            elif queue_item[0] == 'FINISHED-ERROR':\n                raise queue_item[1] # to be handled below\n        except multiprocessing.queues.Empty:\n            continue # try again\n        except Exception as e:\n            exc_type, exc_obj, exc_tb = sys.exc_info()\n            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            exception_str = \"%s:%s %s\" % (fname, exc_tb.tb_lineno, repr(e))\n            logging.info('Error crawling')\n            logging.info(exception_str)\n            break\n    p.join() # ensure process finished its work and queue is empty\n    return\n\n\ndef _scraper_callback(items_queue, timeout, settings, spidercls, spiderkwargs):\n    \"\"\"Auxiliary. Important: this function run in the context of a sub-process.\"\"\"\n\n    optional_features.remove('boto') # see https://github.com/scrapy/scrapy/issues/1099\n\n    # instantiate crawler and spider\n    try:\n        crawler = Crawler(spidercls, settings)\n\n        # connect signals\n        def handle_item(item):\n            items_queue.put(('ITEM', item,))\n        crawler.signals.connect(handle_item, signal=signals.item_scraped)\n        crawler.signals.connect(reactor.stop, signal=signals.spider_closed)\n\n        # run scrapy\n        crawler.crawl(**spiderkwargs)\n        reactor.run()\n        try:\n            stats = crawler.stats.get_stats()\n        except:\n            stats = {}\n        items_queue.put(('FINISHED-SUCCESS', stats))\n    except Exception as e:\n        # any exception on process will make the calling function to return\n        items_queue.put(('FINISHED-ERROR', e))", "issue_status": "Open", "issue_reporting_time": "2015-07-31T17:01:55Z"}, "346": {"issue_url": "https://github.com/scrapy/scrapy/issues/1385", "issue_id": "#1385", "issue_summary": "defect of documentions: provide a list of deprecations", "issue_description": "SirbitoX commented on Jul 25, 2015\nHi,\nToday I was trying to upgrade from 0.24.5 to 1.0.1. But I didn't find any document about how to upgrade from an older version to new one. So I upgraded Scrapy by running the codes.\nI think it is not a good idea to upgrade like this. Maybe I missed some parts on testing and upgrading.\nSo I think it is a good practice to add a document about \"How to upgrade from X to XX?\"", "issue_status": "Open", "issue_reporting_time": "2015-07-25T11:49:37Z"}, "347": {"issue_url": "https://github.com/scrapy/scrapy/issues/1381", "issue_id": "#1381", "issue_summary": "link extractor joining base href to 'tel:' directive", "issue_description": "itamargero commented on Jul 24, 2015\nThe end result I'm getting on the process_links hook is something like:\nhttp://www.domain.com/somepage.htmltel:123456\nhttp://www.domain.com/blog/posttel:123456\nWhen there's an our phone: 123456 Tag\nI would eliminate it at the process_links stage but it gets confusing when the 'tel:' is appended at the end of a url.", "issue_status": "Open", "issue_reporting_time": "2015-07-24T03:47:26Z"}, "348": {"issue_url": "https://github.com/scrapy/scrapy/issues/1372", "issue_id": "#1372", "issue_summary": "Investigate Slack integration", "issue_description": "Member\ncurita commented on Jul 21, 2015\nI know we talked about this before internally, and it's not a development issue, but I wanted to create a ticket to track it somehow and get more feedback.\nI'd love to have a Slack channel for Scrapy since that's where most of Scrapinghub inter-company communication takes place, I'm sure it'd be a lot easier to provide Scrapy support if we used it (and personally I'm not very fond of IRC).\nI think this is possible, I just stumbled upon this repo: gamechanger/dusty which uses this slacking app (http://rauchg.com/slackin/) to create a \"public\" (through invite) channel for the project. I kind of like the approach, and I think it's worth at least investigating it to integrate it to our current work env.", "issue_status": "Open", "issue_reporting_time": "2015-07-20T20:55:55Z"}, "349": {"issue_url": "https://github.com/scrapy/scrapy/issues/1371", "issue_id": "#1371", "issue_summary": "Use priority queues for Downloader slot queues", "issue_description": "Member\nkmike commented on Jul 18, 2015\nCurrently downloader slots use collections.deque for requests queue. It means that once request came from a scheduler to downloader, its priority is no longer respected.\nLet's say global concurrency limit is 10, scheduler returned 10 requests with a low priority (all for a single downloader slot), then user scheduled a request with a high priority (for the same slot), then one of 10 low-priority requests was processed, and downloader fetched high-priority request from a scheduler. In this case this new high-priority request will be only handled after 9 existing low-priority requests.\nWhat about using a priority queue from queuelib instead of deque?\n//cc @dangra @shirk3y", "issue_status": "Open", "issue_reporting_time": "2015-07-18T17:28:29Z"}, "350": {"issue_url": "https://github.com/scrapy/scrapy/issues/1363", "issue_id": "#1363", "issue_summary": "Deprecate `download_delay` spider attribute", "issue_description": "Contributor\nbarraponto commented on Jul 16, 2015\nSince we now have custom settings per spider, I don't see any reason to support the download_delay attribute anymore (other than backwards compatibility). Should we deprecate it?\nhttp://doc.scrapy.org/en/latest/topics/settings.html#download-delay", "issue_status": "Open", "issue_reporting_time": "2015-07-16T00:25:02Z"}, "351": {"issue_url": "https://github.com/scrapy/scrapy/issues/1362", "issue_id": "#1362", "issue_summary": "LogCounterHandler should only handle messages from self.crawler", "issue_description": "Member\nkmike commented on Jul 15, 2015\nLogCounterHandler increases crawler log_count stats for each record, but it should only increase them for logs from the crawler it is created by. This is an issue if you're running several Crawlers in the same process.", "issue_status": "Open", "issue_reporting_time": "2015-07-15T16:34:58Z"}, "352": {"issue_url": "https://github.com/scrapy/scrapy/issues/1355", "issue_id": "#1355", "issue_summary": "Allow to start crawling with just an URL and a callback function", "issue_description": "Contributor\nrmax commented on Jul 12, 2015\nCurrently, Scrapy forces the user to define a spider in order to start scraping any site, but given all execution is centered around requests and callbacks, I think there should be an option to use Scrapy without defining a spider explicitly.\nFor instance, below is a subclass of CrawlerProcess which allows to start crawling with only an URL and a callback function:\nfrom scrapy.crawler import CrawlerProcess as _CrawlerProcess\nfrom scrapy.http import Request\n\n\nclass CrawlerProcess(_CrawlerProcess):\n    \"\"\"This class allows to crawl an URL with a given callback without explictly creating a spider class.\"\"\"\n\n    def crawl(self, request_or_url, callback=None, crawler_or_spidercls=DefaultSpider,\n              *spider_args, **spider_kwargs):\n        if isinstance(request_or_url, basestring):\n            request_or_url = Request(request_or_url, callback=callback, dont_filter=True)\n        spider_kwargs['start_requests'] = lambda: [request_or_url]\n        super(CrawlerProcess, self).crawl(crawler_or_spidercls, *spider_args, **spider_kwargs)\nThen, the example given in www.scrapy.org front page could be written as:\nimport scrapy\n\ndef parse_blog(response):\n    for url in response.css('ul li a::attr(\"href\")').re(r'/\\d\\d\\d\\d/\\d\\d/$'):\n        yield scrapy.Request(response.urljoin(url), parse_titles)\n\ndef parse_titles(response):\n    for post_title in response.css('div.entries > ul > li a::text').extract():\n        yield {'title': post_title}\n\ncrawler = CrawlerProcess()\ncrawler.crawl('http://blog.scrapinghub.com', callback=parse_blog)\ncrawler.start()\nHowever, this approach misses all the options the scrapy command provides, for instance, storing the output of the spider in a file with -o output.jl. And, if the user needs more advanced features, a proper spider class is required in order to take advantage of all the Scrapy features, specially the features that require a specific spider method or attribute.\nPD: If anyone likes this idea and wants to contribute, please feel free to provide a PR.", "issue_status": "Open", "issue_reporting_time": "2015-07-12T01:35:18Z"}, "353": {"issue_url": "https://github.com/scrapy/scrapy/issues/1336", "issue_id": "#1336", "issue_summary": "Support multiple exporters when crawling", "issue_description": "miguelcb84 commented on Jul 3, 2015\nI'm scraping a website to export the data into a semantic format (n3). However, I also want to perform some data analysis on that data, so having it in a csv format is more convenient. To get the data in both formats I can do.\nscrapy spider -t n3 -o data.n3\nscrapy spider -t csv -o data.csv\nHowever, this scrapes the data twice and I cannot afford it with big amounts of data.\nA solution that avoids scraping the data twice consists on implementing Pipeline that exports the data (see alecxe suggestion for details). However, as the documentation explains, this is not the preferred way to export data.\nThus, I consider it would be interesting scrappy's support for multiple exporters.\nscrapy crawl <url> -t n3 -o data.n3 -t csv -o data.csv", "issue_status": "Open", "issue_reporting_time": "2015-07-03T15:12:39Z"}, "354": {"issue_url": "https://github.com/scrapy/scrapy/issues/1330", "issue_id": "#1330", "issue_summary": "when scrapy run to (scrapy\\downloadermiddlewares\\)retry.py appear \" UnicodeDecodeError\"", "issue_description": "movingheart commented on Jul 1, 2015\nSome error message as:\n2015-07-01 16:03:46+0800 [Launcher,7500/stderr] Traceback (most recent call last\n):\nFile \"D:\\Python27\\lib\\logging__init__.py\", line 882, in emit\nstream.write(fs % msg.encode(\"UTF-8\"))\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 267:\ninvalid continuation byte\nLogged from file retry.py, line 73\nTraceback (most recent call last):\nFile \"D:\\Python27\\lib\\logging__init__.py\", line 882, in emit\nstream.write(fs % msg.encode(\"UTF-8\"))\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 267:\ninvalid continuation byte\nLogged from file retry.py, line 73\nTraceback (most recent call last):\nFile \"D:\\Python27\\lib\\logging__init__.py\", line 882, in emit\nstream.write(fs % msg.encode(\"UTF-8\"))", "issue_status": "Open", "issue_reporting_time": "2015-07-01T08:30:19Z"}, "355": {"issue_url": "https://github.com/scrapy/scrapy/issues/1312", "issue_id": "#1312", "issue_summary": "Control download due to response's mime type.", "issue_description": "sardok commented on Jun 22, 2015\nAn issue raised, in development of a generic crawler which was supposed to follow particular rules for extracting and visiting links as well as collecting some statistics about visited page.\nAs websites in the batch, differ a lot, to each other, the defined rules for link extraction, started to be broken and yielded unwanted results, so to speak returned many false positives.\nThe main issue was about links to implicit binary files. By default, binary files are ignored from the link extractor, however not every file link points to the proper filename and file extension.\nI made an experimental work which introduces a parameter called DENY_CONTENT_TYPE in http agent. The http agent cancels downloading the response if reponse's content type is matched with the one in given DENY_CONTENT_TYPE, you may find this implementation attempt here: https://github.com/sardok/scrapy/commit/cb1d941d8cf0f32b9eaac043a17411920a830f61\nThan, Shane gave the idea about giving a spider, the control of response downloading which would allow spider to cancel download operation if needed. This is an another attempt to fix the issue: https://github.com/sardok/scrapy/commits/download-control-callback . I chose to use signals here as download agent and spider has no direct relation and no easy way of passing information between them.\nWhat is the proper way of doing that and how much re-factoring needed? any other ideas about the matter?\nthanks.", "issue_status": "Open", "issue_reporting_time": "2015-06-22T14:31:52Z"}, "356": {"issue_url": "https://github.com/scrapy/scrapy/issues/1308", "issue_id": "#1308", "issue_summary": "DepthMiddleware is too verbose", "issue_description": "Member\nkmike commented on Jun 16, 2015\nDepthMiddleware prints a message like this\nDEBUG: Ignoring link (depth > 1): http://scrapinghub.com/faq/\nfor each filtered outgoing request. If we e.g. have 30 depth-2 links on each depth-1 page (most of which are duplicate), there are 30 depth-1 pages, and DEPTH_LIMIT is set to 1, then 900 messages will be logged.\nI propose to keep a list of seen filtered URLs and don't log duplicate messages - in practice a lot of filtered URLs are duplicate, so a single URL kept in memory can prevent ~10 unneeded log messages.\nAnother way is to add an option to disable \"Ignoring link\" messages (it should be already possible to disable them using Python logging features though).\nSome numbers for DepthMiddleware on scrapinghub.com crawl\nDEPTH_LIMIT requests sent ignored urls log messages (out of total) unique Ignore messages\n1 28 183 572 (627) 32%\n2 190 608 5458 (5830) 11.1%\n3 563 1015 9480 (10599) 10.7%\n4 1031 1202 10131 (12135) 11.8%\nWith depth=3 there were 10599 log messages in total, and 9480 of those are \"Ignoring link\" messages - enabling DepthMiddleware makes Scrapy 10x verbose with LOG_LEVEL=DEBUG.", "issue_status": "Open", "issue_reporting_time": "2015-06-15T21:05:15Z"}, "357": {"issue_url": "https://github.com/scrapy/scrapy/issues/1306", "issue_id": "#1306", "issue_summary": "Speedup & fix URL parsing", "issue_description": "Member\nkmike commented on Jun 15, 2015 \u2022\nedited\nI profiled a simple Scrapy spider which just downloads pages and follows links extracted using LinkExtractor; it turns out one of the main bottlenecks is urlparse module and our related functions like canonicalize_url or request_fingerprint. They take several times more CPU time than e.g. HTML parsing itself.\nAlso, these functions don't follow modern HTML5 standards - see e.g. #1304\nI wonder if we can fix it all in once by wrapping an actual browser implementation. Some options:\nhttps://code.google.com/p/google-url/\nhttp://mxr.mozilla.org/mozilla-central/source/netwerk/base/nsURLParsers.cpp\nhttp://src.chromium.org/viewvc/chrome/trunk/src/url/\nhttps://github.com/servo/rust-url\nModern tests for URL parsing: https://github.com/w3c/web-platform-tests/tree/master/url", "issue_status": "Open", "issue_reporting_time": "2015-06-15T14:23:48Z"}, "358": {"issue_url": "https://github.com/scrapy/scrapy/issues/1305", "issue_id": "#1305", "issue_summary": "Extensions, spider and settings initialization order", "issue_description": "Contributor\nchekunkov commented on Jun 15, 2015\nCurrent initialization order of spider, settings and all sorts of middlewares and extensions:\nCrawler.__init__ (see https://github.com/scrapy/scrapy/blob/master/scrapy/crawler.py#L32):\nself.spidercls.update_settings(self.settings)\nAll extensions __init__\nself.settings.freeze()\nCrawler.crawl (see https://github.com/scrapy/scrapy/blob/master/scrapy/crawler.py#L70):\nSpider.__init__\nAll downloader middlewares __init__\nAll spider middlewares __init__\nAll pipelines __init__\nIt's not clear - why extensions are initialized during Crawler.__init__ and not in Crawler.crawl? Is it some legacy code untouched from times when it was possible to run several spider through the same set of extensions and middlewares?\nI'm asking this because sometimes I feel like I want to change some crawl settings after spider initialization and initialize middlewares only after that. For example I got request from customer to make it possible to set CLOSESPIDER_TIMEOUT based on passed spider argument. Due to CloseSpider implementation to support this I need to override it, disable default and set custom extension in settings. If initialization order was\n'spider init' -> 'update settings' -> 'settings.freeze' -> 'middlewares init' \nthat task would be as easy as set CLOSESPIDER_TIMEOUT in custom_settings.\nI don't speak about command line usage, -s does work in command line, but spiders often started not via command line - in Scrapy Cloud, ScrapyRT - it's not always possible to set per crawl settings in cases like that. It could also happen that spider has some logic to decide whether or not some setting should be set based on spider arguments - this is also the case when -s doesn't work well.\nBased on above arguments I would like to propose different initialization order:\nCrawler.__init__:\nself.settings = settings.copy()\nCrawler.crawl:\nSpider.__init__\nspider.update_settings(self.settings) - notice that in this case it isn't required for update_settings to be a @classmethod\nself.settings.freeze()\nAll extensions __init__\nAll downloader middlewares __init__\nAll spider middlewares __init__\nAll pipelines __init__\nWhat do you think about this proposal?\nDiscussion on this issue was originally started in #1276 (comment)\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2015-06-15T12:52:40Z"}, "359": {"issue_url": "https://github.com/scrapy/scrapy/issues/1304", "issue_id": "#1304", "issue_summary": "Scrapy should handle \"invalid\" relative URLs better", "issue_description": "Member\nkmike commented on Jun 15, 2015\nCurrently Scrapy can't extract links from http://scrapy.org/ page correctly because urls in page header are relative to a non-existing parent: ../download/, ../doc/, etc. Browsers resolve these links as http://scrapy.org/download/ and http://scrapy.org/doc/, while response.urljoin, urlparse.urljoin and our ink extractors resolve them as http://scrapy.org/../download/, etc. This results in 400 Bad Request responses.\nurlparse.urljoin is not correct (or not modern) here. In the URL Living Standard for browsers it is said:\nIf buffer is \"..\", remove url\u2019s path\u2019s last entry, if any, and then if c is neither \"/\" nor \"\", append the empty string to url\u2019s path.", "issue_status": "Open", "issue_reporting_time": "2015-06-15T12:44:17Z"}, "360": {"issue_url": "https://github.com/scrapy/scrapy/issues/1280", "issue_id": "#1280", "issue_summary": "CrawlerProcess settings have no effect on spider settings if Crawler is passed to crawl method", "issue_description": "Member\nkmike commented on Jun 4, 2015\nCrawlerProcess accepts either Crawler or Spider instances in its crawl method.\nFor spiders settings are merged: CrawlerProcess provides defaults and spider provides overrides.\nBut when a Crawler instance is passed CrawlerProcess settings are not used at all. I think they should be also used as defaults.", "issue_status": "Open", "issue_reporting_time": "2015-06-03T22:29:01Z"}, "361": {"issue_url": "https://github.com/scrapy/scrapy/issues/1241", "issue_id": "#1241", "issue_summary": "Provide a way to propagate an exit code from a Spider", "issue_description": "Contributor\nnramirezuy commented on May 18, 2015\nI think we need a way to propagate the exit code from the Spider.\nBased on: #1231", "issue_status": "Open", "issue_reporting_time": "2015-05-18T17:20:05Z"}, "362": {"issue_url": "https://github.com/scrapy/scrapy/issues/1226", "issue_id": "#1226", "issue_summary": "Support starting spiders from spiders", "issue_description": "Member\nkmike commented on May 13, 2015\nCurrently it is not possible to start a spider from a spider using CrawlerRunner / CrawlerProcess API if scrapy crawl is used - in this case CrawlerProcess is not available to spider, and so user needs to do tricks to prevent global CrawlerProcess from stopping the reactor.\nI think it'd be good to support this use case. It will allow to:\norganize code into multiple spiders;\nuse different Scrapy options in the same process;\nuse separate downloader queues / concurrency limits.\nI'm not sure how to best fix it. Some options:\nmake CrawlerProcess a Crawler attribute;\nprovide Spider.crawl method which starts a new spider.\n//cc @Allactaga\n\ud83d\udc4d 5", "issue_status": "Open", "issue_reporting_time": "2015-05-12T19:57:37Z"}, "363": {"issue_url": "https://github.com/scrapy/scrapy/issues/1225", "issue_id": "#1225", "issue_summary": "dupefilter skips a request when a page is redirected to itself", "issue_description": "Contributor\nimmerrr commented on May 12, 2015\nHit this when trying to run a spider against scrapinghub.com: sometimes it responds with 302 moved permanently to scrapinghub.com. Scheduler agrees and tries to schedule another request for scrapinghub.com, but fails because dupefilter already considers it visited.\nMaybe dupefilter should only add hosts when the response is not a redirect? And when it is, the scheduler should probably remember the original address too, so that all the redirection chain can be marked as visited.\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2015-05-12T18:10:15Z"}, "364": {"issue_url": "https://github.com/scrapy/scrapy/issues/1211", "issue_id": "#1211", "issue_summary": "Allow to override `DefaultSpider` by setting", "issue_description": "Contributor\nrmax commented on May 6, 2015\nIn large projects we use to have a BaseSpider class which is the base class for all spiders. This works well except when using the commands shell and fetch with unknown domains that causes to use the DefaultSpider class as spider instance which may not implement the methods or attributes required by custom extensions or middlewares.\nFor example, suppose we have the following downloader middleware:\nclass MyMiddleware(object):\n    \"\"\"This is my fancy downloader middleware.\"\"\"\n\n    def process_response(self, request, response, spider):\n        # The method `is_error_page` is implemented by default in the\n        # base spider and overridden per-spider basis.\n        if spider.is_error_page(response):\n            raise IgnoreRequest(\"page with error message\")\nThis downloader middleware will cause the commands shell and fetch to fail with domains that do not have a spider in the project, resulting in an exception as follows:\nTraceback (most recent call last):\n  File \"/home/rolando/.virtualenvs/myproject/bin/scrapy\", line 9, in <module>\n    load_entry_point('Scrapy==0.25.1', 'console_scripts', 'scrapy')()\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/scrapy/commands/shell.py\", line 65, in run\n    shell.start(url=url)\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/scrapy/shell.py\", line 44, in start\n    self.fetch(url, spider)\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/scrapy/shell.py\", line 87, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 577, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/home/rolando/.virtualenvs/myproject/local/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py\", line 46, in process_response\n    response = method(request=request, response=response, spider=spider)\n  File \"/home/rolando/Projects/myproject/myproject/downloadermw.py\", line 19, in process_response\n    if spider.is_error_page(response):\nAttributeError: 'DefaultSpider' object has no attribute 'is_error_page'\nSo, given Scrapy already allows to override some components by a settings, like SPIDER_LOADER_CLASS=\"myproject.spiderloader.MySpiderLoader\", it makes sense to allow to override the base spider class by a setting, for example: DEFAULT_SPIDER_CLASS=\"myproject.myspider.MySpider\".", "issue_status": "Open", "issue_reporting_time": "2015-05-06T00:55:12Z"}, "365": {"issue_url": "https://github.com/scrapy/scrapy/issues/1202", "issue_id": "#1202", "issue_summary": "LxmlLinkExtractor doesn't extract links with fragment", "issue_description": "Djayb6 commented on May 3, 2015\nLxmlLinkExtractor calls canonicalize_url with the url only, removing any fragment present in the URL. As AJAX URLs rely on fragments, it would be nice if we could initialize the link extractor with an argument keep_fragment which would be passed to canonicalize_url to make LxmlLinkExtractor handle such URLs.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2015-05-03T00:28:52Z"}, "366": {"issue_url": "https://github.com/scrapy/scrapy/issues/1199", "issue_id": "#1199", "issue_summary": "FilesPipeline: Optionally guess media extension from response headers, if provided.", "issue_description": "ldgarcia commented on May 1, 2015\nI needed to download some autogenerated datasheets (in PDF format), and I decided to go with the FilesPipeline provided within Scrapy.\nHowever, the URL structure didn't follow the assumption of the file_path method in the FilesPipeline class (a well structured filename, i.e. example.pdf):\nHere is a snippet of my spider:\nitem['file_urls'] = []\nitem['file_urls'].append(\"https://www.domain.name/path/path/DatasheetService?control=%3C%3Fxml+version%3D%221.0%22+encoding%3D%22UTF-8%22%3F%3E%3Cpdf_generator_control%3E%3Cmode%3EPDF%3C%2Fmode%3E%3Cpdmsystem%3EPMD%3C%2Fpdmsystem%3E%3Ctemplate_selection+mlfb%3D%22{}%22+system%3D%22PRODIS%22%2F%3E%3Clanguage%3Ees%3C%2Flanguage%3E%3Ccaller%3EMall%3C%2Fcaller%3E%3C%2Fpdf_generator_control%3E\".format(item['code']))\nThis is the code that extracts the media extension from the URL ( in scrapy.contrib.pipeline.files.FilesPipeline):\nmedia_ext = os.path.splitext(url)[1]  # change to request.url after deprecation\nNow, the value of media_ext for my URL is\n.0%22+encoding%3D%22UTF-8%22%3F%3E%3Cpdf_generator_control%3E%3Cmode%3EPDF%3C%2Fmode%3E%3Cpdmsystem%3EPMD%3C%2Fpdmsystem%3E%3Ctemplate_selection+mlfb%3D%22CODE%22+system%3D%22PRODIS%22%2F%3E%3Clanguage%3Ees%3C%2Flanguage%3E%3Ccaller%3EMall%3C%2Fcaller%3E%3C%2Fpdf_generator_control%3E\nWhich is not only an incorrect extension type, but also results (in this particular case) in an IOException (filename too long).\nOne quick way to fix this is appending something like &type=.pdf to the URL while constructing the file_urls list:\nitem['file_urls'] = []\nitem['file_urls'].append(\"https://www.domain.name/path/path/DatasheetService?control=%3C%3Fxml+version%3D%221.0%22+encoding%3D%22UTF-8%22%3F%3E%3Cpdf_generator_control%3E%3Cmode%3EPDF%3C%2Fmode%3E%3Cpdmsystem%3EPMD%3C%2Fpdmsystem%3E%3Ctemplate_selection+mlfb%3D%22{}%22+system%3D%22PRODIS%22%2F%3E%3Clanguage%3Ees%3C%2Flanguage%3E%3Ccaller%3EMall%3C%2Fcaller%3E%3C%2Fpdf_generator_control%3E&type=.pdf\".format(item['code']))\nHowever, I think it would be helpful to introduce some code that (optionally) guesses the extension out of the response headers:\nFirst, it checks the Content-Type header and uses the mimetypes module to guess the extension.\nSecond, if the previous step fails it checks the Content-Disposition header.\nThird, if the previous step fails it uses the standard implementation.\nThis would be useful in the case that the crawler gets a list of URLs by some other means (i.e. using an XPath query).\nHere is my approach, and I'd be happy to open a PR if you like the idea and the implementation.\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2015-04-30T23:51:39Z"}, "367": {"issue_url": "https://github.com/scrapy/scrapy/issues/1192", "issue_id": "#1192", "issue_summary": "UnicodeDecodeError in LxmlLinkExtractor", "issue_description": "Djayb6 commented on Apr 29, 2015\nThe following links triggers a UnicodeDecodeError exception when being extracted by LxmlLinkExtractor:\n<a href=\"http://gostariadefazerinscri\u00e7\u00e3oposcursos,obrigada.\">A link</a>\n<a href=\"http://www.domain.org \">Another link</a>\n>>> 'http://gostariadefazerinscri\u00e7\u00e3oposcursos,obrigada.'\n'http://gostariadefazerinscri\\xc3\\xa7\\xc3\\xa3oposcursos,obrigada.'\n>>> 'http://www.domain.org '\n'http://www.domain.org\\xc2\\xa0'\nHere is the traceback:\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/contrib/spidermiddleware/depth.py\", line 50, in <genexpr>\n return (r for r in result or () if _filter(r))\n\nFile \"build/bdist.macosx-10.10-x86_64/egg/Spider/spiders/spider.py\", line 271, in parse_response        \n\nFile \"build/bdist.macosx-10.10-x86_64/egg/Spider/spiders/spider.py\", line 293, in extract_requests_from_response\n\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/contrib/linkextractors/lxmlhtml.py\", line 108, in extract_links\n    all_links.extend(self._process_links(links))\n\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/linkextractor.py\", line 86, in _process_links\n    links = [x for x in links if self._link_allowed(x)]\n\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/linkextractor.py\", line 66, in _link_allowed\n    if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains):\n\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/url.py\", line 23, in url_is_from_any_domain\n    return any(((host == d.lower()) or (host.endswith('.%s' % d.lower())) for d in domains))\n\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/url.py\", line 23, in <genexpr>\n    return any(((host == d.lower()) or (host.endswith('.%s' % d.lower())) for d in domains))\n\nexceptions.UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)", "issue_status": "Open", "issue_reporting_time": "2015-04-28T21:21:57Z"}, "368": {"issue_url": "https://github.com/scrapy/scrapy/issues/1184", "issue_id": "#1184", "issue_summary": "Break down the tutorial, present new topics more gradually", "issue_description": "Member\neliasdorneles commented on Apr 22, 2015\nThe idea is to apply the ideas discussed here: #1180 (comment)\nSo, I'll be working on:\nprefer start_requests over start_urls\nbuild up on the content in overview, start from a single spider yielding dict items, progress to a Scrapy project with the same spider, and then add another spider to the same project using defined Items and crawl features\nsync with code in dirbot project", "issue_status": "Open", "issue_reporting_time": "2015-04-21T19:14:06Z"}, "369": {"issue_url": "https://github.com/scrapy/scrapy/issues/1179", "issue_id": "#1179", "issue_summary": "Scrapy form_response includes parameters not in form", "issue_description": "CuriousG102 commented on Apr 20, 2015\nTo reproduce:\nOpen scrapy shell\nfetch('http://utdirect.utexas.edu/ctl/ecis/results/index.WBX?s_in_page_isn=648311&s_in_page_query=Quesada+Gonzalez%2C+Carlos+20139MUS201M&s_in_max_nbr_return=0&s_in_search_query=Q&s_in_search_type_sw=N&s_in_page_direction=B&s_in_action_sw=P&s_in_search_name=Q')\nscrapy.http.FormRequest.from_response(response, formxpath='//div[@Class=\"page-forward\"]/form[1]')\nCompare get url to that produced when clicking the button in a browser.", "issue_status": "Open", "issue_reporting_time": "2015-04-19T20:43:52Z"}, "370": {"issue_url": "https://github.com/scrapy/scrapy/issues/1163", "issue_id": "#1163", "issue_summary": "FormRequest should raise an error instead of submitting a first form if formname doesn't match anything", "issue_description": "Member\nkmike commented on Apr 15, 2015\nI think this test is weird:\nscrapy/tests/test_http_request.py\nLine 511 in 526aa07\n def test_from_response_formname_notexist(self): \nThis test shows a more sane behaviour:\nscrapy/tests/test_http_request.py\nLine 655 in 526aa07\n def test_from_response_xpath(self): \nSee also: #1137", "issue_status": "Open", "issue_reporting_time": "2015-04-15T11:21:43Z"}, "371": {"issue_url": "https://github.com/scrapy/scrapy/issues/1158", "issue_id": "#1158", "issue_summary": "Why not apply #546 to FEED_URI / FEED_FORMAT?", "issue_description": "umrashrf commented on Apr 15, 2015\n#546", "issue_status": "Open", "issue_reporting_time": "2015-04-14T20:08:44Z"}, "372": {"issue_url": "https://github.com/scrapy/scrapy/issues/1157", "issue_id": "#1157", "issue_summary": "Don't require 'name' attribute for scrapy.Spider", "issue_description": "Member\nkmike commented on Apr 15, 2015\nI think we should make Spider.name attribute optional. The name is used by SpiderManager to find spiders, but Spider can be used without a Scrapy project. It is unnecessary boilerplate for users of runspider command or for CrawlerRunner / CrawlerProcess users.\nWe can also provide a default value, e.g. self.__class__.__name__, to help with discovery; this have an advantage of 1-to-1 mapping between spider class names and names printed to users - spiders can become easier to find.\nOpinions?\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2015-04-14T18:51:27Z"}, "373": {"issue_url": "https://github.com/scrapy/scrapy/issues/1144", "issue_id": "#1144", "issue_summary": "Integrate darkrho/scrapy-inline-requests", "issue_description": "Member\ncurita commented on Apr 10, 2015\nI think we haven't created a ticket for this yet, we discussed integrating https://github.com/darkrho/scrapy-inline-requests a couple of weeks ago with @pablohoffman, @kmike and @nramirezuy.\nI'm copying the whole discussion here if anyone else wants to join in:\nFrom Pablo:\nI've heard many people using (and speaking good things about) inline requests recently. Would you consider it a feature to include in 1.0?\nFrom Mikhail:\nI'd like to have something like inline-requests builtin; it even was one of the draft ideas for 2014 GSoC :) +1 to add it to Scrapy 1.0 if we have time for that.\nPast me wrote the following in one of the emails:\nIt needs a bit more thought before becoming a part of Scrapy: there are no tests, I'm not fan of how callbacks are handled, and downsides of the 'yield' approach should be clearly documented - e.g. state inside the callback lives longer, and it could lead to increased memory usage; it is also unclear how does it work with on-disk queues. There are also some useful features that present in other alike libraries (e.g. adisp) but not in scrapy-inline-requests - for example, waiting for several requests to be executed in parallel: syntax could be resp1, resp2 = yield Request(url1), Request(url2)\nFrom Nicol\u00e1s:\nI like the idea behind inline requests, but not the API of it. It kinda doesn't fit the callback approach since it doesn't work with a callback and you have to manage several requests with in a callback.\nI would prefer to see something like:\ndef callback:\nreturn chain_requests(request1, request2, request3)\nand the callbacks handled normally.\nFrom Mikhail:\nNicol\u00e1s: I think the point of inline-requests is to allow writing code without callbacks and handle several related requests in a single function :) It is a common trick to \"linearize\" callbacks into a generator.\nCallbacks + CPython reference counting (no PyPy) provide a nice approach to resource deallocation: if a variable is not referenced from outside then it is deallocated as soon as the callback exits, without invoking garbage collector. With generators if user writes \"response1 = yield ...; response2 = yield ...\" then these responses are kept alive, possibly for long. Even with \"response = yield ...; response = yield ...\" response is kept in memory longer than needed (if I'm not mistaken, until the second request finishes). One can write \"del response\", but it'd be nice to have some clever solution for that.\nFrom Julia:\nI wouldn't promote it as the preferred way for dealing with requests/responses because of the already mention issues. It's not as flexible as using explicit callbacks (we should document that yielding a request with callback not being None breaks the chain btw) and it's hacky, debugging it is kind of hard.\nStill, it's a really good helper for its primary use-case of downloading some additional page and handling errors (as opposed to using errbacks or downloading the page with another library) so I'd also like to include it in Scrapy.\nNOTE: I didn't mean that I breaks the chain as in raising an exception, just that it won't wait for yielded requests if they have callbacks.\nFrom Pablo:\nI think we're pretty much in agreement that it would be a nice feature for 1.0 (well, if we're not gonna have python 3 ... :). It needs to go with good documentation (explaining the downsides), tests and better error check (raising exception if it's used with a request having a callback).\nShall we make a ticket for this?. I think there's already enough content in this thread for one :).\n/cc @darkrho", "issue_status": "Open", "issue_reporting_time": "2015-04-10T11:50:01Z"}, "374": {"issue_url": "https://github.com/scrapy/scrapy/issues/1133", "issue_id": "#1133", "issue_summary": "Scrapy doesn't normalize URLs returned in redirects which leads to Bad Requests (400)", "issue_description": "wszak commented on Apr 3, 2015\nWhen some site wants to make a redirect and returns a URL like:\nhttp://www.example.com?test=1\nScrapy follows the redirect as is, which results in HTTP error 400 (Bad Request).\nThis is because such URL translates to the following line in the HTTP request:\nGET ?test=1 HTTP/1.1\nObviously it's wrong because it doesn't have the path component so most HTTP servers return error 400.\nThe correct way to handle such an address would be to normalize it to the following form:\nhttp://www.example.com/?test=1\nAnd then the request line would look like this:\nGET /?test=1 HTTP/1.1\nIn 'normal' browsers it works fine - they normalize such incorrect URLs.\nscrapy version -v\nScrapy : 0.24.4\nlxml : 3.3.3.0\nlibxml2 : 2.9.1\nTwisted : 14.0.2\nPython : 2.7.6 (default, Mar 22 2014, 22:59:56) - [GCC 4.8.2]\nPlatform: Linux-3.13.0-24-generic-x86_64\n\u2764\ufe0f 1", "issue_status": "Open", "issue_reporting_time": "2015-04-03T08:59:06Z"}, "375": {"issue_url": "https://github.com/scrapy/scrapy/issues/1113", "issue_id": "#1113", "issue_summary": "canonicalize url - de-duplicate args (avoid some infinite loops)", "issue_description": "jvanasco commented on Mar 28, 2015\ndisclaimer: I'm not sure if this applies to scrapy as a whole. i just use a few library calls in another spider.\nI've been running into some infinite redirect loops (mostly from googledocs and livejournal) in which the other system generates a garbage canonical url by appending some query string data.\ngoogle docs appends another &usp=embed_facebook arg on every request\nlivejournal appends and empty &amp= arg to their login redirect (keep_fragments=False gets around this)\ni've been able to get around both cases by de-duplicating the query string arguments with a lazy one-liner:\n   keyvals = parse_qsl(query, keep_blank_values)\n+ keyvals = list(set(keyvals))\n   keyvals.sort()", "issue_status": "Open", "issue_reporting_time": "2015-03-28T01:36:45Z"}, "376": {"issue_url": "https://github.com/scrapy/scrapy/issues/1097", "issue_id": "#1097", "issue_summary": "dynamic project configuration through scrapy cmdline (idea)", "issue_description": "Contributor\nnyov commented on Mar 24, 2015\nHow about supporting a scrapy config command akin to git config for dynamic configuration updates inside a project folder?\nThis comes to mind after reading the various tries at enhancing the startproject generated settings.py and all the templating stuff (e.g. #665, #580).\nI feel such an approach to provide hopefully useful (commented) defaults won't ever satisfy everyone; for insiders it's too verbose, for newcomers it may be too brief.\nInstead we could have a command which displays all applicable settings (depending on existing extensions), with their default values or currently set values, with the ability to filter by giving a prefix.\nThe question of course being, if the gain of having it is bigger than the pain of implementing, or if it'd just be a fancy way to add code bloat to scrapy.\nSuch a command should have the ability to\nlist all default settings\nlist all currently set settings (if not default?)\nfetch specific settings (default or current)\nset/update/override a setting\npull in/set a whole list of settings (from our personal settings library somewhere)\n(give a description/explanation to a specific setting, when queried)\nMy favorite mailerdaemon, postfix, has a similar way to handle config values:\npostconf [parameter] - list current values (or value to key if given)\npostconf -d [parameter] - list default values instead\npostconf -e param=value - edit config file, set value (could start an editor if no param given)\n... and so on.\nDynamic configuration settings probably wouldn't be as easy as if we had an ini-style configuration, but maybe it's achievable.\nOr it might require refactoring all settings into a kind of project-global k/v-store instead (or ini-style), where enabled extensions/modules register their provided settings and defaults.\nThis dynamic config store could be an import in settings.py (from scrapy.dynconf.values import *); and people might keep using settings.py for advanced logic (import local_settings; custom functions) that can't be achieved with an ini-style config.\nWithout such a refactoring, git config for example can handle something called \"multivars\", which might be a way to manipulate actual lists or dicts in our settings directly (from the git-config manpage):\n; Proxy settings\n[core]\n    gitproxy=proxy-command for kernel.org\n    gitproxy=default-proxy ; for all the rest\n\nTo query a multivar:\n% git config --get core.gitproxy \"for kernel.org$\"\n\nIf you want to know all the values for a multivar, do:\n% git config --get-all core.gitproxy\n\nIf you like to live dangerously, you can replace all core.gitproxy by a new one with\n% git config --replace-all core.gitproxy ssh\n\nHowever, if you really only want to replace the line for the default proxy, i.e. the one without a \"for ...\" postfix, do something like this:\n% git config core.gitproxy ssh '! for '\n\nTo actually match only values with an exclamation mark, you have to\n% git config section.key value '[!]'\n\nTo add a new proxy, without altering any of the existing ones, use\n% git config --add core.gitproxy '\"proxy-command\" for example.com'\nThough I believe flattening all config names to plain_list_names_with_module_prefix instead (and rewriting them to dicts/lists, such as the ITEM_PIPELINES, after parsing) would be less painful in the long run there, than \"multivars\".\n...Additionally, all this might provide a starting point for supporting live-reconfiguration of a running crawler (using events on setting changes).\nThe same functionality might then be addable to the telnet or webservice consoles (remote config).", "issue_status": "Open", "issue_reporting_time": "2015-03-24T00:50:10Z"}, "377": {"issue_url": "https://github.com/scrapy/scrapy/issues/1042", "issue_id": "#1042", "issue_summary": "Redirected responses don't follow allowed_domains", "issue_description": "tarunlalwani commented on Feb 7, 2015\nHi,\nIf we add allowed_domains and a url from the same domain gets redirected to another domain, then the results for that domain also get processed, which I believe should be filtered\nallowed_domains = ('www.knowledgeinbox.com', )\nurl = \"http://www.knowledgeinbox.com/books/\"\nThe url redirects to www.tarlabs.com. The result is still passed to my function which is only interested in knowledgeinbox.com. Not sure if this intentional, as per my understanding its a bug\nPlease let me know your thoughts\nRegards,\nTarun\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2015-02-07T06:37:36Z"}, "378": {"issue_url": "https://github.com/scrapy/scrapy/issues/1039", "issue_id": "#1039", "issue_summary": "Illegal character (<,>,&) in HTML cause xpath extracted value to be empty", "issue_description": "markbaas commented on Feb 6, 2015\nAs Scrapy is using lxml as xml parser. However, as lxml is an xml parser, characters as <, >, etc are invalid, and then by lxml stripped away. Nevertheless, many website use < and > as less and greater then symbols.\nI propose to implement a fix that quote specifically those characters.", "issue_status": "Open", "issue_reporting_time": "2015-02-06T15:30:57Z"}, "379": {"issue_url": "https://github.com/scrapy/scrapy/issues/1008", "issue_id": "#1008", "issue_summary": "Non-ascii field names", "issue_description": "Vanuan commented on Jan 12, 2015\nIn Python 2 variable names are limited by ascii character set.\nCSV Scrapy exporter uses Item's field names as an csv title row.\nI want to use unicode characters for csv title row.\nCan I do that without writing my own exporter?", "issue_status": "Open", "issue_reporting_time": "2015-01-12T00:40:01Z"}, "380": {"issue_url": "https://github.com/scrapy/scrapy/issues/998", "issue_id": "#998", "issue_summary": "HTML entity causes UnicodeEncodeError in LxmlLinkExtractor", "issue_description": "fgpietersz commented on Dec 30, 2014\nA page containing &#9001; causes with error in LxmlLinkExtractor.link_extractor()\n          File \"/[path]/spiders/search_spider.py\", line 112, in parse\n            self.link_extractor.extract_links(response) if\n          File \"/[path to virtualenv]/local/lib/python2.7/site-packages/scrapy/contrib/linkextractors/lxmlhtml.py\", line 107, in extract_links\n            links = self._extract_links(doc, response.url, response.encoding, base_url)\n          File \"/[path to virtualenv]/local/lib/python2.7/site-packages/scrapy/linkextractor.py\", line 94, in _extract_links\n            return self.link_extractor._extract_links(*args, **kwargs)\n          File \"/[path to virtualenv]/local/lib/python2.7/site-packages/scrapy/contrib/linkextractors/lxmlhtml.py\", line 57, in _extract_links\n            url = url.encode(response_encoding)\n          File \"/[path to virtualenv]/lib/python2.7/encodings/cp1252.py\", line 12, in encode\n            return codecs.charmap_encode(input,errors,encoding_table)\n        exceptions.UnicodeEncodeError: 'charmap' codec can't encode character u'\\u2329' in position 87: character maps to <undefined>\nThe encoding is correct and works for displaying the page in a browser, but fails as above in Scrapy.", "issue_status": "Open", "issue_reporting_time": "2014-12-30T08:31:18Z"}, "381": {"issue_url": "https://github.com/scrapy/scrapy/issues/988", "issue_id": "#988", "issue_summary": "A clock signal.", "issue_description": "sardok commented on Dec 18, 2014\nHi everyone,\nSometimes i find myself, need for my spider's callback is called on a particular interval. This functionality could be useful, for example if spider is maintaining list of requests or items to be send when a limit is reached. What do you think about adding such a clock ticker signal:\nfrom time import time\nfrom twisted.internet import task\nfrom scrapy import log\nfrom scrapy.http import Request\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom scrapy.utils.signal import send_catch_log\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors import LinkExtractor\n\n\nclass ClockSignal(object):\n    def __init__(self, interval=1.0):\n        self.ts = time()\n        self.loop = task.LoopingCall(self.callback)\n        self.loop.start(interval)\n\n    def __call__(self, interval):\n        self.loop.stop()\n        self.loop.start(interval)\n\n    def __del__(self):\n        self.loop.stop()\n        return super(ClockSignal, self).__del__()\n\n    def callback(self):\n        now = time()\n        delta = round(now - self.ts)\n        send_catch_log(self, self, delta=delta)\n        self.ts = now\n\n\nclass ExampleSpider(CrawlSpider):\n    name = \"example\"\n    allowed_domains = [\"scrapinghub.com\"]\n    start_urls = (\n        'http://www.scrapinghub.com/',\n    )\n    rules = [Rule(LinkExtractor(), callback='parse', follow=True)]\n\n    def __init__(self, *a, **kw):\n        super(ExampleSpider, self).__init__(*a, **kw)\n        clock_signal_tick_second = ClockSignal()\n        dispatcher.connect(self.clock_signal, clock_signal_tick_second)\n\n    def clock_signal(self, delta):\n        log.msg('Clock tics (delta: %s).' % delta, level=log.INFO)\nSnippet from the output:\n2014-12-17 21:30:13+0200 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-12-17 21:30:13+0200 [scrapy] INFO: Enabled item pipelines: \n2014-12-17 21:30:13+0200 [example] INFO: Spider opened\n2014-12-17 21:30:13+0200 [example] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\nSTart urls: 'http://www.scrapinghub.com/'\nMaking request of url: http://www.scrapinghub.com/\n2014-12-17 21:30:14+0200 [scrapy] INFO: Clock tics (delta: 1.0).\n2014-12-17 21:30:15+0200 [scrapy] INFO: Clock tics (delta: 1.0).\n2014-12-17 21:30:16+0200 [scrapy] INFO: Clock tics (delta: 1.0).\n2014-12-17 21:30:17+0200 [scrapy] INFO: Clock tics (delta: 1.0).\n2014-12-17 21:30:18+0200 [scrapy] INFO: Clock tics (delta: 1.0).\nThis custom signal has some drawbacks,\nOnce instantiated, it keeps running until it is destroyed, even though there is no receiver.\nRaising exception meaningful to engine or yielding deferreds as most of other signals do, is not possible with it.", "issue_status": "Open", "issue_reporting_time": "2014-12-17T19:44:39Z"}, "382": {"issue_url": "https://github.com/scrapy/scrapy/issues/973", "issue_id": "#973", "issue_summary": "[Enhancement] Allow BaseItemExporter to accept arguments", "issue_description": "italomaia commented on Dec 8, 2014\nIt would be nice if BaseItemExporter could accept command line arguments. Imagine the scenario:\nyou have a item exporter using mongo and you want your spider to be written to a specific collection. How do you do that? Hard code? Hack sys.args?", "issue_status": "Open", "issue_reporting_time": "2014-12-07T20:04:35Z"}, "383": {"issue_url": "https://github.com/scrapy/scrapy/issues/965", "issue_id": "#965", "issue_summary": "Sitemap spider support for URLs beginning with double slash", "issue_description": "apogre commented on Dec 5, 2014\nSitemap spider fails with urls starting with double slash:\nraise ValueError('Missing scheme in request url: %s' % self._url)\nexceptions.ValueError: Missing scheme in request url: //www.example.com/abc", "issue_status": "Open", "issue_reporting_time": "2014-12-05T02:24:27Z"}, "384": {"issue_url": "https://github.com/scrapy/scrapy/issues/960", "issue_id": "#960", "issue_summary": "S3 Feed Export throws boto error: \"Connection Reset By Peer\"", "issue_description": "samzhang111 commented on Nov 25, 2014\nPosted details on SO: http://stackoverflow.com/questions/27131693/scrapyd-s3-feed-export-connection-reset-by-peer", "issue_status": "Open", "issue_reporting_time": "2014-11-25T16:22:15Z"}, "385": {"issue_url": "https://github.com/scrapy/scrapy/issues/955", "issue_id": "#955", "issue_summary": "fail to output result to local file on windows", "issue_description": "exitNA commented on Nov 23, 2014\nin setting.py I set feed_uri as:\nFEED_URI = 'file://' + os.getcwd() + '/xxxSpider/res/%(name)s_%(time)s.xml'\nthe output log is:\nINFO: Stored xml feed (14 items) in: file://E:\\work\\xxx_spider/xxxSpider/res/xxx_2014-11-23T09-27-07.xml\nbut there is no file on directory \"E:\\work\\xxx_spider/xxxSpider/res/\", instead, the \"xxx_2014-11-23T09-27-07.xml\" stored in \"E:\\dygodSpider\\res\" which is confusing me.", "issue_status": "Open", "issue_reporting_time": "2014-11-23T09:37:14Z"}, "386": {"issue_url": "https://github.com/scrapy/scrapy/issues/939", "issue_id": "#939", "issue_summary": "Media pipeline problem", "issue_description": "pczhaoyun commented on Nov 5, 2014\nI think contrib.pipline.media.MediaPipeline should be add config to disable downloaded cache, when I use ImagePipeline to scrapy full site image, alway make memory leak. The media download should be make unique by user not scrapy framwork.", "issue_status": "Open", "issue_reporting_time": "2014-11-05T05:21:26Z"}, "387": {"issue_url": "https://github.com/scrapy/scrapy/issues/905", "issue_id": "#905", "issue_summary": "Improve DNS caching", "issue_description": "Contributor\nandrix commented on Sep 25, 2014 \u2022\nedited by redapple\nCurrently, Scrapy does caching of DNS by default (setting DNSCACHE_ENABLED is True on default settings).\nThe current cache implementation does not implement an expiration for the entries which may result in entries that be kept ad infinitum. We need change the current implementation to set an expiration time by default (typical TTL is 86400 seconds -24 hours-) or maybe, customizable by a setting.", "issue_status": "Open", "issue_reporting_time": "2014-09-25T14:04:17Z"}, "388": {"issue_url": "https://github.com/scrapy/scrapy/issues/900", "issue_id": "#900", "issue_summary": "Centralized Request fingerprints", "issue_description": "Member\nkmike commented on Sep 23, 2014\nIt is very easy to have a subtle bug when using a custom duplicates filter that changes how request fingerprint is calculated.\nDuplicate filter checks request fingerprint and makes Scheduler drop the request if it is a duplicate.\nCache storage checks request fingerprint and fetches response from cache if it is a duplicate.\nIf fingerprint algorithms differ we're in trouble.\nThe problem is that there is no way to override request fingerprint globally; to make Scrapy always take something extra in account (an http header, a meta option) user must override duplicates filter and all cache storages that are in use.\nIdeas about how to fix it:\nUse duplicates filter request_fingerprint method in cache storage if this method is available;\ncreate a special Request.meta key that request_fingerprint function will take into account;\ncreate a special Request.meta key that will allow to provide a pre-calculated fingerprint;\nadd a settings.py option to override request fingerprint function globally.", "issue_status": "Open", "issue_reporting_time": "2014-09-23T17:32:23Z"}, "389": {"issue_url": "https://github.com/scrapy/scrapy/issues/899", "issue_id": "#899", "issue_summary": "Exceptions raised in downloader middleware are quietly suppressed", "issue_description": "Member\nkmike commented on Sep 23, 2014\nThis is very similar to #496.", "issue_status": "Open", "issue_reporting_time": "2014-09-23T01:24:42Z"}, "390": {"issue_url": "https://github.com/scrapy/scrapy/issues/892", "issue_id": "#892", "issue_summary": "Crawl-Delay support for robots.txt", "issue_description": "Member\nkmike commented on Sep 20, 2014\nCrawl-Delay directive in robots.txt looks useful. If it is present the delay suggested there looks like a good way to adjust crawling rate. When crawling unknown domains it can be better than autothrottle.\nA draft implementation: https://gist.github.com/kmike/76aca46cad18915b8695\nSome notes:\nit may be useful to respect only Crawl-Delay but not User-Agent rules, so this middleware is not inherited from an existing middleware;\nCrawl-Delay maps nicely to Scrapy downloader slots; it is possible to implement this feature without per-request download delays (see #802) for which semantics are unclear;\ncurrently RobotsCrawlDelayMiddleware doesn't use User-Agent because UA set by Scrapy is quite arbitrary, but I'm not sure this is a right thing to do;\ninstead of stdlib robotparser https://github.com/seomoz/reppy is used; robotparser can't parse robots.txt with Crawl-Delay properly.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2014-09-20T03:39:58Z"}, "391": {"issue_url": "https://github.com/scrapy/scrapy/issues/891", "issue_id": "#891", "issue_summary": "DepthMiddleware should allow \"resetting\" depth", "issue_description": "Member\nkmike commented on Sep 20, 2014\nSometimes it is helpful to \"reset\" depth, e.g. when going to an external domain. It seems this is not possible with existing DepthMiddleware because it increases response.meta['depth'] + 1 unconditionally.\nIt seems that to fix it DepthMiddleware can be changed to look at request.meta['depth'] first.\nWhy was this change made? //cc @dangra", "issue_status": "Open", "issue_reporting_time": "2014-09-20T03:23:27Z"}, "392": {"issue_url": "https://github.com/scrapy/scrapy/issues/889", "issue_id": "#889", "issue_summary": "Scrapy store images to amazon s3, 403 Forbidden.", "issue_description": "DynamicU commented on Sep 18, 2014\nThe problem is described here: http://stackoverflow.com/questions/25777770/scrapy-store-images-to-amazon-s3. A candidate solution is remove policy argument from set_contents_from_string(). I am curious about why adding \"public-read\" policy as default?\nmodification in scrapy/contrib/pipeline/files.py,\nfrom\nreturn threads.deferToThread(k.set_contents_from_string, buf.getvalue(),\nheaders=h, policy=self.POLICY)\nto\nreturn threads.deferToThread(k.set_contents_from_string, buf.getvalue(),\nheaders=h)", "issue_status": "Open", "issue_reporting_time": "2014-09-18T05:03:16Z"}, "393": {"issue_url": "https://github.com/scrapy/scrapy/issues/869", "issue_id": "#869", "issue_summary": "(Feature Request) Throttling number of spiders running concurrently in the same process", "issue_description": "madvas commented on Aug 30, 2014\nIt would be nice to be able to throttle number of spiders running concurrently from the script as in a docs in \"Common Practices\" (http://doc.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process)\nI was doing spider, which started them few hundreds at once, it didn't work, logging didn't even started.\nIt would be nice if we could easily set this number.\nThank you very much,\nYou're doing great job guys!", "issue_status": "Open", "issue_reporting_time": "2014-08-30T13:52:41Z"}, "394": {"issue_url": "https://github.com/scrapy/scrapy/issues/863", "issue_id": "#863", "issue_summary": "`bindaddress` parameter fails for IPv6", "issue_description": "Contributor\nMimino666 commented on Aug 26, 2014\nTrying to pass IPv6 address as bindaddress parameter, for example:\nRequest('http://whatismyv6.com/', meta={'bindaddress': ('1234:5678:111::0a', 0)})\nraises the following error:\nTypeError: getsockaddrarg() takes exactly 2 arguments (4 given)", "issue_status": "Open", "issue_reporting_time": "2014-08-26T13:00:46Z"}, "395": {"issue_url": "https://github.com/scrapy/scrapy/issues/858", "issue_id": "#858", "issue_summary": "Selector Context", "issue_description": "Contributor\nnramirezuy commented on Aug 15, 2014\nSince #818 I been thinking on alternatives.\nI'm thinking that Selectors should be able to handle Context is some way.\nThe one that felt more clear to me was a decorator to be used on functions expecting the argument selector, if selector isn't passed on the call and there is an open context this is filled by the decorator with that result selector.\nhttps://gist.github.com/nramirezuy/7a603cbc5cd381734df4\nFast test:\n>>> from scrapy.http import HtmlResponse\n>>> from scrapy.selector import Selector\n>>> from scrapy.selector.unified import selectorcontext\n>>> response = HtmlResponse('http://example.com', body='<div id=\"depth1\">DEPTH 1<div id=\"depth2\">DEPTH 2</div></div>')\n>>> selector = Selector(response)\n>>> @selectorcontext\n... def show_xpath(selector=None):\n...     print selector\n... \n>>> with selector.xpath('//div[@id=\"depth1\"]'):\n...     show_xpath()\n...     with selector.xpath('.//div[@id=\"depth2\"]'):\n...         show_xpath()\n...     with selector.xpath('//div[@id=\"depth2\"]'):\n...         show_xpath()\n...     show_xpath()\n... \n[<Selector xpath='//div[@id=\"depth1\"]' data=u'<div id=\"depth1\">DEPTH 1<div id=\"depth2\"'>]\n[<Selector xpath='.//div[@id=\"depth2\"]' data=u'<div id=\"depth2\">DEPTH 2</div>'>]\n[<Selector xpath='//div[@id=\"depth2\"]' data=u'<div id=\"depth2\">DEPTH 2</div>'>]\n[<Selector xpath='//div[@id=\"depth1\"]' data=u'<div id=\"depth1\">DEPTH 1<div id=\"depth2\"'>]", "issue_status": "Open", "issue_reporting_time": "2014-08-14T22:18:30Z"}, "396": {"issue_url": "https://github.com/scrapy/scrapy/issues/851", "issue_id": "#851", "issue_summary": "HTTP Basic Auth in URL as parameter to Scrapy shell apparently triggers a \"DNS lookup error\"", "issue_description": "fconcklin commented on Aug 11, 2014\n$ scrapy shell \"http://user@pass:test.com/path/\"\nMaybe this needs to be performed using the special Http Auth middleware, however it looks like the scrapy shell command doesn't enable this middleware by default.\nversion: 0.24.4\nLet me know if you need anymore doc to reproduce.", "issue_status": "Open", "issue_reporting_time": "2014-08-11T09:18:29Z"}, "397": {"issue_url": "https://github.com/scrapy/scrapy/issues/845", "issue_id": "#845", "issue_summary": "Spider Unhandled Error : f.seek(-size-self.SIZE_SIZE, os.SEEK_END) exceptions.IOError", "issue_description": "fpassaniti commented on Aug 8, 2014\nThe spider was working correctly until this :\n2014-08-08 11:55:01+0200 [seloger] DEBUG: Scraped from <200 http://www.seloger.com/annonces/achat/maison/nanteau-sur-essonne-77/91520225.htm?bd=Detail_Nav&div=2238&idtt=2&idtypebien=all&tri=d_dt_crea>\n    {'area': u'Nanteau sur Essonne',\n     'id': '91520225',\n     'nbroom': u'2',\n     'nbsleepingroom': u'1',\n     'phone': u'02 38 34 87 48',\n     'price': u'88000',\n     'source': 'seloger',\n     'surface': u'18',\n     'text': u\"5'de malesherbes, espace et libert\\xe9, c'est la sensation que vous \\xe9prouverez en visitant ce terrain de loisirs de 2236 m\\xb2 avec chalet et garage ferm\\xe9, en bordure de rivi\\xe8re dans un cadre exceptionnel ! \\xc0 saisir ! Classe \\xe9nergie: vierge.\",\n     'title': u'Maison Nanteau Sur Essonne 2 pi\\xe8ce (s) 18 m\\xb2',\n     'type': 'maison',\n     'url': u'http://www.seloger.com/annonces/achat/maison/nanteau-sur-essonne-77/91520225.htm?p=CCCPqX4AAKvgYyNg'}\n2014-08-08 11:55:01+0200 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 93, in start\n        self.start_reactor()\n      File \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 130, in start_reactor\n        reactor.run(installSignalHandlers=False)  # blocking call\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/Library/Python/2.7/site-packages/scrapy/utils/reactor.py\", line 41, in __call__\n        return self._func(*self._a, **self._kw)\n      File \"/Library/Python/2.7/site-packages/scrapy/core/engine.py\", line 107, in _next_request\n        if not self._next_request_from_scheduler(spider):\n      File \"/Library/Python/2.7/site-packages/scrapy/core/engine.py\", line 134, in _next_request_from_scheduler\n        request = slot.scheduler.next_request()\n      File \"/Library/Python/2.7/site-packages/scrapy/core/scheduler.py\", line 64, in next_request\n        request = self._dqpop()\n      File \"/Library/Python/2.7/site-packages/scrapy/core/scheduler.py\", line 94, in _dqpop\n        d = self.dqs.pop()\n      File \"/Library/Python/2.7/site-packages/queuelib/pqueue.py\", line 43, in pop\n        m = q.pop()\n      File \"/Library/Python/2.7/site-packages/scrapy/squeue.py\", line 18, in pop\n        s = super(SerializableQueue, self).pop()\n      File \"/Library/Python/2.7/site-packages/queuelib/queue.py\", line 157, in pop\n        self.f.seek(-size-self.SIZE_SIZE, os.SEEK_END)\n    exceptions.IOError: [Errno 22] Invalid argument\n\n2014-08-08 11:55:01+0200 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 93, in start\n        self.start_reactor()\n      File \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 130, in start_reactor\n        reactor.run(installSignalHandlers=False)  # blocking call\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/Library/Python/2.7/site-packages/scrapy/utils/reactor.py\", line 41, in __call__\n        return self._func(*self._a, **self._kw)\n      File \"/Library/Python/2.7/site-packages/scrapy/core/engine.py\", line 107, in _next_request\n        if not self._next_request_from_scheduler(spider):\n      File \"/Library/Python/2.7/site-packages/scrapy/core/engine.py\", line 134, in _next_request_from_scheduler\n        request = slot.scheduler.next_request()\n      File \"/Library/Python/2.7/site-packages/scrapy/core/scheduler.py\", line 64, in next_request\n        request = self._dqpop()\n      File \"/Library/Python/2.7/site-packages/scrapy/core/scheduler.py\", line 94, in _dqpop\n        d = self.dqs.pop()\n      File \"/Library/Python/2.7/site-packages/queuelib/pqueue.py\", line 43, in pop\n        m = q.pop()\n      File \"/Library/Python/2.7/site-packages/scrapy/squeue.py\", line 18, in pop\n        s = super(SerializableQueue, self).pop()\n      File \"/Library/Python/2.7/site-packages/queuelib/queue.py\", line 157, in pop\n        self.f.seek(-size-self.SIZE_SIZE, os.SEEK_END)\n    exceptions.IOError: [Errno 22] Invalid argument\n\n2014-08-08 11:55:03+0200 [seloger] INFO: Crawled 174 pages (at 23 pages/min), scraped 165 items (at 17 items/min)\n2014-08-08 11:56:03+0200 [seloger] INFO: Crawled 174 pages (at 0 pages/min), scraped 165 items (at 0 items/min)\n2014-08-08 11:57:03+0200 [seloger] INFO: Crawled 174 pages (at 0 pages/min), scraped 165 items (at 0 items/min)\nand nothing more... it stopped here.\ndoes someone has ever seen something similar ?\nthanks", "issue_status": "Open", "issue_reporting_time": "2014-08-08T10:03:36Z"}, "398": {"issue_url": "https://github.com/scrapy/scrapy/issues/844", "issue_id": "#844", "issue_summary": "Item validation", "issue_description": "Contributor\nnramirezuy commented on Aug 7, 2014\nI been thinking in implementing some kind of Item validation making use of Field.\nThe idea behind this is to normalize the data types and formats on the fields.\nDeclaration approaches:\n# Django like\nclass Product(Item):\n    name = CharField(max_length=30, can_be_none=False)\n    colors = ListField(max_length=5)\n    price = FloatField(max_digits=5, max_value=10.0)\n\n# Simpler and more flexible\nclass Product(Item):\n    name = Field(validator=lambda value: return True or False) # maybe pass the item to do some comparation\n    colors = Field(validator=lambda x: isinstance(x, list))\nThe main problem I have right now is understand when we want to do the validation, at first I thought that doing the validation when a value is assigned was good but it can have complications with mutable types. Another options is to add a method that run the validation for every field but who will call this method? the spider?(I don't want to relay on the developer) and a middleware looks overkill.\nWhat do you think?", "issue_status": "Open", "issue_reporting_time": "2014-08-06T22:02:47Z"}, "399": {"issue_url": "https://github.com/scrapy/scrapy/issues/833", "issue_id": "#833", "issue_summary": "Prevent URL encoding option", "issue_description": "DanMcInerney commented on Aug 1, 2014\nI am having the damndest time trying to unquote the URL in some requests. Any plans to add that an as option? It seems like I have to monkeypatch to fix it as middleware won't work but monkeypatching it involves going like 3 libraries deep and I'm not sure how to do that.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2014-08-01T07:15:42Z"}, "400": {"issue_url": "https://github.com/scrapy/scrapy/issues/802", "issue_id": "#802", "issue_summary": "Per request delay", "issue_description": "Contributor\nchekunkov commented on Jul 14, 2014\nSometimes I feel like scrapy is missing per request delays. Any reasons why they weren't implemented?\nWhere can per request delays be used:\nto add exponential backoff for the retry request\nto add some huge delay when site maintenance is detected from request callback\nI think there could be another situations where they can be useful.\nSeems like this question was also raised in #254, but I don't like the way it was implemented there and could spend some time on another implementation and provide PR\n\ud83d\udc4d 6", "issue_status": "Open", "issue_reporting_time": "2014-07-14T14:23:32Z"}, "401": {"issue_url": "https://github.com/scrapy/scrapy/issues/781", "issue_id": "#781", "issue_summary": "CrawlSpider improvements", "issue_description": "Member\ndangra commented on Jul 3, 2014\nA ticket to kickoff the discussion on CrawlSpider enhancements (if any)\nideas:\nSimplify rule definitions by using an implicit linkextractor instanciated from LxmlLinkExtractor.\n... what else?", "issue_status": "Open", "issue_reporting_time": "2014-07-02T23:08:07Z"}, "402": {"issue_url": "https://github.com/scrapy/scrapy/issues/767", "issue_id": "#767", "issue_summary": "scrapy may keep wrong proxy setting when following redirects", "issue_description": "Contributor\nredapple commented on Jun 26, 2014\nWhen:\nhttp_proxy is set for HttpProxyMiddleware,\nand an http:// request is redirected to an https:// location,\nscrapy will use the http_proxy settings for the https scheme.\nThis also happens for https:// to http://\nProxy-Authorization header is also propagated.\nTo test:\nhttp://www.facebook.com redirects to https://www.facebook.com\nhttps://instagram.com/ redirects to http://instagram.com\nNote: interesting discussion on HTTP redirection and headers: https://code.google.com/p/go/issues/detail?id=4800", "issue_status": "Open", "issue_reporting_time": "2014-06-26T11:52:10Z"}, "403": {"issue_url": "https://github.com/scrapy/scrapy/issues/747", "issue_id": "#747", "issue_summary": "Support for socks5 proxy", "issue_description": "cydu commented on Jun 14, 2014\nSupport for socks5 proxy\nhttp://www.ietf.org/rfc/rfc1928.txt\nmaybe we can use https://github.com/habnabit/txsocksx 's SOCKS5Agent\n\ud83d\udc4d 9", "issue_status": "Open", "issue_reporting_time": "2014-06-14T14:23:36Z"}, "404": {"issue_url": "https://github.com/scrapy/scrapy/issues/740", "issue_id": "#740", "issue_summary": "Shortcut method for spider_Idle signal", "issue_description": "Member\ndangra commented on Jun 3, 2014\nImplement a shortcut method handling for spider_idle signal similar to how it is done for spider_closed\nA possible design of the api is an optional spider method named spider.idled() called when spider_idled signal is emitted for the spider, the method can return an iterable of requests to schedule.\nInternally, the implementation will listen for spider_idle signal, schedule requests using engine.crawl() and raise DontCloseSpider exception.", "issue_status": "Open", "issue_reporting_time": "2014-06-03T13:05:34Z"}, "405": {"issue_url": "https://github.com/scrapy/scrapy/issues/730", "issue_id": "#730", "issue_summary": "MailSender fails silently", "issue_description": "xiaohan2012 commented on May 23, 2014\nI am using scrapy.mail.MailSender to send emails. The SMTP server I will use to send mails is not actually running. However, MailSender just fail silently.\nIt is better to give some messages to the developer if SMTP service is not available.", "issue_status": "Open", "issue_reporting_time": "2014-05-23T12:59:36Z"}, "406": {"issue_url": "https://github.com/scrapy/scrapy/issues/713", "issue_id": "#713", "issue_summary": "Bring docstrings back", "issue_description": "Member\nkmike commented on May 7, 2014\nWhat do you think about moving docstrings back to classes, functions and modules?\nIt'll make reading and understanding Scrapy code as well as working in Scrapy shell easier. Also, it will allow adding \"viewcode\" Sphinx extension to docs. Of course, there should be hand-written doc pages that explain main concepts, but the API reference can be mostly auto-generated.\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2014-05-07T09:34:54Z"}, "407": {"issue_url": "https://github.com/scrapy/scrapy/issues/685", "issue_id": "#685", "issue_summary": "Google Cloud Storage Support (Storage backends)", "issue_description": "mattes commented on Apr 9, 2014\nDoes it make sense to support Google Cloud Storage as storage backend? Boto already supports Cloud storage: http://boto.readthedocs.org/en/latest/ref/gs.html\n\ud83d\udc4d 2", "issue_status": "Open", "issue_reporting_time": "2014-04-09T02:13:47Z"}, "408": {"issue_url": "https://github.com/scrapy/scrapy/issues/683", "issue_id": "#683", "issue_summary": "Add n argument to TakeFirst processor", "issue_description": "Member\nkmike commented on Apr 9, 2014\nWhat do you think about adding \"n\" argument to TakeFirst processor?\nBy default it would be None, meaning the existing behaviour.\nTakeFirst(3) would mean \"return a list with at most 3 non-empty values\".", "issue_status": "Open", "issue_reporting_time": "2014-04-08T18:37:37Z"}, "409": {"issue_url": "https://github.com/scrapy/scrapy/issues/669", "issue_id": "#669", "issue_summary": "FTP Handler should read credentials from URL", "issue_description": "umrashrf commented on Mar 26, 2014\nBased on recent chat in support room, it seems like currently FTP handler can't read authentication credentials from URL. If Chrome and Firefox can do this, I think Scrapy should too. Look forward to your objections and/or comments.", "issue_status": "Open", "issue_reporting_time": "2014-03-26T14:30:47Z"}, "410": {"issue_url": "https://github.com/scrapy/scrapy/issues/578", "issue_id": "#578", "issue_summary": "Linkextractors and ItemLoader Unified API", "issue_description": "Contributor\nnramirezuy commented on Feb 3, 2014\nThere are lots of linkextractors with different flavors, but we don't need linkextractors we just need the filters (or processors) and a good way to handle them.\nWhat is the different between using extractors and this approach?\n>>> from scrapy.contrib.linkfilters import *\n>>> from scrapy.contrib.loader import ItemLoader\n>>> from scrapy.selector import Selector\n>>> from scrapy.http import HtmlResponse\n>>> response = HtmlResponse('http://scrapinghub.com', body='<html><body><div><a href=\"./index\">Home</a><a href=\"./index\">Index</a></div><div><a href=\"./faq\">FAQ</a></div></body></html>')\n>>> loader = ItemLoader(response=response)\n>>> list(loader.get_xpath('.//a/@href', Unique()))\n[u'./index', u'./faq']\n>>> sel = Selector(response)\n>>> loader = ItemLoader(response=response)\n>>> list(loader.get_xpath('(.//div)[2]/a/@href', Unique()))\n[u'./faq']\nMy proposal have different points:\nmove all the pipeline logic from ItemLoader to a class Loader. (this is not needed, but fancier)\nRule instead of use a linkextractor should receive a callable that receives a Response as argument and returns a iterable of urls.\ndead or rise of Link, we should add support for instantiate Request using a Link object instead of an url.", "issue_status": "Open", "issue_reporting_time": "2014-02-03T16:53:51Z"}, "411": {"issue_url": "https://github.com/scrapy/scrapy/issues/547", "issue_id": "#547", "issue_summary": "Add a command-line option for overwriting exported file", "issue_description": "Member\nkmike commented on Jan 18, 2014\nWhat do you think about adding an option to overwrite/recreate exported file?\nSomething like\n$ scrapy crawl myspider -o data.jl --overwrite\nor\n$ scrapy crawl myspider -O data.jl\nThis is useful during development where old data is not needed. I usually run\n$ rm data.jl; scrapy crawl myspider -o data.jl\nmultiple times. This is not DRY: if I want to change file name for the next iteration and preserve existing file then I must be careful and update both names (of course, the command comes from shell autocompletion).\n\ud83d\udc4d 8", "issue_status": "Open", "issue_reporting_time": "2014-01-18T00:25:09Z"}, "412": {"issue_url": "https://github.com/scrapy/scrapy/issues/544", "issue_id": "#544", "issue_summary": "Scrapy timeouts after FormRequest to IIS 5.1 server", "issue_description": "llekn commented on Jan 17, 2014\nAs discussed here http://stackoverflow.com/questions/21177090/scrapy-gets-stuck-with-iis-5-1-page , having almost identical applications but being served with different versions of IIS makes scrapy unable to scrap the one served with IIS 5.1.\nWhy I think as this is related with IIS 5.1 servers: as discussed here http://code.google.com/p/go/issues/detail?id=2184 , IIS 5.1 returns HTTP 100 after every POST request, breaking some automated HTTP clients. It might be the case for scrapy also.\nLong story short, how to reproduce the issue:\nFind a webpage with a form being served with IIS 5.1\nCreate a spider that makes a FormRequest to the form\nRun the spider\nScrapy will keep getting timeouts after sending the POST request\nBut forcing HTTP 1.0 by adding\nDOWNLOAD_HANDLERS = {\n    'http': 'scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler',\n}\nto settings.py seems to solve the problem.\nShouldn't the spider work with the default (HTTP 1.1) handler?\nP.S: Using Scrapy 0.20.2 (pip-installed)", "issue_status": "Open", "issue_reporting_time": "2014-01-17T15:58:43Z"}, "413": {"issue_url": "https://github.com/scrapy/scrapy/issues/522", "issue_id": "#522", "issue_summary": "should spider.allowed_domains match same-origin policy?", "issue_description": "Contributor\nnyov commented on Jan 10, 2014\nI believe it would be nice to obey the Web Origin Concept's understanding of same-origin (https://tools.ietf.org/html/rfc6454#section-3.2.1) in allowed_domains.\nSo I'll throw this out here for discussion.\nCurrently it is impossible to specify, using only allowed_domains, when subdomains of a domain should be considered 'off-site', or that actually a different protocol/scheme (or port) should be considered a different origin.\nMy proposal would be to include or accept actual schemes in allowed_domains, such as https://example.com to specifiy http://example.com or ftp://example.com as offsite.\nMore problematic might be specifying where subdomains of a domain should be offsite, where infact //github.com shouldn't match //someuser.github.com by default, but cookies and web origins don't use wildcards (I believe) for specifying something as //*.example.com. This would be where DNS wildcards help,\nbut then again, according to DNS wildcards, this shouldn't match the parent domain, so there's a catch here on how to specify http://[everything.]example.com while excluding https or other schemes.\nSuch a change would bring scrapy's understanding of \"same origin\" into line with Cookie Policy, DNS policy and the Web Origin Concept.\nSee the following RFCs:\nCookies:\nRFC 2109 HTTP State Management Mechanism (hist.)\nRFC 2965 HTTP State Management Mechanism (hist.)\nRFC 6265 HTTP State Management Mechanism (prop.)\nSame-origin:\nRFC 6454 The Web Origin Concept\nDNS:\nRFC 1034 Domain Names\nRFC 4592 The Role of Wildcards in DNS", "issue_status": "Open", "issue_reporting_time": "2014-01-10T01:46:36Z"}, "414": {"issue_url": "https://github.com/scrapy/scrapy/issues/508", "issue_id": "#508", "issue_summary": "FormRequest.from_response fails to parse form action url", "issue_description": "Member\ndangra commented on Dec 31, 2013\nReported in #scrapy IRC channel, if parent url contains an unquoted \"|\" the final url is quoted including its scheme.\n>>> from scrapy.http import FormRequest, HtmlResponse\n>>> response = HtmlResponse(body='<html><body><form></form></body></html>',\n...                                         url='http://foo.com/?a=|')\n>>> FormRequest.from_response(response)\nNone\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-3-b87bf46cf4c1> in <module>()\n----> 1 FormRequest.from_response(response)\n\n/home/daniel/src/scrapy/scrapy/http/request/form.pyc in from_response(cls, response, formname, formnumber, formdata, clickdata, dont_click, formxpath, **kwargs)\n     38         url = _get_form_url(form, kwargs.pop('url', None))\n     39         method = kwargs.pop('method', form.method)\n---> 40         return cls(url=url, method=method, formdata=formdata, **kwargs)\n     41 \n     42 def _get_form_url(form, url):\n\n/home/daniel/src/scrapy/scrapy/http/request/form.pyc in __init__(self, *args, **kwargs)\n     19             kwargs['method'] = 'POST'\n     20 \n---> 21         super(FormRequest, self).__init__(*args, **kwargs)\n     22 \n     23         if formdata:\n\n/home/daniel/src/scrapy/scrapy/http/request/__init__.pyc in __init__(self, url, callback, method, headers, body, cookies, meta, encoding, priority, dont_filter, errback)\n     24         self._encoding = encoding  # this one has to be set first\n     25         self.method = str(method).upper()\n---> 26         self._set_url(url)\n     27         self._set_body(body)\n     28         assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n\n/home/daniel/src/scrapy/scrapy/http/request/__init__.pyc in _set_url(self, url)\n     55                 raise TypeError('Cannot convert unicode url - %s has no encoding' %\n     56                     type(self).__name__)\n---> 57             self._set_url(url.encode(self.encoding))\n     58         else:\n     59             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n/home/daniel/src/scrapy/scrapy/http/request/__init__.pyc in _set_url(self, url)\n     59             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n     60         if ':' not in self._url:\n---> 61             raise ValueError('Missing scheme in request url: %s' % self._url)\n     62 \n     63     url = property(_get_url, obsolete_setter(_set_url, 'url'))\n\nValueError: Missing scheme in request url: http%3A//foo.com/%3Fa=%257C\n>>> ", "issue_status": "Open", "issue_reporting_time": "2013-12-30T22:13:50Z"}, "415": {"issue_url": "https://github.com/scrapy/scrapy/issues/496", "issue_id": "#496", "issue_summary": "Exception in image/files pipeline are quietly suppressed", "issue_description": "Contributor\nmax-arnold commented on Dec 19, 2013\nExceptions (TypeError, Exception and others) raised in media_to_download(), media_failed(), media_downloaded(), file_key() and other pipeline methods are quietly suppressed.\nExample can be found in #490", "issue_status": "Open", "issue_reporting_time": "2013-12-19T03:32:21Z"}, "416": {"issue_url": "https://github.com/scrapy/scrapy/issues/482", "issue_id": "#482", "issue_summary": "S3FilesStore can use a lot of memory", "issue_description": "Member\nkmike commented on Dec 5, 2013\nHi,\n@nramirezuy and me were debugging memory issue with one of the spiders some time ago, and it seems to be caused by ImagesPipeline + S3FilesStore. I haven't confirmed that it was the cause of memory issue, this ticket is based solely on reading the source code.\nFilesPipeline reads the whole file to memory and then defers the uploading to thread (via S3FilesStore.persist_file, passing file contents as bytes). So there could be many files loaded to memory at the same time, and as soon as files are downloaded faster than they are are uploaded to s3, memory usage will grow. This is not unlikely IMHO because s3 is not super-fast. For ImagesPipeline it is worse because it uploads not only the image itself, but also the generated thumbnails.\nI think S3FilesStore should persist files to temporary location before uploading them to S3 (at least optionally). This would allow streaming files without storing them in memory.", "issue_status": "Open", "issue_reporting_time": "2013-12-05T10:55:16Z"}, "417": {"issue_url": "https://github.com/scrapy/scrapy/issues/457", "issue_id": "#457", "issue_summary": "Security enhancement when following a \"redirect\"", "issue_description": "mvsantos commented on Nov 7, 2013\nI believe this is not a bug, but could fit in as a security enhancement.\nTL;DR When following HTTP redirects, scrapy should only follow http/https requests. Quite possibly similar to (CVE-2009-0037) http://curl.haxx.se/docs/adv_20090303.html\nScenario:\na. A spider that simply stores raw or minimally sanitised data.\nb. By default scrapy follows HTTP redirects.\nc. The values of both DOWNLOAD_HANDLERS and DOWNLOAD_HANDLERS_BASE are set to their default.\nDOWNLOAD_HANDLERS = {}\nDOWNLOAD_HANDLERS_BASE = {\n    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n    'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n    'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',\n    'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',\n}\nAnd let's say a malicious client might issue a redirect such as\n# ==== .htaccess ====\nRewriteEngine On\nRedirect 301 /trap.html  file:///etc/passwd\n# ==== ENDof .htaccess ====\nAnd so, my spider will be \"leaking\" my own \"/etc/passwd\" file because Scrapy followed the 301 redirect from \"http:// ... /trap.html\" to \"file:///etc/passwd\".\nShouldn't scrapy enforce some sort of protocol restriction so that it only follows the http/https protocols when following redirects?", "issue_status": "Open", "issue_reporting_time": "2013-11-07T13:06:18Z"}, "418": {"issue_url": "https://github.com/scrapy/scrapy/issues/456", "issue_id": "#456", "issue_summary": "Allow start_requests method running forever", "issue_description": "ipyleaf commented on Nov 7, 2013\nFor\nversion 0.18.4\nSituation\nA Spider gets one Reuqest from start_requests, and start_requests won't stop because it depends on the MQ.\nI know spider is sheduled by \"yield\". But if the MQ hands up because of no message coming in, the start_requests also hands up. That's not what I want.\nSolution\nSo, I have hacked the source scrapy/core/engine.py like below:\n    def _next_request(self, spider):\n        try:\n            slot = self.slots[spider]\n        except KeyError:\n            return\n\n        if self.paused:\n            slot.nextcall.schedule(5)\n            return\n\n        while not self._needs_backout(spider):\n            if not self._next_request_from_scheduler(spider):\n                break\n\n        if slot. and not self._needs_backout(spider):\n            try:\n                request = slot..next()\n            except StopIteration:\n                slot. = \n            except Exception, exc:\n                log.err(, 'Obtaining request from start requests', \\\n                        spider=spider)\n            else:\n                # ------------Start Hacking-----------------------\n                if request is None:\n                    slot.nextcall.schedule(5)\n                    return\n                # ------------End Hacking-----------------------\n                self.crawl(request, spider)\n\n        if self.spider_is_idle(spider) and slot.close_if_idle:\n            self._spider_idle(spider)\nThen, method can be like this:\n    def start_requests(self):\n        while True:\n            data = queue.get()\n            if not isinstance(data, (tuple,)):\n                yield \n            else:\n                yield Request(........)\nThus, start_requests method can run forever, and not hands up any more.\n\ud83d\udc4d 3", "issue_status": "Open", "issue_reporting_time": "2013-11-07T04:06:01Z"}, "419": {"issue_url": "https://github.com/scrapy/scrapy/issues/440", "issue_id": "#440", "issue_summary": "Provide DownloaderMiddleware an interface to read raw HTTP requests and responses", "issue_description": "odie5533 commented on Oct 24, 2013\nHello! I would like to request that Scrapy provide an interface to the DownloaderMiddlewares so that they can read (and possibly edit) the raw HTTP request and response data. An example name would be process_raw_request and process_raw_response which would provide the middleware with the raw GET / ... data.\nThe new HTTP 1.1 parts seem to use a producer pattern, so these could be process_raw_response_chunk/process_raw_response_chunk_decoded methods depending on where in the chain the callback is sent.\nRight now I am accessing the data through a very roundabout way, so providing a direct interface through DownloaderMiddleware would be very useful both for debugging and for saving entire dumps.", "issue_status": "Open", "issue_reporting_time": "2013-10-24T11:17:37Z"}, "420": {"issue_url": "https://github.com/scrapy/scrapy/issues/374", "issue_id": "#374", "issue_summary": "LogFormatter should handle formatting of errors", "issue_description": "Member\nshaneaevans commented on Aug 22, 2013\nIn particular, being able to customize downloader and spider errors would be very useful.", "issue_status": "Open", "issue_reporting_time": "2013-08-21T19:03:29Z"}, "421": {"issue_url": "https://github.com/scrapy/scrapy/issues/356", "issue_id": "#356", "issue_summary": "Support list type arguments from command line.", "issue_description": "wutali commented on Jul 24, 2013\nI want to support Scrapy take list type arguments that is just a text split by some commas like these::\nscrapy crawl a_spider -a arg1=text1,text2,text3\nIn this case, I would write a splitting code to convert it to the list type object.\nBut it is nasty for me and I don't wanna write these code on the spider class again and again.\nDo you have any idea about it and does it match the policy of Scrapy?", "issue_status": "Open", "issue_reporting_time": "2013-07-24T07:12:58Z"}, "422": {"issue_url": "https://github.com/scrapy/scrapy/issues/355", "issue_id": "#355", "issue_summary": "Default downloader fails to get page", "issue_description": "mfyang commented on Jul 23, 2013\n'http://autos.msn.com/research/userreviews/reviewlist.aspx?ModelID=14749'\nLooks like the default downloader implemented with twisted lib can't fetch the above url. I ran 'scrapy shell http://autos.msn.com/research/userreviews/reviewlist.aspx?ModelID=14749', and got the following output.\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 5, in <module>\n    pkg_resources.run_script('Scrapy==0.17.0', 'scrapy')\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py\", line 489, in run_script\n    self.require(requires)[0].run_script(script_name, ns)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py\", line 1207, in run_script\n    execfile(script_filename, namespace, namespace)\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/EGG-INFO/scripts/scrapy\", line 4, in <module>\n    execute()\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/commands/shell.py\", line 47, in run\n    shell.start(url=url, spider=spider)\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/shell.py\", line 43, in start\n    self.fetch(url, spider)\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/shell.py\", line 85, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/threads.py\", line 118, in blockingCallFromThread\n    result.raiseException()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/python/failure.py\", line 370, in raiseException\n    raise self.type, self.value, self.tb\ntwisted.internet.error.ConnectionDone: Connection was closed cleanly.\nBut both urlopen of urllib2 and requests.get can download the page smoothly.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2013-07-23T05:12:00Z"}, "423": {"issue_url": "https://github.com/scrapy/scrapy/issues/305", "issue_id": "#305", "issue_summary": "Make it possible to debounce requests", "issue_description": "Contributor\nbarraponto commented on May 21, 2013\nI just read Ben Alman on jQuery's Throttle and Debounce and wondered: can DebounceRequest be added to Scrapy?\nThe use case: recently I've been scraping a website where I wanted to gather Facebook Likes per URL. Now that comes cheap using Facebook API, particularly the Facebook Query Language. The problem is that FB will eventually stop answering my request, on a undocumented rate limit. But what if I could define a way for those requests to be joined and called only after a certain while, asking for all the parameters in a user-defined way? I'd expect the callback to be called just once, too (or maybe several times but with the full response).", "issue_status": "Open", "issue_reporting_time": "2013-05-21T04:04:11Z"}, "424": {"issue_url": "https://github.com/scrapy/scrapy/issues/239", "issue_id": "#239", "issue_summary": "Contract Fail signal", "issue_description": "jasonlfunk commented on Feb 8, 2013\nWould it be possible to add a signal when a contract fails? I'm looking for a way to run a method when one of my contract fails. I want to do some automated testing with remote logging of failures.", "issue_status": "Open", "issue_reporting_time": "2013-02-08T17:26:29Z"}, "425": {"issue_url": "https://github.com/scrapy/scrapy/issues/210", "issue_id": "#210", "issue_summary": "Unhandled error caused by HTTP non-compliant headers", "issue_description": "yeechen commented on Dec 20, 2012\nEnvironment:\nScrapy 0.16.2\nTwisted-12.2.0\npython 2.7\nmacosx-10.6\nUse Case 1\nRun:\nscrapy shell http://aaa.17domn.com/bt9/file.php/MERH77V.html\nError:\n[ScrapyHTTPPageGetter,client] Unhandled Error\nTraceback (most recent call last):\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/Twisted-12.2.0-py2.7-macosx-10.6-intel.egg/twisted/internet/selectreactor.py\", line 150, in _doReadOrWrite\nwhy = getattr(selectable, method)()\n ...\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/Twisted-12.2.0-py2.7-macosx-10.6-intel.egg/twisted/web/http.py\", line 406, in extractHeader\nkey, val = header.split(':',1)\nexceptions.ValueError: need more than 1 value to unpack\nSolution:\nhttps://groups.google.com/forum/#!msg/scrapy-users/xFKo8ggzPxs/VXDl3CZ4V4cJ They describe this is caused by twisted. Then I patched function extractHeader in /twisted/web/http.py from http://twistedmatrix.com/trac/ticket/2842. It works\nUse Case 2:\nRun:\nscrapy shell http://www1.wkdown.info/fs3/file.php/M994ATR.html\nError:\nTraceback (most recent call last):\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/bin/scrapy\", line 5, in\n...\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/Scrapy-0.16.2-py2.7.egg/scrapy/core/downloader/webclient.py\", line 122, in _build_response\nstatus = int(self.status)\nValueError: invalid literal for int() with base 10: 'html'\nunfix!", "issue_status": "Open", "issue_reporting_time": "2012-12-20T14:23:24Z"}, "426": {"issue_url": "https://github.com/scrapy/scrapy/issues/157", "issue_id": "#157", "issue_summary": "Rate-limiting (Bandwidth limiting) for downloads", "issue_description": "achimnol commented on Jul 16, 2012\nI have to crawl an website that enforces a certain download rate limit for all its URLs, for example, 800 KBytes/sec.\nSince my internet connection is faster than that, accessing the website using my plain web browser and scrapy causes severe delay penalty by the server software. (Using 800 KB/s limit, the download should finish within 30 secs, but if I use unlimited download rates, the server sometimes does not return anything at all, or after a significant artificial delay of 150 secs more or less.)\nBefore using scrapy, I was using my home-made crawler with rate limiting.\nAfter migrating to scrapy, I have realized that I have to re-implement scrapy.core.downloader.webclient stuffs and I need to learn twisted APIs.\n(My first lookup was downloader middlewares, but they seem to be executed after receiving the whole body, unfortunately.)\nThere is an undocumented setting called DOWNLOADER_HTTPCLIENTFACTORY, so I could copy & paste the existing classes, extend them, and override this setting.\nI think we can override rawDataReceived() in ScrapyHTTPPageGetter by inserting calculated delays to limit the receiving rate of the response body traffic, but I think I need more inspection.\nCould scrapy be shipped with its native rate limiting feature?", "issue_status": "Open", "issue_reporting_time": "2012-07-15T18:34:41Z"}, "427": {"issue_url": "https://github.com/scrapy/scrapy/issues/113", "issue_id": "#113", "issue_summary": "Exception handler for scrapy pipelines", "issue_description": "fguilpain commented on Apr 11, 2012\nHello,\nI'm currently working with scrapy for web crawling and I would like to monitor pipelines with an hook which catch every exceptions, when I looked in documentation, I found out that there is already something similar item_dropped() but it catch only DropItem exceptions, it would be useful to have something more generic which catch every kind of exception.\nDangra already helped me, please take a look on his modification : http://paste.pocoo.org/show/579565/\nThanks for your attention.\n\ud83d\udc4d 1", "issue_status": "Open", "issue_reporting_time": "2012-04-11T16:50:45Z"}, "428": {"issue_url": "https://github.com/scrapy/scrapy/issues/50", "issue_id": "#50", "issue_summary": "Offsite middleware ignoring port", "issue_description": "eedeep commented on Nov 16, 2011\nIn my spider I have the following:\nclass MySpider(BaseSpider):\n    allowed_domains = ['192.169.0.15:8080']\nand in the parse method I do something like:\n    yield Request('http://192.169.0.15:8080/mypage.html', self.my_callback_function)\nthe result when I run the code is that that scrapy reports:\nDEBUG: Filtered offsite request to '192.168.0.15': <GET http://192.168.0.15:8080/mypage.html>\nWhich is wrong - it seems to be ignoring the port. If I change the allowed_domains to:\n    allowed_domains = ['192.169.0.15:8080', '192.16.0.15']\nThen it works as you would expect it to. No big deal, can work around it but I think it is a bug. The problem being located in the should_follow method of the OffsiteMiddleware class in contrib/spidermiddleware/offsite.py", "issue_status": "Open", "issue_reporting_time": "2011-11-16T05:20:33Z"}, "429": {"issue_url": "https://github.com/scrapy/scrapy/issues/40", "issue_id": "#40", "issue_summary": "support for two-legged OAuth", "issue_description": "idoshilon commented on Sep 15, 2011\nAdd support so scrapy will support two-legged OAuth , similar to the work of - https://github.com/simplegeo/python-oauth2", "issue_status": "Open", "issue_reporting_time": "2011-09-14T22:44:05Z"}, "430": {"issue_url": "https://github.com/scrapy/scrapy/issues/36", "issue_id": "#36", "issue_summary": "Add command line option for doing POSTs", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nPreviously reported by pablo on Trac http://dev.scrapy.org/ticket/292\nIt would be nice to add a command line option for performing POSTs, like curl -d does.\nThis would be added to all commands that accepts a url argument, like crawl, parse, shell, fetch and view.", "issue_status": "Open", "issue_reporting_time": "2011-09-09T05:44:56Z"}, "431": {"issue_url": "https://github.com/scrapy/scrapy/issues/8", "issue_id": "#8", "issue_summary": "Refactor signals", "issue_description": "Member\ndangra commented on Sep 9, 2011 \u2022\nedited by redapple\nRefactor signal handling similar how Django did for 1.0.\nAccording to them, the new approach brings up to 90% speed improvements", "issue_status": "Open", "issue_reporting_time": "2011-09-09T05:11:32Z"}}, "closed_issues": {"1": {"issue_url": "https://github.com/scrapy/scrapy/issues/4315", "issue_id": "#4315", "issue_summary": "Error while using signals.spider_opened and signals.spider_closed in pipeline.py", "issue_description": "pradhanvickey commented 2 hours ago \u2022\nedited by elacuesta\nHI, I am trying to implement below but getting error. Kindly help:-\nimport pymongo\nfrom scrapy import signals\n\nclass MongoPipeline(object):\n\n    collection_name = 'scrapy_items'\n\n    def __init__(self, mongo_uri, mongo_db,crawler):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n        self.crawler = crawler\n        crawler.signals.connect(self.open_spider, signals.spider_opened)\n        crawler.signals.connect(self.close_spider, signals.spider_closed)\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'),\n            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items'),\n            crawler = crawler\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def close_spider(self, spider,reason):\n        self.client.close()\n\n    def process_item(self, item, spider):\n        self.db[self.collection_name].insert_one(dict(item))\n        return item\nError:\nyield self.engine.open_spider(self.spider, start_requests)\nTypeError: open_spider() missing 1 required positional argument: 'start_requests'\n\nDuring handling of the above exception, another exception occurred:\n\ncurrent.result = callback(current.result, *args, **kw)\nTypeError: close_spider() missing 1 required positional argument: 'reason'\n(edited for syntax highlighting)", "issue_status": "Closed", "issue_reporting_time": "2020-02-06T20:17:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "2": {"issue_url": "https://github.com/scrapy/scrapy/issues/4301", "issue_id": "#4301", "issue_summary": "No module named 'twisted'", "issue_description": "Marsqueenlyy commented 2 days ago \u2022\nedited\nwhen i import scrapy, it runs like 'Traceback (most recent call last):\nFile \"E:/python practicing/\u722c\u866b/l0205_crawler1.py\", line 1, in\nimport scrapy\nFile \"E:\\\u65b0\u5efa\u6587\u4ef6\u5939\\Python\\lib\\site-packages\\scrapy_init_.py\", line 27, in\nfrom scrapy import _monkeypatches\nFile \"E:\\\u65b0\u5efa\u6587\u4ef6\u5939\\Python\\lib\\site-packages\\scrapy_monkeypatches.py\", line 6, in\nimport twisted.persisted.styles # NOQA\nModuleNotFoundError: No module named 'twisted''\nwhy?...\nand i downloaded the resourse code from github, removed this package direclly to \u2018\\Python\\Lib\\site-packages\u2019\uff0ccause i cant 'pip install' it ,with the same problems related to 'twisted'", "issue_status": "Closed", "issue_reporting_time": "2020-02-05T03:13:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "3": {"issue_url": "https://github.com/scrapy/scrapy/issues/4297", "issue_id": "#4297", "issue_summary": "CSS fuzzy selectors are not working?", "issue_description": "RaedsLab commented 5 days ago\nDescription\nCSS fuzzy selectors do not seem to work.\nFor example a[class~=\"logo\"] should return all links with a class that contains logo. MDN\nSteps to Reproduce\nSelect something like a[class~=\"logo\"]\nExpected behavior: Returns all links with a class that contains logo.\nActual behavior: Returns empty array\nReproduces how often: Always.", "issue_status": "Closed", "issue_reporting_time": "2020-02-02T08:18:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "4": {"issue_url": "https://github.com/scrapy/scrapy/issues/4281", "issue_id": "#4281", "issue_summary": "FileNotFoundError: [Errno 2] No such file or directory: 'webcam/webcam/settings.py.tmpl'", "issue_description": "dfriestedt commented 16 days ago \u2022\nedited\nDescription\nwhen I issue the following command\nscrapy startproject webcam\nI get the error message:\nvagrant@vagrant:/vagrant$ scrapy startproject webcam\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/usr/local/lib/python3.6/dist-packages/scrapy/cmdline.py\", line 146, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/usr/local/lib/python3.6/dist-packages/scrapy/cmdline.py\", line 100, in _run_print_help\n    func(*a, **kw)\n  File \"/usr/local/lib/python3.6/dist-packages/scrapy/cmdline.py\", line 154, in _run_command\n    cmd.run(args, opts)\n  File \"/usr/local/lib/python3.6/dist-packages/scrapy/commands/startproject.py\", line 109, in run\n    ProjectName=string_camelcase(project_name))\n  File \"/usr/local/lib/python3.6/dist-packages/scrapy/utils/template.py\", line 9, in render_templatefile\n    with open(path, 'rb') as fp:\nFileNotFoundError: [Errno 2] No such file or directory: 'webcam/webcam/settings.py.tmpl'\nSteps to Reproduce\nCreate a fresh vagrant machine with ubuntu 18.04 on a mac 10.14.6\nInstall scrapy per these instruction with all the dependancies: https://docs.scrapy.org/en/latest/intro/install.html\nissue the command scrapy startproject webcam\nExpected behavior:\nI expect it to create the project structure without errors.\nActual behavior:\nIt creates the folder structure as expected BUT produces the error message in the description.\nReproduces how often:\nThis happens on a fresh install. on a new Ubuntu 18.04 box. I tried to uninstall and reinstall scrapy. It produces the same error.\nVersions\nvagrant@vagrant:/vagrant$ pip show scrapy\nName: Scrapy\nVersion: 1.8.0\nSummary: A high-level Web Crawling and Web Scraping framework\nHome-page: https://scrapy.org\nAuthor: Scrapy developers\nAuthor-email: None\nLicense: BSD\nLocation: /home/vagrant/.local/lib/python2.7/site-packages\nRequires: pyOpenSSL, protego, service-identity, cryptography, Twisted, lxml, six, parsel, cssselect, w3lib, zope.interface, PyDispatcher, queuelib\nAdditional context\nThis issue was identified here: #4019. I provided the same information above but that issue is closed. I'm opening a new ticket here.", "issue_status": "Closed", "issue_reporting_time": "2020-01-22T00:33:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "5": {"issue_url": "https://github.com/scrapy/scrapy/issues/4273", "issue_id": "#4273", "issue_summary": "[scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)", "issue_description": "wahidaa commented 28 days ago\nHi everyone ,\ni'm beginner with scrapy\ni'm trying to scrape multiple pages\nbut it doesn't work with me\nthis is the spider:\nimport scrapy\nclass pageSpider(scrapy.Spider):\nname = 'page'\nallowed_domains = ['www.bureauxlocaux.com']\nstart_url = ['https://www.bureauxlocaux.com/immobilier-d-entreprise/annonces/boulogne-billancourt-92100/location-bureaux']\ndef parse(self, response):\nfor product in response.xpath('//h4[@Class=\"item-card__title\"]'):\nyield {\n'title' : product.xpath('.//span/text()').extract_first()\n}\nnext_url_path = response.xpath('.//a[@Class=\"next-page-selector\"]/@href').extract_first()\nif next_url_path:\nyield scrapy.Request(response.urljoin(next_url_path),callback=self.parse)\nand this is the output:\n2020-01-10 09:58:55 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: pages)\n2020-01-10 09:58:55 [scrapy.utils.log] INFO: Versions: lxml 4.4.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.5.2 (default, Oct 8 2019, 13:06:37) - [GCC 5.4.0 20160609], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d 10 Sep 2019), cryptography 2.8, Platform Linux-4.4.0-170-generic-x86_64-with-Ubuntu-16.04-xenial\n2020-01-10 09:58:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'pages', 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'pages.spiders', 'FEED_FORMAT': 'csv', 'SPIDER_MODULES': ['pages.spiders'], 'FEED_URI': 'page.csv'}\n2020-01-10 09:58:55 [scrapy.extensions.telnet] INFO: Telnet Password: fc54acd3636fa0d8\n2020-01-10 09:58:55 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.logstats.LogStats',\n'scrapy.extensions.feedexport.FeedExporter',\n'scrapy.extensions.memusage.MemoryUsage']\n2020-01-10 09:58:55 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2020-01-10 09:58:55 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2020-01-10 09:58:55 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2020-01-10 09:58:55 [scrapy.core.engine] INFO: Spider opened\n2020-01-10 09:58:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2020-01-10 09:58:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2020-01-10 09:58:55 [scrapy.core.engine] INFO: Closing spider (finished)\n2020-01-10 09:58:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'elapsed_time_seconds': 0.004772,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2020, 1, 10, 8, 58, 55, 842405),\n'log_count/INFO': 10,\n'memusage/max': 53555200,\n'memusage/startup': 53555200,\n'start_time': datetime.datetime(2020, 1, 10, 8, 58, 55, 837633)}\n2020-01-10 09:58:55 [scrapy.core.engine] INFO: Spider closed (finished)\nany hel please !!!", "issue_status": "Closed", "issue_reporting_time": "2020-01-10T09:03:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "6": {"issue_url": "https://github.com/scrapy/scrapy/issues/4265", "issue_id": "#4265", "issue_summary": "scrapy\u4e2d\u4e3a\u4f55\u4e0b\u8f7d\u7684\u6587\u4ef6\u662f2k", "issue_description": "Chenhonli commented on Jan 3\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2020-01-03T05:47:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "7": {"issue_url": "https://github.com/scrapy/scrapy/issues/4264", "issue_id": "#4264", "issue_summary": "Document that Scrapy 1.7.0 breaks custom schedulers", "issue_description": "Member\nGallaecio commented on Jan 3\nThis signature change introduced in 1.7.0 breaks custom schedulers. It should be covered in the list of 1.7.0 backward-incompatible changes.", "issue_status": "Closed", "issue_reporting_time": "2020-01-02T19:34:01Z", "fixed_by": "#4274", "pull_request_summary": "added custom scheduler breakage in news.rst", "pull_request_description": "Contributor\njoybhallaa commented 27 days ago \u2022\nedited by elacuesta\n@Gallaecio created an issue #4264, in which I had to add in the backward-incompatible changes that scrapy 1.7.0 version breaks custom schedulers.\nI have added it to news.rst, please check it out and let me know if it needs to be more thorough.\nI have ran all the tests using tox\n(edit) Fixes #4264", "pull_request_status": "Merged", "issue_fixed_time": "2020-02-06T21:21:34Z", "files_changed": [["7", "docs/news.rst"]]}, "8": {"issue_url": "https://github.com/scrapy/scrapy/issues/4262", "issue_id": "#4262", "issue_summary": "how to not follow links there are lower depth than start_urls", "issue_description": "BenjaminHoegh commented on Jan 1 \u2022\nedited\nHow do I make Scrapy to not follow links there go to a url with lower depth than the one I defined in start_urls\nExample\nif I wanna follow all links from the position example.com/shop/\nLink Follow\nexample.com no\nexample.com/shop/32 yes\nexample.com/shop/32/54 yes\nexample.com/about no\nexample.com/someting/else no", "issue_status": "Closed", "issue_reporting_time": "2020-01-01T09:46:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "9": {"issue_url": "https://github.com/scrapy/scrapy/issues/4256", "issue_id": "#4256", "issue_summary": "Capture scrapy spider status (Success and failure)", "issue_description": "YanzhongSu commented on Dec 24, 2019\nI just started using scrapy. Now I am trying to migrate some of the old crawlers (simple python requests) to scrapy.\nOne thing I am struggling with is figuring out a way to capture the status of scrappy spider. In the old crawl script, I have a decorator called task that would decorates the script start function. The code snippet is like this:\n@task(task_name='tutorial',\n      alert_name='tutorial')\ndef start():\n    raw_data = download_data()\n    data = parse(raw_data)\n    push_to_db(data)\n\nif if __name__ == \"__main__\":\n    start() \nSo this task decorator would send a success or failure message depending on the running status of the script.\nNow I want to use this decorator for scrapy spider. I am struggling to find the right place to put this decorator so that it would capture the running status of the spider from request download all the way to pipeline.\nI use this command to start the spider:\nscrapy crawl spider_name\nThe original question was posted here on Stackoverflow", "issue_status": "Closed", "issue_reporting_time": "2019-12-24T11:54:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "10": {"issue_url": "https://github.com/scrapy/scrapy/issues/4251", "issue_id": "#4251", "issue_summary": "how to solve Connection was closed cleanly . scrapy flask gevent Process", "issue_description": "luoqishuai commented on Dec 20, 2019 \u2022\nedited\nDescription\nscrapy code\n            url='http://192.168.5.9:6010/predict?url={}'.format(response.meta['url'])\n            data={'html_url':response.meta['url'],'html_cn_text':cn_result}\n            meta['status_404']=True\n            meta['response']=response\n            meta['proxy']=''\n            return scrapy.FormRequest(url,formdata=data,callback=self.parse,dont_filter=True,meta=meta)\nflask gevent code\nmonkey.patch_all()\napp = Flask(__name__)\nctx=app.app_context()\nctx.push()\n\n# \u83b7\u53d6\u672c\u673aip\nmyaddr = '192.168.5.73'\nmyport = 6010\ndecide_404=Decide404()\n@app.route('/predict',methods=['POST','GET']) \ndef predict():\n    try:\n        start_time=time.time()\n        if request.method == \"GET\":\n            return 'http method need post'\n        elif request.method == \"POST\":\n            form = request.form\n            data = form.to_dict() #\n            html_url=data['html_url'] if 'html_url' in data else ''\n            html_text=data['html_cn_text']\n            score=decide_404.decide_model(html_text)\n            logging.info('url: {} ;text: {} ;predict: {} ;cost: {} ;'.format(html_url,html_text[:20],score,round(time.time()-start_time,2)))\n            return str(score)\n        else:\n            return 'None'\n    except Exception as e:\n        logging.info('error {} '.format(e),exc_info=True)\n        return 'None'\nserver = WSGIServer(('', 6010), app)\nserver.start()\ndef serve_forever():\n    server.start_accepting()\n    server._stop_event.wait()\n\nif __name__ == \"__main__\":\n        try:\n            for i in range(32):\n                p = Process(target=serve_forever)\n                p.start()\n        except Exception as e:\n            logging.info('404 webserver false')\n            decide_404.send_email('404 webserver false')\nscrapy is responsible for sending post requests to my service, and the service is responsible for processing post requests and returning calculation results\nHowever, scrapy will soon catch in middlwares when making a request. /Scrapy_spider_9.log:2019-12-20 16:13:50 [root] INFO: get url https://news.10jqka.com.cn/tapp/news/ share / 616126829 / is 404 by error {'url': 'https://news.10jqka.com.cn/tapp/news/share/616126829/', 'retry': 0, 'url_format': 'https: / /news.10jqka.com.cn/tapp/news/share/()/ ',' article_id ': 616126829,' upload_type ':' news', 'download_timeout': 60.0, 'proxy': '', 'download_slot' : 'news.10jqka.com.cn', 'download_latency': 1.4705162048339844, 'status_404': True, 'response': <200 https://news.10jqka.com.cn/tapp/news/share/616126829/> , 'depth': 1, 'status': ResponseNeverReceived ([<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>],)}\n[Exception. The server does not have extra anti-crawl processing.\nThe server has captured the requested address:\n:: ffff: 192.168.5.9--[2019-12-20 16:13:58] \"POST /predict?url=https://news.10jqka.com.cn/tapp/news/share/616126829/ HTTP / 1.1 \"200 115 3.574375\nAnd the calculation will only return '0', '1', 'None'\nThe request was recorded in the server's log and the result was obtained\n./decide_404.log:[2019-12-20 16: 13: 58,449] [decide_404.py:300] [INFO] [url: https://news.10jqka.com.cn/tapp/news/share/616126829 /; text: Jiahua Energy's monthly big order reveals the secret number of mobile straight flush financials; predict: 1; cost: 3.57;)\nWhere is the setting wrong? Scrapy flask gevent?\nThe server did not perform any additional anti-crawling measures on the request, why does scrapy catch exceptions such as Connection was closed cleanly.\nscrapy = 1.8.0 falsk =1.1.1 gevent =1.4.0\nWhat else do I need to provide?", "issue_status": "Closed", "issue_reporting_time": "2019-12-20T09:06:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "11": {"issue_url": "https://github.com/scrapy/scrapy/issues/4241", "issue_id": "#4241", "issue_summary": "Non Blocking processing of a request by downloader middleware.", "issue_description": "royahsan commented on Dec 18, 2019 \u2022\nedited\nSummary\nCan we Support returning Deferred object in request_reached_downloader signal, or in downloader middleware process_request method.\nSorry if I am missing something very basic here as I am unfamiliar with internal architecture.\nMotivation\nConsider the following potential use cases:\nA ReCaptcha token is to be solved and inserted by downloader middleware. (It cant be inserted while initializing request as tokens expire pretty quick)\nA proxy is to be assigned from a pool of proxies, and currently, none are available, need to be fetched at this point.\nThere is also an active question at StackOverflow presenting very similar issue!\nDescribe alternatives you've considered\nUsing Python's threading module, we can run multiple threads and try to make sure no request has ever to wait to be processed, but this approach is hard to implement especially in case of captchas where tokens expire pretty quick.", "issue_status": "Closed", "issue_reporting_time": "2019-12-18T10:41:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "12": {"issue_url": "https://github.com/scrapy/scrapy/issues/4235", "issue_id": "#4235", "issue_summary": "Problem in downloading the images using scrapy", "issue_description": "Kunal614 commented on Dec 16, 2019\nimport scrapy\nfrom ..items import DownloadItem\nfrom scrapy.utils.response import open_in_browser\nfrom scrapy.loader import ItemLoader\n\nclass image(scrapy.Spider):\n    name='image'\n    allowed_domains=['www.google.com/search?biw=923&bih=759&tbm=isch&sxsrf=ACYBGNT1fy79gOV2O7XhJeKDIqQ814ST0w%3A1576485570813&sa=1&ei=wkL3Xb6VMZryrAH3tL2IAg&q=nature&oq=nature&gs_l=img.3..0i67l3j0j0i67l2j0l4.22803.25415..25672...1.0..1.218.2447.0j11j2......0....1..gws-wiz-img.....10..35i39j35i362i39j0i131.8wX4nH36Jh4&ved=0ahUKEwi-_sOz4rnmAhUaOSsKHXdaDyEQ4dUDCAc&uact=5']\n    start_urls=[\n        'https://www.google.com/search?biw=923&bih=759&tbm=isch&sxsrf=ACYBGNT1fy79gOV2O7XhJeKDIqQ814ST0w%3A1576485570813&sa=1&ei=wkL3Xb6VMZryrAH3tL2IAg&q=nature&oq=nature&gs_l=img.3..0i67l3j0j0i67l2j0l4.22803.25415..25672...1.0..1.218.2447.0j11j2......0....1..gws-wiz-img.....10..35i39j35i362i39j0i131.8wX4nH36Jh4&ved=0ahUKEwi-_sOz4rnmAhUaOSsKHXdaDyEQ4dUDCAc&uact=5'\n    ]\n\n    def parse(self,response):\n        I = ItemLoader(item=DownloadItem(),response=response)\n       \n        \n        img_url= response.xpath(\"//img/@src\").extract()\n        \n           \n        I.add_value('img_url',img_url)\n         \n        return I.load_item() \n        \n\nimage folder remains empty.", "issue_status": "Closed", "issue_reporting_time": "2019-12-16T08:54:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "13": {"issue_url": "https://github.com/scrapy/scrapy/issues/4232", "issue_id": "#4232", "issue_summary": "I have a problem with login in amazon using scrapy", "issue_description": "Kunal614 commented on Dec 15, 2019\nimport scrapy\nfrom scrapy.http import FormRequest\nfrom ..items import AmazonLoginItem\nfrom scrapy.utils.response import open_in_browser\n\nclass amazon(scrapy.Spider):\n    name='amazon'    \n    start_urls=[\n            'https://www.amazon.com/ap/signin?accountStatusPolicy=P1&clientContext=258-3200785-8273163&language=en_US&openid.assoc_handle=amzn_prime_video_desktop_us&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.mode=checkid_setup&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&openid.ns.pape=http%3A%2F%2Fspecs.openid.net%2Fextensions%2Fpape%2F1.0&openid.pape.max_auth_age=0&openid.return_to=https%3A%2F%2Fwww.primevideo.com%2Fauth%2Freturn%2Fref%3Dav_auth_ap%3F_encoding%3DUTF8%26location%3D%252Fref%253Dav_nav_sign_in'\n    ]\n    page_number=2  \n    def parse(self,response):\n            \n            token = response.css(\"form input::attr(value)\").extract_first()\n            print(token,\"my name osm\")\n            return FormRequest.from_response(response,formdata={\n                    'appActionToken':token,\n                    'Email':'*********',\n                    'password':'********'\n            },callback=self.start_scrapping)\n            \n            \n          \n    def start_scrapping(self,response):\n        open_in_browser(response)\n        item = AmazonLoginItem()\n        book_name= response.css('.a-color-base.a-text-normal').css('::text').extract()\n        book_author=response.css('.a-color-secondary .a-size-base+ .a-size-base').css('::text').extract()\n        book_price = response.css('.a-price-whole').css('::text').extract()\n        k=0\n        y=0\n        \n\n        for i in range(len(book_name)):\n            item['book_name']=book_name[i]\n            item['book_author']=book_author[k]\n            item['book_price']=book_price[y]\n            k+=1\n            y+=1\n            yield item\n        next_page='https://www.amazon.in/s?k=books&page=' + str(amazon.page_number) +'&qid=1575269122&ref=sr_pg_2' \n        if amazon.page_number<=100:\n            amazon.page_number+=1\n            yield response.follow(next_page , callback = self.start_scrapping)", "issue_status": "Closed", "issue_reporting_time": "2019-12-15T08:26:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "14": {"issue_url": "https://github.com/scrapy/scrapy/issues/4230", "issue_id": "#4230", "issue_summary": "Get CrawlerProcess tests into the coverage data", "issue_description": "Member\nGallaecio commented on Dec 13, 2019\nThe tests added at #4218 don\u2019t seem to be taken into account by coverage data, probably due to the usage of subprocess. We need to find a way to get this data taken into account in test coverage resports.", "issue_status": "Closed", "issue_reporting_time": "2019-12-13T13:15:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "15": {"issue_url": "https://github.com/scrapy/scrapy/issues/4229", "issue_id": "#4229", "issue_summary": "mail attachs tcmime***", "issue_description": "Contributor\nApuyuseng commented on Dec 13, 2019\n$scrapy shell\n2019-12-13 20:21:33 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: Book)\n2019-12-13 20:21:33 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.4, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.8.0 (v3.8.0:fa919fdf25, Oct 14 2019, 10:23:27) - [Clang 6.0 (clang-600.0.57)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform macOS-10.15.2-x86_64-i386-64bit\n.....\nIn [1]: from scrapy.mail import MailSender                                                                                       \n\nIn [2]:  mailer = MailSender.from_settings(crawler.settings)                                                                     \n\nIn [3]:  book = open('\u4e3a\u4e86\u5ab3\u5987\u53bb\u4fee\u4ed9.zip', 'rb')                                                                                 \n\nIn [4]: attachments = [('\u4e3a\u4e86\u5ab3\u5987\u53bb\u4fee\u4ed9.zip', \n   ...:                             'application/zip', book)]                                                                    \n\nIn [5]: mailer.send(to=['1173372284@qq.com'], \n   ...:                                     attachs=attachments, \n   ...:                                     subject=\"Convert\", \n   ...:                                     body = '', \n   ...:                                     mimetype='application/zip' \n   ...:                                     ) \n\nIn [6]: mailer.e2019-12-13 20:25:33 [parso.python.diff] DEBUG: diff parser start\n2019-12-13 20:25:33 [parso.python.diff] DEBUG: line_lengths old: 1; new: 1\n2019-12-13 20:25:33 [parso.python.diff] DEBUG: -> code[replace] old[1:1] new[1:1]\n2019-12-13 20:25:33 [parso.python.diff] DEBUG: parse_part from 1 to 1 (to 0 in part parser)\n2019-12-13 20:25:33 [parso.python.diff] DEBUG: diff parser end\n2019-12-13 20:25:35 [scrapy.mail] INFO: Mail sent OK: To=['1173372284@qq.com'] Cc=[] Subject=\"Convert\" Attachs=1                 \nget mail\nwhy attachs \u4e3a\u4e86\u5ab3\u5987\u53bb\u4fee\u4ed9.zip change to tcmime.954.1140.14065.bin", "issue_status": "Closed", "issue_reporting_time": "2019-12-13T12:39:22Z", "fixed_by": "#4239", "pull_request_summary": "Fix mail attachs tcmime *** (#4229)", "pull_request_description": "Contributor\nApuyuseng commented on Dec 17, 2019 \u2022\nedited by Gallaecio\nWhen the file name consists of alphanumeric characters, it is normal to receive the attachment name.\nHowever\uff0cHowever, problems will occur if the file name is changed to Chinese.\nThis has nothing to do with the file type\nFixes #4229", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-17T14:43:31Z", "files_changed": [["3", "scrapy/mail.py"]]}, "16": {"issue_url": "https://github.com/scrapy/scrapy/issues/4228", "issue_id": "#4228", "issue_summary": "Incorrect scrapy.Item when trying to download multiple files from a web page", "issue_description": "zyuchuan commented on Dec 12, 2019\nDescription\nI'm using scrapy.pipelines.files.FilesPipeline to download all midi file from midiworld.com, this is a simple task and there millons of sample code on internet, so no doubt my spider works fine, until I check the log:\n2019-12-12 12:48:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.midiworld.com/search/?q=classic>\n{'author': 'Debussy',\n 'file_urls': ['https://www.midiworld.com/download/4248'],\n 'files': [{'checksum': 'fee5b02d07085556bfc78a074096f188',\n            'path': 'full/f92f7ce78a843b01c0dc6410eef55e07b76dfa30',\n            'url': 'https://www.midiworld.com/download/4240'}],\n 'title': 'Project'}\n2019-12-12 12:48:02 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.midiworld.com/search/?q=classic>\n{'author': 'Debussy',\n 'file_urls': ['https://www.midiworld.com/download/4248'],\n 'files': [{'checksum': 'ff8216574c20acde7f68f00723aea619',\n            'path': 'full/c84495f99f0c9a4b9fb111187db807e551919609',\n            'url': 'https://www.midiworld.com/download/4237'}],\n 'title': 'Project'}\nCheck the files_urls and files::url, they are different, which is not correct!\nThe problem cames from scrapy.piplelines.MediaPipeline:\nclass MediaPipeline(object):\n    ...\n    def process_item(self, item, spider):\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info) for r in requests]\n        dfd = DeferredList(dlist, consumeErrors=1)\n        return dfd.addCallback(self.item_completed, item, info)\nwhen item_completed get called, the item passed to it is always the item that the pipe line is currently processing, not the item registered for this callback.\nI fixed this issue by copying the item before passing it to callback, like this:\ndef process_item(self, item, spider):\n    info = self.spiderinfo\n    requests = arg_to_iter(self.get_media_requests(item, info))\n    dlist = [self._process_request(r, info) for r in requests]\n    dfd = DeferredList(dlist, consumeErrors=1)\n    item_copied = copy.deepcopy(item)\n    return dfd.addCallback(self.item_completed, item_copied, info)\nBut this looks like a workaround, I don't really like it. Actually I think this is more a Twisted bug than a scrapy bug. Unfortunately my scheduel is tight, I don't have time doing further investigation, so I report the issue here. I'll try to fix it as soon as I have time.\nSteps to Reproduce\nIt's easy to reproduce, just run this spider:\nclass MidiWorldSpider(scrapy.Spider):\n    ...\n    def start_requests(self):\n        yield scrapy.Request(url=\"https://www.midiworld.com\", callback=self.parse)\n\n    def parse(self, response):\n        midis_cloud = response.css('.midis-cloud').xpath('./ul/li')\n        midi = midis_cloud[0]\n        style_page = midi.css('li a::attr(href)').get()\n        style_page = response.urljoin(style_page)\n        yield scrapy.Request(style_page, callback=self.parse_style_page)\n\n    def parse_style_page(self, response):\n        item = MidiItem()\n        uls = response.css(\"#page ul\")\n        midi_li = uls[0].css('li')\n        for li in midi_li:\n            download_link = li.css('a::attr(href)').get()\n            item['file_urls'] = [download_link]\n            yield item\n\nclass MidiItem(scrapy.Item):\n    file_urls = scrapy.Field()\n    files = scrapy.Field()\nReproduces how often:\nIt happens every time you run the spider, unless you set CONCURRENT_ITEMS = 1\nVersions\nScrapy       : 1.8.0\nlxml         : 4.4.2.0\nlibxml2      : 2.9.5\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.7.0\nPython       : 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 22:01:29) [MSC v.1900 64 bit (AMD64)]\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1c  28 May 2019)\ncryptography : 2.7\nPlatform     : Windows-10-10.0.16299-SP0", "issue_status": "Closed", "issue_reporting_time": "2019-12-12T05:19:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "17": {"issue_url": "https://github.com/scrapy/scrapy/issues/4224", "issue_id": "#4224", "issue_summary": "Crawled (404) But in the Browser is OK", "issue_description": "BLACKDONGG commented on Dec 9, 2019\nWhen I use the scrapy to crawl the website get a 404!\nBut I have set USER_AGENT,\nThis is my scrapy spider's code\uff1a\n-- coding: utf-8 --\nimport scrapy\nclass TestSpider(scrapy.Spider):\nname = 'Test'\nallowed_domains = ['https://y.qq.com']\nstart_urls = ['https://y.qq.com/n/yqq/playlist/7323933054.html#stat=y_new.playlist.pic_click']\nheader = {\n\"Host\": \"c.y.qq.com\",\n\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0\",\n\"Accept\": \"/\",\n\"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n\"Accept-Encoding\": \"gzip, deflate, br\",\n\"Connection\": \"keep-alive\",\n\"Referer\": \"https://y.qq.com/n/yqq/playlist/7285677678.html\",\n}\ncookie = {\n\"token\": \"123455\",\n\"host\": \"tools.bugscaner.com\",\n\"username\": \"admin Cookie: pgv_pvid=2208230034\",\n\"pgv_pvi\": \"3287928832\",\n\"ts_refer\": \"cn.bing.com/\",\n\"ts_uid\": \"3833260354\",\n\"LW_sid\": \"31f576o7u3b0A7W190C3B8w3E2\",\n\"LW_uid\": \"h1P5v6N7g3o0K619W3h6g3D5N0\",\n\"eas_sid\": \"b1u5x6h7D3k066F9q3N623K7z1\",\n\"RK\": \"cLjFSxBcMN\",\n\"ptcz\": \"58600459bc7cdadbe5d6c429daaa9f72f9918830ba490707e068292ea1879f6f\",\n\"yqq_stat\": \"0\",\n\"pgv_info\": \"ssid=s8260733988\",\n\"ts_last\": \"y.qq.com/portal/playlist.html\",\n\"pgv_si\": \"s709651456\",\n\"password\": \"7a57a5a743894a0e\",\n}\ndef start_requests(self):\n    yield scrapy.Request(url=self.start_urls[0],callback=self.parse,headers=self.header,cookies=self.cookie,method=\"POST\")\n\ndef parse(self, response):\n    print(response.text)\nand the error:\nDEBUG: Crawled (404) <POST https://y.qq.com/n/yqq/playlist/7323933054.html#stat=y_new.pla\nylist.pic_click> (referer: https://y.qq.com/n/yqq/playlist/7285677678.html)\n2019-12-10 00:04:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://y.qq.com/n/yqq/playlist/7323933054.h\ntml>: HTTP status code is not handled or not allowed", "issue_status": "Closed", "issue_reporting_time": "2019-12-09T16:14:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "18": {"issue_url": "https://github.com/scrapy/scrapy/issues/4223", "issue_id": "#4223", "issue_summary": "scrapy crawl quotes - not working on MAC", "issue_description": "abtisam commented on Dec 8, 2019\nI am trying to run demo project for the first time but it will not executed successfully.\nI got given below error.\n(base) 192:tutorial air$ scrapy crawl quotes\n2019-12-08 19:30:17 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: tutorial)\n2019-12-08 19:30:17 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug 13 2019, 15:17:50) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d 10 Sep 2019), cryptography 2.7, Platform Darwin-18.7.0-x86_64-i386-64bit\n2019-12-08 19:30:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders']}\n2019-12-08 19:30:17 [scrapy.extensions.telnet] INFO: Telnet Password: f1a7db4605320b1e\n2019-12-08 19:30:17 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.memusage.MemoryUsage',\n'scrapy.extensions.logstats.LogStats']\nUnhandled error in Deferred:\n2019-12-08 19:30:17 [twisted] CRITICAL: Unhandled error in Deferred:\nTraceback (most recent call last):\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 184, in crawl\nreturn self._crawl(crawler, *args, **kwargs)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 188, in _crawl\nd = crawler.crawl(*args, **kwargs)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 1613, in unwindGenerator\nreturn _cancellableInlineCallbacks(gen)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 1529, in _cancellableInlineCallbacks\n_inlineCallbacks(None, g, status)\n--- ---\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\nresult = g.send(result)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 86, in crawl\nself.engine = self._create_engine()\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 111, in _create_engine\nreturn ExecutionEngine(self, lambda _: self.stop())\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/core/engine.py\", line 69, in init\nself.downloader = downloader_cls(crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/core/downloader/init.py\", line 86, in init\nself.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/middleware.py\", line 53, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/middleware.py\", line 35, in from_settings\nmw = create_instance(mwcls, settings, crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 142, in create_instance\nreturn objcls.from_crawler(crawler, *args, **kwargs)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/downloadermiddlewares/robotstxt.py\", line 39, in from_crawler\nreturn cls(crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/downloadermiddlewares/robotstxt.py\", line 35, in init\nself._parserimpl.from_crawler(self.crawler, b'')\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/robotstxt.py\", line 120, in from_crawler\no = cls(robotstxt_body, spider)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/robotstxt.py\", line 112, in init\nfrom protego import Protego\nbuiltins.ModuleNotFoundError: No module named 'protego'\n2019-12-08 19:30:17 [twisted] CRITICAL:\nTraceback (most recent call last):\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\nresult = g.send(result)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 86, in crawl\nself.engine = self._create_engine()\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 111, in _create_engine\nreturn ExecutionEngine(self, lambda _: self.stop())\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/core/engine.py\", line 69, in init\nself.downloader = downloader_cls(crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/core/downloader/init.py\", line 86, in init\nself.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/middleware.py\", line 53, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/middleware.py\", line 35, in from_settings\nmw = create_instance(mwcls, settings, crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 142, in create_instance\nreturn objcls.from_crawler(crawler, *args, **kwargs)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/downloadermiddlewares/robotstxt.py\", line 39, in from_crawler\nreturn cls(crawler)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/downloadermiddlewares/robotstxt.py\", line 35, in init\nself._parserimpl.from_crawler(self.crawler, b'')\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/robotstxt.py\", line 120, in from_crawler\no = cls(robotstxt_body, spider)\nFile \"/Users/air/opt/anaconda3/lib/python3.7/site-packages/scrapy/robotstxt.py\", line 112, in init\nfrom protego import Protego\nModuleNotFoundError: No module named 'protego'\nSteps to Reproduce\nscrapy startproject tutorial\nVersions\nScrapy : 1.8.0\nlxml : 4.4.1.0\nlibxml2 : 2.9.9\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.21.0\nTwisted : 19.10.0\nPython : 3.7.4 (default, Aug 13 2019, 15:17:50) - [Clang 4.0.1 (tags/RELEASE_401/final)]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1d 10 Sep 2019)\ncryptography : 2.7\nPlatform : Darwin-18.7.0-x86_64-i386-64bit\n(base) 192:tutorial air$", "issue_status": "Closed", "issue_reporting_time": "2019-12-08T14:35:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "19": {"issue_url": "https://github.com/scrapy/scrapy/issues/4220", "issue_id": "#4220", "issue_summary": "cookiejar not updated while the request retried by RetryMiddleware", "issue_description": "imaxwen commented on Dec 8, 2019 \u2022\nedited\nDescription\nEnabled both RetryMiddleware and CookiesMiddleware in my project,\nEvery time after retried, the cookie not updated\nSteps to Reproduce\nsettings.py\n# settings.py\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'ToolkitSpiders.middlewares.saicmotor.RetryLoginMiddleware': 543\n}\n\n# retry_login_middleware.py\nclass RetryLoginMiddleware(RetryMiddleware):\n\n    def process_response(self, request, response, spider):\n        retry_times = request.meta.get('retry_times', 0)\n        if request.meta.get('login_request', False):\n            # \u767b\u5f55\u7ed3\u679c\u9875\u9762\u6821\u9a8c\n            login_errs = ['Code Error', 'Wrong Username/Password']\n            match_errs = [err for err in login_errs if err in response.text]\n            if len(match_errs) > 0:\n                err_msg = match_errs[0]\n                return self._retry(request, err_msg, spider)\n            else:\n                spider.logger.info(\"login succeed!\")\n\n        return response\nDo I need to update the cookie manually? or any thing I missed?\nPlease Help!\nExpected behavior: Auto update cookies in cookiejar after request retried.\nReproduces how often: Every time while the RetryMiddleware fired.\nVersions\n1.8.0\nAdditional context", "issue_status": "Closed", "issue_reporting_time": "2019-12-08T09:30:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "20": {"issue_url": "https://github.com/scrapy/scrapy/issues/4204", "issue_id": "#4204", "issue_summary": "[HELP] scraper not scraping field \u201cDescription\u201d", "issue_description": "Davetheraveuk commented on Dec 1, 2019\nI wish to add an extra field from the website the scraper is scraping from.\nThe column header \"Description\" is created in the CSV database but nothing is scraped.\nBelow is the contents of scraper.py\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom pydispatch import dispatcher\nfrom scrapy.signalmanager import SignalManager\nimport csv,re\nfrom scrapy import signals\nclass Rapid7(scrapy.Spider):\n    name = 'vulns'\n    allowed_domains = ['rapid7.com']\n    main_url = 'https://www.rapid7.com/db/?q=&type=nexpose&page={}'\n    #start_urls = ['https://www.rapid7.com/db/vulnerabilities']\n    keys = ['Published','CVEID', 'Added', 'Modified', 'Related', 'Severity', 'CVSS', 'Created', 'Solution', 'References', 'Description', 'URL']\n    def __init__(self):\n        SignalManager(dispatcher.Any).connect(receiver=self._close, signal=signals.spider_closed)\n    def start_requests(self):\n        for i in range(1,10):\n            url = self.main_url.format(i)\n            yield scrapy.Request(url,callback=self.parse)\n    def parse(self, response):\n        flag = True\n        temp = response.xpath('//div[@class=\"vulndb__intro-content\"]/p/text()').extract_first()\n        if temp:\n            if temp.strip()=='An error occurred.':\n                flag= False\n        temp = [i for i in response.xpath('//*[@class=\"results-info\"]/parent::div/p/text()').extract()if i.strip()]\n        if len(temp)==1:\n            flag= False\n        if flag:\n            for article in response.xpath('//*[@class=\"vulndb__results\"]/a/@href').extract():\n                yield scrapy.Request(response.urljoin(article), callback=self.parse_article, dont_filter=True)\n\n    def parse_article(self,response):\n        item=dict()\n        item['Published'] = item['Added'] = item['Modified'] = item['Related'] = item['Severity'] = item['Description'] =''\n        r=response.xpath('//h1[text()=\"Related Vulnerabilities\"]/..//a/@href').extract()\n        temp = response.xpath('//meta[@property=\"og:title\"]/@content').extract_first()\n        item['CVEID'] = ''\n        try:\n            temp2 = re.search('(CVE-.*-\\d*)',temp).groups()[0]\n            if \":\" in temp2:\n                raise KeyError\n        except:\n            try:\n                temp2 = re.search('(CVE-.*):',temp).groups()[0]\n            except:\n                temp2 = ''\n        if temp2:\n            item['CVEID'] = temp2.replace(': Important',\"\").replace(')','')\n        table = response.xpath('//section[@class=\"tableblock\"]/div')\n        for row in table:\n            header = row.xpath('header/text()').extract_first()\n            data = row.xpath('div/text()').extract_first()\n            item[header]=data\n        temp = [i for i in response.xpath('//div[@class=\"vulndb__related-content\"]//text()').extract() if i.strip()]\n        for ind,i in enumerate(temp):\n            if \"CVE\" in i:\n                temp[ind] = i.replace(' ','')\n\n        item['Related']= \", \".join(temp) if temp else \"\"\n        temp2= [i for i in response.xpath('//h4[text()=\"Solution(s)\"]/parent::*/ul/li/text()').extract() if i.strip()]\n        item['Solution'] =\", \".join(temp2) if temp2 else ''\n        temp3 = [i for i in response.xpath('//h4[text()=\"References\"]/parent::*/ul/li/text()').extract() if i.strip()]\n        item['References'] = \", \".join(temp3) if temp3 else ''\n        temp4 = [i for i in response.xpath('//h4[text()=\"Description\"]/parent::*/ul/li/text()').extract() if i.strip()]\n        item['Description'] = \", \".join(temp4) if temp4 else ''\n        item['URL'] = response.request.url\n        new_item=dict()\n        for key in self.keys:\n            if key not in list(item.keys()):\n                new_item[key] = ''\n            else:\n                new_item[key]=item[key]\n        yield new_item\n\n    def _close(self):\n        print(\"Done Scraping\")\nThank you.\nDavid.", "issue_status": "Closed", "issue_reporting_time": "2019-12-01T17:30:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "21": {"issue_url": "https://github.com/scrapy/scrapy/issues/4202", "issue_id": "#4202", "issue_summary": "Does scrapy view uses active project settings", "issue_description": "vionemc commented on Dec 1, 2019\nDoes scrapy view uses active project settings? Especially middleware. I want to use scrapy fake user agent and scrapy crawlera on scrapy view.", "issue_status": "Closed", "issue_reporting_time": "2019-11-30T23:31:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "22": {"issue_url": "https://github.com/scrapy/scrapy/issues/4200", "issue_id": "#4200", "issue_summary": "How to limit memory use when download big file", "issue_description": "lizhaode commented on Nov 28, 2019 \u2022\nedited\nDescription\nwhen download big file, memory use high\nSteps to Reproduce\nin my scenario, I will download video files, so the file is big, but my machine's memory only 1G with debian9\nso how to limit scrapy memory use?\nThx", "issue_status": "Closed", "issue_reporting_time": "2019-11-28T12:51:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "23": {"issue_url": "https://github.com/scrapy/scrapy/issues/4189", "issue_id": "#4189", "issue_summary": "How to print Chinese/Japanese while yielding items in Scrapy?", "issue_description": "mani619cash commented on Nov 24, 2019\nI tried yielding scrapy.Item from Pipeline class and also tried yielding dict directly\nbut in console, it shows like \\uc18c\\uac1c\\uae30\\uc0ac\nWhereas I print with logging.info(item) it shows Chinese/Japanese correctly\ni tried\nLOG_ENCODING = 'utf-8'\nin settings.py but does not work\nSo, my question is, how do I make Scrapy print Chinese/Japanese characters in console when yielding items?", "issue_status": "Closed", "issue_reporting_time": "2019-11-24T04:35:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "24": {"issue_url": "https://github.com/scrapy/scrapy/issues/4187", "issue_id": "#4187", "issue_summary": "Add new tests for the proxy noconnect mode", "issue_description": "Contributor\nwRAR commented on Nov 21, 2019 \u2022\nedited by Gallaecio\nThis is related to #2545 and #4114.\nCurrently we cannot test the proxy noconnect mode because we use mitmproxy as a test proxy and it doesn't seem to support this mode in the current version. Because of that, the tests for this mode are currently disabled.\nAs described in a comment near the old test functions, now marked as expected failures:\nThe noconnect mode isn't supported by the current mitmproxy, it returns\n\"Invalid request scheme: https\" as it doesn't seem to support full URLs in GET at all,\nand it's not clear what behavior is intended by Scrapy and by mitmproxy here.\nmitmproxy/mitmproxy#848 may be related.\nThe Scrapy noconnect mode was required, at least in the past, to work with Crawlera,\nand scrapy-plugins/scrapy-crawlera#44 seems to be related.", "issue_status": "Closed", "issue_reporting_time": "2019-11-21T18:07:01Z", "fixed_by": "#4198", "pull_request_summary": "Deprecate the HTTPS proxy noconnect mode.", "pull_request_description": "Contributor\nwRAR commented on Nov 28, 2019\nThis deprecates the noconnect mode for HTTPS proxies and removes related tests. As the mode was made for old Crawlera, it doesn't look like anything else needs it.\nscrapy_crawlera version is important because before 1.3.0 it added '?noconnect' to all Crawlera URLs automatically.\nCloses: #4187 (as not needed anymore).", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-18T11:11:25Z", "files_changed": [["7", "scrapy/core/downloader/handlers/http11.py"], ["9", "tests/test_downloader_handlers.py"], ["26", "tests/test_proxy_connect.py"]]}, "25": {"issue_url": "https://github.com/scrapy/scrapy/issues/4177", "issue_id": "#4177", "issue_summary": "FIFO crawling order", "issue_description": "mredaelli commented on Nov 19, 2019\nDescription\nI have a spider like for a website that has a tree of content like:\nRoot\n|- N1\n|   \\_N1_1 -> N1_1_1,  N1_1_2,  N1_1_3\n|   \\_N1_2 -> N1_2_1,  N1_2_2,  N1_2_3\n|- N2\n|   \\_N2_1 -> N2_1_1,  N2_1_2,  N2_1_3\n|   \\_N2_2 -> N2_2_1,  N2_2_2,  N2_2_3\n| ...\nMy spider is like this:\nclass Consultations(Spider):\n    start_urls = [  \"root\"  ]\n\n    def parse(self, resp: Response):\n       # parses Nx\n        for link in resp.xpath(\"Nx_x\").get_all():\n            yield Request(url=link.xpath(\"./@href\").get(), callback=self.parse_list, priority=1)\n\n    def parse_list(self, resp: Response):\n       # parses Nx_x\n        for item in resp.xpath(\"//table[@class='online_datei']\").get_all():\n                yield Request(url=\"...\"\n                    callback=self.parse_document,\n                    priority=2,\n                )\n    def parse_document(self, resp: Response):\n       # parses Nx_x_x\n       yield {\"something\":\"something\"}\nand I added the settings for FIFO crawling as in the FAQ (including the CONCURRENT_xxx=1).\n(BTW, is there a typo? CONCURRENT_REQUESTS_PER_DOMAIN appears twice: maybe it was supposed to be CONCURRENT_REQUESTS_PER_IP?)\nExpected behavior:\nRequests being fired in the order Root, N1, N1_1, N1_1_1, N1_1_2, N1_1_3, N1_2, N1_2_1, N1_2_2, N1_2_3, ...\nActual behavior:\nI still see the requests for all the Nx processed before the Nx_x, etc.\nVersions\nScrapy 1.6.0", "issue_status": "Closed", "issue_reporting_time": "2019-11-19T13:35:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "26": {"issue_url": "https://github.com/scrapy/scrapy/issues/4168", "issue_id": "#4168", "issue_summary": "For gSoc 2020", "issue_description": "Kunal614 commented on Nov 16, 2019\nCan anyone give me idea , how i select scrapy in gsoc 2020, As i think it is very intresting stuff.", "issue_status": "Closed", "issue_reporting_time": "2019-11-16T15:44:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "27": {"issue_url": "https://github.com/scrapy/scrapy/issues/4166", "issue_id": "#4166", "issue_summary": "Allow objects instead of import strings in settings", "issue_description": "Member\nGallaecio commented on Nov 15, 2019\nSome settings, like LOG_FORMATTER, expect a import string as value.\nI think it would be great if they also allowed the corresponding object itself.\nFor example, let users that are doing this:\n    class LogFormatter:\n        pass\n\n    class MySpider:\n        custom_settings = {'LOG_FORMATTER': 'my_project.spiders.my_spider.LogFormatter'}\nTo do this instead:\n    class LogFormatter:\n        pass\n\n    class MySpider:\n        custom_settings = {'LOG_FORMATTER': LogFormatter}\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2019-11-15T12:15:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "28": {"issue_url": "https://github.com/scrapy/scrapy/issues/4160", "issue_id": "#4160", "issue_summary": "Question: Exit process without stopping reactor", "issue_description": "vella-nicholas commented on Nov 14, 2019\nI wish to exit a scraping process without calling reactor.stop(). The reason is that I am deploying a spider in a cloud function. If an instance is reused I am getting ReactorNotRestartable since I am stopping the reactor. If I do not stop the reactor, the call times out.\nI wish to exit without stopping the reactor. Any help on this please?\ndef crawl(url):\n    configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n    runner = CrawlerRunner(get_project_settings())\n\n    d = runner.crawl(MySpider, url=url)\n    d.addBoth(lambda _: just_stop())\n    reactor.run()\n\n\ndef just_stop():\n    print(\"I Reached here\")\n    return 'OK'", "issue_status": "Closed", "issue_reporting_time": "2019-11-14T09:00:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "29": {"issue_url": "https://github.com/scrapy/scrapy/issues/4158", "issue_id": "#4158", "issue_summary": "retry a request,but can't chang ip", "issue_description": "Jerry007s commented on Nov 13, 2019\nDescription\nI use dynamic proxy, but ip can't change when retry this request.\nSteps to Reproduce\nI simulated this process. forcibly 200 status_code to retry. e.g.RETRY_HTTP_CODES = [200, 301, 302]. So it will be retry again and again until give up this request.\nBut even with the proxy can not be changed for each request ip, unless restart or begin another request.\nspider.py\n    def start_requests(self):\n        yield scrapy.Request(url='https://httpbin.org/get')\nmiddlewares.py\nclass RandomProxyMiddleware(object):\n    def __init__(self, proxies):\n        self.proxies = proxies\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        return cls(proxies=settings.get('PROXIES'))\n\n    def process_request(self, request, spider):\n        request.meta['proxy'] = self.proxies\n\nclass TestProxyMiddleware(object):\n    def process_response(self, request, response, spider):\n        spider.logger.info(response.text)\n        return response\nExpected behavior: [What you expect to happen]\nPlease notice origin\n2019-11-13 18:41:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://httpbin.org/get> (failed 1 times): 200 OK\n2019-11-13 18:41:05 [broker] INFO: {\n  \"args\": {}, \n  \"headers\": {\n    \"Host\": \"httpbin.org\", \n  \"origin\": \"180.109.64.86, 180.109.64.86\",\n  \"url\": \"https://httpbin.org/get\"\n}\n\n2019-11-13 18:41:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://httpbin.org/get> (failed 2 times): 200 OK\n2019-11-13 18:41:05 [broker] INFO: {\n  \"args\": {}, \n  \"headers\": {\n    \"Host\": \"httpbin.org\", \n  \"origin\": \"89.10.64.6, 89.10.64.6\",\n  \"url\": \"https://httpbin.org/get\"\n}\nActual behavior: [What actually happens]\nPlease notice origin\n2019-11-13 18:41:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://httpbin.org/get> (failed 1 times): 200 OK\n2019-11-13 18:41:05 [broker] INFO: {\n  \"args\": {}, \n  \"headers\": {\n    \"Host\": \"httpbin.org\", \n  \"origin\": \"180.109.64.86, 180.109.64.86\",\n  \"url\": \"https://httpbin.org/get\"\n}\n\n2019-11-13 18:41:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://httpbin.org/get> (failed 2 times): 200 OK\n2019-11-13 18:41:05 [broker] INFO: {\n  \"args\": {}, \n  \"headers\": {\n    \"Host\": \"httpbin.org\", \n  \"origin\": \"180.109.64.86, 180.109.64.86\",\n  \"url\": \"https://httpbin.org/get\"\n}\nVersions\nScrapy       : 1.6.0\nlxml         : 4.4.0.0\nlibxml2      : 2.9.9\ncssselect    : 1.0.3\nparsel       : 1.5.1\nw3lib        : 1.19.0\nTwisted      : 17.9.0\nPython       : 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) - [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018)\ncryptography : 2.2.2\nPlatform     : Darwin-18.7.0-x86_64-i386-64bit", "issue_status": "Closed", "issue_reporting_time": "2019-11-13T11:10:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "30": {"issue_url": "https://github.com/scrapy/scrapy/issues/4154", "issue_id": "#4154", "issue_summary": "Hi All,", "issue_description": "oscarrobertson commented on Nov 13, 2019\nSummary\nOne paragraph explanation of the feature.\nMotivation\nWhy are we doing this? What use cases does it support? What is the expected outcome?\nDescribe alternatives you've considered\nA clear and concise description of the alternative solutions you've considered. Be sure to explain why Scrapy's existing customizability isn't suitable for this feature.\nAdditional context\nAny additional information about the feature request here.", "issue_status": "Closed", "issue_reporting_time": "2019-11-13T09:12:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "31": {"issue_url": "https://github.com/scrapy/scrapy/issues/4145", "issue_id": "#4145", "issue_summary": "only version 1.8.0 robots.txt forbidden", "issue_description": "HisakaKoji commented on Nov 12, 2019\nDescription\nWhen I scrapyied the url \"https://www.walkerplus.com/\" with scrapy ,\nit is OK in version 1.7 , but it is forbidden by robots.txt in version 1.8.\nThe robots.txt\n`user-agent: *\ndisallow: http://ms-web00.walkerplus.com/\ndisallow: http://www-origin.walkerplus.com/\ndisallow: http://walkerplus.jp/\ndisallow: http://walkerplus.net/\ndisallow: https://ms.walkerplus.com/\nuser-agent: twitterbot\ndisallow:`\nSteps to Reproduce\nscrapy \"https://www.walkerplus.com/\" with version1.8 scrapy\nExpected behavior: [What you expect to happen]\nIt is not forbidden.\nActual behavior: [What actually happens]\nIt is forbidden.\nReproduces how often: [What percentage of the time does it reproduce?]\nIt is 100% reproduced.\nVersions\nScrapy : 1.8.0\nlxml : 4.4.1.0\nlibxml2 : 2.9.9\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.21.0\nTwisted : 19.10.0\nPython : 3.6.8 (default, Oct 7 2019, 12:59:55) - [GCC 8.3.0]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1 11 Sep 2018)\ncryptography : 2.1.4\nPlatform : Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-Ubuntu-18.04-bionic", "issue_status": "Closed", "issue_reporting_time": "2019-11-12T01:11:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "32": {"issue_url": "https://github.com/scrapy/scrapy/issues/4144", "issue_id": "#4144", "issue_summary": "Is image download disabled by default ?", "issue_description": "ishandutta2007 commented on Nov 11, 2019\nI have written a spider which consumes almost 2 GB of data per hour. Now I want to save my data consumption, images are of no use for me, so want to make sure they not being fetched.", "issue_status": "Closed", "issue_reporting_time": "2019-11-11T09:58:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "33": {"issue_url": "https://github.com/scrapy/scrapy/issues/4136", "issue_id": "#4136", "issue_summary": "download_* meta keys are not available if using cache", "issue_description": "turicas commented on Nov 7, 2019\nDescription\nI'm using download_latency in spider's parse method and it's not available (generating a KeyError) if the response is taken from the cache.\nSteps to Reproduce\nCreate a simple spider which uses response.meta[\"download_latency\"] inside parse, called latency.py:\nimport scrapy\n\nclass LatencySpider(scrapy.Spider):\n    name = \"latency\"\n    start_urls = [\"https://brasil.io/home\", \"https://brasil.io/datasets\", \"https://brasil.io/manifesto\"]\n\n    def parse(self, response):\n        yield {\n            \"url\": response.url,\n            \"latency\": response.meta[\"download_latency\"],\n        }\nRun the spider with cache enabled to store values in the cache:\nscrapy runspider -s HTTPCACHE_ENABLED=true latency.py -o latency.csv\nIt's going to run successfully since there's no cache yet.\nRun the spider again (using the same command), so it will use the cache and crashes:\n2019-11-07 10:19:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://brasil.io/home> (referer: None)\nTraceback (most recent call last):\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/turicas/projects/rows/latency.py\", line 10, in parse\n    \"latency\": response.meta[\"download_latency\"],\nKeyError: 'download_latency'\n2019-11-07 10:19:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://brasil.io/datasets> (referer: None)\nTraceback (most recent call last):\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/turicas/projects/rows/latency.py\", line 10, in parse\n    \"latency\": response.meta[\"download_latency\"],\nKeyError: 'download_latency'\n2019-11-07 10:19:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://brasil.io/manifesto> (referer: None)\nTraceback (most recent call last):\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/turicas/software/pyenv/versions/3.7.3/envs/rows/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/turicas/projects/rows/latency.py\", line 10, in parse\n    \"latency\": response.meta[\"download_latency\"],\nKeyError: 'download_latency'\nExpected behavior: all automatically-created meta keys available to parse in a cache miss should be also available in a cache hit.\nActual behavior: the automatically-created meta keys are not available if response is taken from the cache.\nReproduces how often: always.\nVersions\nScrapy       : 1.8.0\nlxml         : 4.4.1.0\nlibxml2      : 2.9.9\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.7.0\nPython       : 3.7.5 (default, Oct 30 2019, 21:03:20) - [GCC 9.2.1 20191027]\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019)\ncryptography : 2.8\nPlatform     : Linux-4.19.0-6-amd64-x86_64-with-debian-bullseye-sid", "issue_status": "Closed", "issue_reporting_time": "2019-11-07T13:24:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "34": {"issue_url": "https://github.com/scrapy/scrapy/issues/4134", "issue_id": "#4134", "issue_summary": "replace the twisted reactor to pollreactor", "issue_description": "SyuuGenn commented on Nov 6, 2019\nIt's related to #2905. When I'm using scrapy, I got the error \"Failure instance: Traceback: <class 'ValueError'>: filedescriptor out of range in select()\". So I want to try replacing the twisted reactor to pollreactor. However, I cannot use my project settings. When I run\n`from twisted.internet import pollreactor\npollreactor.install()\nfrom twisted.internet import reactor\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.project import get_project_settings\nrunner = CrawlerRunner(get_project_settings())`\nit raises the error \"ReactorAlreadyInstalledError(\"reactor already installed\")\ntwisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\".\nCan you help me to fix this?\nMy environment is MacOS Catalina 10.15.1, Python 3.8.0, twisted 19.7.0, and scrapy 1.8.0.", "issue_status": "Closed", "issue_reporting_time": "2019-11-06T13:41:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "35": {"issue_url": "https://github.com/scrapy/scrapy/issues/4133", "issue_id": "#4133", "issue_summary": "Handle it gracefully when start_url is used instead of start_urls", "issue_description": "Member\nGallaecio commented on Nov 6, 2019\nOver the last year I\u2019ve seen a few cases (recent example) of this, people missing the s at the end of the start_urls.\nIt may be nice to find a way to gracefully let the developer know where the issue is, why there is no crawling happening.", "issue_status": "Closed", "issue_reporting_time": "2019-11-06T12:40:45Z", "fixed_by": "#4170", "pull_request_summary": "Raise error when start_url found instead of start_urls.", "pull_request_description": "Contributor\nmabelvj commented on Nov 18, 2019\nFixes #4133\nRaise AttributeError error when empty 'start_urls' and 'start_url' found. Added test.", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-05T12:47:04Z", "files_changed": [["5", "scrapy/spiders/__init__.py"], ["8", "tests/test_spider.py"]]}, "36": {"issue_url": "https://github.com/scrapy/scrapy/issues/4127", "issue_id": "#4127", "issue_summary": "Hi~ How can I find 'Scrapy Docs' in 'Transifex' manager or administrator ?", "issue_description": "lionking6792 commented on Nov 5, 2019 \u2022\nedited by Gallaecio\nHi I'm looking for\nhttps://www.transifex.com/scrapy-translation-team/scrapy-docs-translation/\nmanager or administrator..\nI am interested in translating scrapy docs in Korean but, nobody seems to response Transifex.\nHow can I get contact?\nIf anybody knows it Please reply to me..\nThanks for reading :)\nRelated to #3511 and #4105.", "issue_status": "Closed", "issue_reporting_time": "2019-11-05T14:05:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "37": {"issue_url": "https://github.com/scrapy/scrapy/issues/4124", "issue_id": "#4124", "issue_summary": "Fix wrong fact in JOBDIR documentation about requests needing to be pickle-serializable", "issue_description": "Member\nGallaecio commented on Nov 5, 2019 \u2022\nedited\nThe documentation about using JODBIR says that requests need to be serializable with pickle.\nBut, thanks to feedback from @kmike, now I know that their callback and errback methods do not need to be pickle-serializable as long as they are spider methods.\nThe documentation should be clear about this.\nRelated to #4125.", "issue_status": "Closed", "issue_reporting_time": "2019-11-04T20:19:06Z", "fixed_by": "#4139", "pull_request_summary": "Improve the details about request serialization requirements for JOBDIR", "pull_request_description": "Member\nGallaecio commented on Nov 7, 2019\nFixes #4124", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-08T17:49:34Z", "files_changed": [["31", "docs/topics/jobs.rst"]]}, "38": {"issue_url": "https://github.com/scrapy/scrapy/issues/4120", "issue_id": "#4120", "issue_summary": "Allow failing on potential data loss to trigger a retry", "issue_description": "royahsan commented on Nov 3, 2019 \u2022\nedited\nDescription\nBy default settings of DOWNLOAD_FAIL_ON_DATALOSS implemented in #2590, whenever ResponseFailed([_DataLoss]) error occurs, it should be raised. But very similar error PotentialDataLoss cannot be raised or retried.\nWe pass all such responses through to the callback function after adding the ['partial'] flag to the response object. This is where we add the flag after checking the PotentialDataLoss.\nelif reason.check(PotentialDataLoss):\n    self._finished.callback((self._txresponse, body, ['partial']))\nNow this prevents the retry middleware to step in and retry (depending on settings) the failed request.\nSteps to Reproduce\nTry to Fetch the following URL for around 150 times and somewhere in between the host server starts sending the partial/bad content-length header resulting in PotentialDataLoss error and addition of 'partial' to the flags.\nURL: https://ecorp.azcc.gov/BusinessSearch/BusinessInfo?entityNumber=21816333\nThe above URL sometimes return a 404 before providing a response with missing Content-Length which consequently results in DataLoss / PotentialDataLoss.\nExpected behavior:\nResponseFailed([_PotentialDataLoss]) exception should be raised.\nActual behavior:\nPartial response passes through the engine to the callback function after adding 'partial' flag.\nFollowing is one such example log generated by the example url above.\nDEBUG: scrapy.core.engine: _on_success:  Crawled (200) <GET https://ecorp.azcc.gov/BusinessSearch/BusinessInfo?entityNumber=21816333> ['partial']\nReproduces how often:\nEvery time when the server's response is missing 'Content-Length' header.\nVersions\nScrapy : 1.7.3\nlxml : 4.4.1.0\nlibxml2 : 2.9.5\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.21.0\nTwisted : 19.7.0\nPython : 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1c 28 May 2019)\ncryptography : 2.7\nPlatform : Windows-10-10.0.18362-SP0\n--\nI could not find any URL to regenerate the ResponseFailed([_DataLoss]) error at very first Request, if someone knows any such URL, please let me know so I can update the steps to Reproduce here.\n@rmax, @nyov", "issue_status": "Closed", "issue_reporting_time": "2019-11-03T00:55:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "39": {"issue_url": "https://github.com/scrapy/scrapy/issues/4119", "issue_id": "#4119", "issue_summary": "No module named 'scrapy_splash.settings'", "issue_description": "crianopa commented on Nov 1, 2019\nDescription\nI am running splash by the first time in Ubuntu (virtual Machine) with python3 in PyCharm. I am following this video: https://www.youtube.com/watch?v=rofkkuSf9iA&t=168s\nFirst I set up de settings.py:\nBOT_NAME = 'scrapy_splash'\nSPIDER_MODULES = ['scrapy_splash.spiders']\nNEWSPIDER_MODULE = 'scrapy_splash.spiders'\nROBOTSTXT_OBEY = True\nSPLASH_URL = 'http://localhost:8050'\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy_splash.SplashCookiesMiddleware': 723,\n    'scrapy_splash.SplashMiddleware': 725,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n}\nSPIDER_MIDDLEWARES = {\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\n}\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\nHTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\nThen\nimport scrapy\nfrom scrapy.spiders import Spider\nfrom scrapy_splash.items import GameItem\nfrom scrapy_splash import SplashRequest\nand following the same structure described in the video.\nWhen running scrapy crawl scrapy_splash I got this error: No module named 'scrapy_splash.settings'\nI've tried to look for info on .settings without succeed.\nAny idea what can it ca be?\nThis is the answer in the terminal.\ncarlos@carlos:~/Scrapy_scripts/scrapy_splash$ scrapy crawl ss \nTraceback (most recent call last):\n  File \"/home/carlos/.local/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/home/carlos/.local/lib/python3.6/site-packages/scrapy/cmdline.py\", line 114, in execute\n    settings = get_project_settings()\n  File \"/home/carlos/.local/lib/python3.6/site-packages/scrapy/utils/project.py\", line 68, in get_project_settings\n    settings.setmodule(settings_module_path, priority='project')\n  File \"/home/carlos/.local/lib/python3.6/site-packages/scrapy/settings/__init__.py\", line 294, in setmodule\n    module = import_module(module)\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'scrapy_splash.settings'\nVersions\nScrapy : 1.7.4\nlxml : 4.4.1.0\nlibxml2 : 2.9.9\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.21.0\nTwisted : 19.7.0\nPython : 3.6.8 (default, Oct 7 2019, 12:59:55) - [GCC 8.3.0]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1d 10 Sep 2019)\ncryptography : 2.8\nPlatform : Linux-5.0.0-32-generic-x86_64-with-Ubuntu-18.04-bionic", "issue_status": "Closed", "issue_reporting_time": "2019-11-01T16:11:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "40": {"issue_url": "https://github.com/scrapy/scrapy/issues/4109", "issue_id": "#4109", "issue_summary": "Cover the scrapy-sticky-meta-params middleware in the documentation", "issue_description": "Member\nGallaecio commented on Oct 29, 2019\nhttps://github.com/heylouiz/scrapy-sticky-meta-params\nRelated to #3645", "issue_status": "Closed", "issue_reporting_time": "2019-10-29T09:20:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "41": {"issue_url": "https://github.com/scrapy/scrapy/issues/4105", "issue_id": "#4105", "issue_summary": "Translating the docs to Korean", "issue_description": "Arielalfm commented on Oct 25, 2019\nHello!\nMy team (created in a lecture called 'open source software' which I take in) is very interested in scrapy, but necessarily files need to be read (like README and LICENSE) are written only in English. We thought if these files are translated, more people like us can contribute to this project.\nSo I'm just wondering if we can join here:)", "issue_status": "Closed", "issue_reporting_time": "2019-10-25T17:08:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "42": {"issue_url": "https://github.com/scrapy/scrapy/issues/4103", "issue_id": "#4103", "issue_summary": "Run multiple spiders using CrawlerProcess can\u2018t get one valid XML file", "issue_description": "thousfeet commented on Oct 25, 2019 \u2022\nedited\nDescription\nI tried to run multiple spiders using CrawlerProcess following this doc. But can\u2018t get one valid XML file.\nthis is my code:\nclass Spider1(spider):\n....\n\nclass Spider2(spider):\n....\n\nclass Spider3(spider):\n....\n\nprocess = CrawlerProcess(settings={\n    'FEED_FORMAT': 'xml',\n    'FEED_URI': 'output.xml',\n})\n\nprocess.crawl(Spider1)\nprocess.crawl(Spider2)\nprocess.crawl(Spider3)\nprocess.start()\nI get a output.xml which can't be parsed by xml.etree.ElementTree. Then I found this output file is not a valid XML file.\n...<author><value>Dorany Pineda</value></author><content><?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item><web_url>...\nIt seems one spider wrote the file but another spider hasn't finished writing....\nVersions\nScrapy : 1.6.0\nlxml : 4.4.1.0\nlibxml2 : 2.9.9\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.21.0\nTwisted : 19.7.0\nPython : 3.6.7 (default, Feb 28 2019, 07:28:18) [MSC v.1900 64 bit (AMD64)]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1d 10 Sep 2019)\ncryptography : 2.7\nPlatform : Windows-10-10.0.18362-SP0", "issue_status": "Closed", "issue_reporting_time": "2019-10-25T08:57:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "43": {"issue_url": "https://github.com/scrapy/scrapy/issues/4087", "issue_id": "#4087", "issue_summary": "OffsiteMiddleware doesn't seem to work properly", "issue_description": "django-kz commented on Oct 21, 2019\nI am just learning your great library, but as I got OffsiteMiddleware doesn't filter out all foreign links.\nFor example, I am scrapping \"whitneymiller.com\" website, I am collecting all \"a\" tags within pages and putting them into parse method\ndef parse(self, response):\n        self._parsed_urls.append(response.url)\n        print(\"Parsing: \", response)\n        sel = Selector(response)\n        urls = sel.xpath(\"//*/a/@href\").extract()\n        for url in urls:\n            full_url = response.urljoin(url)\n            print(\"Requesting crawl of: \", full_url)\n            self._requested_urls.append(full_url)\n            yield scrapy.Request(response.urljoin(url), callback=self.parse)\n-->\nmy settings for spider are as follows :\nprocess = CrawlerProcess({\n        'LOG_LEVEL': 'CRITICAL',\n        'DEPTH_LIMIT': 100,\n        'DEPT_STATS': True,\n        'CONCURRENT_REQUESTS_PER_DOMAIN': 10,\n        'DOWNLOADER_MIDDLEWARES':\n            {\n                \"scrapy.downloadermiddlewares.retry.RetryMiddleware\": 500,\n                \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\": 550\n            },\n    })\nand allowed domains are :\nallowed_domains=['whitneymiller.com']\nAs I see my crawler parses links like \"https://www.linkedin.com/cws/share?token&isFramed=false&url=https%3A%2F%2Fwhitneymiller.com%2Frecipe%2Fsmoked-chicken-wings-with-caramelized-onions%2F\"\nAm I doing something wrong or for OffsiteMiddleware links like that looks local for the specified domain?", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T13:30:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "44": {"issue_url": "https://github.com/scrapy/scrapy/issues/4086", "issue_id": "#4086", "issue_summary": "constructor, initializer \u2192 __init__ method", "issue_description": "Member\nGallaecio commented on Oct 21, 2019\nThere are several places in the documentation and comments where we use the words \u2018constructor\u2019 or \u2018initializer\u2019 to refer to the __init__ method of a class. We should change the wording to explicitly say \u2018__init__ method\u2019 instead.\nRelated to #2354.", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T12:05:38Z", "fixed_by": "#4088", "pull_request_summary": "docs: use __init__ method instead of constructor", "pull_request_description": "Contributor\nammarnajjar commented on Oct 21, 2019\nResolves #4086", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-12T16:48:17Z", "files_changed": [["2", "docs/conf.py"], ["10", "docs/news.rst"], ["4", "docs/topics/email.rst"], ["40", "docs/topics/exporters.rst"], ["2", "docs/topics/extensions.rst"], ["8", "docs/topics/items.rst"], ["26", "docs/topics/loaders.rst"], ["14", "docs/topics/request-response.rst"], ["2", "scrapy/exporters.py"], ["2", "scrapy/utils/datatypes.py"], ["2", "scrapy/utils/python.py"], ["12", "sep/sep-009.rst"], ["6", "tests/test_http_request.py"], ["2", "tests/test_http_response.py"], ["12", "tests/test_loader.py"], ["4", "tests/test_spider.py"]]}, "45": {"issue_url": "https://github.com/scrapy/scrapy/issues/4085", "issue_id": "#4085", "issue_summary": "Make Python 3.8 support official", "issue_description": "Member\nGallaecio commented on Oct 21, 2019\nThis involves, among other things:\nEnabling a Python 3.8 environment in Tox\nEnabling a Python 3.8 job in the Travis CI\nUpdating the setup.py file\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T10:46:56Z", "fixed_by": "#4092", "pull_request_summary": "Add Python 3.8 official support", "pull_request_description": "Contributor\nfurther-reading commented on Oct 21, 2019 \u2022\nedited by Gallaecio\nFixes #4085", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-29T11:30:59Z", "files_changed": [["6", ".travis.yml"], ["22", "docs/topics/downloader-middleware.rst"], ["17", "scrapy/extensions/httpcache.py"], ["1", "setup.py"], ["7", "tests/test_downloadermiddleware_httpcache.py"], ["8", "tox.ini"]]}, "46": {"issue_url": "https://github.com/scrapy/scrapy/issues/4084", "issue_id": "#4084", "issue_summary": "Cookies in <Request> seems not working", "issue_description": "BreezeFreak commented on Oct 21, 2019 \u2022\nedited\nHi!\nI am writing a <Spider>, and after login success, I try to send a <FormRequest> by the response from <parse_login>.\nI got cookies from it, and tried to add it into <FormRequest>, but it failed, which causes a redirection to login page.\n# here is the method after login\ndef query_form(self, response: scrapy.http.Response):\n    return [FormRequest(\n        url=url,  \n        cookies=cookies,\n        meta={'cookiejar': response.meta['cookiejar']},\n        method='POST',\n        headers=self.headers,\n        formdata={\n            'app_name': '',\n            'begin_date': str(self.start_date),\n            'end_date': str(self.start_date),\n        },\n        callback=self.parse_page,\n        dont_filter=True\n    )]\n# and here is the console log\nDEBUG [19-10-21 14:47:54] scrapy.downloadermiddlewares.redirect _redirect :43 Redirecting (302) to <GET *************> from <POST *************>\n# redirect from expected url to login page, as I think and test in post man and shell, this happened when giving wrong cookies or without cookies\nbut when I try it in <requests.request()> with cookies, I can get the direct page content.\nres = requests.request(method='POST', cookies=cookies, url=url)\nprint(res.cookies)\nprint(res.content)\nprint(res.text)\nI could really use your help, maybe stupidly I forgot some details. Thanks!", "issue_status": "Closed", "issue_reporting_time": "2019-10-21T06:42:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "47": {"issue_url": "https://github.com/scrapy/scrapy/issues/4082", "issue_id": "#4082", "issue_summary": "Error when run Scrapy 1.7.3 with python 3.6 on Windows", "issue_description": "040840308 commented on Oct 18, 2019\nHi, everyone\n I want to download some firmware images. My snippet code in the get_media_request is as follows:\n url='http://luyou.dl.qihucdn.com/luyou/360P4C/360-P4C-V2.0.4.59512.bin'\n return scrapy.http.Request(url, meta={\"vendor\": item[\"vendor\"]})\n\n However, error happens as follows:\n [(False, <twisted.python.failure.Failure builtins.TypeError: Unicode-objects must be encoded before hashing>)]\n\n Does some know how to deal with this?\n Thanks for any help.", "issue_status": "Closed", "issue_reporting_time": "2019-10-18T13:07:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "48": {"issue_url": "https://github.com/scrapy/scrapy/issues/4076", "issue_id": "#4076", "issue_summary": "Docs: Broken link on developer-tools", "issue_description": "tasawar-hussain commented on Oct 14, 2019\nLink broken to http://quotes.toscrape.com/scroll on developer-tools docs\nThe link quotes.toscrape.com/scroll on developer-tools docs opens incorrect page and thus returns 404\nSteps to Reproduce\nNavigate to https://docs.scrapy.org/en/latest/topics/developer-tools.html#the-network-tool\nClick the link quotes.toscrape.com/scroll on end of first paragraph.\nExpected behavior: It should naigate to http://quotes.toscrape.com/scroll\nActual behavior: It navigates to https://docs.scrapy.org/en/latest/topics/quotes.toscrape.com/scroll/\nReproduces how often: 100%\nVersions\nPlease paste here the output of executing scrapy version --verbose in the command line.\nScrapy : 1.7.3\nlxml : 4.4.1.0\nlibxml2 : 2.9.9\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.21.0\nTwisted : 19.7.0\nPython : 3.7.3 | packaged by conda-forge | (default, Jul 1 2019, 21:52:21) - [GCC 7.3.0]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1c 28 May 2019)\ncryptography : 2.7\nPlatform : Linux-4.15.0-65-generic-x86_64-with-debian-buster-sid", "issue_status": "Closed", "issue_reporting_time": "2019-10-14T10:14:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "49": {"issue_url": "https://github.com/scrapy/scrapy/issues/4075", "issue_id": "#4075", "issue_summary": "Successfully installed Scrapy but when I am trying to launch it is throwing a fatal error.", "issue_description": "ankushreddy commented on Oct 13, 2019\nDescription\nI have installed scrapy but when I run any command related to scrapy like scrapy -v or scrapy start project is is throwing a fatal error I tried multiple ways like re-installing anaconda and all but nothing worked.\nSteps to Reproduce\nVersions\npython 3.7 version.\nUnable to run any commands related to scrapy at all but have installed scrapy.", "issue_status": "Closed", "issue_reporting_time": "2019-10-13T14:20:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "50": {"issue_url": "https://github.com/scrapy/scrapy/issues/4072", "issue_id": "#4072", "issue_summary": "BOT_NAME and the user agent", "issue_description": "mohmad-null commented on Oct 11, 2019 \u2022\nedited\nThe docs for BOT_NAME say that it is used for the User agent (https://docs.scrapy.org/en/latest/topics/settings.html#std:setting-BOT_NAME) :\nThis will be used to construct the User-Agent by default, and also for logging.\nHowever in reality it is not. Scrapy is using the user agent:\n\"Scrapy/1.7.3 (+https://scrapy.org)\"\nSetting USER_AGENT does change the user agent correctly.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2019-10-11T12:15:12Z", "fixed_by": "#4081", "pull_request_summary": "Fixed BOT_NAME documentation", "pull_request_description": "Contributor\nbulatbulat48 commented on Oct 16, 2019 \u2022\nedited by Gallaecio\nFixes #4072\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-16T12:03:39Z", "files_changed": [["3", "docs/topics/settings.rst"]]}, "51": {"issue_url": "https://github.com/scrapy/scrapy/issues/4071", "issue_id": "#4071", "issue_summary": "[docs] Contracts are new?", "issue_description": "mohmad-null commented on Oct 11, 2019\nhttps://docs.scrapy.org/en/latest/topics/contracts.html\nsays:\nThis is a new feature (introduced in Scrapy 0.15) and may be subject to minor functionality/API updates. Check the release notes to be notified of updates.\n\ud83d\ude04 2", "issue_status": "Closed", "issue_reporting_time": "2019-10-11T09:10:56Z", "fixed_by": "#4093", "pull_request_summary": "Remove old note for contracts.", "pull_request_description": "Contributor\nWinterComes commented on Oct 22, 2019\nCloses #4071", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-22T10:19:08Z", "files_changed": [["4", "docs/topics/contracts.rst"]]}, "52": {"issue_url": "https://github.com/scrapy/scrapy/issues/4067", "issue_id": "#4067", "issue_summary": "Question: Handling fragment identifiers", "issue_description": "vella-nicholas commented on Oct 9, 2019\nI am scraping a web page which has fragment identifiers (on the home page) for same page navigation. To make it clear I have this anchor tag <a href=\"#idx1\">1. Histoire</a> and when you click on it you are taken to the 'Histoire' header on the same page. <h2 id=\"idx1\">1. Histoire</h2>.\nScrapy is following this fragment as https://www.mysite.com/#idx1 (or https://www.mysite.com#idx1) and scraping the whole page again. Since this is the home page, it is yielding the same results as if it scraped the home page.\nThis behavior is not repeated for the rest of the fragment identifiers (I have many more). So for this anchor tag (on the same page) the above scenario is not manifested. <a href=\"#idx2\">2. Id\u00e9es re\u00e7ues</a> -> Do not repeat itself for this.\nIs this intentional or am I missing something?\nimport scrapy\nfrom datetime import datetime\nfrom netlinking.items import NetlinkItem\nfrom urllib.parse import urlparse, urlunparse\n\n\nclass NetlinkingSpider(scrapy.Spider):\n    name = \"netlinking\"\n    referent_netloc = ''\n\n    def __init__(self, url=None, *args, **kwargs):\n        super(NetlinkingSpider, self).__init__(*args, **kwargs)\n        parsed_uri = urlparse(url)\n        self.referent_netloc = parsed_uri.netloc\n        self.referent_netloc = self.referent_netloc.replace('www.', '')\n        self.allowed_domains = [('%s' % self.referent_netloc)]\n        self.start_urls = [('%s' % url)]\n\n    def parse(self, response):\n        for anchor in response.css('a'):\n            netlink = NetlinkItem()\n            netlink['anchor_text'] = anchor.css('a::text').get()\n            netlink['referent_website'] = response.request.url\n            netlink['referent_netloc'] = self.referent_netloc\n            netlink['target_url'] = anchor.css('a::attr(href)').get()\n\n            parsed_target_uri = urlparse(netlink['target_url'])\n            if parsed_target_uri.netloc.strip() \\\n                    and (self.referent_netloc !=\n                         parsed_target_uri.netloc.lower().replace('www.', '')):\n\n                netlink['target_website'] = \\\n                    '{uri.scheme}://{uri.netloc}/'.format(\n                    uri=parsed_target_uri)\n\n                netlink['target_netloc'] = '{uri.netloc}'.format(\n                    uri=parsed_target_uri).replace('www.', '')\n\n                netlink['timestamp'] = datetime.now().strftime(\n                    \"%Y-%m-%d %H:%M:%S\")\n\n                yield netlink\n\n       #Here I am aware that I am following all links but this problem does not repeat for #idx2 \n       for a in response.css('a'):\n           yield response.follow(a, callback=self.parse)", "issue_status": "Closed", "issue_reporting_time": "2019-10-09T16:44:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "53": {"issue_url": "https://github.com/scrapy/scrapy/issues/4063", "issue_id": "#4063", "issue_summary": "Scrapy won't follow all Requests, generated by the overridden start_requests method", "issue_description": "Chris8080 commented on Oct 6, 2019 \u2022\nedited\nDescription\nI've got a list of 109 URLs (in this test case - can be 200 or 500 or more, and the same issue persists) which is loaded correctly into a list of requests by the start_requests method and returned.\nSteps to Reproduce\nrun the spider\nExpected behavior: [What you expect to happen]\nThe crawler should crawl (in this case due to CONCURRENT_REQUESTS = 150 roughly 150 pages at the same time, create the folders and files.\nActual behavior: [What actually happens]\nUp to ca. 70 folders are being created / 70 domains are being crawled before the spider eventually stops.\nReproduces how often: [What percentage of the time does it reproduce?]\n100% in my actual spider and in this barebone example.\nVersions\nScrapy : 1.7.3\nlxml : 4.3.3.0\nlibxml2 : 2.9.9\ncssselect : 1.0.3\nparsel : 1.5.1\nw3lib : 1.20.0\nTwisted : 19.2.0\nPython : 3.7.3 (default, Aug 20 2019, 17:04:43) - [GCC 8.3.0]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1b 26 Feb 2019)\ncryptography : 2.6.1\nPlatform : Linux-5.0.0-29-generic-x86_64-with-Ubuntu-19.04-disco\nAdditional context\nFollowing the settings.py and the spider.\nMaybe I'm doing sth wrong or there is an issue somehow?\n# -*- coding: utf-8 -*-\n\nBOT_NAME = 'crawlerscrapy'\nSPIDER_MODULES = ['crawlerscrapy.spiders']\nNEWSPIDER_MODULE = 'crawlerscrapy.spiders'\nUSER_AGENT_LIST = \"useragents.txt\"\nROBOTSTXT_OBEY = False\nCONCURRENT_REQUESTS = 150\nDOWNLOAD_DELAY = 13\nCONCURRENT_REQUESTS_PER_DOMAIN = 1\nCOOKIES_ENABLED = False\n\nDEFAULT_REQUEST_HEADERS = {\n   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n   'Accept-Encoding':'gzip, deflate, sdch',\n   'Connection':'keep-alive',\n   'Cache-Control':'max-age=0',\n   'Accept-Language': 'en',\n}\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware':None,\n    'random_useragent.RandomUserAgentMiddleware': 400\n}\nAUTOTHROTTLE_ENABLED = False\n\n# http://doc.scrapy.org/en/latest/topics/broad-crawls.html\nSCHEDULER_PRIORITY_QUEUE = 'scrapy.pqueues.DownloaderAwarePriorityQueue' # added for 3\nREACTOR_THREADPOOL_MAXSIZE = 20 # added for 2\nLOG_LEVEL = 'INFO'\nDEPTH_LIMIT = 1\nDOWNLOAD_TIMEOUT = 9\nSCHEDULER_DEBUG = True\nSCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'\nSCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'\nRETRY_TIMES = 1\nAJAXCRAWL_ENABLED = True\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy import Request\n\nimport logging\nimport sys\nimport os \nimport os.path\nfrom datetime import datetime\n\nfrom urllib.parse import urlparse\nfrom pathvalidate import sanitize_filename\n\n'''\nscrapy crawl spid -s JOBDIR=crawls/2019_09_18 --logfile=log/2019-09-18-complete.log\nscrapy crawl spid --logfile=log/2019-10-05-complete.log\n'''\n\nclass SpidSpider(CrawlSpider):\n    name = 'spid'\n\n    allowed_domains = []\n\n    with open(\"urls.txt\", \"rt\") as f:\n        start_urls = [url.strip() for url in f.readlines()]\n        allowed_domains = [url.strip() for url in f.readlines()]\n\n    rules = (\n        Rule(LinkExtractor(allow=()), callback='parse_item', follow=True),\n    )\n    \n    def __init__(self, *a, **kw):\n        super(CrawlSpider, self).__init__(*a, **kw)\n\n    def start_requests(self):\n        logging.log(logging.INFO, \"======== Starting with start_requests\")\n        self._compile_rules()\n\n        request_list = []\n        line = 1\n        for link in self.start_urls:\n            o = urlparse(link)\n            result = '{uri.netloc}'.format(uri=o)\n            self.allowed_domains.append(result)\n            logging.log(logging.INFO, \"======== in For:\")\n            logging.log(logging.INFO, link)\n            request_list.append ( Request(url=link, callback=self.parse) )\n            line+=1 \n        \n        logging.log(logging.INFO, \"======== Amount allowed Domains:\")\n        logging.log(logging.INFO, len(self.allowed_domains) )\n        logging.log(logging.INFO, \"======== Amount start URLs:\")\n        logging.log(logging.INFO, len(self.start_urls) )\n        logging.log(logging.INFO, \"======== Amount request list:\")\n        logging.log(logging.INFO, len(request_list) )\n        logging.log(logging.INFO, \"======== Finished with start_requests\")\n        \n        return ( request_list )\n\n\n    def parse_item(self, response):\n        item = {}\n        self.write_html_file ( response )\n        return item\n\n\n    def write_html_file (self, response):\n        current_url = response.url\n        orootdir = 'out/'\n\n        title = response.xpath('//title/text()').get()\n        \n        o = urlparse(current_url)\n        result = '{uri.netloc}'.format(uri=o)\n\n        if '/de/' in current_url:\n            current_url = (current_url.replace('/de/', '')).strip()\n        if '/en/' in current_url:\n            current_url = (current_url.replace('/en/', '')).strip()\n        if current_url.endswith ( \"/\" ):\n            current_url = current_url[:-1]\n\n        page = current_url.split(\"/\")[-1]\n        self.check_path ( orootdir + '/' )\n        self.check_path ( orootdir + '/' + result )\n        \n        filename = datetime.today().strftime('%Y-%m-%d') + '__' + title + '.html'\n        filename = sanitize_filename ( filename ).replace(' ', '_').strip()\n        \n        pathfilename = orootdir + '/' + result + '/'\n        pathfilename = pathfilename + filename\n        \n        self.logger.info(\"Filename to be generated: \" + pathfilename)\n        \n        with open(pathfilename, 'wb') as f:\n            f.write(response.body)\n\n\n    def check_path ( self, path ):\n        if not os.path.exists( path ):\n            os.makedirs( path )\nAnd a list of URLs - works with any.", "issue_status": "Closed", "issue_reporting_time": "2019-10-05T19:10:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "54": {"issue_url": "https://github.com/scrapy/scrapy/issues/4062", "issue_id": "#4062", "issue_summary": "Getting \u201cCrawled 324 pages (at 133 pages/min), scraped 304 items (at 130 items/min)\u201d after paginating 18 pages while there are 42 pages to scrap?", "issue_description": "Danyalm66 commented on Oct 5, 2019 \u2022\nedited by elacuesta\nWhile I have written a script to scrape data from the site and it is working ideally but after scraping about 18 pages' data(as there are about 42 pages), the scrapy get stuck by giving a log info after and after.\nI visited the similar questions answered on stackoverflow but in all of them the scripts were not working from the beginning while in my case the script scraped data from about 18 pages and then get stuck.\nHere is the script\nimport scrapy\nimport logging\n\nclass KhaadiSpider(scrapy.Spider):\n    name = 'khaadi'\n\n    start_urls = ['https://www.khaadi.com/pk/woman.html/']\n\n    def parse(self, response):\n        urls= response.xpath('//ol/li/div/a/@href').extract()\n        for url in urls:\n            yield scrapy.Request(url, callback=self.product_page)\n        next_page=response.xpath('//*[@class=\"action  next\"]/@href').extract_first()\n        while(next_page!=None):\n            yield scrapy.Request(next_page)\n        logging.info(\"Scraped all the pages Successfuly....\")\n\n    def product_page(self,response):\n        image= response.xpath('//*[@class=\"MagicZoom\"]/@href').extract_first()\n        page_title= response.xpath('//*[@class=\"page-title\"]/span/text()').extract_first()\n        price=response.xpath('//*[@class=\"price\"]/text()').extract_first()\n        page_url=response.url\n\n        yield {'Image':image,\n               \"Page Title\":page_title,\n               \"Price\":price,\n               \"Page Url\":page_url\n\n        }\nThis is the Logger info\n2019-10-05 11:22:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.khaadi.com/pk/ksffs19301-blue.html> (referer: https://www.khaadi.com/pk/woman.html?p=18)\n2019-10-05 11:22:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/b19428-pink-3pc.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/b/1/b19428b.jpg', 'Page Title': u'Shirt Shalwar Dupatta', 'Page Url': 'https://www.khaadi.com/pk/b19428-pink-3pc.html', 'Price': u'PKR2,170'}\n2019-10-05 11:22:06 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/i19417-blue-2pc.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/i/1/i19417b.jpg', 'Page Title': u'Shirt Shalwar', 'Page Url': 'https://www.khaadi.com/pk/i19417-blue-2pc.html', 'Price': u'PKR1,680'}\n2019-10-05 11:22:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/wshe19498-off-white.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/w/s/wshe19498_2_.jpg', 'Page Title': u'Embroidered Shalwar', 'Page Url': 'https://www.khaadi.com/pk/wshe19498-off-white.html', 'Price': u'PKR1,800'}\n2019-10-05 11:22:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/wet19401-off-white.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/w/e/wet19401_offwhite__1_.jpg', 'Page Title': u'EMBELLISHED TIGHTS', 'Page Url': 'https://www.khaadi.com/pk/wet19401-off-white.html', 'Price': u'PKR1,000'}\n2019-10-05 11:22:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/wet19402-black.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/w/e/wet19402_black__2_.jpg', 'Page Title': u'EMBELLISHED TIGHTS', 'Page Url': 'https://www.khaadi.com/pk/wet19402-black.html', 'Price': u'PKR1,000'}\n2019-10-05 11:22:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/k19407-yellow-3pc.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/k/1/k19407b.jpg', 'Page Title': u'Shirt Shalwar Dupatta', 'Page Url': 'https://www.khaadi.com/pk/k19407-yellow-3pc.html', 'Price': u'PKR2,940'}\n2019-10-05 11:22:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/k19408-blue-3pc.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/k/1/k19408a.jpg', 'Page Title': u'Shirt Shalwar Dupatta', 'Page Url': 'https://www.khaadi.com/pk/k19408-blue-3pc.html', 'Price': u'PKR2,940'}\n2019-10-05 11:22:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/wet19408-pink.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/w/e/wet19408_pink__1_.jpg', 'Page Title': u'EMBELLISHED TIGHTS', 'Page Url': 'https://www.khaadi.com/pk/wet19408-pink.html', 'Price': u'PKR1,000'}\n2019-10-05 11:22:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/wbme19474-off-white.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/w/b/wbme19474_offwhite__1_.jpg', 'Page Title': u'Embroidered Metallica Pants', 'Page Url': 'https://www.khaadi.com/pk/wbme19474-off-white.html', 'Price': u'PKR2,400'}\n2019-10-05 11:22:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.khaadi.com/pk/ksffs19301-blue.html>\n{'Image': u'https://www.khaadi.com/media/catalog/product/cache/10f519365b01716ddb90abc57de5a837/k/s/ksffs19301_blue__2_.jpg', 'Page Title': u'Semi Formal Full Suit', 'Page Url': 'https://www.khaadi.com/pk/ksffs19301-blue.html', 'Price': u'PKR18,000'}\n2019-10-05 11:22:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 326 pages/min), scraped 307 items (at 307 items/min)\n2019-10-05 11:23:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:24:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:25:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:26:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:27:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:28:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:29:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:30:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:31:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:32:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:33:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:34:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:35:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\n2019-10-05 11:36:24 [scrapy.extensions.logstats] INFO: Crawled 326 pages (at 0 pages/min), scraped 307 items (at 0 items/min)\nAll the other files left default.", "issue_status": "Closed", "issue_reporting_time": "2019-10-05T07:45:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "55": {"issue_url": "https://github.com/scrapy/scrapy/issues/4060", "issue_id": "#4060", "issue_summary": "Unaccounted time consumed by scrapy crawl which is not listed in stats", "issue_description": "Contributor\nDharmeshPandav commented on Oct 4, 2019\nI have been trying to run a scrapy crawl in a time constrained envrionment where we want to make sure that scrapy stops after fixed run time limit.\nWe have set the following CLOSESPIDER_TIMEOUT as 60 seconds and DOWNLOAD_TIMEOUT as 30 seconds.\nWe have set the forceful termination (kill the subprocess) at 90 seconds to cover the edge case scenario if page request is made at 59 seconds and twisted fails to download page and download timeout signal is triggred. ( 59 + 30 = 89 < 90 seconds)\nbut still scripts runs for random amount of time. range is between 95-115 seconds.\nI am not sure where this variable 5-30 seconds are being used and why scrapy is failing to close gracelully after 90 seconds as expected\nis this the default and accepted behaviour in scrapy/twisted reactor architecture where this lag is being introduced\nor if any setting is there to override this behavior\nThanks", "issue_status": "Closed", "issue_reporting_time": "2019-10-04T14:01:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "56": {"issue_url": "https://github.com/scrapy/scrapy/issues/4047", "issue_id": "#4047", "issue_summary": "CrawlerRunnerTestCase.test_deprecated_attribute_spiders is flaky", "issue_description": "Contributor\ns-sanjay commented on Oct 1, 2019\nDescription\n[test_deprecated_attribute_spiders is flaky it fails randomly in different environments.\nsee https://travis-ci.org/scrapy/scrapy/jobs/590415988 ]\nSteps to Reproduce\n[just run the test in travis ci environment build]\n[re trigger the build and you can see the test being flaky]\nExpected behavior: [the test should pass reliably ]\nActual behavior: [test is flaky]\nReproduces how often: [for me it happened twice in 3 times but I don't have a concrete number. I see it happening in other pulls as well]\nVersions\n1.7.0", "issue_status": "Closed", "issue_reporting_time": "2019-10-01T13:52:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "57": {"issue_url": "https://github.com/scrapy/scrapy/issues/4044", "issue_id": "#4044", "issue_summary": "Scrappy don't call the first request", "issue_description": "anhtuanmanga commented on Sep 29, 2019\nI have problem when call two request in parse function\nhere is my code\n`import re\nimport logging\nimport os\nimport scrapy\nfrom scrapy import Spider\nfrom scrapy.selector import Selector\nclass StackSpider(Spider):\nname = \"batdongsan\"\nallowed_domains = [\"batdongsan.com.vn\"]\nstart_urls = [\n\"https://batdongsan.com.vn/nha-dat-ban\",\n]\nurl_prefix = \"\"\nurl_last = \"\"\nmax = \"\"\nnext_page = \"\"\ncount = 1\ndef parse(self, response):\n    # questions = Selector(response).xpath('//div[@class=\"p-title\"]/h3')\n    # # This part of code collect only titles. You need to add more fields to be collected if you need.\n    # for question in questions:\n    #     title = question.xpath(\n    #      'a/text()').extract_first().encode('utf-8').strip()\n    #     with open('title.txt', 'a') as f:\n    #         f.writelines(title+'\\n')\n    #     # yield {'title': title}\n    # block 1\n    yield scrapy.Request(response.url, callback=self.detail_page)\n\n    # block 2\n    if not re.search(r'\\d+', response.url):\n        # Now we have to go th\n        url_last = response.css('div.background-pager-right-controls a::attr(href)').extract()[-1]\n        max = re.findall(r'\\d+', url_last)[0]\n        for n in range(2, int(max) + 1):\n            next_page = self.start_urls[0] + '/p' + str(n)\n            yield response.follow(next_page, callback=self.parse)\n\ndef detail_page(self, response):\n    detail_pages = Selector(response).xpath('//div[@class=\"p-title\"]/h3/a')\n    for detail_page in detail_pages:\n        urlpage = detail_page.xpath(\n            '@href').extract_first().strip()\n        page = self.start_urls[0] + urlpage\n        yield scrapy.Request(page, callback=self.export)\n\ndef export(self, response):\n    title = Selector(response).xpath(\n        '//*[@id=\"product-detail\"]/div[1]/h1/text()').extract_first().encode('utf-8').strip()\n    with open('title.txt', 'a') as f:\n        f.writelines(title + '\\n')\n    place = Selector(response).xpath(\n        '//*[@id=\"product-detail\"]/div[8]/div/div[1]/div/div[2]/div[2]/div[2]/text()'\n    ).extract_first().encode('utf-8').strip()\n    with open('title.txt', 'a') as f:\n        f.writelines(place + '\\n')\n`\nwhen i run both block 1 and 2 in cmd screen scrappy only load url get from block 2 but when i comment code in block 2, code in block 1 run normall.\ncan everyone help me why this happen and how to fix this?\nsorry for my crappy english.", "issue_status": "Closed", "issue_reporting_time": "2019-09-29T17:09:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "58": {"issue_url": "https://github.com/scrapy/scrapy/issues/4040", "issue_id": "#4040", "issue_summary": "Can't disable SSL verification in Scrapy", "issue_description": "bigtang5 commented on Sep 27, 2019\nI think it's a bug, not a feature.\nThe proxy provider give the demo code:\nr = requests.get(\"https://www.baidu.com/\", headers=headers, proxies=proxy, verify=False, allow_redirects=False)\nIt works fine, It will fail when remove \"verify=False\".\nNow I have to use the proxy in my scrapy project, There is no way to add \"verify=False\" in Scrapy unless I write my own HTTPDownloadHandler.\nHEW, what a huge work......", "issue_status": "Closed", "issue_reporting_time": "2019-09-27T09:16:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "59": {"issue_url": "https://github.com/scrapy/scrapy/issues/4038", "issue_id": "#4038", "issue_summary": "python3.6 start_requests method can not concurrently reques", "issue_description": "daidai5771 commented on Sep 25, 2019\nI rewrote the start_requests method, such as :\ndef start_requests(self):\nfor (click_type, url, css_path) in self.start_urls:\nprint (url)\nyield Request(url, callback=self.loop_parse, dont_filter=True)\nThe effect of execution under Python 2.7 is as follows:\n\"\nhttp://www.flyme.cn\nhttps://cloud.flyme.cn\nhttp://www.meizu.com\nhttp://open.flyme.cn\nhttps://member.meizu.com/uc/webjsp/member/detail\nhttp://app.meizu.com/apps/public/index\nhttp://app.meizu.com/apps/public/index\nhttp://app.meizu.com/apps/public/index\nhttp://app.meizu.com/apps/public/index\nhttp://app.meizu.com/apps/public/index\nhttp://app.meizu.com/apps/public/index\n\"\nBut it got stuck behind http://www.flyme.cn at Python 3.6 , Only one request can be sent at a time", "issue_status": "Closed", "issue_reporting_time": "2019-09-25T11:38:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "60": {"issue_url": "https://github.com/scrapy/scrapy/issues/4035", "issue_id": "#4035", "issue_summary": "CLOSESPIDER_TIMEOUT Settings 36000 Invalid Settings 60 ok ?", "issue_description": "kjxy commented on Sep 25, 2019 \u2022\nedited\nDescription\nCLOSESPIDER_TIMEOUT Settings 36000 run for five days...\nCLOSESPIDER_TIMEOUT Settings 60 closespider_timeout(is ok)\nSteps to Reproduce\ncustom_settings = {\n\"CLOSESPIDER_TIMEOUT\": 60,\n}", "issue_status": "Closed", "issue_reporting_time": "2019-09-25T02:27:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "61": {"issue_url": "https://github.com/scrapy/scrapy/issues/4034", "issue_id": "#4034", "issue_summary": "Rephrase image pipeline documentation", "issue_description": "samLozier commented on Sep 24, 2019\nSummary\nClarity of the pipeline documentation could be improved by more clearly calling out the steps that need to be taken to enable a custom pipeline. I was encouraged to issue this request in my Reddit thread where I was struggling to resolve this issue: thread.\nExisting documentation:\nhttps://docs.scrapy.org/en/latest/topics/media-pipeline.html\nTo enable your media pipeline you must first add it to your project item pipeline setting.\nFor Images Pipeline, use:\nITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}\nTo me, it reads like \"to use this custom pipeline, just add this line to your settings.py file\". If the doc maintainers wanted to be more clear, it could have read \"add this line to your item pipeline settings and replace these fields with the appropriate fields from your project\", like I do below:\nITEM_PIPELINES = {f'{your_scrapername}.pipelines.{your_item_pipeline_class_name}: 1*}*\nMotivation\nImprove clarity/readability, help newer users get up to speed more quickly.\nDescribe alternatives you've considered\non reddit /u/Gallaecio/ suggested the following as an alternative:\nI think you might have a point. I bet something as simple as changing\nscrapy.pipelines.images.ImagesPipeline to something like\nmy_project.pipelines.MyImagesPipeline would make things more obvious.\nEither solution would help to show that 'scrapy' references the specific project you're working on, and 'ImagesPipeline' is the custom class you setup for your project.\nAdditional context\nIn my attempts to resolve my own problem I found many threads going back years on stack overflow and reddit that highlighted the same issue. I believe that the change would help new users avoid a potentially frustrating issue.", "issue_status": "Closed", "issue_reporting_time": "2019-09-24T17:40:20Z", "fixed_by": "#4252", "pull_request_summary": "Rephrasing documentation for image and file pipelines", "pull_request_description": "Contributor\n1um0s commented on Dec 22, 2019 \u2022\nedited by elacuesta\nRephrasing documentation for image and file pipelines\nFixes #4034", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-29T19:56:22Z", "files_changed": [["9", "docs/topics/media-pipeline.rst"]]}, "62": {"issue_url": "https://github.com/scrapy/scrapy/issues/4032", "issue_id": "#4032", "issue_summary": "Error 302 redirection with headers location starts with 3 slash", "issue_description": "sicklife commented on Sep 23, 2019\nDescription\nwhen the 302 response return a headers's location startswith 3 slash, the scrapy redirect to a url different from what the browser do.\nSteps to Reproduce\nscrapy shell https://www.hjenglish.com/new/p1285798/\nExpected behavior:\nredirect to https://fr.hujiang.com/new/p1285798/ as browser Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36 do.\nActual behavior:\nredirct to https://www.hjenglish.com/fr.hujiang.com/new/p1285798\nReproduces how often:\neverytime\nVersions\nScrapy : 1.7.3\nlxml : 4.3.2.0\nlibxml2 : 2.9.9\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.20.0\nTwisted : 19.7.0\nPython : 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1c 28 May 2019)\ncryptography : 2.6.1\nPlatform : Windows-10-10.0.17134-SP0\nAdditional context\nI check the defination of Location in rfc and end with reference resolution. But I fail to findout how to resolve the Location startswith ///. So I don't know why Chrome did so.\nThe behavior of scrapy is determined by redirect.py#L73, which will truncate /// to /\u3002\nI'm wandering the differents betweent scarpy and browser...", "issue_status": "Closed", "issue_reporting_time": "2019-09-23T07:56:49Z", "fixed_by": "#4042", "pull_request_summary": "Error 302 redirection with headers location starts with 3 slash #4032\u2026", "pull_request_description": "Contributor\namardeep24 commented on Sep 28, 2019 \u2022\nedited by elacuesta\nThe issue was when the location header returned a URL starting with multiple forward slashes ///.\nThe browser converted such URLs to its absolute form e.g. : ///fr.hujiang.com/new/p1285798/ was changed to https://fr.hujiang.com/new/p1285798/. But in Scrapy the URL was converted to the absolute form w.r.t to the original base URL: ///fr.hujiang.com/new/p1285798/ was changed to https://www.hjenglish.com/fr.hujiang.com/new/p1285798.\nThe fix checks whether the location header value has an URL value starting with more than one / if it is the logic will change it to the absolute form https://fr.hujiang.com/new/p1285798/ by appending the <scheme>:// before it.\n(Edit) Fixes #4032", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-30T08:09:13Z", "files_changed": [["7", "scrapy/downloadermiddlewares/redirect.py"], ["16", "tests/test_downloadermiddleware_redirect.py"]]}, "63": {"issue_url": "https://github.com/scrapy/scrapy/issues/4026", "issue_id": "#4026", "issue_summary": "how to disable the log information when a return/yield of an item from a parse of spider?", "issue_description": "loveJasmine commented on Sep 19, 2019\nit looks very lousy print out on terminal when I use return/yield,\nhowever,since it a lot of other infor are helpful I won't change the log level to suppress it.\nIs there anyway to disable the print of return/yield of parse without change log level?\nclass ZixunhtmlSpider(scrapy.Spider):\nname = 'zixunHtml'\nallowed_domains = ['xxxx.com']\nstart_urls = []\nwith open(\"G:/crawl_data/choice/obsolete/zixun_temp.txt\", 'r', encoding='utf-8') as fgg:\n\n    for line in fgg:\n          start_urls.append(line)\n\ndef parse(self, response):\n    ex = ''\n    if response.status != 200:\n        ex = 'None 200 happens!'\n    else:\n        ex = response.text\n\n    output[response.url] = ex\n    print(\"===========================================\\n\")\n    #return output\n    yield output", "issue_status": "Closed", "issue_reporting_time": "2019-09-19T03:39:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "64": {"issue_url": "https://github.com/scrapy/scrapy/issues/4019", "issue_id": "#4019", "issue_summary": "FileNotFoundError: [Errno 2] No such file or directory: 'newproject/newproject/settings.py.tmpl'", "issue_description": "ivngithub commented on Sep 14, 2019 \u2022\nedited\nDescription\nWhen I do scrapy startproject I got this problem:\nFileNotFoundError: [Errno 2] No such file or directory: 'newproject/newproject/settings.py.tmpl'\nBut this fil is already!\nand I use VBox and have share folder from Windows", "issue_status": "Closed", "issue_reporting_time": "2019-09-14T17:57:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "65": {"issue_url": "https://github.com/scrapy/scrapy/issues/4014", "issue_id": "#4014", "issue_summary": "Random(?) test_crawler.py failure on Travis", "issue_description": "Member\nelacuesta commented on Sep 13, 2019 \u2022\nedited\nEvery now and then the Travis build fails on tests.test_crawler.CrawlerRunnerTestCase.test_deprecated_attribute_spiders, I'm not sure why.\nSome examples:\nhttps://travis-ci.org/scrapy/scrapy/jobs/582184413#L407\nhttps://travis-ci.org/scrapy/scrapy/jobs/580207692#L419\nhttps://travis-ci.org/scrapy/scrapy/jobs/584701732#L415\nhttps://travis-ci.org/scrapy/scrapy/jobs/584762868#L418\nPS: Sorry, I feel bad for ignoring the bug report template, but I felt like it didn't apply in this case.\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2019-09-13T14:49:40Z", "fixed_by": "#4095", "pull_request_summary": "Use communicate() instead of wait() after killing the mock server", "pull_request_description": "Member\nGallaecio commented on Oct 22, 2019 \u2022\nedited\nFixes #4014\nWhen using pipes, and we are using a pipe for stdout in MockServer, you are meant to call process.communicate() instead of process.wait().\nThis was causing unexpected warnings in test_crawler.py tests, when the garbage collection happened to warn about unclosed resources while one of the affected tests was capturing warnings.\nI\u2019ve run the tests a few rounds with this fix, and it seems to have worked.\n\ud83d\udc4d 2", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-31T12:31:33Z", "files_changed": [["3", "tests/mockserver.py"]]}, "66": {"issue_url": "https://github.com/scrapy/scrapy/issues/4007", "issue_id": "#4007", "issue_summary": "Exception when using DummyStatsCollector", "issue_description": "Contributor\nLukas0907 commented on Sep 9, 2019\nDescription\nUsing the DummyStatsCollector results in an exception:\n2019-09-09 13:51:23 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method CoreStats.spider_closed of <scrapy.extensions.corestats.CoreStats object at 0x7f86269cac18>>\nTraceback (most recent call last):\n  File \".../lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \".../lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \".../lib/python3.6/site-packages/scrapy/extensions/corestats.py\", line 28, in spider_closed\n    elapsed_time = finish_time - self.stats.get_value('start_time')\nTypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'NoneType'\nThis problem has been introduced in aa46e19.\nSteps to Reproduce\nSet STATS_CLASS = \"scrapy.statscollectors.DummyStatsCollector\" in the settings module as described in the documentation (https://docs.scrapy.org/en/latest/topics/stats.html#dummystatscollector).\nExpected behavior: no exception\nActual behavior: exception thrown\nReproduces how often: always\nVersions\nAt least master as of 534de73\nFix\nA possible fix is to use the elapsed time as a default argument so that get_value() does not return None. I can prepare a PR if needed.\n--- a/scrapy/extensions/corestats.py\n+++ b/scrapy/extensions/corestats.py\n@@ -25,7 +25,7 @@ class CoreStats(object):\n \n     def spider_closed(self, spider, reason):\n         finish_time = datetime.datetime.utcnow()\n-        elapsed_time = finish_time - self.stats.get_value('start_time')\n+        elapsed_time = finish_time - self.stats.get_value('start_time', finish_time)\n         elapsed_time_seconds = elapsed_time.total_seconds()\n         self.stats.set_value('elapsed_time_seconds', elapsed_time_seconds, spider=spider)\n         self.stats.set_value('finish_time', finish_time, spider=spider)", "issue_status": "Closed", "issue_reporting_time": "2019-09-09T12:18:03Z", "fixed_by": "#4052", "pull_request_summary": "Fix TypeError when using DummyStatsCollector", "pull_request_description": "Member\nelacuesta commented on Oct 1, 2019\nFixes #4007", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-02T07:06:23Z", "files_changed": [["11", "scrapy/extensions/corestats.py"], ["49", "tests/test_stats.py"]]}, "67": {"issue_url": "https://github.com/scrapy/scrapy/issues/4004", "issue_id": "#4004", "issue_summary": "ModuleNotFoundError: No module named 'tutsplus'", "issue_description": "tommyamz commented on Sep 7, 2019 \u2022\nedited\nHi please help me on this errror when i trying run project on Mac OS\nI install Python 3.7.4 and Scrapy 1.7.3\n(AMZSpider) ToanVoMBP:tutsplus toanvo$ scrapy crawl tutplus\n\nTraceback (most recent call last):\n  File \"/Users/toanvo/.virtualenvs/AMZSpider/bin/scrapy\", line 10, in <module>\n    sys.exit(execute())\n  File \"/Users/toanvo/.virtualenvs/AMZSpider/lib/python3.7/site-packages/scrapy/cmdline.py\", line 114, in execute\n    settings = get_project_settings()\n  File \"/Users/toanvo/.virtualenvs/AMZSpider/lib/python3.7/site-packages/scrapy/utils/project.py\", line 68, in get_\nproject_settings\n    settings.setmodule(settings_module_path, priority='project')\n  File \"/Users/toanvo/.virtualenvs/AMZSpider/lib/python3.7/site-packages/scrapy/settings/__init__.py\", line 294, in\n setmodule\n    module = import_module(module)\n  File \"/Users/toanvo/.virtualenvs/AMZSpider/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\nModuleNotFoundError: No module named 'tutsplus'", "issue_status": "Closed", "issue_reporting_time": "2019-09-07T09:43:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "68": {"issue_url": "https://github.com/scrapy/scrapy/issues/3997", "issue_id": "#3997", "issue_summary": "Is 'ResponseNeverReceived' retried automatically?", "issue_description": "botzill commented on Sep 1, 2019 \u2022\nedited\nI have a spider that is using proxy and I see when finshed this:\n{'bans/error/twisted.web._newclient.ResponseNeverReceived': 5580,\n 'downloader/exception_count': 5580,\n 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 5580,\n 'downloader/request_bytes': 10857668,\n 'downloader/request_count': 17877,\n 'downloader/request_method_count/GET': 1,\n 'downloader/request_method_count/POST': 17876,\n 'downloader/response_bytes': 173289486,\n 'downloader/response_count': 12297,\n 'downloader/response_status_count/200': 11799,\n 'downloader/response_status_count/502': 498,\n 'elapsed_time_seconds': 1574.070208,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2019, 9, 1, 10, 23, 37, 55794),\n 'item_scraped_count': 10669,\n 'log_count/DEBUG': 53156,\n 'log_count/ERROR': 5580,\n 'log_count/INFO': 50,\n 'memusage/max': 257974272,\n 'memusage/startup': 53932032,\n 'proxies/dead': 16,\n 'proxies/good': 17,\n 'proxies/mean_backoff': 49.755788241608414,\n 'proxies/reanimated': 2,\n 'proxies/unchecked': 0,\n 'request_depth_max': 10,\n 'response_received_count': 11799,\n 'retry/count': 498,\n 'retry/reason_count/502 Bad Gateway': 498,\n 'scheduler/dequeued': 17877,\n 'scheduler/dequeued/memory': 17877,\n 'scheduler/enqueued': 17877,\n 'scheduler/enqueued/memory': 17877,\n 'start_time': datetime.datetime(2019, 9, 1, 9, 57, 22, 985586)}\n2019-09-01 13:23:37 [scrapy.core.engine] INFO: Spider closed (finished)\nI did update the retry download middleware retry to include these exceptions as well :\nclass RetryMiddleware(retry.RetryMiddleware):\n    EXCEPTIONS_TO_RETRY = retry.RetryMiddleware.EXCEPTIONS_TO_RETRY + (\n        ConnectBindError,\n        ResponseNeverReceived\n    )\nSo, does this mean that this error will be retried automatically?\nbecause I see 'retry/count': 498 is less then ResponseNeverReceived count.\nThx!", "issue_status": "Closed", "issue_reporting_time": "2019-09-01T10:10:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "69": {"issue_url": "https://github.com/scrapy/scrapy/issues/3996", "issue_id": "#3996", "issue_summary": "Can't POST with binary body using Python 3", "issue_description": "vionemc commented on Aug 31, 2019\nDescription\nMy code was working in Python 2, but you are ending your support to Python 2 soon. I am trying to migrate to Python 3 but it seems you have some compatibility issues with binary in POST request.\nSteps to Reproduce\nI am trying to do this request with reponse.body populated with an image binary.\n  yield scrapy.Request(u\"{}/formrecognizer/v1.0-preview/prebuilt/receipt/asyncBatchAnalyze\".format(self.endpoint), \n   method='POST',\n   body=response.body,\n   headers=self.binary_headers,\n   callback=self.parse_result_url)\nBut then I get this error:\nTraceback (most recent call last):\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\core\\spidermw.py\", line 84, in evaluate_iterable\n    for r in iterable:\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"D:\\Kerja\\HIT\\Python Projects\\<my_project>\\receipts\\receipts\\receipts\\spiders\\receipt_recognizer.py\", line 63, in parse_result_url\n    yield scrapy.Request(response.headers['Operation-Location'], headers=self.receipt_headers, callback=self.parse_result)\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\http\\request\\__init__.py\", line 25, in __init__\n    self._set_url(url)\n  File \"c:\\python374\\lib\\site-packages\\scrapy\\http\\request\\__init__.py\", line 63, in _set_url\n    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\nTypeError: Request url must be str or unicode, got bytes:\nVersions\nScrapy : 1.7.3\nlxml : 4.4.1.0\nlibxml2 : 2.9.5\ncssselect : 1.1.0\nparsel : 1.5.2\nw3lib : 1.21.0\nTwisted : 19.7.0\nPython : 3.7.4 (tags/v3.7.4:e09359112e, Jul 8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1c 28 May 2019)\ncryptography : 2.7\nPlatform : Windows-10-10.0.17134-SP0", "issue_status": "Closed", "issue_reporting_time": "2019-08-31T10:05:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "70": {"issue_url": "https://github.com/scrapy/scrapy/issues/3995", "issue_id": "#3995", "issue_summary": "ImagesPipeline does not follow redirection", "issue_description": "rsarky commented on Aug 31, 2019\nDescription\nThis issue was already reported #2397 but the reporter closed the issue after posting a temporary fix. I reckon there should be an elegant way to hande this as this is a common use case I guess. Maybe a configurable setting that allows image to be downloaded even when the URL is redirected.", "issue_status": "Closed", "issue_reporting_time": "2019-08-31T05:39:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "71": {"issue_url": "https://github.com/scrapy/scrapy/issues/3992", "issue_id": "#3992", "issue_summary": "Errors occur when the crawler is closed", "issue_description": "Luokun2016 commented on Aug 30, 2019 \u2022\nedited by Gallaecio\nDescription\nThe following errors occur when the crawler is closed\n'NoneType' object has no attribute 'start_requests'\nlog\uff1a\n{'finish_reason': 'response msg error \u6211\u4eec\u7684\u7cfb\u7edf\u68c0\u6d4b\u5230\u60a8\u7f51\u7edc\u4e2d\u5b58\u5728\u5f02\u5e38\u8bbf\u95ee\u8bf7\u6c42, url '\n                  'https://weixin.sogou.com/weixin?type=2&s_from=input&query=mjl_tfsteel&ie=utf8&_sug_=y&_sug_type_=!',\n 'finish_time': datetime.datetime(2019, 8, 30, 9, 22, 3, 628463),\n 'memusage/max': 679272448,\n 'memusage/startup': 679272448,\n 'start_time': datetime.datetime(2019, 8, 30, 9, 22, 2, 54762)}\n2019-08-30 17:22:03,628 [scrapy.core.engine] INFO: Spider closed (response msg error \u6211\u4eec\u7684\u7cfb\u7edf\u68c0\u6d4b\u5230\u60a8\u7f51\u7edc\u4e2d\u5b58\u5728\u5f02\u5e38\u8bbf\u95ee\u8bf7\u6c42, url https://weixin.sogou.com/weixin?type=2&s_from=input&query=mjl_tfsteel&ie=utf8&_sug_=y&_sug_type_=!)\n2019-08-30 17:22:04,016 [twisted] CRITICAL: Unhandled Error\nTraceback (most recent call last):\n  File \"/home/user/.local/lib/python3.6/site-packages/scrapy/commands/crawl.py\", line 58, in run\n    self.crawler_process.start()\n  File \"/home/user/.local/lib/python3.6/site-packages/scrapy/crawler.py\", line 293, in start\n    reactor.run(installSignalHandlers=False)  # blocking call\n  File \"/home/user/.local/lib/python3.6/site-packages/twisted/internet/base.py\", line 1272, in run\n    self.mainLoop()\n  File \"/home/user/.local/lib/python3.6/site-packages/twisted/internet/base.py\", line 1281, in mainLoop\n    self.runUntilCurrent()\n--- <exception caught here> ---\n  File \"/home/user/.local/lib/python3.6/site-packages/twisted/internet/base.py\", line 902, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"/home/user/.local/lib/python3.6/site-packages/scrapy/utils/reactor.py\", line 41, in __call__\n    return self._func(*self._a, **self._kw)\n  File \"/home/user/.local/lib/python3.6/site-packages/scrapy/core/engine.py\", line 137, in _next_request\n    if self.spider_is_idle(spider) and slot.close_if_idle:\n  File \"/home/user/.local/lib/python3.6/site-packages/scrapy/core/engine.py\", line 189, in spider_is_idle\n    if self.slot.start_requests is not None:\nbuiltins.AttributeError: 'NoneType' object has no attribute 'start_requests'\ncode\n def start_requests(self):\n        for wechat_config in self.wechat_list:\n            wechat_name = wechat_config.name\n            variety = wechat_config.variety\n            search_allow_rule = wechat_config.search_allow_rule\n            wechat_id = wechat_config.wechat_id\n            wx_id = wechat_config.wx_id\n\n            url = \"https://weixin.sogou.com/weixin?type=2&s_from=input&query={}&ie=utf8&_sug_=y&_sug_type_=\".format(parse.quote(wechat_id))\n            self.headers = {\n                    \"Host\": 'weixin.sogou.com',\n                    \"Upgrade-Insecure-Requests\":'1',\n                    \"Accept\":'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n                    \"Referer\": 'https://weixin.sogou.com/',\n                    \"Accept-Encoding\":'gzip, deflate, br',\n                    \"Accept-Language\":'en-US,en;q=0.9'\n                }\n            self.headers['User-Agent'] = random.choice(get_project_settings().get('MC_USER_AGENT'))\n            response = requests.get(url, headers=self.headers, cookies={}, timeout=300)\n            if str(response.content.decode('utf-8')).find(self.error_msg) > 0:\n                self.crawler.engine.close_spider(self, 'response msg error {}, url {}!'.format(self.error_msg, url))\n                return\nVersions\nScrapy (1.6.0)", "issue_status": "Closed", "issue_reporting_time": "2019-08-30T10:19:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "72": {"issue_url": "https://github.com/scrapy/scrapy/issues/3986", "issue_id": "#3986", "issue_summary": "Logformatter for errors", "issue_description": "mredaelli commented on Aug 29, 2019\nSummary\nAllow users to specify the logging behaviour in case of errors in the item pipeline\nMotivation\nLooking at https://github.com/scrapy/scrapy/blob/3adf09bc4f1468f54a703481d6d7ebfe25b6f232/scrapy/core/scraper.py, it is possible to override the logging behaviour for the DropItem exception, but there is no equivalent\nself.logformatter.error.\nIn our case, sometimes the item can be very large, in which case it being printed completely to the log is problematic -- specifically, on ScrapyingHub the log entry is refused and the exception is not visible.\nDescribe alternatives you've considered\nModify that file adding a generic error logformatter function.", "issue_status": "Closed", "issue_reporting_time": "2019-08-29T05:34:26Z", "fixed_by": "#3989", "pull_request_summary": "LogFormatter improvements", "pull_request_description": "Member\nelacuesta commented on Aug 29, 2019 \u2022\nedited\nFixes #3986\nI'll add tests shortly Done", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-19T08:44:44Z", "files_changed": [["6", "scrapy/core/scraper.py"], ["11", "scrapy/logformatter.py"], ["49", "tests/test_logformatter.py"]]}, "73": {"issue_url": "https://github.com/scrapy/scrapy/issues/3985", "issue_id": "#3985", "issue_summary": "Contracts can't be written for callbacks that receive cb_kwargs", "issue_description": "jvani commented on Aug 29, 2019 \u2022\nedited\nDescription\nContracts can't be written for callbacks that receive cb_kwargs\nSteps to Reproduce\nRun scrapy check with the following spider:\nclass CbKwargsContract(scrapy.Spider):\n    name = 'cb_kwargs_contract'\n\n    def start_requests(self):\n        yield scrapy.Request(\"https://httpbin.org/get\", cb_kwargs={\"arg1\": \"foo\"})\n\n    def parse(self, response, arg1):\n        \"\"\"\n        @url https://httpbin.org/get\n        \"\"\"\n        self.logger.info(arg1)\nExpected behavior:\nIdeally would be possible to add args to the contract (e.g., @url https://httpbin.org/get foo)\nActual behavior: Fails with a TypeError\nReproduces how often: 100%\nVersions\nScrapy 1.7.3", "issue_status": "Closed", "issue_reporting_time": "2019-08-28T19:11:27Z", "fixed_by": "#3988", "pull_request_summary": "CallbackKeywordArgumentsContract", "pull_request_description": "Member\nelacuesta commented on Aug 29, 2019 \u2022\nedited\nFixes #3985\nThis patch works partially, in the sense that it prevents the reported TypeError (I checked, for instance by adding a simple print(request.cb_kwargs) to https://github.com/scrapy/scrapy/blob/1.7.3/scrapy/core/scheduler.py#L90, and the generated request does include a valid cb_kwargs attribute). However, adding the @cb_kwargs contract causes the whole callback to be excluded from the tests, and I can't for the life of me figure out why.\nUpdate: Working now, thanks to @victor-torres \ud83d\ude80\nRemaining tasks:\nDocs\nTests\nFigure out why it doesn't work", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-07T23:23:16Z", "files_changed": [["10", "docs/topics/contracts.rst"], ["12", "scrapy/contracts/__init__.py"], ["16", "scrapy/contracts/default.py"], ["1", "scrapy/settings/default_settings.py"], ["78", "tests/test_contracts.py"]]}, "74": {"issue_url": "https://github.com/scrapy/scrapy/issues/3984", "issue_id": "#3984", "issue_summary": "Add flag to disable Scraped debug messages", "issue_description": "Contributor\nwatsta commented on Aug 28, 2019\nSummary\nAdd a flag to settings (e.g. LOG_SCRAPED_ENABLED) to be able to disable the logging of each scraped Item on DEBUG level.\nMotivation\nCrawling with Scrapy on log level DEBUG provides a lot of useful context during development. The only problem is the \"scraped\" message (constructed in scrapy.logformatter.LogFormatter.scraped) - it obfuscates the log entirely in the command line and can be really frustrating in general. Adding a flag to disable this log message (with a default value so that the current behaviour doesn't change) would be really useful.\nDescribe alternatives you've considered\nRight now you either put up with it or use INFO level instead which makes you lose a lot of useful log entries available in DEBUG level.", "issue_status": "Closed", "issue_reporting_time": "2019-08-28T12:36:12Z", "fixed_by": "#3987", "pull_request_summary": "LogFormatter: Add the ability to skip log messages", "pull_request_description": "Contributor\nwatsta commented on Aug 29, 2019\nResolves #3984", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-16T12:12:04Z", "files_changed": [["7", "scrapy/core/engine.py"], ["6", "scrapy/core/scraper.py"], ["4", "scrapy/logformatter.py"], ["66", "tests/test_logformatter.py"]]}, "75": {"issue_url": "https://github.com/scrapy/scrapy/issues/3981", "issue_id": "#3981", "issue_summary": "How to prevent scrapy from forcing the key of the request headers into uppercase?", "issue_description": "LoveDarkblue commented on Aug 28, 2019\nDescription\nI have a api doc that need send a header with the key that start with lowercase word.\nBut the scrapy always change the key of the headers into capitalized.\nI'm setting the headers like this:\nheaders = {\n    \"sign\": 'sssssssign',\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n}\nyield scrapy.Request(url,headers=headers, callback=self.parse_page, meta={'cookiejar': 1})\nThen i found that \"sign\" is changed into \"Sign\" in the DEBUG log.\nAnd then i try to do like this:\nfrom twisted.web.http_headers import Headers as TwistedHeaders\n\nTwistedHeaders._caseMappings.update({\n    b'sign': b'sign',  \n    b'User-Agent': b'user-agent',\n})\nI put the code in start_request method of spider file, but I still got the following log:\n{b'User-Agent': [b'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'], b'Accept-Language': [b'en'], b'Sign': [b'sssssssign'], b'Accept': [b'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8']}\nHow to prevent scrapy from forcing the key of the request headers in uppercase?", "issue_status": "Closed", "issue_reporting_time": "2019-08-28T03:39:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "76": {"issue_url": "https://github.com/scrapy/scrapy/issues/3980", "issue_id": "#3980", "issue_summary": "command 'x86_64-linux-gnu-gcc' failed with exit status 1", "issue_description": "bhojanidhrumil commented on Aug 28, 2019\nroot@kali:~/PycharmProjects/net_cut# pip install netfilterqueue\nCollecting netfilterqueue\nUsing cached https://files.pythonhosted.org/packages/39/c4/8f73f70442aa4094b3c37876c96cddad2c3e74c058f6cd9cb017d37ffac0/NetfilterQueue-0.8.1.tar.gz\nBuilding wheels for collected packages: netfilterqueue\nRunning setup.py bdist_wheel for netfilterqueue ... error\nComplete output from command /usr/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-build-aQnS2K/netfilterqueue/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /tmp/tmp3xRJkVpip-wheel- --python-tag cp27:\nrunning bdist_wheel\nrunning build\nrunning build_ext\nbuilding 'netfilterqueue' extension\ncreating build\ncreating build/temp.linux-x86_64-2.7\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-rHYsq1/python2.7-2.7.15=. -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c netfilterqueue.c -o build/temp.linux-x86_64-2.7/netfilterqueue.o\nnetfilterqueue.c:437:10: fatal error: libnfnetlink/linux_nfnetlink.h: No such file or directory\n#include \"libnfnetlink/linux_nfnetlink.h\"\n^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nerror: command 'x86_64-linux-gnu-gcc' failed with exit status 1\nFailed building wheel for netfilterqueue\nRunning setup.py clean for netfilterqueue\nFailed to build netfilterqueue\nInstalling collected packages: netfilterqueue\nRunning setup.py install for netfilterqueue ... error\nComplete output from command /usr/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-build-aQnS2K/netfilterqueue/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-qEsuww-record/install-record.txt --single-version-externally-managed --compile:\nrunning install\nrunning build\nrunning build_ext\nbuilding 'netfilterqueue' extension\ncreating build\ncreating build/temp.linux-x86_64-2.7\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fdebug-prefix-map=/build/python2.7-rHYsq1/python2.7-2.7.15=. -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c netfilterqueue.c -o build/temp.linux-x86_64-2.7/netfilterqueue.o\nnetfilterqueue.c:437:10: fatal error: libnfnetlink/linux_nfnetlink.h: No such file or directory\n#include \"libnfnetlink/linux_nfnetlink.h\"\n^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nerror: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n----------------------------------------\nCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-build-aQnS2K/netfilterqueue/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-qEsuww-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-aQnS2K/netfilterqueue/", "issue_status": "Closed", "issue_reporting_time": "2019-08-28T03:04:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "77": {"issue_url": "https://github.com/scrapy/scrapy/issues/3977", "issue_id": "#3977", "issue_summary": "Image Pipeline can't be used", "issue_description": "vionemc commented on Aug 27, 2019 \u2022\nedited\nEvery time I import image pipeline, not even using it, I get this error.\n     from scrapy.pipelines.images import ImagesPipeline\n  File \"c:\\python27\\lib\\site-packages\\scrapy\\pipelines\\images.py\", line 15, in <module>\n    from PIL import Image\nImportError: No module named PIL\nI've tried all kind of installation method mentioned here\nhttps://stackoverflow.com/questions/8863917/importerror-no-module-named-pil\nNone works. Is there any way I can use imagepipeline?\nVersions\nScrapy : 1.5.1\nlxml : 4.2.5.0\nlibxml2 : 2.9.5\ncssselect : 1.0.3\nparsel : 1.5.1\nw3lib : 1.19.0\nTwisted : 18.9.0\nPython : 2.7.15 (v2.7.15:ca079a3ea3, Apr 30 2018, 16:30:26) [MSC v.1500 64 bit (AMD64)]\npyOpenSSL : 18.0.0 (OpenSSL 1.1.0j 20 Nov 2018)\ncryptography : 2.4.2\nPlatform : Windows-10-10.0.17134", "issue_status": "Closed", "issue_reporting_time": "2019-08-27T06:58:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "78": {"issue_url": "https://github.com/scrapy/scrapy/issues/3976", "issue_id": "#3976", "issue_summary": "`ItemLoader` fields initialized from `item` are reprocessed", "issue_description": "alexander-matsievsky commented on Aug 26, 2019 \u2022\nedited\nDescription\n#3804 introduced a bug where ItemLoader fields are reprocessed.\nRelated #3897.\nSteps to Reproduce\nfrom pprint import pprint\n\nfrom scrapy import Field, Item\nfrom scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import TakeFirst\n\n\nclass X(Item):\n    x = Field(output_processor=TakeFirst())\n\n\nloader = ItemLoader(X())\nloader.add_value(\"x\", [\"value1\", \"value2\"])\nx = loader.load_item()\npprint(x)\n# {'x': 'value1'}\n\npprint(ItemLoader(x).load_item())\n# {'x': 'v'}\nExpected behavior: ItemLoader initialized from the x item does not reprocess its fields and loads {'x': 'value1'}.\nActual behavior: ItemLoader initialized from the x item reprocesses its fields and loads {'x': 'v'}.\nVersions\nScrapy       : 1.7.3\nlxml         : 4.4.1.0\nlibxml2      : 2.9.9\ncssselect    : 1.1.0\nparsel       : 1.5.2\nw3lib        : 1.21.0\nTwisted      : 19.7.0\nPython       : 3.6.5 (default, May  3 2018, 10:08:28) - [GCC 5.4.0 20160609]\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1c  28 May 2019)\ncryptography : 2.7\nPlatform     : Linux-4.4.0-127-generic-x86_64-with-LinuxMint-18.1-serena\nAdditional context\nHere's the behavior of the previous version:\nScrapy       : 1.6.0\nlxml         : 4.4.0.0\nlibxml2      : 2.9.9\ncssselect    : 1.0.3\nparsel       : 1.5.1\nw3lib        : 1.20.0\nTwisted      : 19.7.0\nPython       : 3.6.5 (default, May  3 2018, 10:08:28) - [GCC 5.4.0 20160609]\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1c  28 May 2019)\ncryptography : 2.7\nPlatform     : Linux-4.4.0-127-generic-x86_64-with-LinuxMint-18.1-serena\n# {'x': 'value1'}\n# {'x': 'value1'}\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2019-08-26T12:18:42Z", "fixed_by": "#4036", "pull_request_summary": "ItemLoader: improve handling of initial item", "pull_request_description": "Member\nelacuesta commented on Sep 25, 2019 \u2022\nedited\nThird time's the charm \ud83e\udd1e\nFixes #3897, fixes #3976, supersedes #3998.\nContinuing the work of @sortafreel, updating the implementation and adding more granular tests. Please do let me know if you think I'm missing some test case.", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-28T09:53:54Z", "files_changed": [["10", "docs/topics/loaders.rst"], ["31", "scrapy/loader/__init__.py"], ["319", "tests/test_loader.py"]]}, "79": {"issue_url": "https://github.com/scrapy/scrapy/issues/3974", "issue_id": "#3974", "issue_summary": "A question about robots.txt in ebay site", "issue_description": "Remember2018 commented on Aug 26, 2019\nHi Dear all, I use the great scrapy to crawl the information in some sites. And when I'm trying to crawl the items in ebay site, the ebay site seems disable the crawler using robots.txt. But when I use the detail information page url, e.g., \"https://www.ebay.com/itm/8-New-Universal-12pcs-Hot-Air-Fryer-Kits-w-Cake-Cup-Cake-Basket-5-2QT-8QT/401752292159\", the scrapy can get the webpage without \"Disallow by robots.txt\". So my question is, can I continue to crawl the eBay with Scrapy without considering the Robots.txt? I have set the \"ROBOTSTXT_OBEY=True\", but it seems some pages in eBay can be crawlled.\nI want to obey the rules and I don't want to pay fees for the crawling from the eBay. I'm afraid that the eBay will send me a notice for the crawling.", "issue_status": "Closed", "issue_reporting_time": "2019-08-26T03:43:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "80": {"issue_url": "https://github.com/scrapy/scrapy/issues/3970", "issue_id": "#3970", "issue_summary": "LinkExtractor transparently failing with xpaths", "issue_description": "floggle commented on May 7, 2018\nI was trying to extract links using the LinkExtractor (https://doc.scrapy.org/en/latest/topics/link-extractors.html) and some xpaths, and it's failing to find any links at all if I include an Xpath.\nThese two URL's:\nhttps://coastalmap.marine.usgs.gov/mapservices/wms_svc.html\nhttps://nationalmap.gov/small_scale/infodocs/webservices.html\nlink_extractor = LinkExtractor( restrict_xpaths=my_xpath) tmp_links = link_extractor.extract_links(response)\nEven with something as simple as /html as the xpath, I get no links back.\nIf I remove the restrict_xpaths keyword arg, it works and returns all of the links on the page.\nI understand that the HTML on those two pages seems a bit broken (I tried some online xpath extractors to see what was going on, they all choked on it), but in those circumstances I'd expect it to raise an exception or something to indicate the problem is with the page and not a valid-but-bad xpath which is what just I spent a bunch of time investigating.\nAlso a docs problem: the link extractors page (linked above) says this:\n\"restrict_xpaths (str or list) \u2013 is an XPath (or list of XPath\u2019s) which defines regions inside the response where links should be extracted from. If given, only the text selected by those XPath will be scanned for links. See examples below.\"\nBut there are no \"examples below\", or anywhere.", "issue_status": "Closed", "issue_reporting_time": "2018-05-07T13:42:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "81": {"issue_url": "https://github.com/scrapy/scrapy/issues/3969", "issue_id": "#3969", "issue_summary": "Make Protego the default robots.txt parser", "issue_description": "Contributor\nanubhavp28 commented on Aug 19, 2019 \u2022\nedited\nProtego is more compliant robots.txt parser than the current default RobotFileParser. It supports wildcard matching, length based directive ordering, sitemaps, crawl-delay and request-rate directive. It is also compatible with Google's robots.txt parser. AFAIK, It has bigger test suite than any other open source robots.txt parser. The latest version of Protego is on PyPI and can be installed with pip install protego.\nIt also faster than other Python based candidates. I performed a benchmark using popular robots.txt parsers -\nI crawled multiple domains (~1,100 domains) and downloaded links to pages as the crawler encounter them. I downloaded 111, 824 links in total. Next I made each robots.txt parser - parse and answer query (I made parsers answer each query 5 times) in an order similar to how they would need to in a broad crawl. Here are the results :\nProtego (Python based) :\n25th percentile : 0.002419 seconds\n50th percentile : 0.006798 seconds\n75th percentile : 0.014307 seconds\n100th percentile : 2.546978 seconds\nTotal Time : 19.545984 seconds\nRobotFileParser (default in Scrapy) :\n25th percentile : 0.002188 seconds\n50th percentile : 0.005350 secondsstyle\n75th percentile : 0.010492 seconds\n100th percentile : 1.805923 seconds\nTotal Time : 13.799954 seconds\nRobotexclusionrulesparser (Python based) :\n25th percentile : 0.001288 seconds\n50th percentile : 0.005222 seconds\n75th percentile : 0.014640 seconds\n100th percentile : 52.706880 seconds\nTotal Time : 76.460496 seconds\nReppy Parser (C++ based with Python interface) :\n25th percentile : 0.000210 seconds\n50th percentile : 0.000492 seconds\n75th percentile : 0.000997 seconds\n100th percentile : 0.129440 seconds\nTotal Time: 1.405558 seconds\nThe code and data used for benchmarking is here.", "issue_status": "Closed", "issue_reporting_time": "2019-08-19T08:02:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "82": {"issue_url": "https://github.com/scrapy/scrapy/issues/3968", "issue_id": "#3968", "issue_summary": "Spider allowed_domains error", "issue_description": "hewm commented on Aug 19, 2019\nallowed_domains = ['digikey.com']\nThe url of the spider request only enters the process_request in middlewares.py but some of the urls that have not entered the spider may be filtered. However, filtering the domain name list cannot change the following request.\n2019-08-19 15:37:48 [dgk_update_detail] INFO: [request url] https://www.digikey.com/product-detail/en/te-connectivity-raychem-cable-protection/55A6866-24-9-96CS2621/55A6866-24-9-96CS2621-ND/5293718\n2019-08-19 15:37:55 [dgk_update_detail] INFO: [request url] https://www.digikey.com/product-detail/en/laird-technologies-ias/EXC821BN/994-1012-ND/2392191\n2019-08-19 15:38:02 [dgk_update_detail] INFO: [request url] https://www.digikey.com/product-detail/en/taoglas-limited/G24.A.W.305111/931-1244-ND/4250291\n2019-08-19 15:38:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)", "issue_status": "Closed", "issue_reporting_time": "2019-08-19T07:43:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "83": {"issue_url": "https://github.com/scrapy/scrapy/issues/3965", "issue_id": "#3965", "issue_summary": "Random Twisted error: Builtins.KeyError: -2", "issue_description": "Colafroth commented on Aug 19, 2019\n2019-08-19 13:20:57 [twisted] CRITICAL: Unhandled Error\n2019-08-19T03:20:57.473794+00:00 app[worker.1]: Traceback (most recent call last):\n2019-08-19T03:20:57.473796+00:00 app[worker.1]: File \"/usr/src/ScrapyProjects/ScrapyController.py\", line 37, in execute\n2019-08-19T03:20:57.473798+00:00 app[worker.1]: self.crawler.start()\n2019-08-19T03:20:57.473800+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/scrapy/crawler.py\", line 293, in start\n2019-08-19T03:20:57.473803+00:00 app[worker.1]: reactor.run(installSignalHandlers=False)  # blocking call\n2019-08-19T03:20:57.473805+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/twisted/internet/base.py\", line 1272, in run\n2019-08-19T03:20:57.473807+00:00 app[worker.1]: self.mainLoop()\n2019-08-19T03:20:57.473809+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/twisted/internet/base.py\", line 1281, in mainLoop\n2019-08-19T03:20:57.473811+00:00 app[worker.1]: self.runUntilCurrent()\n2019-08-19T03:20:57.473813+00:00 app[worker.1]: --- <exception caught here> ---\n2019-08-19T03:20:57.473815+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/twisted/internet/base.py\", line 902, in runUntilCurrent\n2019-08-19T03:20:57.473817+00:00 app[worker.1]: call.func(*call.args, **call.kw)\n2019-08-19T03:20:57.473819+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/scrapy/utils/reactor.py\", line 41, in __call__\n2019-08-19T03:20:57.473821+00:00 app[worker.1]: return self._func(*self._a, **self._kw)\n2019-08-19T03:20:57.473822+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/scrapy/core/engine.py\", line 122, in _next_request\n2019-08-19T03:20:57.473824+00:00 app[worker.1]: if not self._next_request_from_scheduler(spider):\n2019-08-19T03:20:57.473825+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/scrapy/core/engine.py\", line 149, in _next_request_from_scheduler\n2019-08-19T03:20:57.473827+00:00 app[worker.1]: request = slot.scheduler.next_request()\n2019-08-19T03:20:57.473828+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/scrapy/core/scheduler.py\", line 71, in next_request\n2019-08-19T03:20:57.473830+00:00 app[worker.1]: request = self._dqpop()\n2019-08-19T03:20:57.473831+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/scrapy/core/scheduler.py\", line 106, in _dqpop\n2019-08-19T03:20:57.473833+00:00 app[worker.1]: d = self.dqs.pop()\n2019-08-19T03:20:57.473834+00:00 app[worker.1]: File \"/usr/local/lib/python3.7/site-packages/queuelib/pqueue.py\", line 42, in pop\n2019-08-19T03:20:57.473836+00:00 app[worker.1]: q = self.queues[self.curprio]\n2019-08-19T03:20:57.473837+00:00 app[worker.1]: builtins.KeyError: -2\nThe spider runs smoothly but sometimes this error keeps popping up. I have no clue what it is and couldn't find anything from online.", "issue_status": "Closed", "issue_reporting_time": "2019-08-19T03:29:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "84": {"issue_url": "https://github.com/scrapy/scrapy/issues/3964", "issue_id": "#3964", "issue_summary": "Scrapy redis distributed", "issue_description": "hewm commented on Aug 19, 2019\nThe scrapy crawler request link only enters the process_request method in middlewares.py, does not enter the parse method in the spider, is the scrapy distributed using scrapy-redis, I would like to ask if it is possible because the current url has been requested. Did not continue to execute downward", "issue_status": "Closed", "issue_reporting_time": "2019-08-19T01:24:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "85": {"issue_url": "https://github.com/scrapy/scrapy/issues/3963", "issue_id": "#3963", "issue_summary": "Implementing distributed problems with scrapy-redis", "issue_description": "hewm commented on Aug 19, 2019\nWhen using scrapy-redis for distributed, if there are already ten machines crawling at the moment, but the speed is not in line with the intention, can the original code be deployed on the new machine, and the new machine will not be down on the basis of new machines. Crawling machine", "issue_status": "Closed", "issue_reporting_time": "2019-08-19T01:21:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "86": {"issue_url": "https://github.com/scrapy/scrapy/issues/3962", "issue_id": "#3962", "issue_summary": "337", "issue_description": "15610026667 commented on Aug 18, 2019\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2019-08-18T12:53:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "87": {"issue_url": "https://github.com/scrapy/scrapy/issues/3957", "issue_id": "#3957", "issue_summary": "responsetypes.py can have better backward compatibility", "issue_description": "Contributor\nsbs2001 commented on Aug 14, 2019\nIn line 8 we are directly importing StringIO from io which as far as I know is not supported in Python 2.x.x\nI think we should first check the Python version and then do the import.", "issue_status": "Closed", "issue_reporting_time": "2019-08-14T17:38:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "88": {"issue_url": "https://github.com/scrapy/scrapy/issues/3956", "issue_id": "#3956", "issue_summary": "Recursive download of webpage is not working.", "issue_description": "rameshrpi commented on Aug 14, 2019\nHi,\nI have created the spider as given below,scrap_urls has list of links which needs to downloded.\nMy website has some authentication I have handled it and I can able to download some files.\nI just added the recursive download in the function\ndef parse_httpbin(self, response):\nyield scrapy.Request(response.urljoin(href), self.parse_httpbin)\nAfter some time it shows the error message\nDEBUG: Retrying <GET https://xxxx.com> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>].\nI hope I am sending the mutiple Urls to server from def after_login(self, response): function and as well as inside def parse_httpbin(self, response): function.\nCould please help us how to resolve the issue?\nscrap_urls = [\n\"http://www.google.com\",\n\"http://www.google.com\",\n\"http://www.google.com\"\n]\nrecursive_urls = []\n\ndef __init__(self, *args, **kwargs):\n    super(WebSpider, self).__init__(*args, **kwargs)\n\ndef start_requests(self):\n    for u in self.start_urls:\n        yield scrapy.Request(u, callback=self.parse,\n                             errback=self.errback_httpbin,\n                             dont_filter=True\n                             )\n\ndef parse(self, response):\n    return FormRequest.from_response(response,\n                                     headers={\"X-Requested-With\": \"XMLHttpRequest\"},\n                                     formdata={'password': '',\n                                               'username': ''},\n                                     callback=self.after_login)\n\ndef after_login(self, response):\n    # check login succeed before going on        \n    for url in self.scrap_urls:           \n  yield scrapy.Request(url, callback=self.parse_httpbin, errback=self.errback_httpbin, dont_filter=True)\n        \n\ndef parse_httpbin(self, response):\n    image = ImageItem()\n    file = FileItem()\n    file_urls = []\n    image_urls = []\n    time.sleep(6)\n    page_str = response.url.split(\"page\")\n    if len(page_str) < 2:\n        page_str = response.url.split(\"/\")        \n    filename = 'downloads\\\\' + self.get_page_name(response.url,False) + \".html\"\n    logging.info('filename:----->'+filename)\n    for href in response.xpath('//a/@href').getall():\n        if href != 'javascript:;':\n            if str(href).__contains__('.pdf') or str(href).__contains__('.wmv'):\n                if response.urljoin(href) not in file_urls:\n                    file_urls.append(response.urljoin(href))\n            for link in response.xpath('//link/@href').getall():\n                #print(\"Link=>\"+response.urljoin(link))\n                if not str(link).__contains__('javascript:;'):\n                    if response.urljoin(link) not in file_urls:\n                        file_urls.append(response.urljoin(link))\n            for jslink in response.xpath('//script/@src').getall():\n                #print(\"jslink=>\"+response.urljoin(jslink))\n                if not str(jslink).__contains__('javascript:;'):\n                    if response.urljoin(jslink) not in file_urls:\n                        file_urls.append(response.urljoin(jslink))\n            for imglink in response.xpath('//img/@src').getall():\n                #print(\"imglink=>\"+response.urljoin(imglink))\n                if not str(imglink).__contains__('javascript:;'):\n                    if response.urljoin(imglink) not in image_urls:\n                        image_urls.append(response.urljoin(imglink))\n            with open(filename, 'w') as f:\n                f.write(self.update_response(response))\n                file['file_urls'] = file_urls\n                image['image_urls'] = image_urls\n                yield file\n                yield image\n                f.close();\n                logging.info('File downloaded :' + filename)\n                print(\"href=>\"+response.urljoin(href))\n                logging.info(\"URL\"+response.urljoin(href))                  \n                yield scrapy.Request(response.urljoin(href), self.parse_httpbin)", "issue_status": "Closed", "issue_reporting_time": "2019-08-14T12:03:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "89": {"issue_url": "https://github.com/scrapy/scrapy/issues/3955", "issue_id": "#3955", "issue_summary": "Why does scrapy spider reintialize again if I am passing start_urls as an arguments", "issue_description": "rajathpatel23 commented on Aug 13, 2019 \u2022\nedited\nBelow is the error I am facing\nrunfile('/Users/rajatpatel/Documents/my_scripts_2/test_cluster_script.py', wdir='/Users/rajatpatel/Documents/my_scripts_2') /Users/rajatpatel/anaconda3/envs/new_python_36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+. warnings.warn(msg, category=DeprecationWarning) 2019-08-13 12:04:18 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot) 2019-08-13 12:04:18 [scrapy.utils.log] INFO: Versions: lxml 4.3.4.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.1, Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c 28 May 2019), cryptography 2.6.1, Platform Darwin-18.7.0-x86_64-i386-64bit args () **kwargs {'start_urls': ['http://www.easternmicro.com/']} This is url list: ['http://www.easternmicro.com/'] This is urls: ['http://www.easternmicro.com/']** 2019-08-13 12:04:19 [scrapy.crawler] INFO: Overridden settings: {'FEED_EXPORT_ENCODING': 'utf-8'} 2019-08-13 12:04:19 [scrapy.extensions.telnet] INFO: Telnet Password: c9554bb46519d64e 2019-08-13 12:04:19 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole', 'scrapy.extensions.memusage.MemoryUsage', 'scrapy.extensions.logstats.LogStats'] **args () kwargs {} This is url list: []** 2019-08-13 12:04:19 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats'] 2019-08-13 12:04:20 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:62141/session {\"capabilities\": {\"firstMatch\": [{}], \"alwaysMatch\": {\"browserName\": \"chrome\", \"platformName\": \"any\", \"goog:chromeOptions\": {\"extensions\": [], \"args\": [\"--headless\"]}}}, \"desiredCapabilities\": {\"browserName\": \"chrome\", \"version\": \"\", \"platform\": \"ANY\", \"goog:chromeOptions\": {\"extensions\": [], \"args\": [\"--headless\"]}}} 2019-08-13 12:04:20 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 127.0.0.1:62141 2019-08-13 12:04:20 [urllib3.connectionpool] DEBUG: http://127.0.0.1:62141 \"POST /session HTTP/1.1\" 200 689 2019-08-13 12:04:20 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request 2019-08-13 12:04:20 [scrapy.middleware] INFO: Enabled spider middlewares: ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy_selenium.SeleniumMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware'] 2019-08-13 12:04:20 [scrapy.middleware] INFO: Enabled item pipelines: [] 2019-08-13 12:04:20 [scrapy.core.engine] INFO: Spider opened 2019-08-13 12:04:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2019-08-13 12:04:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023 2019-08-13 12:04:20 [scrapy.core.engine] INFO: Closing spider (finished) [] 2019-08-13 12:04:20 [selenium.webdriver.remote.remote_connection] DEBUG: DELETE http://127.0.0.1:62141/session/78dc13cbe5d4fda7801b4cd77a76de87 {} 2019-08-13 12:04:20 [urllib3.connectionpool] DEBUG: http://127.0.0.1:62141 \"DELETE /session/78dc13cbe5d4fda7801b4cd77a76de87 HTTP/1.1\" 200 14 2019-08-13 12:04:20 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request 2019-08-13 12:04:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'finish_reason': 'finished', 'finish_time': datetime.datetime(2019, 8, 13, 16, 4, 20, 677469), 'log_count/DEBUG': 7, 'log_count/INFO': 9, 'memusage/max': 185020416, 'memusage/startup': 185016320, 'start_time': datetime.datetime(2019, 8, 13, 16, 4, 20, 667446)} 2019-08-13 12:04:20 [scrapy.core.engine] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2019-08-13T16:10:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "90": {"issue_url": "https://github.com/scrapy/scrapy/issues/3953", "issue_id": "#3953", "issue_summary": "OSError when downloading a very long url", "issue_description": "zaxtyson commented on Aug 12, 2019\nWhen you run into some horrible image url, like this\uff1a\nhttps://o.aolcdn.com/images/dims?resize=2000%2C2000%2Cshrink&image_uri=https%3A%2F%2Fo.aolcdn.com%2Fimages%2Fdimse%2F5845cadfecd996e0372f%2Fccc34660c41122e3170c0d586c151a29397c0fcf%2FY3JvcD0xOTIwJTJDMTA5NyUyQzAlMkMwJnF1YWxpdHk9ODUmZm9ybWF0PWpwZyZyZXNpemU9MTYwMCUyQzkxNCZpbWFnZV91cmk9aHR0cHMlM0ElMkYlMkZzLnlpbWcuY29tJTJGb3MlMkZjcmVhdHItdXBsb2FkZWQtaW1hZ2VzJTJGMjAxOS0wOCUyRjg2YjNlYjkwLWI5YjgtMTFlOS05ZWFlLTQ5YWU2NTcxMjM0MyZjbGllbnQ9YTFhY2FjM2UxYjMyOTA5MTdkOTImc2lnbmF0dXJlPTZmZWJkYjQwN2E0NzU0YzM0YTJjY2ViMDczNDc1YTE1ZjBiODA3OGQ%3D&client=a1acac3e1b3290917d92&signature=bf3461468aef0cb3ecaea00d2ed611e04a88bc70\nThen...\nTraceback (most recent call last):\n  File \"c:\\program files\\python37\\lib\\site-packages\\scrapy\\pipelines\\files.py\", line 419, in media_downloaded\n    checksum = self.file_downloaded(response, request, info)\n  File \"c:\\program files\\python37\\lib\\site-packages\\scrapy\\pipelines\\files.py\", line 452, in file_downloaded\n    self.store.persist_file(path, buf, info)\n  File \"c:\\program files\\python37\\lib\\site-packages\\scrapy\\pipelines\\files.py\", line 53, in persist_file\n    with open(absolute_path, 'wb') as f:\nOSError: [Errno 22] Invalid argument: 'E:\\\\2019-08-12\\\\resources\\\\885443110bae0e1149e017dbea5ca3935efa38c0.com%2Fimages%2Fdimse%2F5845cadfecd996e0372f%2F108a4af73772ae197fa2c4ec4e9fe7a47390433c%2FY3JvcD0xMTc0JTJDNTgwJTJDMCUyQzAmcXVhbGl0eT04NSZmb3JtYXQ9anBnJnJlc2l6ZT0xNjAwJTJDNzkxJmltYWdlX3VyaT1odHRwcyUzQSUyRiUyRnMueWltZy5jb20lMkZvcyUyRmNyZWF0ci11cGxvYWRlZC1pbWFnZXMlMkYyMDE5LTA4JTJGMWJmZGQxNDAtYjliYy0xMWU5LWJmZjMtMjMyNzcwMTg1MzE5JmNsaWVudD1hMWFjYWMzZTFiMzI5MDkxN2Q5MiZzaWduYXR1cmU9OTFiNzQ3Y2MyZTY5ODY3OGIxNWI0OTkyMjdjM2NmZWRlYTE1NGIxOA%3D%3D&client=a1acac3e1b3290917d92&signature=6517aece82e79d536edeaccc275ad88090df0252'\nSo\uff0cI think that when downloading a file, you should use a random name instead of intercepting it from the url.For some particularly weird urls, this will cause an OSErro when writing to the file", "issue_status": "Closed", "issue_reporting_time": "2019-08-12T10:44:19Z", "fixed_by": "#3954", "pull_request_summary": "Disallow media extensions unregistered with IANA", "pull_request_description": "Contributor\nOmarFarrag commented on Aug 13, 2019 \u2022\nedited by Gallaecio\nIf a media extension is empty string or is unregistered with IANA, then try to guess the MIME type from the url then the extension from MIME type using built-in mimetypes lib..\nFixes #3953, fixes #1287", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-16T12:04:06Z", "files_changed": [["9", "scrapy/pipelines/files.py"], ["6", "tests/test_pipeline_files.py"]]}, "91": {"issue_url": "https://github.com/scrapy/scrapy/issues/3951", "issue_id": "#3951", "issue_summary": "Scrapy disable retry middleware", "issue_description": "nedark commented on Aug 9, 2019\nI commented the line in settings.py but it continues being enabled.\nDOWNLOADER_MIDDLEWARES = {  \n       #'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,\n       'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,\n    }\nAt the start of the program it loads a lot of middlewares I didn't enable\n2019-08-09 10:43:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\nAm I missing something? Is there a way to disable it?", "issue_status": "Closed", "issue_reporting_time": "2019-08-09T08:56:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "92": {"issue_url": "https://github.com/scrapy/scrapy/issues/3949", "issue_id": "#3949", "issue_summary": "optimize the TextResponse's selector code", "issue_description": "financialfly commented on Aug 9, 2019\nI have noticed the TextResponse's selector code at scrapy/http/response/text.py:\n    @property\n    def selector(self):\n        from scrapy.selector import Selector\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector\nwhy don't change to:\n    @property\n    def selector(self):\n        if self._cached_selector is None:\n            from scrapy.selector import Selector\n            self._cached_selector = Selector(self)\n        return self._cached_selector\nThis can avoids import the Selector object multiple times", "issue_status": "Closed", "issue_reporting_time": "2019-08-09T01:46:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "93": {"issue_url": "https://github.com/scrapy/scrapy/issues/3948", "issue_id": "#3948", "issue_summary": "Counting links with scrapy", "issue_description": "natescrape commented on Aug 8, 2019\nI am trying to work out how to count the number of times a word occurs on a site. I have got to the following code, which prints true or false when the word is found, but how can i count these and return in a singe cell in csv?\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nclass ExampleSpider(CrawlSpider):\nname = 'example'\nallowed_domains = ['examplewebsite.com']\nstart_urls = ['https://examplewebsite.com']\nrules = (\n    Rule(LinkExtractor(), callback='parse_item', follow=True),\n)\n\ndef parse_item(self, response):\n example1 = bool(response.xpath('//*').re('example1'))\n example2 = bool(response.xpath('//*').re('example2'))\n \n yield{'example1' : example1,\n   'example2' : example2}", "issue_status": "Closed", "issue_reporting_time": "2019-08-07T19:48:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "94": {"issue_url": "https://github.com/scrapy/scrapy/issues/3944", "issue_id": "#3944", "issue_summary": "Addressing PEP8 by including pytest-flake8", "issue_description": "Contributor\nnoviluni commented on Aug 7, 2019 \u2022\nedited\nAs mentioned in the Scrapy Coding Style section in the docs (https://docs.scrapy.org/en/master/contributing.html#coding-style) :\n\u201cUnless otherwise specified, follow PEP 8.\u201d.\nHowever, it\u2019s known that the current codebase has a lot of parts that don\u2019t respect PEP8 guidelines (#2144).\nI know that it\u2019s difficult to address that, as it involves a lot of code to be reviewed but, however, the more time we skip that task the harder it becomes.\nTwo obvious things that we could do are:\nFixing the current code\nAdding a pipeline to check pep8\nFixing the current code can\u2019t be done in just one commit, but instead of fixing pep8 in specific project parts (example: #2207, #2429), it could better and easier to review if we fix rules one by one (for example: \"Unused imports\", \"Missing blank lines after class or function definition.\"...).\nMy proposal to avoid to write a hard to maintain pipeline is to include the pytest-flake8 package that executes flake8 when executing pytest (by adding the --flake8 flag) and raises an error when the check fails.\nIf we start excluding all the rules then we can just fix one by one from more obvious rules to more discussed rules just excluding that rule from the ignored rules list. In that way we ensure that new code respects the previous agreed flake8/pep8 rules and on the other hand we can discuss every rule if we want to follow it or not (example: #3697).\nI will add a PR to illustrate this idea.\n\ud83c\udf89 1", "issue_status": "Closed", "issue_reporting_time": "2019-08-07T10:07:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "95": {"issue_url": "https://github.com/scrapy/scrapy/issues/3942", "issue_id": "#3942", "issue_summary": "Drop support of Python 3.4?", "issue_description": "Contributor\nstarrify commented on Aug 6, 2019\nPython 3.4 has now reached its end-of-life.\nAlso Twisted has dropped support of Python 3.4 since version 19.7.0\nThis is also believed to be the cause of recent CI build failures (one example).", "issue_status": "Closed", "issue_reporting_time": "2019-08-06T15:41:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "96": {"issue_url": "https://github.com/scrapy/scrapy/issues/3931", "issue_id": "#3931", "issue_summary": "Using separate user agent for robots.txt", "issue_description": "Contributor\nanubhavp28 commented on Aug 3, 2019\nThe current implementation of RobotsTxtMiddleware (link) uses USER_AGENT setting and User-Agent header of the request to determine the user agent to use for matching in robots.txt. There is no easy way for a user to send one user-agent header value but have a different user agent string evaluated to decide which pages to crawl. This use case seems to be more common that we thought previously for example - while Googlebot uses \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\" as User-Agent header of the request, it actually uses user agent \"Googlebot\" in robots.txt for deciding which pages to crawl. Here is the an article describing this. The same seems to be the case with Bing - link. Infact, Google made this distinction very clear in their proposed robots.txt specification where they described the user agent used in robots.txt as the \"product token\".\nThe proposed solutions are implementing a new ROBOTSTXT_USER_AGENT setting, implementing a robotstxt_user_agent meta key for overriding user agent per request or both. Here is a thread that has previous discussion on the topic - #3796 (comment).\nI'm creating this issue as a place for discussion regarding whether to add this functionality into Scrapy? If yes, which route we should take?", "issue_status": "Closed", "issue_reporting_time": "2019-08-02T20:10:53Z", "fixed_by": "#3966", "pull_request_summary": "Adds ROBOTSTXT_USER_AGENT setting", "pull_request_description": "Contributor\nanubhavp28 commented on Aug 19, 2019\nFixes #3931", "pull_request_status": "Merged", "issue_fixed_time": "2019-08-28T17:00:18Z", "files_changed": [["6", "docs/topics/downloader-middleware.rst"], ["17", "docs/topics/settings.rst"], ["6", "scrapy/downloadermiddlewares/robotstxt.py"], ["1", "scrapy/settings/default_settings.py"], ["9", "tests/test_downloadermiddleware_robotstxt.py"]]}, "97": {"issue_url": "https://github.com/scrapy/scrapy/issues/3929", "issue_id": "#3929", "issue_summary": "JSONRequest naming issue", "issue_description": "Contributor\nnoviluni commented on Aug 2, 2019 \u2022\nedited\nA few months ago JSONRequest was added to Scrapy (#3505) and released with the new Scrapy version 1.7.\nHowever, I realized that the name is not in accordance with the rest of the classes: HtmlResponse, XmlResponse, XmlRpcRequest.\nIt should be named:\nJsonRequest.\nI know it isn't backward-compatible, but as it's a really recent addition and Scrapy 2.0 is on the way it could be fixed.", "issue_status": "Closed", "issue_reporting_time": "2019-08-02T17:13:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "98": {"issue_url": "https://github.com/scrapy/scrapy/issues/3928", "issue_id": "#3928", "issue_summary": "Save images on remote server", "issue_description": "bezkos commented on Aug 2, 2019\nI want to save images on a remote server via (FTP?). Is this possible with scrapy?\n(I know about S3 or Google but i mean in a custom FTP server).", "issue_status": "Closed", "issue_reporting_time": "2019-08-02T15:52:58Z", "fixed_by": "#3961", "pull_request_summary": "Add FTPFileStore to FilesPipeline", "pull_request_description": "Contributor\nOmarFarrag commented on Aug 18, 2019 \u2022\nedited\nIn this PR, a FTPFileStore class is added to scrapy.pipelines.files to add support for saving files on FTP servers.\nI added close_spider function to the FilesPipeline and from it call close_connection function of the store object, if exists. This allows opening the FTP connection only once at initialization and closing it when the spider is closed (This is different from scrapy.extensions.feedexports where the connection is opened and closed every time an item is exported).\nThere are two things I need feedback on them. First, if the username and password should be supplied in the file_store setting or have separated settings fields for them. Second thing if a more exception handling is needed when opening the connection.\nAfter resolving these things, will write tests and documentation :)\nCLOSES #3928", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-24T23:58:24Z", "files_changed": [["19", "docs/topics/media-pipeline.rst"], ["19", "scrapy/extensions/feedexport.py"], ["51", "scrapy/pipelines/files.py"], ["5", "scrapy/pipelines/images.py"], ["21", "scrapy/utils/ftp.py"], ["23", "scrapy/utils/test.py"], ["28", "tests/test_pipeline_files.py"]]}, "99": {"issue_url": "https://github.com/scrapy/scrapy/issues/3926", "issue_id": "#3926", "issue_summary": "Enable override signal handlers", "issue_description": "Urahara commented on Jul 31, 2019 \u2022\nedited\nI read the docs and i see the only method that exists is \"spider_closed\" but this is executed after the termination , i want to ignore signal and keep my spider running?\nOutput when i run kill -TERM {spider_pid} command:\n[scrapy.crawler] INFO: Received SIGTERM, shutting down gracefully. Send again to force\nDiging a lit bit i found out that crawler.py file its where the signals are handled.\nscrapy/scrapy/crawler.py\nLines 272 to 277 in 0d51f9c\n def _signal_shutdown(self, signum, _): \n     install_shutdown_handlers(self._signal_kill) \n     signame = signal_names[signum] \n     logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \", \n                 {'signame': signame}) \n     reactor.callFromThread(self._graceful_stop_reactor) \nIs possible to make this configurable? Like configure_logging?", "issue_status": "Closed", "issue_reporting_time": "2019-07-31T15:04:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "100": {"issue_url": "https://github.com/scrapy/scrapy/issues/3924", "issue_id": "#3924", "issue_summary": "command: scrapy crawl XXX, has started wrong spider", "issue_description": "My-captain commented on Jul 31, 2019\nI have 2 spiders in my scrapy project, one is 'metalnews', the other is 'metalnewshistory'.\nHowever, when I run the command: scrapy crawl metalnewshistory, It's run the 'metalnews' spider.\nHow can I solve the situation?\nMetalnewsHistory.py\n\nMetalnewsSpider.py\n\nBest wishes for you.", "issue_status": "Closed", "issue_reporting_time": "2019-07-31T05:45:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "101": {"issue_url": "https://github.com/scrapy/scrapy/issues/3921", "issue_id": "#3921", "issue_summary": "How to provide data to other app\u2160ication", "issue_description": "yfgu commented on Jul 30, 2019\nAfter saving data into db\uff0chow does scrapy provide data to other app\u2160ication.Do l need to write another web app\u2160ication to provide open api\uff1f", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T17:45:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "102": {"issue_url": "https://github.com/scrapy/scrapy/issues/3917", "issue_id": "#3917", "issue_summary": "Work around tox #1369", "issue_description": "Member\nGallaecio commented on Jul 30, 2019\nThe Jessie build is affected by tox-dev/tox#1369, which may be hiding a different issue.\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T10:15:32Z", "fixed_by": "#3923", "pull_request_summary": "Pin Travis-ci build environment to previous default: Trusty", "pull_request_description": "Member\ndangra commented on Jul 31, 2019 \u2022\nedited\nTravis-ci changed the default build environment to Xenial as explained in https://blog.travis-ci.com/2019-04-15-xenial-default-build-environment\nThis causes builds meant for Debian Jessie to break as noted by @wRAR in #3917 (comment)\nThis change pins the environment to known working ubuntu trusty distribution prior to dropping Jessie support and upgrade to Xenial as base.\nCloses #3917\nNote: Backport this change to 1.7 branch once it is merged to master.\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-31T15:07:45Z", "files_changed": [["1", ".travis.yml"]]}, "103": {"issue_url": "https://github.com/scrapy/scrapy/issues/3916", "issue_id": "#3916", "issue_summary": "Parsing the robots.txt in RobotsTxtMiddleware should skip the asterisk symbol at the end.", "issue_description": "MikhailKavaliou commented on Jul 30, 2019\nThe RobotsTxtMiddleware is enabled with the ROBOTSTXT_OBEY=True.\nNonetheless, the built-in filter in the middleware passes URLs, that are basically disallowed.\nFor example there is the robots.txt rules like the following:\n...\nDisallow: /beta/\nDisallow: /inventory/srp.html?*\nDisallow: /search/*\n...\nIn this case, all /beta/ links will be ignored as expected, but the 2 next rules do not filter correct URLs, because the asterisk at the end is interpreted as a symbol to be included in the URL.\nRegarding that the condition in the RobotsTxtMiddleware#the process_request_2 is False for disallowed URLs:\nif not rp.can_fetch(to_native_str(self._useragent), request.url):\n            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                         {'request': request}, extra={'spider': spider})\n            self.crawler.stats.inc_value('robotstxt/forbidden')\n            raise IgnoreRequest(\"Forbidden by robots.txt\")", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T09:39:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "104": {"issue_url": "https://github.com/scrapy/scrapy/issues/3914", "issue_id": "#3914", "issue_summary": "block comment should start with '# '", "issue_description": "serhii73 commented on Jul 30, 2019\nscrapy startproject creating a scrapy project with settings.py. In this file broken pep8 rule.\nE265 block comment should start with '# '\nhttps://www.python.org/dev/peps/pep-0008/#block-comments\nInline comments should be separated by at least two spaces from the statement", "issue_status": "Closed", "issue_reporting_time": "2019-07-30T09:33:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "105": {"issue_url": "https://github.com/scrapy/scrapy/issues/3912", "issue_id": "#3912", "issue_summary": "Install on Python 3.4 is failing", "issue_description": "Contributor\nrennerocha commented on Jul 30, 2019 \u2022\nedited\nlxml 4.4.0 dropped support for Python 3.4 so installing Scrapy using pip install Scrapy is failing.\n(.venv) renne@girafa:~/projects/tmp/test-scrapy-py34 $ pip install scrapy\nDEPRECATION: Python 3.4 support has been deprecated. pip 19.1 will be the last one supporting it. Please upgrade your Python as Python 3.4 won't be maintained after March 2019 (cf PEP 429).\nCollecting scrapy\n  Using cached https://files.pythonhosted.org/packages/a3/b1/d1ab5b3f84640097cf5ff642e2e357546781746d4fec2ebb40432904c57d/Scrapy-1.7.2-py2.py3-none-any.whl\nCollecting pyOpenSSL (from scrapy)\n  Using cached https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl\nCollecting Twisted<=19.2.0,>=13.1.0; python_version == \"3.4\" (from scrapy)\n  Using cached https://files.pythonhosted.org/packages/f8/2b/a80a70f71eb2b86992ffa5aaae41457791ae67faa70927fd16b76127c2b7/Twisted-19.2.0.tar.bz2\nCollecting parsel>=1.5 (from scrapy)\n  Using cached https://files.pythonhosted.org/packages/96/69/d1d5dba5e4fecd41ffd71345863ed36a45975812c06ba77798fc15db6a64/parsel-1.5.1-py2.py3-none-any.whl\nRequirement already satisfied: six>=1.5.2 in ./.venv/lib/python3.4/site-packages (from scrapy) (1.12.0)\nRequirement already satisfied: PyDispatcher>=2.0.5 in ./.venv/lib/python3.4/site-packages (from scrapy) (2.0.5)\nCollecting w3lib>=1.17.0 (from scrapy)\n  Using cached https://files.pythonhosted.org/packages/81/43/9dcf92a77f5f0afe4f4df2407d7289eea01368a08b64bda00dd318ca62a6/w3lib-1.20.0-py2.py3-none-any.whl\nCollecting cssselect>=0.9 (from scrapy)\n  Using cached https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\nCollecting service-identity (from scrapy)\n  Using cached https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\nRequirement already satisfied: queuelib in ./.venv/lib/python3.4/site-packages (from scrapy) (1.5.0)\nCollecting lxml (from scrapy)\n  Using cached https://files.pythonhosted.org/packages/e1/f5/5eb3b491958dcfdcfa5daae3c655ab59276bc216ca015e44743c9c220e9e/lxml-4.4.0.tar.gz\n    ERROR: Complete output from command python setup.py egg_info:\n    ERROR: This lxml version requires Python 2.7, 3.5 or later.\n    ----------------------------------------\nERROR: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-qdo2c8yz/lxml/\nUsing lxml 4.3.5 works fine.", "issue_status": "Closed", "issue_reporting_time": "2019-07-29T21:49:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "106": {"issue_url": "https://github.com/scrapy/scrapy/issues/3906", "issue_id": "#3906", "issue_summary": "_monkeypatches.py has some code that can be deleted", "issue_description": "Contributor\nsbs2001 commented on Jul 27, 2019\nI may be wrong but,let me explain:\nIn our init.py code we are terminating the program if the Python version is less than 2.7\nAfter that we are applying monkeypatches, which again checks for Python version(if it is Python 2.x it applies some tweaks to fix some bugs).\nWhat's the point of checking of the python version again in monkeypatches if we already know it is greater than 2.7?", "issue_status": "Closed", "issue_reporting_time": "2019-07-27T06:28:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "107": {"issue_url": "https://github.com/scrapy/scrapy/issues/3904", "issue_id": "#3904", "issue_summary": "S3FilesStore has incomplete header to botocore mapping", "issue_description": "davkuyek commented on Jul 26, 2019\nIf botocore is installed, Scrapy's S3FilesStore allows a limited number of headers to be applied when persisting files to S3. The headers are converted to their botocore s3 put_object method equivalent argument. The mapping however is incomplete.\nPut-object request headers\nBotocore arguments\nMissing headers and their equivalent options:\nx-amz-storage-class -> StorageClass\nx-amz-tagging -> Tagging\nx-amz-website -redirect-location -> WebsiteRedirectLocation\nx-amz-object-lock-mode -> ObjectLockMode\nx-amz-object-lock-retain-until-date -> ObjectLockRetainUntilDate\nx-amz-object-lock-legal-holdv -> ObjectLockLegalHoldStatus\nx-amz-server-side -encryption -> ServerSideEncryption\nx-amz-server-side-encryption-aws-kms-key-id -> SSEKMSKeyId\nx-amz-server-side-encryption-context -> SSEKMSEncryptionContext\nx-amz-server-side -encryption -customer-algorithm -> SSECustomerAlgorithm\nx-amz-server-side -encryption -customer-key -> SSECustomerKey\nIf you don't have botocore installed you are able to persist a file with the storage class STANDARD_IA using the header x-amz-storage-class. With botocore, if you try to do the same thing, the type error Header \"x-amz-storage-class\" is not supported by botocore will be raised.\nTest code:\nfrom scrapy.pipelines.files import S3FilesStore\nfrom io import BytesIO\n\ns3_file_store = S3FilesStore('s3://your-bucket/')\ns3_file_store.persist_file('s3://your-bucket/your-key', BytesIO(b'test'), None, None, {'x-amz-storage-class': 'STANDARD_IA'})\nscrapy version -v\nScrapy : 1.4.0\nlxml : 4.3.3.0\nlibxml2 : 2.9.9\ncssselect : 1.0.3\nparsel : 1.5.1\nw3lib : 1.20.0\nTwisted : 19.2.1\nPython : 2.7.10 (default, Feb 22 2019, 21:55:15) - [GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.37.14)]\npyOpenSSL : 16.2.0 (OpenSSL 1.1.1c 28 May 2019)\nPlatform : Darwin-18.6.0-x86_64-i386-64bit", "issue_status": "Closed", "issue_reporting_time": "2019-07-25T21:20:17Z", "fixed_by": "#3905", "pull_request_summary": "s3 file store persist_file should accept all supported headers", "pull_request_description": "Member\nlucywang000 commented on Jul 26, 2019\nFix #3904.\nThe new headers mapping are read from botocore service json file:\nimport requests, json\nr = requests.get('https://raw.githubusercontent.com/boto/botocore/master/botocore/data/s3/2006-03-01/service-2.json')\ns3_service_json = r.json()\nmembers = s3_service_json['shapes']['PutObjectRequest']['members']\n\ndef to_camel_case(s):\n    return '-'.join(x.upper() if x.lower() in ('acp', 'md5') else x.capitalize() for x in s.split('-'))\n\nprint({\n    to_camel_case(v['locationName'].encode()): k.encode()  \n    for k, v in members.items()\n    if v.get('location') == 'header' and k.lower() not in ('meta', 'acl' )\n})\nTested manually with:\nimport os\nfrom twisted.internet import reactor\n\nos.environ['AWS_ACCESS_KEY_ID'] = 'xxx'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'xxx'\n\nfrom scrapy.pipelines.files import S3FilesStore\nfrom io import BytesIO\n\ns3_file_store = S3FilesStore('s3://bucket/')\ndfd = s3_file_store.persist_file('test.txt', BytesIO(b'test'), None, None, {'x-amz-storage-class': 'STANDARD_IA'})\n\nreactor.callLater(2, reactor.stop)\nreactor.run()\nprint(dfd.result)\nThe result is:\n{'ResponseMetadata': {'RequestId': '...',\n  'HostId': '...',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amz-id-2': '...',\n   'x-amz-request-id': '...',\n   'date': 'Fri, 26 Jul 2019 01:24:08 GMT',\n   'etag': '\"...\"',\n   'x-amz-storage-class': 'STANDARD_IA',\n   'content-length': '0',\n   'server': 'AmazonS3'},\n  'RetryAttempts': 0},\n 'ETag': '\"...\"'}", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-29T17:02:14Z", "files_changed": [["13", "scrapy/pipelines/files.py"]]}, "108": {"issue_url": "https://github.com/scrapy/scrapy/issues/3901", "issue_id": "#3901", "issue_summary": "I do't known how to continue?", "issue_description": "joyang1 commented on Jul 24, 2019\n2019-07-24 18:15:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:16:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:17:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:18:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:19:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:20:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:21:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:22:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:23:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:24:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\n2019-07-24 18:25:51 [scrapy.extensions.logstats] INFO: Crawled 10397 pages (at 0 pages/min), scraped 9839 items (at 0 items/min)\nWhen my project is running, there is no problem at first. After the scrapy prints the log above all the time, I don't know what happened inside. What should I do?", "issue_status": "Closed", "issue_reporting_time": "2019-07-24T10:30:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "109": {"issue_url": "https://github.com/scrapy/scrapy/issues/3900", "issue_id": "#3900", "issue_summary": "Change `FEED_URI` in spider to a value that depends on the argument passed", "issue_description": "serhii73 commented on Jul 24, 2019\nHello everyone.\nI have a question.\nI have a simple scrapy project.\nI run my project like:\nscrapy crawl mp -a city=\"Toronto, ON\"\nIn\ndef __init__(self, *args, **kwargs):\nI get argument value like:\nself.city = kwargs[\"city\"]\nIn seetings.py I can add a line like:\nFEED_FORMAT = \"csv\"\nFEED_URI = \"output/some_city.csv\"\nAnd information about need city will be saved in output/some_city.csv file.\nProblem\nBut if I will be pass different cities like arguments to my spider, like:\nscrapy crawl mp -a city=\"New York, NY\"\nthen\nscrapy crawl mp -a city=\"Toronto, ON\"\nthen\nscrapy crawl mp -a city=\"Chicago, IL\"\netc\nAll information will be saved in output/some_city.csv file.\nWhat I want\nI want to find a decision, when I pass \"Chicago, IL\" - data will be saved in Chicago_IL.csv if I pass \"Toronto, ON\" in Toronto_ON.csv\nI just need to change FEED_URI in my spider.\nBut how?\nself.settings.attributes['FEED_URI']\nI can get my FEED_URI in spider use this command, but how change this data?\nI read this link but I wrote a solution and it does not work.\nI can use -s but it not conveniently.\nDo you know how I can resolve it?\nThank you very much.", "issue_status": "Closed", "issue_reporting_time": "2019-07-24T09:04:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "110": {"issue_url": "https://github.com/scrapy/scrapy/issues/3897", "issue_id": "#3897", "issue_summary": "ItemLoaders can break if instantiated with pre-populated items", "issue_description": "Member\nelacuesta commented on Jul 23, 2019 \u2022\nedited\nBefore I start, I know item loaders have been a big source of discussion for a long time; I'm only opening this issue because the latest release breaks some of our spiders.\nIn one of our projects, our Autounit tests fail under 1.7.1 due to some item loaders which are created from partially populated items. I suspect the relevant change is #3819 (which BTW I think inadvertently closes #3046).\nPersonally I think a better approach here would be something closer to the solution proposed in #3149, although not exactly the same.\nConsider the following:\nIn [1]: import scrapy\n\nIn [2]: scrapy.__version__\nOut[2]: '1.6.0'\n\nIn [3]: from scrapy.loader import ItemLoader\n   ...: lo = ItemLoader(item={'key': 'value'})\n   ...: lo.add_value('key', 'other value')\n   ...: print(lo.load_item())\n{'key': ['other value']}\nIn [1]: import scrapy\n\nIn [2]: scrapy.__version__\nOut[2]: '1.7.1'\n\nIn [3]: from scrapy.loader import ItemLoader\n   ...: lo = ItemLoader(item={'key': 'value'})\n   ...: lo.add_value('key', 'other value')\n   ...: print(lo.load_item())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-3-6aa64a41edb1> in <module>\n      1 from scrapy.loader import ItemLoader\n      2 lo = ItemLoader(item={'key': 'value'})\n----> 3 lo.add_value('key', 'other value')\n      4 print(lo.load_item())\n\n~/venv-temporal/lib/python3.6/site-packages/scrapy/loader/__init__.py in add_value(self, field_name, value, *processors, **kw)\n     77                 self._add_value(k, v)\n     78         else:\n---> 79             self._add_value(field_name, value)\n     80\n     81     def replace_value(self, field_name, value, *processors, **kw):\n\n~/venv-temporal/lib/python3.6/site-packages/scrapy/loader/__init__.py in _add_value(self, field_name, value)\n     93         processed_value = self._process_input_value(field_name, value)\n     94         if processed_value:\n---> 95             self._values[field_name] += arg_to_iter(processed_value)\n     96\n     97     def _replace_value(self, field_name, value):\n\nTypeError: must be str, not list\nI'm not directly opening a PR because I think this needs discussion. What if we changed\nfor field_name, value in item.items():\n    self._values[field_name] = self._process_input_value(field_name, value)\nto\nfor field_name, value in item.items():\n    self._add_value(field_name, value)\nwhich calls arg_to_iter internally?\nWith that change, the following happens which is more reasonable IMHO:\nIn [3]: from scrapy.loader import ItemLoader \n   ...: lo = ItemLoader(item={'key': 'value'}) \n   ...: lo.add_value('key', 'other value') \n   ...: print(lo.load_item())                                                                                                                                                                                                                 \n{'key': ['value', 'other value']}\nLooking forward to reading your thoughts on the matter\n/cc @Gallaecio @kmike @andrewbaxter @fcanobrash @sortafreel\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T21:47:20Z", "fixed_by": "#4036", "pull_request_summary": "ItemLoader: improve handling of initial item", "pull_request_description": "Member\nelacuesta commented on Sep 25, 2019 \u2022\nedited\nThird time's the charm \ud83e\udd1e\nFixes #3897, fixes #3976, supersedes #3998.\nContinuing the work of @sortafreel, updating the implementation and adding more granular tests. Please do let me know if you think I'm missing some test case.", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-28T09:53:54Z", "files_changed": [["10", "docs/topics/loaders.rst"], ["31", "scrapy/loader/__init__.py"], ["319", "tests/test_loader.py"]]}, "111": {"issue_url": "https://github.com/scrapy/scrapy/issues/3895", "issue_id": "#3895", "issue_summary": "ImportError", "issue_description": "rorat23 commented on Jul 22, 2019\n!scrapy startproject stack\nwhenever i run this code in jupyter i get the error:\nTraceback (most recent call last):\nFile \"C:\\Users\\Rohan\\Anaconda3\\Scripts\\scrapy-script.py\", line 10, in\nsys.exit(execute())\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 149, in execute\ncmd.crawler_process = CrawlerProcess(settings)\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 254, in init\nlog_scrapy_info(self.settings)\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\scrapy\\utils\\log.py\", line 149, in log_scrapy_info\nfor name, version in scrapy_components_versions()\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\scrapy\\utils\\versions.py\", line 35, in scrapy_components_versions\n(\"pyOpenSSL\", _get_openssl_version()),\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\scrapy\\utils\\versions.py\", line 43, in get_openssl_version\nimport OpenSSL\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\OpenSSL_init.py\", line 8, in\nfrom OpenSSL import crypto, SSL\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\OpenSSL\\crypto.py\", line 16, in\nfrom OpenSSL._util import (\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\OpenSSL_util.py\", line 6, in\nfrom cryptography.hazmat.bindings.openssl.binding import Binding\nFile \"C:\\Users\\Rohan\\Anaconda3\\lib\\site-packages\\cryptography\\hazmat\\bindings\\openssl\\binding.py\", line 14, in\nfrom cryptography.hazmat.bindings._openssl import ffi, lib\nImportError: DLL load failed: The specified procedure could not be found.", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T16:51:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "112": {"issue_url": "https://github.com/scrapy/scrapy/issues/3893", "issue_id": "#3893", "issue_summary": "[Bug] 1.7.1 not support 1.6.0 script", "issue_description": "Kenith commented on Jul 22, 2019 \u2022\nedited\nHello All,\nMy spider is created by scrapy 1.6.0.\nThese days, the scrapy updated to 1.7.1, and we found that it cannot support the code build by 1.6.0.\nHere is the error:\nTraceback (most recent call last):\n  File \"/usr/bin/scrapy\", line 6, in <module>\n    from scrapy.cmdline import execute\n  File \"/usr/lib64/python2.7/site-packages/scrapy/cmdline.py\", line 10, in <module>\n    from scrapy.crawler import CrawlerProcess\n  File \"/usr/lib64/python2.7/site-packages/scrapy/crawler.py\", line 11, in <module>\n    from scrapy.core.engine import ExecutionEngine\n  File \"/usr/lib64/python2.7/site-packages/scrapy/core/engine.py\", line 14, in <module>\n    from scrapy.core.scraper import Scraper\n  File \"/usr/lib64/python2.7/site-packages/scrapy/core/scraper.py\", line 18, in <module>\n    from scrapy.core.spidermw import SpiderMiddlewareManager\n  File \"/usr/lib64/python2.7/site-packages/scrapy/core/spidermw.py\", line 13, in <module>\n    from scrapy.utils.conf import build_component_list\n  File \"/usr/lib64/python2.7/site-packages/scrapy/utils/conf.py\", line 4, in <module>\n    import configparser\nImportError: No module named configparser\nWould you please take time to check the issue?\nAppreciate for your help in advance.\nThank you.", "issue_status": "Closed", "issue_reporting_time": "2019-07-22T03:48:39Z", "fixed_by": "#3896", "pull_request_summary": "Fix ConfigParser import in py2", "pull_request_description": "Member\nelacuesta commented on Jul 22, 2019 \u2022\nedited\nFixes #3893, fixes #3889\nCleaning up my own mess \ud83d\ude05", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-23T09:59:54Z", "files_changed": [["7", "scrapy/utils/conf.py"]]}, "113": {"issue_url": "https://github.com/scrapy/scrapy/issues/3889", "issue_id": "#3889", "issue_summary": "Installing on CentOS7 (Python 2.7.5)", "issue_description": "Hiroki111 commented on Jul 20, 2019\nI've run pip install scrapy, but it doesn't seem that this has been installed properly.\nNow when I ran scrapy on CLI, it ends up in the following message.\nTraceback (most recent call last):\n  File \"/usr/bin/scrapy\", line 6, in <module>\n    from scrapy.cmdline import execute\n  File \"/usr/lib64/python2.7/site-packages/scrapy/cmdline.py\", line 10, in <module>\n    from scrapy.crawler import CrawlerProcess\n  File \"/usr/lib64/python2.7/site-packages/scrapy/crawler.py\", line 11, in <module>\n    from scrapy.core.engine import ExecutionEngine\n  File \"/usr/lib64/python2.7/site-packages/scrapy/core/engine.py\", line 14, in <module>\n    from scrapy.core.scraper import Scraper\n  File \"/usr/lib64/python2.7/site-packages/scrapy/core/scraper.py\", line 18, in <module>\n    from scrapy.core.spidermw import SpiderMiddlewareManager\n  File \"/usr/lib64/python2.7/site-packages/scrapy/core/spidermw.py\", line 13, in <module>\n    from scrapy.utils.conf import build_component_list\n  File \"/usr/lib64/python2.7/site-packages/scrapy/utils/conf.py\", line 4, in <module>\n    import configparser\nImportError: No module named configparser\nI googled configparser, and apparently this used to be called ConfigParser.\nHas anyone seen this issue?\nAny advice will be appreciated.", "issue_status": "Closed", "issue_reporting_time": "2019-07-19T18:52:00Z", "fixed_by": "#3896", "pull_request_summary": "Fix ConfigParser import in py2", "pull_request_description": "Member\nelacuesta commented on Jul 22, 2019 \u2022\nedited\nFixes #3893, fixes #3889\nCleaning up my own mess \ud83d\ude05", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-23T09:59:54Z", "files_changed": [["7", "scrapy/utils/conf.py"]]}, "114": {"issue_url": "https://github.com/scrapy/scrapy/issues/3875", "issue_id": "#3875", "issue_summary": "Problem using proxy with scrapy", "issue_description": "Nizarazo commented on Jul 13, 2019 \u2022\nedited\nHi\nI am having problem using proxy with Scrapy, I have private proxy with HTTPS supported.\nI tried with simplest spider without proxy with the following extensions:\nEXTENSIONS = {\n'scrapy.extensions.telnet.TelnetConsole': None,\n}\nSPIDER_MIDDLEWARES = {\n'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\n}\nDOWNLOADER_MIDDLEWARES = {\n'scrapy_splash.SplashCookiesMiddleware': 723,\n'scrapy_splash.SplashMiddleware': 725,\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n}\nIt worked fine 200 ok response\nThen I configured the proxy as suggested in the documenation:\nhttps://docs.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=proxy#module-scrapy.downloadermiddlewares.httpproxy\nadded this to the middle ware:\nDOWNLOADER_MIDDLEWARES = {\n...\n'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 1\n...\n}\nI added the meta key proxy per each request as follows:\nrequest = SplashRequest(url='https://www.example.com', callback=self.parse, endpoint='execute',\nargs={'wait': 1,\n'lua_source': login_script,\n'timeout':90,\n}\n)\nrequest.meta['proxy'] = proxy\nyield request\nwhere proxy = 'https://username:password@proxyip:proxyport'\nI am getting the error below when running scrapy:\n2019-07-19 16:11:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.example.com via http://127.0.0.1:8050/execute> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL rout\nines', 'ssl3_get_record', 'wrong version number')]>]\nI tried to fix by using the values of DOWNLOADER_CLIENT_TLS_METHOD such as:\n'TLSv1.0' 'TLSv1.1' 'TLSv1.2' 'SSLv3'\nbut it didn't change anything same error.\nThanks", "issue_status": "Closed", "issue_reporting_time": "2019-07-13T16:54:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "115": {"issue_url": "https://github.com/scrapy/scrapy/issues/3872", "issue_id": "#3872", "issue_summary": "Document changed CrawlerProcess.crawl(spider) functionality in Release notes", "issue_description": "Contributor\nnyov commented on Jul 12, 2019 \u2022\nedited\nPossible Regression. See explanation beneath spider.\nMWE Testcode:\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\nimport logging\nimport scrapy\n\nlogger = logging.getLogger(__name__)\n\n\nclass Spider(scrapy.Spider):\n\n    name = 'Spidy'\n\n    def start_requests(self):\n        yield scrapy.Request('https://scrapy.org/')\n\n    def parse(self, response):\n        logger.info('Here I fetched %s for you. [%s]' % (response.url, response.status))\n        return {\n            'status': response.status,\n            'url': response.url,\n            'test': 'item',\n        }\n\n\nclass LogPipeline(object):\n\n    def process_item(self, item, spider):\n        logger.warning('HIT ME PLEASE')\n        logger.info('Got hit by:\\n %r' % item)\n        return item\n\n\nif __name__ == \"__main__\":\n    from scrapy.settings import Settings\n    from scrapy.crawler import CrawlerProcess\n\n    settings = Settings(values={\n        'TELNETCONSOLE_ENABLED': False, # necessary evil :(\n        'EXTENSIONS': {\n            'scrapy.extensions.telnet.TelnetConsole': None,\n        },\n        'ITEM_PIPELINES': {\n            '__main__.LogPipeline': 800,\n        },\n    })\n\n    spider = Spider()\n\n    process = CrawlerProcess(settings=settings)\n    process.crawl(spider)\n    process.start()\nI just tried this functional (with Scrapy 1.5.1) example script on current master codebase and I got this error:\n2019-07-12 13:54:16 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n2019-07-12 13:54:16 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.4, cssselect 1.0.3, parsel 1.5.0, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.3 (default, Apr  3 2019, 05:39:12) - [GCC 8.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.6.1, Platform Linux-4.9.0-8-amd64-x86_64-with-debian-10.0\nTraceback (most recent call last):\n  File \"./test.py\", line 60, in <module>\n    process.crawl(spider)\n  File \"[...]/scrapy.git/scrapy/crawler.py\", line 180, in crawl\n    'The crawler_or_spidercls argument cannot be a spider object, '\nValueError: The crawler_or_spidercls argument cannot be a spider object, it must be a spider class (or a Crawler object)\nLooking at the codebase, blame blames this change: #3610\nBut that procedure (passing a spider instance as process.crawl(spider)) is taken pretty much verbatim from the (latest) docs, so it should continue to work, or first get deprecated?: https://docs.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script\nedit:/ to clarify, I don't mind the functionality getting removed without deprecation, if it was never documented, as it seems it wasn't.", "issue_status": "Closed", "issue_reporting_time": "2019-07-12T14:14:20Z", "fixed_by": "#3846", "pull_request_summary": "Write the 1.7 release notes", "pull_request_description": "Member\nGallaecio commented on Jun 27, 2019 \u2022\nedited\nCurrent coverage: 44eb21a\nIncludes a note about dropping Python 2 support in Scrapy 2.0.\nFixes #3872.", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-18T13:37:18Z", "files_changed": [["11", "docs/conf.py"], ["4", "docs/contributing.rst"], ["2", "docs/faq.rst"]]}, "116": {"issue_url": "https://github.com/scrapy/scrapy/issues/3868", "issue_id": "#3868", "issue_summary": "Returned Dictionary From Parse Function Not Getting Stored in the Output", "issue_description": "mntolia commented on Jul 12, 2019 \u2022\nedited\nIn my Scrapy spider class, I have several functions to parse different types of URLs. It comes to a point where if a regex search is not found it will call the parse_product() function directly with the current response as a parameter and return the data from that function else it must send a form request with the parse_product() as the callback.\nThe issue is that the data is not coming in the output in the first case.\nIn the Scrapy documentation, it says I need to return a dictionary in the callback function for it to show in the output but only the items returned from the form request made in the else statement are there in the output.\n   def parse_variation(self, response):\n        self.logger.info(\"Parsing Variation\")\n        url_search = re.findall(variation_request_url_pattern, str(response.body))\n        if not url_search:\n            self.logger.info(\"URL SEARCH IS EMPTY\")\n            data = self.parse_product(response)\n            #No Output with this statement\n            return data\n        else: \n            for url in url_search:\n               yield FormRequest(url=url, body=body callback=self.parse_product)\n\n\n    def parse_product(self, response):\n        self.logger.info(\"Parsing Product\")\n        data = {}\n        data[\"url\"] = response.url\n        data[\"name\"] = response.xpath(title_xpath).extract_first()\n        return data\nOnly the output from the Form Requests in the else statement comes. Why is it that?", "issue_status": "Closed", "issue_reporting_time": "2019-07-11T19:21:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "117": {"issue_url": "https://github.com/scrapy/scrapy/issues/3865", "issue_id": "#3865", "issue_summary": "Installation errors, pip, python 3.7.4 x64", "issue_description": "Meta-Cortex commented on Jul 10, 2019 \u2022\nedited\nI have no idea what is going on. I had no problem with python 3.7.3 32 bits version.\n    ERROR: Complete output from command 'c:\\users\\xxxxx\\appdata\\local\\programs\\python\\python37\\python.exe' -u -c 'import setuptools, tokenize;__file__='\"'\"'C:\\\\Users\\\\XXXXX\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9pfqvjxq\\\\Twisted\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\XXXXX\\AppData\\Local\\Temp\\pip-record-lls0x5g5\\install-record.txt' --single-version-externally-managed --compile:\n    ERROR: running install\n    running build\n    running build_py\n    creating build\n    creating build\\lib.win-amd64-3.7\n    creating build\\lib.win-amd64-3.7\\twisted\n    copying src\\twisted\\copyright.py -> build\\lib.win-amd64-3.7\\twisted\n    copying src\\twisted\\plugin.py -> build\\lib.win-amd64-3.7\\twisted\n    copying src\\twisted\\_version.py -> build\\lib.win-amd64-3.7\\twisted\n    copying src\\twisted\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\n    copying src\\twisted\\__main__.py -> build\\lib.win-amd64-3.7\\twisted\n    creating build\\lib.win-amd64-3.7\\twisted\\application\n    copying src\\twisted\\application\\app.py -> build\\lib.win-amd64-3.7\\twisted\\application\n    copying src\\twisted\\application\\internet.py -> build\\lib.win-amd64-3.7\\twisted\\application\n    copying src\\twisted\\application\\reactors.py -> build\\lib.win-amd64-3.7\\twisted\\application\n    copying src\\twisted\\application\\service.py -> build\\lib.win-amd64-3.7\\twisted\\application\n    copying src\\twisted\\application\\strports.py -> build\\lib.win-amd64-3.7\\twisted\\application\n    copying src\\twisted\\application\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\application\n    creating build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\avatar.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\checkers.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\endpoints.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\error.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\interfaces.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\ls.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\manhole.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\manhole_ssh.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\manhole_tap.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\mixin.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\recvline.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\stdio.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\tap.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\telnet.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\ttymodes.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\unix.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    copying src\\twisted\\conch\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\conch\n    creating build\\lib.win-amd64-3.7\\twisted\\cred\n    copying src\\twisted\\cred\\checkers.py -> build\\lib.win-amd64-3.7\\twisted\\cred\n    copying src\\twisted\\cred\\credentials.py -> build\\lib.win-amd64-3.7\\twisted\\cred\n    copying src\\twisted\\cred\\error.py -> build\\lib.win-amd64-3.7\\twisted\\cred\n    copying src\\twisted\\cred\\portal.py -> build\\lib.win-amd64-3.7\\twisted\\cred\n    copying src\\twisted\\cred\\strcred.py -> build\\lib.win-amd64-3.7\\twisted\\cred\n    copying src\\twisted\\cred\\_digest.py -> build\\lib.win-amd64-3.7\\twisted\\cred\n    copying src\\twisted\\cred\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\cred\n    creating build\\lib.win-amd64-3.7\\twisted\\enterprise\n    copying src\\twisted\\enterprise\\adbapi.py -> build\\lib.win-amd64-3.7\\twisted\\enterprise\n    copying src\\twisted\\enterprise\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\enterprise\n    creating build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\abstract.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\address.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\asyncioreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\base.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\cfreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\default.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\defer.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\endpoints.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\epollreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\error.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\fdesc.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\gireactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\glib2reactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\gtk2reactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\gtk3reactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\inotify.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\interfaces.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\kqreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\main.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\pollreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\posixbase.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\process.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\protocol.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\pyuisupport.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\reactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\selectreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\serialport.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\ssl.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\stdio.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\task.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\tcp.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\threads.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\tksupport.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\udp.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\unix.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\utils.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\win32eventreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\wxreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\wxsupport.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_baseprocess.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_dumbwin32proc.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_glibbase.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_idna.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_newtls.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_pollingfile.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_posixserialport.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_posixstdio.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_producer_helpers.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_resolver.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_signals.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_sslverify.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_threadedselect.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_win32serialport.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\_win32stdio.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    copying src\\twisted\\internet\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\internet\n    creating build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_buffer.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_file.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_filter.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_flatten.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_format.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_global.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_io.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_json.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_legacy.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_levels.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_logger.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_observer.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_stdlib.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\_util.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    copying src\\twisted\\logger\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\logger\n    creating build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\imap4.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\interfaces.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\pop3.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\pop3client.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\protocols.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\relay.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\smtp.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\_cred.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\_except.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    copying src\\twisted\\mail\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\mail\n    creating build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\authority.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\cache.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\client.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\common.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\dns.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\error.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\hosts.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\resolve.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\root.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\secondary.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\server.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\srvconnect.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\tap.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\_rfc1982.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    copying src\\twisted\\names\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\names\n    creating build\\lib.win-amd64-3.7\\twisted\\pair\n    copying src\\twisted\\pair\\ethernet.py -> build\\lib.win-amd64-3.7\\twisted\\pair\n    copying src\\twisted\\pair\\ip.py -> build\\lib.win-amd64-3.7\\twisted\\pair\n    copying src\\twisted\\pair\\raw.py -> build\\lib.win-amd64-3.7\\twisted\\pair\n    copying src\\twisted\\pair\\rawudp.py -> build\\lib.win-amd64-3.7\\twisted\\pair\n    copying src\\twisted\\pair\\testing.py -> build\\lib.win-amd64-3.7\\twisted\\pair\n    copying src\\twisted\\pair\\tuntap.py -> build\\lib.win-amd64-3.7\\twisted\\pair\n    copying src\\twisted\\pair\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\pair\n    creating build\\lib.win-amd64-3.7\\twisted\\persisted\n    copying src\\twisted\\persisted\\aot.py -> build\\lib.win-amd64-3.7\\twisted\\persisted\n    copying src\\twisted\\persisted\\crefutil.py -> build\\lib.win-amd64-3.7\\twisted\\persisted\n    copying src\\twisted\\persisted\\dirdbm.py -> build\\lib.win-amd64-3.7\\twisted\\persisted\n    copying src\\twisted\\persisted\\sob.py -> build\\lib.win-amd64-3.7\\twisted\\persisted\n    copying src\\twisted\\persisted\\styles.py -> build\\lib.win-amd64-3.7\\twisted\\persisted\n    copying src\\twisted\\persisted\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\persisted\n    creating build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\cred_anonymous.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\cred_file.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\cred_memory.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\cred_sshkeys.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\cred_unix.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_conch.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_core.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_ftp.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_inet.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_names.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_portforward.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_reactors.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_runner.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_socks.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_trial.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_web.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\twisted_words.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    copying src\\twisted\\plugins\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\plugins\n    creating build\\lib.win-amd64-3.7\\twisted\\positioning\n    copying src\\twisted\\positioning\\base.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\n    copying src\\twisted\\positioning\\ipositioning.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\n    copying src\\twisted\\positioning\\nmea.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\n    copying src\\twisted\\positioning\\_sentence.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\n    copying src\\twisted\\positioning\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\n    creating build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\amp.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\basic.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\dict.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\finger.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\ftp.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\htb.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\ident.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\loopback.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\memcache.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\pcp.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\policies.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\portforward.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\postfix.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\sip.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\socks.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\stateful.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\tls.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\wire.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    copying src\\twisted\\protocols\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\n    creating build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\compat.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\components.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\constants.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\context.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\deprecate.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\failure.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\fakepwd.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\filepath.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\formmethod.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\htmlizer.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\lockfile.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\log.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\logfile.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\modules.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\monkey.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\procutils.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\randbytes.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\rebuild.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\reflect.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\release.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\roots.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\runtime.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\sendmsg.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\shortcut.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\syslog.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\systemd.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\text.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\threadable.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\threadpool.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\url.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\urlpath.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\usage.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\util.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\versions.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\win32.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\zippath.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\zipstream.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_appdirs.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_inotify.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_oldstyle.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_release.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_setup.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_shellcomp.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_textattributes.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_tzhelper.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\_url.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    copying src\\twisted\\python\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\python\n    creating build\\lib.win-amd64-3.7\\twisted\\runner\n    copying src\\twisted\\runner\\inetd.py -> build\\lib.win-amd64-3.7\\twisted\\runner\n    copying src\\twisted\\runner\\inetdconf.py -> build\\lib.win-amd64-3.7\\twisted\\runner\n    copying src\\twisted\\runner\\inetdtap.py -> build\\lib.win-amd64-3.7\\twisted\\runner\n    copying src\\twisted\\runner\\procmon.py -> build\\lib.win-amd64-3.7\\twisted\\runner\n    copying src\\twisted\\runner\\procmontap.py -> build\\lib.win-amd64-3.7\\twisted\\runner\n    copying src\\twisted\\runner\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\runner\n    creating build\\lib.win-amd64-3.7\\twisted\\scripts\n    copying src\\twisted\\scripts\\htmlizer.py -> build\\lib.win-amd64-3.7\\twisted\\scripts\n    copying src\\twisted\\scripts\\trial.py -> build\\lib.win-amd64-3.7\\twisted\\scripts\n    copying src\\twisted\\scripts\\twistd.py -> build\\lib.win-amd64-3.7\\twisted\\scripts\n    copying src\\twisted\\scripts\\_twistd_unix.py -> build\\lib.win-amd64-3.7\\twisted\\scripts\n    copying src\\twisted\\scripts\\_twistw.py -> build\\lib.win-amd64-3.7\\twisted\\scripts\n    copying src\\twisted\\scripts\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\scripts\n    creating build\\lib.win-amd64-3.7\\twisted\\spread\n    copying src\\twisted\\spread\\banana.py -> build\\lib.win-amd64-3.7\\twisted\\spread\n    copying src\\twisted\\spread\\flavors.py -> build\\lib.win-amd64-3.7\\twisted\\spread\n    copying src\\twisted\\spread\\interfaces.py -> build\\lib.win-amd64-3.7\\twisted\\spread\n    copying src\\twisted\\spread\\jelly.py -> build\\lib.win-amd64-3.7\\twisted\\spread\n    copying src\\twisted\\spread\\pb.py -> build\\lib.win-amd64-3.7\\twisted\\spread\n    copying src\\twisted\\spread\\publish.py -> build\\lib.win-amd64-3.7\\twisted\\spread\n    copying src\\twisted\\spread\\util.py -> build\\lib.win-amd64-3.7\\twisted\\spread\n    copying src\\twisted\\spread\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\spread\n    creating build\\lib.win-amd64-3.7\\twisted\\tap\n    copying src\\twisted\\tap\\ftp.py -> build\\lib.win-amd64-3.7\\twisted\\tap\n    copying src\\twisted\\tap\\portforward.py -> build\\lib.win-amd64-3.7\\twisted\\tap\n    copying src\\twisted\\tap\\socks.py -> build\\lib.win-amd64-3.7\\twisted\\tap\n    copying src\\twisted\\tap\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\tap\n    creating build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\crash_test_dummy.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\iosim.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\mock_win32process.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\myrebuilder1.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\myrebuilder2.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\plugin_basic.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\plugin_extra1.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\plugin_extra2.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_cmdline.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_echoer.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_fds.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_getargv.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_getenv.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_linger.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_reader.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_signal.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_stdinreader.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_tester.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_tty.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\process_twisted.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\proto_helpers.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\reflect_helper_IE.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\reflect_helper_VE.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\reflect_helper_ZDE.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\ssl_helpers.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\stdio_test_consumer.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\stdio_test_halfclose.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\stdio_test_hostpeer.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\stdio_test_lastwrite.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\stdio_test_loseconn.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\stdio_test_producer.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\stdio_test_write.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\stdio_test_writeseq.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\testutils.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_abstract.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_adbapi.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_amp.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_application.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_compat.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_context.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_cooperator.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_defer.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_defgen.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_dict.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_dirdbm.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_error.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_factories.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_failure.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_fdesc.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_finger.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_formmethod.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_ftp.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_ftp_options.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_htb.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_ident.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_internet.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_iosim.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_iutils.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_lockfile.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_log.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_logfile.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_loopback.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_main.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_memcache.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_modules.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_monkey.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_nooldstyle.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_paths.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_pcp.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_persisted.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_plugin.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_policies.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_postfix.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_process.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_protocols.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_randbytes.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_rebuild.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_reflect.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_roots.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_shortcut.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_sip.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_sob.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_socks.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_ssl.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_sslverify.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_stateful.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_stdio.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_strerror.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_stringtransport.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_strports.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_task.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_tcp.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_tcp_internals.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_text.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_threadable.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_threadpool.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_threads.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_tpfile.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_twistd.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_twisted.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_udp.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_unix.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_usage.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\itrial.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\reporter.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\runner.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\unittest.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\util.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\_asyncrunner.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\_asynctest.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\_synctest.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    copying src\\twisted\\trial\\__main__.py -> build\\lib.win-amd64-3.7\\twisted\\trial\n    creating build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\client.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\demo.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\distrib.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\domhelpers.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\error.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\guard.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\html.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\http.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\http_headers.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\iweb.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\microdom.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\proxy.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\resource.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\rewrite.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\script.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\server.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\static.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\sux.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\tap.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\template.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\twcgi.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\util.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\vhost.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\wsgi.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\xmlrpc.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\_element.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\_flatten.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\_http2.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\_newclient.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\_responses.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\_stan.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    copying src\\twisted\\web\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\web\n    creating build\\lib.win-amd64-3.7\\twisted\\words\n    copying src\\twisted\\words\\ewords.py -> build\\lib.win-amd64-3.7\\twisted\\words\n    copying src\\twisted\\words\\iwords.py -> build\\lib.win-amd64-3.7\\twisted\\words\n    copying src\\twisted\\words\\service.py -> build\\lib.win-amd64-3.7\\twisted\\words\n    copying src\\twisted\\words\\tap.py -> build\\lib.win-amd64-3.7\\twisted\\words\n    copying src\\twisted\\words\\xmpproutertap.py -> build\\lib.win-amd64-3.7\\twisted\\words\n    copying src\\twisted\\words\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\words\n    creating build\\lib.win-amd64-3.7\\twisted\\_threads\n    copying src\\twisted\\_threads\\_convenience.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\n    copying src\\twisted\\_threads\\_ithreads.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\n    copying src\\twisted\\_threads\\_memory.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\n    copying src\\twisted\\_threads\\_pool.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\n    copying src\\twisted\\_threads\\_team.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\n    copying src\\twisted\\_threads\\_threadworker.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\n    copying src\\twisted\\_threads\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\n    creating build\\lib.win-amd64-3.7\\twisted\\application\\runner\n    copying src\\twisted\\application\\runner\\_exit.py -> build\\lib.win-amd64-3.7\\twisted\\application\\runner\n    copying src\\twisted\\application\\runner\\_pidfile.py -> build\\lib.win-amd64-3.7\\twisted\\application\\runner\n    copying src\\twisted\\application\\runner\\_runner.py -> build\\lib.win-amd64-3.7\\twisted\\application\\runner\n    copying src\\twisted\\application\\runner\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\application\\runner\n    creating build\\lib.win-amd64-3.7\\twisted\\application\\test\n    copying src\\twisted\\application\\test\\test_internet.py -> build\\lib.win-amd64-3.7\\twisted\\application\\test\n    copying src\\twisted\\application\\test\\test_service.py -> build\\lib.win-amd64-3.7\\twisted\\application\\test\n    copying src\\twisted\\application\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\application\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\application\\twist\n    copying src\\twisted\\application\\twist\\_options.py -> build\\lib.win-amd64-3.7\\twisted\\application\\twist\n    copying src\\twisted\\application\\twist\\_twist.py -> build\\lib.win-amd64-3.7\\twisted\\application\\twist\n    copying src\\twisted\\application\\twist\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\application\\twist\n    creating build\\lib.win-amd64-3.7\\twisted\\application\\runner\\test\n    copying src\\twisted\\application\\runner\\test\\test_exit.py -> build\\lib.win-amd64-3.7\\twisted\\application\\runner\\test\n    copying src\\twisted\\application\\runner\\test\\test_pidfile.py -> build\\lib.win-amd64-3.7\\twisted\\application\\runner\\test\n    copying src\\twisted\\application\\runner\\test\\test_runner.py -> build\\lib.win-amd64-3.7\\twisted\\application\\runner\\test\n    copying src\\twisted\\application\\runner\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\application\\runner\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\application\\twist\\test\n    copying src\\twisted\\application\\twist\\test\\test_options.py -> build\\lib.win-amd64-3.7\\twisted\\application\\twist\\test\n    copying src\\twisted\\application\\twist\\test\\test_twist.py -> build\\lib.win-amd64-3.7\\twisted\\application\\twist\\test\n    copying src\\twisted\\application\\twist\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\application\\twist\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\conch\\client\n    copying src\\twisted\\conch\\client\\agent.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\client\n    copying src\\twisted\\conch\\client\\connect.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\client\n    copying src\\twisted\\conch\\client\\default.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\client\n    copying src\\twisted\\conch\\client\\direct.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\client\n    copying src\\twisted\\conch\\client\\knownhosts.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\client\n    copying src\\twisted\\conch\\client\\options.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\client\n    copying src\\twisted\\conch\\client\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\client\n    creating build\\lib.win-amd64-3.7\\twisted\\conch\\insults\n    copying src\\twisted\\conch\\insults\\client.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\insults\n    copying src\\twisted\\conch\\insults\\colors.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\insults\n    copying src\\twisted\\conch\\insults\\helper.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\insults\n    copying src\\twisted\\conch\\insults\\insults.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\insults\n    copying src\\twisted\\conch\\insults\\text.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\insults\n    copying src\\twisted\\conch\\insults\\window.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\insults\n    copying src\\twisted\\conch\\insults\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\insults\n    creating build\\lib.win-amd64-3.7\\twisted\\conch\\openssh_compat\n    copying src\\twisted\\conch\\openssh_compat\\factory.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\openssh_compat\n    copying src\\twisted\\conch\\openssh_compat\\primes.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\openssh_compat\n    copying src\\twisted\\conch\\openssh_compat\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\openssh_compat\n    creating build\\lib.win-amd64-3.7\\twisted\\conch\\scripts\n    copying src\\twisted\\conch\\scripts\\cftp.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\scripts\n    copying src\\twisted\\conch\\scripts\\ckeygen.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\scripts\n    copying src\\twisted\\conch\\scripts\\conch.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\scripts\n    copying src\\twisted\\conch\\scripts\\tkconch.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\scripts\n    copying src\\twisted\\conch\\scripts\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\scripts\n    creating build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\address.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\agent.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\channel.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\common.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\connection.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\factory.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\filetransfer.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\forwarding.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\keys.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\service.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\session.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\sexpy.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\transport.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\userauth.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\_kex.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    copying src\\twisted\\conch\\ssh\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ssh\n    creating build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\keydata.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\loopback.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_address.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_agent.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_cftp.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_channel.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_checkers.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_ckeygen.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_conch.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_connection.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_default.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_endpoints.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_filetransfer.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_forwarding.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_helper.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_insults.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_keys.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_knownhosts.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_manhole.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_manhole_tap.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_mixin.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_openssh_compat.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_recvline.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_scripts.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_session.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_ssh.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_tap.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_telnet.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_text.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_transport.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_unix.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_userauth.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\test_window.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    copying src\\twisted\\conch\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\conch\\ui\n    copying src\\twisted\\conch\\ui\\ansi.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ui\n    copying src\\twisted\\conch\\ui\\tkvt100.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ui\n    copying src\\twisted\\conch\\ui\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\conch\\ui\n    creating build\\lib.win-amd64-3.7\\twisted\\cred\\test\n    copying src\\twisted\\cred\\test\\test_cramauth.py -> build\\lib.win-amd64-3.7\\twisted\\cred\\test\n    copying src\\twisted\\cred\\test\\test_cred.py -> build\\lib.win-amd64-3.7\\twisted\\cred\\test\n    copying src\\twisted\\cred\\test\\test_digestauth.py -> build\\lib.win-amd64-3.7\\twisted\\cred\\test\n    copying src\\twisted\\cred\\test\\test_simpleauth.py -> build\\lib.win-amd64-3.7\\twisted\\cred\\test\n    copying src\\twisted\\cred\\test\\test_strcred.py -> build\\lib.win-amd64-3.7\\twisted\\cred\\test\n    copying src\\twisted\\cred\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\cred\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\iocpreactor\\abstract.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\iocpreactor\\const.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\iocpreactor\\interfaces.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\iocpreactor\\reactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\iocpreactor\\setup.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\iocpreactor\\tcp.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\iocpreactor\\udp.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\iocpreactor\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    creating build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\connectionmixins.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\fakeendpoint.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\modulehelpers.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\process_cli.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\process_connectionlost.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\process_gireactornocompat.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\process_helper.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\reactormixins.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_abstract.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_address.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_asyncioreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_base.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_baseprocess.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_core.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_coroutines.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_default.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_endpoints.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_epollreactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_error.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_fdset.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_filedescriptor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_gireactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_glibbase.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_inlinecb.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_inotify.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_iocp.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_kqueuereactor.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_main.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_newtls.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_pollingfile.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_posixbase.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_posixprocess.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_process.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_protocol.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_resolver.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_serialport.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_sigchld.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_socket.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_stdio.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_tcp.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_threads.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_time.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_tls.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_udp.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_udp_internals.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_unix.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_win32events.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\test_win32serialport.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\_posixifaces.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\_win32ifaces.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_buffer.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_file.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_filter.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_flatten.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_format.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_global.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_io.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_json.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_legacy.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_levels.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_logger.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_observer.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_stdlib.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\test_util.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    copying src\\twisted\\logger\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\logger\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\mail\\scripts\n    copying src\\twisted\\mail\\scripts\\mailmail.py -> build\\lib.win-amd64-3.7\\twisted\\mail\\scripts\n    creating build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    copying src\\twisted\\mail\\test\\pop3testserver.py -> build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    copying src\\twisted\\mail\\test\\test_imap.py -> build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    copying src\\twisted\\mail\\test\\test_mailmail.py -> build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    copying src\\twisted\\mail\\test\\test_pop3.py -> build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    copying src\\twisted\\mail\\test\\test_pop3client.py -> build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    copying src\\twisted\\mail\\test\\test_smtp.py -> build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    copying src\\twisted\\mail\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_cache.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_client.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_common.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_dns.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_examples.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_hosts.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_names.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_resolve.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_rfc1982.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_rootresolve.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_server.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_srvconnect.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_tap.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\test_util.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    copying src\\twisted\\names\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\names\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\pair\\test\n    copying src\\twisted\\pair\\test\\test_ethernet.py -> build\\lib.win-amd64-3.7\\twisted\\pair\\test\n    copying src\\twisted\\pair\\test\\test_ip.py -> build\\lib.win-amd64-3.7\\twisted\\pair\\test\n    copying src\\twisted\\pair\\test\\test_rawudp.py -> build\\lib.win-amd64-3.7\\twisted\\pair\\test\n    copying src\\twisted\\pair\\test\\test_tuntap.py -> build\\lib.win-amd64-3.7\\twisted\\pair\\test\n    copying src\\twisted\\pair\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\pair\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\persisted\\test\n    copying src\\twisted\\persisted\\test\\test_styles.py -> build\\lib.win-amd64-3.7\\twisted\\persisted\\test\n    copying src\\twisted\\persisted\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\persisted\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\positioning\\test\n    copying src\\twisted\\positioning\\test\\receiver.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\\test\n    copying src\\twisted\\positioning\\test\\test_base.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\\test\n    copying src\\twisted\\positioning\\test\\test_nmea.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\\test\n    copying src\\twisted\\positioning\\test\\test_sentence.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\\test\n    copying src\\twisted\\positioning\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\positioning\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    copying src\\twisted\\protocols\\haproxy\\_exceptions.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    copying src\\twisted\\protocols\\haproxy\\_info.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    copying src\\twisted\\protocols\\haproxy\\_interfaces.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    copying src\\twisted\\protocols\\haproxy\\_parser.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    copying src\\twisted\\protocols\\haproxy\\_v1parser.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    copying src\\twisted\\protocols\\haproxy\\_v2parser.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    copying src\\twisted\\protocols\\haproxy\\_wrapper.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    copying src\\twisted\\protocols\\haproxy\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\n    creating build\\lib.win-amd64-3.7\\twisted\\protocols\\test\n    copying src\\twisted\\protocols\\test\\test_basic.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\test\n    copying src\\twisted\\protocols\\test\\test_tls.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\test\n    copying src\\twisted\\protocols\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\\test\n    copying src\\twisted\\protocols\\haproxy\\test\\test_parser.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\\test\n    copying src\\twisted\\protocols\\haproxy\\test\\test_v1parser.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\\test\n    copying src\\twisted\\protocols\\haproxy\\test\\test_v2parser.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\\test\n    copying src\\twisted\\protocols\\haproxy\\test\\test_wrapper.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\\test\n    copying src\\twisted\\protocols\\haproxy\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\protocols\\haproxy\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\deprecatedattributes.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\modules_helpers.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\pullpipe.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_appdirs.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_components.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_constants.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_deprecate.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_dist3.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_fakepwd.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_htmlizer.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_inotify.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_release.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_runtime.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_sendmsg.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_setup.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_shellcomp.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_syslog.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_systemd.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_textattributes.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_tzhelper.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_url.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_urlpath.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_util.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_versions.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_zippath.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\test_zipstream.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\python\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\runner\\test\n    copying src\\twisted\\runner\\test\\test_inetdconf.py -> build\\lib.win-amd64-3.7\\twisted\\runner\\test\n    copying src\\twisted\\runner\\test\\test_procmon.py -> build\\lib.win-amd64-3.7\\twisted\\runner\\test\n    copying src\\twisted\\runner\\test\\test_procmontap.py -> build\\lib.win-amd64-3.7\\twisted\\runner\\test\n    copying src\\twisted\\runner\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\runner\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\scripts\\test\n    copying src\\twisted\\scripts\\test\\test_scripts.py -> build\\lib.win-amd64-3.7\\twisted\\scripts\\test\n    copying src\\twisted\\scripts\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\scripts\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\spread\\test\n    copying src\\twisted\\spread\\test\\test_banana.py -> build\\lib.win-amd64-3.7\\twisted\\spread\\test\n    copying src\\twisted\\spread\\test\\test_jelly.py -> build\\lib.win-amd64-3.7\\twisted\\spread\\test\n    copying src\\twisted\\spread\\test\\test_pb.py -> build\\lib.win-amd64-3.7\\twisted\\spread\\test\n    copying src\\twisted\\spread\\test\\test_pbfailure.py -> build\\lib.win-amd64-3.7\\twisted\\spread\\test\n    copying src\\twisted\\spread\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\spread\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\detests.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\erroneous.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\mockcustomsuite.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\mockcustomsuite2.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\mockcustomsuite3.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\mockdoctest.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\moduleself.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\moduletest.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\novars.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\ordertests.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\packages.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\sample.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\scripttest.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\skipping.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\suppression.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_assertions.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_asyncassertions.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_deferred.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_doctest.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_keyboard.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_loader.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_log.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_output.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_plugins.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_pyunitcompat.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_reporter.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_runner.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_script.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_suppression.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_testcase.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_tests.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_util.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\test_warning.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\weird.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    copying src\\twisted\\trial\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\distreporter.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\disttrial.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\managercommands.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\options.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\worker.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\workercommands.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\workerreporter.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\workertrial.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    copying src\\twisted\\trial\\_dist\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\n    creating build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\\test\n    copying src\\twisted\\trial\\_dist\\test\\test_distreporter.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\\test\n    copying src\\twisted\\trial\\_dist\\test\\test_disttrial.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\\test\n    copying src\\twisted\\trial\\_dist\\test\\test_options.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\\test\n    copying src\\twisted\\trial\\_dist\\test\\test_worker.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\\test\n    copying src\\twisted\\trial\\_dist\\test\\test_workerreporter.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\\test\n    copying src\\twisted\\trial\\_dist\\test\\test_workertrial.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\\test\n    copying src\\twisted\\trial\\_dist\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\trial\\_dist\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\injectionhelpers.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\requesthelper.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_agent.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_cgi.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_client.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_distrib.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_domhelpers.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_error.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_flatten.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_html.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_http.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_http2.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_httpauth.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_http_headers.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_newclient.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_proxy.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_resource.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_script.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_stan.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_static.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_tap.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_template.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_util.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_vhost.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_web.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_webclient.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_web__responses.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_wsgi.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_xml.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\test_xmlrpc.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\_util.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    copying src\\twisted\\web\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\web\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\web\\_auth\n    copying src\\twisted\\web\\_auth\\basic.py -> build\\lib.win-amd64-3.7\\twisted\\web\\_auth\n    copying src\\twisted\\web\\_auth\\digest.py -> build\\lib.win-amd64-3.7\\twisted\\web\\_auth\n    copying src\\twisted\\web\\_auth\\wrapper.py -> build\\lib.win-amd64-3.7\\twisted\\web\\_auth\n    copying src\\twisted\\web\\_auth\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\web\\_auth\n    creating build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\im\\baseaccount.py -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\im\\basechat.py -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\im\\basesupport.py -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\im\\interfaces.py -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\im\\ircsupport.py -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\im\\locals.py -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\im\\pbsupport.py -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\im\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    creating build\\lib.win-amd64-3.7\\twisted\\words\\protocols\n    copying src\\twisted\\words\\protocols\\irc.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\n    copying src\\twisted\\words\\protocols\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\n    creating build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_basechat.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_basesupport.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_domish.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_irc.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_ircsupport.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_irc_service.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabberclient.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabbercomponent.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabbererror.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabberjid.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabberjstrports.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabbersasl.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabbersaslmechanisms.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabberxmlstream.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_jabberxmppstringprep.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_service.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_tap.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_xishutil.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_xmlstream.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_xmpproutertap.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\test_xpath.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    copying src\\twisted\\words\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\words\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\words\\xish\n    copying src\\twisted\\words\\xish\\domish.py -> build\\lib.win-amd64-3.7\\twisted\\words\\xish\n    copying src\\twisted\\words\\xish\\utility.py -> build\\lib.win-amd64-3.7\\twisted\\words\\xish\n    copying src\\twisted\\words\\xish\\xmlstream.py -> build\\lib.win-amd64-3.7\\twisted\\words\\xish\n    copying src\\twisted\\words\\xish\\xpath.py -> build\\lib.win-amd64-3.7\\twisted\\words\\xish\n    copying src\\twisted\\words\\xish\\xpathparser.py -> build\\lib.win-amd64-3.7\\twisted\\words\\xish\n    copying src\\twisted\\words\\xish\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\words\\xish\n    creating build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\client.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\component.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\error.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\ijabber.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\jid.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\jstrports.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\sasl.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\sasl_mechanisms.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\xmlstream.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\xmpp_stringprep.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    copying src\\twisted\\words\\protocols\\jabber\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\words\\protocols\\jabber\n    creating build\\lib.win-amd64-3.7\\twisted\\_threads\\test\n    copying src\\twisted\\_threads\\test\\test_convenience.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\\test\n    copying src\\twisted\\_threads\\test\\test_memory.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\\test\n    copying src\\twisted\\_threads\\test\\test_team.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\\test\n    copying src\\twisted\\_threads\\test\\test_threadworker.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\\test\n    copying src\\twisted\\_threads\\test\\__init__.py -> build\\lib.win-amd64-3.7\\twisted\\_threads\\test\n    running egg_info\n    writing src\\Twisted.egg-info\\PKG-INFO\n    writing dependency_links to src\\Twisted.egg-info\\dependency_links.txt\n    writing entry points to src\\Twisted.egg-info\\entry_points.txt\n    writing requirements to src\\Twisted.egg-info\\requires.txt\n    writing top-level names to src\\Twisted.egg-info\\top_level.txt\n    reading manifest file 'src\\Twisted.egg-info\\SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    warning: no previously-included files matching '*.misc' found under directory 'src\\twisted'\n    warning: no previously-included files matching '*.bugfix' found under directory 'src\\twisted'\n    warning: no previously-included files matching '*.doc' found under directory 'src\\twisted'\n    warning: no previously-included files matching '*.feature' found under directory 'src\\twisted'\n    warning: no previously-included files matching '*.removal' found under directory 'src\\twisted'\n    warning: no previously-included files matching 'NEWS' found under directory 'src\\twisted'\n    warning: no previously-included files matching 'README' found under directory 'src\\twisted'\n    warning: no previously-included files matching 'newsfragments' found under directory 'src\\twisted'\n    warning: no previously-included files found matching 'src\\twisted\\topfiles\\CREDITS'\n    warning: no previously-included files found matching 'src\\twisted\\topfiles\\ChangeLog.Old'\n    warning: no previously-included files found matching 'pyproject.toml'\n    warning: no previously-included files found matching 'codecov.yml'\n    warning: no previously-included files found matching 'appveyor.yml'\n    warning: no previously-included files found matching '.circleci'\n    warning: no previously-included files matching '*' found under directory '.circleci'\n    no previously-included directories found matching 'bin'\n    no previously-included directories found matching 'admin'\n    no previously-included directories found matching '.travis'\n    no previously-included directories found matching '.github'\n    warning: no previously-included files found matching 'docs\\historic\\2003'\n    warning: no previously-included files matching '*' found under directory 'docs\\historic\\2003'\n    writing manifest file 'src\\Twisted.egg-info\\SOURCES.txt'\n    copying src\\twisted\\python\\twisted-completion.zsh -> build\\lib.win-amd64-3.7\\twisted\\python\n    creating build\\lib.win-amd64-3.7\\twisted\\python\\_pydoctortemplates\n    copying src\\twisted\\python\\_pydoctortemplates\\common.html -> build\\lib.win-amd64-3.7\\twisted\\python\\_pydoctortemplates\n    copying src\\twisted\\python\\_pydoctortemplates\\index.html -> build\\lib.win-amd64-3.7\\twisted\\python\\_pydoctortemplates\n    copying src\\twisted\\python\\_pydoctortemplates\\summary.html -> build\\lib.win-amd64-3.7\\twisted\\python\\_pydoctortemplates\n    copying src\\twisted\\test\\cert.pem.no_trailing_newline -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\key.pem.no_trailing_newline -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\server.pem -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\test\\test_defer.py.3only -> build\\lib.win-amd64-3.7\\twisted\\test\n    copying src\\twisted\\internet\\iocpreactor\\notes.txt -> build\\lib.win-amd64-3.7\\twisted\\internet\\iocpreactor\n    copying src\\twisted\\internet\\test\\_awaittests.py.3only -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    copying src\\twisted\\internet\\test\\_yieldfromtests.py.3only -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\n    creating build\\lib.win-amd64-3.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\chain.pem -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\not-a-certificate -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing1.pem -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing2-duplicate.pem -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing2.pem -> build\\lib.win-amd64-3.7\\twisted\\internet\\test\\fake_CAs\n    copying src\\twisted\\mail\\test\\rfc822.message -> build\\lib.win-amd64-3.7\\twisted\\mail\\test\n    copying src\\twisted\\python\\test\\_deprecatetests.py.3only -> build\\lib.win-amd64-3.7\\twisted\\python\\test\n    copying src\\twisted\\words\\im\\instancemessenger.glade -> build\\lib.win-amd64-3.7\\twisted\\words\\im\n    copying src\\twisted\\words\\xish\\xpathparser.g -> build\\lib.win-amd64-3.7\\twisted\\words\\xish\n    running build_ext\n    building 'twisted.test.raiser' extension\n    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\n    ----------------------------------------\nERROR: Command \"'c:\\users\\xxxxx\\appdata\\local\\programs\\python\\python37\\python.exe' -u -c 'import setuptools, tokenize;__file__='\"'\"'C:\\\\Users\\\\XXXXX\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9pfqvjxq\\\\Twisted\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\XXXXX\\AppData\\Local\\Temp\\pip-record-lls0x5g5\\install-record.txt' --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\XXXXX\\AppData\\Local\\Temp\\pip-install-9pfqvjxq\\Twisted\\", "issue_status": "Closed", "issue_reporting_time": "2019-07-10T14:23:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "118": {"issue_url": "https://github.com/scrapy/scrapy/issues/3864", "issue_id": "#3864", "issue_summary": "scrapy could not download pdf files completely.", "issue_description": "lycanthropes commented on Jul 10, 2019\nI want to download pdf files from the pdf web links crawled by scrapy spiders, but when I use scrapy.Request (pdf_url) in filespipeline to crawl the pdf web, eventually it downloaded incomplete pdf files. all the pdf files was 1 KB Other than the first few files( which are complete). I had to use requests.get(pdf_url, stream=True) to download all the pdf files completely, but it is too slow. I want to know if scrapy filespipelines has similar method like this ?", "issue_status": "Closed", "issue_reporting_time": "2019-07-10T13:06:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "119": {"issue_url": "https://github.com/scrapy/scrapy/issues/3863", "issue_id": "#3863", "issue_summary": "ImagePipeline ignoring alternative S3 urls", "issue_description": "reinoldus commented on Jul 10, 2019\nIn the FilesPipline the following AWS settings are pulled:\n@classmethod\ndef from_settings(cls, settings):\n    s3store = cls.STORE_SCHEMES['s3']\n    s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n    s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n    s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n    s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n    s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n    s3store.AWS_VERIFY = settings['AWS_VERIFY']\n    s3store.POLICY = settings['FILES_STORE_S3_ACL']\n\n    gcs_store = cls.STORE_SCHEMES['gs']\n    gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n    gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n\n    store_uri = settings['FILES_STORE']\n    return cls(store_uri, settings=settings)\nBut the ImagePipeline only pulls these:\n@classmethod\ndef from_settings(cls, settings):\n    s3store = cls.STORE_SCHEMES['s3']\n    s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n    s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n    s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n\n    gcs_store = cls.STORE_SCHEMES['gs']\n    gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n    gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None\n\n    store_uri = settings['IMAGES_STORE']\n    return cls(store_uri, settings=settings)\nThat makes it impossible to use none-aws S3 hosting services. Is there any reason for that?\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2019-07-10T06:36:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "120": {"issue_url": "https://github.com/scrapy/scrapy/issues/3856", "issue_id": "#3856", "issue_summary": "Running Scrapy Spider from Script, Using Output in Script Issue", "issue_description": "Goldrush12 commented on Jul 6, 2019\nI want to run multiple spiders from a script and retrieve the output of the spiders to process it inside the script. To accomplish this, I wrote the following code based on a Stackoverflow thread.\nThe issue I'm facing is that the function spider_results() only returns a list of the last item over and over again instead of a list with all the found items. When I run the same spider manually with the scrapy crawl command, I get the desired output. The output of the script, the manual json output and the spider itself are below.\nfrom scrapy import signals\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\nfrom circus.spiders.circus import MySpider\n\nfrom scrapy.signalmanager import dispatcher\n\n\ndef spider_results():\n    results = []\n\n    def crawler_results(signal, sender, item, response, spider):\n        results.append(item)\n        \n\n    dispatcher.connect(crawler_results, signal=signals.item_passed)\n\n    process = CrawlerProcess(get_project_settings())\n    process.crawl(MySpider)\n    process.start()  # the script will block here until the crawling is finished\n    return results\n\n\nif __name__ == '__main__':\n    print(spider_results())\nScript output:\n{'away_odds': 1.44,\n 'away_team': 'Los Angeles Dodgers',\n 'event_time': datetime.datetime(2019, 6, 8, 2, 15),\n 'home_odds': 2.85,\n 'home_team': 'San Francisco Giants',\n 'last_update': datetime.datetime(2019, 6, 6, 20, 58, 41, 655497),\n 'league': 'MLB'}, {'away_odds': 1.44,\n 'away_team': 'Los Angeles Dodgers',\n 'event_time': datetime.datetime(2019, 6, 8, 2, 15),\n 'home_odds': 2.85,\n 'home_team': 'San Francisco Giants',\n 'last_update': datetime.datetime(2019, 6, 6, 20, 58, 41, 655497),\n 'league': 'MLB'}, {'away_odds': 1.44,\n 'away_team': 'Los Angeles Dodgers',\n 'event_time': datetime.datetime(2019, 6, 8, 2, 15),\n 'home_odds': 2.85,\n 'home_team': 'San Francisco Giants',\n 'last_update': datetime.datetime(2019, 6, 6, 20, 58, 41, 655497),\n 'league': 'MLB'}]\nJson output with scrapy crawl:\n[{\"home_team\": \"Los Angeles Angels\", \"away_team\": \"Seattle Mariners\", \"event_time\": \"2019-06-08 02:07:00\", \"home_odds\": 1.58, \"away_odds\": 2.4, \"last_update\": \"2019-06-06 20:48:16\", \"league\": \"MLB\"},\n{\"home_team\": \"San Diego Padres\", \"away_team\": \"Washington Nationals\", \"event_time\": \"2019-06-08 02:10:00\", \"home_odds\": 1.87, \"away_odds\": 1.97, \"last_update\": \"2019-06-06 20:48:16\", \"league\": \"MLB\"},\n{\"home_team\": \"San Francisco Giants\", \"away_team\": \"Los Angeles Dodgers\", \"event_time\": \"2019-06-08 02:15:00\", \"home_odds\": 2.85, \"away_odds\": 1.44, \"last_update\": \"2019-06-06 20:48:16\", \"league\": \"MLB\"}]\nMy Spider:\nfrom scrapy.spiders import Spider\nfrom ..items import MatchItem\nimport json\nimport datetime\nimport dateutil.parser\n\nclass MySpider(Spider):\n    name = 'first_spider'\n\n    start_urls = [\"https://websiteXYZ.com\"]\n\n    def parse(self, response):\n        item = MatchItem()\n\n        timestamp = datetime.datetime.utcnow()\n        \n        response_json = json.loads(response.body)\n\n        for event in response_json[\"el\"]:\n            for team in event[\"epl\"]:\n                if team[\"so\"] == 1: item[\"home_team\"] = team[\"pn\"]\n                if team[\"so\"] == 2: item[\"away_team\"] = team[\"pn\"]\n\n            for market in event[\"ml\"]:\n                if market[\"mn\"] == \"Match result\":\n                    item[\"event_time\"] = dateutil.parser.parse(market[\"dd\"]).replace(tzinfo=None)\n                    for outcome in market[\"msl\"]:\n                        if outcome[\"mst\"] == \"1\": item[\"home_odds\"] = outcome[\"msp\"]\n                        if outcome[\"mst\"] == \"X\": item[\"draw_odds\"] = outcome[\"msp\"]\n                        if outcome[\"mst\"] == \"2\": item[\"away_odds\"] = outcome[\"msp\"]\n\n                if market[\"mn\"] == 'Moneyline':\n                    item[\"event_time\"] = dateutil.parser.parse(market[\"dd\"]).replace(tzinfo=None)\n                    for outcome in market[\"msl\"]:\n                        if outcome[\"mst\"] == \"1\": item[\"home_odds\"] = outcome[\"msp\"]\n                        #if outcome[\"mst\"] == \"X\": item[\"draw_odds\"] = outcome[\"msp\"]\n                        if outcome[\"mst\"] == \"2\": item[\"away_odds\"] = outcome[\"msp\"]\n                    \n                            \n            item[\"last_update\"] = timestamp\n            item[\"league\"] = event[\"scn\"]\n            \n            yield item\nBased on a reply I got on Stackoverflow, I created the following two scrips:\ncontroller.py\nfrom klein import route, run\nfrom scrapy import signals\nfrom scrapy.crawler import CrawlerRunner\nfrom my_project.spiders.my_spider import My_Spider\nimport json\n\n\nclass MyCrawlerRunner(CrawlerRunner):\n    \"\"\"\n    Crawler object that collects items and returns output after finishing crawl.\n    \"\"\"\n    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n        # keep all items scraped\n        self.items = []\n\n        # create crawler (Same as in base CrawlerProcess)\n        crawler = self.create_crawler(crawler_or_spidercls)\n\n        # handle each item scraped\n        crawler.signals.connect(self.item_scraped, signals.item_scraped)\n\n        # create Twisted.Deferred launching crawl\n        dfd = self._crawl(crawler, *args, **kwargs)\n\n        # add callback - when crawl is done cal return_items\n        dfd.addCallback(self.return_items)\n        return dfd\n\n    def item_scraped(self, item, response, spider):\n        self.items.append(item)\n\n    def return_items(self, result):\n        return self.items\n\ndef return_spider_output(output):\n    return json.dumps([dict(item) for item in output])\n\ndef spider_controller():\n    runner = MyCrawlerRunner()\n    spider = My_Spider()\n    deferred = runner.crawl(spider)\n    deferred.addCallback(return_spider_output)\n    return deferred\nmain_script.py\nfrom controller import spider_controller\n\nprint(spider_controller())\nBut this just prints:\n<Deferred at 0x7f9dda5232e8>\nWhat's wrong with my code? I have been trying to find a solution for this problem for weeks now. Reddit and Stackoverflow weren't helpful.", "issue_status": "Closed", "issue_reporting_time": "2019-07-05T21:21:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "121": {"issue_url": "https://github.com/scrapy/scrapy/issues/3848", "issue_id": "#3848", "issue_summary": "nothing important", "issue_description": "Rama-Alwattar commented on Jul 1, 2019 \u2022\nedited\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2019-07-01T00:59:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "122": {"issue_url": "https://github.com/scrapy/scrapy/issues/3847", "issue_id": "#3847", "issue_summary": "can't get sites html", "issue_description": "anthonysanchezmk commented on Jun 29, 2019\nWhen I check the response.body from certain sites, like https://spong.com/games-db/browse I only get byte data, as well as the response.text. I've tried response.body.decode('utf-8') and gave me the error 'utf-8' codec can't decode byte 0xe4 in position 4: invalid continuation byte.", "issue_status": "Closed", "issue_reporting_time": "2019-06-28T18:41:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "123": {"issue_url": "https://github.com/scrapy/scrapy/issues/3844", "issue_id": "#3844", "issue_summary": "Consider making METAREFRESH_IGNORE_TAGS an empty list by default", "issue_description": "Member\nGallaecio commented on Jun 26, 2019\nAs a way to allow users to fix #1422, #3768 introduced the METAREFRESH_IGNORE_TAGS setting.\nTo keep backward compatibility, the setting was introduced with ['script', 'noscript'] as default value. However, to reproduce the behavior of web browsers, it seems the right value would be [].\nShould we switch the default value of the METAREFRESH_IGNORE_TAGS setting to [], even though the change breaks backward compatibility?", "issue_status": "Closed", "issue_reporting_time": "2019-06-26T12:53:28Z", "fixed_by": "#4311", "pull_request_summary": "Make METAREFRESH_IGNORE_TAGS an empty list by default", "pull_request_description": "Member\nGallaecio commented 12 hours ago\nFixes #3844", "pull_request_status": "Merged", "issue_fixed_time": "2020-02-06T18:40:46Z", "files_changed": [["2", "docs/topics/downloader-middleware.rst"], ["2", "scrapy/settings/default_settings.py"], ["16", "tests/test_downloadermiddleware_redirect.py"]]}, "124": {"issue_url": "https://github.com/scrapy/scrapy/issues/3843", "issue_id": "#3843", "issue_summary": "Unable to fetch information on Urls that reached the max retries", "issue_description": "jsvachon2 commented on Jun 26, 2019 \u2022\nedited\nI'm getting a lot of 503 from a site I am scrapping with Scrapy and I can't get it to record a list of all the failures. I can always parse the logs at the very end but I'd like to simply requeue the failed URL when needed.\nI've enabled the Retry module using RETRY_ENABLED = True and set RETRY_TIMES = 2 for testing purposes.\nI've added the from_crawler class method and registered three callbacks...\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(MySpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(spider.item_error, signal=signals.item_error)\n        crawler.signals.connect(spider.item_dropped, signal=signals.item_dropped)\n\n        return spider\nThe first callback is working fine but the other two are not working at all. They are defined as:\ndef item_error(self, item, response, spider, failure):\n        self.log(\"************************ ERROR ********\", logging.ERROR)\n        self.log(item, logging.ERROR)\n\ndef item_dropped(self, item, response, exception, spider):\n        self.log(\"*********************** DROPPED *********\", logging.ERROR)\n        self.log(item, logging.ERROR)\nAm I doing something wrong? My spider reads the Urls to process from a queue so upon failure, I'd like to simply requeue them. To make it faster to test and to prevent hitting the remote server too hard, I am using a fake server that returns 503 for everything...\nwhile true; do echo -e \"HTTP/1.1 503 FAILED\\n\\n $(date)\" | nc -l -p 1500 -q 1; done\nThanks", "issue_status": "Closed", "issue_reporting_time": "2019-06-26T01:14:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "125": {"issue_url": "https://github.com/scrapy/scrapy/issues/3841", "issue_id": "#3841", "issue_summary": "Only first part of paragraph being returned, up until child tag is encountered?", "issue_description": "gingerbeardman commented on Jun 25, 2019 \u2022\nedited\nI am encountering the following type of markup:\n<p>Noting the presence of a footnote<sup>1</sup> is one common way for superscripts to be used.</p>\nTest page:\nhttps://html.com/tags/sup/\nTest query:\nresponse.css('div.render p::text')[0].extract()\nTest results:\nNoting the presence of a footnote\nTest expected:\nNoting the presence of a footnote is one common way for superscripts to be used.\nQuestion:\nHow can I get the full text of the paragraph, ignoring the child tag?", "issue_status": "Closed", "issue_reporting_time": "2019-06-25T15:19:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "126": {"issue_url": "https://github.com/scrapy/scrapy/issues/3839", "issue_id": "#3839", "issue_summary": "Understanding scrapy engine", "issue_description": "amirduran commented on Jun 24, 2019 \u2022\nedited\nThis is question to understand deeper scrapy engine. On my server I can run multiple scrapy processes in parallel (we can start it by using scrapy API or just starting two scrapy instances in command line). That sounds good, but let's refer to the following scrapy architecture photo:\nAs far as I can understand, every scrapy process (in my case 2 processes) will have its own engine, scheduler, downloader etc. There is option to instruct/configure scrapy process to perform multiple crawls in parallel. This means if I instruct every process to run 50 requests in parallel my server will do 100 requests in parallel (because I have 2 processes running). What is the best way to instruct scrapy to run parallel crawls? Should I define all URLs in start URLs and then scrapy's scheduler will schedule crawling automatically for me or there is another way?\nAlso, if I have a mixture of 2 domains in my start urls attribute: 20 URLS belonging domain A and 50 URLs belonging domain B and if I configure scrapy that maximum 10 request in parallel should be performed on domain A and 5 on domain B, is scrapy's scheduler going to recognise this? Is there also way to say, if there are X different domains available in start URLs please always send max 10 requests in parallel for each of X domains?\nThx for answering my questions.", "issue_status": "Closed", "issue_reporting_time": "2019-06-24T15:26:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "127": {"issue_url": "https://github.com/scrapy/scrapy/issues/3838", "issue_id": "#3838", "issue_summary": "DeprecationWarning in tests", "issue_description": "Contributor\nStasDeep commented on Jun 24, 2019\nI'm using unittest to create tests for my spiders. Internally, I use CrawlerRunner to invoke callbacks.\nBut every time I launch tests, I get several deprecation warnings.\n/usr/local/lib/python3.6/site-packages/scrapy/utils/conf.py:97: DeprecationWarning: The SafeConfigParser class has been renamed to ConfigParser in Python 3.2. This alias will be removed in future versions. Use ConfigParser directly instead.\n  cfg = SafeConfigParser()\n/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/webclient.py:4: DeprecationWarning: twisted.web.client.HTTPClientFactory was deprecated in Twisted 16.7.0: please use https://pypi.org/project/treq/ or twisted.web.client.Agent instead\n  from twisted.web.client import HTTPClientFactory\n/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/contextfactory.py:51: DeprecationWarning: Passing method to twisted.internet.ssl.CertificateOptions was deprecated in Twisted 17.1.0. Please use a combination of insecurelyLowerMinimumTo, raiseMinimumTo, and lowerMaximumSecurityTo instead, as Twisted will correctly configure the method.\n  acceptableCiphers=DEFAULT_CIPHERS)\nHowever, I don't see any of these warning when using any of the scrapy commands.", "issue_status": "Closed", "issue_reporting_time": "2019-06-23T19:03:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "128": {"issue_url": "https://github.com/scrapy/scrapy/issues/3837", "issue_id": "#3837", "issue_summary": "python 3.7.3 ModuleNotFoundError: No module named 'scrapy.dupefilter'", "issue_description": "lx1054331851 commented on Jun 22, 2019\nUnder python 3.7.3 run scrapy, error occurred as below:\n2019-06-22 23:35:14 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: SpeedToMarket)\n2019-06-22 23:35:14 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.1, Python 3.7.3 (default, Mar 27 2019, 16:54:48) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.6.1, Platform Darwin-18.6.0-x86_64-i386-64bit\n2019-06-22 23:35:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'SpeedToMarket', 'CONCURRENT_REQUESTS_PER_DOMAIN': 16, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'DUPEFILTER_CLASS': 'scrapy.dupefilter.BaseDupeFilter', 'NEWSPIDER_MODULE': 'SpeedToMarket.spiders', 'SPIDER_MODULES': ['SpeedToMarket.spiders']}\n2019-06-22 23:35:14 [scrapy.extensions.telnet] INFO: Telnet Password: 861f841c3fee4b29\n2019-06-22 23:35:14 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats']\n2019-06-22 23:35:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'SpeedToMarket.middlewares.MyUserAgentMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'SpeedToMarket.middlewares.SpeedtomarketCheckRepeatMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2019-06-22 23:35:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-06-22 23:35:14 [scrapy.middleware] INFO: Enabled item pipelines:\n['SpeedToMarket.pipelines.SpeedtomarketPipeline']\n2019-06-22 23:35:14 [scrapy.core.engine] INFO: Spider opened\nUnhandled error in Deferred:\n2019-06-22 23:35:14 [twisted] CRITICAL: Unhandled error in Deferred:\n\nTraceback (most recent call last):\n  File \"/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 172, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 176, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 1613, in unwindGenerator\n    return _cancellableInlineCallbacks(gen)\n  File \"/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 1529, in _cancellableInlineCallbacks\n    _inlineCallbacks(None, g, status)\n--- <exception caught here> ---\n  File \"/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\n    result = g.send(result)\n  File \"/anaconda3/lib/python3.7/site-packages/scrapy/crawler.py\", line 82, in crawl\n    yield self.engine.open_spider(self.spider, start_requests)\nbuiltins.ModuleNotFoundError: No module named 'scrapy.dupefilter'", "issue_status": "Closed", "issue_reporting_time": "2019-06-22T15:41:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "129": {"issue_url": "https://github.com/scrapy/scrapy/issues/3836", "issue_id": "#3836", "issue_summary": "Increase verbosity for ItemLoader, Compose and MapCompose errors", "issue_description": "Contributor\nmabelvj commented on Jun 20, 2019\nItemLoader already provides error raising for the case of the output processor:\n    def get_output_value(self, field_name):\n        proc = self.get_output_processor(field_name)\n        proc = wrap_loader_context(proc, self.context)\n        try:\n            return proc(self._values[field_name])\n        except Exception as e:\n            raise ValueError(\"Error with output processor: field=%r value=%r error='%s: %s'\" % \\\n                (field_name, self._values[field_name], type(e)._name__, str(e)))\nIt could be helpful to extend this behaviour could to other ItemLoader methods:\n_process_input_value for input processors\n    def _process_input_value(self, field_name, value):\n        proc = self.get_input_processor(field_name)\n        proc = wrap_loader_context(proc, self.context)\n        return proc(value)\nget_value for processors that are passed as an argument to add_css, add_xpath or add_value\n    def get_value(self, value, *processors, **kw):\n        regex = kw.get('re', None)\n        if regex:\n            value = arg_to_iter(value)\n            value = flatten(extract_regex(regex, x) for x in value)\n\n        for proc in processors:\n            if value is None:\n                break\n            proc = wrap_loader_context(proc, self.context)\n            value = proc(value)\n        return value\nAlso, Compose and MapCompose could raise errors occurring while processing the values.\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2019-06-20T11:01:56Z", "fixed_by": "#3840", "pull_request_summary": "[MRG+1] Itemloader errors #3836", "pull_request_description": "Contributor\nmabelvj commented on Jun 25, 2019 \u2022\nedited by Gallaecio\nAdded error messages to ItemLoader (for input processor, and processor as argument) and also for MapCompose and Compose.\nFor MapCompose and Compose, I'm not sure though if the format is correct, since these errors will be displayed when processing the input/out processors and they will be nested to these ones, example:\nValueError: Error with processor MapCompose value=['change,deep-thoughts,thinking,world'] \nerror='ValueError: Error in MapCompose with function _strip \nvalue=['change,deep-thoughts,thinking,world'] error='TypeError: must be str, not int''\nI would appreciate some suggestions here.\nFixes #3836", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-05T11:55:44Z", "files_changed": [["17", "scrapy/loader/__init__.py"], ["15", "scrapy/loader/processors.py"], ["47", "tests/test_loader.py"]]}, "130": {"issue_url": "https://github.com/scrapy/scrapy/issues/3834", "issue_id": "#3834", "issue_summary": "Provide a way to pass meta information to an Item", "issue_description": "chaplin89 commented on Jun 18, 2019\nHi,\nI'm trying to use the ImagesPipeline but I'm not happy with its naming convention.\nI've extended ImagesPipeline in order to provide an alternate file_path method, but now I'm stucked.\nThe request originate from an Item object and I want to save images using a meta-information that is available in the method that build this Item.\nI was not able to find a way to propagate this information to the request that arrives in the file_path method. Is there a way to do this?\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-06-17T19:10:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "131": {"issue_url": "https://github.com/scrapy/scrapy/issues/3832", "issue_id": "#3832", "issue_summary": "remove_namespaces method is very slow", "issue_description": "floggle commented on Jun 16, 2019 \u2022\nedited\nThe remove_namespaces method of the response selector is really really slow.\nI'd suggest this should either be documented or optimised.\nExamples:\nI was post-processing a bunch of XML files - using this function they took about 2-3 days. Without this function they took less than 20 minutes!\nOne of the files I was processing turned out to be a 2.7million line XML file(!). This was running for multiple hrs before I killed it (12?). Without going through that function it was done in about 30 seconds!\nI'm still using selectors and xpaths, but now using the *[local-name()= function instead.", "issue_status": "Closed", "issue_reporting_time": "2019-06-16T13:49:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "132": {"issue_url": "https://github.com/scrapy/scrapy/issues/3831", "issue_id": "#3831", "issue_summary": "how to pass scraped value to pipiline init", "issue_description": "hadesfv commented on Jun 16, 2019 \u2022\nedited\npipepine.py code\n`import csv,datetime\nclass AaexampleSpider(object):\n def __init__(self,file_name):\n  now = datetime.datetime.now()\n  self.current_date = now.strftime(\"%d%b\")\n  self.file_name = file_name #file_name\n  self.infile = open(\"{}_{}.csv\".format(self.current_date,self.file_name),\"w\")\n  self.dict_writer = csv.DictWriter(self.infile,fieldnames=[\"Hello\"])\n  self.dict_writer.writeheader()\n def process_item(self, item, spider):\n  self.dict_writer.writerow(item)\ndef close_spider(self, spider):\n self.infile.close()`\nHow can I pass file_name from the parse to the init of the pipeline, Here is the spider code\nimport scrapy\n\nclass ExampleSpider(scrapy.Spider):\n name = 'example'\n allowed_domains = ['example.com']\n start_urls = ['http://example.com/']\n def parse(self, response):\n  file_name = response.xpath('//div/h1/text()').extract_first()\n  #code", "issue_status": "Closed", "issue_reporting_time": "2019-06-16T12:38:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "133": {"issue_url": "https://github.com/scrapy/scrapy/issues/3830", "issue_id": "#3830", "issue_summary": "Scrapy with peer 2 peer search engine", "issue_description": "ROBERT-MCDOWELL commented on Jun 15, 2019\nDoes Scrapy data can be used with a p2p search engine? if yes any good open source p2p search engine? thanks", "issue_status": "Closed", "issue_reporting_time": "2019-06-15T11:49:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "134": {"issue_url": "https://github.com/scrapy/scrapy/issues/3828", "issue_id": "#3828", "issue_summary": "Scrapy does not allow using active mode in FTPFeedStorage", "issue_description": "Contributor\nheylouiz commented on Jun 15, 2019\nIt is not possible to export feeds to a FTP server configured in active mode with FTPFeedStorage.\nScrapy use ftplib and it uses passive mode by default, there is no way to configure this in Scrapy, there is a setting to enable active mode (or disable passive mode) in the downloader, so I think it would be a good idea to have this option in the feed exporter too.", "issue_status": "Closed", "issue_reporting_time": "2019-06-14T18:51:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "135": {"issue_url": "https://github.com/scrapy/scrapy/issues/3826", "issue_id": "#3826", "issue_summary": "Request url missed tail from #", "issue_description": "ludongfang commented on Jun 11, 2019\nI have url like this \"https://www.domin.com/productlist#?pl&t=1\", when I send the request, \"#?pl&t=1\" missed. How should I add some settings to ignore the url checking or how should I send the exactly url with \"https://www.domin.com/productlist#?pl&t=1\".", "issue_status": "Closed", "issue_reporting_time": "2019-06-11T13:21:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "136": {"issue_url": "https://github.com/scrapy/scrapy/issues/3821", "issue_id": "#3821", "issue_summary": "Issue with Twisted and Python 3.4", "issue_description": "ghost commented on Jun 9, 2019\nTwisted had a patch 3 days ago and it's causing test suite to fail for py34 environment.\nTwisted , according to their Readme, support Python 3.5+. This needs to be fixed if the builds need to pass\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2019-06-09T05:12:20Z", "fixed_by": "#3825", "pull_request_summary": "Require Twisted<=19.2.0 for Python 3.4", "pull_request_description": "Member\nGallaecio commented on Jun 11, 2019 \u2022\nedited\nTwisted 19.2.0 is the final release with Python 3.4 support.\nThese changes are based on the setuptools documentation and PEP 508.\nFixes #3821", "pull_request_status": "Merged", "issue_fixed_time": "2019-06-12T20:40:26Z", "files_changed": [["3", "setup.py"]]}, "137": {"issue_url": "https://github.com/scrapy/scrapy/issues/3815", "issue_id": "#3815", "issue_summary": "dont_merge_cookies or dont_send_cookies?", "issue_description": "RyQcan commented on Jun 6, 2019 \u2022\nedited\ndont_merge_cookies has some bugs: they say that\nWhen some site returns cookies (in a response) those are stored in the cookies for that domain and will be sent again in future requests. That\u2019s the typical behaviour of any regular web browser. However, if, for some reason, you want to avoid merging with existing cookies you can instruct Scrapy to do so by setting the dont_merge_cookies key to True in the Request.meta.\nBut this flag not only prevents merging of cookies, but also prevents sending of them:\nscrapy/scrapy/downloadermiddlewares/cookies.py\nwhy? it may be a bug,I just want to do not merge cookies,but not to do not send cookies", "issue_status": "Closed", "issue_reporting_time": "2019-06-06T15:15:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "138": {"issue_url": "https://github.com/scrapy/scrapy/issues/3814", "issue_id": "#3814", "issue_summary": "Scraping images from urls in a nested item", "issue_description": "qbolgi commented on Jun 6, 2019 \u2022\nedited\nHi,\nI havea Scrapy Spider set up. I have urls that I scrape and then I store multiple results by url. I have nested arrays in each item.\nI'd like to use images urls contained in these items to download images but I can't do that.\n`def init(self, *args, **kwargs):\ndata_file = pkgutil.get_data(\n\"auctions_results\", \"json/input/demo_db_urls_glenmarch.json\")\nself.data = json.loads(data_file)\ndef start_requests(self):\n    for item in self.data:\n        request = scrapy.Request(item['gm_url'], callback=self.parse)\n        request.meta['item'] = item\n        yield request\n\ndef parse(self, response):\n    item = response.meta['item']\n    item['results'] = []\n\n    for caritem in response.css(\"div.car-item-border\"):\n        data = AuctionItem()\n\n        data[\"marque\"] = caritem.css(\"div.make::text\").extract_first().strip().split(\" \", 2)[1]\n        data[\"model\"] = caritem.css(\"div.make::text\").extract_first().strip().split(\" \", 2)[2]\n        data[\"model_year\"] = caritem.css(\"div.make::text\").extract_first().strip().split(\" \", 1)[0]\n        data[\"price_str\"] = caritem.css(\"div.price::text\").extract_first().strip().replace(\",\", \" \")\n        \n        if caritem.css(\"div.price::text\").extract_first().find(\"Estimate\"):\n            data[\"price_int\"] = re.sub(\"\\D\", \"\", data[\"price_str\"])\n            data[\"price_int\"] = int(data[\"price_int\"])\n            data[\"price_currency\"] = re.sub(\n                \"[0-9]\", \"\", data[\"price_str\"]).replace(\" \", \"\")\n            data[\"sold\"] = True\n        else:\n            data[\"price_int\"] = None\n            data[\"price_currency\"] = None\n            data[\"sold\"] = False\n            \n        data[\"auction_house\"] = caritem.css(\"div.auctionHouse::text\").extract_first().split(\"-\", 1)[0].strip()\n        data[\"auction_country\"] = caritem.css(\"div.auctionHouse::text\").extract_first().rsplit(\",\", 1)[1].strip()\n        data[\"auction_date\"] = caritem.css(\"div.date::text\").extract_first().replace(\",\", \"\").strip()\n        \n        if \" - \" in data[\"auction_date\"]:\n            auctiondate = re.sub(r\".*-\", \"\", data[\"auction_date\"]).strip()\n            data[\"auction_datetime\"] = datetime.strptime(auctiondate, '%d %B %Y').date()\n        else:\n            data[\"auction_datetime\"] = datetime.strptime(data[\"auction_date\"], '%d %B %Y').date()\n\n        auctionurl = caritem.css(\"div.view-auction a::attr(href)\").extract_first()\n        if auctionurl != None and \"/auction-cars/show-backup-image\" not in auctionurl:\n            data[\"auction_url\"] = caritem.css(\"div.view-auction a::attr(href)\").extract_first()\n        else :\n            data[\"auction_url\"] = None\n\n        data[\"image_urls\"] = caritem.css(\"div.view-auction a img::attr(src)\").extract_first()\n\n        item['results'].append(data)\n        \n    yield item`\nMy output in JSON look like this:\n{ \"objectID\": 10202, \"gm_url\": \"myurl\", \"results\": [{ \"marque\": \"Alfa\", \"model\": \"Romeo Giulia Sprint GT Veloce 1600\", \"model_year\": \"1966\", \"price_str\": \"\u20ac49 280\", \"price_int\": 49280, \"price_currency\": \"\u20ac\", \"sold\": true, \"auction_house\": \"RM Sotheby's\", \"auction_country\": \"Italy\", \"auction_date\": \"25 - 27 November 2016\", \"auction_datetime\": \"2016-11-27\", \"auction_url\": null, \"image_urls\": \"imagesurl\" }, { \"marque\": \"Alfa\", \"model\": \"Romeo Giulia Sprint GT Veloce Coupe\", \"model_year\": \"1966\", \"price_str\": \"\u20ac46 000\", \"price_int\": 46000, \"price_currency\": \"\u20ac\", \"sold\": true, \"auction_house\": \"Bonhams\", \"auction_country\": \"France\", \"auction_date\": \"6 February 2014\", \"auction_datetime\": \"2014-02-06\", \"auction_url\": \"https://www.bonhams.com//auctions/21768/lot/434/?category=list&length=100000&page=1\", \"image_urls\": \"imagesurl\" }] }\nHow can I download images from \"images_urls\" ?", "issue_status": "Closed", "issue_reporting_time": "2019-06-06T07:41:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "139": {"issue_url": "https://github.com/scrapy/scrapy/issues/3811", "issue_id": "#3811", "issue_summary": "CrawlerProcess.crawl doesn't add my spider", "issue_description": "tombka commented on Jun 4, 2019 \u2022\nedited\nHello everybody,\nI have a problem with running my spider through a worker who is listening for new job(RQ) :\nclass Scraper:\n    def __init__(self):\n        self.process = CrawlerProcess(get_project_settings())\n        self.spider = mySpider\n\n    def run_spiders(self, url):\n        print(self.process.crawlers) # <= output : set()\n        self.process.crawl(self.spider[0], url=url)\n        print(self.process.crawlers) # <= output : {<scrapy.crawler.Crawler object at 0x7f2d9419e6a0>}\n        self.process.start()  # the script will block here until the crawling is finished\nWhile i run my spider with python3 crawl_site.py everything seems to be ok as you can see above.\nBUT, when i try to perform a crawl through my worker my self.process.crawlers remains the same : an empty set()\nIt doesn't make any sense for me...\nFeel free to ask for more informations if needed.\nEDIT :\nIt's weird the self.process.start() seems to be absolutely useless : nothing if performed while i executed it within my worker, but as always it's working perfectly while being run in command line", "issue_status": "Closed", "issue_reporting_time": "2019-06-04T15:24:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "140": {"issue_url": "https://github.com/scrapy/scrapy/issues/3809", "issue_id": "#3809", "issue_summary": "Fix test_command_parse.py on Windows", "issue_description": "Member\nGallaecio commented on Jun 4, 2019\nAll the tests from that test file are failing on the Windows CI platform.\nWe need to look into it, and fix the underlying issue or have those tests skipped on Windows if they are not meant to work on that platform.", "issue_status": "Closed", "issue_reporting_time": "2019-06-04T12:40:42Z", "fixed_by": "#3827", "pull_request_summary": "Set the cloned directory as PYTHONPATH in appveyor.yml", "pull_request_description": "Member\nGallaecio commented on Jun 11, 2019 \u2022\nedited\nFixes #3809\nI\u2019m unsure how it broke, I suspect it was a change in the AppVeyor side.", "pull_request_status": "Merged", "issue_fixed_time": "2019-06-12T20:40:09Z", "files_changed": [["3", "appveyor.yml"]]}, "141": {"issue_url": "https://github.com/scrapy/scrapy/issues/3807", "issue_id": "#3807", "issue_summary": "Tutorial: scrapy shell example should say \"text\" not \"title\"", "issue_description": "kiwisquash commented on Jun 4, 2019\nThe text currently says:\nNow, let\u2019s extract title, author and the tags from that quote using the quote object we just created:\n>>> title = quote.css(\"span.text::text\").get()\n>>>title\n'\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\u201d'\nThetitle should be changed to text. In fact, if you scroll down slightly down in the for-loop, text is used instead of title.\nhttps://docs.scrapy.org/en/1.6/intro/tutorial.html#extracting-quotes-and-authors", "issue_status": "Closed", "issue_reporting_time": "2019-06-03T21:28:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "142": {"issue_url": "https://github.com/scrapy/scrapy/issues/3804", "issue_id": "#3804", "issue_summary": "Items missing", "issue_description": "azharf commented on Jun 2, 2019\nHi,\nwhen working with the itemLoader() to populate items from a created list in python. Here I have attached a working of the suspected bug.\nThis appears to be present when the loader.get_output_value method has been executed. Once that executes then, the populated list initially created would have one of the items missing. This appears to be a malfunction bug within the software presented. If we only have loader.load_item, the values are presented, however loder.get_output_value() fails to display the value/s in the data set as tested.\nloader.get_output_value causes the items value to be missing from School:\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2019-06-02T02:01:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "143": {"issue_url": "https://github.com/scrapy/scrapy/issues/3802", "issue_id": "#3802", "issue_summary": "Scrapy Proxy Issue", "issue_description": "rameshrpi commented on May 29, 2019\nI have installed scrapy1.6 with python2.7 and I have created the sample spider from scrapy documentation.\nwhen I run the spider from pycharm I am getting the below error.\n2019-05-29 17:02:10 [scrapy.core.engine] INFO: Spider opened\n2019-05-29 17:02:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-05-29 17:02:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n2019-05-29 17:02:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/page/1/> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_record', 'wrong version number')]>]\n2019-05-29 17:02:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/page/1/> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_record', 'wrong version number')]>]\n2019-05-29 17:02:11 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/page/1/> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_record', 'wrong version number')]>]\nerrback_httpbin method calleed...!\n2019-05-29 17:02:11 [httpbin] ERROR: <twisted.python.failure.Failure twisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_record', 'wrong version number')]>]>\nI have setup the proxy authentication in pycharm.\nAlso I have tried from command prompt with shell command.\nscrapy shell \"http://quotes.toscrape.com/page/1/\"\nAfter that when I view the response in browser It shows \"Access denied\" error message.\nI have set the proxy in command prompt as well (set HTTP_PROXY=http://username:password:)\nKindly let me know how to resolve this issue?", "issue_status": "Closed", "issue_reporting_time": "2019-05-29T11:47:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "144": {"issue_url": "https://github.com/scrapy/scrapy/issues/3801", "issue_id": "#3801", "issue_summary": "I want to run multiple spiders in multiple threads, and spiders have two different parameters, 'LOG_FILE' and one that I set myself", "issue_description": "luoqishuai commented on May 28, 2019 \u2022\nedited\nI used to have a spider, but the key parameters in setting are different. At that time, I copied and pasted 30 copies one by one, and then used docker to start the crawl one by one.\nNow I want to use a python program to run 30 spiders in a row, using 30 threads, each spider with a different Settings, and everything else is the same.\nAt first, I tried to build a thread pool, but python told me builtins.valueerror: signal only works in main thread.\nThen I start the way of using https://doc.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process, because each spiders will require different Settings, so there is no success\nHow can you implement this kind of train of thought please?\nDid I have to copy the code one by one and run the shell commands to boot it up one by one?\nIt's stupid, but that's all I can think of.\nI would appreciate it if you could help me. If my description is not very clear, please let me know.\ndef task(i):\n# settings=get_project_settings()\n# settings.update({'IDQUEUE_MOD':i,'LOG_FILE' : \"scrapy_spider_{}.log\".format(i)})\n# runner = CrawlerRunner(settings)\n# runner.crawl(SpiderSpider)\n# reactor.run(installSignalHandlers=False)\ncmdline.execute('scrapy crawl spider -s IDQUEUE_MOD={} -s LOG_FILE=scrapy_spider_{}.log'.format(i,i).split())\nreturn 1\nsettings = get_project_settings()\nqueuesize = settings.getint('IDQUEUE_SIZE') # \u961f\u5217\u5927\u5c0f \u4ee3\u8868\u8d77\u51e0\u4e2a\u722c\u866b\npool=ThreadPoolExecutor(queuesize)\nname_list=['name_{}'.format(i) for i in range(queuesize)]\nfor i in range(queuesize):\nname_list[i]=pool.submit(task,i)\nb=[i.result() for i in name_list]\nprint(b)", "issue_status": "Closed", "issue_reporting_time": "2019-05-28T14:33:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "145": {"issue_url": "https://github.com/scrapy/scrapy/issues/3800", "issue_id": "#3800", "issue_summary": "Scrapy response returns repeated items despite change in page settings at the URL link after the spider runs a while", "issue_description": "pc2000sg commented on May 28, 2019\nTrying to scrape data from 'https://redmart.lazada.sg/meat-and-seafood/?m=redmart&page=102&spm=a2o42.redmart_channel.nav_category_tree.30.2e5d48a6aXcB49', 'https://redmart.lazada.sg/shop-Groceries-DairyChilled/?. After a few pages items received back start repeating. Looks like there are some protection from the web to stop people gettings all items data. Tried the followings but it only help to get more items but not all. Not sure what more can be done to get around this issue. Appreciate any inputs or suggestions. Thanks.\n1.) Set the followings 'CONCURRENT_REQUESTS': 1, 'CONCURRENT_REQUESTS_PER_DOMAIN':1, 'RANDOMIZE_DOWNLOAD_DELAY':True,\n'CONCURRENT_REQUESTS_PER_IP': 1,\n2.) Include headers in request for example: yield scrapy.Request(url=next_page, headers={'JSESSIONID':'2DE61BF1E734471FBB8C768B21D47D85'}\n3.) Set page from 102 to 1 instead of 1 to 102\nMy code is like this:\ndef parse_category(self, response):\nprint(\"@@@ Parsing: %s \" % (response.url))\nif len(response.body) == 0:\nprint(\"@@@ Response empty, retry parsing: %s\" % (response.url))\nyield scrapy.Request(url=response.url,callback=self.parse_category, meta={'cat_link': response.meta['cat_link'], 'cat_name': response.meta['cat_name'],'page': response.meta['page']}, dont_filter=True)\nelse:\n#print(\"debug url %s\" % response.url)\ndata=response.xpath(\"//script[contains(.,'mod')]/text()\").extract_first()\nsdata=re.sub('window.pageData=','',data)\njson_response=json.loads(sdata)\n#checksuccess=json_response['msg'] json_response is a dict with len 2 //json_response.keys\n#> a['mods'].keys()\nif ('mods' in json_response.keys()):\nprint(\"@@@ %s: It's got %d items\" % (response.url,len(json_response['mods']['listItems'])))\nfor product in range(len(json_response['mods']['listItems'])):\nyield self.parse_item(response, json_response['mods']['listItems'][product], json_response['mainInfo']['title'])\nnext_page=response.xpath(\"//link[@rel='prev']//@href\").extract_first()\npage=int(response.meta['page'])\nif (next_page is not None):\npre_page='page='+str(page)\n#print(\"debug 1 pre-page %s\" % pre_page)\nnext_page='page='+str(page-1)\n#print(\"debug 1 next-page %s\" % next_page)\nnext_page=re.sub(pre_page,next_page,response.url)\n#print(\"debug next page %s\" % next_page)\n#yield scrapy.Request(url=next_page, callback=self.parse_category, meta={'cat_link': response.meta['cat_link'], 'cat_name': response.meta['cat_name'],'page': next_page}, dont_filter=True)\nyield scrapy.Request(url=next_page, headers={'JSESSIONID':'2DE61BF1E734471FBB8C768B21D47D85'},callback=self.parse_category, meta={'page': response.meta['page']-1})\ndef parse_item(self, response, json_product, cat_name):\nitem_loader = ProductLoader(DiffmartsItem(), None)\nitem_loader.add_value('id', str(json_product['itemId']))\n#print(\"debug url %s\" % response.url)\n#print(\"debug ID %s\" % json_product['name'])\nitem_loader.add_value('cat_name', cat_name)\nitem_loader.add_value('name', json_product['name'], Compose(to_name))\nif 'originalPrice' in json_product.keys():\nitem_loader.add_value('price', re.sub('$','',json_product['priceShow']), Compose(to_price))\nitem_loader.add_value('prev_price', json_product['originalPrice'], Compose(to_price))\nitem_loader.add_value('promotion', json_product['discount'])\nelse:\nitem_loader.add_value('price',json_product['price'], Compose(to_price))\nitem_loader.add_value('prev_price', '0')\nitem_loader.add_value('link', json_product['productUrl'], Compose(lambda v:to_link(v, response)))\nitem_loader.add_value('image_link', json_product['image'], Compose(lambda v:to_link(v, response)))\nchecksoldout=json_product['inStock']\nif checksoldout=='Yes':\nitem_loader.add_value('sold_out', 1)\nelse:\nitem_loader.add_value('sold_out', 0)\nreturn item_loader.load_item()", "issue_status": "Closed", "issue_reporting_time": "2019-05-28T01:48:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "146": {"issue_url": "https://github.com/scrapy/scrapy/issues/3795", "issue_id": "#3795", "issue_summary": "Fallback parser rules in ItemLoader - discussion for spider maintenance", "issue_description": "Contributor\nBurnzZ commented on May 26, 2019\nRelated to issue #3771\nI'm stoked with the idea of having the ItemLoader support fallback parsers in any API possible as Scrapy needs to provide convenient ways for developers to keep up with the site changes. However, some sites perform layout changes more often than others, and some of the fallback parser rules gets obsolete real fast, posing a problem in the spiders' long term maintenance.\nWith this, the main challenge would be determining if a given fallback css/xpath rule in the parser is safe to remove (meaning that it hasn't been encountered anywhere during a crawl). We could confirm this via looking at the distribution of how many times a fallback xpath/css rule was used for the full spider job.\nI'd like to discuss the idea of:\nhow should this information be better presented?\nwhere might we put this info on, via the logs? via stats?\nshould this feature be put into the ItemLoader class itself? or should it be subclassed for better backward compatibility (as this might pose to have an effect on performance)?\nand lastly, should this feature be even worthy of being implemented in Scrapy itself? or should it be implemented on a separate repo as a Scrapy plugin?\nCheers!\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2019-05-26T06:06:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "147": {"issue_url": "https://github.com/scrapy/scrapy/issues/3788", "issue_id": "#3788", "issue_summary": "How to download files (pdf) from a server what listen only PHP request?", "issue_description": "anibal-aguila commented on May 23, 2019 \u2022\nedited\nHi everyone! I'm new here and want to know if scrapy can get multiples PDF from a site what resolve every request with PHP, the URL where I can get each file is:\nhttp://extranet.domain.com/?p=downloads&did=1\nThe files are referenced whit numbers, in this case, the URL get the file with the id 1, I have 3800 files for downloads...\nHope someone knows how to solve my issue, thanks in advance.\nAnd for the login request I use something like this..\nfrom scrapy.http import FormRequest\nrequest=FormRequest(url='http://extranet.domain.com/index.php',formdata={'username': 'theUserName','password':'thePassword',})", "issue_status": "Closed", "issue_reporting_time": "2019-05-22T18:53:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "148": {"issue_url": "https://github.com/scrapy/scrapy/issues/3787", "issue_id": "#3787", "issue_summary": "Issues need opened cleen up", "issue_description": "insspb commented on May 22, 2019\nI just wanted to check what issues and their direction are really actual now, but I found tons of issues not closed.\nThey are already answered with \"Please, ask your question through StackOverflow.\" from @Gallaecio or other contributors, but still not closed. This is a real-time loss if somebody just browsing to search interesting topic for self-checking/debugging/fixing. Maybe such persons just do not have additional issues moderation rights?\nI expect the same thing in PRs.\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2019-05-22T14:55:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "149": {"issue_url": "https://github.com/scrapy/scrapy/issues/3785", "issue_id": "#3785", "issue_summary": "Visual C++ Build Tool needed", "issue_description": "vRivier commented on May 21, 2019\nGot this error while pip installing scrapy on windows:\nerror: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visualstudio.com/visual-cpp-build-tools\nI had to install the software and then it worked. The installation took about 20min so I would recommand to try another way.", "issue_status": "Closed", "issue_reporting_time": "2019-05-21T17:01:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "150": {"issue_url": "https://github.com/scrapy/scrapy/issues/3784", "issue_id": "#3784", "issue_summary": "scrapy didn't work on aws lambda", "issue_description": "xuleijian commented on May 20, 2019\nI use thest codes below. But it didn't work with no error.\ndef lambda_handler(event, context):\nconfigure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\nrunner = CrawlerRunner()\nd = runner.crawl(LoginSpider)\nd.addBoth(lambda _: reactor.stop())\nreactor.run()\nlambda log:\n02:55:25 2019-05-20 02:55:25 [scrapy.core.engine] INFO: Spider opened\n02:55:28 +END RequestId: b3c28ec2-e8e4-4383-a61d-7a321e4ee528\nWho knows the reason? Please help me.Thank you !", "issue_status": "Closed", "issue_reporting_time": "2019-05-20T05:23:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "151": {"issue_url": "https://github.com/scrapy/scrapy/issues/3783", "issue_id": "#3783", "issue_summary": "When running scrapy crawl xxxspider, it is ABCspider.", "issue_description": "kjxy commented on May 20, 2019 \u2022\nedited\nWhen running scrapy crawl xxxspider, it starts ABCspider and throws an exception. I fixed the exception (caused by fake_useragent, it will be fine after uninstalling it)\nNow there is a new \"error\". When running xxx spider, the printout is the custom_settings information of ABC spider.\nFrom the picture, the sohuAppSpider is running, but the custom_settings information is the custom_settings of weixinxxxspider.\nPython 3.6 windows 7 scrapy 1.5.1", "issue_status": "Closed", "issue_reporting_time": "2019-05-20T02:43:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "152": {"issue_url": "https://github.com/scrapy/scrapy/issues/3782", "issue_id": "#3782", "issue_summary": "scrapy.Item hashes equality", "issue_description": "ronek22 commented on May 17, 2019\nHi,\nI want to use scrapy.Items in my scraper, but I ran into some trouble.\nI created two items with the same data inside each of them.\nIs it possible to override __hash__ and __eq__ method for Link class below?\nCode for reproduce issue:\nimport scrapy\n\nclass Link(scrapy.Item):\n    link = scrapy.Field()\n    domain = scrapy.Field()\n\nall_links = set()\n\nhref = \"https://google.com\"\nlink1 = Link(link=href, domain=href)\nlink2 = Link(link=href, domain=href)\nprint(link1 == link2) # print True\n\nall_links.add(link1)\nall_links.add(link2)\nprint(len(all_links)) # should be 1, but 2 was printed out", "issue_status": "Closed", "issue_reporting_time": "2019-05-17T14:29:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "153": {"issue_url": "https://github.com/scrapy/scrapy/issues/3781", "issue_id": "#3781", "issue_summary": "Use PEP8", "issue_description": "Contributor\nanubhavp28 commented on May 16, 2019\nScrapy docs recommends using PEP8 coding convention when writing code for inclusion in Scrapy, with the exception of 79 character line length rule . A quick run of pep8 utility shows me that there are several occurrence of PEP8 violation in the master branch. If given the nod from the maintainers, I would like to work on resolving this.", "issue_status": "Closed", "issue_reporting_time": "2019-05-16T06:52:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "154": {"issue_url": "https://github.com/scrapy/scrapy/issues/3780", "issue_id": "#3780", "issue_summary": "How can i change proxy every 3 seconds", "issue_description": "w1101662433 commented on May 15, 2019\nIm dizzy, How can i change proxy every 3 seconds", "issue_status": "Closed", "issue_reporting_time": "2019-05-15T09:02:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "155": {"issue_url": "https://github.com/scrapy/scrapy/issues/3779", "issue_id": "#3779", "issue_summary": "multithread-scrapy error: one thread error, but the other threads cannot stop?", "issue_description": "qingmei-migu commented on May 15, 2019 \u2022\nedited\nbefore 12:54:43, one thread meets an exception, and the others threads run as usual, this is ok.\nBut the errors after 12:57:28 is due to the scrapy still running, and canot create a new instance.\nI wait for one day, and the scrapy just don't stop. There must have some wrongs.\nLacking essential process after one thread happens an excepetion?\nBut why? and maybe this bug need to be fixed.", "issue_status": "Closed", "issue_reporting_time": "2019-05-15T07:57:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "156": {"issue_url": "https://github.com/scrapy/scrapy/issues/3777", "issue_id": "#3777", "issue_summary": "Whitelist form methods in FormRequest.from_response", "issue_description": "Contributor\ncsalazar commented on May 14, 2019\nHi team, according to this article, there are 3 methods that are accepted in form's method attribute. I can't remember about other methods, but I'd agree to consider the rest of REST verbs if that's common. Anyway, I think it should be a good idea to whitelist the accepted methods to avoid scenarios like this vulnerability exploitation that took advantage of form's method.\nThis issue affects FormRequest.from_response and the affected line is:\nscrapy/scrapy/http/request/form.py\nLine 51 in a3d3804\n method = kwargs.pop('method', form.method) \nI want to know if there some reason behind this behavior, otherwise I could send a pull request.", "issue_status": "Closed", "issue_reporting_time": "2019-05-14T08:28:01Z", "fixed_by": "#3794", "pull_request_summary": "[MRG+1] Fix form methods in FormRequest.from_response (#3777)", "pull_request_description": "Contributor\ncsalazar commented on May 25, 2019 \u2022\nedited\nFixes #3777", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-02T15:08:15Z", "files_changed": [["7", "scrapy/http/request/form.py"], ["14", "tests/test_http_request.py"]]}, "157": {"issue_url": "https://github.com/scrapy/scrapy/issues/3776", "issue_id": "#3776", "issue_summary": "twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup:", "issue_description": "littleningmeng commented on May 14, 2019\nRefer to the tile.\nWhy this error occured ? It's ok on my windows 10 with the same code, but on Linux, this error occured!", "issue_status": "Closed", "issue_reporting_time": "2019-05-14T07:23:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "158": {"issue_url": "https://github.com/scrapy/scrapy/issues/3759", "issue_id": "#3759", "issue_summary": "I hope the project support a function", "issue_description": "perfect-network commented on Apr 26, 2019\nLike this:\ncss = {\n    'name'  :   \"dd>a::text\",\n    'href'  :   \"dd>a::attr(href)\"\n}\ndata = response.css(css).extract()\n#then return\nprint(data)\n#print\ndata = [\n    {\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    },{\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    },{\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    },{\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    },{\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    },{\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    },{\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    },{\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    },{\n        \"name\"  :   \"\u6597\u7834\u82cd\u7a79\",\n        \"href\"  :   \"https://google.com\"\n    }\n]\nI hope it can be supported ...thanks you read", "issue_status": "Closed", "issue_reporting_time": "2019-04-26T11:26:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "159": {"issue_url": "https://github.com/scrapy/scrapy/issues/3758", "issue_id": "#3758", "issue_summary": "Email throwing issue", "issue_description": "Tanya312 commented on Apr 25, 2019 \u2022\nedited\nWas trying to send email through scrapy.\nSettings I used are:-\n#settings for mail\nMAIL_FROM = '...@gmail.com'\nMAIL_HOST = 'smtp.gmail.com'\nMAIL_PORT = 587\nMAIL_USER = '...@gmail.com'\nMAIL_PASS = 'xxxxxxx'\nMAIL_TLS = True\nMAIL_SSL = False\nEmail was sent successfully. But the following exception is thrown;\n2019-04-25 18:10:15 [twisted] CRITICAL: Unhandled Error\nTraceback (most recent call last):\nFile \"/lib/python3.6/site-packages/twisted/python/log.py\", line 103, in callWithLogger\nreturn callWithContext({\"system\": lp}, func, *args, **kw)\nFile \"/python3.6/site-packages/twisted/python/log.py\", line 86, in callWithContext\nreturn context.call({ILogContext: newCtx}, func, *args, **kw)\nFile \"/lib/python3.6/site-packages/twisted/python/context.py\", line 122, in callWithContext\nreturn self.currentContext().callWithContext(ctx, func, *args, **kw)\nFile \"/lib/python3.6/site-packages/twisted/python/context.py\", line 85, in callWithContext\nreturn func(*args,**kw)\n--- ---\nFile \"/lib/python3.6/site-packages/twisted/internet/posixbase.py\", line 614, in _doReadOrWrite\nwhy = selectable.doRead()\nFile \"/lib/python3.6/site-packages/twisted/internet/tcp.py\", line 243, in doRead\nreturn self._dataReceived(data)\nFile \"/lib/python3.6/site-packages/twisted/internet/tcp.py\", line 249, in _dataReceived\nrval = self.protocol.dataReceived(data)\nFile \"/lib/python3.6/site-packages/twisted/protocols/tls.py\", line 330, in dataReceived\nself._flushReceiveBIO()\nFile \"/lib/python3.6/site-packages/twisted/protocols/tls.py\", line 300, in _flushReceiveBIO\nself._flushSendBIO()\nFile \"/lib/python3.6/site-packages/twisted/protocols/tls.py\", line 252, in _flushSendBIO\nbytes = self._tlsConnection.bio_read(2 ** 15)\nbuiltins.AttributeError: 'NoneType' object has no attribute 'bio_read'", "issue_status": "Closed", "issue_reporting_time": "2019-04-25T13:02:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "160": {"issue_url": "https://github.com/scrapy/scrapy/issues/3757", "issue_id": "#3757", "issue_summary": "Cannot find the spider", "issue_description": "MarcSteven commented on Apr 25, 2019\nI used the framework to do something ,but when I executed the code ,it tell me the error:Cannot find the spider,I use venv\nThe error as the link cannot find the spider\nHope can find a complete solution", "issue_status": "Closed", "issue_reporting_time": "2019-04-25T09:56:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "161": {"issue_url": "https://github.com/scrapy/scrapy/issues/3756", "issue_id": "#3756", "issue_summary": "What is the correct way to stop or hang the crawler?", "issue_description": "rodrigogonegit commented on Apr 24, 2019\nHey, I'm wondering what would be the correct way of suspending the crawler in a scenario like this:\nCrawl a table page per page\nStop once an item on the page has already been processed\nLet's say I have 5 pages defined in the start_urls, these pages urls look something like:\ntarget.com/list-0\ntarget.com/list-50\ntarget.com/list-100\ntarget.com/list-150\ntarget.com/list-200\nThe items on the pages change over time, the number means \"0 to 49 most recent\", \"50 to 99 most recent\", and so on.\nAssuming the last processed item is on page 100, should I check for duplicates in the item Pipeline and stop the crawler there? To avoid the older page's items from being processed and not wasting the target's resources.", "issue_status": "Closed", "issue_reporting_time": "2019-04-23T21:50:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "162": {"issue_url": "https://github.com/scrapy/scrapy/issues/3752", "issue_id": "#3752", "issue_summary": "pip install scrapy", "issue_description": "gbell27 commented on Apr 21, 2019\nYou need first \"pip install incremental\" due to a misspelling in the lines for the installation of dependencies.", "issue_status": "Closed", "issue_reporting_time": "2019-04-20T22:25:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "163": {"issue_url": "https://github.com/scrapy/scrapy/issues/3749", "issue_id": "#3749", "issue_summary": "Add a new issue and pr template", "issue_description": "abhinavsagar commented on Apr 17, 2019\nCurrently the project is not having an issue and pull request template.\nAdding one helps in having standardized information when submitting a pr or an issue.", "issue_status": "Closed", "issue_reporting_time": "2019-04-17T14:59:04Z", "fixed_by": "#3471", "pull_request_summary": "[MRG+1] Add Bug report and Feature request templates", "pull_request_description": "Member\nelacuesta commented on Oct 22, 2018 \u2022\nedited\n(Almost) Shamelessly copied from the Atom repository \ud83d\ude07\nLately I've been seeing a great deal of support questions at the Github issue tracker, things that should go to other channels like the StackOverflow \"scrapy\" tag. Hopefully this change could help with that.\nUpdate: fixes #3749", "pull_request_status": "Merged", "issue_fixed_time": "2019-08-26T07:35:45Z", "files_changed": [["41", ".github/ISSUE_TEMPLATE/bug_report.md"], ["33", ".github/ISSUE_TEMPLATE/feature_request.md"]]}, "164": {"issue_url": "https://github.com/scrapy/scrapy/issues/3747", "issue_id": "#3747", "issue_summary": "invalid hostname", "issue_description": "qq703048949 commented on Apr 17, 2019\nwhen I request http://tjw_161130192022480.company.qihuiwang.com/\nreturn this: invalid hostname: tjw_161130192022480.company.qihuiwang.com,\nI don't know how to solve this question !\nhlep me !", "issue_status": "Closed", "issue_reporting_time": "2019-04-17T03:03:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "165": {"issue_url": "https://github.com/scrapy/scrapy/issues/3746", "issue_id": "#3746", "issue_summary": "Scrapy killed for some unkonwn reasons", "issue_description": "Firststep2014 commented on Apr 14, 2019\nHello,I've got stuck in dealling with the issue that scrapy killed sometime ,and I've used such codes so as to track any contional result,however it seems all right .\npid = os.getpid() p = psutil.Process(pid) print('Process name : ', p.name()) print('cpu_times : ' , p.cpu_times()) print('Memory usage : ', p.memory_percent()) print('Process number of threads : s', p.num_threads())\nand the results returned are listed below\ncpu_times : pcputimes(user=1.72, system=0.19, children_user=0.0, children_system=0.01) Memory usage : 0.0397387235557129 Process number of threads : s 2 Process name : scrapy cpu_times : pcputimes(user=1.79, system=0.19, children_user=0.0, children_system=0.01) Memory usage : 0.04000367534652104 Process number of threads : s 2 Process name : scrapy cpu_times : pcputimes(user=20.48, system=0.6, children_user=0.0, children_system=0.01) Memory usage : 0.072099247623883 Process number of threads : s 2 Process name : scrapy cpu_times : pcputimes(user=20.53, system=0.61, children_user=0.0, children_system=0.01) Memory usage : 0.07213969827896823 Process number of threads : s 2 Process name : scrapy cpu_times : pcputimes(user=20.54, system=0.61, children_user=0.0, children_system=0.01) Memory usage : 0.07214172081172249 Process number of threads : s 2 Process name : scrapy cpu_times : pcputimes(user=20.56, system=0.62, children_user=0.0, children_system=0.01) Memory usage : 0.07225902771146961 Process number of threads : s 2 Process name : scrapy cpu_times : pcputimes(user=20.57, system=0.62, children_user=0.0, children_system=0.01) Memory usage : 0.07278893129308589 Process number of threads : s 2 Process name : scrapy cpu_times : pcputimes(user=20.69, system=0.62, children_user=0.0, children_system=0.01) Memory usage : 0.07390334684068349 Process number of threads : s 2 Process name : scrapy cpu_times : pcputimes(user=20.74, system=0.62, children_user=0.0, children_system=0.01) Memory usage : 0.07499955959349276 Process number of threads : s 2 Killed u25654@s001-n011:~/test_dir/quotesbot$\nThis seems nothing with memory leak, could you please give a hand?", "issue_status": "Closed", "issue_reporting_time": "2019-04-14T15:59:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "166": {"issue_url": "https://github.com/scrapy/scrapy/issues/3744", "issue_id": "#3744", "issue_summary": "Configure LOG_LEVEL on `scrapy` logger, not on handler", "issue_description": "dmugtasimov commented on Apr 12, 2019\nPeople may use scrapy with Django and have other handlers that have DEBUG level to log from other loggers. When you hardcode DEBUG level on scrapy logger it is not possible to reduce scrapy verbosity. The only workaround is to run django.setup() after scrapy has configured its logging.", "issue_status": "Closed", "issue_reporting_time": "2019-04-12T16:22:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "167": {"issue_url": "https://github.com/scrapy/scrapy/issues/3743", "issue_id": "#3743", "issue_summary": "AttributeError: 'TelnetConsole' object has no attribute 'port'", "issue_description": "Adhders commented on Apr 11, 2019\nsystem w10 ,scrapy 1.6.0\nmy scrapy can run nomally long before, but raise AttributeError recently , I have reinstall the Scrapy ,It's not help.", "issue_status": "Closed", "issue_reporting_time": "2019-04-11T10:42:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "168": {"issue_url": "https://github.com/scrapy/scrapy/issues/3742", "issue_id": "#3742", "issue_summary": "CrawlSpider use UrlJoin failed", "issue_description": "GaryLea commented on Apr 11, 2019 \u2022\nedited\nwhen I use crawlspider, I find a question.\nThis is detail_url: ./201904/t20190409_468256.html\nThis is base_url: http://www.xxx.xxx.cn/xwzx/dzyw/\nThis is success url : http://www.xxx.xxx.cn/xwzx/dzyw/201904/t20190409_468256.html\nThis is crawlspider send url: http://www.xxx.xxx.cn/201904/t20190409_468256.html\nI found url join in HtmlParserLinkExtractor._extract_links\nBut I can't overwrite _extract_links function, what should I do?", "issue_status": "Closed", "issue_reporting_time": "2019-04-11T09:15:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "169": {"issue_url": "https://github.com/scrapy/scrapy/issues/3740", "issue_id": "#3740", "issue_summary": "cd \\", "issue_description": "archegyral commented on Apr 10, 2019\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2019-04-10T15:42:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "170": {"issue_url": "https://github.com/scrapy/scrapy/issues/3736", "issue_id": "#3736", "issue_summary": "Incorrect link mentioned in comments (scrapy.extensions.httpcache)", "issue_description": "Contributor\nanubhavp28 commented on Apr 9, 2019\nscrapy/scrapy/extensions/httpcache.py\nLine 73 in 9280185\n # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1 \nLink mentioned : https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1\nIt should have been https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\nI am sending a pull request for the same.", "issue_status": "Closed", "issue_reporting_time": "2019-04-09T12:18:33Z", "fixed_by": "#3737", "pull_request_summary": "[MRG+1] Fixes a link in comment", "pull_request_description": "Contributor\nanubhavp28 commented on Apr 9, 2019\nFixes #3736", "pull_request_status": "Merged", "issue_fixed_time": "2019-04-12T20:50:20Z", "files_changed": [["2", "scrapy/extensions/httpcache.py"]]}, "171": {"issue_url": "https://github.com/scrapy/scrapy/issues/3734", "issue_id": "#3734", "issue_summary": "Name parameters uniformly (Cache Policy API)", "issue_description": "Contributor\nanubhavp28 commented on Apr 9, 2019\nscrapy/scrapy/extensions/httpcache.py\nLine 34 in 9280185\n def is_cached_response_fresh(self, response, request): \nIt will look better if we name the second parameter as cachedresponse (in place of just response).\nIn fact, implementation of policy for RFC2616 compliant HTTP cache names it correctly.\nscrapy/scrapy/extensions/httpcache.py\nLine 101 in 9280185\n def is_cached_response_fresh(self, cachedresponse, request): \nI am sending a pull request for the same.", "issue_status": "Closed", "issue_reporting_time": "2019-04-09T12:01:23Z", "fixed_by": "#3735", "pull_request_summary": "[MRG+1] Changes a parameter name in class `DummyPolicy`", "pull_request_description": "Contributor\nanubhavp28 commented on Apr 9, 2019\nFixes #3734", "pull_request_status": "Merged", "issue_fixed_time": "2019-04-12T20:52:23Z", "files_changed": [["2", "scrapy/extensions/httpcache.py"]]}, "172": {"issue_url": "https://github.com/scrapy/scrapy/issues/3733", "issue_id": "#3733", "issue_summary": "[twisted] CRITICAL: Unhandled Error", "issue_description": "huzhonglan commented on Apr 9, 2019\n2019-04-09 09:02:21 [twisted] CRITICAL: Unhandled Error\nTraceback (most recent call last):\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/scrapy/commands/crawl.py\", line 58, in run\nself.crawler_process.start()\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/scrapy/crawler.py\", line 293, in start\nreactor.run(installSignalHandlers=False) # blocking call\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/twisted/internet/base.py\", line 1267, in run\nself.mainLoop()\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/twisted/internet/base.py\", line 1276, in mainLoop\nself.runUntilCurrent()\n--- ---\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/twisted/internet/base.py\", line 902, in runUntilCurrent\ncall.func(*call.args, **call.kw)\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/scrapy/utils/reactor.py\", line 41, in call\nreturn self._func(*self._a, **self._kw)\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/scrapy/core/engine.py\", line 122, in _next_request\nif not self._next_request_from_scheduler(spider):\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/scrapy/core/engine.py\", line 149, in _next_request_from_scheduler\nrequest = slot.scheduler.next_request()\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/scrapy/core/scheduler.py\", line 71, in next_request\nrequest = self._dqpop()\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/scrapy/core/scheduler.py\", line 106, in _dqpop\nd = self.dqs.pop()\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/queuelib/pqueue.py\", line 43, in pop\nm = q.pop()\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/scrapy/squeues.py\", line 19, in pop\ns = super(SerializableQueue, self).pop()\nFile \"/home/admin/.virtualenvs/yellowpage/lib/python3.4/site-packages/queuelib/queue.py\", line 162, in pop\nself.f.seek(-size-self.SIZE_SIZE, os.SEEK_END)\nbuiltins.OSError: [Errno 22] Invalid argument", "issue_status": "Closed", "issue_reporting_time": "2019-04-09T09:14:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "173": {"issue_url": "https://github.com/scrapy/scrapy/issues/3732", "issue_id": "#3732", "issue_summary": "Install scrapy error", "issue_description": "jackwener commented on Apr 8, 2019 \u2022\nedited\nenvironment: debian 8\nvirtualenv python3.7\nerror info:\n x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/jakevin/PycharmProjects/spider/venv/include -I/usr/include/python3.7m -c src/twisted/test/raiser.c -o build/temp.linux-x86_64-3.7/src/twisted/test/raiser.o\n    src/twisted/test/raiser.c:4:10: fatal error: Python.h: No such file or directory\n\n     #include \"Python.h\"\n              ^~~~~~~~~~\n    compilation terminated.\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n    \n    ----------------------------------------\nCommand \"/home/xxx/PycharmProjects/spider/venv/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-o_pghiuc/Twisted/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-scgb62mo/install-record.txt --single-version-externally-managed --compile --install-headers /home/jakevin/PycharmProjects/spider/venv/include/site/python3.7/Twisted\" failed with error code 1 in /tmp/pip-install-o_pghiuc/Twisted/\ni am sure that the steps of installation are as described in the documentation \uff08it's seem that there is enough steps about installationin in virtualenv in the doc)", "issue_status": "Closed", "issue_reporting_time": "2019-04-08T06:20:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "174": {"issue_url": "https://github.com/scrapy/scrapy/issues/3731", "issue_id": "#3731", "issue_summary": "Support pathlib.Path in FEED_URI", "issue_description": "Member\nGallaecio commented on Apr 8, 2019 \u2022\nedited\nMake things work the same when the value assigned to the FEED_URI setting is a string containing a path or an instance of pathlib.Path.", "issue_status": "Closed", "issue_reporting_time": "2019-04-07T20:34:17Z", "fixed_by": "#4074", "pull_request_summary": "Added Pathlib.Path support: Issue #3731", "pull_request_description": "Contributor\npurvaudai commented on Oct 13, 2019 \u2022\nedited by Gallaecio\nFixes #3731", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-12T16:43:29Z", "files_changed": [["18", "requirements-py2.txt"], ["4", "scrapy/extensions/feedexport.py"], ["19", "tests/test_feedexport.py"]]}, "175": {"issue_url": "https://github.com/scrapy/scrapy/issues/3721", "issue_id": "#3721", "issue_summary": "\"Edit on Github\" link gives 404 error from docs.scrapy.org documentation pages", "issue_description": "Contributor\nfloat13 commented on Apr 4, 2019\nFor example, from this doc page: https://docs.scrapy.org/en/latest/topics/exporters.html\nclicking the \"Edit on Github\" link in the upper-right corner leads to a 404 error page (tested in Firefox and Safari):\nhttps://github.com/scrapy/scrapy/blob/origin/1.6/docs/topics/exporters.rst\nThis has happened on every doc page I tried the link from.\nI would like to help with some small grammatical edits in the docs (I'm new to Scrapy so I wouldn't make any technical edits). Would a pull request be appropriate for this type of doc edit?\nAlso, specifying which files certain code snippets belong in would be very helpful for beginners.\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2019-04-04T04:29:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "176": {"issue_url": "https://github.com/scrapy/scrapy/issues/3720", "issue_id": "#3720", "issue_summary": "raise error.ReactorNotRestartable() ? why\u3000this error ? but : using : scrapy crawl finance is normal running", "issue_description": "uyplayer commented on Apr 3, 2019 \u2022\nedited\n\u5728\u811a\u672c\u8fd0\u884c\nprint(\"\u5728\u811a\u672c\u8fd0\u884c\")\nprocess = CrawlerProcess(get_project_settings())\nprocess.crawl(FinanceSpider)\nprocess.start()", "issue_status": "Closed", "issue_reporting_time": "2019-04-03T07:29:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "177": {"issue_url": "https://github.com/scrapy/scrapy/issues/3719", "issue_id": "#3719", "issue_summary": "Why is the last two lines of code not executing output?", "issue_description": "uyplayer commented on Apr 3, 2019 \u2022\nedited\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2019-04-03T02:33:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "178": {"issue_url": "https://github.com/scrapy/scrapy/issues/3717", "issue_id": "#3717", "issue_summary": "ERROR: Error downloading", "issue_description": "uyplayer commented on Apr 2, 2019\n[scrapy.core.scraper] ERROR: Error downloading", "issue_status": "Closed", "issue_reporting_time": "2019-04-02T11:36:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "179": {"issue_url": "https://github.com/scrapy/scrapy/issues/3715", "issue_id": "#3715", "issue_summary": "start_request yield Request faster than other", "issue_description": "Gravellent commented on Apr 2, 2019\nI'm using a proxy vendor to crawl a website. The proxy vendor limits the number of requests you can send to the proxy servers per second (say 2 requests per second). In order the fulfill this requirement, I set a DOWNLOAD_DELAY to 0.5 in settings.py. However, in start_requests, if I use a list, the proxy server will return an error for sending too many requests, but after that, the other Requests yielded in the actual spider code never cause any issues. I'm wondering if the delay of Requests in start_requests() is different than the rest of Requests?", "issue_status": "Closed", "issue_reporting_time": "2019-04-02T08:00:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "180": {"issue_url": "https://github.com/scrapy/scrapy/issues/3713", "issue_id": "#3713", "issue_summary": "How do I dynamically generate items", "issue_description": "GaryLea commented on Apr 2, 2019\nI found a solution to this idea in the official document:\nfrom scrapy.item import DictItem, Field\n\ndef create_item_class(class_name, field_list):\nfields = {field_name: Field() for field_name in field_list}\n\nreturn type(class_name, (DictItem,), {'fields': fields})\nbut i don't know how to use them", "issue_status": "Closed", "issue_reporting_time": "2019-04-02T06:24:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "181": {"issue_url": "https://github.com/scrapy/scrapy/issues/3711", "issue_id": "#3711", "issue_summary": "way to get text inside anchor tag in Crawlspider", "issue_description": "suraj-deshmukh commented on Apr 1, 2019\nI have a crawlspider which crawls given site upto certain dept and download the pdfs on that site. Everything works fine but along with link of pdf, i also need text inside anchor tag.\nfor eg:\n<a href='../some/pdf/url/pdfname.pdf'>Project Report</a>\nconsider this anchor tag, in callback i get response object and along with this object i need text inside that tag for eg 'Project Report'.\nIs there any way to get this information along with the response object. i have gone through https://docs.scrapy.org/en/latest/topics/selectors.html link but it not something that i am looking for.", "issue_status": "Closed", "issue_reporting_time": "2019-04-01T07:50:09Z", "fixed_by": "#3712", "pull_request_summary": "[MRG+1] [Docs] CrawlSpider: update Rule docs", "pull_request_description": "Member\nelacuesta commented on Apr 1, 2019\nFixes #3711\nMore information at https://stackoverflow.com/q/55450472/939364", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-09T22:18:52Z", "files_changed": [["14", "docs/topics/spiders.rst"]]}, "182": {"issue_url": "https://github.com/scrapy/scrapy/issues/3705", "issue_id": "#3705", "issue_summary": "I install scrapy in my home directory", "issue_description": "Firststep2014 commented on Mar 29, 2019\nHello, I have install scrapy in /home/u25654/scrapy\nAnd also I have add path ,since I will get\n/glob/intel-python/python3/bin/:/glob/development-tools/versions/intel-parallel-studio/compilers_and_libraries_2019.3.199/linux/bin/intel64:/glob/development-tools/versions/intel-parallel-studio/compilers_and_libraries_2019.3.199/linux/mpi/intel64/libfabric/bin:/glob/development-tools/versions/intel-parallel-studio/compilers_and_libraries_2019.3.199/linux/mpi/intel64/bin:/glob/intel-python/python3/bin/:/glob/intel-python/python2/bin/:/glob/development-tools/versions/intel-parallel-studio/compilers_and_libraries_2019.3.199/linux/bin/intel64:/glob/development-tools/versions/intel-parallel-studio/compilers_and_libraries_2019.3.199/linux/mpi/intel64/libfabric/bin:/glob/development-tools/versions/intel-parallel-studio/compilers_and_libraries_2019.3.199/linux/mpi/intel64/bin:/glob/intel-python/python3/bin/:/glob/intel-python/python2/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/home/u25654/.local/bin:/home/u25654/bin:/home/u25654/.local/bin:/home/u25654/bin:/usr/local/bin:/bin:/home/u25654/scapy/bin:/home/u25654/virtualenv/bin\nHowever, I get \"Command 'scrapy' not found\" when I run in the shell, while it perform well in Jupyter", "issue_status": "Closed", "issue_reporting_time": "2019-03-29T11:59:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "183": {"issue_url": "https://github.com/scrapy/scrapy/issues/3704", "issue_id": "#3704", "issue_summary": "Set environment variable when running scrapy check", "issue_description": "Jesse-Bakker commented on Mar 27, 2019\nSometimes it is nice to be able to enable/disable functionality, e.g. calculating things in settings.py when just checking spider contracts instead of running a crawl. I therefor propose setting an environment variable like SCRAPY_CHECK when using the check command.", "issue_status": "Closed", "issue_reporting_time": "2019-03-27T12:21:12Z", "fixed_by": "#3739", "pull_request_summary": "[MRG+1] Add SCRAPY_CHECK environment variable", "pull_request_description": "Contributor\nMatthijsy commented on Apr 10, 2019 \u2022\nedited\nThis PR implements the environment variable when running scrapy check as asked in #3704 . This way it is possible to have different behaviour in the scraper when running the check (like requiring less settings to be set)\nfixes #3704", "pull_request_status": "Merged", "issue_fixed_time": "2019-06-13T22:43:02Z", "files_changed": [["20", "docs/topics/contracts.rst"], ["23", "scrapy/commands/check.py"], ["20", "scrapy/utils/misc.py"], ["16", "tests/test_utils_misc/__init__.py"]]}, "184": {"issue_url": "https://github.com/scrapy/scrapy/issues/3702", "issue_id": "#3702", "issue_summary": "After scrapy crawl command sometimes scrapy hanging up/waiting", "issue_description": "sebascreen commented on Mar 26, 2019\nAfter command scrapy crawl job_name scrapy stack and nothing happend for ~1minute or more, after this time scrapy run spider job and everything is finished successfully. Issue appears after update scrapy to 1.6 version in 1.5.1 issue does't exists. Is anybody who observed similar issue?", "issue_status": "Closed", "issue_reporting_time": "2019-03-25T21:52:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "185": {"issue_url": "https://github.com/scrapy/scrapy/issues/3700", "issue_id": "#3700", "issue_summary": "why is process_request called twice?", "issue_description": "stever123 commented on Mar 24, 2019 \u2022\nedited\nI have a downloader middleware that will do some stuff when its process_request is called. The problem is, for all my middlewares and some signals, that they are called twice. So for the exact same request (I even gave them a unique ID to be sure), process_request is called twice - why? The reason why I do not some code as an example is that it happens in all my projects, so I think it is more related to Scrapy in general. Note, it only happens with SplashRequest\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2019-03-24T15:09:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "186": {"issue_url": "https://github.com/scrapy/scrapy/issues/3699", "issue_id": "#3699", "issue_summary": "Scrapy returns wrong datetime", "issue_description": "Iliyass commented on Mar 22, 2019 \u2022\nedited\nHello guys,\nas the title says, scrapy returns wrong datetime, I have run scrapy shell into my project and checked datetime there.\nIn [222]: from datetime import datetime\nIn [224]: datetime.now()\nOut[224]: datetime.datetime(2019, 3, 22, 17, 55, 29, 265411)\nand when I check the machine time, it returns the correct time\n$ date\nFri Mar 22 18:55:30 CET 2019\nand when I check python shell\ndatetime.datetime(2019, 3, 22, 18, 57, 4, 58497)\nas you can see the Scrapy datetime returns -1h\nMy Environment:\nScrapy 1.5.1\nPython 2.7.6\nUbuntu 14.04.3 LTS, Trusty Tahr", "issue_status": "Closed", "issue_reporting_time": "2019-03-22T17:59:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "187": {"issue_url": "https://github.com/scrapy/scrapy/issues/3695", "issue_id": "#3695", "issue_summary": "When opening a documentation page, focus its table of contents on the side bar", "issue_description": "Member\nGallaecio commented on Mar 22, 2019\nWhen you open the Item Exporters page, the sidebar does not show its table of contents, and you need to scroll down the sidebar to see it.\nI think our documentation should behave like the Godot documentation does in the same scenario.", "issue_status": "Closed", "issue_reporting_time": "2019-03-22T12:33:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "188": {"issue_url": "https://github.com/scrapy/scrapy/issues/3683", "issue_id": "#3683", "issue_summary": "Document how to implement custom HTTP cache storage backed & cache policy.", "issue_description": "Contributor\nanubhavp28 commented on Mar 15, 2019\nhttps://docs.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache does let readers know they could implement custom HTTP cache storage backend and cache policy, but provides no information about the API. I would like to work on documenting it. Should I go ahead?", "issue_status": "Closed", "issue_reporting_time": "2019-03-14T18:53:32Z", "fixed_by": "#3692", "pull_request_summary": "[MRG+1] doc for creating custom cache storage backend.", "pull_request_description": "Contributor\nanubhavp28 commented on Mar 17, 2019\nFixes #3683", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-02T15:18:15Z", "files_changed": [["57", "docs/topics/downloader-middleware.rst"]]}, "189": {"issue_url": "https://github.com/scrapy/scrapy/issues/3674", "issue_id": "#3674", "issue_summary": "Closing quote missing in example in Using your browser\u2019s Developer Tools for scraping", "issue_description": "vondersam commented on Mar 8, 2019 \u2022\nedited\nThe last spider on this page is missing a closing quote in the following line:\nstart_urls = ['http://quotes.toscrape.com/api/quotes?page=1]\nhttps://docs.scrapy.org/en/latest/topics/developer-tools.html#topics-developer-tools", "issue_status": "Closed", "issue_reporting_time": "2019-03-08T16:57:28Z", "fixed_by": "#3676", "pull_request_summary": "Update developer-tools.rst", "pull_request_description": "Member\nGallaecio commented on Mar 8, 2019\nFixes #3674", "pull_request_status": "Merged", "issue_fixed_time": "2019-03-14T16:52:38Z", "files_changed": [["2", "docs/topics/developer-tools.rst"]]}, "190": {"issue_url": "https://github.com/scrapy/scrapy/issues/3664", "issue_id": "#3664", "issue_summary": "Move API docs to source code", "issue_description": "Contributor\nanubhavp28 commented on Mar 8, 2019\nI have noticed that few classes ( such as Spider class in https://github.com/scrapy/scrapy/blob/master/scrapy/spiders/__init__.py ) lacks docstring inside the source code. Instead, the documentation is provided in the project documentation files. If the developers and maintainers permit, I would like to work on moving the api docs to the source code. I will use autodoc to link it inside the project documentation files. I would love some suggestions on it, should I move ahead?", "issue_status": "Closed", "issue_reporting_time": "2019-03-08T07:58:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "191": {"issue_url": "https://github.com/scrapy/scrapy/issues/3661", "issue_id": "#3661", "issue_summary": "Docs contains a link to a non-existent page.", "issue_description": "Contributor\nanubhavp28 commented on Mar 6, 2019\nhttps://docs.scrapy.org/en/latest/topics/architecture.html has a link to blog post \"Twisted - hello, asynchronous programming\" ( http://jessenoller.com/2009/02/11/twisted-hello-asynchronous-programming/ ) which has been moved or taken offline by the author.", "issue_status": "Closed", "issue_reporting_time": "2019-03-06T08:58:10Z", "fixed_by": "#3662", "pull_request_summary": "Fix a link inside docs", "pull_request_description": "Contributor\nanubhavp28 commented on Mar 7, 2019\nFixes #3661\n\ud83d\udc4d 2", "pull_request_status": "Merged", "issue_fixed_time": "2019-03-14T17:18:05Z", "files_changed": [["2", "docs/topics/architecture.rst"]]}, "192": {"issue_url": "https://github.com/scrapy/scrapy/issues/3659", "issue_id": "#3659", "issue_summary": "Spider error processing", "issue_description": "zhangxinheng commented on Mar 5, 2019\nD:\\SoftwareInstall\\python3.7\\python.exe D:/ScrapyProject/ScrapyText/main.py\n2019-03-06 01:17:31 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: ScrapyText)\n2019-03-06 01:17:31 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b 26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0\n2019-03-06 01:17:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'ScrapyText', 'NEWSPIDER_MODULE': 'ScrapyText.spiders', 'SPIDER_MODULES': ['ScrapyText.spiders']}\n2019-03-06 01:17:31 [scrapy.extensions.telnet] INFO: Telnet Password: 8b498bd6e3499fa3\n2019-03-06 01:17:31 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.logstats.LogStats']\n2019-03-06 01:17:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2019-03-06 01:17:32 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-03-06 01:17:32 [scrapy.middleware] INFO: Enabled item pipelines:\n['ScrapyText.pipelines.ScrapytextPipeline']\n2019-03-06 01:17:32 [scrapy.core.engine] INFO: Spider opened\n2019-03-06 01:17:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-03-06 01:17:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2019-03-06 01:17:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://blog.jobbole.com/all-posts/> (referer: None)\n2019-03-06 01:17:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://blog.jobbole.com/all-posts/> (referer: None)\nTraceback (most recent call last):\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\nyield next(it)\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\nfor x in result:\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in\nreturn (_set_referer(r) for r in result or ())\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in\nreturn (r for r in result or () if _filter(r))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in\nreturn (r for r in result or () if _filter(r))\nFile \"D:\\ScrapyProject\\ScrapyText\\ScrapyText\\spiders\\jobbolen.py\", line 14, in parse\nimage_url=re_node.css(\"image::attr(src\").extract()\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\parsel\\selector.py\", line 262, in css\nreturn self.xpath(self._css2xpath(query))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\parsel\\selector.py\", line 265, in _css2xpath\nreturn self._csstranslator.css_to_xpath(query)\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\parsel\\csstranslator.py\", line 109, in css_to_xpath\nreturn super(HTMLTranslator, self).css_to_xpath(css, prefix)\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\xpath.py\", line 192, in css_to_xpath\nfor selector in parse(css))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 355, in parse\nreturn list(parse_selector_group(stream))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 368, in parse_selector_group\nyield Selector(*parse_selector(stream))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 376, in parse_selector\nresult, pseudo_element = parse_simple_selector(stream)\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 444, in parse_simple_selector\npseudo_element, parse_arguments(stream))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 494, in parse_arguments\n\"Expected an argument, got %s\" % (next,))\nFile \"\", line None\ncssselect.parser.SelectorSyntaxError: Expected an argument, got <EOF at 15>\n2019-03-06 01:17:32 [scrapy.core.engine] INFO: Closing spider (finished)\n2019-03-06 01:17:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 225,\n'downloader/request_count': 1,\n'downloader/request_method_count/GET': 1,\n'downloader/response_bytes': 17774,\n'downloader/response_count': 1,\n'downloader/response_status_count/200': 1,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2019, 3, 5, 17, 17, 32, 795171),\n'log_count/DEBUG': 1,\n'log_count/ERROR': 1,\n'log_count/INFO': 9,\n'response_received_count': 1,\n'scheduler/dequeued': 1,\n'scheduler/dequeued/memory': 1,\n'scheduler/enqueued': 1,\n'scheduler/enqueued/memory': 1,\n'spider_exceptions/SelectorSyntaxError': 1,\n'start_time': datetime.datetime(2019, 3, 5, 17, 17, 32, 21952)}\n2019-03-06 01:17:32 [scrapy.core.engine] INFO: Spider closed (finished)\nProcess finished with exit code 0\n.py\n-- coding: utf-8 --\nimport scrapy\nfrom scrapy.http import Request\nfrom urllib import parse\nfrom ScrapyText.items import Article_Item\nclass JobbolenSpider(scrapy.Spider):\nname = 'jobbolen'\nallowed_domains = ['blog.jobbole.com']\nstart_urls = ['http://blog.jobbole.com/all-posts/']\ndef parse(self, response):\n    re_nodes= response.css('#archive .floated-thumb .post-thumb a')\n    for re_node in re_nodes:\n        image_url=re_node.css(\"image::attr(src\").extract()\n        re_url=re_node.css('::attr(href)').extract()\n        yield Request(url=parse.urljoin(response.url,re_url),meta={'Front_image':image_url},callback=self.text_parse)\n    next_urls=response.css('.next.page-numbers::attr(href)').extract_first()\n    if next_urls:\n        yield Request(url=parse.urljoin(response.url, re_url), callback=self.parse)\n\ndef text_parse(self,response):\n    article_item=Article_Item()\n    re_title = response.css('.entry-header h1::text').extract()[0]\n    re_text = response.css('.entry p::text').extract()\n    front_image=response.meta.get(\"Front_image\",\"\")\n    article_item[\"Title\"]=re_title\n    article_item[\"Text\"]=re_text\n    article_item[\"Front_image\"]=[front_image]\n    yield article_item\n\ud83d\udc4e 1", "issue_status": "Closed", "issue_reporting_time": "2019-03-05T17:40:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "193": {"issue_url": "https://github.com/scrapy/scrapy/issues/3658", "issue_id": "#3658", "issue_summary": "Spider error processing <GET http://blog.jobbole.com/all-posts/> (referer: None)", "issue_description": "zhangxinheng commented on Mar 5, 2019\nD:\\SoftwareInstall\\python3.7\\python.exe D:/ScrapyProject/ScrapyText/main.py\n2019-03-06 01:17:31 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: ScrapyText)\n2019-03-06 01:17:31 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b 26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0\n2019-03-06 01:17:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'ScrapyText', 'NEWSPIDER_MODULE': 'ScrapyText.spiders', 'SPIDER_MODULES': ['ScrapyText.spiders']}\n2019-03-06 01:17:31 [scrapy.extensions.telnet] INFO: Telnet Password: 8b498bd6e3499fa3\n2019-03-06 01:17:31 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.logstats.LogStats']\n2019-03-06 01:17:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2019-03-06 01:17:32 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-03-06 01:17:32 [scrapy.middleware] INFO: Enabled item pipelines:\n['ScrapyText.pipelines.ScrapytextPipeline']\n2019-03-06 01:17:32 [scrapy.core.engine] INFO: Spider opened\n2019-03-06 01:17:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-03-06 01:17:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2019-03-06 01:17:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://blog.jobbole.com/all-posts/> (referer: None)\n2019-03-06 01:17:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://blog.jobbole.com/all-posts/> (referer: None)\nTraceback (most recent call last):\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\nyield next(it)\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\nfor x in result:\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in\nreturn (_set_referer(r) for r in result or ())\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in\nreturn (r for r in result or () if _filter(r))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in\nreturn (r for r in result or () if _filter(r))\nFile \"D:\\ScrapyProject\\ScrapyText\\ScrapyText\\spiders\\jobbolen.py\", line 14, in parse\nimage_url=re_node.css(\"image::attr(src\").extract()\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\parsel\\selector.py\", line 262, in css\nreturn self.xpath(self._css2xpath(query))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\parsel\\selector.py\", line 265, in _css2xpath\nreturn self._csstranslator.css_to_xpath(query)\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\parsel\\csstranslator.py\", line 109, in css_to_xpath\nreturn super(HTMLTranslator, self).css_to_xpath(css, prefix)\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\xpath.py\", line 192, in css_to_xpath\nfor selector in parse(css))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 355, in parse\nreturn list(parse_selector_group(stream))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 368, in parse_selector_group\nyield Selector(*parse_selector(stream))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 376, in parse_selector\nresult, pseudo_element = parse_simple_selector(stream)\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 444, in parse_simple_selector\npseudo_element, parse_arguments(stream))\nFile \"D:\\SoftwareInstall\\python3.7\\lib\\site-packages\\cssselect\\parser.py\", line 494, in parse_arguments\n\"Expected an argument, got %s\" % (next,))\nFile \"\", line None\ncssselect.parser.SelectorSyntaxError: Expected an argument, got <EOF at 15>\n2019-03-06 01:17:32 [scrapy.core.engine] INFO: Closing spider (finished)\n2019-03-06 01:17:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 225,\n'downloader/request_count': 1,\n'downloader/request_method_count/GET': 1,\n'downloader/response_bytes': 17774,\n'downloader/response_count': 1,\n'downloader/response_status_count/200': 1,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2019, 3, 5, 17, 17, 32, 795171),\n'log_count/DEBUG': 1,\n'log_count/ERROR': 1,\n'log_count/INFO': 9,\n'response_received_count': 1,\n'scheduler/dequeued': 1,\n'scheduler/dequeued/memory': 1,\n'scheduler/enqueued': 1,\n'scheduler/enqueued/memory': 1,\n'spider_exceptions/SelectorSyntaxError': 1,\n'start_time': datetime.datetime(2019, 3, 5, 17, 17, 32, 21952)}\n2019-03-06 01:17:32 [scrapy.core.engine] INFO: Spider closed (finished)\nProcess finished with exit code 0\n\ud83d\udc4e 1", "issue_status": "Closed", "issue_reporting_time": "2019-03-05T17:29:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "194": {"issue_url": "https://github.com/scrapy/scrapy/issues/3657", "issue_id": "#3657", "issue_summary": "TypeError: close_spider() missing 1 required positional argument: 'reason'", "issue_description": "gun2021 commented on Mar 5, 2019 \u2022\nedited\nHello,\nI'm getting this error. This is the first time i get it and I don't really understand the reason:\n> Traceback (most recent call last):\n>   File \"C:\\Users\\EAgnelli\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\twisted\\internet\\defer.py\", line 654, in _runCallbacks\n>     current.result = callback(current.result, *args, **kw)\n> TypeError: close_spider() missing 1 required positional argument: 'reason'\nThis is my spider class\nclass CointelegraphspiderSpider(scrapy.Spider):\n    name = 'cointelegraphspider'\n    allowed_domains = ['cointelegraph.com']\n    start_urls = ['http://cointelegraph.com/']\n\n    def start_requests(self):\n        \n        \"\"\"\n        Doc string\n        \"\"\"\n        \n        # Execute the LUA script for \"Load Mor\" button\n        script = \"\"\"\n        \n            function main(splash, args)\n                assert(splash:go(args.url))\n                splash:wait(0.5)\n                local num_clicks = 2\n                local delay = 1.5\n                local load_more = splash:jsfunc(\n                            [[\n                                function ()\n                                {\n                                    var el = document.getElementsByClassName('post-preview-list-navigation__btn post-preview-list-navigation__btn_load-more');\n                                    el[0].click();\n                                } \n                            ]]\n                                \n                            )\n      \n                for _ = 1, num_clicks do\n                    load_more()\n                    splash:wait(delay)\n                end        \n    \n                return \n                {\n                    html = splash:html(),\n                }\n            end\n\n        \"\"\"\n        \n        for url in self.start_urls:\n            \n            yield scrapy_splash.SplashRequest(\n                    url=url,\n                    callback=self.parse_main_page,\n                    args={\n                            'wait':3,\n                            'lua_source':script,\n                            #'timeout': 3600 # Here the max-timeout is 60 -- to increase it launch the docker with --max-timeout xxxxx\n                            },\n                    endpoint=\"execute\",\n                    )\n        \n    def parse_main_page(self, response):\n        \"\"\"\n        Doc string\n        \"\"\"        \n        # Convert Splash response into html response object\n        html = scrapy.Selector(response)\n        \n        # Check DB for existing records\n        conn = sq3.connect(\"D:\\\\DCC\\\\Projects\\\\crypto_projects\\\\master_data.db\")\n        db_links = conn.execute(\"select link from cointelegraph\").fetchall() # list of tuples\n        db_links = [elem[0] for elem in db_links] # flattening list\n        print(\"DB LINKS! \", db_links)\n        #db_links = [\"aaa\",]\n        conn.close() # close connection\n           \n        # Extract all links to be followed\n        news_links = LinkExtractor(restrict_xpaths=['//ul[@class=\"post-preview-list-cards\"]/li/div/article/a', # Main Body\n                                                    '//div[@class=\"main-news-tabs__wrp\"]/ul/li/div/a'] # \"Editor's Choice\" & \"Hot Stories\"\n                                    ).extract_links(html.response)\n        \n        for link in news_links[:2]:\n            # Follow only new links\n            if link.url not in db_links:\n                yield scrapy.Request(link.url, callback=self.parse_article)\n        \n\n    def parse_article(self, response):\n        \"\"\"\n        Doc string\n        \"\"\"\n        \n        # Create Item for Pipeline\n        item = CointelegraphSpiderItem()\n\n        item['author'] = response.xpath('//div[@class=\"name\"]/a/text()').extract_first().strip()\n        item['timestamp'] = response.xpath('//div/@datetime').extract_first().split('t')[0] # %Y-%m-%d\n        item['title'] = response.xpath('//h1[@class=\"header\"]/text()').extract_first().strip()\n        item['body'] = ' '.join(response.xpath('//div[@class=\"post-full-text contents js-post-full-text\"]/p//text()').extract())\n        item['quotes'] = ';;;'.join(response.xpath('//div[@class=\"post-full-text contents js-post-full-text\"]/blockquote//text()').extract())\n        item['int_links'] = ';;;'.join(response.xpath('//div[@class=\"post-full-text contents js-post-full-text\"]/p/a/@href').extract())\n        _tmp = [elem.replace('#','') for elem in response.xpath('//div[@class=\"tags\"]/ul/li/a/text()').extract()]\n        item['tags'] = ';;;'.join([elem.replace(' ','') for elem in _tmp])\n        item['link'] = response.url\n        item['news_id'] = str(hash(item['link']))\n        \n        yield item\nThis is my Pipeline\nimport sqlite3 as sq3\nimport sqlite3_functions as sq_f\nimport logging\nfrom scrapy.exceptions import DropItem\n\nclass CointelegraphSpiderPipeline(object):\n    \"\"\"\n    Doc string\n    \"\"\"\n\n    def __init__(self, stats):\n        \"\"\"\n        Doc string\n        \"\"\"\n        self.stats = stats\n        self.db_file = 'D:\\\\DCC\\\\Projects\\\\crypto_projects\\\\master_data.db'\n        self.conn = sq3.connect(self.db_file)\n        self.table_name = 'cointelegraph'\n        self.commit_counter = 0\n        \n        \n    @classmethod\n    def from_crawler(cls, crawler):\n        \"\"\"\n        Doc string\n        \"\"\"\n        stats = crawler.stats\n        return stats   #cls(crawler.stats)\n        \n    def open_spider(self, spider):\n        \"\"\"\n        Doc string\n        \"\"\"\n        print(\"I'm starting the pipeline\")\n        logging.INFO(\"Starting Pipeline...\")\n        \n    def process_item(self, item, spider):\n        \"\"\"\n        Doc string\n        \"\"\"\n        item_checked = True\n        \n        try:\n            # Sanity Check\n            for key, value in item.items():\n                print(\"Inside the loop!!!\")\n                if value == '':\n                    item_checked = False\n                    raise DropItem(\"Item '{0}:{1}' has empty data - Link: {3}\".format(key, value, item['link']))\n                else:\n                    logging.INFO(\"Item check OK\")\n                    item_checked = True\n            \n            # Insert row and increase counter\n            if item_checked:\n                self.conn = sq_f.insert_row(self.db_file, table_name=self.table_name, conn=self.conn, **item)\n                self.commit_counter += 1\n                self.conn.commit()\n                \n            # Commit every 500 inserted rows\n            if self.commit_counter % 500 == 0:\n                self.conn.commit()\n                \n            print(item)\n        \n        except Exception as e:\n            logging.WARNING(e)\n        \n        \n\n\n    def close_spider(self, spider):\n        \"\"\"\n        Doc string\n        \"\"\"\n        logging.INFO(\"Commiting rows...\")\n        self.conn.commit()\n        logging.INFO(\"Saving spider stats...\")\n        print(self.stats.get_stats())\n        logging.INFO(\"Closing pipeline..\")\n        self.conn.close()`\nThis are my Settings\nBOT_NAME = 'cointelegraph_spider'\n\nSPIDER_MODULES = ['cointelegraph_spider.spiders']\nNEWSPIDER_MODULE = 'cointelegraph_spider.spiders'\n\n\nSPLASH_URL = 'http://localhost:8050'\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\n\n\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3699.0 Safari/537.36'\n\nROBOTSTXT_OBEY = True\n\n\n\n\n\n\nSPIDER_MIDDLEWARES = {\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\n    \n}\n\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy_splash.SplashCookiesMiddleware': 723,\n    'scrapy_splash.SplashMiddleware': 725,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n}\n\n\nITEM_PIPELINES = {\n    'cointelegraph_spider.pipelines.CointelegraphSpiderPipeline': 300,\n}", "issue_status": "Closed", "issue_reporting_time": "2019-03-05T17:21:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "195": {"issue_url": "https://github.com/scrapy/scrapy/issues/3656", "issue_id": "#3656", "issue_summary": "[GSoC 2019] Support for Different robots.txt Parsers", "issue_description": "Contributor\nwhalebot-helmsman commented on Mar 5, 2019 \u2022\nedited by Gallaecio\nThis issue is a single place for all students and mentors to discuss ideas and proposals for Support for Different robots.txt Parsers GSoC project.\nFirst of all, every student involved should have not very big contribution to https://github.com/scrapy/scrapy project. It should not be very big, just to get your hands dirty and get accustomed to processes and tools. Contribution should be done in a form of open Pull Request to solve a problem not related to robots.txt project. You can read open issues or open PRs and choose one for yourself. Or you can ask here and mentors and contributors will some recommendations.\nProblems for current robots.txt implementation can be traced in relevant issues:\n#754 (this is a main one)\n#892\n#2443\n#3637\nPrevious attempts to fix issues can be seen in relevant PRs:\n#2669\n#2385\nAsk for more details in comments\n\ud83d\udc4d 6", "issue_status": "Closed", "issue_reporting_time": "2019-03-05T09:37:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "196": {"issue_url": "https://github.com/scrapy/scrapy/issues/3655", "issue_id": "#3655", "issue_summary": "GSoC 2019: Thoughts on request fingerprint", "issue_description": "Gravellent commented on Mar 5, 2019\nHi all,\nMy name is Martin, a master candidate in information system at Carnegie Mellon. I have worked as an analyst and product manager before, but I would like to become more involved in the development space and contribute more to open source projects.\nI have personally used scrapy a lot, using it both for personal projects as well as for work. I would be extremely excited to contribute to this project through GoSC.\nI find the request fingerprints project very interesting, as I have personally faced issues with this before. For example, when I scraped Instagram, the website uses a \"session_id\" to check on the frequency of requests. The default behavior works in this case, but if I want to see how different users sees the same URL (for example, the recommendation page), it would be hard with the current framework.\nI checked the source code and found the fingerprint is implemented here.\n_fingerprint_cache = weakref.WeakKeyDictionary()\ndef request_fingerprint(request, include_headers=None):\n    if include_headers:\n        include_headers = tuple(to_bytes(h.lower())\n                                 for h in sorted(include_headers))\n    cache = _fingerprint_cache.setdefault(request, {})\n    if include_headers not in cache:\n        fp =  ()\n        fp.update(to_bytes(request.method))\n        fp.update(to_bytes(canonicalize_url(request.url)))\n        fp.update(request.body or b'')\n        if include_headers:\n            for hdr in include_headers:\n                if hdr in request.headers:\n                    fp.update(hdr)\n                    for v in request.headers.getlist(hdr):\n                        fp.update(v)\n        cache[include_headers] = fp.hexdigest()\n    return cache[include_headers]\nThe function is used in the dupefilter class, but the optional parameter \"include_header\" is never used anywhere in the project. The documentation (see below) says that you can customize this by overwriting the request_fingerprint function. However, this will be out of scope for most users who did the pip install, and I think users should be provided more convinient ways to adjust this setting.\nThe default (RFPDupeFilter) filters based on request fingerprint using the scrapy.utils.request.request_fingerprint function. In order to change the way duplicates are checked you could subclass RFPDupeFilter and override its request_fingerprint method. This method should accept scrapy Request object and return its fingerprint (a string).\nOn the idea page, it says\nScrapy uses a Request fingerprinting scheme for de-duplicating requests and for caching. Currently, the fingerprinting algorithm cannot be modified by Scrapy users.\nNo use cases were mentioned here. The use case I can think of is to provide user simple ways to treat certain headers as part of the fingerprint (for example, language settings or session_id for some sites). Under the current implementation, even if the user chooses to enable the \"include_headers\" parameter, he/she cannot specify the part of headers to be included, meaning things like rotating User-Agent will cause the program to crawl the same page multiple times.\nThose are just my two cents on this issue. If anyone can provide more use cases on this, it can really help me think this through.\n@Gallaecio If you can provide some comment on this it would be great :)\nThanks in advance,\nMartin", "issue_status": "Closed", "issue_reporting_time": "2019-03-05T06:41:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "197": {"issue_url": "https://github.com/scrapy/scrapy/issues/3652", "issue_id": "#3652", "issue_summary": "[twisted] CRITICAL: Unhandled error in Deferred:", "issue_description": "ArunAug commented on Mar 4, 2019\n2019-03-04 13:01:00 [scrapy.crawler] INFO: Overridden settings: {}\n2019-03-04 13:01:00 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.logstats.LogStats']\n2019-03-04 13:01:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2019-03-04 13:01:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-03-04 13:01:01 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2019-03-04 13:01:01 [scrapy.core.engine] INFO: Spider opened\nUnhandled error in Deferred:\n2019-03-04 13:01:01 [twisted] CRITICAL: Unhandled error in Deferred:\n\nTraceback (most recent call last):\n  File \"site-packages\\scrapy\\crawler.py\", line 172, in crawl\n\n  File \"site-packages\\scrapy\\crawler.py\", line 176, in _crawl\n\n  File \"site-packages\\twisted\\internet\\defer.py\", line 1613, in unwindGenerator\n\n  File \"site-packages\\twisted\\internet\\defer.py\", line 1529, in _cancellableInlineCallbacks\n\n--- <exception caught here> ---\n  File \"site-packages\\twisted\\internet\\defer.py\", line 1418, in _inlineCallbacks\n\n  File \"site-packages\\scrapy\\crawler.py\", line 82, in crawl\n\nbuiltins.ModuleNotFoundError: No module named '_sqlite3'", "issue_status": "Closed", "issue_reporting_time": "2019-03-04T07:43:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "198": {"issue_url": "https://github.com/scrapy/scrapy/issues/3651", "issue_id": "#3651", "issue_summary": "How to let RetryMiddleware handle exceptions within retry times?", "issue_description": "Mactarvish commented on Mar 3, 2019\nHi, I'm trying to write a retry middleware to handle exceptions. I implemented the process_exception function, but it only executed when RETRY_TIME is up. I'd like to know how can I set the process_exception triggered in every time the request retries. Thanks in advance.", "issue_status": "Closed", "issue_reporting_time": "2019-03-03T13:47:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "199": {"issue_url": "https://github.com/scrapy/scrapy/issues/3650", "issue_id": "#3650", "issue_summary": "Contributing to Scrapy in GSoC 2019", "issue_description": "Contributor\nmaramsumanth commented on Mar 2, 2019 \u2022\nedited\nHello all,\nMy name is Maram Sumanth from IIT Roorkee, India. I have gone through the ideas of scrapy and I have found the project on Support for Different robots.txt Parsers to be particularly interesting.\nI am curious to know about this project. Is this project based on creating many robots.txt parsers and their corresponding middlewares with API? Also what does fully-compliant exactly mean ?\n@lopuhin @whalebot-helmsman , can you please provide me the detailed explanation of this project, so that it I will start working on it ASAP.\nI do have some open source experience and I would love to contribute to Scrapy and participate in GSoC 2019. :)\nThanks \ud83d\udc4d", "issue_status": "Closed", "issue_reporting_time": "2019-03-02T08:08:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "200": {"issue_url": "https://github.com/scrapy/scrapy/issues/3644", "issue_id": "#3644", "issue_summary": "How to make RetryMiddleware handle exceptions during retry times?", "issue_description": "Mactarvish commented on Mar 1, 2019 \u2022\nedited\nHi, I'm trying to write my own RetryMiddleware to handle exceptions, but I noticed that the process_exception in my RetryMiddleware only works when RETRY_TIMES is up. Is there any way to handle exceptions within the RETRY_TIMES? Thanks in advance.\nThat's my RetryMiddleware ...\n`class XiciRetryMiddleware(RetryMiddleware):\ndef process_exception(self, request, exception, spider):\nif isinstance(exception, self.EXCEPTIONS_TO_RETRY)\nand not request.meta.get('dont_retry', False):\nself.delete_proxy(spider)\n# time.sleep(random.randint(3, 5))\nrequest.meta['retry_times'] = 0\nreturn self._retry(request, exception, spider)\ndef process_response(self, request, response, spider):\n    return response`", "issue_status": "Closed", "issue_reporting_time": "2019-03-01T07:58:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "201": {"issue_url": "https://github.com/scrapy/scrapy/issues/3642", "issue_id": "#3642", "issue_summary": "Selector Raises lxml.etree.XMLSyntaxError on certain xml", "issue_description": "mohmad-null commented on Feb 25, 2019 \u2022\nedited\npage = b'\\x00\\x00ws:ExceptionReport></ows:ExceptionReport>\\x00'\nresponse = XmlResponse(\n url='http://example.com',\n body=page,\n encoding='utf-8'\n)\nif hasattr(response, 'selector'):\n pass\nThis crashes with the below traceback.\nGiven I'm doing a simple \"hasattr\" for the selector, I don't think it should be crashing out but should be returnin False.\nscrapy==1.5.0\nlxml==4.1.1\nTraceback:\n   File \"c:\\proj\\src\\spider_aux_funcs.py\", line 1335, in get_resp_content_type\n  if hasattr(response, 'selector'):\n   File \"c:\\proj\\venv\\lib\\site-packages\\scrapy\\http\\response\\text.py\", line 115, in selector\n  self._cached_selector = Selector(self)\n   File \"c:\\proj\\venv\\lib\\site-packages\\scrapy\\selector\\unified.py\", line 71, in __init__\n  super(Selector, self).__init__(text=text, type=st, root=root, **kwargs)\n   File \"c:\\proj\\venv\\lib\\site-packages\\parsel\\selector.py\", line 180, in __init__\n  root = self._get_root(text, base_url)\n   File \"c:\\proj\\venv\\lib\\site-packages\\parsel\\selector.py\", line 191, in _get_root\n  return create_root_node(text, self._parser, base_url=base_url)\n   File \"c:\\proj\\venv\\lib\\site-packages\\parsel\\selector.py\", line 43, in create_root_node\n  root = etree.fromstring(body, parser=parser, base_url=base_url)\n   File \"src\\lxml\\etree.pyx\", line 3230, in lxml.etree.fromstring (src\\lxml\\etree.c:81056)\n   File \"src\\lxml\\parser.pxi\", line 1871, in lxml.etree._parseMemoryDocument (src\\lxml\\etree.c:121236)\n   File \"src\\lxml\\parser.pxi\", line 1759, in lxml.etree._parseDoc (src\\lxml\\etree.c:119912)\n   File \"src\\lxml\\parser.pxi\", line 1125, in lxml.etree._BaseParser._parseDoc (src\\lxml\\etree.c:114159)\n   File \"src\\lxml\\parser.pxi\", line 598, in lxml.etree._ParserContext._handleParseResultDoc (src\\lxml\\etree.c:107724)\n   File \"src\\lxml\\parser.pxi\", line 709, in lxml.etree._handleParseResult (src\\lxml\\etree.c:109433)\n   File \"src\\lxml\\parser.pxi\", line 638, in lxml.etree._raiseParseError (src\\lxml\\etree.c:108287)\n   File \"http://example.com\", line 1\n lxml.etree.XMLSyntaxError: Document is empty, line 1, column 1", "issue_status": "Closed", "issue_reporting_time": "2019-02-25T18:10:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "202": {"issue_url": "https://github.com/scrapy/scrapy/issues/3641", "issue_id": "#3641", "issue_summary": "for loop only gets last div", "issue_description": "Alainvdz99 commented on Feb 25, 2019\nHi everyone,\ni am trying to get the title and the price from every div that has the class product-shop. The problem with my code now is that when I run it I am only having the information from the last div. This is my code:\nclass FiyoSpider(scrapy.Spider):\n    name = 'fiyo'\n    allowed_domains = ['fiyo.nl']\n    start_urls = ['https://www.fiyo.nl/stofzuiger/stofzuigerborstel/kierenzuigmond/-siemens?device_brand=624']\n\n    def parse(self, response):\n        for vacuumCleaner in response.css('div.product-shop'):\n            vacuumCleanerMounth = {\n            'product name': vacuumCleaner.css('h2.product-name::text').extract_first(),\n            'price': vacuumCleaner.css('span.price::text').extract_first(),\n        }\n        yield vacuumCleanerMounth", "issue_status": "Closed", "issue_reporting_time": "2019-02-25T09:51:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "203": {"issue_url": "https://github.com/scrapy/scrapy/issues/3640", "issue_id": "#3640", "issue_summary": "download middlewares don't work", "issue_description": "Mactarvish commented on Feb 25, 2019\nHi, I'm trying to set a proxy for my spider. Firstly I modified the settings.py in my work directory:\n`\nEnable or disable downloader middlewares\nSee https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\nDOWNLOADER_MIDDLEWARES = {\n'xici.middlewares.XiciDownloaderMiddleware': 543,\n}\nThen I modified the middlewares.py:\nclass XiciDownloaderMiddleware(object):\n# Not all methods need to be defined. If a method is not defined,\n# scrapy acts as if the downloader middleware does not modify the\n# passed objects.\n@classmethod\ndef from_crawler(cls, crawler):\n    # This method is used by Scrapy to create your spiders.\n    s = cls()\n    crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n    return s\n\ndef process_request(self, request, spider):\n    # Called for each request that goes through the downloader\n    # middleware.\n    request.meta['proxy'] = \"http://116.209.56.19:9999\"\n\n    # Must either:\n    # - return None: continue processing this request\n    # - or return a Response object\n    # - or return a Request object\n    # - or raise IgnoreRequest: process_exception() methods of\n    #   installed downloader middleware will be called\n    return None\n\ndef process_response(self, request, response, spider):\n    # Called with the response returned from the downloader.\n\n    # Must either;\n    # - return a Response object\n    # - return a Request object\n    # - or raise IgnoreRequest\n    return response\n\ndef process_exception(self, request, exception, spider):\n    # Called when a download handler or a process_request()\n    # (from other downloader middleware) raises an exception.\n\n    # Must either:\n    # - return None: continue processing this exception\n    # - return a Response object: stops process_exception() chain\n    # - return a Request object: stops process_exception() chain\n    pass\n\ndef spider_opened(self, spider):\n    spider.logger.info('Spider opened: %s' % spider.name)\n`\nBut when the code executed the process_request was never executed. Is there anything I ignored? Thanks in advance!", "issue_status": "Closed", "issue_reporting_time": "2019-02-25T07:32:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "204": {"issue_url": "https://github.com/scrapy/scrapy/issues/3637", "issue_id": "#3637", "issue_summary": "False negatives in robots.txt processing?", "issue_description": "alanbchristie commented on Feb 23, 2019\nhttps://www.idealista.it has a robots.txt which appears complex but essentially has the following: -\nUser-agent: *\nAllow: /en/geo/\nScrapy (1.6.0) keeps telling me that where-ever I go on this site that I'm Forbidden by robots.tx: -\n2019-02-23T11:06:44.226Z scrapy.downloadermiddlewares.robotstxt DEBUG # Forbidden by robots.txt: <GET https://www.idealista.it/en/geo/vendita-case/molise/>\nI'm confused. I don't think I should blocked and I suspect that Scrapy may be thrown by other instructions in the robots.txt file.\nI'm no expert by any means but when I validate an apparently legitimate URL (https://www.idealista.it/en/geo/vendita-case/molise/) using an independent tool like http://tools.seobook.com/robots-txt/analyzer/ (and I've tried more than one to gain confidence) I'm told...\nUrl: https://www.idealista.it/en/geo/vendita-case/molise/\nMultiple robot rules found \nRobots allowed: All robots\nSo, is the robot.txt analysis in scrapy broken?\nScrapy tells me that everywhere on this site is blocked by the robots.txt. Just looking at the file myself, and not fully understanding the order of precedence, that just doesn't seem right.\nIf the answer is \"Scrapy is correct\" then why does it conflict with other analysers?\nIs there more I need to configure in Scrapy?\nIs there some middlewhere I'm missing?\nAnd, most importantly, how do I continue to use Scrapy now and analyse sites like this? Suggestions I don't want are: circumvent robots with set ROBOTSTXT_OBEY = False or write your own robots.txt analyser.", "issue_status": "Closed", "issue_reporting_time": "2019-02-23T11:18:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "205": {"issue_url": "https://github.com/scrapy/scrapy/issues/3633", "issue_id": "#3633", "issue_summary": "error on \"pip install scrapy\"", "issue_description": "kennblvnp commented on Feb 19, 2019\nPython 3.7\npip 19.0.2\nCommand \"c:\\users\\kenda\\appdata\\local\\programs\\python\\python37-32\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-p0r6426p\\Twisted\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-record-0rrj6dmu\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\kenda\\AppData\\Local\\Temp\\pip-install-p0r6426p\\Twisted\\", "issue_status": "Closed", "issue_reporting_time": "2019-02-19T07:38:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "206": {"issue_url": "https://github.com/scrapy/scrapy/issues/3632", "issue_id": "#3632", "issue_summary": "Unable to connect through HTTPS proxy with authentication", "issue_description": "nawashy commented on Feb 19, 2019\nHi,\nI'm trying to scrap HTTPS site through HTTPS proxy that requires Basic authentication, the connection is established but scrapy does not issue client hello, and then does not get any data from the web site.\nI can do the same using curl for example\ncurl -U user:password -x https://proxy:8443 -v https://icanhazip.com\nit works without any problem.\nScrapy works with the HTTPS proxy if I change the target site from https to http\nScrapy : 1.6.0\nlxml : 4.3.1.0\nlibxml2 : 2.9.9\ncssselect : 1.0.3\nparsel : 1.5.1\nw3lib : 1.20.0\nTwisted : 18.9.0\nPython : 3.7.2 (default, Jan 10 2019, 23:51:51) - [GCC 8.2.1 20181127]\npyOpenSSL : 19.0.0 (OpenSSL 1.1.1a 20 Nov 2018)\ncryptography : 2.5\nPlatform : Linux-4.19.20-1-MANJARO-x86_64-with-arch-Manjaro-Linux\nthanks in advance", "issue_status": "Closed", "issue_reporting_time": "2019-02-19T03:02:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "207": {"issue_url": "https://github.com/scrapy/scrapy/issues/3624", "issue_id": "#3624", "issue_summary": "Database validation does not work in for loop", "issue_description": "milanHirpara commented on Feb 13, 2019 \u2022\nedited\nHello\nI am having trouble in checking existing data in scrapy. i have used elasticsearch as my database below code i am trying to execute ??\n`\n    def checkIfURLExistsInCrawler(single_url):\n                 elastic_query = json.dumps({\n                       \"query\": {\n                              \"match_phrase\": {\n                                       \"url\": single_url\n                               }\n                        }\n                })\n    \n                 result = es.search(index='test', doc_type='test', body=elastic_query)['hits']['hits']\n                 return result\n\n   def start_requests(self):\n         urls = [\n\n          here i have some url there might be chance that some urls are duplicate so i have to put \n          validation but in for loop it doesn't working  \n\n        ]\n\n         for request_url in urls:\n              checkExists = self.checkIfURLExistsInCrawler(request_url)\n\n              if not checkExists :\n                     beingCrawledUrl = {}\n                     beingCrawledUrl['url'] = single_url\n                     beingCrawledUrl['added_on'] = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n                     json_data = json.dumps(beingCrawledUrl)\n                     InsertData = es.index(index='test', doc_type='test', body=json.loads(json_data))\n                     yield scrapy.Request();\n`\nif i execute this code all record inside urls = [ ] are inserted into \"test\" index even if its duplicated because of validation i put above is not working .\nbut if i run this again with same data validation works .so please can any one help this out.", "issue_status": "Closed", "issue_reporting_time": "2019-02-13T11:55:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "208": {"issue_url": "https://github.com/scrapy/scrapy/issues/3623", "issue_id": "#3623", "issue_summary": "Database validation does not work in for loop", "issue_description": "milanHirpara commented on Feb 13, 2019\nHello\nI am having trouble in checking existing data in scrapy. i have used elasticsearch as my database below code i am trying to execute ??", "issue_status": "Closed", "issue_reporting_time": "2019-02-13T11:55:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "209": {"issue_url": "https://github.com/scrapy/scrapy/issues/3622", "issue_id": "#3622", "issue_summary": "Feature: Ability to filter extracted links by tag's text value", "issue_description": "Contributor\nmatthieucham commented on Feb 13, 2019\nThis is a very simple feature that I happen to have to develop in my Scrapy-based project because I found no built-in acceptable way to do it. Now I am offering to push this little piece of evolution into the Scrapy codebase:\nIn FilteringLinkExtractor, you can filter links whose url (href attribute's value) match a given regex, which is really helpful. However, it's not always sufficient. For instance, I once wanted to crawl a website where all urls look the same (some random uuid) but I only wanted to follow some : the ones with some special keyword in the text value of the tag. Like this:\n<a href=\"https://www.website.org/someuuid1>Pick me!</a>\n<a href=\"https://www.website.org/someuuid2>Not !</a>\n<a href=\"https://www.website.org/someuuid3>Do pick me please !</a>\nAnd my crawler had to follow the links having the word \"pick\" in their text.\nTo handle this case, I developed an extension of the FilteringLinkExtractor with an additional argument filter_text=\nThe value of this arg is handled the same way as the allow= arg of the constructor, except, it works on the text() value of the tag instead of its href attribute.\nSo what do you think ? Would it be a positive addition to the features of the link extractor ? Or did I miss an already existing way to do what I wanted in the first place ?\nRegards", "issue_status": "Closed", "issue_reporting_time": "2019-02-12T21:23:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "210": {"issue_url": "https://github.com/scrapy/scrapy/issues/3619", "issue_id": "#3619", "issue_summary": "issues in installing 'src'", "issue_description": "samohant commented on Feb 11, 2019\nUnable to install 'src'\nCollecting src\nUsing cached https://files.pythonhosted.org/packages/9a/2b/a6ccfc80af698319c54f00da05f6c798cf72291938893f8bd3f730c2daf9/src-0.0.7.zip\nBuilding wheels for collected packages: src\nBuilding wheel for src (setup.py) ... error\n. . .\n. . .\nreturn (self.distribution.has_pure_modules() or\nAttributeError: 'NoneType' object has no attribute 'has_pure_modules'\nFailed building wheel for src\nRunning setup.py clean for src\nFailed to build src\nInstalling collected packages: src\nRunning setup.py install for src ... error", "issue_status": "Closed", "issue_reporting_time": "2019-02-11T00:22:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "211": {"issue_url": "https://github.com/scrapy/scrapy/issues/3616", "issue_id": "#3616", "issue_summary": "Document LogFormatter", "issue_description": "Member\nGallaecio commented on Feb 6, 2019\nCurrently, the LogFormatter class is only mentioned in the Release notes page of the documentation. This class should be properly documented, both its API members and a small section introducing it on the documentation page about Logging.\nThe responses to Scrapy - Silently drop an item in StackOverflow would be a good starting point.", "issue_status": "Closed", "issue_reporting_time": "2019-02-06T16:26:46Z", "fixed_by": "#3660", "pull_request_summary": "[MRG+1] docs for scrapy.logformatter", "pull_request_description": "Contributor\nanubhavp28 commented on Mar 6, 2019\nFixes #3616 by documenting logFormatter class.", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-23T20:40:56Z", "files_changed": [["11", "docs/topics/logging.rst"], ["9", "docs/topics/settings.rst"], ["46", "scrapy/logformatter.py"]]}, "212": {"issue_url": "https://github.com/scrapy/scrapy/issues/3611", "issue_id": "#3611", "issue_summary": "try to pass two arguments with -a to spider", "issue_description": "digitaldust commented on Feb 1, 2019\nI am trying to pass two argument to scrapy as per the docs, like this:\nscrapy crawl myspider -a product_id=BEHEH -a last_update='01 January 1990' -s LOG_FILE=scrapy.log\nbut get the error:\ncrawl: error: running 'scrapy crawl' with more than one spider is no longer supported\nIs this a known behavior or am I missing something?\nI am on Mac OSX 10.14.2 with Scrapy 1.5.1", "issue_status": "Closed", "issue_reporting_time": "2019-02-01T14:27:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "213": {"issue_url": "https://github.com/scrapy/scrapy/issues/3606", "issue_id": "#3606", "issue_summary": "CrawlerRunner not crawl pages inside into a function", "issue_description": "nicoparsa commented on Jan 28, 2019 \u2022\nedited\nHi,\nI am trying to launch a Scrapy from script with CrawlerRunner(), because with CrawlerProcess() I have another problem: #3600\nI watched in Stackoverflow the solution with crochet library, but it doesn\u00b4t work for me.\nLinks: Stackoverflow 1, Stackoverflow 2\nThis is my code:\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.utils.log import configure_logging\n\n# From response in Stackoverflow: https://stackoverflow.com/questions/41495052/scrapy-reactor-not-restartable\nfrom crochet import setup\nsetup()\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n\n    def start_requests(self):\n        urls = [\n            'http://quotes.toscrape.com/page/1/',\n            'http://quotes.toscrape.com/page/2/',\n        ]\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n\n    def parse(self, response):\n        page = response.url.split(\"/\")[-2]\n\n        print ('Scrapped page n', page)\n\n\n    def closed(self, reason):\n        print ('Closed Spider: ', reason)\n\n\ndef run_spider():\n    \n    configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n    \n    crawler = CrawlerRunner(get_project_settings())\n    crawler.crawl(QuotesSpider)        \n\n\nrun_spider()\nand when I execute the script, I returned this log:\nINFO: Overridden settings: {}\n2019-01-28 16:49:52 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats']\n2019-01-28 16:49:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2019-01-28 16:49:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-01-28 16:49:52 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2019-01-28 16:49:52 [scrapy.core.engine] INFO: Spider opened\n2019-01-28 16:49:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-01-28 16:49:52 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\nWhy the crawler not crawl the Spider? I run with Mac and Python 3.7.1.\nMaybe the scrapy or crochet version??", "issue_status": "Closed", "issue_reporting_time": "2019-01-28T15:51:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "214": {"issue_url": "https://github.com/scrapy/scrapy/issues/3601", "issue_id": "#3601", "issue_summary": "Cannot reproduce the documentation example about remove_namespaces()", "issue_description": "Member\nGallaecio commented on Jan 25, 2019 \u2022\nedited\nGiven:\n>>> from urllib.request import urlopen\n>>> with urlopen('https://github.com/blog.atom') as html:\n...     body = html.read()\n... \n>>> from scrapy.http import TextResponse\n>>> response = TextResponse(url='https://github.com/blog.atom',\n...                         body=body, encoding='utf8')\nAccording to the documentation, this is expected:\n>>> response.xpath(\"//link\")\n[]\nHowever, this is what I get:\n>>> response.xpath(\"//link\")\n[<Selector xpath='//link' data='<link rel=\"canonical\" href=\"https://gith'>, <Selector xpath='//link' data='<link rel=\"next\" href=\"https://github.bl'>, <Selector xpath='//link' data='<link rel=\"dns-prefetch\" href=\"//s0.wp.c'>, <Selector xpath='//link' data='<link rel=\"dns-prefetch\" href=\"//s.w.org'>, <Selector xpath='//link' data='<link rel=\"alternate\" type=\"application/'>, <Selector xpath='//link' data='<link rel=\"stylesheet\" id=\"all-css-0\" hr'>, <Selector xpath='//link' data='<link rel=\"https://api.w.org/\" href=\"htt'>, <Selector xpath='//link' data='<link rel=\"EditURI\" type=\"application/rs'>, <Selector xpath='//link' data='<link rel=\"wlwmanifest\" type=\"applicatio'>, <Selector xpath='//link' data='<link rel=\"shortlink\" href=\"https://wp.m'>, <Selector xpath='//link' data='<link rel=\"dns-prefetch\" href=\"//v0.word'>, <Selector xpath='//link' data='<link rel=\"icon\" href=\"https://github.bl'>, <Selector xpath='//link' data='<link rel=\"icon\" href=\"https://github.bl'>, <Selector xpath='//link' data='<link rel=\"apple-touch-icon-precomposed\"'>]\nIs removing namespaces no longer necessary? Is the example URL provided not a good choice to explain this feature?", "issue_status": "Closed", "issue_reporting_time": "2019-01-25T14:52:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "215": {"issue_url": "https://github.com/scrapy/scrapy/issues/3600", "issue_id": "#3600", "issue_summary": "Runtime Error on AWS Lambda with Scrapy - Reuse container issue", "issue_description": "nicoparsa commented on Jan 24, 2019 \u2022\nedited\nI had a problem with AWS Lambda container and Scrapy.\nWhen I execute the code in local with SAM, it never fails but when execute the code in AWS Lambda containers two times in a short period of time, it produce this error:\nSTART RequestId: cbd8f1cf-a9a1-41eb-89e9-bedf5ba1a0f7 Version: $LATEST\n2019-01-24 12:02:01 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n2019-01-24 12:02:01 [scrapy.utils.log] INFO: Versions: lxml 4.3.0.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.2 (default, Jan  4 2019, 13:56:51) - [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1a  20 Nov 2018), cryptography 2.5, Platform Linux-4.14.88-72.76.amzn1.x86_64-x86_64-with-glibc2.2.5\n2019-01-24 12:02:01 [scrapy.crawler] INFO: Overridden settings: {'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': '/tmp/result.json', 'USER_AGENT': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.85 Safari/537.36'}\n2019-01-24 12:02:01 [scrapy.extensions.telnet] INFO: Telnet Password: 570e0d9ceee5e1d4\n2019-01-24 12:02:01 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats']\n2019-01-24 12:02:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2019-01-24 12:02:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-01-24 12:02:01 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2019-01-24 12:02:01 [scrapy.core.engine] INFO: Spider opened\n2019-01-24 12:02:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-01-24 12:02:01 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\nTraceback (most recent call last):\n  File \"/var/runtime/bootstrap\", line 12, in <module>\n    main()\n  File \"/var/runtime/bootstrap.py\", line 349, in main\n    event_request.deadline_time_in_ms)\n  File \"/var/runtime/bootstrap.py\", line 127, in handle_event_request\n    log_error(error_result)\n  File \"/var/runtime/bootstrap.py\", line 96, in log_error\n    error_message_lines = [\"[ERROR] {}: {}\".format(error_result['errorType'], error_result['errorMessage'])]\nKeyError: 'errorMessage'\nEND RequestId: cbd8f1cf-a9a1-41eb-89e9-bedf5ba1a0f7\nREPORT RequestId: cbd8f1cf-a9a1-41eb-89e9-bedf5ba1a0f7  Duration: 2246.92 ms    Billed Duration: 2300 ms    Memory Size: 128 MB Max Memory Used: 60 MB  \nRequestId: cbd8f1cf-a9a1-41eb-89e9-bedf5ba1a0f7 Error: Runtime exited with error: exit status 1\nRuntime.ExitError\nMy Lambda's code is:\nimport os\nimport logging\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nlogging.getLogger().handlers = [] # Necessary to manage the Log Level of Scrapy in AWS Lambda \n\ndef handler(event, context):\n\n    # SPIDER\n\n    settings = {\n            'LOG_ENABLED': True,\n            'LOG_LEVEL': 'ERROR'\n            }\n\n    process = CrawlerProcess(settings=settings)    \n\n    process.crawl(QuotesSpider)\n    process.start()\n\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n\n    def start_requests(self):\n        urls = [\n            'http://quotes.toscrape.com/page/1/',\n            'http://quotes.toscrape.com/page/2/',\n        ]\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n\n    def parse(self, response):\n        page = response.url.split(\"/\")[-2]\n\n        print ('Scrapped page n\u00ba', page)\nIs like Crawler Process is already running in the re-use container and fails (Or another Scrapy Process). Any help?", "issue_status": "Closed", "issue_reporting_time": "2019-01-24T17:24:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "216": {"issue_url": "https://github.com/scrapy/scrapy/issues/3598", "issue_id": "#3598", "issue_summary": "Set cookies from middleware", "issue_description": "kadimon commented on Jan 23, 2019\nHi! I use it in my DOWNLOADER_MIDDLEWARES. And cookies are not updated.\ndef process_response(self, request, response, spider):\n    coookie_dict={'name': 'value'}\n    request.cookies.update(coookie_dict)\n    request.priority = 99999\n    return request\nIf I use it in the spider code, then everything works\ncoookie_dict={'name': 'value'}\nyield Request(url', cookies=coookie_dict)\nHow to set cookies for all spider requests (including those that are in the schedule) from my downloader middleware?", "issue_status": "Closed", "issue_reporting_time": "2019-01-23T09:42:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "217": {"issue_url": "https://github.com/scrapy/scrapy/issues/3597", "issue_id": "#3597", "issue_summary": "may be 'accessible'?", "issue_description": "amchii commented on Jan 23, 2019\nin the function request_fingerprint \uff0c\u2018accesible\u2019 may be \u2018accessible\u2019 in comments. OCD XD..", "issue_status": "Closed", "issue_reporting_time": "2019-01-23T06:50:26Z", "fixed_by": "#4033", "pull_request_summary": "Fix documentation typo: accesible \u2192 accessible", "pull_request_description": "Member\nGallaecio commented on Sep 24, 2019\nFixes #3597\nWhile this change will not be necessary after #3975 is merged, that could take a while, or not happen at all.", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-25T09:13:38Z", "files_changed": [["2", "scrapy/utils/request.py"]]}, "218": {"issue_url": "https://github.com/scrapy/scrapy/issues/3595", "issue_id": "#3595", "issue_summary": "Yielding more requests if scraper was idle more than 20s", "issue_description": "casertap commented on Jan 22, 2019 \u2022\nedited\nHi,\nI would like to yield more requests at the end of a CrawlSpider that uses Rules.\nI noticed I was not able to feed more requests by doing this in the spider_closed method:\nself.crawler.engine.crawl(r, self)\nI noticed that this technic work in spider_idle method but I would like to wait to be sure that the crawl is finished before feeding more requests.\nI set the setting CLOSESPIDER_TIMEOUT = 30\nWhat would be the code to wait 20 seconds idle before triggering the process of feeding more requests?\nIs there a better way?", "issue_status": "Closed", "issue_reporting_time": "2019-01-22T06:08:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "219": {"issue_url": "https://github.com/scrapy/scrapy/issues/3594", "issue_id": "#3594", "issue_summary": "CRITICAL : Unhandled Error", "issue_description": "xyhuang1116 commented on Jan 22, 2019\nIt seems like every time I use telnet to print stats info,it comes the error.\nI looked privious issues\uff0cand I can\u2018t find a solution.\nscrapy -version -v gives me\n2019-01-22 10:45:11,125 : INFO : Scrapy 1.5.1 started (bot: kantest3)\n2019-01-22 10:45:11,131 : INFO : Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3 .6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) - [GCC 7.2.0], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o 27 Mar 2018), cryptography 2.2.2, Pla tform Linux-4.4.0-130-generic-x86_64-with-debian-stretch-sid\nScrapy : 1.5.1\nlxml : 4.2.1.0\nlibxml2 : 2.9.8\ncssselect : 1.0.3\nparsel : 1.5.1\nw3lib : 1.19.0\nTwisted : 18.9.0\nPython : 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) - [GCC 7.2.0]\npyOpenSSL : 18.0.0 (OpenSSL 1.0.2o 27 Mar 2018)\ncryptography : 2.2.2\nPlatform : Linux-4.4.0-130-generic-x86_64-with-debian-stretch-sid\nWhen I connect to telnet,info is:\nubuntu@VM-0-20-ubuntu:/data/xxx/xxx$ telnet 127.0.0.1 6025\nTrying 127.0.0.1...\nConnected to 127.0.0.1.\nEscape character is '^]'.\np(stats.get_stats())\n{'downloader/exception_count': 29,\n'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 21,\n'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 4,\n'downloader/exception_type_count/twisted.internet.error.TimeoutError': 2,\n'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,\nThe full stack trace is :\n2019-01-22 10:05:25,783 : CRITICAL : Unhandled Error\nTraceback (most recent call last):\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/python/log.py\", line 103, in callWithLogger\nreturn callWithContext({\"system\": lp}, func, *args, **kw)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/python/log.py\", line 86, in callWithContext\nreturn context.call({ILogContext: newCtx}, func, *args, **kw)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/python/context.py\", line 122, in callWithContext\nreturn self.currentContext().callWithContext(ctx, func, *args, **kw)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/python/context.py\", line 85, in callWithContext\nreturn func(*args,**kw)\n--- ---\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/internet/posixbase.py\", line 614, in _doReadOrWrite\nwhy = selectable.doRead()\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/internet/tcp.py\", line 243, in doRead\nreturn self._dataReceived(data)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/internet/tcp.py\", line 249, in _dataReceived\nrval = self.protocol.dataReceived(data)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/telnet.py\", line 636, in dataReceived\nself.applicationDataReceived(b''.join(appDataBuffer))\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/telnet.py\", line 988, in applicationDataReceived\nself.protocol.dataReceived(data)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/telnet.py\", line 1035, in dataReceived\nself.protocol.dataReceived(data)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/insults/insults.py\", line 537, in dataReceived\nself.terminalProtocol.keystrokeReceived(ch, None)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/recvline.py\", line 225, in keystrokeReceived\nm()\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/recvline.py\", line 374, in handle_RETURN\nreturn RecvLine.handle_RETURN(self)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/recvline.py\", line 292, in handle_RETURN\nself.lineReceived(line)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/manhole.py\", line 267, in lineReceived\nmore = self.interpreter.push(line)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/manhole.py\", line 106, in push\nmore = self.runsource(source, self.filename)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/code.py\", line 75, in runsource\nself.runcode(code)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/twisted/conch/manhole.py\", line 117, in runcode\ncode.InteractiveInterpreter.runcode(self, *a, **kw)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/code.py\", line 91, in runcode\nexec(code, self.locals)\nFile \"\", line 1, in\nFile \"/home/ubuntu/anaconda3/lib/python3.6/_sitebuiltins.py\", line 26, in call\nraise SystemExit(code)\nbuiltins.SystemExit: None", "issue_status": "Closed", "issue_reporting_time": "2019-01-22T04:15:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "220": {"issue_url": "https://github.com/scrapy/scrapy/issues/3593", "issue_id": "#3593", "issue_summary": "set_crawler not working neither warning deprecated?", "issue_description": "Contributor\nNewUserHa commented on Jan 21, 2019\nclass test(scrapy.Spider):\n    name = 'test'\n\n    start_urls = ['http://httpbin.org']\n\n    def set_crawler(self, crawler):\n        super(test, self).set_crawler(crawler)\n        print(999999999999999)\n\n    def parse(self, response):\n        print('ok')\nconsidering fixing or removing it?", "issue_status": "Closed", "issue_reporting_time": "2019-01-21T15:31:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "221": {"issue_url": "https://github.com/scrapy/scrapy/issues/3589", "issue_id": "#3589", "issue_summary": "Spider download failed", "issue_description": "wangwei1016 commented on Jan 19, 2019\nHi, guys\nI tried to crawl the APP from https://sj.qq.com/myapp/ and download apk files. But the spilder looks like instability when downloading. It's will report \"'SSL routines', 'ssl3_get_record', 'wrong version number'\" sometimes. Attach the detail log. Can you help check this issues? thanks!\nspider.log", "issue_status": "Closed", "issue_reporting_time": "2019-01-19T06:38:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "222": {"issue_url": "https://github.com/scrapy/scrapy/issues/3587", "issue_id": "#3587", "issue_summary": "[Logs] Different behaviour running on AWS Lambda vs Local", "issue_description": "nicoparsa commented on Jan 17, 2019 \u2022\nedited\nI run Scrapy from script https://doc.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script to launch a script from AWS Lambda. I compile the project with SAM and everything is correct.\nBut now, I have the problem with LOG_LEVEL parameter.\nsettings = {\n                'USER_AGENT': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.85 Safari/537.36',\n                'LOG_ENABLED': True,\n                'LOG_LEVEL': 'ERROR'\n             }\n\nprocess = CrawlerProcess(settings=settings)\nWhen execute this code in local, all is correct, only receive the LOG_LEVEL: ERROR, but when execute this code in AWS Lambda, I receive the LOG_LEVEL: DEBUG, and I don\u00b4t know how to resolve.\nIs like the Spider doesn\u00b4t receive the settings of the Crawler. Any help?\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2019-01-17T12:04:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "223": {"issue_url": "https://github.com/scrapy/scrapy/issues/3586", "issue_id": "#3586", "issue_summary": "Request doesn't set callback to self.parse automatically when errback is set", "issue_description": "Contributor\njoaquingx commented on Jan 17, 2019 \u2022\nedited\nHi, I'm facing an AssertionError when set errback and not callback in Request, is that expected? :\n...\n    def start_requests(self):\n        yield Request(\n            self.start_url,\n            # callback=self.parse,\n            errback=self.parse_error,\n            meta={\n                'dont_redirect': True,\n            },\n        )\n...\ntrigger this :\n2019-01-16 15:47:22 [scrapy.core.engine] ERROR: Error while obtaining start requests\nTraceback (most recent call last):\n  File \"/home/joaquin/Repos/example/env/lib/python3.6/site-packages/scrapy/core/engine.py\", line 127, in _next_request\n    request = next(slot.start_requests)\n  File \"/home/joaquin/Repos/example/example/spiders/br/rj/example.py\", line 128, in start_requests\n    'dont_redirect': True,\n  File \"/home/joaquin/Repos/example/env/lib/python3.6/site-packages/scrapy/http/request/__init__.py\", line 34, in __init__\n    assert callback or not errback, \"Cannot use errback without a callback\"\nAssertionError: Cannot use errback without a callback\n2019-01-16 15:47:22 [scrapy.core.engine] INFO: Closing spider (finished)\n2019-01-16 15:47:22 [scrapy.core.engine] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2019-01-16T20:57:58Z", "fixed_by": "#4008", "pull_request_summary": "Request: remove restriction about errback without callback", "pull_request_description": "Member\nelacuesta commented on Sep 12, 2019 \u2022\nedited\nFixes #3586", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-23T18:12:44Z", "files_changed": [["1", "scrapy/http/request/__init__.py"], ["46", "tests/test_http_request.py"]]}, "224": {"issue_url": "https://github.com/scrapy/scrapy/issues/3584", "issue_id": "#3584", "issue_summary": "lambda packaging has two error", "issue_description": "up0617 commented on Jan 16, 2019\nHi,\nI tried to run my spider on aws lambda.\nThen I need to make a zip file with whole scrapy lib packaged.\nSo I installed scrapy at project folder via below command:\npip install scrapy --target ~/myprojectfolder/package\nand import lib in script file(MySpider.py) via\nfrom package.scrapy.http import Request\nthen I tried to run scrapy via below command:\npython MySpider.py\nat first it gives twisted import error\nthen I add below code to /project/scrapy/init.py, above importing _monkeypatches\nsys.path.append('./package/')\nso the prob is resolved, but I encountered another one.\nIt stops after below message.\n2019-01-16 13:48:06 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023", "issue_status": "Closed", "issue_reporting_time": "2019-01-16T05:23:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "225": {"issue_url": "https://github.com/scrapy/scrapy/issues/3581", "issue_id": "#3581", "issue_summary": "Redirect doesn't expose original status code", "issue_description": "mohmad-null commented on Jan 14, 2019\nI'm being redirected with a 307 - the log says so. I'd like to handle this differently to a 302. However the redirect middleware doesn't seem to keep the original status code as far as I can see, not even in meta. It would be nice if there was a list of redirect status codes to go with the list of redirect_urls.", "issue_status": "Closed", "issue_reporting_time": "2019-01-13T22:23:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "226": {"issue_url": "https://github.com/scrapy/scrapy/issues/3577", "issue_id": "#3577", "issue_summary": "AttributeError: 'HtmlResponse' object has no attribute 'folllow'", "issue_description": "MarcWarrior commented on Jan 12, 2019\nI got the error when I run a spider with command 'scrapy crawl spider'\nThis is the trace:\n2019-01-12 22:22:11 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: )\n2019-01-12 22:22:11 [scrapy.utils.log] INFO: Versions: lxml 4.3.0.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 14:57:15) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0j 20 Nov 2018), cryptography 2.4.2, Platform Windows-8.1-6.3.9600-SP0\n2019-01-12 22:22:11 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': '', 'CONCURRENT_REQUESTS': 2, 'CONCURRENT_REQUESTS_PER_DOMAIN': 2, 'CONCURRENT_REQUESTS_PER_IP': 2, 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'HTTPCACHE_ENABLED': True, 'HTTPCACHE_EXPIRATION_SECS': 3600, 'HTTPCACHE_GZIP': True, 'HTTPCACHE_IGNORE_HTTP_CODES': [301, 302, 303, 401, 403, 404, 500, 502, 503, 504], 'NEWSPIDER_MODULE': '.spiders', 'SPIDER_MODULES': ['.spiders']}\n2019-01-12 22:22:11 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.logstats.LogStats']\n2019-01-12 22:22:11 [py.warnings] WARNING: ...\\venv\\lib\\site-packages\\scrapy\\utils\\deprecate.py:156: ScrapyDeprecationWarning: scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware class is deprecated, use scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware instead\nScrapyDeprecationWarning)\n2019-01-12 22:22:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['.middlewares.RandomUserAgentMiddleware',\n'.middlewares.ProxyMiddleware',\n'.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats',\n'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware']\n2019-01-12 22:22:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2019-01-12 22:22:13 [scrapy.middleware] INFO: Enabled item pipelines:\n['.pipelines.****Pipeline']\n2019-01-12 22:22:13 [scrapy.core.engine] INFO: Spider opened\n2019-01-12 22:22:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-01-12 22:22:13 [scrapy.extensions.httpcache] DEBUG: Using filesystem cache storage in ....scrapy\\httpcache\n2019-01-12 22:22:13 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2019-01-12 22:22:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET ****> (referer: None) ['cached']\n2019-01-12 22:22:14 [scrapy.core.scraper] ERROR: Spider error processing <GET ****> (referer: None)\nTraceback (most recent call last):\nFile \"...\\Python37\\lib\\site-packages\\twisted\\internet\\defer.py\", line 654, in _runCallbacks\ncurrent.result = callback(current.result, *args, **kw)\nFile \"...\\spiders\\novel.py\", line 21, in parse\nnovel_item['introduction'] = response.folllow(\nAttributeError: 'HtmlResponse' object has no attribute 'folllow'\nAs you see,I had update the scrapy to ver.1.5.1.But I still got the error,I had no idea.", "issue_status": "Closed", "issue_reporting_time": "2019-01-12T15:17:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "227": {"issue_url": "https://github.com/scrapy/scrapy/issues/3574", "issue_id": "#3574", "issue_summary": "ItemLoader adds wrong value instead of an empty array", "issue_description": "hibpm commented on Jan 10, 2019 \u2022\nedited\nPlease take a look at this code:\nimport scrapy\nfrom scrapy.loader import ItemLoader\n\n\nclass MyItem(scrapy.Item):\n    a = scrapy.Field()\n\n\nclass MyLoader(ItemLoader):\n    item = MyItem()\n\n\nloader = MyLoader()\nloader.add_value(\"a\", \"b\")\nitem = loader.load_item()\nprint(item.get(\"a\"))\n\nloader = MyLoader()\nloader.add_value(\"a\", [])\nitem = loader.load_item()\nprint(item.get(\"a\"))\nThe output should be something like\n['b']\nNone\nBut it actually is\n['b']\n['b']", "issue_status": "Closed", "issue_reporting_time": "2019-01-10T13:56:39Z", "fixed_by": "#4099", "pull_request_summary": "update docs of scrapy.loader.ItemLoader.item", "pull_request_description": "Contributor\nBurnzZ commented on Oct 23, 2019 \u2022\nedited by Gallaecio\nAttempts to clear up the doc misunderstanding in #3574 that results in wrong expected behavior due to incorrect usage.\nFixes #3574", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-03T08:14:46Z", "files_changed": [["2", "docs/topics/loaders.rst"]]}, "228": {"issue_url": "https://github.com/scrapy/scrapy/issues/3572", "issue_id": "#3572", "issue_summary": "Asynchronous requests in parse()", "issue_description": "HelloEdit commented on Jan 10, 2019\nHi,\nI wanted to know if Scrapy had an internal mechanism, using Twisted or other, to make request without callback like this:\nimport scrapy\nfrom scrapy import spiders, FormRequest\n\n\nclass PagesSpider(spiders.SitemapSpider):\n    name = 'pages'\n\n\n    sitemap_urls = ['http://site.com/sitemap.xml']\n\n\n    def parse(self, response):\n        id = response.css('span.id::text').extract_first()\n\n        # here i would like to do something like this in JS\n        # let [content, comments] = Promise.all([\n        #         request('http://raw.mysite.com/{}'.format(id)),\n        #         request('http://comments.mysite.com/{}'.format(id))\n        # ])\n        # Which is the equivalent of making several requests in parallel, waiting for it to be\n        # executed and recovering the body of each one (Promise.all is like asyncio.gather I think)\n\n        yield { 'id': id, 'content': content, 'comments': comments }\nI don't know if I was clear, but for me, scrapy cruelly lacks the possibility of being able to make requests without callback directly in a function, in parallel if necessary and non-blocking.\nThanks,", "issue_status": "Closed", "issue_reporting_time": "2019-01-09T18:36:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "229": {"issue_url": "https://github.com/scrapy/scrapy/issues/3571", "issue_id": "#3571", "issue_summary": "Applying for Gsoc 2019", "issue_description": "Octopus67 commented on Jan 8, 2019\nHello Mentors and Developers,\nI am Manav Mehta, a student at BITS Pilani, Goa Campus. I went through the Scrapy GSoC ideas page and found all the projects to be very interesting.\nI would love to contribute to Scrapy and participate in GSoC 2019.\nI do have some open source experience and I would like to try my hand by implementing a small new feature and/or resolving bugs.\nI will be extremely thankful if someone could guide me regarding the same.\nThanking you.\nRegards,\nManav Mehta.\n\ud83c\udf89 2", "issue_status": "Closed", "issue_reporting_time": "2019-01-08T16:50:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "230": {"issue_url": "https://github.com/scrapy/scrapy/issues/3570", "issue_id": "#3570", "issue_summary": "POST request works with REQUESTS module but not with FormRequest", "issue_description": "mani619cash commented on Jan 8, 2019 \u2022\nedited\nI have this POST request\nimport requests\n\ncookies = {\n '__cfduid': 'dbec85a39da4864353e2d58cb92b87c851546939705',\n 'JSESSIONID': 'D27B45546C5E8C3BA8E5707CEDFCB439.bea-prd-dal-app-01-p-app2',\n '_ga': 'GA1.2.1063259619.1546939723',\n '_gid': 'GA1.2.1467992224.1546939723',\n '_gat_UA-7616758-7': '1',\n}\n\nheaders = {\n 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0',\n 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n 'Accept-Language': 'en-US,en;q=0.5',\n 'Referer': 'https://www.shopklx.com/index.jsp;jsessionid=D27B45546C5E8C3BA8E5707CEDFCB439.bea-prd-dal-app-01-p-app2?_DARGS=/index.jsp.lineSForm',\n 'Content-Type': 'application/x-www-form-urlencoded',\n 'Connection': 'keep-alive',\n 'Upgrade-Insecure-Requests': '1',\n 'TE': 'Trailers',\n}\n\nparams = (\n ('_DARGS', '/index.jsp.lineSForm'),\n)\n\ndata = {\n  '_dyncharset': 'UTF-8',\n  '_dynSessConf': '-3041302679957646628',\n  '/com/klx/profile/SessionBean.advancedSearchTerm': 'asc',\n  '_D:/com/klx/profile/SessionBean.advancedSearchTerm': ' ',\n  '/com/klx/search/MultiPartSearchFormHandler.activeTab': 'line',\n  '_D:/com/klx/search/MultiPartSearchFormHandler.activeTab': ' ',\n  '/com/klx/search/MultiPartSearchFormHandler.lazySearch': 'false',\n  '_D:/com/klx/search/MultiPartSearchFormHandler.lazySearch': ' ',\n  '/com/klx/search/MultiPartSearchFormHandler.lineSearch': 'Search',\n  '_D:/com/klx/search/MultiPartSearchFormHandler.lineSearch': ' ',\n  '_DARGS': '/index.jsp.lineSForm'\n}\n\nresponse = requests.post('https://www.shopklx.com/index.jsp?_DARGS=/.lineSForm', headers=headers, cookies=cookies, data=data)\nprint(response.status_code)\nIt works perfectly\nWhereas I have this same code in Scrapy and it doesnt work at all, it gives me 429 status\nclass KlxSpider(scrapy.Spider):\n name = 'klx'\n\n already_scraped = None\n\n cookies = {\n  '__cfduid': 'dbec85a39da4864353e2d58cb92b87c851546939705',\n  'JSESSIONID': 'D27B45546C5E8C3BA8E5707CEDFCB439.bea-prd-dal-app-01-p-app2',\n  '_ga': 'GA1.2.1063259619.1546939723',\n  '_gid': 'GA1.2.1467992224.1546939723',\n  '_gat_UA-7616758-7': '1',\n }\n\n headers = {\n  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0',\n  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n  'Accept-Language': 'en-US,en;q=0.5',\n  'Referer': 'https://www.shopklx.com/index.jsp;jsessionid=D27B45546C5E8C3BA8E5707CEDFCB439.bea-prd-dal-app-01-p-app2?_DARGS=/index.jsp.lineSForm',\n  'Content-Type': 'application/x-www-form-urlencoded',\n  'Connection': 'keep-alive',\n  'Upgrade-Insecure-Requests': '1',\n  'TE': 'Trailers',\n }\n\n\n def start_requests(self):\n\n  formdata = {\n    '_dyncharset': 'UTF-8',\n    '_dynSessConf': '6786910114521215946',\n    '/com/klx/profile/SessionBean.advancedSearchTerm': 'abc',\n    '_D:/com/klx/profile/SessionBean.advancedSearchTerm': ' ',\n    '/com/klx/search/MultiPartSearchFormHandler.activeTab': 'line',\n    '_D:/com/klx/search/MultiPartSearchFormHandler.activeTab': ' ',\n    '/com/klx/search/MultiPartSearchFormHandler.lazySearch': 'false',\n    '_D:/com/klx/search/MultiPartSearchFormHandler.lazySearch': ' ',\n    '/com/klx/search/MultiPartSearchFormHandler.lineSearch': 'Search',\n    '_D:/com/klx/search/MultiPartSearchFormHandler.lineSearch': ' ',\n    '_DARGS': '/.lineSForm'\n  }\n\n  yield FormRequest('https://www.shopklx.com/index.jsp?_DARGS=/.lineSForm', \n   callback=self.parse_results, \n   headers=self.headers,\n   formdata=formdata,\n   method='post',\n   cookies=self.cookies, \n   )\n\n\n def parse_results(self, response):\n  logging.info(response.text)", "issue_status": "Closed", "issue_reporting_time": "2019-01-08T09:53:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "231": {"issue_url": "https://github.com/scrapy/scrapy/issues/3569", "issue_id": "#3569", "issue_summary": "Get proxy response in middleware", "issue_description": "t75bernd commented on Jan 7, 2019 \u2022\nedited\nHello,\ni do a request to a site with https and also use a proxy. When defining a middleware and using process_response in it response.headers does only have the headers from the website. Is there any way to get the headers from the CONNECT request the proxy tunnel establish? The proxy we are using is adding some informations as headers in this response, we want to use it in the middleware.\nI found out that in TunnelingTCP4ClientEndpoint.processProxyResponse the parameter rcvd_bytes has all infos i need.\nThanks in advance for your help.", "issue_status": "Closed", "issue_reporting_time": "2019-01-07T15:18:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "232": {"issue_url": "https://github.com/scrapy/scrapy/issues/3564", "issue_id": "#3564", "issue_summary": "NameError: name 'sanitize' is not defined", "issue_description": "Bassi1997 commented on Jan 4, 2019 \u2022\nedited\nI am trying to sort some data which are not uniformly formated , so i am trying to apply sanitize() but it won't execute and alerts me with the message: Traceback (most recent call last):\nFile \"C:\\Users\\Ayman\\AppData\\Local\\Programs\\Python\\Builds\\HeadFirstPython\\Chapter5\\coach.py\", line 20, in\nclean_james.append(sanitize(each_t))\nNameError: name 'sanitize' is not defined\nI am using python37\nso i found that this function was removed from this version library ,, so what is the alternative for this procedure in this version ?\nthis is the code structure :\nimport os\nos.chdir('C:\\Users\\Ayman\\AppData\\Local\\Programs\\Python\\Builds\\HeadFirstPython\\Chapter5')\nwith open ('james.txt') as jaf:\ndata = jaf.readline()\njames = data.strip().split(',')\nwith open ('julie.txt') as juf:\ndata = juf.readline()\njulie = data.strip().split(',')\nwith open ('mikey.txt') as mif:\ndata = mif.readline()\nmikey = data.strip().split(',')\nwith open ('sarah.txt') as saf:\ndata = saf.readline()\nsarah = data.strip().split(',')\nclean_james = []\nclean_jolie = []\nclean_mikey = []\nclean_sarah = []\nfor each_t in james:\nclean_james.append(sanitize(each_t))\nfor each_t in jolie:\nclean_jolie.append(sanitize(each_t))\nfor each_t in mikey:\nclean_mikey.append(sanitize(each_t))\nfor each_t in sarah:\nclean_sarah.append(sanitize(each_t))\nprint(sorted(clean_james))\nprint(sorted(clean_julie))\nprint(sorted(clean_mikey))\nprint(sorted(clean_sarah))\nTraceback (most recent call last):\nFile \"<pyshell#5>\", line 1, in\nclean = [sanitize(t) for t in dirty]\nFile \"<pyshell#5>\", line 1, in\nclean = [sanitize(t) for t in dirty]\nNameError: name 'sanitize' is not defined", "issue_status": "Closed", "issue_reporting_time": "2019-01-03T20:58:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "233": {"issue_url": "https://github.com/scrapy/scrapy/issues/3562", "issue_id": "#3562", "issue_summary": "is a \"button\" a valid \"clickable element\"?", "issue_description": "mfcd commented on Jan 3, 2019\nI have a page with a structure as follow:\n<div>\n    <div>\n        <select ..>\n            <option value=\"1\"></option>\n        </select>\n    </div>\n    <div>\n        <button attrib=\"myAttrib\">\n            <span>My Button</span>\n        </button>\n    </div>\n</div>\nI tried to hit the page with:\n        FormRequest.from_response(\n            sr,\n            formdata={\n                \"value_one\": 2,\n                \"foo\": \"2019-01-03T20:00:00\"\n            },\n            clickdata={\"attrib\": \"myAttrib\"},\n            callback=\"self.parse_availability\"\n        )\nBut I get\nValueError: No clickable element matching clickdata\nwhile I find the relative element with xpath.", "issue_status": "Closed", "issue_reporting_time": "2019-01-03T15:11:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "234": {"issue_url": "https://github.com/scrapy/scrapy/issues/3561", "issue_id": "#3561", "issue_summary": "Scrapy stucked in 400 error when scraping https://www.watsons.com.sg/", "issue_description": "pc2000sg commented on Jan 2, 2019 \u2022\nedited\nI am trying to scrape this webpage: \"https://www.watsons.com.sg/\" but scrapy always result in 400 error. Attached the scrapy run log as reference here:\n['diffmarts.pipelines.DiffmartsPipeline']\n2019-01-01 21:17:14 [scrapy.core.engine] INFO: Spider opened\n2019-01-01 21:17:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-01-01 21:17:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <400 https://queue.watsons.com.sg/?c=aswatson&e=watsonprdsg&ver=v3-java-3.5.2&cver=62&cid=zh-CN&l=PoC+Layout+SG&t=https%3A%2F%2Fwww.watsons.com.sg%2F>: HTTP status code is not handled or not allowed\n2019-01-01 21:17:15 [scrapy.core.engine] INFO: Closing spider (finished)\n2019-01-01 21:17:17 [scrapy.core.engine] ERROR: Scraper close failure\nI have tried inspect source in chrome and post and the content can be retrieved back by simple \"get\". So there should be no protection on the web site. Wonder what is wrong. Appreciate if anyone can offer suggestions or helps. Thanks.", "issue_status": "Closed", "issue_reporting_time": "2019-01-02T15:04:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "235": {"issue_url": "https://github.com/scrapy/scrapy/issues/3560", "issue_id": "#3560", "issue_summary": "scrapy getting stuck after some time", "issue_description": "suraj-deshmukh commented on Jan 2, 2019\nI have a master-worker network on aws ec2 using dask distributed library. For now i have one master machine and one worker machine. Master has REST api (flask) for scheduling scrapy jobs on worker machine. I am using docker for both master and worker that means both master container and worker container communicating with each other using dask distributed.\nWhen i scheduler scrapy job, crawling starts successfully and scrapy uploads data to s3 as well. But after some time scrapy gets stuck at one point and nothing happens after that.\nPlease check attached log file for more info\nlog.txt\n2019-01-02 08:05:30 [botocore.hooks] DEBUG: Event needs-retry.s3.PutObject: calling handler <bound method S3RegionRedirector.redirect_from_error of <botocore.utils.S3RegionRedirector object at 0x7f1fe54adf28>>\nscrapy get stuck at above point.\ncommand to run docker:\nsudo docker run --network host -d crawler-worker # for worker\nsudo docker run -p 80:80 -p 8786:8786 -p 8787:8787 --net=host -d crawler-master # for master\nNOTE:\nWhen i open docker image in interactive mode and execute scrapy crawl command then scrapy successfully crawls the given site and stops execution successfully after depth limit.", "issue_status": "Closed", "issue_reporting_time": "2019-01-02T10:18:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "236": {"issue_url": "https://github.com/scrapy/scrapy/issues/3556", "issue_id": "#3556", "issue_summary": "How can i fix this\uff1f", "issue_description": "lovezeropython commented on Dec 29, 2018\nSyntaxError: invalid syntax\n\u279c  Desktop python3 -V\nPython 3.6.5 :: Anaconda, Inc.\n\u279c  Desktop pip3 -V\npip 18.0 from /Users/songhao/anaconda3/lib/python3.6/site-packages/pip (python 3.6)\n\u279c  Desktop\n`\u279c Desktop scrapy startproject scrapyuniversal\nTraceback (most recent call last):\nFile \"/Users/songhao/anaconda3/bin/scrapy\", line 7, in\nfrom scrapy.cmdline import execute\nFile \"/Users/songhao/anaconda3/lib/python3.6/site-packages/scrapy/init.py\", line 34, in\nfrom scrapy.spiders import Spider\nFile \"/Users/songhao/anaconda3/lib/python3.6/site-packages/scrapy/spiders/init.py\", line 10, in\nfrom scrapy.http import Request\nFile \"/Users/songhao/anaconda3/lib/python3.6/site-packages/scrapy/http/init.py\", line 10, in\nfrom scrapy.http.request import Request\nFile \"/Users/songhao/anaconda3/lib/python3.6/site-packages/scrapy/http/request/init.py\", line 17\nscrclass Request(object_ref):\n^\nSyntaxError: invalid syntax\n`", "issue_status": "Closed", "issue_reporting_time": "2018-12-29T03:21:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "237": {"issue_url": "https://github.com/scrapy/scrapy/issues/3546", "issue_id": "#3546", "issue_summary": "why shutting down", "issue_description": "saseo90 commented on Dec 26, 2018\n[log]\n2018-12-26 13:12:50 [scrapy.crawler] 258:INFO: Received SIGTERM, shutting down gracefully. Send again to force", "issue_status": "Closed", "issue_reporting_time": "2018-12-26T05:31:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "238": {"issue_url": "https://github.com/scrapy/scrapy/issues/3545", "issue_id": "#3545", "issue_summary": "sleep", "issue_description": "Hecate2 commented on Dec 25, 2018 \u2022\nedited\nWhen I execute time.sleep in a parse of scrapy, does it block other concurrent requests? And what if I execute Request (from urllib)in a parse? On what occasions are all the concurrent requests blocked?\n(Sorry for my lousy English!)", "issue_status": "Closed", "issue_reporting_time": "2018-12-25T13:26:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "239": {"issue_url": "https://github.com/scrapy/scrapy/issues/3541", "issue_id": "#3541", "issue_summary": "can not install pyndri", "issue_description": "ZahraGithub commented on Dec 21, 2018\nHi,\nI try to install pyndri using command: pip install pyndri but I received this error:\nerror: command 'x86_64-linux-gnu-gcc' failed with exit status 1\nWhat should I do?\nRegards", "issue_status": "Closed", "issue_reporting_time": "2018-12-21T14:58:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "240": {"issue_url": "https://github.com/scrapy/scrapy/issues/3540", "issue_id": "#3540", "issue_summary": "Request URL changed during redirect result in 400 error", "issue_description": "pc2000sg commented on Dec 21, 2018\nCertain website URL will not be correct during redirect in spider request resulting in 400 error. This is regardless of the user agent (chrome, firefox, etc) selected in setting file.\nFor example: when start url = https://www.watsons.com.sg/\nScrapy path becomes: https://queue.watsons.com.sg?c=aswatson&e=watsonprdsg&ver=v3-java-3.5.2&cver=55&cid=zh-CN&l=PoC+Layout+SG&t=https%3A%2F%2Fwww.watsons.com.sg%2F\nWebbrowser path should be: https://queue.watsons.com.sg/?c=aswatson&e=watsonprdsg&ver=v3-java-3.5.2&cver=55&cid=zh-CN&l=PoC+Layout+SG&t=https%3A%2F%2Fwww.watsons.com.sg%2F\nTried workaround using download middleware but not working. Any way to solve this in scrapy level? Such that the request path will be the same as web browser behavior? Thanks.", "issue_status": "Closed", "issue_reporting_time": "2018-12-21T01:55:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "241": {"issue_url": "https://github.com/scrapy/scrapy/issues/3539", "issue_id": "#3539", "issue_summary": "Release?", "issue_description": "rasendubi commented on Dec 21, 2018\nI would like to request a new Scrapy release. The feature I am interested in is Python 3.7 support. (NixOS has switched default python3 to be python 3.7 and that broke default scrapy.)\nThe feature was implemented in #3326 but hasn't been released yet.", "issue_status": "Closed", "issue_reporting_time": "2018-12-21T00:33:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "242": {"issue_url": "https://github.com/scrapy/scrapy/issues/3535", "issue_id": "#3535", "issue_summary": "scrapy crawl quotes in the docs gives a traceback still I cd to the desired directory", "issue_description": "Naba7 commented on Dec 14, 2018\nTraceback (most recent call last):\nFile \"/home/chad7/ENV/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/home/chad7/ENV/lib/python3.6/site-packages/scrapy/cmdline.py\", line 149, in execute\ncmd.crawler_process = CrawlerProcess(settings)\nFile \"/home/chad7/ENV/lib/python3.6/site-packages/scrapy/crawler.py\", line 249, in init\nsuper(CrawlerProcess, self).init(settings)\nFile \"/home/chad7/ENV/lib/python3.6/site-packages/scrapy/crawler.py\", line 137, in init\nself.spider_loader = _get_spider_loader(settings)\nFile \"/home/chad7/ENV/lib/python3.6/site-packages/scrapy/crawler.py\", line 336, in _get_spider_loader\nreturn loader_cls.from_settings(settings.frozencopy())\nFile \"/home/chad7/ENV/lib/python3.6/site-packages/scrapy/spiderloader.py\", line 61, in from_settings\nreturn cls(settings)\nFile \"/home/chad7/ENV/lib/python3.6/site-packages/scrapy/spiderloader.py\", line 25, in init\nself._load_all_spiders()\nFile \"/home/chad7/ENV/lib/python3.6/site-packages/scrapy/spiderloader.py\", line 47, in _load_all_spiders\nfor module in walk_modules(name):\nFile \"/home/chad7/ENV/lib/python3.6/site-packages/scrapy/utils/misc.py\", line 71, in walk_modules\nsubmod = import_module(fullpath)\nFile \"/home/chad7/ENV/lib/python3.6/importlib/init.py\", line 126, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 994, in _gcd_import\nFile \"\", line 971, in _find_and_load\nFile \"\", line 955, in _find_and_load_unlocked\nFile \"\", line 665, in _load_unlocked\nFile \"\", line 674, in exec_module\nFile \"\", line 781, in get_code\nFile \"\", line 741, in source_to_code\nFile \"\", line 219, in _call_with_frames_removed\nFile \"/home/chad7/tutorial/tutorial/spiders/quote_spider.py\", line 18\n~\n^\nSyntaxError: invalid syntax", "issue_status": "Closed", "issue_reporting_time": "2018-12-14T12:41:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "243": {"issue_url": "https://github.com/scrapy/scrapy/issues/3534", "issue_id": "#3534", "issue_summary": "How can I remove referer in scrapy? I can set it to none, but cannot remove it", "issue_description": "1337189261 commented on Dec 14, 2018\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2018-12-14T09:00:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "244": {"issue_url": "https://github.com/scrapy/scrapy/issues/3531", "issue_id": "#3531", "issue_summary": "The documentation builds at Read the Docs are broken", "issue_description": "Member\nGallaecio commented on Dec 13, 2018\nSee https://readthedocs.org/projects/scrapy/builds/\nThe error message is:\nerror: The 'pyasn1' distribution was not found and is required by service-identity\nIt looks like a long-standing issue. Also, there were a couple of builds without the issue.", "issue_status": "Closed", "issue_reporting_time": "2018-12-13T08:03:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "245": {"issue_url": "https://github.com/scrapy/scrapy/issues/3530", "issue_id": "#3530", "issue_summary": "I get invalid syntax while I install scrapy 1.5 using pip", "issue_description": "Naba7 commented on Dec 13, 2018\nscrapy startproject stack\nFile \"\", line 1\nscrapy startproject stack\n^\nSyntaxError: invalid syntax", "issue_status": "Closed", "issue_reporting_time": "2018-12-13T05:55:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "246": {"issue_url": "https://github.com/scrapy/scrapy/issues/3528", "issue_id": "#3528", "issue_summary": "Use twisted default resolver instead of ThreadedResolver to support IPv6", "issue_description": "heroesm commented on Dec 11, 2018\ntwisted.internet.base.ThreadedResolver currently (18.9.1.dev0) uses socket.gethostbyname to do DNS resolution, which does not support IPv6.\nMeanwhile, scrapy currently (1.5.0) uses ThreadedResolver to do DNS resolution, thus unable to crawl host name with IPv6 address only or pure IPv6 ip address.\nhttps://github.com/scrapy/scrapy/blob/1fd1702a11a56ecbe9851ba4f9d3c10797e262dd/scrapy/resolver.py\nScript to reproduce the scrapy exception:\nif [ ! -d testvenv ]; then virtualenv -p python testvenv ; fi\n. testvenv/bin/activate\npip install -q scrapy  # or install twisted and scrapy from source repository\n\npython -m scrapy shell http://ipv6.google.com\n# raises:\n# twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ipv6.google.com.\n\npython -m scrapy shell http://[::1]  # no need for existing web server in [::1]:80\n# raises:\n# twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ::1.\nBut the default resolver in recent twisted supports IPv6 well.\nDelete reactor.installResolver(self._get_dns_resolver()) in\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/crawler.py#L289\nand test again:\npython -m scrapy shell http://ipv6.google.com\npython -m scrapy shell http://[::1]\nit works and no DNSLookupError exception raised.\nAlthough CachingThreadedResolver is used to cache DNS query, I think that compatibility with IPv6 outweigh the requirement of in process DNS cache, or we can reimplement DNS cache using middleware. At least, how about supply a option for user to explicitly enable IPv6 support in case the DNSCACHE_ENABLED = False?\nRegarding the timeout option in getHostByName, I think there should be other approaches to set DNS query timeout (I'm not familiar with twisted).\nIn deed, I tried creating PR to twisted to make ThreadedResolver.getHostByName use getaddrinfo to support IPv6, but after discussing with cdunklau in irc://freenode/twisted-dev , I realised it's a backward incompatible change and is unlikely to be accepted, then concluded that the best option is to use the the default IPv6 compatible resolver in twisted instead of antiquated ThreadedResolver.getHostByName.\n(note: to use the new default resolver which is IPv6 compatible, the required Twisted version should also be updated)", "issue_status": "Closed", "issue_reporting_time": "2018-12-11T15:02:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "247": {"issue_url": "https://github.com/scrapy/scrapy/issues/3525", "issue_id": "#3525", "issue_summary": "sorry newbie mistake, please ignore", "issue_description": "loschky commented on Dec 10, 2018\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2018-12-10T01:47:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "248": {"issue_url": "https://github.com/scrapy/scrapy/issues/3524", "issue_id": "#3524", "issue_summary": "Two consecutive yields, only the first work", "issue_description": "Sshuichi commented on Dec 8, 2018\nI have this piece of code that only executes the first yield's callback and not the next one. I have tried reordering them and it gives the same result:\nOnly the first yield callback gets executed.\n        for j in range(totalOrderPages):  # the code gets in the loop\n            productURI = feedUrl % (productId, j + 1)\n            print \"Got in the loop\" # this gets printed \n            yield response.follow(productURI, self.parse_orders, meta={'pid': productId, 'categories': categories})\n        yield response.follow(first_page, self.parse_product, meta={'pid': productId, 'categories': categories})\nIs there anything in Python or scrapy that prevents 2 consecutive yields?\nSecond question:\nI'm trying to debug this using pdb.set_trace() but when I try to execute yield from the debugging console, it give the yield outside function error.\nDoes anyone know how can we debug yields?\nThank you.", "issue_status": "Closed", "issue_reporting_time": "2018-12-08T13:58:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "249": {"issue_url": "https://github.com/scrapy/scrapy/issues/3523", "issue_id": "#3523", "issue_summary": "During handling of the above exception, another exception occurred:", "issue_description": "NileshBharti2 commented on Dec 6, 2018\nimport csv\nimport scrapy\nfrom scrapy.http import Request\nfrom scrapy.loader import ItemLoader\nfrom scrapy.item import Item, Field\nclass FromcsvSpider(scrapy.Spider):\nname = 'from'\ndef parse(self, response):\n with open(\"todo.csv\",\"rU\") as f:\n  reader=csv.DictReader(f)\n  for line in reader:\n   request = Request(line.pop('url'))\n   request.meta['fields']= line\n   yield request\n   \ndef parse(self, response):\n item=Item()\n l=ItemLoader(item=item,respone=response)\n for name,xpath in response.meta['fields'].iteritems():\n  if xpath : item,fields[name] = Field()\n  l.add_xpath(name, xpath)\n  return l.load_item()      ", "issue_status": "Closed", "issue_reporting_time": "2018-12-06T04:58:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "250": {"issue_url": "https://github.com/scrapy/scrapy/issues/3516", "issue_id": "#3516", "issue_summary": "should raise `AttributeError: Response.meta not available` when access response.meta in `process_resonse`?", "issue_description": "Contributor\nNewUserHa commented on Dec 3, 2018 \u2022\nedited\nshould raise AttributeError: Response.meta not available when access response.meta in process_resonse?\n2018-12-03 23:12:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://httpbin.org/status/200>\nTraceback (most recent call last):\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1418, in _inlineCallbacks\n    result = g.send(result)\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1362, in returnValue\n    raise _DefGen_Return(val)\ntwisted.internet.defer._DefGen_Return: <200 https://httpbin.org/status/200>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\scrapy\\http\\response\\__init__.py\", line 30, in meta\n    return self.request.meta\nAttributeError: 'NoneType' object has no attribute 'meta'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1418, in _inlineCallbacks\n    result = g.send(result)\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 53, in process_response\n    spider=spider)\n  File \"C:\\Users\\user\\Desktop\\try_scrapy_Snippet.py\", line 11, in process_response\n    print('in response:', response.url, response.meta)\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\scrapy\\http\\response\\__init__.py\", line 33, in meta\n    \"Response.meta not available, this response \"\nAttributeError: Response.meta not available, this response is not tied to any request\nIs this right behavior? process_response can't access response.meta???", "issue_status": "Closed", "issue_reporting_time": "2018-12-03T15:20:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "251": {"issue_url": "https://github.com/scrapy/scrapy/issues/3515", "issue_id": "#3515", "issue_summary": "'", "issue_description": "ciweguwe commented on Dec 3, 2018 \u2022\nedited\n'", "issue_status": "Closed", "issue_reporting_time": "2018-12-03T11:55:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "252": {"issue_url": "https://github.com/scrapy/scrapy/issues/3514", "issue_id": "#3514", "issue_summary": "Scrapy to send a request using the specified network card python 3", "issue_description": "itsmnthn commented on Dec 3, 2018\nI have created one scrapy project it is working well, I wanted it to host on the server to run it daily and it is working, But my server has two Network Card one is specially added for scrapy, still project is working but I wanted to use only one Network Card for scrapy or python and that I can specify that this Network card Python or Scrapy can use.\nServer: Windows 10\nPython: 3.6\nScrapy: 1.5\nI was looking for the solution and found this Python sends an HTTP request using the specified network card on the internet but actually, I did not understand how it can be used.\nPlease help me to solve this solution may be like assign Network Card to python or assign Network card to socket or core library that scrapy used to request the website.", "issue_status": "Closed", "issue_reporting_time": "2018-12-03T10:40:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "253": {"issue_url": "https://github.com/scrapy/scrapy/issues/3513", "issue_id": "#3513", "issue_summary": "module 'scrapy' has no attribute 'Spider'", "issue_description": "bcbcbcbcbcl commented on Dec 1, 2018\nTry to run sample script in documentation but got this error: module 'scrapy' has no attribute 'Spider'\nimport scrapy\n\nclass BlogSpider(scrapy.Spider):\n    name = 'blogspider'\n    start_urls = ['https://blog.scrapinghub.com']\n\n    def parse(self, response):\n        for title in response.css('.post-header>h2'):\n            yield {'title': title.css('a ::text').extract_first()}\n\n        for next_page in response.css('div.prev-post > a'):\n            yield response.follow(next_page, self.parse)\nOS: Windows10\npython version: 3.7.0\nscrapy version: Scrapy 1.5.1", "issue_status": "Closed", "issue_reporting_time": "2018-12-01T10:52:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "254": {"issue_url": "https://github.com/scrapy/scrapy/issues/3508", "issue_id": "#3508", "issue_summary": "Robots.txt Bug", "issue_description": "saseo90 commented on Nov 27, 2018 \u2022\nedited\n[robots.txt]Rule allowed but scrapped\nrobots.txt\nResult", "issue_status": "Closed", "issue_reporting_time": "2018-11-27T07:41:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "255": {"issue_url": "https://github.com/scrapy/scrapy/issues/3504", "issue_id": "#3504", "issue_summary": "JSON requests support", "issue_description": "Contributor\nkasun commented on Nov 24, 2018\nThis is a minor enhancement when dealing with JSON requests.\nRight now to do a json request we have to do something like this,\nheaders = {\n    'Content-Type': 'application/json'\n}\nyield Request(url, method='POST', body=json.loads(data), headers=headers)\nI'm proposing a new Request subclass that can save a few keystrokes.\nyield JSONRequest(url, data=data)\nWill be sending a PR for this.\n\ud83d\udc4d 1\n\ud83c\udf89 1", "issue_status": "Closed", "issue_reporting_time": "2018-11-24T16:34:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "256": {"issue_url": "https://github.com/scrapy/scrapy/issues/3502", "issue_id": "#3502", "issue_summary": "How to deal with redirects to a bookmark within a page (911 error)", "issue_description": "theresearchant commented on Nov 21, 2018\nI am very new to programming, so apologies if this is a rookie issue. I am a researcher, and I've been building spiders to allow me to crawl specific search results of IGN, the gaming forum. The first spider collects each entry in the search results, along with URLs, and then the second spider crawls each of those URLs for the content.\nThe problem is that IGN redirects URLs associated with a specific post to a new URL that incorporates a #bookmark at the end of the address. This allows the visitor to the page to jump directly down to the post in question, but I want my spider to crawl over the entire thread. In addition, my spider ends up with a (911) error after the redirect and returns no data. The only data retrieved is from any search results that linked directly to a thread rather than a post.\nI am absolutely stumped and confused, so any help would amazing! Both spiders are attached below.\nbroforceIGN_spider.txt\nbroforceIGN2_spider.txt", "issue_status": "Closed", "issue_reporting_time": "2018-11-21T15:13:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "257": {"issue_url": "https://github.com/scrapy/scrapy/issues/3500", "issue_id": "#3500", "issue_summary": "Fetch data from API inside Scrapy", "issue_description": "HelloEdit commented on Nov 20, 2018\nHi,\nI am working on a project that is divided into two parts:\nRetrieve a specific page\nOnce the ID of this page is extracted,\nSend requests to an API to obtain additional information on this page\nFor the second point, and to follow Scrapy's asynchronous philosophy, where should such a code be placed? (I hesitate between in the spider or in a pipeline).\nDo we have to use different libraries like asyncio & aiohttp to be able to achieve this goal asynchronously? (I <3 aiohttp so this is not a problem)\nI think that updating the documentation to give an example of this case could be used by someone other than me.\nThanks you ^^", "issue_status": "Closed", "issue_reporting_time": "2018-11-20T16:11:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "258": {"issue_url": "https://github.com/scrapy/scrapy/issues/3499", "issue_id": "#3499", "issue_summary": "How to call spider.parse from a single url", "issue_description": "hiro-o918 commented on Nov 17, 2018\nHi, I am trying to use a method parse of Spider in a source.\nThe following is an ideal code, though it does not work.\nurl = 'http://www.foo.com'\nresponce = Response(url)\nmyitem = MySpider().parse(response)\n\nreturn myitem\nwhere MySpider implements some parse method to yield an Item instance.\nThe reason might be that the response in the above downloads nothing.\nI think Request must be utilized to realize that, but I do not know how to do.\nthanks.", "issue_status": "Closed", "issue_reporting_time": "2018-11-17T15:40:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "259": {"issue_url": "https://github.com/scrapy/scrapy/issues/3498", "issue_id": "#3498", "issue_summary": "requests to scraperapi fail", "issue_description": "fabouille commented on Nov 17, 2018 \u2022\nedited\nI'm trying to use scrapy with the proxy / captcha / rendering service \"scraperapi\" but for some reason all my requests fail with the error below:\n[scrapy.core.scraper] ERROR: Error downloading <GET https://api.scraperapi.com?key=private_key&url=https://www.leboncoin.fr>\nTraceback (most recent call last):\n  File \"scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]\nThe same request works fine in chrome, postman or a regular python request though. Would anyone have experience with using scrapy in combination with scraperapi or know what could cause the issue?\nI'm using the following setup\nScrapy       : 1.5.1\nlxml         : 4.2.5.0\nlibxml2      : 2.9.8\ncssselect    : 1.0.3\nparsel       : 1.4.0\nw3lib        : 1.19.0\nTwisted      : 18.7.0\nPython       : 3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\npyOpenSSL    : 18.0.0 (OpenSSL 1.0.2p  14 Aug 2018)\ncryptography : 2.3.1\nPlatform     : Windows-10-10.0.17134-SP0\nThanks", "issue_status": "Closed", "issue_reporting_time": "2018-11-17T04:29:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "260": {"issue_url": "https://github.com/scrapy/scrapy/issues/3497", "issue_id": "#3497", "issue_summary": "Select element outside html tag", "issue_description": "llermaly commented on Nov 17, 2018\nI have a special case where a script tag is placed outside the html tag :\n<html>\n....\n</html>\n\n<script>data</script>\nboth css and xpath selectors are not finding this script tag, the only way I found is using response.text , but that responds with a giant string and I can not make regex operations on it with selector re() function.\nIs there a way to CSS or Xpath tags outside html tag?\nI tried with\nresponse.css('script')\nBut only consider script tags inside html tag\nThanks", "issue_status": "Closed", "issue_reporting_time": "2018-11-17T01:00:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "261": {"issue_url": "https://github.com/scrapy/scrapy/issues/3493", "issue_id": "#3493", "issue_summary": "It's wrong when i run `view(response)` after `scrapy shell <url>`.", "issue_description": "xxiaocheng commented on Nov 15, 2018\nI using Scrapy =1.5.1 and vscode=1.28.2\nI run view(response) when i want to see the response in the chrome ,but then the vscode was opened,the Chrome not open .\nPlease reply my question ,thanks.", "issue_status": "Closed", "issue_reporting_time": "2018-11-15T15:53:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "262": {"issue_url": "https://github.com/scrapy/scrapy/issues/3491", "issue_id": "#3491", "issue_summary": "ls", "issue_description": "minheejun commented on Nov 15, 2018\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2018-11-14T23:38:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "263": {"issue_url": "https://github.com/scrapy/scrapy/issues/3490", "issue_id": "#3490", "issue_summary": "pymongo.MongoClient() in pipelines return a tuple?????", "issue_description": "sshoop commented on Nov 14, 2018\nclass MongoPipeline(object):\n    def __init__(self, mongo_url, mongo_db):\n        self.mongo_url = mongo_url\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_url=crawler.settings.get('MONGO_URL'),\n            mongo_db=crawler.settings.get('MONGO_DB')\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(),\n        self.db = self.client[self.mongo_db]\n\n    def close_spider(self, spider):\n        self.client.close()\ni want to store item in mongodb, but when i use a pipeline, scrapy occurs errors.\nthis is error message:\nyield self.engine.open_spider(self.spider, start_requests)\nTypeError: tuple indices must be integers or slices, not str\nand i print slef.client, it shows that the self.client is a tuple, not <class 'pymongo.mongo_client.MongoClient'>", "issue_status": "Closed", "issue_reporting_time": "2018-11-14T15:17:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "264": {"issue_url": "https://github.com/scrapy/scrapy/issues/3489", "issue_id": "#3489", "issue_summary": "why not define a interface like java for pipeline and middleware?", "issue_description": "mouday commented on Nov 13, 2018\nwhy not define a interface like java for pipeline and middleware?\ni think can do this, maybe we implement a pipeline and middleware become simple.", "issue_status": "Closed", "issue_reporting_time": "2018-11-13T08:48:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "265": {"issue_url": "https://github.com/scrapy/scrapy/issues/3488", "issue_id": "#3488", "issue_summary": "Line break is splitting the result of text()", "issue_description": "plmrlnsnts commented on Nov 13, 2018 \u2022\nedited\nEnvironment:\nPython 2.7\nScrapy 1.5\nNormally, using text() in an xpath selector would return the content of the node. Please refer to the code snippet below.\n<h2>Item A</h2>\n<div>\n    Why <br><br> is this separated?\n</div>\n<h2>Item B</h2>\n<div>\n    But this is not.\n</div>\nI am expecting that this would return a list with a single item but instead it splits the string using the line breaks\n>>> response.xpath('//div/div[1]/text()').extract()\n# Output\n[u'Why ',  u' is this separated?\\r\\n' ]\n# Expected\n[u'Why <br><br> is this separated?\\r\\n' ]", "issue_status": "Closed", "issue_reporting_time": "2018-11-12T21:53:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "266": {"issue_url": "https://github.com/scrapy/scrapy/issues/3484", "issue_id": "#3484", "issue_summary": "On better handling use cases of PEP-380 inside request callbacks", "issue_description": "Contributor\nstarrify commented on Nov 8, 2018 \u2022\nedited\nRecently I observed people writing something like this (when using Python 3.3+):\n# coding: utf8\n\nimport scrapy\n\n\nclass TestSpider(scrapy.Spider):\n    name = 'test'\n    start_urls = ['http://httpbin.org/get']\n\n    def parse(self, response):\n        yield scrapy.Request('http://httpbin.org/get?foo=bar')\n        return {'url': response.url}\nThat won't work as expected as the return <some value> statement here, as defined in PEP-380, is just equivalent to raise StopIteration(<some value>).\nSample job logs: link\nCurrently (as of dc65e75) Scrapy simply ignores the \"returned\" value, which might be causing confusions to users. Sample job log of the code above: link\nA proposal is to generate an error (or at least warning) when such cases happen (StopIteration observed to have an value).\n\ud83d\udc4d 5", "issue_status": "Closed", "issue_reporting_time": "2018-11-08T12:51:04Z", "fixed_by": "#3869", "pull_request_summary": "[MRG+1][Py3] Check for 'return' with arguments inside generators", "pull_request_description": "Member\nelacuesta commented on Jul 12, 2019\nFixes #3484\nI also considered a version using ast.NodeVisitor, I think the walk-based solution should be more efficient since it can stop the iteration if the conditions are satisfied, while the NodeVisitor class would visit all Return nodes.\nclass ReturnNodeVisitor(ast.NodeVisitor):\n\n    def __init__(self, *args, **kwargs):\n        super(ReturnNodeVisitor, self).__init__(*args, **kwargs)\n        self.return_value = False\n\n    def visit_Return(self, node):\n        \"\"\"\n        Sets the 'return_value' instance variable to True if the value returned\n        by the node is not None (either explicitly or implicitly)\n        \"\"\"\n        if not is_safe_return_node(node):\n            self.return_value = True\n\n\ndef check_gen_with_no_none_return_visitor(callable):\n    \"\"\"\n    Logs a warning if a callable is a generator function and includes\n    a 'return' statement with a value different than None\n    \"\"\"\n    if inspect.isgeneratorfunction(callable):\n        tree = ast.parse(inspect.getsource(callable))\n        node_visitor = ReturnNodeVisitor()\n        node_visitor.visit(tree)\n        if node_visitor.return_value:\n            print('The generator has a return statement with a value different than None')\nI plan on doing some benchmarking to determine if this has a noticeable performance impact, but I look forward to reading your feedback.\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2019-12-20T17:46:46Z", "files_changed": [["19", "scrapy/core/scraper.py"], ["31", "scrapy/utils/datatypes.py"], ["46", "scrapy/utils/misc.py"], ["66", "tests/test_utils_datatypes.py"], ["37", "tests/test_utils_misc/test_return_with_argument_inside_generator.py"]]}, "267": {"issue_url": "https://github.com/scrapy/scrapy/issues/3483", "issue_id": "#3483", "issue_summary": "How to use Request parameter : body?", "issue_description": "mouday commented on Nov 7, 2018\nHow to use Request parameter : body?, have any example?\nrequests module have the method:\nurl = \"https://www.baidu.com/s\"\n\nparams = {\n    \"word\": \"google\"\n}\n\nrequests.get(url, params=params)\nthis means:\nurl = https://www.baidu.com/s?word=google\nscrapy can use for this?", "issue_status": "Closed", "issue_reporting_time": "2018-11-07T02:19:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "268": {"issue_url": "https://github.com/scrapy/scrapy/issues/3482", "issue_id": "#3482", "issue_summary": "scrapy not able to terminate but keeps showing log stats continously", "issue_description": "suraj-deshmukh commented on Nov 2, 2018 \u2022\nedited\nI have written a spider which crawls the website up to certain depth and downloads pdf/docs files using scrapy's inbuild file downloader. It works well except for one url(http://www.imerys.com).\nscrapy_pdf.py\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.shell import inspect_response\n# from scrapy_splash import SplashRequest\nfrom scrapy.http import Request\n# from urllib.parse import urlencode, parse_qs\n# from O365 import Message\nimport subprocess\nimport datetime\nimport re\nimport pandas as pd\nfrom ..items import PdfCrawlerItem\n\ndef check_link(url):\n    # function to check url for relevancy\n    return check\n\ndef extract_domain(url):\n    url = url.replace(\"http://\",\"\")\n    url = url.replace(\"https://\",\"\")\n    url = url.replace(\"www.\",\"\")\n    if url[-1] == '/':\n        url = url[0:-1]\n    return url.strip()\n\nclass MySpider(CrawlSpider):\n    name = 'pdf_extractor'\n    rules = (\n        Rule(LinkExtractor(tags=\"a\", deny_extensions = []), callback='parse_document',follow=True),\n    )\n\n    def __init__(self, ip, **kwargs):\n        domain = extract_domain(ip)\n        self.domain = domain\n        subprocess.call([\"mkdir\",\"/home/dev/scrapy-inbuild-downloader-example/pdf_crawler/documents/\"+domain])\n        self.start_time = datetime.datetime.now()\n        self.start_urls =  [ip] # py36\n        self.allowed_domains = [domain]\n        super().__init__(**kwargs)  # python3\n        \n    def parse_document(self, response):\n        content_type = response.headers.get('Content-Type',None).decode(\"utf-8\")\n        url = response.url\n        if content_type == \"application/pdf\" or content_type == \"application/msword\":\n            # print(\"checking url: %s\"%url)\n            check = check_link(url)\n            if check:\n                # print(\"pass url: %s\"%url)\n                name = response.headers.get('Content-Disposition',None)\n                if name:\n                    name = name.decode(\"utf-8\")\n                    name = re.findall(r\"filename=(.*)\", name)\n                    if name:\n                        name = name[0].replace(\"\\\"\",'').replace('\\'','')\n                        if name.endswith('.pdf') or name.endswith('.doc') or name.endswith('.docx'):\n                            pass\n                        else:\n                            name = name + '.pdf' if content_type == \"application/pdf\" else name + '.docx'\n                    else:\n                        name = url.split('/')[-1]\n                else:\n                    name = url.split('/')[-1]\n                item = PdfCrawlerItem()\n                item['file_urls'] = url\n                item['name'] = self.domain+\"/\"+name\n                print(item)\n                return item\n            # else:\n                # print(\"checking url: %s\"%url)\n\n    def close(self, spider, reason): # override this method for receiving notification after job finished.\n        time = datetime.datetime.now() - self.start_time\n        time = time.total_seconds() / 3600.\n        print(\"total time:\", time)\nitems.py\nimport scrapy\n\nclass PdfCrawlerItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    file_urls = scrapy.Field()\n    name = scrapy.Field()\npipelines.py\nfrom scrapy.pipelines.files import FilesPipeline\nfrom scrapy import Request\n\nclass PdfCrawlerPipeline(FilesPipeline):       \n    def file_path(self, request, response=None, info=None):\n        return request.meta.get('filename','')\n\n    def get_media_requests(self, item, info):\n        file_url = item['file_urls']\n        meta = {'filename': item['name']}\n        yield Request(url=file_url, meta=meta)\n    \n    # def item_completed(self, results, item, info):\n        # print(item['name'])\n        # return item\nlogs\n2018-11-02 16:05:33 [scrapy.extensions.logstats] INFO: Crawled 1796 pages (at 343 pages/min), scraped 18 items (at 3 items/min)\n2018-11-02 16:06:33 [scrapy.extensions.logstats] INFO: Crawled 1796 pages (at 0 pages/min), scraped 18 items (at 0 items/min)\n\n\n{'downloader/exception_count': 5,\n 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,\n 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,\n 'downloader/request_bytes': 724471,\n 'downloader/request_count': 1819,\n 'downloader/request_method_count/GET': 1819,\n 'downloader/response_bytes': 448477779,\n 'downloader/response_count': 1814,\n 'downloader/response_status_count/200': 1776,\n 'downloader/response_status_count/301': 8,\n 'downloader/response_status_count/302': 3,\n 'downloader/response_status_count/404': 18,\n 'downloader/response_status_count/500': 9,\n 'dupefilter/filtered': 24148,\n 'file_count': 18,\n 'file_status_count/downloaded': 15,\n 'file_status_count/uptodate': 3,\n 'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2018, 11, 2, 10, 10, 56, 530946),\n 'httperror/response_ignored_count': 19,\n 'httperror/response_ignored_status_count/404': 16,\n 'httperror/response_ignored_status_count/500': 3,\n 'item_scraped_count': 18,\n 'log_count/DEBUG': 240624,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 31,\n 'log_count/WARNING': 3,\n 'memusage/max': 258433024,\n 'memusage/startup': 84455424,\n 'offsite/domains': 58,\n 'offsite/filtered': 1536,\n 'request_depth_max': 2,\n 'response_received_count': 1797,\n 'retry/count': 10,\n 'retry/max_reached': 4,\n 'retry/reason_count/500 Internal Server Error': 6,\n 'retry/reason_count/twisted.web._newclient.ResponseFailed': 2,\n 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,\n 'scheduler/dequeued': 1792,\n 'scheduler/dequeued/memory': 1792,\n 'scheduler/enqueued': 1794,\n 'scheduler/enqueued/memory': 1794,\n 'start_time': datetime.datetime(2018, 11, 2, 10, 7, 7, 304081)}\nI am not able to figure out whats wrong with above code because it works well with the sites that i have tested except above one.", "issue_status": "Closed", "issue_reporting_time": "2018-11-02T10:23:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "269": {"issue_url": "https://github.com/scrapy/scrapy/issues/3481", "issue_id": "#3481", "issue_summary": "Scrapy view <url> causing TypeError", "issue_description": "vionemc commented on Nov 1, 2018 \u2022\nedited\nscrapy view https://example.com\nWhen I run the command line above, I get this error:\n2018-11-01 20:49:29 [twisted] CRITICAL: Unhandled error in Deferred:\n\n2018-11-01 20:49:29 [twisted] CRITICAL:\nTraceback (most recent call last):\n  File \"d:\\kerja\\hit\\python projects\\my_project\\my_project-env\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1386, in _inlineCallbacks\n    result = g.send(result)\n  File \"d:\\kerja\\hit\\python projects\\my_project\\my_project-env\\lib\\site-packages\\scrapy\\crawler.py\", line 98, in crawl\n    six.reraise(*exc_info)\n  File \"d:\\kerja\\hit\\python projects\\my_project\\my_project-env\\lib\\site-packages\\scrapy\\crawler.py\", line 79, in crawl\n    self.spider = self._create_spider(*args, **kwargs)\n  File \"d:\\kerja\\hit\\python projects\\my_project\\my_project-env\\lib\\site-packages\\scrapy\\crawler.py\", line 102, in _create_spider\n    return self.spidercls.from_crawler(self, *args, **kwargs)\n  File \"d:\\kerja\\hit\\python projects\\my_project\\my_project-env\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 51, in from_crawler\n    spider = cls(*args, **kwargs)\nTypeError: __init__() got an unexpected keyword argument 'start_requests'\n'page' is not recognized as an internal or external command,\noperable program or batch file.", "issue_status": "Closed", "issue_reporting_time": "2018-11-01T13:57:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "270": {"issue_url": "https://github.com/scrapy/scrapy/issues/3479", "issue_id": "#3479", "issue_summary": "scrapy shell disfunction and require for any solution", "issue_description": "hanahanamarket commented on Oct 30, 2018\nHello. I'm beginner of scrapy package.\nThank you for developing useful scrapy package. I am exciting in using it!!\n[About my environment]\nI have a trouble in use of scrapy shell from Jupyter Notebook and for each tools I updated to latest version(October, 30, 2018). I operate scrapy shell from Windows10. When I installed scrapy package, I used conda install command from Anaconda prompt.\n[What I had tried to use scrapy shell]\n\u2460While reffering to official document of scrapy shell, I wrote command like [!scrapy shell \"URL\" ] on JupyterNotebook cell, but I can't get any response. The cell is running continuously with showing mark of [*] , until I interrupt JupyterNotebook Kernel.\n\u2462I had tried same code written in avobe on IPython kernel which I open with magic command \"%qtconsole\", but running cell respond nothing as well as former one.\n\u2462In part of arbitrary URL after scrapy shell command, I tried to change some URLs from lots of sites. But nothing is changed in returning.\n\u2463As an additional trial, I tried to use scrapy shell as root authority. As soon as Ithe shell window opened, it goes down and close.\n[purpose of this question]\nWould you mind if I ask you any solution in this problem? I can't specify causes .\nWhat action should I take?\nThank you for your kind!!", "issue_status": "Closed", "issue_reporting_time": "2018-10-30T14:56:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "271": {"issue_url": "https://github.com/scrapy/scrapy/issues/3475", "issue_id": "#3475", "issue_summary": "For Tutorial , NameError name 'null' is not defined.", "issue_description": "jiahaozhou commented on Oct 25, 2018\nI just start with scrapy, I followed the tutorial, after enter scrapy crawl quotes I got an error.\n(snowflakes) C:\\Users\\Jiaha\\tutorial>scrapy crawl quotes\nTraceback (most recent call last):\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\Scripts\\scrapy-script.py\", line 10, in\nsys.exit(execute())\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\cmdline.py\", line 149, in execute\ncmd.crawler_process = CrawlerProcess(settings)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\crawler.py\", line 249, in init\nsuper(CrawlerProcess, self).init(settings)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\crawler.py\", line 137, in init\nself.spider_loader = _get_spider_loader(settings)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\crawler.py\", line 336, in _get_spider_loader\nreturn loader_cls.from_settings(settings.frozencopy())\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\spiderloader.py\", line 61, in from_settings\nreturn cls(settings)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\spiderloader.py\", line 25, in init\nself._load_all_spiders()\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\spiderloader.py\", line 47, in load_all_spiders\nfor module in walk_modules(name):\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 71, in walk_modules\nsubmod = import_module(fullpath)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\importlib_init.py\", line 127, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 1006, in _gcd_import\nFile \"\", line 983, in _find_and_load\nFile \"\", line 967, in _find_and_load_unlocked\nFile \"\", line 677, in _load_unlocked\nFile \"\", line 728, in exec_module\nFile \"\", line 219, in _call_with_frames_removed\nFile \"C:\\Users\\Jiaha\\tutorial\\tutorial\\spiders\\quotes_spider.py\", line 33, in\n\"execution_count\": null,\nNameError: name 'null' is not defined\n(snowflakes) C:\\Users\\Jiaha\\tutorial>scrapy crawl quotes\nTraceback (most recent call last):\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\Scripts\\scrapy-script.py\", line 10, in\nsys.exit(execute())\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\cmdline.py\", line 149, in execute\ncmd.crawler_process = CrawlerProcess(settings)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\crawler.py\", line 249, in init\nsuper(CrawlerProcess, self).init(settings)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\crawler.py\", line 137, in init\nself.spider_loader = _get_spider_loader(settings)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\crawler.py\", line 336, in _get_spider_loader\nreturn loader_cls.from_settings(settings.frozencopy())\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\spiderloader.py\", line 61, in from_settings\nreturn cls(settings)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\spiderloader.py\", line 25, in init\nself._load_all_spiders()\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\spiderloader.py\", line 47, in load_all_spiders\nfor module in walk_modules(name):\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 71, in walk_modules\nsubmod = import_module(fullpath)\nFile \"C:\\Users\\Jiaha\\AppData\\Local\\conda\\conda\\envs\\snowflakes\\lib\\importlib_init.py\", line 127, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 1006, in _gcd_import\nFile \"\", line 983, in _find_and_load\nFile \"\", line 967, in _find_and_load_unlocked\nFile \"\", line 677, in _load_unlocked\nFile \"\", line 728, in exec_module\nFile \"\", line 219, in _call_with_frames_removed\nFile \"C:\\Users\\Jiaha\\tutorial\\tutorial\\spiders\\quotes_spider.py\", line 33, in\n\"execution_count\": null,\nNameError: name 'null' is not defined\nCode: # I exactly followed the tutorial.\nimport scrapy\nclass QuotesSpider(scrapy.Spider):\nname = \"quotes\"\ndef start_requests(self):\n    urls = [\n        'http://quotes.toscrape.com/page/1/',\n        'http://quotes.toscrape.com/page/2/',\n    ]\n    for url in urls:\n        yield scrapy.Request(url=url,callback=self.parse)\n        \ndef parse(self,response):\n    page = response.url.split(\"/\")[-2]\n    filename = 'quotes-%s.html' % page\n    with open(filename, 'wb') as f:\n        f.write(response.body)\n    self.log('Saved file %s' % filename)", "issue_status": "Closed", "issue_reporting_time": "2018-10-25T03:38:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "272": {"issue_url": "https://github.com/scrapy/scrapy/issues/3474", "issue_id": "#3474", "issue_summary": "There is issue With Twisted Library", "issue_description": "this-is-r-gaurav commented on Oct 24, 2018\nThere is some issue with Twisted Package while installing Scrapy in Virtual Env, It stucks and raise Errors and raise error of Python.h not found, some suggested to update python3-dev. I did but the python3-dev is already updated, I am using ubuntu 18.04 and Pip3 for virtualenv. If i try to install globally It get installed perfectly, but i need it as depemdency for one of my Project", "issue_status": "Closed", "issue_reporting_time": "2018-10-24T03:00:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "273": {"issue_url": "https://github.com/scrapy/scrapy/issues/3473", "issue_id": "#3473", "issue_summary": "Scrapy Log None", "issue_description": "Flyraty commented on Oct 22, 2018\nwhy scrapy logging print many None.\nscrapy \u65e5\u5fd7\u6253\u5370\u51fa\u6765\u7684None \u662f\u4ec0\u4e48\u610f\u601d\u5462", "issue_status": "Closed", "issue_reporting_time": "2018-10-22T15:58:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "274": {"issue_url": "https://github.com/scrapy/scrapy/issues/3472", "issue_id": "#3472", "issue_summary": "Spider can't GET response after being redirected", "issue_description": "Kenisy commented on Oct 22, 2018 \u2022\nedited\nHi,\nMy spider can't crawl the url after being redirected to but if I scrape the url directly, it works normally. For example, when I scrape url 'https://www.novelupdates.com/extnu/1965286/' :\ndef start_requests(self):\n            yield scrapy.Request('https://www.novelupdates.com/extnu/1965286/', self.parse)\nthe response is:\n[scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://translatinotaku.net/2018/10/10/r-p-chapter-2-hunter/> from <GET https://www.novelupdates.com/extnu/1965286/>\n[scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n[scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://translatinotaku.net/2018/10/10/r-p-chapter-2-hunter/> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_record', 'wrong version number')]>]\n[scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n[scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://translatinotaku.net/2018/10/10/r-p-chapter-2-hunter/> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_record', 'wrong version number')]>]\n[scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n[scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://translatinotaku.net/2018/10/10/r-p-chapter-2-hunter/> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_record', 'wrong version number')]>]\n[scrapy.core.scraper] ERROR: Error downloading <GET http://translatinotaku.net/2018/10/10/r-p-chapter-2-hunter/>\nTraceback (most recent call last):\nFile \"e:\\projects\\virutalenv\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\ndefer.returnValue((yield download_func(request=request,spider=spider)))\nResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_get_record', 'wrong version number')]>]\n[scrapy.core.engine] INFO: Closing spider (finished)\nBut if i scrape the redirected url 'http://translatinotaku.net/2018/10/10/r-p-chapter-2-hunter/' directly it works ok.\nMy scrapy version:\nScrapy : 1.5.1\nlxml : 4.2.5.0\nlibxml2 : 2.9.5\ncssselect : 1.0.3\nparsel : 1.5.0\nw3lib : 1.19.0\nTwisted : 18.7.0\nPython : 2.7.15 (v2.7.15:ca079a3ea3, Apr 30 2018, 16:30:26) [MSC v.1500 64 bit (AMD64)]\npyOpenSSL : 18.0.0 (OpenSSL 1.1.0i 14 Aug 2018)\ncryptography : 2.3.1\nPlatform : Windows-10-10.0.17134", "issue_status": "Closed", "issue_reporting_time": "2018-10-22T15:19:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "275": {"issue_url": "https://github.com/scrapy/scrapy/issues/3467", "issue_id": "#3467", "issue_summary": "Crawler not returning the result in python django", "issue_description": "adeel3612 commented on Oct 17, 2018\nHello i am using this library to create an api in django which return me the result.\nThis is my code.\nfrom django.http import HttpResponse, JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess,CrawlerRunner\nimport json\n\n@csrf_exempt\ndef some_view(request, username):\n    process = CrawlerRunner({\n        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n        'LOG_ENABLED': 'false'\n    })\n    process_test = process.crawl(QuotesSpider)\n\n    return JsonResponse({'return': process_test})\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n\n    def start_requests(self):\n        urls = [\n            'http://quotes.toscrape.com/random',\n        ]\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n\n    def parse(self, response):\n        return json.dumps(response.css('.text::text').extract_first())\nfirst i was using CrawlerProcess but that was giving me the error of main thread so i user CrawlerRunner and now error is is not JSON serializable. Any body can help me in this regard i try to read the documentation but no success.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-10-17T09:45:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "276": {"issue_url": "https://github.com/scrapy/scrapy/issues/3466", "issue_id": "#3466", "issue_summary": "Adding trace id to scrapy logs", "issue_description": "Chelsea31 commented on Oct 17, 2018\nI want to add a trace id to my logs. This trace id is present in my request. But I am unable to add it to my scrapy logs. I have tried multiple solutions.\ncreating my own logger and adding trace id using loggerAdapter.\nThis works but only for the logs which I personally code. Also, there are failures for logs which are generated by engine.py, middleware.py and so on.\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 861, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 734, in format\n    return fmt.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 469, in format\n    s = self._fmt % record.__dict__\nKeyError: 'trace_id'\nLogged from file middlewares.py, line 69\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 861, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 734, in format\n    return fmt.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 469, in format\n    s = self._fmt % record.__dict__\nKeyError: 'trace_id'\nLogged from file redirect.py, line 41\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 861, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 734, in format\n    return fmt.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 469, in format\n    s = self._fmt % record.__dict__\nKeyError: 'trace_id'\nLogged from file middlewares.py, line 69\n^CTraceback (most recent call last):\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 861, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 734, in format\n    return fmt.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 469, in format\n    s = self._fmt % record.__dict__\nKeyError: 'trace_id'\nLogged from file crawler.py, line 258\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 861, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 734, in format\n    return fmt.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 469, in format\n    s = self._fmt % record.__dict__\nKeyError: 'trace_id'\nLogged from file engine.py, line 295\nAdded trace id in the LOG_FORMAT setting. But I can't find a way to pass that argument to scrapy.\nLOG_FORMAT = \"%(asctime)s %(levelname)s [%(trace_id)s]: %(message)s\" \nTurning off scrapy logs is not an option to me.", "issue_status": "Closed", "issue_reporting_time": "2018-10-17T05:54:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "277": {"issue_url": "https://github.com/scrapy/scrapy/issues/3462", "issue_id": "#3462", "issue_summary": "Python 3.7 compatibility", "issue_description": "BORN2LOSE commented on Oct 15, 2018\nI'm sure developers are working hard to catch up with Python 3.7. Is there any milestones?\n$ scrapy shell \"https://blog.scrapinghub.com\"\n2018-10-15 12:38:09 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n2018-10-15 12:38:09 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.7.0 (default, Sep 15 2018, 19:13:07) - [GCC 8.2.1 20180831], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-4.14.74-1-MANJARO-x86_64-with-arch-Manjaro-Linux\n2018-10-15 12:38:09 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'EDITOR': 'nvim', 'LOGSTATS_INTERVAL': 0}\n\nTraceback (most recent call last):\n  File \"/home/vrmorgue/.local/bin/scrapy\", line 10, in <module>\n    sys.exit(execute())\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 150, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\n    func(*a, **kw)\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 157, in _run_command\n    cmd.run(args, opts)\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 65, in run\n    crawler = self.crawler_process._create_crawler(spidercls)\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/crawler.py\", line 203, in _create_crawler\n    return Crawler(spidercls, self.settings)\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/crawler.py\", line 55, in __init__\n    self.extensions = ExtensionManager.from_crawler(self)\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/middleware.py\", line 34, in from_settings\n    mwcls = load_object(clspath)\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 44, in load_object\n    mod = import_module(module)\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/scrapy/extensions/telnet.py\", line 12, in <module>\n    from twisted.conch import manhole, telnet\n  File \"/home/vrmorgue/.local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 154\n    def write(self, data, async=False):\n                              ^\nSyntaxError: invalid syntax", "issue_status": "Closed", "issue_reporting_time": "2018-10-15T09:40:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "278": {"issue_url": "https://github.com/scrapy/scrapy/issues/3461", "issue_id": "#3461", "issue_summary": "Scrapy can not work with py_translator or googletrans", "issue_description": "RobotSe7en commented on Oct 14, 2018\nIn scrapy project,\nwhen I\nimport py_translator\nor googletrans,\nthe scrapy\nraise KeyError(\"Spider not found: {}\".format(spider_name))\nKeyError: 'Spider not found: miui'\nwhen I comment\nimport py_translator\nthe scrapy works.\nwhy?", "issue_status": "Closed", "issue_reporting_time": "2018-10-14T06:28:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "279": {"issue_url": "https://github.com/scrapy/scrapy/issues/3460", "issue_id": "#3460", "issue_summary": "No attention to pull requests", "issue_description": "Contributor\nthernstig commented on Oct 10, 2018\nI am reporting this issue to give attention to that there are 214 pull requests, many valid, which no maintainer is looking at. Is there a reason for this?\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-10-10T18:11:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "280": {"issue_url": "https://github.com/scrapy/scrapy/issues/3459", "issue_id": "#3459", "issue_summary": "SOCKS proxies support?", "issue_description": "TheOnlyArtz commented on Oct 9, 2018\nDoes Scrapy support socks4/5 ? and if not , is there a way to make it to?", "issue_status": "Closed", "issue_reporting_time": "2018-10-09T15:00:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "281": {"issue_url": "https://github.com/scrapy/scrapy/issues/3458", "issue_id": "#3458", "issue_summary": "Filling forms doesn't work for me", "issue_description": "TheOnlyArtz commented on Oct 8, 2018\nI got this code:\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass NetflixSpider(scrapy.Spider):\n    name = 'netflix'\n    allowed_domains = ['netflix.com']\n    login_url = \"https://www.netflix.com/il-en/login\"\n    start_urls = ['https://www.netflix.com/login']\n    handle_httpstatus_list = [302, 200]\n\n    def parse(self, response):\n        self.log(\"I just visited \" + response.url)\n        self.log(response.status)\n        data = {\n            \"email\": \"mail@gmail.com\",\n            \"password\": \"password\"\n        }\n\n        # self.log(self.login_url)\n        yield scrapy.FormRequest(url=\"https://www.netflix.com/il-en/login\", formdata=data, callback=self.parse_sent)\n\n    def parse_sent(self, response):\n        yield {\n            \"email\": response.css(\"input[name=email]::attr(value)\").extract_first()\n        }\nAnd the parse_sent outputs {'email': ''} why is that?", "issue_status": "Closed", "issue_reporting_time": "2018-10-08T14:05:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "282": {"issue_url": "https://github.com/scrapy/scrapy/issues/3457", "issue_id": "#3457", "issue_summary": "twisted error stack trace doesn't output when the spider is using proxymiddleware", "issue_description": "VikShiv commented on Oct 6, 2018\nI am using ProxyMiddleware in scrapy , and its throwing error.\nHere is Traceback :\nUnhandled error in Deferred:\n2018-10-06 14:15:31 [twisted] CRITICAL: Unhandled error in Deferred:\n2018-10-06 14:15:31 [twisted] CRITICAL:\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.5/dist-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\nresult = g.send(result)\nFile \"/usr/local/lib/python3.5/dist-packages/scrapy/crawler.py\", line 80, in crawl\nself.engine = self._create_engine()\nFile \"/usr/local/lib/python3.5/dist-packages/scrapy/crawler.py\", line 105, in _create_engine\nreturn ExecutionEngine(self, lambda _: self.stop())\nFile \"/usr/local/lib/python3.5/dist-packages/scrapy/core/engine.py\", line 69, in init\nself.downloader = downloader_cls(crawler)\nFile \"/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/init.py\", line 88, in init\nself.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\nFile \"/usr/local/lib/python3.5/dist-packages/scrapy/middleware.py\", line 58, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/usr/local/lib/python3.5/dist-packages/scrapy/middleware.py\", line 40, in from_settings\nmw = mwcls()\nTypeError: init() missing 1 required positional argument: 'arg'\nI have attached screenshot of my proxymiddleware and setting.py file\nThanks in advance.", "issue_status": "Closed", "issue_reporting_time": "2018-10-06T08:54:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "283": {"issue_url": "https://github.com/scrapy/scrapy/issues/3452", "issue_id": "#3452", "issue_summary": "AttributeError: 'Pipeline' object has no attribute 'crawler'", "issue_description": "Kelaxon commented on Oct 6, 2018\nI wanted to initialize a variable uploader in my custom image pipeline, so I used the from_crawler method and overrode the constructor in the pipeline.\nclass ProductAllImagesPipeline(ImagesPipeline):\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            settings=crawler.settings\n        )\n\n    def __init__(self, settings=None):\n        self.store_uri = settings['IMAGES_STORE']\n        self.qiniu_uploader = QiniuUploader(settings)\n        super(ImagesPipeline, self).__init__(store_uri=settings['IMAGES_STORE'], settings=settings)\nBut when the program ran to item_complete() in the pipeline, after try several times, results variable always got the error\n<class 'tuple'>: (False, <twisted.python.failure.Failure builtins.AttributeError: 'ProductAllImagesPipeline' object has no attribute 'crawler'>)\nThen I commented all the code above, it got the correct results:\nWeirdly, I uncommented the code again, it still got the correct results:\nDoes anybody have any ideas? Thanks.", "issue_status": "Closed", "issue_reporting_time": "2018-10-06T07:48:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "284": {"issue_url": "https://github.com/scrapy/scrapy/issues/3448", "issue_id": "#3448", "issue_summary": "how to extract data from search page", "issue_description": "radharani1306 commented on Oct 5, 2018\nI need to extract data from a search result page, which is taking time to load. Because the page is taking time, the scrapy is returning with basic html page, not the search result page. How can I wait until the page is load, to extract the elements after page load", "issue_status": "Closed", "issue_reporting_time": "2018-10-05T07:03:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "285": {"issue_url": "https://github.com/scrapy/scrapy/issues/3447", "issue_id": "#3447", "issue_summary": "Cryptography - ImportError: DLL load failed: The operating system cannot run %1", "issue_description": "Anthon02 commented on Oct 4, 2018\nI am trying to create a scrapy project with a version of scrapy installed with\nconda install -c conda-forge scrapy\nI am getting the following error during install.\nfrom cryptography.hazmat.bindings._openssl import ffi,lib\nImportError: DLL load failed: The operating system cannot run %1.\nAny help would be appreciated.\nThanks\n\n\n\nEnvironment (conda list):\n\nDetails about conda and system ( conda info ):", "issue_status": "Closed", "issue_reporting_time": "2018-10-04T02:12:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "286": {"issue_url": "https://github.com/scrapy/scrapy/issues/3444", "issue_id": "#3444", "issue_summary": "Local Relative URLs work in SHELL but don't work in scrapper", "issue_description": "tarunteckedge commented on Oct 2, 2018\nThis may relate to #1550 which is closed now, but it still doesn't work in the scrapper.\nPlease guide further on the same.\nThanks", "issue_status": "Closed", "issue_reporting_time": "2018-10-02T11:17:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "287": {"issue_url": "https://github.com/scrapy/scrapy/issues/3443", "issue_id": "#3443", "issue_summary": "Regarding GSoc 2019", "issue_description": "Contributor\nmaramsumanth commented on Oct 2, 2018\nIs it officially confirmed that scrapinghub will be a sub-organisation under PSF in upcoming GSoc 2019?\n@cathalgarvey , I am much interested to contribute to scrapy.", "issue_status": "Closed", "issue_reporting_time": "2018-10-02T09:52:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "288": {"issue_url": "https://github.com/scrapy/scrapy/issues/3438", "issue_id": "#3438", "issue_summary": "ImportError: dlopen failed: library \"libssl.so.1.0.0\" not found", "issue_description": "eaze999 commented on Sep 30, 2018 \u2022\nedited\nwhen i run ' scrapy crawl xxspider.py ' in Termux envirement of my moible phone,  it run's well befor i run ' pkg upgrade ' command. \nI find that openssl become to openssl/stable,now 1.1.1-2 aarch64 [installed]\nPlsease help me, thanks a lot!\n'''\nTraceback (most recent call last):\nFile \"/data/data/com.termux/files/usr/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/scrapy/cmdline.py\", line 149, in execute\ncmd.crawler_process = CrawlerProcess(settings)\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/scrapy/crawler.py\", line 252, in init\nlog_scrapy_info(self.settings)\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/scrapy/utils/log.py\", line 149, in log_scrapy_info\nfor name, version in scrapy_components_versions()\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/scrapy/utils/versions.py\", line 35, in scrapy_components_versions\n(\"pyOpenSSL\", _get_openssl_version()),\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/scrapy/utils/versions.py\", line 43, in _get_openssl_version\nimport OpenSSL\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/OpenSSL/init.py\", line 8, in\nfrom OpenSSL import crypto, SSL\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/OpenSSL/crypto.py\", line 16, in\nfrom OpenSSL._util import (\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/OpenSSL/_util.py\", line 6, in\nfrom cryptography.hazmat.bindings.openssl.binding import Binding\nFile \"/data/data/com.termux/files/usr/lib/python3.6/site-packages/cryptography/hazmat/bindings/openssl/binding.py\", line 13, in\nfrom cryptography.hazmat.bindings._openssl import ffi, lib\nImportError: dlopen failed: library \"libssl.so.1.0.0\" not found\n'''\nthen when i want to install libssl by apt-get or pkg, it cann't find it:\n'$ apt-get install libssl\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nE: Unable to locate package libssl'", "issue_status": "Closed", "issue_reporting_time": "2018-09-30T06:33:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "289": {"issue_url": "https://github.com/scrapy/scrapy/issues/3437", "issue_id": "#3437", "issue_summary": "TLS handshake failure", "issue_description": "mikesxf commented on Sep 30, 2018 \u2022\nedited\nHi\nI'm getting a handshake failure when running:\nscrapy shell https://androidappsapk.co\nand the results:\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 149, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 89, in _run_print_help\nfunc(*a, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 156, in _run_command\ncmd.run(args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/shell.py\", line 73, in run\nshell.start(url=url, redirect=not opts.no_redirect)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 48, in start\nself.fetch(url, spider, redirect=redirect)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 115, in fetch\nreactor, self._schedule, request, spider)\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\nresult.raiseException()\nFile \"string\", line 2, in raiseException\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\nI have read many issues about this problem, but I am sorry that I can't get the point on how to solve it. And I wonder whether it can be solved by updating scrapy or openssl as I am using old version of scrapy and openssl. My environment:\nScrapy : 1.4.0\nlxml : 3.3.3.0\nlibxml2 : 2.9.1\ncssselect : 1.0.1\nparsel : 1.2.0\nw3lib : 1.17.0\nTwisted : 13.2.0\nPython : 2.7.6 (default, Oct 26 2016, 20:30:19) - [GCC 4.8.4]\npyOpenSSL : 0.13 (OpenSSL 1.0.1f 6 Jan 2014)\nPlatform : Linux-3.13.0-144-generic-x86_64-with-Ubuntu-14.04-trusty\nWhen I use wireshark to capture the traffic, it shows that after client hello, server return\nTLSv1 Record Layer: Alert (Level: Fatal, Description: Handshake Failure)", "issue_status": "Closed", "issue_reporting_time": "2018-09-30T02:29:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "290": {"issue_url": "https://github.com/scrapy/scrapy/issues/3435", "issue_id": "#3435", "issue_summary": "Is it possible to close the spider at spider_opened signal?", "issue_description": "pauloromeira commented on Sep 27, 2018\nHello,\nI'm working on a middleware that loads some resources at spider_opened handler method. If those resources can't be loaded, I need the spider to be closed.\nI tried to do that by raising the CloseSpider exception and also by calling crawler.engine.close_spider(...), but none of them works.\nIs there a way to do that?\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2018-09-27T17:05:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "291": {"issue_url": "https://github.com/scrapy/scrapy/issues/3434", "issue_id": "#3434", "issue_summary": "why not tie spider to request or response", "issue_description": "baby5 commented on Sep 27, 2018\ni saw this line in engine.py\ni have a question is why not do this for spider?", "issue_status": "Closed", "issue_reporting_time": "2018-09-27T12:59:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "292": {"issue_url": "https://github.com/scrapy/scrapy/issues/3433", "issue_id": "#3433", "issue_summary": "Spider should also yield Deferred", "issue_description": "baby5 commented on Sep 26, 2018\ni run scrapy example try to yield Deferred to sleep and not block, and i got this log output:\n[scrapy.core.scraper] ERROR: Spider must return Request, BaseItem, dict or None, got 'Deferred' in <GET http://quotes.toscrape.com/tag/humor/>\nthink about this situation:\ni got url from redis in start_requests function, the redis return error for the moment, i do not want to hit redis and block thread, so i can yield a Deferred to let cpu do other things", "issue_status": "Closed", "issue_reporting_time": "2018-09-26T08:48:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "293": {"issue_url": "https://github.com/scrapy/scrapy/issues/3432", "issue_id": "#3432", "issue_summary": "Twisted failure while running scrapy shell", "issue_description": "Chelsea31 commented on Sep 26, 2018\n[<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nI am experiencing this error occasionally while running the scraper.\nDoes this problem has a permanent resolution?", "issue_status": "Closed", "issue_reporting_time": "2018-09-26T05:17:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "294": {"issue_url": "https://github.com/scrapy/scrapy/issues/3431", "issue_id": "#3431", "issue_summary": "ERROR:scrapy.core.scraper:Error downloading", "issue_description": "vnmani1 commented on Sep 26, 2018\nTraceback (most recent call last):\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\ndefer.returnValue((yield download_func(request=request,spider=spider)))\nValueError: invalid hostname: advisors-dev-mra-2333_profile_update.swarm.cxawsnprd.massmutual.com\nERROR:scrapy.core.scraper:Error downloading <GET https://advisors-dev-mra-2333_profile_update.swarm.cxawsnprd.massmutual.com/shealyn-mcgaffey>\nScrapy : 1.3.0\nlxml : 4.2.5.0\nlibxml2 : 2.9.8\ncssselect : 1.0.3\nparsel : 1.5.0\nw3lib : 1.19.0\nTwisted : 18.7.0\nPython : 2.7.15rc1 (default, Apr 15 2018, 21:51:34) - [GCC 7.3.0]\npyOpenSSL : 18.0.0 (OpenSSL 1.1.0i 14 Aug 2018)", "issue_status": "Closed", "issue_reporting_time": "2018-09-26T04:33:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "295": {"issue_url": "https://github.com/scrapy/scrapy/issues/3428", "issue_id": "#3428", "issue_summary": "Python 2: logging shows Unicode characters in \\u form", "issue_description": "ariasuni commented on Sep 24, 2018\nIssue\nWhen running scrapy crawl, item fields with Unicode characters can be quite unreadable, for example:\nu'[\"M\\u0117sa, \\u017euvys ir kulinarija\", \"\\u0160vie\\u017eia pauk\\u0161tiena\"]\ninstead of\nu'[\"M\u0117sa, \u017euvys ir kulinarija\", \"\u0160vie\u017eia pauk\u0161tiena\"]'\nCause\nscrapy/scrapy/item.py\nLine 94 in 1fd1702\n return pformat(dict(self)) \npformat in Python 2 behaves differently than in Python 3.\nPython 2:\nIn: pformat(u'\u00e5')\nOut: \"u'\\\\xe5'\"\nPython 3:\nIn: pformat('\u00e5')\nOut: \"'\u00e5'\"\nSuggested solution\nOverriding PrettyPrinter.format:\nhttps://stackoverflow.com/a/10883893/8981993\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-09-24T09:51:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "296": {"issue_url": "https://github.com/scrapy/scrapy/issues/3427", "issue_id": "#3427", "issue_summary": "scrapy shell error: SyntaxError: invalid syntax", "issue_description": "jazzber commented on Sep 24, 2018\nI've just installed scrapy and I'm trying to just run \"scrapy shell\" command, yet am somehow met with a weird error, see output below:\nPC:~ user$ scrapy shell\n2018-09-24 06:12:34 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n2018-09-24 06:12:34 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 26 2018, 23:26:24) - [Clang 6.0 (clang-600.0.57)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.4.0-x86_64-i386-64bit\n2018-09-24 06:12:34 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/cmdline.py\", line 150, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\n    func(*a, **kw)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/cmdline.py\", line 157, in _run_command\n    cmd.run(args, opts)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 65, in run\n    crawler = self.crawler_process._create_crawler(spidercls)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/crawler.py\", line 203, in _create_crawler\n    return Crawler(spidercls, self.settings)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/crawler.py\", line 55, in __init__\n    self.extensions = ExtensionManager.from_crawler(self)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/middleware.py\", line 34, in from_settings\n    mwcls = load_object(clspath)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 44, in load_object\n    mod = import_module(module)\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/extensions/telnet.py\", line 12, in <module>\n    from twisted.conch import manhole, telnet\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 154\n    def write(self, data, async=False):\n                              ^\nSyntaxError: invalid syntax\nPC:~ user$ \nI'm having a lot of trouble finding any hits on google for similar errors, might anyone have some insight here?", "issue_status": "Closed", "issue_reporting_time": "2018-09-24T09:33:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "297": {"issue_url": "https://github.com/scrapy/scrapy/issues/3425", "issue_id": "#3425", "issue_summary": "Multi user login\uff1f", "issue_description": "chunfytseng commented on Sep 23, 2018\nI have the following scenarios:\n1, user login (multiple users may log in at the same time, what should I do? )\n2, get data after login success (get)\n3, according to the data obtained in the second step, post request data again (support post request?)\nThe third step has two results, success or failure, if the successful completion of this user's task, otherwise continue to take the third step.\nThe above is my usage scenario. Can I make an API instead of shutting down the crawler every time I crawl data, and how to support multiple users crawling data at the same time?", "issue_status": "Closed", "issue_reporting_time": "2018-09-22T23:54:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "298": {"issue_url": "https://github.com/scrapy/scrapy/issues/3424", "issue_id": "#3424", "issue_summary": "Does it support simultaneous requests from multiple users? That is, multi user login (second times with cookie login).", "issue_description": "chunfytseng commented on Sep 23, 2018\nDoes it support simultaneous requests from multiple users? That is, multi user login (second times with cookie login).", "issue_status": "Closed", "issue_reporting_time": "2018-09-22T23:48:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "299": {"issue_url": "https://github.com/scrapy/scrapy/issues/3423", "issue_id": "#3423", "issue_summary": "TypeError: 'NoneType' object is not subscriptable", "issue_description": "techaks commented on Sep 20, 2018", "issue_status": "Closed", "issue_reporting_time": "2018-09-20T08:45:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "300": {"issue_url": "https://github.com/scrapy/scrapy/issues/3422", "issue_id": "#3422", "issue_summary": "Question about proper architecture for complex crawler project", "issue_description": "FinnFrotscher commented on Sep 20, 2018 \u2022\nedited\nMy Goal I want to keep track of multiple twitter profiles over time (say, 12 weeks).\nWhat I want to build: A SpiderMother class that interfaces with some Database (holding CrawlJobs) to spawn and manage many small spiders.\nEach spider will crawl 1 user-profile on twitter at an irregular interval (the jobs will be added to the database according to some algorithm).\nThe Spiders get spawned as subprocesses by SpiderMother and depending on the success of the crawl, the database job get removed.\nIs this a good (the right) architecture?", "issue_status": "Closed", "issue_reporting_time": "2018-09-19T21:19:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "301": {"issue_url": "https://github.com/scrapy/scrapy/issues/3421", "issue_id": "#3421", "issue_summary": "scrapy not parsing content of tables correctly", "issue_description": "zergar commented on Sep 17, 2018 \u2022\nedited\nHi everyone,\nI'm trying to crawl the syllabus of my university and have encountered a problem: It seems that scrapy isn't parsing the content of tables at all. I've tried it with XPath as well as CSS-Selectors and neither does work.\nHave a look at the following minimal working example (robots have to be ignored for the spider to work):\nimport scrapy\n\n\nclass TableTestSpider(scrapy.Spider):\n    name = \"table-test\"\n\n    def start_requests(self):\n        urls = [\n            \"https://lsf.tubit.tu-berlin.de/qisserver/servlet/de.his.servlet.RequestDispatcherServlet?state=wplan&week=40_2018&act=Raum&pool=Raum&show=liste&P.vx=lang&P.subc=plan&raum.rgid=166\",\n        ]\n\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n\n    def parse(self, response):\n        content_div = response.xpath(\"//div[@class='content_max']/*\")\n\n        print(\"\\ntable\")\n        table = response.xpath(\"//table\")\n        print(table)\n\n        print(\"\\n//tbody/tr/th\")\n        tbody_tr_th = response.xpath(\"//tbody/tr/th\")\n        print(tbody_tr_th)\n\n        for div in content_div:\n            if div.xpath(\"h3/a\"):\n                print(\"h3/a\")\n\n            if div.css(\"table\"):\n                print(\"table\")\n\n            if div.xpath(\"tbody/tr/th\"):\n                print(\"tbody tr th\")\nOutput:\n2018-09-16 20:52:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://lsf.tubit.tu-berlin.de/qisserver/servlet/de.his.servlet.RequestDispatcherServlet?state=wplan&week=40_2018&act=Raum&pool=Raum&show=liste&P.vx=lang&P.subc=plan&raum.rgid=166> (referer: None)\n\ntable\n[<Selector xpath='//table' data='<table width=\"100%\" cellpadding=\"0\" cell'>, <Selector xpath='//table' data='<table width=\"100%\" cellpadding=\"0\" cell'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>, <Selector xpath='//table' data='<table summary=\"\u00dcbersicht \u00fcber alle Vera'>]\n\n//tbody/tr/th\n[]\ntable\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\nh3/a\ntable\n2018-09-16 20:52:00 [scrapy.core.engine] INFO: Closing spider (finished)\nIf you run the example you will notice that scrapy is able to find the tables via XPath (and CSS-Selectors) but not the content (tbody/tr/th) as well as any content (//tbody/tr/th). On the other hand, if you open the URL in your browser (https://lsf.tubit.tu-berlin.de/qisserver/servlet/de.his.servlet.RequestDispatcherServlet?state=wplan&raum.rgid=166&week=40_2018&act=Raum&pool=Raum&show=liste&P.vx=lang&P.subc=plan) and run $x(\"//tbody/tr/th\") in the browsers console it returns the heads of the tables (part of the content of a table) without any errors, indicating an error probably within the XML-parsing-algorithm. Also, querying for a h3/a-combination works flawlessly in scrapy, indicating that it isn't an error regarding the depth of the XML-DOM-tree.\nOne thing I've noticed when looking at the source code of the website was that it regularly works with tabs and linebreaks and non-breaking spaces causing me some trouble when parsing other elements, probably causing some hick-ups. Another thing I've noticed is that the website uses the summary-attribute within the table-tag, which, according to the W3C (https://www.w3.org/TR/WCAG-TECHS/H73.html#H73-description) is deprecated in HTML 5, though ths shouldn't matter since the doctype indicates a HTML 4.01-document (<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">).\nI'm hoping to being able root out the cause with your help.\nBest regards,\nzergar", "issue_status": "Closed", "issue_reporting_time": "2018-09-16T19:14:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "302": {"issue_url": "https://github.com/scrapy/scrapy/issues/3419", "issue_id": "#3419", "issue_summary": "twisted.web.error.SchemeNotSupported: Unsupported scheme: b'' | Adding proxy to scrapy spider", "issue_description": "baninaveen commented on Sep 12, 2018\nI am trying to add proxy for single url\ndef start_requests(self): proxy_main = proxy_rotate() yield scrapy.Request(url=self.url, callback=self.parse, meta={'proxy':some_proxy_ip})\nWhen I add proxy it show error", "issue_status": "Closed", "issue_reporting_time": "2018-09-12T12:37:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "303": {"issue_url": "https://github.com/scrapy/scrapy/issues/3418", "issue_id": "#3418", "issue_summary": "Idea: Have all config values of plugins inside 'meta' in a subkey", "issue_description": "MaxValue commented on Sep 12, 2018 \u2022\nedited\nThis concerns the meta attribute of requests and responses which allows to store additional data and configuration of plugins.\nI noticed that all settings for all plugins are stored directly under the meta attribute. If one wants to store custom data, they first need to make sure not to overwrite/shadow existing keys using this list, but there are also other keys like splash (for the splash plugin) which are not listed.\nSo, a question/suggestion:\nHow about having another attribute (like spider_meta or self_meta or whatever) in requests/responses where the programmer can store custom values, thus avoiding collision with special keys?\nAn alternative could also be having a special key in meta (like user or spider or whatever) with the same purpose as above.\nOr something entirely different: Move all current special keys and plugin-keys to a special key inside meta and the user just has to avoid this special key.", "issue_status": "Closed", "issue_reporting_time": "2018-09-11T19:24:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "304": {"issue_url": "https://github.com/scrapy/scrapy/issues/3417", "issue_id": "#3417", "issue_summary": "Outdated tutorial pages", "issue_description": "ElToro1966 commented on Sep 10, 2018\nThe Scrapy Tutorial contain links to two pages,\nUsing Firebug for scraping and Using Firefox for scraping, that are outdated. Firebug isn't around any more, and the Firefox-page is full of references to outdated plugins. As the tutorial page is a starting point for Scrapy-beginners, an update is fairly important.", "issue_status": "Closed", "issue_reporting_time": "2018-09-10T10:20:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "305": {"issue_url": "https://github.com/scrapy/scrapy/issues/3414", "issue_id": "#3414", "issue_summary": "The call to callback function failed in yield request in Scrapy", "issue_description": "simba999 commented on Sep 5, 2018\nwhen i use yield scrapy.Request(url=link, callback=self.parse_detail), script doesn't call parse_detail function.", "issue_status": "Closed", "issue_reporting_time": "2018-09-05T08:21:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "306": {"issue_url": "https://github.com/scrapy/scrapy/issues/3413", "issue_id": "#3413", "issue_summary": "Resuming crawls gives the error: self.size, = struct.unpack(self.SIZE_FORMAT, qsize)", "issue_description": "foomoto commented on Sep 4, 2018\nI am running out of memory after crawling 60k pages on an 8GB server, so I am attempting the JOBSDIR settings to reduce memory consumption. If I stop scrapy and attempt to resume I get the below error: Please help.\n2018-09-03 21:24:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.therosefactory.com/bloom-box> (referer: None)\nTraceback (most recent call last):\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\nyield next(it)\nGeneratorExit\nException ignored in: <generator object iter_errback at 0x109230a98>\nRuntimeError: generator ignored GeneratorExit\nException ignored in: <generator object GlobalSpider.parse_new at 0x108aff1a8>\nRuntimeError: generator ignored GeneratorExit\nUnhandled error in Deferred:\n2018-09-03 21:24:06 [twisted] CRITICAL: Unhandled error in Deferred:\n2018-09-03 21:24:06 [twisted] CRITICAL:\nTraceback (most recent call last):\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/twisted/internet/task.py\", line 517, in _oneWorkUnit\nresult = next(self._iterator)\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 63, in\nwork = (callable(elem, *args, **named) for elem in iterable)\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/scraper.py\", line 183, in _process_spidermw_output\nself.crawler.engine.crawl(request=output, spider=spider)\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/engine.py\", line 210, in crawl\nself.schedule(request, spider)\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/engine.py\", line 216, in schedule\nif not self.slot.scheduler.enqueue_request(request):\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/scheduler.py\", line 57, in enqueue_request\ndqok = self._dqpush(request)\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/scheduler.py\", line 86, in _dqpush\nself.dqs.push(reqd, -request.priority)\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/queuelib/pqueue.py\", line 33, in push\nself.queues[priority] = self.qfactory(priority)\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/scheduler.py\", line 114, in _newdq\nreturn self.dqclass(join(self.dqdir, 'p%s' % priority))\nFile \"/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/queuelib/queue.py\", line 142, in init\nself.size, = struct.unpack(self.SIZE_FORMAT, qsize)\nstruct.error: unpack requires a buffer of 4 bytes", "issue_status": "Closed", "issue_reporting_time": "2018-09-03T20:26:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "307": {"issue_url": "https://github.com/scrapy/scrapy/issues/3410", "issue_id": "#3410", "issue_summary": "SSL Handshake Failure", "issue_description": "Mattnmoore commented on Aug 29, 2018\nI'm getting the following error when I try to crawl a website that has an issue with their SSL cert.\n ERROR: Error downloading <GET (removed): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\nHere are my versions:\nScrapy    : 1.3.3\nlxml      : 3.7.3.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2g  1 Mar 2016)\nPlatform  : Linux-4.4.0-51-generic-x86_64-with-Ubuntu-16.04-xenia\nI've noticed several other related issues, but none of the fixes in them have resolved the issue for me, any help would be greatly appreciated.", "issue_status": "Closed", "issue_reporting_time": "2018-08-29T15:35:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "308": {"issue_url": "https://github.com/scrapy/scrapy/issues/3408", "issue_id": "#3408", "issue_summary": "user-agent middleware does not actually change request user agent", "issue_description": "KeeeeiZ commented on Aug 29, 2018\nIn user-agent middleware, the process request method does not actually change the user agent", "issue_status": "Closed", "issue_reporting_time": "2018-08-29T06:52:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "309": {"issue_url": "https://github.com/scrapy/scrapy/issues/3406", "issue_id": "#3406", "issue_summary": "Scrapy with JSESSIONID", "issue_description": "heltleo commented on Aug 28, 2018 \u2022\nedited\nI'm trying to scrape the data from website with Form. I set all necessary information with headers, cookies and formData.\nBut it doesn't scrape the data(TableView), but scrape the website.\nimport scrapy\n\nclass NominationSpider(scrapy.Spider):\n    name = \"nomination\"\n    start_urls = [\n        'https://ivo.opal-gastransport.biz/ivo/nominations'\n    ]\n\n    def parse(self, response):\n        print(response.url)\n\n        index = response.url.split('?')[1]\n\n        headers = {\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n            'Accept-Encoding': 'gzip, deflate, br',\n            'Accept-Language': 'en-US,en;q=0.9',\n            'Connection': 'keep-alive',\n            'Host': 'ivo.opal-gastransport.biz',\n            'Referer': str(response.url),\n            'Upgrade-Insecure-Requests': 1,\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'\n        }\n\n        cookies = {\n            'JSESSIONID': '8A0837174D603BE97441F0B579307316'\n        }\n\n        formData = {\n            'reportparameterselect_hf_0': str(index) + '-1.IFormSubmitListener-form:',\n            'netpoint': '92IEAPR',\n            'flowDirection': 'EXIT',\n            'from': '01/08/2018',\n            'to': '08/08/2018',\n            'fileType': '0',\n            'p::submit': 'Preview'\n        }\n\n        yield scrapy.FormRequest.from_response(\n            response,\n            method=\"GET\",\n            headers=headers,\n            formdata=formData,\n            cookies=cookies,\n            clickdata={'name': 'p::submit'},\n            callback=self.parseResponse\n        )\n\n\n\n    def parseResponse(self, response):\n        print('------------------------')\n        print(response.body)\nit seems cookies doesn't applied or clickdata doesn't work.\nI am not sure what is problem correctly.\nbut I can see tableview with data on browser by selecting options and clicking Preview button.\nhttps://ivo.opal-gastransport.biz/ivo/nominations?0\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-08-27T20:51:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "310": {"issue_url": "https://github.com/scrapy/scrapy/issues/3398", "issue_id": "#3398", "issue_summary": "AttributeError: 'XmlItemExporter' object has no attribute 'file'", "issue_description": "vidarlee commented on Aug 22, 2018\nThe example in the document Item Exporters\ncalled the file attribute of XmlItemExporter instance as follow:\n    def close_spider(self, spider):\n        for exporter in self.year_to_exporter.values():\n            exporter.finish_exporting()\n            exporter.file.close()\nI checked the source of scrapy/scrapy/exporters.py,\nactually, there is no attribute named 'file' in XmlItemExporter.\nSo, when i run the script the error occurred:\n\"AttributeError: 'XmlItemExporter' object has no attribute 'file'\".\nAlso, there is no 'file' attribute in CsvItemExporter while exists in JsonItemExporter,\nPickleItemExporter, MarshalItemExporter , PprintItemExporter.\nI think it is necessary to and a file descriptor attribute into XmlItemExporter and CsvItemExporter for consistency. Also the file descriptor should be closed when exporting has been finished.\nIf the file descriptor attribute will add to XmlItemExporter and CsvItemExporter, then i have another question:\nIs it a good idea to add the self.file.close() into the function finish_exporting()?\nIt seems it is simple for user just call function finish_exporting() and won't care about the file descriptor whether closed.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-08-22T00:36:22Z", "fixed_by": "#4022", "pull_request_summary": "Fix the item exporter example", "pull_request_description": "Member\nGallaecio commented on Sep 17, 2019\nFixes #3398", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-19T07:17:24Z", "files_changed": [["1", "docs/topics/exporters.rst"]]}, "311": {"issue_url": "https://github.com/scrapy/scrapy/issues/3397", "issue_id": "#3397", "issue_summary": "Unhandled error in Deferred", "issue_description": "d7laungani commented on Aug 20, 2018\nI looked at previous issues regarding this error and I could not find a similar problem. My own stems from a missing package that scrapy seems to need. It says ImportError: No module named 'emailext'.\nscrapy -version -v gives me\nScrapy : 1.5.0 lxml : 4.2.1.0 libxml2 : 2.9.8 cssselect : 1.0.3 parsel : 1.4.0 w3lib : 1.19.0 Twisted : 17.9.0 Python : 3.5.2 (v3.5.2:4def2a2901a5, Jun 26 2016, 10:47:25) - [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] pyOpenSSL : 17.5.0 (OpenSSL 1.1.0g 2 Nov 2017) cryptography : 2.1.4 Platform : Darwin-16.7.0-x86_64-i386-64bit\nThe full stack trace is : `\nTraceback (most recent call last):\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\nresult = g.send(result)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/crawler.py\", line 80, in crawl\nself.engine = self._create_engine()\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/crawler.py\", line 105, in _create_engine\nreturn ExecutionEngine(self, lambda _: self.stop())\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/core/engine.py\", line 70, in init\nself.scraper = Scraper(crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/core/scraper.py\", line 71, in init\nself.itemproc = itemproc_cls.from_crawler(crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/middleware.py\", line 58, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/middleware.py\", line 34, in from_settings\nmwcls = load_object(clspath)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/utils/misc.py\", line 44, in load_object\nmod = import_module(module)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/init.py\", line 126, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 986, in _gcd_import\nFile \"\", line 969, in _find_and_load\nFile \"\", line 944, in _find_and_load_unlocked\nFile \"\", line 222, in _call_with_frames_removed\nFile \"\", line 986, in _gcd_import\nFile \"\", line 969, in _find_and_load\nFile \"\", line 956, in _find_and_load_unlocked\nImportError: No module named 'emailext'\n2018-08-19 21:18:23 [scrapy.crawler] INFO: Overridden settings: {}\n2018-08-19 21:18:23 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.logstats.LogStats',\n'scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.memusage.MemoryUsage']\n2018-08-19 21:18:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2018-08-19 21:18:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\nUnhandled error in Deferred:\n2018-08-19 21:18:23 [twisted] CRITICAL: Unhandled error in Deferred:\n2018-08-19 21:18:23 [twisted] CRITICAL:\nTraceback (most recent call last):\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\nresult = g.send(result)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/crawler.py\", line 80, in crawl\nself.engine = self._create_engine()\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/crawler.py\", line 105, in _create_engine\nreturn ExecutionEngine(self, lambda _: self.stop())\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/core/engine.py\", line 70, in init\nself.scraper = Scraper(crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/core/scraper.py\", line 71, in init\nself.itemproc = itemproc_cls.from_crawler(crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/middleware.py\", line 58, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/middleware.py\", line 34, in from_settings\nmwcls = load_object(clspath)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/utils/misc.py\", line 44, in load_object\nmod = import_module(module)\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/init.py\", line 126, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 986, in _gcd_import\nFile \"\", line 969, in _find_and_load\nFile \"\", line 944, in _find_and_load_unlocked\nFile \"\", line 222, in _call_with_frames_removed\nFile \"\", line 986, in _gcd_import\nFile \"\", line 969, in _find_and_load\nFile \"\", line 956, in _find_and_load_unlocked\nImportError: No module named 'emailext'`", "issue_status": "Closed", "issue_reporting_time": "2018-08-20T02:20:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "312": {"issue_url": "https://github.com/scrapy/scrapy/issues/3395", "issue_id": "#3395", "issue_summary": "Source learning path", "issue_description": "zhaojiedi1992 commented on Aug 18, 2018\nI want to learn scrapy source code, is there a suitable study route recommended.", "issue_status": "Closed", "issue_reporting_time": "2018-08-18T03:59:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "313": {"issue_url": "https://github.com/scrapy/scrapy/issues/3392", "issue_id": "#3392", "issue_summary": "dh key too small", "issue_description": "SurelySomeday commented on Aug 17, 2018\nurl: \u201chttps://portal.hyit.edu.cn/zfca/login\u201d\nerror: ERROR: Error downloading <GET https://portal.hyit.edu.cn/zfca/login>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'tls_process_ske_dhe', 'dh key too small')", "issue_status": "Closed", "issue_reporting_time": "2018-08-17T12:51:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "314": {"issue_url": "https://github.com/scrapy/scrapy/issues/3391", "issue_id": "#3391", "issue_summary": "File descriptor leak in FeedExporter", "issue_description": "Member\ndangra commented on Aug 15, 2018\nTest suite under windows fails to remove a file at https://github.com/scrapy/scrapy/pull/3315/files#diff-0f004a6e06393cc5e29012b83956eed2L245 because FeedExporter instances keeps an open file descriptor created for the test case.\nWe need to find the leak and close the file descriptor properly on close_spider signal.", "issue_status": "Closed", "issue_reporting_time": "2018-08-15T16:17:49Z", "fixed_by": "#4023", "pull_request_summary": "Fix windows file descriptor leak", "pull_request_description": "Member\nGallaecio commented on Sep 17, 2019\nFixes #3391", "pull_request_status": "Merged", "issue_fixed_time": "2019-10-22T13:12:53Z", "files_changed": [["4", "scrapy/extensions/feedexport.py"], ["2", "tests/test_feedexport.py"]]}, "315": {"issue_url": "https://github.com/scrapy/scrapy/issues/3389", "issue_id": "#3389", "issue_summary": "Scrapy installed successfully, but error occurred at crawl.", "issue_description": "kenneth663 commented on Aug 15, 2018\n0\ndown vote\nfavorite\nI built the scrapy framework on my mac, and everything went well, but when I followed the tutorial in scrapy, it went wrong when I typed in scrapy crawl dmoz. My Python version was 3.6 Mac version was 10.13.6. How do I fix it? The error message is as follows:\nkennethdeMBP:spiders kenneth$ scrapy crawl dmoz\n2018-08-15 17:56:28 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: tutorial)\n2018-08-15 17:56:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.3.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.6.3 (v3.6.3:2c5fed86e0, Oct 3 2017, 00:32:08) - [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h 27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.7.0-x86_64-i386-64bit\n2018-08-15 17:56:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders']}\n2018-08-15 17:56:28 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.memusage.MemoryUsage',\n'scrapy.extensions.logstats.LogStats']\nUnhandled error in Deferred:\n2018-08-15 17:56:28 [twisted] CRITICAL: Unhandled error in Deferred:\n2018-08-15 17:56:28 [twisted] CRITICAL:\nTraceback (most recent call last):\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\nresult = g.send(result)\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/crawler.py\", line 80, in crawl\nself.engine = self._create_engine()\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/crawler.py\", line 105, in _create_engine\nreturn ExecutionEngine(self, lambda _: self.stop())\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/core/engine.py\", line 69, in init\nself.downloader = downloader_cls(crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/core/downloader/init.py\", line 88, in init\nself.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/middleware.py\", line 58, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/middleware.py\", line 34, in from_settings\nmwcls = load_object(clspath)\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/utils/misc.py\", line 44, in load_object\nmod = import_module(module)\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/init.py\", line 126, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 994, in _gcd_import\nFile \"\", line 971, in _find_and_load\nFile \"\", line 955, in _find_and_load_unlocked\nFile \"\", line 665, in _load_unlocked\nFile \"\", line 678, in exec_module\nFile \"\", line 219, in _call_with_frames_removed\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 5, in\nfrom urllib2 import _parse_proxy\nFile \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib2.py\", line 220\nraise AttributeError, attr\n^\nSyntaxError: invalid syntax\nkennethdeMBP:spiders kenneth$", "issue_status": "Closed", "issue_reporting_time": "2018-08-15T10:13:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "316": {"issue_url": "https://github.com/scrapy/scrapy/issues/3388", "issue_id": "#3388", "issue_summary": "scrapy different with requests? requests work, but scrapy not", "issue_description": "zeroleo12345 commented on Aug 15, 2018 \u2022\nedited\nuse requests library, it work.\nimport requests\n\nheaders = {}\nheaders['Host'] = 'www.bloomberg.com'\nheaders['User-Agent'] = 'Charles/4.2.1'\n\nresponse = requests.get('https://www.xxx.com/quote/700:HK', headers=headers\nWhen use scrapy, it was block because the Website realize scrapy is a robot. what's the different with requests?\n2018-08-15 16:48:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (307) to <GET https://www.xxx.com/tosv2.html?vid=&uuid=f3262880-a067-11e8-922c-0f4bf8a1cda2&url=L3F1b3RlLzcwMDpISw==> from <GET https://www.xxx.com/quote/700:HK>", "issue_status": "Closed", "issue_reporting_time": "2018-08-15T08:52:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "317": {"issue_url": "https://github.com/scrapy/scrapy/issues/3384", "issue_id": "#3384", "issue_summary": "Error crawler Unhandled error in Deferred", "issue_description": "ricoxor commented on Aug 12, 2018\nHello,\nI want to run an old crawler build with Scrapy but I got an error that stop everyting. I try multiple thing but nothing change.\nCan someone help me to fix this issue ?\nThank you in advance.\nLogs :\nroot@xxxxx:/home/CrawlerNDD# scrapy crawl expired_one\n:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named x509'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.\nWithout the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n2018-08-12 15:14:55 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36)\n2018-08-12 15:14:55 [scrapy.utils.log] INFO: Versions: lxml 3.4.0.0, libxml2 2.9.1, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.9 (default, Jun 29 2016, 13:08:31) - [GCC 4.9.2], pyOpenSSL 0.14 (OpenSSL 1.0.1t\n3 May 2016), cryptography 0.6.1, Platform Linux-3.16.0-6-amd64-x86_64-with-debian-8.11\n2018-08-12 15:14:55 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dirbot.spiders', 'DOWNLOAD_MAXSIZE': 23554432, 'SPIDER_MODULES': ['dirbot.spiders'], 'CONCURRENT_REQUESTS': 128, 'DOWNLOAD_WARNSIZE': 0, 'DUPEFILTER_CLASS': 'dirbot.custom_filters.BLOOMDupeFilter', 'BOT_NAME': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36', 'AJAXCRAWL_ENABLED': True, 'DEPTH_PRIORITY': 1, 'COOKIES_ENABLED': False, 'DOWNLOAD_TIMEOUT': 15, 'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue', 'DNS_TIMEOUT': 15, 'LOG_ENABLED': False, 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue'}\nUnhandled error in Deferred:\nMy Spider :\nfrom __future__ import division\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\nfrom dirbot.settings import *\nfrom scrapy import signals\nimport time\nimport tldextract, json, pika, os, signal\n\nclass HttpbinSpider(CrawlSpider):\n    name = \"expired_one\"\n    rules = (Rule(LxmlLinkExtractor(allow=(), canonicalize=False), callback='parse_items', follow=True),)\n\n    blacklist = [...]\n\n    domains = ['http://www.website.com']\n\n    allowed_suffix = [...]\n\n    def __init__(self, domains=None, **kwargs):\n\n        self.connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n        self.channel = self.connection.channel()\n        self.channel.queue_declare(queue='expired', passive=False, durable=True, auto_delete=False)\n        self.channel.confirm_delivery()\n        #self.start_urls = json.loads(domains)\n     self.start_urls = ['http://www.website.com']\n        domain = json.loads(domains)\n        ext = tldextract.extract(domain[0])\n        self.allowed_domains = []\n        self.allowed_domains.append(ext.registered_domain)\n\n        dispatcher.connect(self.spider_closed, signals.spider_closed)\n        super(HttpbinSpider, self).__init__()\n\n    def parse_items(self, response):\n        for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains, canonicalize=False).extract_links(response):\n            ext = tldextract.extract(link.url)\n            domain = ext.registered_domain\n            if ext.suffix in self.allowed_suffix:\n                if domain not in self.domains and domain not in self.blacklist:\n                    self.connection.sleep(0.05)\n                    self.channel.basic_publish(exchange='', routing_key='expired', body=domain,\n                                               properties=pika.BasicProperties(\n                                                   delivery_mode=2,\n                                               ))\n                    self.domains.append(domain)\n\n    def spider_closed(self, spider):\n        self.connection.close()\n        pid = os.getpid()\n        os.kill(pid, signal.SIGTERM)", "issue_status": "Closed", "issue_reporting_time": "2018-08-12T15:25:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "318": {"issue_url": "https://github.com/scrapy/scrapy/issues/3382", "issue_id": "#3382", "issue_summary": "Contracts for FormRequest", "issue_description": "Contributor\nStasDeep commented on Aug 12, 2018\nAt the moment there's no way to create a contract for a callback that is passed a response from FormRequest.\nHow about something simple like this (ContractsManager.from_method):\nrequest = FormRequest(**kwargs) if 'formdata' in kwargs else Request(**kwargs)", "issue_status": "Closed", "issue_reporting_time": "2018-08-11T18:44:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "319": {"issue_url": "https://github.com/scrapy/scrapy/issues/3380", "issue_id": "#3380", "issue_summary": "scrapy check command ignores contracts with the same URL", "issue_description": "Contributor\nStasDeep commented on Aug 11, 2018\nDescription\nWhen testing callbacks with the same URL, only one callback is tested.\nReproduce\nclass DemoSpider(Spider):\n    name = 'demo_spider'\n\n    def returns_item_with_url(self, response):\n        \"\"\" method which returns request\n        @url http://scrapy.org\n        @returns items 1 1\n        \"\"\"\n        return TestItem(url=response.url)\n\n    def returns_item_with_name(self, response):\n        \"\"\" method which returns request\n        @url http://scrapy.org\n        @returns items 1 1\n        \"\"\"\n        return TestItem(name='scrapy')\nThen run scrapy check.\nYou'll get the following output:\n.\n----------------------------------------------------------------------\nRan 1 contract in 0.894s\n\nOK\nReason\nThis is default behavior for crawlers to filter same URLs.\nSolution\nUse dont_filter in requests returned by ContractsManager.", "issue_status": "Closed", "issue_reporting_time": "2018-08-11T16:23:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "320": {"issue_url": "https://github.com/scrapy/scrapy/issues/3378", "issue_id": "#3378", "issue_summary": "Cannot use contracts with inherited callbacks", "issue_description": "Contributor\nStasDeep commented on Aug 11, 2018\nDescription\nIf you want to scrapy check a spider that has inherited methods, these methods' contracts will be ignored.\nReproduce\nclass BaseSpider(Spider):\n\n    def returns_request(self, response):\n        \"\"\" method which returns request\n        @url https://docs.scrapy.org/en/latest/\n        @returns requests 1\n        \"\"\"\n        return Request('http://scrapy.org', callback=self.returns_item)\n\n\nclass DemoSpider(BaseSpider):\n    name = 'demo_spider'\nAnd then run scrapy check.\nYou'll get the following output:\n----------------------------------------------------------------------\nRan 0 contracts in 0.000s\n\nOK\nReason\nContractsManager.tested_methods_from_spidercls uses vars(spidercls).items() to get methods.\nSolution\nUse inspect.getmembers(spidercls) instead.", "issue_status": "Closed", "issue_reporting_time": "2018-08-11T14:09:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "321": {"issue_url": "https://github.com/scrapy/scrapy/issues/3376", "issue_id": "#3376", "issue_summary": "ValueError while opening scrapy shell on anaconda prompt", "issue_description": "Contributor\nmaramsumanth commented on Aug 10, 2018\nI am not able to open the url because of ValueError: invalid hostname: 'http error", "issue_status": "Closed", "issue_reporting_time": "2018-08-10T17:39:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "322": {"issue_url": "https://github.com/scrapy/scrapy/issues/3375", "issue_id": "#3375", "issue_summary": "Scrapy process less than succesfully crawled", "issue_description": "jogspokoen commented on Aug 10, 2018\nI have 2 problems with my scraper:\nIt get's a lot of 302s after a while, despite the fact I use 'COOKIES_ENABLED': False, and rotating proxy which should provide different IP for each request. I solved it by restarting scraper after several 302s\nI see that scraper successfully crawls much more than it process, and I can't do anything with it. In the example below I've got 121 200s responses, but only 27 was processed.\nclass MySpider(Spider):\n    name = 'MySpider'\n    custom_settings = {\n        'DOWNLOAD_DELAY': 0,\n        'RETRY_TIMES': 1,\n        'LOG_LEVEL': 'DEBUG',\n        'CLOSESPIDER_ERRORCOUNT': 3,\n        'COOKIES_ENABLED': False,\n    }\n    # I need to manually control when spider to stop, otherwise it runs forever\n    handle_httpstatus_list = [301, 302]\n\n    def start_requests(self):\n        for row in self.df.itertuples():\n            yield Request(\n                url=row.link,\n                callback=self.parse,\n                priority=100\n            )\n\n    def close(self, reason):\n        self.logger.info('TOTAL ADDED: %s' % self.added)\n\n    def parse(self, r):\n        if r.status == 302:\n            # I need to manually control when spider to stop, otherwise it runs forever\n            raise CloseSpider(\"\")\n        else:\n            # do parsing stuff\n            self.added += 1\n            self.logger.info('{} left'.format(len(self.df[self.df['status'] == 0])))\nOutput\n2018-08-08 12:24:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://mytarget.com/url1> (referer: None)\n2018-08-08 12:24:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://mytarget.com/url2> (referer: None)\n2018-08-08 12:24:24 [MySpider] INFO: 52451 left\n2018-08-08 12:24:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://mytarget.com/url3> (referer: None)\n2018-08-08 12:24:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://mytarget.com/url4> (referer: None)\n2018-08-08 12:24:24 [MySpider] INFO: 52450 left\n2018-08-08 12:24:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://mytarget.com/url4> (referer: None)\n\n\n2018-08-08 12:24:37 [MySpider] INFO: TOTAL ADDED: 27\n2018-08-08 12:24:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 1,\n...\n...\n 'downloader/response_status_count/200': 121,\n 'downloader/response_status_count/302': 4,\nIt succesfully crawls much (3x or 4x times more than crawls).\nHow can I force to process everything that was crawled? I can sacrifice the speed, but I don't want to waste what was succesfully crawled with 200, I want to process it!", "issue_status": "Closed", "issue_reporting_time": "2018-08-10T10:45:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "323": {"issue_url": "https://github.com/scrapy/scrapy/issues/3374", "issue_id": "#3374", "issue_summary": "DepthMiddleware", "issue_description": "zxpeter commented on Aug 10, 2018\nI want to modify some layer's depth in my scrapy request tree Because some requests is parallel in my program. When I'm using\n\"depth = response.meta['depth'] - 1 & request.meta['depth'] = depth\"\nor \"meta = {'depth': 1 # start from 1}\"\nIt didn't work.\nSo I checked the follow link and find the meta['depth'] is not a special key.\nhttps://doc.scrapy.org/en/latest/topics/request-response.html#request-meta-special-keys\nSo my question is request.meta['depth'] is set by DepthMiddleware, Can they be mess around and modified by myself ??\nThanks a lot!", "issue_status": "Closed", "issue_reporting_time": "2018-08-10T06:11:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "324": {"issue_url": "https://github.com/scrapy/scrapy/issues/3373", "issue_id": "#3373", "issue_summary": "Replace \"Using Firebug for scraping\" section is docs", "issue_description": "Member\nkmike commented on Aug 10, 2018\nI think https://doc.scrapy.org/en/latest/topics/firebug.html should be replaced with something else, maybe an example of scraping http://toscrape.com using SelectorGadget. Or maybe a rewritten version of this tutorial can become a part of CrawlSpider docs.\nFirebug no longer works (https://getfirebug.com/), example website no longer works, and this particular tutorial got very little love over the years - e.g. there is a junk text in the bottom of the page, this junk is introduced 9 years ago, and nobody took care of it.", "issue_status": "Closed", "issue_reporting_time": "2018-08-09T20:21:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "325": {"issue_url": "https://github.com/scrapy/scrapy/issues/3372", "issue_id": "#3372", "issue_summary": "Rewrite \"Using Firefox for scraping\" docs section", "issue_description": "Member\nkmike commented on Aug 10, 2018\n\"FF for scraping\" section (https://doc.scrapy.org/en/latest/topics/firefox.html) is outdated, e.g. none of the suggested \"Useful Firefox addons\" work. Firebug is discontinued, XPath Checker is removed, all other extensions don't work with recent Firefox.\nI think it should be re-written as \"Using Firefox developer tools for scraping\", or just \"using browser developer tools for scraping\" - all browsers have developer tools these days, and these developer tools work about the same.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-08-09T20:14:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "326": {"issue_url": "https://github.com/scrapy/scrapy/issues/3370", "issue_id": "#3370", "issue_summary": "AttributeError from contract errback", "issue_description": "Contributor\nStasDeep commented on Aug 9, 2018 \u2022\nedited\nWhen running a contract with a URL that returns non-200 response, I get the following:\n2018-08-09 14:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.bureauxlocaux.com/annonce/a-louer-bureaux-a-louer-a-nantes--1289-358662> (referer: None)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py\", line 653, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/usr/local/lib/python3.6/site-packages/scrapy/contracts/__init__.py\", line 89, in eb_wrapper\n    results.addError(case, exc_info)\n  File \"/usr/local/lib/python3.6/unittest/runner.py\", line 67, in addError\n    super(TextTestResult, self).addError(test, err)\n  File \"/usr/local/lib/python3.6/unittest/result.py\", line 17, in inner\n    return method(self, *args, **kw)\n  File \"/usr/local/lib/python3.6/unittest/result.py\", line 115, in addError\n    self.errors.append((test, self._exc_info_to_string(err, test)))\n  File \"/usr/local/lib/python3.6/unittest/result.py\", line 186, in _exc_info_to_string\n    exctype, value, tb, limit=length, capture_locals=self.tb_locals)\n  File \"/usr/local/lib/python3.6/traceback.py\", line 470, in __init__\n    exc_value.__cause__.__traceback__,\nAttributeError: 'getset_descriptor' object has no attribute '__traceback__'\nHere is how exc_info looks like:\n(HttpError('Ignoring non-200 response',), <class 'scrapy.spidermiddlewares.httperror.HttpError'>, <traceback object at 0x7f4bdca1d948>)", "issue_status": "Closed", "issue_reporting_time": "2018-08-09T14:47:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "327": {"issue_url": "https://github.com/scrapy/scrapy/issues/3369", "issue_id": "#3369", "issue_summary": "How to use double proxies(proxies+shadowsocks)", "issue_description": "waitfor2long commented on Aug 9, 2018 \u2022\nedited\nI am using scrapy to crawl some website, so I have to build a proxy pool, most of the proxies are working well but I can't directly connect to them due to the Chinese GFW, I tried to use shadowsocks(a socket5 proxy) as a middle proxy to make it work, but I failed, I tried Proxifier too, it doesn't work, I add python.exe to Proxifier and start crawling, it just show:\npython.exe (9012) *64 - 127.0.0.1:10661 colsed, 0 kb sent, 0 kb received, duration< 1 second\nit seems that the scrapy use twisted to handle the proxy and change the real target(proxy) to local address(127.0.0.1:10661)\nI am currently using scrapy+shadowsocks+tor(we can't directly connect to tor in China), it works well, but it is too slow.\nHow to make it work? scrapy+shadowsocks+proxy pool. By the way, I am using scrapy on windows 7 x64 and windows 10 x64", "issue_status": "Closed", "issue_reporting_time": "2018-08-09T08:22:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "328": {"issue_url": "https://github.com/scrapy/scrapy/issues/3368", "issue_id": "#3368", "issue_summary": "Setting downloader middler-wares in self-contained spiders", "issue_description": "medecau commented on Aug 7, 2018\nPython 3.6.6\nScrapy 1.5.1\nI'm trying to add the ROBOTS and HTTPCACHE middle-wares to a self-contained spider but for some reason these are not being added to the list of downloader middle-wares.\nI have tried generating this self-contained spider using the scrapy cli tool and the same thing happens. But when I create it using the project tree structure it does work. I have also tried for a few days to find this information on the docs but all attempts to set these values have failed.\nFor my setup I prefer to have as flat a folder structure with as few files as possible. What would then be the preferred way to activate the robots.txt and http-cache middle-wares in this circumstance?", "issue_status": "Closed", "issue_reporting_time": "2018-08-07T08:48:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "329": {"issue_url": "https://github.com/scrapy/scrapy/issues/3366", "issue_id": "#3366", "issue_summary": "Documentation: Why suggest only Learn Python The Hard Way?", "issue_description": "Contributor\ntestingcan commented on Aug 6, 2018\nI was curious as to why the documentation explicitely suggests Learn Python The Hard Way in the tutorial-section, especially since it links to the Python 2 version of the book.\nI certainly don't want to discredit the book as a learning resource, but since it has received quite some criticism before (and after) its Python 3 edition, I am skeptical of endorsing only this book in the tutorial.\nWithout cluttering the documentation and enabling some sort paralysis by analysis I would suggest adding either or both of Automate the Boring Stuff with Python or How To Think Like a Computer Scientist from the list of Python resources for non-programmers to the documentation directly instead of only through the link to the Python resources.", "issue_status": "Closed", "issue_reporting_time": "2018-08-06T14:35:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "330": {"issue_url": "https://github.com/scrapy/scrapy/issues/3364", "issue_id": "#3364", "issue_summary": "scrapy check exit code on exception", "issue_description": "Contributor\nStasDeep commented on Aug 6, 2018\nWhen I run scrapy check and a contract raises unhandled exception, the command exits with successful code 0.\n$ scrapy check $SPIDER_NAME\n... some stuff ...\nRan 0 contracts in 0.000s\nOK\n$ echo $?\n0\nIs this intended, or should I fix it?", "issue_status": "Closed", "issue_reporting_time": "2018-08-06T10:44:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "331": {"issue_url": "https://github.com/scrapy/scrapy/issues/3357", "issue_id": "#3357", "issue_summary": "Misleading error log when exporting to S3", "issue_description": "Contributor\nBurnzZ commented on Jul 31, 2018\nThere's this log message that was popping up when we intended to export data via S3:\nDisabled feed storage scheme: s3.\nThe problem was that the existing code is catching the NotConfigured exceptions:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/extensions/feedexport.py#L272\nThis occurs when either of botocore or boto libs aren't installed and the user is attempting to export via S3 (set via FEED_URI scheme): https://github.com/scrapy/scrapy/blob/master/scrapy/utils/boto.py#L19\nThis leads to a few hour of head scratching figuring out why it was disabled. A better error log would be along the lines of:\nDisabled feed storage scheme: s3. Reason: missing botocore library\nSteps to Reproduce:\nOpen a new shell without botocore installed/declared in it\nRun: scrapy shell -s FEED_URI='s3://test.com'\n2018-07-31 18:56:20 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n2018-07-31 18:56:20 [scrapy.utils.log] INFO: Versions: lxml 4.2.3.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.15 (default, May  1 2018, 16:44:37) - [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.3, Platform Darwin-16.7.0-x86_64-i386-64bit\n2018-07-31 18:56:20 [scrapy.crawler] INFO: Overridden settings: {'FEED_URI': 's3://test.com', 'LOGSTATS_INTERVAL': 0, 'EDITOR': 'vim', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2018-07-31 18:56:20 [scrapy.extensions.feedexport] ERROR: Disabled feed storage scheme: s3\n\n<more output truncated>", "issue_status": "Closed", "issue_reporting_time": "2018-07-31T11:07:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "332": {"issue_url": "https://github.com/scrapy/scrapy/issues/3356", "issue_id": "#3356", "issue_summary": "tried to install scrapy on anaconda 2 problems.", "issue_description": "leokingai commented on Jul 30, 2018\nfollowing the instruction. get 2 errors.\n1 is said pip need upgrade. 2 Microsoft Visual C++ 14.0 is required.\nhow to deal with them?", "issue_status": "Closed", "issue_reporting_time": "2018-07-30T08:17:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "333": {"issue_url": "https://github.com/scrapy/scrapy/issues/3355", "issue_id": "#3355", "issue_summary": "Getting handle of original request in process_request", "issue_description": "Karan-GM commented on Jul 27, 2018\nI am unable to get hold of original request URL in process_request callable of the rule in crawlspider.\nIs there any way I can achieve this?", "issue_status": "Closed", "issue_reporting_time": "2018-07-27T12:47:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "334": {"issue_url": "https://github.com/scrapy/scrapy/issues/3353", "issue_id": "#3353", "issue_summary": "spider_closed can't fire", "issue_description": "pcwang0205 commented on Jul 26, 2018\nwhen I tap ctrl + C to stop spider\uff0cbut I can't get the callback of spider_closed.", "issue_status": "Closed", "issue_reporting_time": "2018-07-26T02:38:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "335": {"issue_url": "https://github.com/scrapy/scrapy/issues/3352", "issue_id": "#3352", "issue_summary": "Add logging to HttpProxyMiddleware", "issue_description": "Contributor\nMatthijsy commented on Jul 25, 2018\nCurrently there is no logging in the HttpProxyMiddleware. It is not clear whether a spider is using a proxy or not. I think it would be great to do a info log on initialization which informs that a proxy is used and which one is used.\nI am interested if more people face this problem and what your opinion is about this.", "issue_status": "Closed", "issue_reporting_time": "2018-07-25T10:59:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "336": {"issue_url": "https://github.com/scrapy/scrapy/issues/3351", "issue_id": "#3351", "issue_summary": "How can I use jupyter notebook instead of Ipython or Bpython", "issue_description": "javenleeCH commented on Jul 25, 2018\nHow can I use jupyter notebook instead of Ipython or Bpython\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2018-07-25T07:27:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "337": {"issue_url": "https://github.com/scrapy/scrapy/issues/3349", "issue_id": "#3349", "issue_summary": "Race condition in CsvItemExporter from exporters.py", "issue_description": "medvedev1088 commented on Jul 23, 2018\nIn the below piece of code when _headers_not_written is checked a race condition happens which results in headers being written in the middle of the file (rows are written in another thread):\ndef export_item(self, item):\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value='',\n                                             include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)", "issue_status": "Closed", "issue_reporting_time": "2018-07-23T17:10:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "338": {"issue_url": "https://github.com/scrapy/scrapy/issues/3344", "issue_id": "#3344", "issue_summary": "Unknown Command when I enter \"scrapy crawl quotes\"", "issue_description": "roxybilson commented on Jul 19, 2018\nHI I AM NEW TO SCRAPY, AND IM HAVING AN ISSUE. CAN SOMEONE HELP PLEASE?\nWhen I enter \"scrapy crawl quotes\" on terminal, it returns \"Unknown command: crawl\"\nDoes anyone know what could be the reason? I followed everything from the tutorial but except that part was when it was incorrect.", "issue_status": "Closed", "issue_reporting_time": "2018-07-19T16:18:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "339": {"issue_url": "https://github.com/scrapy/scrapy/issues/3341", "issue_id": "#3341", "issue_summary": "Overriding the MailSender class", "issue_description": "nadzimo commented on Jul 19, 2018\nI'd like to use the built-in email notification service for when a scraper exceeds a certain memory limit (MEMUSAGE_NOTIFY_MAIL setting), but it looks like it's not possible to specify the MailSender class to use to send the email. I don't want to use SMTP, I'd like to use a third-party mail sender (e.g. sendgrid).\nIs there a way around this?\nThanks", "issue_status": "Closed", "issue_reporting_time": "2018-07-19T12:52:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "340": {"issue_url": "https://github.com/scrapy/scrapy/issues/3340", "issue_id": "#3340", "issue_summary": "If callback=None as a callback function we will use current function", "issue_description": "amarynets commented on Jul 17, 2018 \u2022\nedited\nIf we will make a request like this\ndef parse(self, response):\n    # some code\n    yield scrapy.Request(url, callback=self.None, meta={'foo': bar}, dont_filter=True)\nWe will not get an exception or something like that. The spider will just make a request and use the same function for parsing.\nIs it a correct logic? Because I didn't find it in the documentation", "issue_status": "Closed", "issue_reporting_time": "2018-07-17T10:12:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "341": {"issue_url": "https://github.com/scrapy/scrapy/issues/3339", "issue_id": "#3339", "issue_summary": "When I use scrapy-redis for crawlers, and manually add redis_key", "issue_description": "wybigsea9 commented on Jul 17, 2018\nScrapy_redis: when I run two crawlers at the same time, redis_key is passed to one of the crawlers, and ideally the other crawler won't start, but it starts up and reports an error.\n[scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\nUnhandled Error\nTraceback (most recent call last):\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 58, in run\nself.crawler_process.start()\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 291, in start\nreactor.run(installSignalHandlers=False) # blocking call\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 1261, in run\nself.mainLoop()\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 1270, in mainLoop\nself.runUntilCurrent()\n--- ---\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 896, in runUntilCurrent\ncall.func(*call.args, **call.kw)\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\utils\\reactor.py\", line 41, in call\nreturn self._func(*self._a, **self._kw)\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 122, in _next_request\nif not self._next_request_from_scheduler(spider):\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 149, in _next_request_from_scheduler\nrequest = slot.scheduler.next_request()\nFile \"c:\\python27\\lib\\site-packages\\scrapy_redis\\scheduler.py\", line 172, in next_request\nrequest = self.queue.pop(block_pop_timeout)\nFile \"c:\\python27\\lib\\site-packages\\scrapy_redis\\queue.py\", line 117, in pop\nreturn self._decode_request(results[0])\nFile \"c:\\python27\\lib\\site-packages\\scrapy_redis\\queue.py\", line 48, in _decode_request\nreturn request_from_dict(obj, self.spider)\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\utils\\reqser.py\", line 50, in request_from_dict\ncb = _get_method(spider, cb)\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\utils\\reqser.py\", line 87, in _get_method\nraise ValueError(\"Method %r not found in: %s\" % (name, obj))\nexceptions.ValueError: Method 'parse_item' not found in: <SpiderSpider 'spider' at 0x59cb710>\nThe crawler project is written The data from The yield link in The parse function to The parse_item function\nI don't know how to make this mistake, please help me to see it\nScrapy 1.5.0 started (bot: chongqingribao)\n[scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h 27 Mar 2018), cryptography 2.2.2, Platform Windows-8-6.2.9200", "issue_status": "Closed", "issue_reporting_time": "2018-07-17T08:02:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "342": {"issue_url": "https://github.com/scrapy/scrapy/issues/3338", "issue_id": "#3338", "issue_summary": "can't specify relative path with 'file' feed scheme", "issue_description": "WaferJay commented on Jul 16, 2018\nHello. I specify FEED_URI:FEED_URI = \"file://feeds/%(name)s/%(time)s.json\" and running the scrapy project with root user. But the feed file saved in \"/%(name)s/%(time)s.json\"\nI found this and I try it in Python Shell.\nscrapy/scrapy/extensions/feedexport.py\nLine 82 in dfe6d3d\n self.path = file_uri_to_path(uri) \nIn [1]: from w3lib.url import file_uri_to_path\nIn [2]: file_uri_to_path(\"file://test/file.txt\")\nOut[2]: '/file.txt'\nBy the way, my w3lib module is v1.18.0.\nAlthough, I can write without 'file://' prefix like this to avoid that problem.\nFEED_URI = \"test/file.txt\"\nI'm so sorry. My English is pool.", "issue_status": "Closed", "issue_reporting_time": "2018-07-16T15:01:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "343": {"issue_url": "https://github.com/scrapy/scrapy/issues/3333", "issue_id": "#3333", "issue_summary": "The size of requests.queue may be wrong when resuming crawl from unclean shutdown.", "issue_description": "my8100 commented on Jul 12, 2018 \u2022\nedited\nAs we can see from the code of class LifoDiskQueue, when forcing unclean shutdown, the method close() may not be executed. And scrapy would wrongly take the previous size as self.size in the method init() when resuming crawl.\nsite-packages/queuelib/queue.py\nclass LifoDiskQueue(object):\n    \"\"\"Persistent LIFO queue.\"\"\"\n\n    SIZE_FORMAT = \">L\"\n    SIZE_SIZE = struct.calcsize(SIZE_FORMAT)\n\n    def __init__(self, path):\n        self.path = path\n        if os.path.exists(path):\n            self.f = open(path, 'rb+')\n            qsize = self.f.read(self.SIZE_SIZE)\n            self.size, = struct.unpack(self.SIZE_FORMAT, qsize)\n            self.f.seek(0, os.SEEK_END)\n        else:\n            self.f = open(path, 'wb+')\n            self.f.write(struct.pack(self.SIZE_FORMAT, 0))\n            self.size = 0\n            \n    ...\n\n    def close(self):\n        if self.size:\n            self.f.seek(0)\n            self.f.write(struct.pack(self.SIZE_FORMAT, self.size))\n        self.f.close()\n        if not self.size:\n            os.remove(self.path)\nMy own solution is overriding the method init() of class PickleLifoDiskQueue, where the bytes indicating the size of requests.queue would be set to 0 purposely right after the size is read.\nAs a result, the method init() would update the actual size of requests.queue when needed when resuming crawl.\nUpdate settings.py of a project:\nSCHEDULER_DISK_QUEUE = 'my_squeues.MyPickleLifoDiskQueue'\nsite-packages/my_squeues.py\n# -*- coding: utf-8 -*-\nimport os\nimport struct\nimport logging\n\nfrom scrapy.squeues import PickleLifoDiskQueue\n\nlogger = logging.getLogger(__name__)\n\n\n# SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'\nclass MyPickleLifoDiskQueue(PickleLifoDiskQueue):\n    \"\"\"Persistent LIFO queue.\"\"\"\n\n    SIZE_FORMAT = \">L\"\n    SIZE_SIZE = struct.calcsize(SIZE_FORMAT)\n\n    # Override the method __init__() of class queue.LifoDiskQueue\n    def __init__(self, path):\n        self.path = path\n        if os.path.exists(path):\n            self.f = open(path, 'rb+')\n            qsize = self.f.read(self.SIZE_SIZE)\n            self.size, = struct.unpack(self.SIZE_FORMAT, qsize)\n\n            # Set to 0 purposely\n            self.f.seek(0)\n            self.f.write(struct.pack(self.SIZE_FORMAT, 0))\n\n            self.f.seek(0, os.SEEK_END)\n\n            ori_size = self.size\n            if self.size == 0:\n                while True:\n                    if self.f.tell() == self.SIZE_SIZE:\n                        break\n                    self.f.seek(-self.SIZE_SIZE, os.SEEK_CUR)\n                    size, = struct.unpack(self.SIZE_FORMAT, self.f.read(self.SIZE_SIZE))\n                    self.f.seek(-size-self.SIZE_SIZE, os.SEEK_CUR)\n                    self.size += 1\n                self.f.seek(0, os.SEEK_END)\n                logger.info('%s FIX size from %s to %s'%(self.path, ori_size, self.size))\n            else:\n                logger.info('%s ori_size: %s'%(self.path, ori_size))\n\n        else:\n            self.f = open(path, 'wb+')\n            self.f.write(struct.pack(self.SIZE_FORMAT, 0))\n            self.size = 0\nScrapy log when resuming crawl from normal shutdown:\n2018-07-12 16:43:11 [scrapy.core.engine] INFO: Spider opened\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p0 ori_size: 25\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-1 ori_size: 25\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-2 ori_size: 26\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-3 ori_size: 25\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-4 ori_size: 25\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-5 ori_size: 25\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-6 ori_size: 25\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-7 ori_size: 25\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-8 ori_size: 16\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-9 ori_size: 1\n2018-07-12 16:43:11 [my_squeues] INFO: crawls/test\\requests.queue\\p-11 ori_size: 1\n2018-07-12 16:43:11 [scrapy.core.scheduler] INFO: Resuming crawl (219 requests scheduled)\nScrapy log when resuming crawl from unclean shutdown:\n2018-07-12 16:43:18 [scrapy.core.engine] INFO: Spider opened\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p0 FIX size from 0 to 25\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p-1 FIX size from 0 to 25\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p-2 FIX size from 0 to 26\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p-3 FIX size from 0 to 25\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p-4 FIX size from 0 to 25\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p-5 FIX size from 0 to 25\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p-6 FIX size from 0 to 25\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p-7 FIX size from 0 to 25\n2018-07-12 16:43:18 [my_squeues] INFO: crawls/test\\requests.queue\\p-8 FIX size from 0 to 15\n2018-07-12 16:43:18 [scrapy.core.scheduler] INFO: Resuming crawl (216 requests scheduled)", "issue_status": "Closed", "issue_reporting_time": "2018-07-12T04:09:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "344": {"issue_url": "https://github.com/scrapy/scrapy/issues/3331", "issue_id": "#3331", "issue_summary": "Documentation example fails with `proxy URL with no authority`", "issue_description": "a-palchikov commented on Jul 11, 2018 \u2022\nedited\nRunning the example from the documentation yields this:\n10:11 $ scrapy runspider quotes.py \n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n2018-07-11 10:12:04 [scrapy.utils.log] INFO: Versions: lxml 3.5.0.0, libxml2 2.9.3, cssselect 0.9.1, parsel 1.5.0, w3lib 1.19.0, Twisted 16.0.0, Python 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609], pyOpenSSL 0.15.1 (OpenSSL 1.0.2g  1 Mar 2016), cryptography 1.2.3, Platform Linux-4.4.0-130-generic-x86_64-with-Ubuntu-16.04-xenial\n2018-07-11 10:12:04 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\n2018-07-11 10:12:04 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\nUnhandled error in Deferred:\n2018-07-11 10:12:04 [twisted] CRITICAL: Unhandled error in Deferred:\n\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/runspider.py\", line 88, in run\n    self.crawler_process.crawl(spidercls, **opts.spargs)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 171, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 175, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1274, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 98, in crawl\n    six.reraise(*exc_info)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 80, in crawl\n    self.engine = self._create_engine()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 105, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 69, in __init__\n    self.downloader = downloader_cls(crawler)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/__init__.py\", line 88, in __init__\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 36, in from_settings\n    mw = mwcls.from_crawler(crawler)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 29, in from_crawler\n    return cls(auth_encoding)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 22, in __init__\n    self.proxies[type] = self._get_proxy(url, type)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/downloadermiddlewares/httpproxy.py\", line 39, in _get_proxy\n    proxy_type, user, password, hostport = _parse_proxy(url)\n  File \"/usr/lib/python2.7/urllib2.py\", line 721, in _parse_proxy\n    raise ValueError(\"proxy URL with no authority: %r\" % proxy)\nexceptions.ValueError: proxy URL with no authority: '/var/run/docker.sock'\n2018-07-11 10:12:04 [twisted] CRITICAL:\nLooks like proxy code does not handle no_proxy correctly.", "issue_status": "Closed", "issue_reporting_time": "2018-07-11T08:15:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "345": {"issue_url": "https://github.com/scrapy/scrapy/issues/3330", "issue_id": "#3330", "issue_summary": "pip3 install scrapy failed with python3.7", "issue_description": "cc-alvin commented on Jul 10, 2018\nI use pip3 to install scrapy with python3.7.But it failed.\nIt showed that the twisted install failed.\nI am not sure about if scrapy can works with python3.7.\nPlease. Thank you!", "issue_status": "Closed", "issue_reporting_time": "2018-07-10T13:38:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "346": {"issue_url": "https://github.com/scrapy/scrapy/issues/3325", "issue_id": "#3325", "issue_summary": "Python 3.7 and Scrapy incompatible", "issue_description": "jstnms123 commented on Jul 9, 2018\nThis kind of crap is unacceptable. Python3.7 team should remove their collective head from their collective butt. (keyword mismatch)\njA$ scrapy shell\n2018-07-08 14:42:44 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n2018-07-08 14:42:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.3.0, libxml2 2.9.2, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.7.0 (default, Jun 29 2018, 20:13:53) - [Clang 8.0.0 (clang-800.0.42.1)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h 27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.6.0-x86_64-i386-64bit\n2018-07-08 14:42:44 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 150, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\nfunc(*a, **kw)\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 157, in _run_command\ncmd.run(args, opts)\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 65, in run\ncrawler = self.crawler_process._create_crawler(spidercls)\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/crawler.py\", line 203, in _create_crawler\nreturn Crawler(spidercls, self.settings)\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/crawler.py\", line 55, in init\nself.extensions = ExtensionManager.from_crawler(self)\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/middleware.py\", line 34, in from_settings\nmwcls = load_object(clspath)\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/utils/misc.py\", line 44, in load_object\nmod = import_module(module)\nFile \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/init.py\", line 127, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 1006, in _gcd_import\nFile \"\", line 983, in _find_and_load\nFile \"\", line 967, in _find_and_load_unlocked\nFile \"\", line 677, in _load_unlocked\nFile \"\", line 728, in exec_module\nFile \"\", line 219, in _call_with_frames_removed\nFile \"/usr/local/lib/python3.7/site-packages/scrapy/extensions/telnet.py\", line 12, in\nfrom twisted.conch import manhole, telnet\nFile \"/usr/local/lib/python3.7/site-packages/twisted/conch/manhole.py\", line 154\ndef write(self, data, async=False):\n^\nSyntaxError: invalid syntax", "issue_status": "Closed", "issue_reporting_time": "2018-07-08T21:45:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "347": {"issue_url": "https://github.com/scrapy/scrapy/issues/3324", "issue_id": "#3324", "issue_summary": "Official support for 3.7", "issue_description": "jslay88 commented on Jul 8, 2018\nWith the public release of Python 3.7, it would be nice to have it 'officially' supported by scrapy, and have tox test for it as well. This would allow 3.7 production environments to 'safely' use scrapy.", "issue_status": "Closed", "issue_reporting_time": "2018-07-08T15:40:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "348": {"issue_url": "https://github.com/scrapy/scrapy/issues/3323", "issue_id": "#3323", "issue_summary": "Quick 1.5.1 release tag?", "issue_description": "Contributor\nnyov commented on Jul 7, 2018\nI was wondering if you guys could do a 1.5.x release on 8be28fe (or later) so debian can have a fix for #891725 while waiting on 1.6 ?", "issue_status": "Closed", "issue_reporting_time": "2018-07-07T16:08:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "349": {"issue_url": "https://github.com/scrapy/scrapy/issues/3322", "issue_id": "#3322", "issue_summary": "Installation Error", "issue_description": "teenageknight commented on Jul 7, 2018\nI tried to install Scrapy using the recommended way...\npython -m pip install scrapy\nThere were a few errors with dependencies not being installed but I installed them. However, the twisted dependency seems to be causing a problem. I'm just a beginner but i wanted to check out some python modules.\nUsing windows 10, python 3.7\nThe following is what happens when i try to install\nC:\\Windows\\system32>python -m pip install scrapy\nCollecting scrapy\nUsing cached https://files.pythonhosted.org/packages/db/9c/cb15b2dc6003a805afd21b9b396e0e965800765b51da72fe17cf340b9be2/Scrapy-1.5.0-py2.py3-none-any.whl\nRequirement already satisfied: lxml in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (4.2.3)\nRequirement already satisfied: w3lib>=1.17.0 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (1.19.0)\nCollecting Twisted>=13.1.0 (from scrapy)\nUsing cached https://files.pythonhosted.org/packages/12/2a/e9e4fb2e6b2f7a75577e0614926819a472934b0b85f205ba5d5d2add54d0/Twisted-18.4.0.tar.bz2\nRequirement already satisfied: service-identity in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (17.0.0)\nRequirement already satisfied: six>=1.5.2 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (1.11.0)\nRequirement already satisfied: cssselect>=0.9 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (1.0.3)\nRequirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (2.0.5)\nRequirement already satisfied: queuelib in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (1.5.0)\nRequirement already satisfied: pyOpenSSL in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (18.0.0)\nRequirement already satisfied: parsel>=1.1 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from scrapy) (1.5.0)\nRequirement already satisfied: zope.interface>=4.4.2 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (4.5.0)\nRequirement already satisfied: constantly>=15.1 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (15.1.0)\nRequirement already satisfied: incremental>=16.10.1 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (17.5.0)\nRequirement already satisfied: Automat>=0.3.0 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (0.7.0)\nRequirement already satisfied: hyperlink>=17.1.1 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from Twisted>=13.1.0->scrapy) (18.0.0)\nRequirement already satisfied: attrs in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from service-identity->scrapy) (18.1.0)\nRequirement already satisfied: pyasn1-modules in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from service-identity->scrapy) (0.2.2)\nRequirement already satisfied: pyasn1 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from service-identity->scrapy) (0.4.3)\nRequirement already satisfied: cryptography>=2.2.1 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from pyOpenSSL->scrapy) (2.2.2)\nRequirement already satisfied: setuptools in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->scrapy) (39.0.1)\nRequirement already satisfied: idna>=2.5 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=13.1.0->scrapy) (2.7)\nRequirement already satisfied: asn1crypto>=0.21.0 in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (0.24.0)\nRequirement already satisfied: cffi>=1.7; platform_python_implementation != \"PyPy\" in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from cryptography>=2.2.1->pyOpenSSL->scrapy) (1.11.5)\nRequirement already satisfied: pycparser in c:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from cffi>=1.7; platform_python_implementation != \"PyPy\"->cryptography>=2.2.1->pyOpenSSL->scrapy) (2.18)\nInstalling collected packages: Twisted, scrapy\nRunning setup.py install for Twisted ... error\nComplete output from command C:\\Users\\bkaja\\AppData\\Local\\Programs\\Python\\Python37-32\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\bkaja\\AppData\\Local\\Temp\\pip-install-vrxv__11\\Twisted\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\Users\\bkaja\\AppData\\Local\\Temp\\pip-record-fnu_lwpt\\install-record.txt --single-version-externally-managed --compile:\nrunning install\nrunning build\nrunning build_py\ncreating build\ncreating build\\lib.win32-3.7\ncreating build\\lib.win32-3.7\\twisted\ncopying src\\twisted\\copyright.py -> build\\lib.win32-3.7\\twisted\ncopying src\\twisted\\plugin.py -> build\\lib.win32-3.7\\twisted\ncopying src\\twisted_version.py -> build\\lib.win32-3.7\\twisted\ncopying src\\twisted_init_.py -> build\\lib.win32-3.7\\twisted\ncopying src\\twisted_main_.py -> build\\lib.win32-3.7\\twisted\ncreating build\\lib.win32-3.7\\twisted\\application\ncopying src\\twisted\\application\\app.py -> build\\lib.win32-3.7\\twisted\\application\ncopying src\\twisted\\application\\internet.py -> build\\lib.win32-3.7\\twisted\\application\ncopying src\\twisted\\application\\reactors.py -> build\\lib.win32-3.7\\twisted\\application\ncopying src\\twisted\\application\\service.py -> build\\lib.win32-3.7\\twisted\\application\ncopying src\\twisted\\application\\strports.py -> build\\lib.win32-3.7\\twisted\\application\ncopying src\\twisted\\application_init_.py -> build\\lib.win32-3.7\\twisted\\application\ncreating build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\avatar.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\checkers.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\endpoints.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\error.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\interfaces.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\ls.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\manhole.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\manhole_ssh.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\manhole_tap.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\mixin.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\recvline.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\stdio.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\tap.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\telnet.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\ttymodes.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch\\unix.py -> build\\lib.win32-3.7\\twisted\\conch\ncopying src\\twisted\\conch_init_.py -> build\\lib.win32-3.7\\twisted\\conch\ncreating build\\lib.win32-3.7\\twisted\\cred\ncopying src\\twisted\\cred\\checkers.py -> build\\lib.win32-3.7\\twisted\\cred\ncopying src\\twisted\\cred\\credentials.py -> build\\lib.win32-3.7\\twisted\\cred\ncopying src\\twisted\\cred\\error.py -> build\\lib.win32-3.7\\twisted\\cred\ncopying src\\twisted\\cred\\portal.py -> build\\lib.win32-3.7\\twisted\\cred\ncopying src\\twisted\\cred\\strcred.py -> build\\lib.win32-3.7\\twisted\\cred\ncopying src\\twisted\\cred_digest.py -> build\\lib.win32-3.7\\twisted\\cred\ncopying src\\twisted\\cred_init_.py -> build\\lib.win32-3.7\\twisted\\cred\ncreating build\\lib.win32-3.7\\twisted\\enterprise\ncopying src\\twisted\\enterprise\\adbapi.py -> build\\lib.win32-3.7\\twisted\\enterprise\ncopying src\\twisted\\enterprise_init_.py -> build\\lib.win32-3.7\\twisted\\enterprise\ncreating build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\abstract.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\address.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\asyncioreactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\base.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\cfreactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\default.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\defer.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\endpoints.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\epollreactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\error.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\fdesc.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\gireactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\glib2reactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\gtk2reactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\gtk3reactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\inotify.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\interfaces.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\kqreactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\main.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\pollreactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\posixbase.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\process.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\protocol.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\pyuisupport.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\reactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\selectreactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\serialport.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\ssl.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\stdio.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\task.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\tcp.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\threads.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\tksupport.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\udp.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\unix.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\utils.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\win32eventreactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\wxreactor.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet\\wxsupport.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_baseprocess.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_dumbwin32proc.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_glibbase.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_idna.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_newtls.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_pollingfile.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_posixserialport.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_posixstdio.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_producer_helpers.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_resolver.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_signals.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_sslverify.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_threadedselect.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_win32serialport.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_win32stdio.py -> build\\lib.win32-3.7\\twisted\\internet\ncopying src\\twisted\\internet_init_.py -> build\\lib.win32-3.7\\twisted\\internet\ncreating build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_buffer.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_file.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_filter.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_flatten.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_format.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_global.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_io.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_json.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_legacy.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_levels.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_logger.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_observer.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_stdlib.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_util.py -> build\\lib.win32-3.7\\twisted\\logger\ncopying src\\twisted\\logger_init_.py -> build\\lib.win32-3.7\\twisted\\logger\ncreating build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail\\imap4.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail\\interfaces.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail\\pop3.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail\\pop3client.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail\\protocols.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail\\relay.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail\\smtp.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail_cred.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail_except.py -> build\\lib.win32-3.7\\twisted\\mail\ncopying src\\twisted\\mail_init_.py -> build\\lib.win32-3.7\\twisted\\mail\ncreating build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\authority.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\cache.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\client.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\common.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\dns.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\error.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\hosts.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\resolve.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\root.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\secondary.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\server.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\srvconnect.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names\\tap.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names_rfc1982.py -> build\\lib.win32-3.7\\twisted\\names\ncopying src\\twisted\\names_init_.py -> build\\lib.win32-3.7\\twisted\\names\ncreating build\\lib.win32-3.7\\twisted\\pair\ncopying src\\twisted\\pair\\ethernet.py -> build\\lib.win32-3.7\\twisted\\pair\ncopying src\\twisted\\pair\\ip.py -> build\\lib.win32-3.7\\twisted\\pair\ncopying src\\twisted\\pair\\raw.py -> build\\lib.win32-3.7\\twisted\\pair\ncopying src\\twisted\\pair\\rawudp.py -> build\\lib.win32-3.7\\twisted\\pair\ncopying src\\twisted\\pair\\testing.py -> build\\lib.win32-3.7\\twisted\\pair\ncopying src\\twisted\\pair\\tuntap.py -> build\\lib.win32-3.7\\twisted\\pair\ncopying src\\twisted\\pair_init_.py -> build\\lib.win32-3.7\\twisted\\pair\ncreating build\\lib.win32-3.7\\twisted\\persisted\ncopying src\\twisted\\persisted\\aot.py -> build\\lib.win32-3.7\\twisted\\persisted\ncopying src\\twisted\\persisted\\crefutil.py -> build\\lib.win32-3.7\\twisted\\persisted\ncopying src\\twisted\\persisted\\dirdbm.py -> build\\lib.win32-3.7\\twisted\\persisted\ncopying src\\twisted\\persisted\\sob.py -> build\\lib.win32-3.7\\twisted\\persisted\ncopying src\\twisted\\persisted\\styles.py -> build\\lib.win32-3.7\\twisted\\persisted\ncopying src\\twisted\\persisted_init_.py -> build\\lib.win32-3.7\\twisted\\persisted\ncreating build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\cred_anonymous.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\cred_file.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\cred_memory.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\cred_sshkeys.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\cred_unix.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_conch.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_core.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_ftp.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_inet.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_names.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_portforward.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_reactors.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_runner.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_socks.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_trial.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_web.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins\\twisted_words.py -> build\\lib.win32-3.7\\twisted\\plugins\ncopying src\\twisted\\plugins_init_.py -> build\\lib.win32-3.7\\twisted\\plugins\ncreating build\\lib.win32-3.7\\twisted\\positioning\ncopying src\\twisted\\positioning\\base.py -> build\\lib.win32-3.7\\twisted\\positioning\ncopying src\\twisted\\positioning\\ipositioning.py -> build\\lib.win32-3.7\\twisted\\positioning\ncopying src\\twisted\\positioning\\nmea.py -> build\\lib.win32-3.7\\twisted\\positioning\ncopying src\\twisted\\positioning_sentence.py -> build\\lib.win32-3.7\\twisted\\positioning\ncopying src\\twisted\\positioning_init_.py -> build\\lib.win32-3.7\\twisted\\positioning\ncreating build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\amp.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\basic.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\dict.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\finger.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\ftp.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\htb.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\ident.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\loopback.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\memcache.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\pcp.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\policies.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\portforward.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\postfix.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\sip.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\socks.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\stateful.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\tls.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols\\wire.py -> build\\lib.win32-3.7\\twisted\\protocols\ncopying src\\twisted\\protocols_init_.py -> build\\lib.win32-3.7\\twisted\\protocols\ncreating build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\compat.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\components.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\constants.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\context.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\deprecate.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\failure.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\fakepwd.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\filepath.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\formmethod.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\htmlizer.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\lockfile.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\log.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\logfile.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\modules.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\monkey.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\procutils.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\randbytes.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\rebuild.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\reflect.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\release.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\roots.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\runtime.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\sendmsg.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\shortcut.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\syslog.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\systemd.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\text.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\threadable.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\threadpool.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\url.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\urlpath.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\usage.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\util.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\versions.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\win32.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\zippath.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\zipstream.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_appdirs.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_inotify.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_oldstyle.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_release.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_setup.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_shellcomp.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_textattributes.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_tzhelper.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_url.py -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python_init_.py -> build\\lib.win32-3.7\\twisted\\python\ncreating build\\lib.win32-3.7\\twisted\\runner\ncopying src\\twisted\\runner\\inetd.py -> build\\lib.win32-3.7\\twisted\\runner\ncopying src\\twisted\\runner\\inetdconf.py -> build\\lib.win32-3.7\\twisted\\runner\ncopying src\\twisted\\runner\\inetdtap.py -> build\\lib.win32-3.7\\twisted\\runner\ncopying src\\twisted\\runner\\procmon.py -> build\\lib.win32-3.7\\twisted\\runner\ncopying src\\twisted\\runner\\procmontap.py -> build\\lib.win32-3.7\\twisted\\runner\ncopying src\\twisted\\runner_init_.py -> build\\lib.win32-3.7\\twisted\\runner\ncreating build\\lib.win32-3.7\\twisted\\scripts\ncopying src\\twisted\\scripts\\htmlizer.py -> build\\lib.win32-3.7\\twisted\\scripts\ncopying src\\twisted\\scripts\\trial.py -> build\\lib.win32-3.7\\twisted\\scripts\ncopying src\\twisted\\scripts\\twistd.py -> build\\lib.win32-3.7\\twisted\\scripts\ncopying src\\twisted\\scripts_twistd_unix.py -> build\\lib.win32-3.7\\twisted\\scripts\ncopying src\\twisted\\scripts_twistw.py -> build\\lib.win32-3.7\\twisted\\scripts\ncopying src\\twisted\\scripts_init_.py -> build\\lib.win32-3.7\\twisted\\scripts\ncreating build\\lib.win32-3.7\\twisted\\spread\ncopying src\\twisted\\spread\\banana.py -> build\\lib.win32-3.7\\twisted\\spread\ncopying src\\twisted\\spread\\flavors.py -> build\\lib.win32-3.7\\twisted\\spread\ncopying src\\twisted\\spread\\interfaces.py -> build\\lib.win32-3.7\\twisted\\spread\ncopying src\\twisted\\spread\\jelly.py -> build\\lib.win32-3.7\\twisted\\spread\ncopying src\\twisted\\spread\\pb.py -> build\\lib.win32-3.7\\twisted\\spread\ncopying src\\twisted\\spread\\publish.py -> build\\lib.win32-3.7\\twisted\\spread\ncopying src\\twisted\\spread\\util.py -> build\\lib.win32-3.7\\twisted\\spread\ncopying src\\twisted\\spread_init_.py -> build\\lib.win32-3.7\\twisted\\spread\ncreating build\\lib.win32-3.7\\twisted\\tap\ncopying src\\twisted\\tap\\ftp.py -> build\\lib.win32-3.7\\twisted\\tap\ncopying src\\twisted\\tap\\portforward.py -> build\\lib.win32-3.7\\twisted\\tap\ncopying src\\twisted\\tap\\socks.py -> build\\lib.win32-3.7\\twisted\\tap\ncopying src\\twisted\\tap_init_.py -> build\\lib.win32-3.7\\twisted\\tap\ncreating build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\crash_test_dummy.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\iosim.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\mock_win32process.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\myrebuilder1.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\myrebuilder2.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\plugin_basic.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\plugin_extra1.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\plugin_extra2.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_cmdline.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_echoer.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_fds.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_getargv.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_getenv.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_linger.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_reader.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_signal.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_stdinreader.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_tester.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_tty.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\process_twisted.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\proto_helpers.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\reflect_helper_IE.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\reflect_helper_VE.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\reflect_helper_ZDE.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\ssl_helpers.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\stdio_test_consumer.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\stdio_test_halfclose.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\stdio_test_hostpeer.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\stdio_test_lastwrite.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\stdio_test_loseconn.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\stdio_test_producer.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\stdio_test_write.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\stdio_test_writeseq.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\testutils.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_abstract.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_adbapi.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_amp.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_application.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_compat.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_context.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_cooperator.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_defer.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_defgen.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_dict.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_dirdbm.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_error.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_factories.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_failure.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_fdesc.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_finger.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_formmethod.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_ftp.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_ftp_options.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_htb.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_ident.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_internet.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_iosim.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_iutils.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_lockfile.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_log.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_logfile.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_loopback.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_main.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_memcache.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_modules.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_monkey.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_nooldstyle.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_paths.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_pcp.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_persisted.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_plugin.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_policies.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_postfix.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_process.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_protocols.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_randbytes.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_rebuild.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_reflect.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_roots.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_shortcut.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_sip.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_sob.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_socks.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_ssl.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_sslverify.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_stateful.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_stdio.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_strerror.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_stringtransport.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_strports.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_task.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_tcp.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_tcp_internals.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_text.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_threadable.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_threadpool.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_threads.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_tpfile.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_twistd.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_twisted.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_udp.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_unix.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\test_usage.py -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test_init_.py -> build\\lib.win32-3.7\\twisted\\test\ncreating build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial\\itrial.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial\\reporter.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial\\runner.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial\\unittest.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial\\util.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial_asyncrunner.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial_asynctest.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial_synctest.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial_init_.py -> build\\lib.win32-3.7\\twisted\\trial\ncopying src\\twisted\\trial_main_.py -> build\\lib.win32-3.7\\twisted\\trial\ncreating build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\client.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\demo.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\distrib.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\domhelpers.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\error.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\guard.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\html.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\http.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\http_headers.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\iweb.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\microdom.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\proxy.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\resource.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\rewrite.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\script.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\server.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\static.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\sux.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\tap.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\template.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\twcgi.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\util.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\vhost.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\wsgi.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web\\xmlrpc.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web_element.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web_flatten.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web_http2.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web_newclient.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web_responses.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web_stan.py -> build\\lib.win32-3.7\\twisted\\web\ncopying src\\twisted\\web_init_.py -> build\\lib.win32-3.7\\twisted\\web\ncreating build\\lib.win32-3.7\\twisted\\words\ncopying src\\twisted\\words\\ewords.py -> build\\lib.win32-3.7\\twisted\\words\ncopying src\\twisted\\words\\iwords.py -> build\\lib.win32-3.7\\twisted\\words\ncopying src\\twisted\\words\\service.py -> build\\lib.win32-3.7\\twisted\\words\ncopying src\\twisted\\words\\tap.py -> build\\lib.win32-3.7\\twisted\\words\ncopying src\\twisted\\words\\xmpproutertap.py -> build\\lib.win32-3.7\\twisted\\words\ncopying src\\twisted\\words_init_.py -> build\\lib.win32-3.7\\twisted\\words\ncreating build\\lib.win32-3.7\\twisted_threads\ncopying src\\twisted_threads_convenience.py -> build\\lib.win32-3.7\\twisted_threads\ncopying src\\twisted_threads_ithreads.py -> build\\lib.win32-3.7\\twisted_threads\ncopying src\\twisted_threads_memory.py -> build\\lib.win32-3.7\\twisted_threads\ncopying src\\twisted_threads_pool.py -> build\\lib.win32-3.7\\twisted_threads\ncopying src\\twisted_threads_team.py -> build\\lib.win32-3.7\\twisted_threads\ncopying src\\twisted_threads_threadworker.py -> build\\lib.win32-3.7\\twisted_threads\ncopying src\\twisted_threads_init_.py -> build\\lib.win32-3.7\\twisted_threads\ncreating build\\lib.win32-3.7\\twisted\\application\\runner\ncopying src\\twisted\\application\\runner_exit.py -> build\\lib.win32-3.7\\twisted\\application\\runner\ncopying src\\twisted\\application\\runner_pidfile.py -> build\\lib.win32-3.7\\twisted\\application\\runner\ncopying src\\twisted\\application\\runner_runner.py -> build\\lib.win32-3.7\\twisted\\application\\runner\ncopying src\\twisted\\application\\runner_init_.py -> build\\lib.win32-3.7\\twisted\\application\\runner\ncreating build\\lib.win32-3.7\\twisted\\application\\test\ncopying src\\twisted\\application\\test\\test_internet.py -> build\\lib.win32-3.7\\twisted\\application\\test\ncopying src\\twisted\\application\\test\\test_service.py -> build\\lib.win32-3.7\\twisted\\application\\test\ncopying src\\twisted\\application\\test_init_.py -> build\\lib.win32-3.7\\twisted\\application\\test\ncreating build\\lib.win32-3.7\\twisted\\application\\twist\ncopying src\\twisted\\application\\twist_options.py -> build\\lib.win32-3.7\\twisted\\application\\twist\ncopying src\\twisted\\application\\twist_twist.py -> build\\lib.win32-3.7\\twisted\\application\\twist\ncopying src\\twisted\\application\\twist_init_.py -> build\\lib.win32-3.7\\twisted\\application\\twist\ncreating build\\lib.win32-3.7\\twisted\\application\\runner\\test\ncopying src\\twisted\\application\\runner\\test\\test_exit.py -> build\\lib.win32-3.7\\twisted\\application\\runner\\test\ncopying src\\twisted\\application\\runner\\test\\test_pidfile.py -> build\\lib.win32-3.7\\twisted\\application\\runner\\test\ncopying src\\twisted\\application\\runner\\test\\test_runner.py -> build\\lib.win32-3.7\\twisted\\application\\runner\\test\ncopying src\\twisted\\application\\runner\\test_init_.py -> build\\lib.win32-3.7\\twisted\\application\\runner\\test\ncreating build\\lib.win32-3.7\\twisted\\application\\twist\\test\ncopying src\\twisted\\application\\twist\\test\\test_options.py -> build\\lib.win32-3.7\\twisted\\application\\twist\\test\ncopying src\\twisted\\application\\twist\\test\\test_twist.py -> build\\lib.win32-3.7\\twisted\\application\\twist\\test\ncopying src\\twisted\\application\\twist\\test_init_.py -> build\\lib.win32-3.7\\twisted\\application\\twist\\test\ncreating build\\lib.win32-3.7\\twisted\\conch\\client\ncopying src\\twisted\\conch\\client\\agent.py -> build\\lib.win32-3.7\\twisted\\conch\\client\ncopying src\\twisted\\conch\\client\\connect.py -> build\\lib.win32-3.7\\twisted\\conch\\client\ncopying src\\twisted\\conch\\client\\default.py -> build\\lib.win32-3.7\\twisted\\conch\\client\ncopying src\\twisted\\conch\\client\\direct.py -> build\\lib.win32-3.7\\twisted\\conch\\client\ncopying src\\twisted\\conch\\client\\knownhosts.py -> build\\lib.win32-3.7\\twisted\\conch\\client\ncopying src\\twisted\\conch\\client\\options.py -> build\\lib.win32-3.7\\twisted\\conch\\client\ncopying src\\twisted\\conch\\client_init_.py -> build\\lib.win32-3.7\\twisted\\conch\\client\ncreating build\\lib.win32-3.7\\twisted\\conch\\insults\ncopying src\\twisted\\conch\\insults\\client.py -> build\\lib.win32-3.7\\twisted\\conch\\insults\ncopying src\\twisted\\conch\\insults\\colors.py -> build\\lib.win32-3.7\\twisted\\conch\\insults\ncopying src\\twisted\\conch\\insults\\helper.py -> build\\lib.win32-3.7\\twisted\\conch\\insults\ncopying src\\twisted\\conch\\insults\\insults.py -> build\\lib.win32-3.7\\twisted\\conch\\insults\ncopying src\\twisted\\conch\\insults\\text.py -> build\\lib.win32-3.7\\twisted\\conch\\insults\ncopying src\\twisted\\conch\\insults\\window.py -> build\\lib.win32-3.7\\twisted\\conch\\insults\ncopying src\\twisted\\conch\\insults_init_.py -> build\\lib.win32-3.7\\twisted\\conch\\insults\ncreating build\\lib.win32-3.7\\twisted\\conch\\openssh_compat\ncopying src\\twisted\\conch\\openssh_compat\\factory.py -> build\\lib.win32-3.7\\twisted\\conch\\openssh_compat\ncopying src\\twisted\\conch\\openssh_compat\\primes.py -> build\\lib.win32-3.7\\twisted\\conch\\openssh_compat\ncopying src\\twisted\\conch\\openssh_compat_init_.py -> build\\lib.win32-3.7\\twisted\\conch\\openssh_compat\ncreating build\\lib.win32-3.7\\twisted\\conch\\scripts\ncopying src\\twisted\\conch\\scripts\\cftp.py -> build\\lib.win32-3.7\\twisted\\conch\\scripts\ncopying src\\twisted\\conch\\scripts\\ckeygen.py -> build\\lib.win32-3.7\\twisted\\conch\\scripts\ncopying src\\twisted\\conch\\scripts\\conch.py -> build\\lib.win32-3.7\\twisted\\conch\\scripts\ncopying src\\twisted\\conch\\scripts\\tkconch.py -> build\\lib.win32-3.7\\twisted\\conch\\scripts\ncopying src\\twisted\\conch\\scripts_init_.py -> build\\lib.win32-3.7\\twisted\\conch\\scripts\ncreating build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\address.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\agent.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\channel.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\common.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\connection.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\factory.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\filetransfer.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\forwarding.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\keys.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\service.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\session.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\sexpy.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\transport.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh\\userauth.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh_kex.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncopying src\\twisted\\conch\\ssh_init_.py -> build\\lib.win32-3.7\\twisted\\conch\\ssh\ncreating build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\keydata.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\loopback.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_address.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_agent.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_cftp.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_channel.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_checkers.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_ckeygen.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_conch.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_connection.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_default.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_endpoints.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_filetransfer.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_forwarding.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_helper.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_insults.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_keys.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_knownhosts.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_manhole.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_manhole_tap.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_mixin.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_openssh_compat.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_recvline.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_scripts.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_session.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_ssh.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_tap.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_telnet.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_text.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_transport.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_unix.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_userauth.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test\\test_window.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncopying src\\twisted\\conch\\test_init_.py -> build\\lib.win32-3.7\\twisted\\conch\\test\ncreating build\\lib.win32-3.7\\twisted\\conch\\ui\ncopying src\\twisted\\conch\\ui\\ansi.py -> build\\lib.win32-3.7\\twisted\\conch\\ui\ncopying src\\twisted\\conch\\ui\\tkvt100.py -> build\\lib.win32-3.7\\twisted\\conch\\ui\ncopying src\\twisted\\conch\\ui_init_.py -> build\\lib.win32-3.7\\twisted\\conch\\ui\ncreating build\\lib.win32-3.7\\twisted\\cred\\test\ncopying src\\twisted\\cred\\test\\test_cramauth.py -> build\\lib.win32-3.7\\twisted\\cred\\test\ncopying src\\twisted\\cred\\test\\test_cred.py -> build\\lib.win32-3.7\\twisted\\cred\\test\ncopying src\\twisted\\cred\\test\\test_digestauth.py -> build\\lib.win32-3.7\\twisted\\cred\\test\ncopying src\\twisted\\cred\\test\\test_simpleauth.py -> build\\lib.win32-3.7\\twisted\\cred\\test\ncopying src\\twisted\\cred\\test\\test_strcred.py -> build\\lib.win32-3.7\\twisted\\cred\\test\ncopying src\\twisted\\cred\\test_init_.py -> build\\lib.win32-3.7\\twisted\\cred\\test\ncreating build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor\\abstract.py -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor\\const.py -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor\\interfaces.py -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor\\reactor.py -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor\\setup.py -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor\\tcp.py -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor\\udp.py -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor_init_.py -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncreating build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\connectionmixins.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\fakeendpoint.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\modulehelpers.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\process_cli.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\process_connectionlost.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\process_gireactornocompat.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\process_helper.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\reactormixins.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_abstract.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_address.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_asyncioreactor.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_base.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_baseprocess.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_core.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_coroutines.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_default.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_endpoints.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_epollreactor.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_fdset.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_filedescriptor.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_gireactor.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_glibbase.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_inlinecb.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_inotify.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_iocp.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_kqueuereactor.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_main.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_newtls.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_pollingfile.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_posixbase.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_posixprocess.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_process.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_protocol.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_resolver.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_serialport.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_sigchld.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_socket.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_stdio.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_tcp.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_threads.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_time.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_tls.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_udp.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_udp_internals.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_unix.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_win32events.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test\\test_win32serialport.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test_posixifaces.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test_win32ifaces.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test_init_.py -> build\\lib.win32-3.7\\twisted\\internet\\test\ncreating build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_buffer.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_file.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_filter.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_flatten.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_format.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_global.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_io.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_json.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_legacy.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_levels.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_logger.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_observer.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_stdlib.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test\\test_util.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncopying src\\twisted\\logger\\test_init_.py -> build\\lib.win32-3.7\\twisted\\logger\\test\ncreating build\\lib.win32-3.7\\twisted\\mail\\scripts\ncopying src\\twisted\\mail\\scripts\\mailmail.py -> build\\lib.win32-3.7\\twisted\\mail\\scripts\ncreating build\\lib.win32-3.7\\twisted\\mail\\test\ncopying src\\twisted\\mail\\test\\pop3testserver.py -> build\\lib.win32-3.7\\twisted\\mail\\test\ncopying src\\twisted\\mail\\test\\test_imap.py -> build\\lib.win32-3.7\\twisted\\mail\\test\ncopying src\\twisted\\mail\\test\\test_mailmail.py -> build\\lib.win32-3.7\\twisted\\mail\\test\ncopying src\\twisted\\mail\\test\\test_pop3.py -> build\\lib.win32-3.7\\twisted\\mail\\test\ncopying src\\twisted\\mail\\test\\test_pop3client.py -> build\\lib.win32-3.7\\twisted\\mail\\test\ncopying src\\twisted\\mail\\test\\test_smtp.py -> build\\lib.win32-3.7\\twisted\\mail\\test\ncopying src\\twisted\\mail\\test_init_.py -> build\\lib.win32-3.7\\twisted\\mail\\test\ncreating build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_cache.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_client.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_common.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_dns.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_examples.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_hosts.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_names.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_resolve.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_rfc1982.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_rootresolve.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_server.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_srvconnect.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_tap.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test\\test_util.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncopying src\\twisted\\names\\test_init_.py -> build\\lib.win32-3.7\\twisted\\names\\test\ncreating build\\lib.win32-3.7\\twisted\\pair\\test\ncopying src\\twisted\\pair\\test\\test_ethernet.py -> build\\lib.win32-3.7\\twisted\\pair\\test\ncopying src\\twisted\\pair\\test\\test_ip.py -> build\\lib.win32-3.7\\twisted\\pair\\test\ncopying src\\twisted\\pair\\test\\test_rawudp.py -> build\\lib.win32-3.7\\twisted\\pair\\test\ncopying src\\twisted\\pair\\test\\test_tuntap.py -> build\\lib.win32-3.7\\twisted\\pair\\test\ncopying src\\twisted\\pair\\test_init_.py -> build\\lib.win32-3.7\\twisted\\pair\\test\ncreating build\\lib.win32-3.7\\twisted\\persisted\\test\ncopying src\\twisted\\persisted\\test\\test_styles.py -> build\\lib.win32-3.7\\twisted\\persisted\\test\ncopying src\\twisted\\persisted\\test_init_.py -> build\\lib.win32-3.7\\twisted\\persisted\\test\ncreating build\\lib.win32-3.7\\twisted\\positioning\\test\ncopying src\\twisted\\positioning\\test\\receiver.py -> build\\lib.win32-3.7\\twisted\\positioning\\test\ncopying src\\twisted\\positioning\\test\\test_base.py -> build\\lib.win32-3.7\\twisted\\positioning\\test\ncopying src\\twisted\\positioning\\test\\test_nmea.py -> build\\lib.win32-3.7\\twisted\\positioning\\test\ncopying src\\twisted\\positioning\\test\\test_sentence.py -> build\\lib.win32-3.7\\twisted\\positioning\\test\ncopying src\\twisted\\positioning\\test_init_.py -> build\\lib.win32-3.7\\twisted\\positioning\\test\ncreating build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncopying src\\twisted\\protocols\\haproxy_exceptions.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncopying src\\twisted\\protocols\\haproxy_info.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncopying src\\twisted\\protocols\\haproxy_interfaces.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncopying src\\twisted\\protocols\\haproxy_parser.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncopying src\\twisted\\protocols\\haproxy_v1parser.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncopying src\\twisted\\protocols\\haproxy_v2parser.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncopying src\\twisted\\protocols\\haproxy_wrapper.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncopying src\\twisted\\protocols\\haproxy_init_.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\ncreating build\\lib.win32-3.7\\twisted\\protocols\\test\ncopying src\\twisted\\protocols\\test\\test_basic.py -> build\\lib.win32-3.7\\twisted\\protocols\\test\ncopying src\\twisted\\protocols\\test\\test_tls.py -> build\\lib.win32-3.7\\twisted\\protocols\\test\ncopying src\\twisted\\protocols\\test_init_.py -> build\\lib.win32-3.7\\twisted\\protocols\\test\ncreating build\\lib.win32-3.7\\twisted\\protocols\\haproxy\\test\ncopying src\\twisted\\protocols\\haproxy\\test\\test_parser.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\\test\ncopying src\\twisted\\protocols\\haproxy\\test\\test_v1parser.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\\test\ncopying src\\twisted\\protocols\\haproxy\\test\\test_v2parser.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\\test\ncopying src\\twisted\\protocols\\haproxy\\test\\test_wrapper.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\\test\ncopying src\\twisted\\protocols\\haproxy\\test_init_.py -> build\\lib.win32-3.7\\twisted\\protocols\\haproxy\\test\ncreating build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\deprecatedattributes.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\modules_helpers.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\pullpipe.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_appdirs.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_components.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_constants.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_deprecate.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_dist3.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_fakepwd.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_htmlizer.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_inotify.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_release.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_runtime.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_sendmsg.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_setup.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_shellcomp.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_syslog.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_systemd.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_textattributes.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_tzhelper.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_url.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_urlpath.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_util.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_versions.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_zippath.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test\\test_zipstream.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\python\\test_init_.py -> build\\lib.win32-3.7\\twisted\\python\\test\ncreating build\\lib.win32-3.7\\twisted\\runner\\test\ncopying src\\twisted\\runner\\test\\test_inetdconf.py -> build\\lib.win32-3.7\\twisted\\runner\\test\ncopying src\\twisted\\runner\\test\\test_procmon.py -> build\\lib.win32-3.7\\twisted\\runner\\test\ncopying src\\twisted\\runner\\test\\test_procmontap.py -> build\\lib.win32-3.7\\twisted\\runner\\test\ncopying src\\twisted\\runner\\test_init_.py -> build\\lib.win32-3.7\\twisted\\runner\\test\ncreating build\\lib.win32-3.7\\twisted\\scripts\\test\ncopying src\\twisted\\scripts\\test\\test_scripts.py -> build\\lib.win32-3.7\\twisted\\scripts\\test\ncopying src\\twisted\\scripts\\test_init_.py -> build\\lib.win32-3.7\\twisted\\scripts\\test\ncreating build\\lib.win32-3.7\\twisted\\spread\\test\ncopying src\\twisted\\spread\\test\\test_banana.py -> build\\lib.win32-3.7\\twisted\\spread\\test\ncopying src\\twisted\\spread\\test\\test_jelly.py -> build\\lib.win32-3.7\\twisted\\spread\\test\ncopying src\\twisted\\spread\\test\\test_pb.py -> build\\lib.win32-3.7\\twisted\\spread\\test\ncopying src\\twisted\\spread\\test\\test_pbfailure.py -> build\\lib.win32-3.7\\twisted\\spread\\test\ncopying src\\twisted\\spread\\test_init_.py -> build\\lib.win32-3.7\\twisted\\spread\\test\ncreating build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\detests.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\erroneous.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\mockcustomsuite.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\mockcustomsuite2.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\mockcustomsuite3.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\mockdoctest.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\moduleself.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\moduletest.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\novars.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\ordertests.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\packages.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\sample.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\scripttest.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\skipping.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\suppression.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_assertions.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_asyncassertions.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_deferred.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_doctest.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_keyboard.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_loader.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_log.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_output.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_plugins.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_pyunitcompat.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_reporter.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_runner.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_script.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_suppression.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_testcase.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_tests.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_util.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\test_warning.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test\\weird.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncopying src\\twisted\\trial\\test_init_.py -> build\\lib.win32-3.7\\twisted\\trial\\test\ncreating build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist\\distreporter.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist\\disttrial.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist\\managercommands.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist\\options.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist\\worker.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist\\workercommands.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist\\workerreporter.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist\\workertrial.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncopying src\\twisted\\trial_dist_init_.py -> build\\lib.win32-3.7\\twisted\\trial_dist\ncreating build\\lib.win32-3.7\\twisted\\trial_dist\\test\ncopying src\\twisted\\trial_dist\\test\\test_distreporter.py -> build\\lib.win32-3.7\\twisted\\trial_dist\\test\ncopying src\\twisted\\trial_dist\\test\\test_disttrial.py -> build\\lib.win32-3.7\\twisted\\trial_dist\\test\ncopying src\\twisted\\trial_dist\\test\\test_options.py -> build\\lib.win32-3.7\\twisted\\trial_dist\\test\ncopying src\\twisted\\trial_dist\\test\\test_worker.py -> build\\lib.win32-3.7\\twisted\\trial_dist\\test\ncopying src\\twisted\\trial_dist\\test\\test_workerreporter.py -> build\\lib.win32-3.7\\twisted\\trial_dist\\test\ncopying src\\twisted\\trial_dist\\test\\test_workertrial.py -> build\\lib.win32-3.7\\twisted\\trial_dist\\test\ncopying src\\twisted\\trial_dist\\test_init_.py -> build\\lib.win32-3.7\\twisted\\trial_dist\\test\ncreating build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\requesthelper.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_agent.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_cgi.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_client.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_distrib.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_domhelpers.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_error.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_flatten.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_html.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_http.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_http2.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_httpauth.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_http_headers.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_newclient.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_proxy.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_resource.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_script.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_stan.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_static.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_tap.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_template.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_util.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_vhost.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_web.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_webclient.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_web__responses.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_wsgi.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_xml.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test\\test_xmlrpc.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test_util.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncopying src\\twisted\\web\\test_init_.py -> build\\lib.win32-3.7\\twisted\\web\\test\ncreating build\\lib.win32-3.7\\twisted\\web_auth\ncopying src\\twisted\\web_auth\\basic.py -> build\\lib.win32-3.7\\twisted\\web_auth\ncopying src\\twisted\\web_auth\\digest.py -> build\\lib.win32-3.7\\twisted\\web_auth\ncopying src\\twisted\\web_auth\\wrapper.py -> build\\lib.win32-3.7\\twisted\\web_auth\ncopying src\\twisted\\web_auth_init_.py -> build\\lib.win32-3.7\\twisted\\web_auth\ncreating build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\im\\baseaccount.py -> build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\im\\basechat.py -> build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\im\\basesupport.py -> build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\im\\interfaces.py -> build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\im\\ircsupport.py -> build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\im\\locals.py -> build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\im\\pbsupport.py -> build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\im_init_.py -> build\\lib.win32-3.7\\twisted\\words\\im\ncreating build\\lib.win32-3.7\\twisted\\words\\protocols\ncopying src\\twisted\\words\\protocols\\irc.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\ncopying src\\twisted\\words\\protocols_init_.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\ncreating build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_basechat.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_basesupport.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_domish.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_irc.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_ircsupport.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_irc_service.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabberclient.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabbercomponent.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabbererror.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabberjid.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabberjstrports.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabbersasl.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabbersaslmechanisms.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabberxmlstream.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_jabberxmppstringprep.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_service.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_tap.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_xishutil.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_xmlstream.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_xmpproutertap.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test\\test_xpath.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncopying src\\twisted\\words\\test_init_.py -> build\\lib.win32-3.7\\twisted\\words\\test\ncreating build\\lib.win32-3.7\\twisted\\words\\xish\ncopying src\\twisted\\words\\xish\\domish.py -> build\\lib.win32-3.7\\twisted\\words\\xish\ncopying src\\twisted\\words\\xish\\utility.py -> build\\lib.win32-3.7\\twisted\\words\\xish\ncopying src\\twisted\\words\\xish\\xmlstream.py -> build\\lib.win32-3.7\\twisted\\words\\xish\ncopying src\\twisted\\words\\xish\\xpath.py -> build\\lib.win32-3.7\\twisted\\words\\xish\ncopying src\\twisted\\words\\xish\\xpathparser.py -> build\\lib.win32-3.7\\twisted\\words\\xish\ncopying src\\twisted\\words\\xish_init_.py -> build\\lib.win32-3.7\\twisted\\words\\xish\ncreating build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\client.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\component.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\error.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\ijabber.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\jid.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\jstrports.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\sasl.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\sasl_mechanisms.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\xmlstream.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber\\xmpp_stringprep.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncopying src\\twisted\\words\\protocols\\jabber_init_.py -> build\\lib.win32-3.7\\twisted\\words\\protocols\\jabber\ncreating build\\lib.win32-3.7\\twisted_threads\\test\ncopying src\\twisted_threads\\test\\test_convenience.py -> build\\lib.win32-3.7\\twisted_threads\\test\ncopying src\\twisted_threads\\test\\test_memory.py -> build\\lib.win32-3.7\\twisted_threads\\test\ncopying src\\twisted_threads\\test\\test_team.py -> build\\lib.win32-3.7\\twisted_threads\\test\ncopying src\\twisted_threads\\test\\test_threadworker.py -> build\\lib.win32-3.7\\twisted_threads\\test\ncopying src\\twisted_threads\\test_init_.py -> build\\lib.win32-3.7\\twisted_threads\\test\nrunning egg_info\nwriting src\\Twisted.egg-info\\PKG-INFO\nwriting dependency_links to src\\Twisted.egg-info\\dependency_links.txt\nwriting entry points to src\\Twisted.egg-info\\entry_points.txt\nwriting requirements to src\\Twisted.egg-info\\requires.txt\nwriting top-level names to src\\Twisted.egg-info\\top_level.txt\nreading manifest file 'src\\Twisted.egg-info\\SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwarning: no previously-included files matching '.misc' found under directory 'src\\twisted'\nwarning: no previously-included files matching '.bugfix' found under directory 'src\\twisted'\nwarning: no previously-included files matching '.doc' found under directory 'src\\twisted'\nwarning: no previously-included files matching '.feature' found under directory 'src\\twisted'\nwarning: no previously-included files matching '.removal' found under directory 'src\\twisted'\nwarning: no previously-included files matching 'NEWS' found under directory 'src\\twisted'\nwarning: no previously-included files matching 'README' found under directory 'src\\twisted'\nwarning: no previously-included files matching 'newsfragments' found under directory 'src\\twisted'\nwarning: no previously-included files found matching 'src\\twisted\\topfiles\\CREDITS'\nwarning: no previously-included files found matching 'src\\twisted\\topfiles\\ChangeLog.Old'\nwarning: no previously-included files found matching 'pyproject.toml'\nwarning: no previously-included files found matching 'codecov.yml'\nwarning: no previously-included files found matching 'appveyor.yml'\nwarning: no previously-included files found matching '.circleci'\nwarning: no previously-included files matching '' found under directory '.circleci'\nno previously-included directories found matching 'bin'\nno previously-included directories found matching 'admin'\nno previously-included directories found matching '.travis'\nno previously-included directories found matching '.github'\nwarning: no previously-included files found matching 'docs\\historic\\2003'\nwarning: no previously-included files matching '*' found under directory 'docs\\historic\\2003'\nwriting manifest file 'src\\Twisted.egg-info\\SOURCES.txt'\ncopying src\\twisted\\python_sendmsg.c -> build\\lib.win32-3.7\\twisted\\python\ncopying src\\twisted\\python\\twisted-completion.zsh -> build\\lib.win32-3.7\\twisted\\python\ncreating build\\lib.win32-3.7\\twisted\\python_pydoctortemplates\ncopying src\\twisted\\python_pydoctortemplates\\common.html -> build\\lib.win32-3.7\\twisted\\python_pydoctortemplates\ncopying src\\twisted\\python_pydoctortemplates\\index.html -> build\\lib.win32-3.7\\twisted\\python_pydoctortemplates\ncopying src\\twisted\\python_pydoctortemplates\\summary.html -> build\\lib.win32-3.7\\twisted\\python_pydoctortemplates\ncopying src\\twisted\\test\\cert.pem.no_trailing_newline -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\key.pem.no_trailing_newline -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\raiser.c -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\raiser.pyx -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\test\\server.pem -> build\\lib.win32-3.7\\twisted\\test\ncopying src\\twisted\\internet\\iocpreactor\\build.bat -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncopying src\\twisted\\internet\\iocpreactor\\notes.txt -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\ncreating build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\iocpreactor\\iocpsupport\\acceptex.pxi -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\iocpreactor\\iocpsupport\\connectex.pxi -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\iocpreactor\\iocpsupport\\iocpsupport.c -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\iocpreactor\\iocpsupport\\iocpsupport.pyx -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\iocpreactor\\iocpsupport\\winsock_pointers.c -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\iocpreactor\\iocpsupport\\winsock_pointers.h -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\iocpreactor\\iocpsupport\\wsarecv.pxi -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\iocpreactor\\iocpsupport\\wsasend.pxi -> build\\lib.win32-3.7\\twisted\\internet\\iocpreactor\\iocpsupport\ncopying src\\twisted\\internet\\test_awaittests.py.3only -> build\\lib.win32-3.7\\twisted\\internet\\test\ncopying src\\twisted\\internet\\test_yieldfromtests.py.3only -> build\\lib.win32-3.7\\twisted\\internet\\test\ncreating build\\lib.win32-3.7\\twisted\\internet\\test\\fake_CAs\ncopying src\\twisted\\internet\\test\\fake_CAs\\chain.pem -> build\\lib.win32-3.7\\twisted\\internet\\test\\fake_CAs\ncopying src\\twisted\\internet\\test\\fake_CAs\\not-a-certificate -> build\\lib.win32-3.7\\twisted\\internet\\test\\fake_CAs\ncopying src\\twisted\\internet\\test\\fake_CAs\\thing1.pem -> build\\lib.win32-3.7\\twisted\\internet\\test\\fake_CAs\ncopying src\\twisted\\internet\\test\\fake_CAs\\thing2-duplicate.pem -> build\\lib.win32-3.7\\twisted\\internet\\test\\fake_CAs\ncopying src\\twisted\\internet\\test\\fake_CAs\\thing2.pem -> build\\lib.win32-3.7\\twisted\\internet\\test\\fake_CAs\ncopying src\\twisted\\mail\\test\\rfc822.message -> build\\lib.win32-3.7\\twisted\\mail\\test\ncopying src\\twisted\\python\\test_deprecatetests.py.3only -> build\\lib.win32-3.7\\twisted\\python\\test\ncopying src\\twisted\\words\\im\\instancemessenger.glade -> build\\lib.win32-3.7\\twisted\\words\\im\ncopying src\\twisted\\words\\xish\\xpathparser.g -> build\\lib.win32-3.7\\twisted\\words\\xish\nrunning build_ext\nbuilding 'twisted.test.raiser' extension\ncreating build\\temp.win32-3.7\ncreating build\\temp.win32-3.7\\Release\ncreating build\\temp.win32-3.7\\Release\\src\ncreating build\\temp.win32-3.7\\Release\\src\\twisted\ncreating build\\temp.win32-3.7\\Release\\src\\twisted\\test\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x86\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -DWIN32=1 -IC:\\Users\\bkaja\\AppData\\Local\\Programs\\Python\\Python37-32\\include -IC:\\Users\\bkaja\\AppData\\Local\\Programs\\Python\\Python37-32\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\cppwinrt\" /Tcsrc/twisted/test/raiser.c /Fobuild\\temp.win32-3.7\\Release\\src/twisted/test/raiser.obj\nraiser.c\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x86\\link.exe /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\\Users\\bkaja\\AppData\\Local\\Programs\\Python\\Python37-32\\libs /LIBPATH:C:\\Users\\bkaja\\AppData\\Local\\Programs\\Python\\Python37-32\\PCbuild\\win32 \"/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\lib\\x86\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\ucrt\\x86\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17134.0\\um\\x86\" /EXPORT:PyInit_raiser build\\temp.win32-3.7\\Release\\src/twisted/test/raiser.obj /OUT:build\\lib.win32-3.7\\twisted\\test\\raiser.cp37-win32.pyd /IMPLIB:build\\temp.win32-3.7\\Release\\src/twisted/test\\raiser.cp37-win32.lib\nCreating library build\\temp.win32-3.7\\Release\\src/twisted/test\\raiser.cp37-win32.lib and object build\\temp.win32-3.7\\Release\\src/twisted/test\\raiser.cp37-win32.exp\nGenerating code\nFinished generating code\nbuilding 'twisted.internet.iocpreactor.iocpsupport' extension\ncreating build\\temp.win32-3.7\\Release\\src\\twisted\\internet\ncreating build\\temp.win32-3.7\\Release\\src\\twisted\\internet\\iocpreactor\ncreating build\\temp.win32-3.7\\Release\\src\\twisted\\internet\\iocpreactor\\iocpsupport\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x86\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -DWIN32=1 -IC:\\Users\\bkaja\\AppData\\Local\\Programs\\Python\\Python37-32\\include -IC:\\Users\\bkaja\\AppData\\Local\\Programs\\Python\\Python37-32\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17134.0\\cppwinrt\" /Tcsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c /Fobuild\\temp.win32-3.7\\Release\\src/twisted/internet/iocpreactor/iocpsupport/iocpsupport.obj\niocpsupport.c\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(1933): warning C4047: '=': '__pyx_t_11iocpsupport_HANDLE' differs in levels of indirection from 'HANDLE'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(2077): warning C4022: 'CreateIoCompletionPort': pointer mismatch for actual parameter 1\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(2077): warning C4022: 'CreateIoCompletionPort': pointer mismatch for actual parameter 2\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(2077): warning C4047: '=': '__pyx_t_11iocpsupport_HANDLE' differs in levels of indirection from 'HANDLE'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(2192): warning C4022: 'GetQueuedCompletionStatus': pointer mismatch for actual parameter 1\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(2537): warning C4022: 'PostQueuedCompletionStatus': pointer mismatch for actual parameter 1\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(2648): warning C4022: 'CloseHandle': pointer mismatch for actual parameter 1\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7194): warning C4020: 'function through pointer': too many actual parameters\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7637): error C2039: 'exc_type': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7638): error C2039: 'exc_value': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7639): error C2039: 'exc_traceback': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7640): error C2039: 'exc_type': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7641): error C2039: 'exc_value': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7642): error C2039: 'exc_traceback': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7664): error C2039: 'exc_type': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7665): error C2039: 'exc_value': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7666): error C2039: 'exc_traceback': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7667): error C2039: 'exc_type': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7668): error C2039: 'exc_value': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7669): error C2039: 'exc_traceback': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7688): error C2039: 'exc_type': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7689): error C2039: 'exc_value': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7690): error C2039: 'exc_traceback': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7697): error C2039: 'exc_type': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7698): error C2039: 'exc_value': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7699): error C2039: 'exc_traceback': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7700): error C2039: 'exc_type': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7701): error C2039: 'exc_value': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nsrc/twisted/internet/iocpreactor/iocpsupport/iocpsupport.c(7702): error C2039: 'exc_traceback': is not a member of '_ts'\nc:\\users\\bkaja\\appdata\\local\\programs\\python\\python37-32\\include\\pystate.h(209): note: see declaration of '_ts'\nerror: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x86\\cl.exe' failed with exit status 2\n----------------------------------------\nCommand \"C:\\Users\\bkaja\\AppData\\Local\\Programs\\Python\\Python37-32\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\bkaja\\AppData\\Local\\Temp\\pip-install-vrxv__11\\Twisted\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\Users\\bkaja\\AppData\\Local\\Temp\\pip-record-fnu_lwpt\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\bkaja\\AppData\\Local\\Temp\\pip-install-vrxv__11\\Twisted\\", "issue_status": "Closed", "issue_reporting_time": "2018-07-07T05:05:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "350": {"issue_url": "https://github.com/scrapy/scrapy/issues/3320", "issue_id": "#3320", "issue_summary": "How to Ignore certificate verification in dynamic forwarding proxy ip", "issue_description": "dragon-catcher commented on Jul 6, 2018\nnow, I have a problem, I am using dynamic forwarding proxy ip in scrapy.\nIp provider tell me It is important to note that accessing https requires neglecting certificate verification. So, If I use request ,I can use verify=false ,It workd!\nNow ,in python3 Scrapy, what should I do?\n(I tested access to http website ,It's no problem,But in https,It's can't work!)\nSomeone help me!\nthis is my scrapy version.\nScrapy       : 1.5.0\nlxml         : 4.2.1.0\nlibxml2      : 2.9.8\ncssselect    : 1.0.3\nparsel       : 1.4.0\nw3lib        : 1.19.0\nTwisted      : 18.4.0\nPython       : 3.6.5 (default, Mar 30 2018, 06:42:10) - [GCC 4.2.1 \nCompatible Apple LLVM 9.0.0 (clang-900.0.39.2)]\npyOpenSSL    : 18.0.0 (OpenSSL 1.1.0h  27 Mar 2018)\ncryptography : 2.2.2\nPlatform     : Darwin-16.7.0-x86_64-i386-64bit", "issue_status": "Closed", "issue_reporting_time": "2018-07-06T09:04:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "351": {"issue_url": "https://github.com/scrapy/scrapy/issues/3317", "issue_id": "#3317", "issue_summary": "Update Scrapy tutorial and other docs to use parsel 1.5", "issue_description": "Member\nkmike commented on Jul 5, 2018\npromote .get / .getall instead of .extract / .extract_first\nmake sure new .attrib property is mentioned\n\ud83d\udc4d 5", "issue_status": "Closed", "issue_reporting_time": "2018-07-04T19:36:39Z", "fixed_by": "#3390", "pull_request_summary": "[MRG+1] update Scrapy to use parsel 1.5", "pull_request_description": "Member\nkmike commented on Aug 15, 2018 \u2022\nedited\nFixes #3317.\nTodo:\nupdate Selector and SelectorList API documentation\nrevise Selectors tutorial: either bring more from the parsel's tutorial, or link to missing pieces (like has-class XPath extension); maybe rearrange some of the sections.\ncheck .extract usages\nfind more places where .attrib makes code simpler\nupdate after #3400.\nfix \"Removing namespaces\" example - see scrapy/parsel#119\n\ud83c\udf89 6\n\u2764\ufe0f 3", "pull_request_status": "Merged", "issue_fixed_time": "2018-09-18T16:07:41Z", "files_changed": [["1", ".gitignore"], ["6", "docs/intro/overview.rst"], ["87", "docs/intro/tutorial.rst"], ["6", "docs/topics/commands.rst"], ["6", "docs/topics/items.rst"], ["4", "docs/topics/jobs.rst"], ["14", "docs/topics/loaders.rst"]]}, "352": {"issue_url": "https://github.com/scrapy/scrapy/issues/3314", "issue_id": "#3314", "issue_summary": "Scrapy Shell <url> ValueError: invalid hostname:", "issue_description": "Mahbub-A-Rob commented on Jul 1, 2018\nI was following the Scrapy official documentation. When I run the command scrapy shell 'quotes.toscrape.com/page/1/' it shows me the below error\nC:\\WINDOWS\\system32>scrapy shell 'http://quotes.toscrape.com/page/1/' 2018-07-01 20:54:02 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot) 2018-07-01 20:54:02 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.0.2n 7 Dec 2017), cryptography 2.1.4, Platform Windows-10-10.0.14393-SP0 2018-07-01 20:54:02 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0} 2018-07-01 20:54:02 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole'] 2018-07-01 20:54:03 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats'] 2018-07-01 20:54:03 [scrapy.middleware] INFO: Enabled spider middlewares: ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware'] 2018-07-01 20:54:03 [scrapy.middleware] INFO: Enabled item pipelines: [] 2018-07-01 20:54:03 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023 2018-07-01 20:54:03 [scrapy.core.engine] INFO: Spider opened Traceback (most recent call last): File \"C:\\ProgramData\\Anaconda3\\Scripts\\scrapy-script.py\", line 10, in <module> sys.exit(execute()) File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 150, in execute _run_print_help(parser, _run_command, cmd, args, opts) File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 90, in _run_print_help func(*a, **kw) File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 157, in _run_command cmd.run(args, opts) File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scrapy\\commands\\shell.py\", line 73, in run shell.start(url=url, redirect=not opts.no_redirect) File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scrapy\\shell.py\", line 48, in start self.fetch(url, spider, redirect=redirect) File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scrapy\\shell.py\", line 115, in fetch reactor, self._schedule, request, spider) File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\twisted\\internet\\threads.py\", line 122, in blockingCallFromThread result.raiseException() File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\twisted\\python\\failure.py\", line 372, in raiseException raise self.value.with_traceback(self.tb) ValueError: invalid hostname: 'http\nBut while using scrapy shell only, it starts and I can do basic operations like working with xpath, etc.\nI'm using Windows 10, Scrapy 1.5.0 in Anaconda 1.6.9", "issue_status": "Closed", "issue_reporting_time": "2018-07-01T15:08:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "353": {"issue_url": "https://github.com/scrapy/scrapy/issues/3313", "issue_id": "#3313", "issue_summary": "how can i set dynamic proxy and useragent with phnomatjs or other driver in scrapy", "issue_description": "Dashu-Xu commented on Jun 30, 2018\nhi, thanks for this lib !\ni want set different proxy and useragent in different requests via selenium , who can edit my code?\ni had search in the web but not get help.", "issue_status": "Closed", "issue_reporting_time": "2018-06-30T01:47:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "354": {"issue_url": "https://github.com/scrapy/scrapy/issues/3312", "issue_id": "#3312", "issue_summary": "Use pipfile instead of requirements Files", "issue_description": "Urahara commented on Jun 30, 2018 \u2022\nedited\nI think this will be a great enhancement to the project, one of the main advantages:\nEnables truly deterministic builds, while easily specifying only what you want.\nPipenv: Python Dev Workflow for Humans", "issue_status": "Closed", "issue_reporting_time": "2018-06-29T19:02:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "355": {"issue_url": "https://github.com/scrapy/scrapy/issues/3310", "issue_id": "#3310", "issue_summary": "Same spider for different websites", "issue_description": "maarusaa commented on Jun 27, 2018\nHi :)\nIs it possible to use same spider for different websites at the same time? Lets say I have an original spider which can crawl any website. Then I have a program which calls the original spider at the same time and changes the start urls and the spider name.\nI tried with this:\nfor _ in range(2):\nprint(\"Write spider name:\")\nspider = input()\nSpider.name = spider\nrunner = CrawlerRunner(get_project_settings())\nrunner.crawl(OriginalSpider)\nd = runner.join()\nd.addBoth(lambda _: reactor.stop())\nreactor.run()\nBut i have two errors:\nwhen I write the second spider it ends crawling the first one\ncrawling of the second spider stops somewhere in the middle, doesn't crawl the whole page (if I crawl it separately it work perfect )", "issue_status": "Closed", "issue_reporting_time": "2018-06-27T13:05:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "356": {"issue_url": "https://github.com/scrapy/scrapy/issues/3302", "issue_id": "#3302", "issue_summary": "No idea how to scrape from website changing quotes on text change", "issue_description": "JafferWilson commented on Jun 21, 2018\nI am willing to scrape the data from the website: website quotes\nI thought of using scrapy framework so that I can scrape it. But I did't how I can work out when the values are changing when the date and time is changing.\nI didn't found anything which can help me. Kindly, help me find out how I can scrape the website as I have mentioned above and extract all the quotes so that I can plant my analysis on the data.\nHow is it possible to scrape such a website with Scrapy? Please help.\nThe data of the website changes when the text, i.e., date and time frame changes on the website.", "issue_status": "Closed", "issue_reporting_time": "2018-06-21T05:49:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "357": {"issue_url": "https://github.com/scrapy/scrapy/issues/3301", "issue_id": "#3301", "issue_summary": "Is there any way to use Scrapy with IBM Cloud functions?", "issue_description": "Borisboky commented on Jun 20, 2018\nHi,\nCan Scrapy library be used with IBM Cloud functions?\nIn the docs the they say that Python 3 Actions (Jessie based) has Scrapy as package, but I'm couldn't figure out how to use it?", "issue_status": "Closed", "issue_reporting_time": "2018-06-20T13:31:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "358": {"issue_url": "https://github.com/scrapy/scrapy/issues/3300", "issue_id": "#3300", "issue_summary": "S3 Region isn't being used from settings AWS_REGION_NAME", "issue_description": "ntindicator commented on Jun 20, 2018\nIn the settings file I specified the AWS_REGION_NAME = 'ap-southeast-1', but it's not used and it generates errors. It still manages to upload to the correct region after it errors. Scrapy 1.5 is being used - INFO: Scrapy 1.5.0 started\n2018-06-20 11:06:05 [botocore.parsers] DEBUG: Response body: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error><Code>AuthorizationHeaderMalformed</Code><Message>The authorization header is malformed; the region \\'us-east-1\\' is wrong; expecting \\'ap-southeast-1\\'</Message><Region>ap-southeast-1</Region></Error>' 2018-06-20 11:06:05 [botocore.hooks] DEBUG: Event needs-retry.s3.PutObject: calling handler 2018-06-20 11:06:05 [botocore.retryhandler] DEBUG: No retry needed. 2018-06-20 11:06:05 [botocore.hooks] DEBUG: Event needs-retry.s3.PutObject: calling handler 2018-06-20 11:06:05 [botocore.utils] DEBUG: S3 client configured for region us-east-1 but the bucket bucket is in region ap-southeast-1; Please configure the proper region to avoid multiple unnecessary redirects and signing attempts. 2018-06-20 11:06:05 [botocore.utils] DEBUG: Updating URI from https://s3.amazonaws.com/filepath to https://s3.ap-southeast-1.amazonaws.com/filepath 2018-06-20 11:06:05 [botocore.endpoint] DEBUG: Response received to retry, sleeping for 0 seconds 2018-06-20 11:06:05 [botocore.awsrequest] DEBUG: Rewinding stream: 2018-06-20 11:06:05 [botocore.hooks] DEBUG: Event request-created.s3.PutObject: calling handler 2018-06-20 11:06:05 [botocore.hooks] DEBUG: Event choose-signer.s3.PutObject: calling handler 2018-06-20 11:06:05 [botocore.hooks] DEBUG: Event choose-signer.s3.PutObject: calling handler 2018-06-20 11:06:05 [botocore.hooks] DEBUG: Event before-sign.s3.PutObject: calling handler 2018-06-20 11:06:05 [botocore.utils] DEBUG: Checking for DNS compatible bucket for: https://s3.ap-southeast-1.amazonaws.com/filepath 2018-06-20 11:06:05 [botocore.utils] DEBUG: URI updated to: https://bucket.s3.ap-southeast-1.amazonaws.com/filepath 2018-06-20 11:06:05 [botocore.auth] DEBUG: Calculating signature using v4 auth.", "issue_status": "Closed", "issue_reporting_time": "2018-06-20T12:17:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "359": {"issue_url": "https://github.com/scrapy/scrapy/issues/3296", "issue_id": "#3296", "issue_summary": "Scrapy Unable to extract data from Expedia", "issue_description": "Abhishek-Choudharyy commented on Jun 15, 2018 \u2022\nedited\nUnable to extract data from Expedia.com.It is showing HTTP Status code is not handled or not allowed\n(2018-06-15 10:10:07 [scrapy.core.engine] INFO: Spider opened\n2018-06-15 10:10:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2018-06-15 10:10:07 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2018-06-15 10:10:08 [scrapy.core.engine] DEBUG: Crawled (429) <GET https://www.expedia.com/Seattle-Hotels-The-Paramount-Hotel.h58703.Hotel-Information?chkin=6%2F15%2F2018&chkout=6%2F16%2F2018&rm1=a2&hwrqCacheKey=61c80081-8806-40c6-a59b-f685b8822debHWRQ1529057286298&cancellable=false&regionId=178307&vip=false&c=7787b38c-d480-4887-911d-934731599d2d&&exp_dp=301&exp_ts=1529057287306&exp_curr=USD&swpToggleOn=false&exp_pg=HSR> (referer: None)\n2018-06-15 10:10:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://www.expedia.com/Seattle-Hotels-The-Paramount-Hotel.h58703.Hotel-Information?chkin=6%2F15%2F2018&chkout=6%2F16%2F2018&rm1=a2&hwrqCacheKey=61c80081-8806-40c6-a59b-f685b8822debHWRQ1529057286298&cancellable=false&regionId=178307&vip=false&c=7787b38c-d480-4887-911d-934731599d2d&&exp_dp=301&exp_ts=1529057287306&exp_curr=USD&swpToggleOn=false&exp_pg=HSR>: HTTP status code is not handled or not allowed\n2018-06-15 10:10:08 [scrapy.core.engine] INFO: Closing spider (finished))", "issue_status": "Closed", "issue_reporting_time": "2018-06-15T10:18:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "360": {"issue_url": "https://github.com/scrapy/scrapy/issues/3293", "issue_id": "#3293", "issue_summary": "scrapy sometimes works sometimes it doesnt", "issue_description": "maarusaa commented on Jun 13, 2018\nHi :)\ni have a scrapy project which works and doesn't have any errors. But when i start crawling (I am crawling the same website every time) i don't get the same number of scraped items (one time I get 1029 items, second time i get 324, third 888, ...). Is there a solution for this?", "issue_status": "Closed", "issue_reporting_time": "2018-06-13T12:16:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "361": {"issue_url": "https://github.com/scrapy/scrapy/issues/3291", "issue_id": "#3291", "issue_summary": "Where is the help?", "issue_description": "MalikRumi commented on Jun 12, 2018\nWhere can users go for reasonably well informed, reasonably rapid, and reasonably polite answers to questions about scrapy? I know that's not what this place is for, but the irc channel is a ghost town, the subreddit isn't much better, and Stack Overflow is full of uninformed trolls. Maybe there is some other place that doesn't come up very high in google? Anything?", "issue_status": "Closed", "issue_reporting_time": "2018-06-12T17:21:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "362": {"issue_url": "https://github.com/scrapy/scrapy/issues/3290", "issue_id": "#3290", "issue_summary": "Not able to install scrapy via pip in macOS Sierra", "issue_description": "Amithsk commented on Jun 12, 2018\nHello,\nI am trying to install scrapy via pip,but getting an error\nCommand: pip install Scrapy\no/p\n(scrapy) apples-MacBook-Air:scrapy Amith$ pip install Scrapy\nCollecting Scrapy\nUsing cached https://files.pythonhosted.org/packages/db/9c/cb15b2dc6003a805afd21b9b396e0e965800765b51da72fe17cf340b9be2/Scrapy-1.5.0-py2.py3-none-any.whl\nRequirement already satisfied: lxml in /Library/Python/2.7/site-packages (from Scrapy) (4.2.1)\nCollecting PyDispatcher>=2.0.5 (from Scrapy)\nUsing cached https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\nCollecting Twisted>=13.1.0 (from Scrapy)\nUsing cached https://files.pythonhosted.org/packages/12/2a/e9e4fb2e6b2f7a75577e0614926819a472934b0b85f205ba5d5d2add54d0/Twisted-18.4.0.tar.bz2\nComplete output from command python setup.py egg_info:\nDownload error on https://pypi.python.org/simple/incremental/: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:590) -- Some packages may not be found!\nCouldn't find index page for 'incremental' (maybe misspelled?)\nDownload error on https://pypi.python.org/simple/: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:590) -- Some packages may not be found!\nNo local packages or download links found for incremental>=16.10.1\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/private/var/folders/vk/wbp690w965gcdvmg_67chy380000gn/T/pip-install-9zdxXV/Twisted/setup.py\", line 21, in\nsetuptools.setup(**_setup\"getSetupArgs\")\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/core.py\", line 111, in setup\n_setup_distribution = dist = klass(attrs)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py\", line 268, in init\nself.fetch_build_eggs(attrs['setup_requires'])\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py\", line 313, in fetch_build_eggs\nreplace_conflicting=True,\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/init.py\", line 843, in resolve\ndist = best[req.key] = env.best_match(req, ws, installer)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/init.py\", line 1088, in best_match\nreturn self.obtain(req, installer)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources/init.py\", line 1100, in obtain\nreturn installer(requirement)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/dist.py\", line 380, in fetch_build_egg\nreturn cmd.easy_install(req)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/setuptools/command/easy_install.py\", line 632, in easy_install\nraise DistutilsError(msg)\ndistutils.errors.DistutilsError: Could not find suitable distribution for Requirement.parse('incremental>=16.10.1')\n----------------------------------------\nCommand \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/vk/wbp690w965gcdvmg_67chy380000gn/T/pip-install-9zdxXV/Twisted/", "issue_status": "Closed", "issue_reporting_time": "2018-06-12T16:48:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "363": {"issue_url": "https://github.com/scrapy/scrapy/issues/3289", "issue_id": "#3289", "issue_summary": "Missing syntax highlighting in docs", "issue_description": "faheel commented on Jun 12, 2018\nThe code blocks in the Scrapy docs do not have syntax highlighting which makes it slightly difficult to go through the code.\nIt can be easily added by starting the code blocks with the following RST syntax for defining the language (Python in the following example):\n.. code:: python\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-06-11T18:51:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "364": {"issue_url": "https://github.com/scrapy/scrapy/issues/3287", "issue_id": "#3287", "issue_summary": "raise SchemeNotSupported(\"Unsupported scheme: %r\" % (uri.scheme,)) twisted.web.error.SchemeNotSupported: Unsupported scheme: b'' 2018-06-11 13:02:26 [scrapy.core.engine] INFO: Closing spider (finished) 2018-06-11 13:02:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:", "issue_description": "venkateshchary commented on Jun 11, 2018\ndef start_requests(self):\nrequest = scrapy.Request(url=\"http://www.xxxxx.com\",callback=self.parse ,dont_filter = True,)\nyield request\ncan anyone please help\nif the website is not secured and calling from the start_requests it is raising the error like unsupported scheme b' '\nbut the same url is working in start_urls =[\"http://www.xxxxx.com\"]", "issue_status": "Closed", "issue_reporting_time": "2018-06-11T07:49:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "365": {"issue_url": "https://github.com/scrapy/scrapy/issues/3286", "issue_id": "#3286", "issue_summary": "Scrapy in High Sierra", "issue_description": "canonlp commented on Jun 9, 2018\nI'm getting this error when I run scrapy. Any ideas how to solve this?\n`Unhandled error in Deferred:\n2018-06-09 16:48:56 [twisted] CRITICAL: Unhandled error in Deferred:\n2018-06-09 16:48:56 [twisted] CRITICAL:\nTraceback (most recent call last):\nFile \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\nresult = g.send(result)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 98, in crawl\nsix.reraise(*exc_info)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 80, in crawl\nself.engine = self._create_engine()\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 105, in _create_engine\nreturn ExecutionEngine(self, lambda _: self.stop())\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 70, in init\nself.scraper = Scraper(crawler)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/core/scraper.py\", line 71, in init\nself.itemproc = itemproc_cls.from_crawler(crawler)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/middleware.py\", line 36, in from_settings\nmw = mwcls.from_crawler(crawler)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/pipelines/media.py\", line 68, in from_crawler\npipe = cls.from_settings(crawler.settings)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/pipelines/images.py\", line 98, in from_settings\nreturn cls(store_uri, settings=settings)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/pipelines/images.py\", line 52, in init\ndownload_func=download_func)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/pipelines/files.py\", line 276, in init\nself.store = self._get_store(store_uri)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/pipelines/files.py\", line 315, in _get_store\nreturn store_cls(uri)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/pipelines/files.py\", line 48, in init\nself._mkdir(self.basedir)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/pipelines/files.py\", line 77, in _mkdir\nos.makedirs(dirname)\nFile \"/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 150, in makedirs\nmakedirs(head, mode)\nFile \"/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 150, in makedirs\nmakedirs(head, mode)\nFile \"/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 150, in makedirs\nmakedirs(head, mode)\nFile \"/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 150, in makedirs\nmakedirs(head, mode)\nFile \"/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 157, in makedirs\nmkdir(name, mode)\nOSError: [Errno 45] Operation not supported: '/home/admin'`", "issue_status": "Closed", "issue_reporting_time": "2018-06-09T15:51:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "366": {"issue_url": "https://github.com/scrapy/scrapy/issues/3282", "issue_id": "#3282", "issue_summary": "Update debugging memory leaks section in the docs", "issue_description": "Member\nlopuhin commented on Jun 7, 2018\nhttps://doc.scrapy.org/en/latest/topics/leaks.html#debugging-memory-leaks-with-guppy\nguppy library is recommended, but I think it does not support python 3. It's possible to use muppy instead, which is very similar and has python 3 support: https://pythonhosted.org/Pympler/muppy.html", "issue_status": "Closed", "issue_reporting_time": "2018-06-07T10:52:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "367": {"issue_url": "https://github.com/scrapy/scrapy/issues/3278", "issue_id": "#3278", "issue_summary": "IMAGES_STORE_S3_ACL is not applied to thumbnails", "issue_description": "gcrookie commented on May 30, 2018\nHi,\nI have been playing around with the images pipeline to store images on AWS S3. The full images and thumbnails get stored in S3, but only full images get the ACL applied, the thumbnails don't.\nI was expecting the thumbnails to set the same ACL as defined by IMAGES_STORE_S3_ACL . I could not find any specific scrapy setting for this.\nAppreciate your feedback.\nCheers", "issue_status": "Closed", "issue_reporting_time": "2018-05-30T04:25:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "368": {"issue_url": "https://github.com/scrapy/scrapy/issues/3277", "issue_id": "#3277", "issue_summary": "TypeError: 'generator' object is not callable", "issue_description": "aditya-15 commented on May 29, 2018\nWhen running scrapy crawl <spider_name> from commandline , I am getting the below error .\n`Unhandled error in Deferred:\n2018-05-29 03:00:52 [twisted] CRITICAL: Unhandled error in Deferred:\n2018-05-29 03:00:52 [twisted] CRITICAL:\nTraceback (most recent call last):\nFile \"/usr/src/venv/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\nresult = g.send(result)\nFile \"/usr/src/venv/lib/python3.6/site-packages/scrapy/crawler.py\", line 81, in crawl\nstart_requests = iter(self.spider.start_requests())\nTypeError: 'generator' object is not callable\n`", "issue_status": "Closed", "issue_reporting_time": "2018-05-29T03:24:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "369": {"issue_url": "https://github.com/scrapy/scrapy/issues/3275", "issue_id": "#3275", "issue_summary": "Issue in writing unittestcase for spider", "issue_description": "sdavara commented on May 26, 2018 \u2022\nedited\nHello,\nI've written below test case to test my spider\nimport unittest\nimport base64\nimport json\nfrom scrapy.crawler import Crawler\nfrom scrapy.utils.project import get_project_settings\nfrom myproject.spiders.spider1file import SpiderClass1\ndef create_crawler(spider_class, place):\nspider = spider_class(base64.b64encode(json.dumps(place)))\nsettings = get_project_settings()\ncrawler = Crawler(spider, settings)\ncrawler.configure()\ncrawler.start()\nreturn spider\nspider1 = create_crawler(\nspider_class=SpiderClass1,\nbusiness={\"business\": \"welcome store\"}\n)\nclass TestSpider(unittest.TestCase):\ndef test_searchurl(self):\nself.assertEqual(len(spider.search_requests.url), 20)\nif name == 'main':\nunittest.main()\nWhen, I try to run it with below command:\npython -m unittest myproject.test.Test_spider\nI get following error:\nFile \"myproject/test/Test_spider.py\", line 19, in setup_crawler\ncrawler.configure()\nAttributeError: 'Crawler' object has no attribute 'configure'\nIs there anyone facing same issue?", "issue_status": "Closed", "issue_reporting_time": "2018-05-26T07:59:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "370": {"issue_url": "https://github.com/scrapy/scrapy/issues/3274", "issue_id": "#3274", "issue_summary": "Broadening Scrapy query causes less results to be returned", "issue_description": "dailenspencer commented on May 25, 2018 \u2022\nedited\nI am running into an issue and have not yet been able to figure out the root cause. I have the following Scrapy jobs.py file\n# -*- coding: utf-8 -*-\nimport scrapy\nimport re\nfrom scrapy import Request\nimport csv\nimport os\nfrom datetime import datetime, timedelta\n\n\nclass JobsSpider(scrapy.Spider):\n    \n    name = 'jobs'\n    allowed_domains = ['craigslist.org']\n    present_timestamp = datetime.now()\n\n    \n    # This function handles the verification of a craigslist result item top-level data (title, and url)\n    # TODO: get rid of this extra argument 'log' that is attached to the extract() response from scrapy\n    def verifyTopLevelData (log, title, url, date):\n\n        dataIsValid = True\n                \n        # ensure post date is not older than 2 days\n        post_timestamp = datetime.strptime(date, \"%Y-%m-%d %H:%M\")\n        cutoff_time = datetime.strptime( (datetime.now() - timedelta(days=2)).strftime(\"%Y-%m-%d 00:00\"), \"%Y-%m-%d %H:%M\" )\n\n        # enforce time-restriction\n        if (bool(post_timestamp > cutoff_time) == False):\n            dataIsValid = False\n        \n        return bool(dataIsValid)\n    \n    # This function handles the building of all craigslist ulrs to extract result list items from\n    # TEMPLATE: <rootlocation>.craigstlist.org/search/<search_category>?query=<search_query>\n    def gatherCraigsListUrls():\n         \n        urls = []\n                \n        # craigslist urls are location based, we build urls based on these root locations\n        rootLocations = [\n            \"bakersfield\",\n            \"chico\",\n            \"fresno\",\n            \"goldcountry\",\n            \"hanford\",\n            \"humboldt\",\n            \"imperial\",\n            \"inlandempire\",\n            \"losangeles\",\n            \"mendocino\",\n            \"merced\",\n            \"modesto\",\n            \"monterey\",\n            \"orangecounty\",\n            \"palmsprings\",\n            \"redding\",\n            \"sacramento\",\n            \"sandiego\",\n            \"sfbay\",\n            \"slo\",\n            \"santabarbara\",\n            \"santamaria\",\n            \"siskiyou\",\n            \"stockton\",\n            \"susanville\",\n            \"ventura\",\n            \"visalia\",\n            \"yubasutter\"\n        ]\n\n        # craigslist url search categories \n        searchCategories = [\n            'ggg',\n            'cpg',\n            'web',\n            'sof',\n            'sad',\n            'tch',\n        ]\n\n        # craigslit url search queries\n        # TODO: add regex so we can replace whitespace with '+'\n        searchQueries = [\n            # web\n            'web+developer',\n            'web+design',\n            'react+developer',\n            'angular+developer'\n        ]\n\n        # build urls\n        for location in rootLocations:\n            for category in searchCategories:\n                for query in searchQueries:\n                    url = 'https://' + location + '.craigslist.org/search/' + category + '?query=' + query\n                    urls.append(url)\n       \n        return urls\n  \n    start_urls = gatherCraigsListUrls ()\n\n    \n    def parse(self, response):\n        \n        # proxies can sometimes fail. we check to ensure we've landed on the right page. and if not, re-try but using local port\n        # if not response.xpath('//a[@class=\"header-logo\"]'):\n        #     yield Request(url=response.url, dont_filter=True)\n\n        # grabbing list items from DOM on craigslist result page\n        posts = response.xpath('//p[@class=\"result-info\"]') \n        \n        # loop through result listings and retrieve top-level data (title, and url)\n        for post in posts:\n\n            # top-level data\n            post_title = post.xpath('a/text()').extract_first(\"\")\n            post_relative_url = post.xpath('a/@href').extract_first()\n            post_absolute_url = response.urljoin(post_relative_url)\n            \n            post_date = post.xpath('time[@class=\"result-date\"]/@datetime').extract_first(\"\")\n            push_date = datetime.now().strftime(\"%Y-%m-%d 00:00\"), \"%Y-%m-%d %H:%M\"\n\n            # verify top-level data and proceed with extracting contents from craigslist post page\n            if (self.verifyTopLevelData (post_title, post_absolute_url, post_date)):\n                yield Request(post_absolute_url, callback=self.parse_page, meta={'Title':post_title, 'URL': post_absolute_url})\n            \n\n        # move to next page (using pagination button on DOM) and repeat \"parse\" process\n        nextpage_relative_url = response.xpath('//a[@class=\"button next\"]/@href').extract_first()\n        nextpage_absolute_url = response.urljoin(nextpage_relative_url)\n\n        yield Request(nextpage_absolute_url,callback=self.parse)\n        \n       \n    def parse_page(self, response):\n\n        # proxies can sometimes fail. we check to ensure we've landed on the right page. and if not, re-try but using local port\n        # if not response.xpath('//a[@class=\"header-logo\"]'):\n        #     yield Request(url=response.url, dont_filter=True)\n        \n        description = \"\".join(line for line in response.xpath('//*[@id=\"postingbody\"]/text()').extract())\n        response.meta['Description'] = description\n        \n        yield response.meta\nIf rootLocations is set to an array of just a few items, for example:\nrootLocations = [\"losangeles\", \"sfbay\", \"sandieg\"]\nthen my crawl outputs a bunch of different results. You can see the results here\nresults for small query\nHowever, when I broaden the query to include all craigslist root url locations in california(like in the code example above), I only get back two results. You can see the results here\nresults for broad query\nWhy would my results shrink when adding more urls to the start_urls array?", "issue_status": "Closed", "issue_reporting_time": "2018-05-25T17:00:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "371": {"issue_url": "https://github.com/scrapy/scrapy/issues/3271", "issue_id": "#3271", "issue_summary": "beef is not working in my pc", "issue_description": "prasanth111 commented on May 23, 2018 \u2022\nedited\nUserWarning: You do not have a working installation of the service_identity module: 'cannot import name opentype'. Please install it from https://pypi.python.org/pypi/service_identity and make sure all of its dependencies are satisfied. Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification. Many valid certificate/hostname mappings may be rejected.\nplz help me to start beef", "issue_status": "Closed", "issue_reporting_time": "2018-05-23T07:43:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "372": {"issue_url": "https://github.com/scrapy/scrapy/issues/3268", "issue_id": "#3268", "issue_summary": "Documentation error about 'max_retry_times'", "issue_description": "rbarbaresco commented on May 18, 2018\nThe documentation says that if we add a max_retry_times in meta of a request it would take higher precedence over the RETRY_TIMES settings.\nWell I don't think it's true, because max_retry_times doesn't work at all, but I noticed that retry_times works perfectly as the documentation says max_retry_times would work.\nI believe that the right thing to do is keep the retry_times and fix the documentation, once it looks like the standard for any settings in Scrapy projects.\nI can open a pull request to fix this, but I'm not sure if it's a bug or the documentation is wrong.\n$ scrapy version -v\nScrapy    : 1.3.3\nlxml      : 4.2.1.0\nlibxml2   : 2.9.8\ncssselect : 1.0.3\nparsel    : 1.4.0\nw3lib     : 1.19.0\nTwisted   : 16.4.1\nPython    : 2.7.15rc1 (default, Apr 15 2018, 21:51:34) - [GCC 7.3.0]\npyOpenSSL : 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018)\nPlatform  : Linux-4.15.0-20-generic-x86_64-with-Ubuntu-18.04-bionic\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2018-05-18T18:08:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "373": {"issue_url": "https://github.com/scrapy/scrapy/issues/3267", "issue_id": "#3267", "issue_summary": "scrapyd-deploy default - finishes with error 404", "issue_description": "AndriiIsiuk commented on May 18, 2018 \u2022\nedited\nI am trying to deploy Django + Scrapy project on Ubuntu 16.04. I have installed pip install scrapyd and pip install scrapyd-client. Here is my scrapy.cfg file:\n[settings]\ndefault = first_scrapy.settings\n\n[deploy]\nurl = http://my_ip\nusername = root\npassword = rootpassword\nproject = first_scrapy\nAnd my scrapyd.conf from prod server etc/scrapyd/scrapyd.conf:\n[scrapyd]\nlogs_dir=/home/chiefir/logs/scrapyd\neggs_dir=/home/chiefir/scrapyd/eggs\ndbs_dir=/home/chiefir/scrapyd/dbs\njobs_to_keep=5\nmax_proc=0\nmax_proc_per_cpu=4\nfinished_to_keep=100\npoll_interval=5.0\nbind_address=my_ip\nhttp_port=6550\ndebug=off\nrunner=scrapyd.runner\napplication=scrapyd.app.application\nlauncher=scrapyd.launcher.Launcher\nwebroot=scrapyd.website.Root\n\n[services]\nschedule.json=scrapyd.webservice.Schedule\ncancel.json=scrapyd.webservice.Cancel\naddversion.json=scrapyd.webservice.AddVersion\nlistprojects.json=scrapyd.webservice.ListProjects\nlistversions.json=scrapyd.webservice.ListVersions\nlistspiders.json=scrapyd.webservice.ListSpiders\ndelproject.json=scrapyd.webservice.DeleteProject\ndelversion.json=scrapyd.webservice.DeleteVersion\nlistjobs.json=scrapyd.webservice.ListJobs\ndaemonstatus.json=scrapyd.webservice.DaemonStatus\nWhen I run from my django project folder scrapyd-deploy -l I see: default http://my_ip\nWhen I run scrapyd-deploy default - I see:\nPacking version 1526663819\nDeploying to project \"first_scrapy\" in http://my_ip/addversion.json\nDeploy failed (404): <and here goes full html code of '404.html' page>\nWhy does that happen? And how to fix that? I have made everything as it is written in the documents.", "issue_status": "Closed", "issue_reporting_time": "2018-05-18T17:19:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "374": {"issue_url": "https://github.com/scrapy/scrapy/issues/3264", "issue_id": "#3264", "issue_summary": "Command parse unhandled error :AttributeError: 'NoneType' object has no attribute 'start_requests'", "issue_description": "wangrenlei commented on May 18, 2018 \u2022\nedited\nScrapy version :1.5.0\nWhen i run the command scrapy parse http://www.baidu.com, and the url www.baidu.com dosn't have spider matched , then i got the error:\n2018-03-11 16:23:35 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: DouTu)\n2018-03-11 16:23:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.9.0, Python 2.7.12 (default, Dec 4 2017, 14:50:18) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.1.0h 27 Mar 2018), cryptography 2.2.2, Platform Linux-4.13.0-38-generic-x86_64-with-Ubuntu-16.04-xenial\n2018-05-18 16:23:35 [scrapy.commands.parse] ERROR: Unable to find spider for: http://www.baidu.com\nTraceback (most recent call last):\nFile \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 239, in\nexecute(['scrapy','parse','http://www.baidu.com'])\nFile \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 168, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 98, in _run_print_help\nfunc(*a, **kw)\nFile \"/home/wangsir/code/sourceWorkSpace/scrapy/cmdline.py\", line 176, in _run_command\ncmd.run(args, opts)\nFile \"/home/wangsir/code/sourceWorkSpace/scrapy/commands/parse.py\", line 250, in run\nself.set_spidercls(url, opts)\nFile \"/home/wangsir/code/sourceWorkSpace/scrapy/commands/parse.py\", line 151, in set_spidercls\nself.spidercls.start_requests = _start_requests\nAttributeError: 'NoneType' object has no attribute 'start_requests'.\nThe failed reason should be follwing code(scrapy/commands/parse.py line 151):\nself.spidercls.start_requests = _start_requests\nbecause the url www.baidu.com dosn't have spider matched,so self.spidercls is none,so self.spidercls.start_requests throw the error.", "issue_status": "Closed", "issue_reporting_time": "2018-05-18T08:57:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "375": {"issue_url": "https://github.com/scrapy/scrapy/issues/3262", "issue_id": "#3262", "issue_summary": "crawl zomato.com with error: twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.", "issue_description": "offbye commented on May 16, 2018\nI met a error when crawl zomato.com.\nmy env is\nscrapy version -v\nScrapy       : 1.5.0\nlxml         : 4.2.1.0\nlibxml2      : 2.9.8\ncssselect    : 1.0.3\nparsel       : 1.4.0\nw3lib        : 1.19.0\nTwisted      : 18.4.0\nPython       : 2.7.12 (default, Dec  4 2017, 14:50:18) - [GCC 5.4.0 20160609]\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.0h  27 Mar 2018)\ncryptography : 2.2.2\nPlatform     : Linux-4.4.0-116-generic-x86_64-with-Ubuntu-16.04-xenial\n\n\n scrapy shell  https://www.zomato.com/dubai/abs-absolute-barbecues-al-sufouh/photos?category=ambience\nwith below errors , I have found sevaral closed issues about it , but just not solved.\n2018-05-16 11:28:54 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.zomato.com/dubai/abs-absolute-barbecues-al-sufouh/photos?category=ambience> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]", "issue_status": "Closed", "issue_reporting_time": "2018-05-16T03:31:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "376": {"issue_url": "https://github.com/scrapy/scrapy/issues/3261", "issue_id": "#3261", "issue_summary": "Can not telnet into Scrapy in a container of docker.", "issue_description": "kingname commented on May 15, 2018 \u2022\nedited\nTo track the performance, I usually use telnet localhost 6023 to Scrapy and then execute prefs() to check some info.\nHowever, when my scrapy project running in a container of Docker, I can not execute telnet localhost 6023:\ntelnet localhost 6023\nTrying 127.0.0.1...\ntelnet: Unable to connect to remote host: Connection refused\nP.S.: I have already attach into the container, this command is executed in the container.", "issue_status": "Closed", "issue_reporting_time": "2018-05-15T11:48:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "377": {"issue_url": "https://github.com/scrapy/scrapy/issues/3249", "issue_id": "#3249", "issue_summary": "Error with yield from on scrapy, some requests have been ignored", "issue_description": "Alvis-is-coding commented on May 7, 2018 \u2022\nedited\nHi, there,\nI tried to crawl some data using scrapy. And I did this.\nfor i in range(1,last+1): link = re.sub('pn=(\\d+)', 'pn=%s' %i, respurl, re.S) log.msg(link, log.DEBUG) yield scrapy.Request(url=link, headers=self.headers, meta={'keyword': response.meta['keyword']}, dont_filter=True, callback=self.parse_single)\nI changed the parameter 'pn' in the url to crawl the data. I can see all the urls are generated successfully from the 'log.Debug'. BUT!!! lots of urls are ignored (I don't know if 'ignored' is the right way to describe), these urls just don't enter the parse_single function. I have tried to search for the solution for such a long time, but in vain.\ndo you guys have any idea? thanks!", "issue_status": "Closed", "issue_reporting_time": "2018-05-07T15:34:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "378": {"issue_url": "https://github.com/scrapy/scrapy/issues/3247", "issue_id": "#3247", "issue_summary": "`scrapy.FormRequest.from_response` observed to eliminate duplicate keys in `formdata`", "issue_description": "Contributor\nstarrify commented on May 6, 2018\nThis looks good:\nIn [2]: scrapy.FormRequest('http://example.com', method='GET', formdata=(('foo', 'bar'), ('foo', 'baz')))\nOut[2]: <GET http://example.com?foo=bar&foo=baz>\nWhile here is the issue:\nIn [3]: response = scrapy.http.TextResponse(url='http://example.com', body='<form></form>', encoding='utf8')\n\nIn [4]: scrapy.FormRequest.from_response(response, method='GET', formdata=(('foo', 'bar'), ('foo', 'baz')))\nOut[4]: <GET http://example.com?foo=baz>\n(Tested with Scrapy 1.5.0 and Python 3.6.5)\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-05-06T06:46:39Z", "fixed_by": "#3283", "pull_request_summary": "Fix #3247: Allow scrapy.FormRequest.from_response method to handle duplicate keys", "pull_request_description": "Contributor\nCCInCharge commented on Jun 8, 2018\nFix #3247\nDicts do not allow duplicate keys. When form requests have duplicate keys, one of the values of the duplicate keys is dropped when created via the FormRequest's from_response method.\nformdata is either a dict or iterable of tuples - this patch fixes the above issue by not converting formdata to a dict and keeping it as an iterable of tuples if possible.\nAlso, a unit test is added to verify correct functionality. This is my first PR, definitely let me know if any changes are needed!", "pull_request_status": "Merged", "issue_fixed_time": "2018-08-02T17:49:15Z", "files_changed": [["11", "scrapy/http/request/form.py"], ["23", "tests/test_http_request.py"]]}, "379": {"issue_url": "https://github.com/scrapy/scrapy/issues/3245", "issue_id": "#3245", "issue_summary": "DEPTH_STATS has no effect", "issue_description": "Contributor\nrpkilby commented on May 4, 2018\nIt looks like this flag stopped functioning in #99. crawler.stats are always passed to the middleware, and are always collected.\nAdditionally, it looks like this middleware would error if stats weren't provided, as the initial depth value would otherwise never be set.\nscrapy/scrapy/spidermiddlewares/depth.py\nLines 53 to 56 in c4f096d\n if self.stats and 'depth' not in response.meta: \n     response.meta['depth'] = 0 \n     if self.verbose_stats: \n         self.stats.inc_value('request_depth_count/0', spider=spider) ", "issue_status": "Closed", "issue_reporting_time": "2018-05-03T22:01:11Z", "fixed_by": "#3253", "pull_request_summary": "[MRG+1] Update depth middleware stats (fixes #3245)", "pull_request_description": "Contributor\nrpkilby commented on May 9, 2018 \u2022\nedited by kmike\nUpdate the docs & default settings to reference DEPTH_STATS_VERBOSE instead of DEPTH_STATS, which is not currently used.\nRemove unneeded branches for self.stats checks, as stats are always provided (no way to not provide a stats object).\n(fixes #3245)", "pull_request_status": "Merged", "issue_fixed_time": "2018-07-11T15:00:09Z", "files_changed": [["11", "docs/topics/settings.rst"], ["3", "docs/topics/spider-middleware.rst"], ["2", "scrapy/settings/default_settings.py"], ["6", "scrapy/spidermiddlewares/depth.py"]]}, "380": {"issue_url": "https://github.com/scrapy/scrapy/issues/3243", "issue_id": "#3243", "issue_summary": "How can i change a new proxy with middleware when i get 403/302?", "issue_description": "Yuan-Hang commented on Apr 30, 2018 \u2022\nedited\ni have write down the middleware:\ndef process_request(self, request, spider):\n    request = self.change_proxy(request)   # set request proxy\n\ndef process_response(self, request, response, spider):\n    if response.status != 200:\n        self.delete_proxy()           # remove unusable proxy\n        return request.copy()         # send request to process_request to change proxy\n    return response\n\ndef process_exception(self, request, exception, spider):\n    self.delete_proxy()              # remove unusable proxy\n    # if comment the code below, i will not get 302. but maybe i will lost crawl some webpage?\n    return request.copy()            # send request to process_request to change proxy\nFirst, I have some proxies in redis, what i want to do is:\ngive each request a random proxy,\nwhen the connection to proxy failed(process_exception called), change a new proxy and re-crawl the webpage.\nwhen proxy is banned by the website(response.status != 200), change a new proxy and re-crawl the webpage.", "issue_status": "Closed", "issue_reporting_time": "2018-04-30T12:04:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "381": {"issue_url": "https://github.com/scrapy/scrapy/issues/3242", "issue_id": "#3242", "issue_summary": "Installation fails on Void Linux", "issue_description": "Vintodrimmer commented on Apr 29, 2018 \u2022\nedited\nGood day,\nI'm trying to install Scrapy for Python3, using pip install --user scrapy. This results in the following error:\n       { result = SSL_set_mode(x0, x1); }\n                  ^~~~~~~~~~~~\n    build/temp.linux-x86_64-3.6/_openssl.c:52849:14: warning: conversion to 'long unsigned int' from 'long int' may change the sign of the result [-Wsign-conversion]\n    build/temp.linux-x86_64-3.6/_openssl.c: In function '_cffi_d_SSL_set_options':\n    build/temp.linux-x86_64-3.6/_openssl.c:52862:10: warning: conversion to 'long int' from 'long unsigned int' may change the sign of the result [-Wsign-conversion]\n       return SSL_set_options(x0, x1);\n              ^~~~~~~~~~~~~~~\n    build/temp.linux-x86_64-3.6/_openssl.c:52862:10: warning: conversion to 'long unsigned int' from 'long int' may change the sign of the result [-Wsign-conversion]\n    build/temp.linux-x86_64-3.6/_openssl.c: In function '_cffi_f_SSL_set_options':\n    build/temp.linux-x86_64-3.6/_openssl.c:52895:14: warning: conversion to 'long int' from 'long unsigned int' may change the sign of the result [-Wsign-conversion]\n       { result = SSL_set_options(x0, x1); }\n                  ^~~~~~~~~~~~~~~\n    build/temp.linux-x86_64-3.6/_openssl.c:52895:14: warning: conversion to 'long unsigned int' from 'long int' may change the sign of the result [-Wsign-conversion]\n    error: command 'gcc' failed with exit status 1\n\n    ----------------------------------------\nCommand \"/usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-21gspbgp/cryptography/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-t8vxql47-record/install-record.txt --single-version-externally-managed --compile --user --prefix=\" failed with error code 1 in /tmp/pip-build-21gspbgp/cryptography/\nI have installed following packages:\npython3-devel\nlibffi-devel\nlibxml2-devel\nlibressl-devel\nlibxslt-devel\nzlib-devel\nMay that be because of libreSSL?", "issue_status": "Closed", "issue_reporting_time": "2018-04-29T13:46:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "382": {"issue_url": "https://github.com/scrapy/scrapy/issues/3241", "issue_id": "#3241", "issue_summary": "[bug?] sending cookies with `cookies=` and `cookie in headers` and `'dont merge cookie' meta`", "issue_description": "Contributor\nNewUserHa commented on Apr 28, 2018\nfetch('https://httpbin.org/cookies/set?foo=bar', cookies={'foo1':'bar1'}, meta={'dont_merge_cookies': True}, headers={'Cookie':'foor2=bar2'})\nrequest.cookies\nrequest.headers['cookie']\nis:\n{'foo1': 'bar1'}\nb'foor2=bar2'\nbut response.text:\n{\\n \"cookies\": {\\n \"foor2\": \"bar2\"\\n }\\n}\\n\nshouldn't request.cookies been sent with the fetch request?", "issue_status": "Closed", "issue_reporting_time": "2018-04-28T04:05:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "383": {"issue_url": "https://github.com/scrapy/scrapy/issues/3239", "issue_id": "#3239", "issue_summary": "CSVItemExporter creates multiple files", "issue_description": "dmikhaylov commented on Apr 27, 2018\nI'm not sure if it's right place to post this issue, but what I've just noticed is that CSVItemExporter creates multiple files for some reason with the same name (they have a number as the suffix though), so they look like\nmyspider_2018-04-26_15.09.06.231201 (26).csv\nmyspider_2018-04-26_15.09.06.231201 (27).csv\nI use docker to run scrapy and it happens only on my Ubuntu machine. Again, I'm not sure if it's scrapy issue...", "issue_status": "Closed", "issue_reporting_time": "2018-04-27T06:40:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "384": {"issue_url": "https://github.com/scrapy/scrapy/issues/3235", "issue_id": "#3235", "issue_summary": "Is possible to configure rollover log file via settings?", "issue_description": "Urahara commented on Apr 25, 2018 \u2022\nedited\nOr the only way today to rotate logs is to implement a handle on spiders?\nI configured:\nLOG_FILE = 'log.txt'\nLOG_LEVEL = 'INFO'\nFile name and log level, but unfortunately on doc i can't find anything related to rotate the logs, the only way i see that is possible is to add another handler on spiders, i want to know if is possible to change to change this on settings.py.\nIf is not possible i think this will be a great feature.", "issue_status": "Closed", "issue_reporting_time": "2018-04-24T20:18:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "385": {"issue_url": "https://github.com/scrapy/scrapy/issues/3234", "issue_id": "#3234", "issue_summary": "scrapy+tor+privoxy+HTTPS=403 (User-Agent is correct) but for requests+tor+privoxy+HTTPS=200", "issue_description": "tillroy commented on Apr 24, 2018 \u2022\nedited\nHello Scrapy team, thank you for your great work.\nI've implement all neccesary parts for anonymous scraping using Scrapy with Tor and Privoxy as proxy. It works great with HTTP protocol and returns values wich are expected. But in case of HTTPS - got 403 error. I know that it could be connected with User-Agent, but in this case it's setuped in download middleware:\nclass Obscure(object):\n    def __init__(self, settings):\n        self.settings = settings\n        user_agent_list_file = self.settings.get('USER_AGENTS_FILE')\n\n        if not user_agent_list_file:\n            ua = self.settings.get('USER_AGENT', \"Scrapy-bot\")\n            self.user_agents = [ua]\n        else:\n            with open(user_agent_list_file, 'r') as f:\n                self.user_agents = [line.strip() for line in f.readlines()]\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        s = cls(crawler.settings)\n        return s\n\n    def change_tor_ip(self):\n        with Controller.from_port(port=9051) as controller:\n            controller.authenticate(password=self.settings.get('TOR_PASSWORD'))\n            controller.signal(Signal.NEWNYM)\n            controller.close()\n\n    def get_user_agent(self):\n        return choice(self.user_agents)\n\n    def process_request(self, request, spider):\n        self.change_tor_ip()\n        request.headers.setdefault('User-Agent', self.get_user_agent())\n        proxy_host = self.settings.get(\"PRIVOXY_HOST\")\n        if proxy_host:\n            proxy_url = proxy_host\n            request.meta['proxy'] = proxy_url\nhere is sertting.py:\nUSER_AGENTS_FILE = \"user_agents.txt\"\nTOR_PORT = 9050\nTOR_PASSWORD = \"pwd\"\nPRIVOXY = \"http://127.0.0.1:8118\"\nand spider itself:\nclass CheckIp(Spider):\n    name = \"checkip\"\n    # start_urls = (\"http://icanhazip.com/\",)\n    start_urls = (\"https://botproxy.net/docs/how-to/how-to-solve-403-error-in-scrapy/\",)\n\n    def parse(self, resp):\n\n        item = Ip()\n\n        request_headers = resp.request.headers\n        ua = request_headers.get(\"User-Agent\")\n        ip = resp.text.strip()\n\n        item[\"ip\"] = ip\n        item[\"ua\"] = ua\n\n        return item\nBut when i try to do the same with requests library i got 200 response with changed IP:\nr = requests.get('https://botproxy.net/docs/how-to/how-to-solve-403-error-in-scrapy/', proxies={\"http\": \"http://127.0.0.1:8118\"})\nprint(r)\nWould be really grateful for your suggestions and comments\nThank you", "issue_status": "Closed", "issue_reporting_time": "2018-04-24T13:44:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "386": {"issue_url": "https://github.com/scrapy/scrapy/issues/3233", "issue_id": "#3233", "issue_summary": "Settings from command line not working?", "issue_description": "Contributor\nthernstig commented on Apr 24, 2018\nHi,\nI am trying to use the -s flag according to this:\nhttps://doc.scrapy.org/en/latest/topics/settings.html#command-line-options\nBut when I try it like this, the setting does not take effect: scrapy crawl spider.quotes.toscrape.com -s ELASTICSEARCH_ENABLED=False\nIt works if I manually edit settings.py and set ELASTICSEARCH_ENABLED to False.\nWhy is this?", "issue_status": "Closed", "issue_reporting_time": "2018-04-23T21:44:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "387": {"issue_url": "https://github.com/scrapy/scrapy/issues/3232", "issue_id": "#3232", "issue_summary": "New sphinx==1.7.3 breaks all docs", "issue_description": "Contributor\nwhalebot-helmsman commented on Apr 23, 2018\nWith new version of sphinx, documentation is failed to build, e.g. https://travis-ci.org/scrapy/scrapy/jobs/369994787\nWith sphinx==1.7.2 all works as intended", "issue_status": "Closed", "issue_reporting_time": "2018-04-23T15:48:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "388": {"issue_url": "https://github.com/scrapy/scrapy/issues/3229", "issue_id": "#3229", "issue_summary": "Any scrapy command I run gives me 'ResponseNeverReceived' error", "issue_description": "thecpdubguy commented on Apr 22, 2018\nI've seen variations of this issue but no \"solution\" I've seen has worked for me.\nThe crawler was working for me at least a month ago. It stopped working (when I was still using scrapy 1.4) no code change, now I'm getting the issue below. I've tried upgrading to scrapy 1.5 but still hasn't fixed the problem.\nI have also tried updating twisted but that caused other errors, I've tried downgrading cryptography to version 1.9 and that has not worked either. (Those are the only suggestions I've seen in other closed threads.)\n2018-04-21 18:17:31 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: GatherRecipes)\n2018-04-21 18:17:31 [scrapy.utils.log] INFO: Versions: lxml 3.8.0.0, libxml2 2.9.4, cssselect 1.0.1, parsel 1.2.0, w3lib 1.18.0,Twisted 16.0.0, Python 2.7.10 (default, Oct 23 2015, 19:19:21) - [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)], pyOpenSSL 0.13.1 (OpenSSL 0.9.8zh 14 Jan 2016), cryptography 2.2.2, Platform Darwin-15.6.0-x86_64-i386-64bit\n2018-04-21 18:17:31 [scrapy.crawler] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'GatherRecipes.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 1, 'SPIDER_MODULES': ['GatherRecipes.spiders'], 'BOT_NAME': 'GatherRecipes', 'DOWNLOAD_DELAY': 3}\n2018-04-21 18:17:31 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2018-04-21 18:17:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2018-04-21 18:17:31 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2018-04-21 18:17:31 [scrapy.middleware] INFO: Enabled item pipelines:\n['GatherRecipes.pipelines.ValidatePipeline',\n 'GatherRecipes.pipelines.DedupePipeline',\n 'GatherRecipes.pipelines.FormatPipeline',\n 'GatherRecipes.pipelines.PostPipeline']\n2018-04-21 18:17:31 [scrapy.core.engine] INFO: Spider opened\n2018-04-21 18:17:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2018-04-21 18:17:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://allrecipes.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]\n2018-04-21 18:17:39 [scrapy.core.engine] INFO: Closing spider (finished)\n2018-04-21 18:17:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 3,\n 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,\n 'downloader/request_bytes': 639,\n 'downloader/request_count': 3,\n 'downloader/request_method_count/GET': 3,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2018, 4, 21, 22, 17, 39, 1797),\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'memusage/max': 53305344,\n 'memusage/startup': 53305344,\n 'retry/count': 2,\n 'retry/max_reached': 1,\n 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,\n 'scheduler/dequeued': 3,\n 'scheduler/dequeued/memory': 3,\n 'scheduler/enqueued': 3,\n 'scheduler/enqueued/memory': 3,\n 'start_time': datetime.datetime(2018, 4, 21, 22, 17, 31, 770852)}\n2018-04-21 18:17:39 [scrapy.core.engine] INFO: Spider closed (finished)\nThis is the results of running scrapy version -v\nScrapy       : 1.5.0\nlxml         : 3.8.0.0\nlibxml2      : 2.9.4\ncssselect    : 1.0.1\nparsel       : 1.2.0\nw3lib        : 1.18.0\nTwisted      : 16.0.0\nPython       : 2.7.10 (default, Oct 23 2015, 19:19:21) - [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)]\npyOpenSSL    : 0.13.1 (OpenSSL 0.9.8zh 14 Jan 2016)\ncryptography : 2.2.2\nPlatform     : Darwin-15.6.0-x86_64-i386-64bit", "issue_status": "Closed", "issue_reporting_time": "2018-04-21T22:23:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "389": {"issue_url": "https://github.com/scrapy/scrapy/issues/3228", "issue_id": "#3228", "issue_summary": "'SelectorList' object has no attribute 'get'", "issue_description": "mouday commented on Apr 21, 2018 \u2022\nedited\ni can use this code in local, but! move other computer show some error\nlink = classify.xpath(\"@href\").get()\nerror\nAttributeError: 'SelectorList' object has no attribute 'get'\nuse version Scrapy 1.1.2 == Scrapy 1.1.2\nand i read scrapy source code\n    def extract(self):\n        \"\"\"\n        Call the ``.extract()`` method for each element is this list and return\n        their results flattened, as a list of unicode strings.\n        \"\"\"\n        return [x.extract() for x in self]\n    getall = extract\n\n    def extract_first(self, default=None):\n        \"\"\"\n        Return the result of ``.extract()`` for the first element in this list.\n        If the list is empty, return the default value.\n        \"\"\"\n        for x in self:\n            return x.extract()\n        else:\n            return default\n    get = extract_first", "issue_status": "Closed", "issue_reporting_time": "2018-04-21T05:47:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "390": {"issue_url": "https://github.com/scrapy/scrapy/issues/3227", "issue_id": "#3227", "issue_summary": "Provide a way of getting response download time", "issue_description": "Member\neliasdorneles commented on Apr 20, 2018\nHi folks!\nSo today I needed a simple way to evaluate the performance for all the pages of a website, and I thought scrapy would be a great tool for quickly visiting all the pages and getting this information.\nBasically, I'm interested in the time of downloading a page, and maybe more details if possible (time to start a connection, and etc).\nWhile it was fairly easy to get the spider visiting all the pages in no time (+700 pages in ~3 min), getting the response download times per URL seems quite a challenge.\nI found this question in SO which made me feel less alone, but the recommended approach of using a downloader middleware in the existing answer doesn't really work because the requests are often processed way before they are actually sent.\nI tried looking at the code to see where one would need to hook to get this information, I noticed that there is some code collecting time information (like here and here), but I didn't have time to dig much.\nIn the end, I gave up and went with a requests-based script that takes 30 minutes to run but it was easy to get the information, but I would much prefer to use Scrapy for this. =)\nI'd love to hear if anyone has any thoughts about this -- thanks in advance!", "issue_status": "Closed", "issue_reporting_time": "2018-04-20T16:29:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "391": {"issue_url": "https://github.com/scrapy/scrapy/issues/3225", "issue_id": "#3225", "issue_summary": "I can't get entire page source, but truncated instead :(", "issue_description": "xzycn commented on Apr 20, 2018 \u2022\nedited\nAs I said in issue title, it's happened at I crawled a Amazon page\u3002\nThen I took a test with scrapy shell\uff1a\nscrapy shell https://www.amazon.com/Funko-Animation-Morty-Pickle-Collectible-Figure/dp/B07569DYGN/ref=sr_1_28?s=toys-and-games&ie=UTF8&qid=1524195528&sr=1-28%3E%20\\(referer:%20https://www.amazon.com/s/ref=lp_165993011_pg_2/143-3511218-0066800?rh=n%3A165793011%2Cn%3A%21165795011%2Cn%3A165993011&page=2&ie=UTF8&qid=1524195518\nThe issue still there.", "issue_status": "Closed", "issue_reporting_time": "2018-04-20T06:37:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "392": {"issue_url": "https://github.com/scrapy/scrapy/issues/3223", "issue_id": "#3223", "issue_summary": "530 error trying to open FTP directory even when supplying correct username and password", "issue_description": "chrismp commented on Apr 18, 2018\nI want to use Scrapy to download files and navigate folders at ftp://ftp.co.palm-beach.fl.us/Building%20Permits/.\nHere's my spider:\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.http import Request\n\nclass LatestPermitsSpider(scrapy.Spider):\n name=   \"latest_permits\"\n allowed_domains=[\"ftp.co.palm-beach.fl.us\"]\n handle_httpstatus_list = [404]\n \n ftpUser=  \"the_username\"\n ftpPW=   \"the_password\"\n permitFilesDir= \"ftp://ftp.co.palm-beach.fl.us/Building%20Permits/\"\n\n def start_requests(self):\n  yield Request(\n   url=self.permitFilesDir,\n   meta={\n    \"ftp_user\": self.ftpUser,\n    \"ftp_password\": self.ftpPW\n   }\n  )\n\n def parse(self,response):\n  print response.body\nWhen I run scrapy crawl latest_permits, I get this error:\nConnectionLost: ('FTP connection lost', <twisted.python.failure.Failure twisted.protocols.ftp.CommandFailed: ['530 Sorry, no ANONYMOUS access allowed.']>)\nWhy does this error come up even when I supply the correct username and password?\nI do not know the FTP server's port number.", "issue_status": "Closed", "issue_reporting_time": "2018-04-17T20:23:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "393": {"issue_url": "https://github.com/scrapy/scrapy/issues/3222", "issue_id": "#3222", "issue_summary": "scrapy check Unhandled error in Deferred: with DOWNLOADER_MIDDLEWARES", "issue_description": "aleroot commented on Apr 17, 2018 \u2022\nedited\nWhen I have some downloadmiddlewares and I ran scrapy check I got the error:\nUnhandled error in Deferred:\nthe only way to solve is to comment out the DOWNLOADER_MIDDLEWARES section inside settings.py !\nI usually have these downloader middlewares :\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,\n    'scrapy_proxies.RandomProxy': 100,\n    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,\n}\nI was able to workaround the issue running scrapy check with the following commnad line args:\nscrapy check -s DOWNLOADER_MIDDLEWARES={}", "issue_status": "Closed", "issue_reporting_time": "2018-04-17T14:10:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "394": {"issue_url": "https://github.com/scrapy/scrapy/issues/3219", "issue_id": "#3219", "issue_summary": "Does response.meta[redirect_urls] include the final response.url?", "issue_description": "DallanQ commented on Apr 16, 2018\nWhen my spider.parse function is called, does response.meta[redirect_urls] include the final response.url? I thought it did, but when I ran a spider today it did not.", "issue_status": "Closed", "issue_reporting_time": "2018-04-16T16:57:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "395": {"issue_url": "https://github.com/scrapy/scrapy/issues/3218", "issue_id": "#3218", "issue_summary": "Help", "issue_description": "catsled commented on Apr 16, 2018\nI need help\ni want to deep in Downloader, but i'm a beginner of scrapy, and the source code is too difficult for me.If anybody can give me some advice. (like: 1. you can study some source code , and then ...).\nthanks a lot!", "issue_status": "Closed", "issue_reporting_time": "2018-04-16T02:11:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "396": {"issue_url": "https://github.com/scrapy/scrapy/issues/3216", "issue_id": "#3216", "issue_summary": "how to run Multiple spider ** independently ** at the same time ?", "issue_description": "xiaochonzi commented on Apr 14, 2018 \u2022\nedited\nwhen i wrote two simple spider\nclass Bing(Spider):\n\n    name = \"bing\"\n\n    def start_requests(self):\n        for _ in range(1000):\n            yield Request(\"http://bing.com\", dont_filter=True)\n\n    def parse(self, response):\n        print(self.name, response.url)\n\nclass Sogou(Spider):\n    name = \"sogou\"\n    def start_requests(self):\n        for _ in range(1000):\n            yield Request('http://sogou.com', dont_filter=True)\n\n    def parse(self, response):\n        while True:\n            print('123')\n        print(self.name, response.url)\ndef run(spider_cls):\n    crawler = Crawler(spider_cls)\n    crawler.crawl()\n\nrun(Bing)\nrun(Sogou)\n\nreactor.run()\njust like this. Use Crawler and reactor to run these spider. Sogou spider was blocked, it was obvious. but Bing spider was blocked too, why ? and how to solve it . use Crawler class make multiple spider run independently at the same time? I used to use twisted.threads like deferToThread , or callInThread , it couldn't work . Can someone have some idea? Thank you very much!", "issue_status": "Closed", "issue_reporting_time": "2018-04-14T08:11:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "397": {"issue_url": "https://github.com/scrapy/scrapy/issues/3215", "issue_id": "#3215", "issue_summary": "Can't run `scrapy startproject tutorial`", "issue_description": "dustinmichels commented on Apr 14, 2018\nI can't seem to run the scrapy startproject tutorial command successfully (though I've been to in the past).\nI'm using macOS High Sierra, 10.13.4 (recently updated)\nI created a fresh conda environment with conda create --name scrape python=3.6 and source activate scrape\nI ran conda install -c conda-forge scrapy then scrapy startproject tutorial\nI get this error:\nTraceback (most recent call last):\n  File \"/Users/dustymichels/miniconda3/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/Users/dustymichels/miniconda3/lib/python3.6/site-packages/scrapy/__init__.py\", line 34, in <module>\n    from scrapy.spiders import Spider\n  File \"/Users/dustymichels/miniconda3/lib/python3.6/site-packages/scrapy/spiders/__init__.py\", line 10, in <module>\n    from scrapy.http import Request\n  File \"/Users/dustymichels/miniconda3/lib/python3.6/site-packages/scrapy/http/__init__.py\", line 11, in <module>\n    from scrapy.http.request.form import FormRequest\n  File \"/Users/dustymichels/miniconda3/lib/python3.6/site-packages/scrapy/http/request/form.py\", line 11, in <module>\n    import lxml.html\n  File \"/Users/dustymichels/miniconda3/lib/python3.6/site-packages/lxml/html/__init__.py\", line 54, in <module>\n    from .. import etree\nImportError: dlopen(/Users/dustymichels/miniconda3/lib/python3.6/site-packages/lxml/etree.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libicui18n.58.dylib\n  Referenced from: /Users/dustymichels/miniconda3/lib/libxslt.1.dylib\n  Reason: image not found\nAlso: xcode-select --install returns\nxcode-select: error: command line tools are already installed, use \"Software Update\" to install updates", "issue_status": "Closed", "issue_reporting_time": "2018-04-14T02:15:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "398": {"issue_url": "https://github.com/scrapy/scrapy/issues/3214", "issue_id": "#3214", "issue_summary": "crawling", "issue_description": "Six-wars commented on Apr 13, 2018\nI have this snippet of a spider in scrapy\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nfrom infoseeker.items import InfoseekerItem as InfoItem\n\n\n\nclass SeekerSpider(CrawlSpider):\n    name = 'seeker'\n    allowed_domains = ['info.mzalendo.com']\n    start_urls = ['http://info.mzalendo.com/position/member-national-assembly/?page=1']\n    main_url = 'http://info.mzalendo.com/position/member-national-assembly/'\n    urls = []\n    retrieving = False\n\n    def parse(self, response):\n        if not self.retrieving:\n            selector_list = response.css('.position')\n\n            for selector in selector_list:\n                self.urls.append(selector.css('a::attr(href)').extract()[0])\n\n            found = response.css('.next::attr(href)').extract()\n            if found:\n                next_page = self.main_url + found[0] #uses ?page=2, ?page=3 format so appended to main url to avoid issues\n            else:\n                next_page = None\n\n            if next_page is not None:\n                yield response.follow(next_page, self.parse)\n            else:\n                self.retrieving = True\n\n        #should run once all urls have been found\n        for url in self.urls:\n            pass #get content for url to be parsed\nsince the content I want to query are listed on pages, first I've queried all the pages and retrieved the list of urls and stored them in self.urls what I intend to do once this process is complete is start querying the urls to now retrieve the useful info.\nNot sure if yield would be the suitable command to use.\nI've tried also first checking if someone has asked a similar question on stackoverflow after this asked the question and so far no response.", "issue_status": "Closed", "issue_reporting_time": "2018-04-13T07:56:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "399": {"issue_url": "https://github.com/scrapy/scrapy/issues/3213", "issue_id": "#3213", "issue_summary": "HttpCompressionMiddleware bug", "issue_description": "a232319779 commented on Apr 13, 2018\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/downloadermiddlewares/httpcompression.py#L28\nIt's should be modify.\nrequest.headers.setdefault('Accept-Encoding',\nb\",\".join(ACCEPTED_ENCODINGS))\n===>>\nrequest.headers.setdefault(b'Accept-Encoding',\nb\",\".join(ACCEPTED_ENCODINGS))", "issue_status": "Closed", "issue_reporting_time": "2018-04-13T06:15:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "400": {"issue_url": "https://github.com/scrapy/scrapy/issues/3211", "issue_id": "#3211", "issue_summary": "Incorrect return code for spiders with exception raising in __init__ or from_crawler", "issue_description": "Contributor\nwhalebot-helmsman commented on Apr 11, 2018\nSometimes you raise an exception from spider __init__ or from_crawler methods. Exit code of scrapy runspider discovery.py process is 0 in such case. This behaviour prevents correct management of spider runs by any supervisor.\nShould scrapy report such problems using non-zero exit code?\nE.g. for spider\nimport scrapy\n\nclass DiscoverySpider(scrapy.Spider):\n    name = 'discovery'\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        raise ValueError('Exception in from_crawler method')\noutput is\n[]$  scrapy runspider discovery.py \n2018-04-11 12:51:22 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n2018-04-11 12:51:22 [scrapy.utils.log] INFO: Versions: lxml 4.0.0.0, libxml2 2.9.5, cssselect 1.0.1, parsel 1.2.0, w3lib 1.18.0, Twisted 17.9.0, Python 3.4.3 (default, Nov 17 2016, 01:08:31) - [GCC 4.8.4], pyOpenSSL 17.3.0 (OpenSSL 1.1.0f  25 May 2017), cryptography 2.0.3, Platform Linux-4.4.0-96-generic-x86_64-with-Ubuntu-14.04-trusty\n2018-04-11 12:51:22 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}\n2018-04-11 12:51:22 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.memusage.MemoryUsage']\nUnhandled error in Deferred:\n2018-04-11 12:51:22 [twisted] CRITICAL: Unhandled error in Deferred:\n\n2018-04-11 12:51:22 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"python3.4/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\n    result = g.send(result)\n  File \"python3.4/site-packages/scrapy/crawler.py\", line 79, in crawl\n    self.spider = self._create_spider(*args, **kwargs)\n  File \"python3.4/site-packages/scrapy/crawler.py\", line 102, in _create_spider\n    return self.spidercls.from_crawler(self, *args, **kwargs)\n  File \"fail/discovery.py\", line 9, in from_crawler\n    raise ValueError('Exception in from crawler method')\nValueError: Exception in from crawler method\n\n[]$ echo $?\n0", "issue_status": "Closed", "issue_reporting_time": "2018-04-11T13:09:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "401": {"issue_url": "https://github.com/scrapy/scrapy/issues/3208", "issue_id": "#3208", "issue_summary": "Scrapy exits before all items have been processed", "issue_description": "sampeka commented on Apr 10, 2018 \u2022\nedited\nI have a crawler that scrapes hundreds of items per request that need to be processed by the item pipeline. Usually this involves downloading files, uploading them to a cloud provider, then posting some json to a web service. Due to this, an item can take a long time to pass through the whole pipeline.\nThis means that most of the time the spider is finished crawling new urls before the items have finished being processed. I may be misreading this, but after a while it looks like the process is killed and some items are left unprocessed. My settings are pretty much out of the box, no max depth etc.\nIs this a bug or intended behaviour? Is there a way to prevent the scrapy process from exiting until all items have been processed?", "issue_status": "Closed", "issue_reporting_time": "2018-04-10T14:41:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "402": {"issue_url": "https://github.com/scrapy/scrapy/issues/3207", "issue_id": "#3207", "issue_summary": "scrapy first try setting up links to search", "issue_description": "Six-wars commented on Apr 9, 2018\nI'm new to scrapy, I've been able to crawl one page but expect it to also crawl specific new pages. Started with an attempt to crawl stackoverflow like so\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nfrom items import QuestionItem\n\n\nclass FirstSpider(CrawlSpider):\n    name = 'first'\n    allowed_domains = ['stackoverflow.com']\n    start_urls = ['https://stackoverflow.com/questions']\n\n    rules = (\n            Rule(LinkExtractor(allow=['/questions/\\?page=\\d&sort=newest']), callback='parse'),\n    )\n\n    def parse(self, response):\n\n        selector_list = response.css('.question-summary')\n\n        for selector in selector_list:\n            item = QuestionItem()\n            item['question'] = selector.css('h3 a::text').extract()\n            item['votes'] = selector.css('.vote-count-post strong::text').extract()\n            item['answers'] = selector.css('.status strong::text').extract()\n            item['views'] = selector.css('.views ::text').extract()[0].replace('\\n','').replace('\\r','').lstrip()\n            item['username'] = selector.css('.user-details a::text').extract()\n            item['userlink'] = selector.css('.user-details a::attr(href)').extract()\n\n            yield item\npages to \"pages\" of questions on stackoverflow have this format /questions?page=2&sort=newest and the way I've setup my regex\nrules = (\n        Rule(LinkExtractor(allow=['/questions/\\?page=\\d&sort=newest']), callback='parse'),\n)\ndoesn't seem to lead to any new pages being discovered. I've asked this on stackoverflow and slack group for python without getting any response so any help would be appreciated.", "issue_status": "Closed", "issue_reporting_time": "2018-04-09T13:26:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "403": {"issue_url": "https://github.com/scrapy/scrapy/issues/3206", "issue_id": "#3206", "issue_summary": "[STORAGE] Is there a way to export items every XX item scraped?", "issue_description": "maximenannan commented on Apr 9, 2018\nHi there,\nIs it possible to export items scraped every X items ?\nI mean I have millions of items to scrape and am looking for a way to export my items every 100k items scraped for example.\nOne solution could be to have batch of urls and run one scraper per batch but maybe there is a more clever solution.\nThanks a lot", "issue_status": "Closed", "issue_reporting_time": "2018-04-09T12:16:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "404": {"issue_url": "https://github.com/scrapy/scrapy/issues/3203", "issue_id": "#3203", "issue_summary": "How can I get return data after spider yield item", "issue_description": "heianhu commented on Apr 6, 2018\nI have a question.\nNow I have a spider to do something\n\nAnd then I yield this item to pipeline\n\nIn items.py, it will do something\n\nSo how can I get the return data after spider yield item?", "issue_status": "Closed", "issue_reporting_time": "2018-04-06T16:39:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "405": {"issue_url": "https://github.com/scrapy/scrapy/issues/3202", "issue_id": "#3202", "issue_summary": "DropItem interrupts Generator", "issue_description": "doprdele commented on Apr 6, 2018\nConsider that you are scraping multiple items on a site in a loop\nfor p in [..]:\nyield Item({'title': p['title']})\nand a middleware exists that Drops items with certain titles\nIf one of the items is dropped in the loop then the rest of the items do not process", "issue_status": "Closed", "issue_reporting_time": "2018-04-06T16:34:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "406": {"issue_url": "https://github.com/scrapy/scrapy/issues/3200", "issue_id": "#3200", "issue_summary": "[feature] need a new request method that handle with ajax and return value without callback", "issue_description": "Contributor\nNewUserHa commented on Apr 4, 2018 \u2022\nedited\nis it possible to make a new request method that can be added to scrapy schedule queue by scrapy engine?\nsituation: there's a page that contains all information, and a small part(is not at the bottom of page) of it are returned by an ajax whose url is available(seeable) in network developer tool\nsolution: so the simplest way is to use requests.get() of requests module to get that url, but how to achieve this with only scrapy that can also handle the same cookies of the site.\ndiscuss: that add one more callback and pass all result/items to it by meta is possible, but is it also redundancy?\nthanks", "issue_status": "Closed", "issue_reporting_time": "2018-04-04T11:35:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "407": {"issue_url": "https://github.com/scrapy/scrapy/issues/3192", "issue_id": "#3192", "issue_summary": "Unable to pass spider-specific settings programmatically", "issue_description": "marcfrancis961 commented on Mar 29, 2018\nI build a small app to deploy, launch and monitor spiders from one big scrapy project.\nI am using scrapyd to scheduel the spiders through basic python requests:\nrequests.post('http://localhost:6800/schedule.json', params=spider_body)\nspider_body being:\n{'project': 'skwid_robot', 'spider': spider_ref, 'settings': json.dumps(spider_to_schedule.settings.to_json()), 'custom_settings': json.dumps(spider_to_schedule.other_settings.to_json()), 'schema': json.dumps(spider_to_schedule.schemas[0].schema)}\nHere is my problem: the spider do not take the settings form spider_body but generates default settings. So I've tried a workaround to update custom_settings in spider __init__ this way self.update_settings(json.loads(kwargs.get('custom_settings'))) . I get the following error when launching the spider: ' AttributeError: 'str' object has no attribute 'update' '\nHow can I handle the settings for each spider knowing that my settings dict is build in my app and then have to sent to the spider ?\nThanks", "issue_status": "Closed", "issue_reporting_time": "2018-03-29T10:54:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "408": {"issue_url": "https://github.com/scrapy/scrapy/issues/3190", "issue_id": "#3190", "issue_summary": "Are intermittent uploads to S3 possible during a crawl?", "issue_description": "colinfike commented on Mar 28, 2018 \u2022\nedited\nI was looking around to see if this functionality existed and didn't come across anything.\nThis particular scrape I am doing will take a day or two and I am uploading the files to S3 since I am hosting this on Heroku. I was wondering if there was a way to set the spider to upload to S3 every X pages scraped or something like that. I want to avoid the situation where my dyno crashes and the temp output file is lost before being uploaded to S3.\nDoes something like this already exist? Otherwise I may take a stab at trying to figure out a good way to implement something like this.\nClosing as this does not exist. Probably going to add a PR for this.", "issue_status": "Closed", "issue_reporting_time": "2018-03-28T02:34:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "409": {"issue_url": "https://github.com/scrapy/scrapy/issues/3188", "issue_id": "#3188", "issue_summary": "Scrapy script works only for some parts on a webpage", "issue_description": "madhurtewani commented on Mar 27, 2018 \u2022\nedited\nI'm trying to scrape data from https://www.abcxyz.com/search/category-speaker/.\nI'm able to scrape data from some elements of the page using both selector.xpath and response.css.\nBut not able to scrap data from some other elements of the same page using both selector.xpath and response.css.\nMy script is as follows and I have put comment for the lines that are working and not working.\nimport scrapy\nfrom scrapy import Selector\n\nclass AbcxyzSpider(scrapy.Spider):\n name = 'abcxyzspider'\n ROBOTSTXT_OBEY = False\n rotate_user_agent = True\n rotate_proxies = True\n start_urls = ['https://www.abcxyz.com/search/category-speaker/']\n\n def parse(self, response):\n  print response\n  sel = Selector(response)\n  print sel.xpath('//span[@class=\"order-count\"]/text()').extract_first() #working\n  print sel.xpath('//span[@class=\"products__item-price--final\"]/span/text()').extract_first() #not_working\n\n  print response.css('span.order-count::text').extract_first() #working\n  print response.css('span.products__item-price--final > span::text').extract_first() #not_working\nPlease help.", "issue_status": "Closed", "issue_reporting_time": "2018-03-27T14:16:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "410": {"issue_url": "https://github.com/scrapy/scrapy/issues/3186", "issue_id": "#3186", "issue_summary": "sslv3 alert handshake failure when scraping a website", "issue_description": "bdqnghi commented on Mar 26, 2018 \u2022\nedited\nMy problem at this moment is similar to https://github.com/scrapy/scrapy/issues/2944 and sslv3 alert handshake failure.\nDid quite some search on this problem and none of the solution works for me, my scrapy version is 1.5.0\nThe URL to scrap is: https://dcrsdecorations.com.sg", "issue_status": "Closed", "issue_reporting_time": "2018-03-26T05:09:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "411": {"issue_url": "https://github.com/scrapy/scrapy/issues/3183", "issue_id": "#3183", "issue_summary": "Enclosing URLs passing to shell with single/double quotes", "issue_description": "javadmokhtari commented on Mar 23, 2018\nIn Scrapy Tutorial we read:\nRemember to always enclose urls in quotes when running Scrapy shell from command-line, otherwise urls containing arguments (ie. & character) will not work.\nlater in Using Selectors we see:\nscrapy shell https://doc.scrapy.org/en/latest/_static/selectors-sample1.html\nAs you see in above command, there is no sign of single/double quotes. so I think it might be a good idea to change this to:\nscrapy shell 'https://doc.scrapy.org/en/latest/_static/selectors-sample1.html'\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2018-03-23T10:27:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "412": {"issue_url": "https://github.com/scrapy/scrapy/issues/3180", "issue_id": "#3180", "issue_summary": "Wrong \"start_urls\"", "issue_description": "hsumerf commented on Mar 20, 2018 \u2022\nedited\nI created a spider with command\nscrapy genspider my_crawler1 http://smrafiq.com\nand when i see file of spiders/my_crawler1 there was start_urls and allowed_domains value like this\nallowed_domains = ['http://smrafiq.com']\nstart_urls = ['http://http://smrafiq.com/']\nwhy http:// is again in start_urls? can't we solve this issue, if we can what files should I watch to repair it?", "issue_status": "Closed", "issue_reporting_time": "2018-03-20T10:10:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "413": {"issue_url": "https://github.com/scrapy/scrapy/issues/3179", "issue_id": "#3179", "issue_summary": "GSoC 2018 Proposal: Spider Auto Repair", "issue_description": "Contributor\nvirmht commented on Mar 19, 2018 \u2022\nedited\nHere is the link to my revised GSoC Project proposal.\nThe deadline for proposal submission is only about a week away.\n@cathalgarvey, it will be great if you review this proposal and suggest changes, if any.\nSuggestions are welcome :)\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2018-03-19T13:45:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "414": {"issue_url": "https://github.com/scrapy/scrapy/issues/3178", "issue_id": "#3178", "issue_summary": "log_count/ERROR with Scrapy", "issue_description": "ghost commented on Mar 18, 2018 \u2022\nedited by ghost\nI am getting the following log_count/ERROR while scraping a site with Scrapy:\n`\n2018-03-19 00:31:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 18455,\n 'downloader/request_count': 43,\n 'downloader/request_method_count/GET': 43,\n 'downloader/response_bytes': 349500,\n 'downloader/response_count': 43,\n 'downloader/response_status_count/200': 38,\n 'downloader/response_status_count/301': 5,\n 'dupefilter/filtered': 39,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2018, 3, 18, 15, 31, 30, 227072),\n 'item_scraped_count': 11,\n 'log_count/DEBUG': 56,\n 'log_count/ERROR': 21,\n 'log_count/INFO': 8,\n 'memusage/max': 53444608,\n 'memusage/startup': 53444608,\n 'request_depth_max': 1,\n 'response_received_count': 38,\n 'scheduler/dequeued': 40,\n 'scheduler/dequeued/memory': 40,\n 'scheduler/enqueued': 40,\n 'scheduler/enqueued/memory': 40,\n 'spider_exceptions/AttributeError': 21,\n 'start_time': datetime.datetime(2018, 3, 18, 15, 31, 20, 91856)}\n2018-03-19 00:31:30 [scrapy.core.engine] INFO: Spider closed (finished)\nand this is my code :\n  ```\nfrom scrapy import Spider\nfrom scrapy.http import Request\nimport re\n    class EventSpider(Spider):\n        name = 'event' #name of the spider\n        allowed_domains = ['......com']\n        start_urls = ['http://.....com',\n                      'http://......com',\n                      'http://......com',\n                      'http://......com',]\n    \n        def parse(self, response):\n            events = response.xpath('//h2/a/@href').extract()\n            #events = response.xpath('//a[@class = \"event-overly\"]').extract()\n    \n            for event in events: \n                  absolute_url = response.urljoin(event)\n                  yield Request(absolute_url, callback = self.parse_event)\n    \n        def parse_event(self, response):\n              title = response.xpath('//h1/text()').extract_first()          \n              start_date = response.xpath('//div/p/text()')[0]. extract()\n              start_date_final = re.search(\"^[0-9]{1,2}(th|st|nd|rd)\\s[A-Z][a-z]{2}\\s[0-9]{4}\", start_date)\n              #start_date_final2 = start_date_final.group(0)          \n              end_date = response.xpath('//div/p/text()')[0]. extract()\n              end_date_final = re.search(\"\\s[0-9]{1,2}(th|st|nd|rd)\\s[A-Z][a-z]{2}\\s[0-9]{4}\", end_date)\n              email = response.xpath('//*[@id=\"more-email-with-dots\"]/@value').extract_first()\n              email_final = re.findall(\"[a-zA-Z0-9_.+-]+@(?!......)[\\.[a-zA-Z0-9-.]+\",email)          \n              description = response.xpath('//*[@class = \"events-discription-block\"]//p//text()').extract()\n              start_time = response.xpath('//div/p/text()')[1]. extract() \n              venue = response.xpath('//*[@id =\"more-text-with-dots\"]/@value').extract()          \n              yield{\n                  'title': title,\n                  'start_date': start_date_final.group(0),\n                  'end_date': end_date_final.group(0),\n                  'start_time': start_time,\n                  'venue': venue,\n                  'email': email_final,\n                  'description': description\n              }  \nHow to overcome this error?", "issue_status": "Closed", "issue_reporting_time": "2018-03-18T15:49:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "415": {"issue_url": "https://github.com/scrapy/scrapy/issues/3177", "issue_id": "#3177", "issue_summary": "GSOC 2018 proposal", "issue_description": "Member\nnctl144 commented on Mar 18, 2018\nHey @lopuhin , I have some questions relating to the GSoC proposal.\nRight now I am about to finish the proposal for the project Scrapy performance improvement and the draft is ready. I am just wondering if it is possible for one of the mentors (I noticed that you are one of the mentors for the project) to review it for me? The deadline is in one week so I just want to make sure everything is ready early :)\nPlease let me know when you have a chance :)", "issue_status": "Closed", "issue_reporting_time": "2018-03-18T04:25:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "416": {"issue_url": "https://github.com/scrapy/scrapy/issues/3176", "issue_id": "#3176", "issue_summary": "Getting 1 error in report while using \"tox\" command in shell", "issue_description": "hsumerf commented on Mar 16, 2018 \u2022\nedited\nI used \"tox\" command as per the documentation and in the end i m getting 1 error even i havn't changed anything in souce code which I cloned from remote.\n=================================== FAILURES ===================================\n_________________________ ShellTest.test_dns_failures __________________________\nresult = (0, '{}\\n', 'Coverage.py warning: --include is ignored because --source is set (include-ignored)\\nCoverage.py warning:...wled (200) <GET http://searchguide.level3.com/search/?q=http://www.somedomainthatdoesntexi.st/&t=0> (referer: None)\\n')\ng = <generator object test_dns_failures at 0x7f72acdcde60>\ndeferred = <Deferred at 0x7f72adee1758 current result: None>\ndef _inlineCallbacks(result, g, deferred):\n    \"\"\"\n    See L{inlineCallbacks}.\n    \"\"\"\n    # This function is complicated by the need to prevent unbounded recursion\n    # arising from repeatedly yielding immediately ready deferreds.  This while\n    # loop and the waiting variable solve that by manually unfolding the\n    # recursion.\n\n    waiting = [True, # waiting for result?\n               None] # result\n\n    while 1:\n        try:\n            # Send the last result back as the result of the yield expression.\n            isFailure = isinstance(result, failure.Failure)\n            if isFailure:\n                result = result.throwExceptionIntoGenerator(g)\n            else:\n              result = g.send(result)\n/home/hsumerf/PycharmProjects/scrapy1/scrapy/.tox/py27/local/lib/python2.7/site-packages/twisted/internet/defer.py:1386:\n/home/hsumerf/PycharmProjects/scrapy1/scrapy/tests/test_command_shell.py:114: in test_dns_failures\nself.assertEqual(errcode, 1, out or err)\n/home/hsumerf/PycharmProjects/scrapy1/scrapy/.tox/py27/local/lib/python2.7/site-packages/twisted/trial/_synctest.py:432: in assertEqual\nsuper(_Assertions, self).assertEqual(first, second, msg)\nE FailTest: {}\n======== 1 failed, 1686 passed, 6 skipped, 14 xfailed in 282.12 seconds ========\nERROR: InvocationError: '/home/hsumerf/PycharmProjects/scrapy1/scrapy/.tox/py27/bin/py.test --cov=scrapy --cov-report= scrapy tests'\n___________________________________ summary ____________________________________\nERROR: py27: commands failed\ncan I anybody please help me why I'm getting error?\nThanks in advance.", "issue_status": "Closed", "issue_reporting_time": "2018-03-16T10:58:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "417": {"issue_url": "https://github.com/scrapy/scrapy/issues/3173", "issue_id": "#3173", "issue_summary": "Add an option to add exceptions to be retried", "issue_description": "ariasuni commented on Mar 16, 2018\nCurrently, the list of exception to retry is static. In a middleware I wrote, I created custom exceptions that I want to be caught by the RetryMiddleware without being forced to make my exceptions derive from already caught but not semantically related exceptions.", "issue_status": "Closed", "issue_reporting_time": "2018-03-15T18:30:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "418": {"issue_url": "https://github.com/scrapy/scrapy/issues/3172", "issue_id": "#3172", "issue_summary": "Option to log error instead of info when a non-200 response is ignored", "issue_description": "ariasuni commented on Mar 15, 2018\nCurrently, when scrapy encounter a 404, it ignore the request and log an info. But I want to know right away if my spider has an error \u2014 because most of the time, it\u2019s an error in my code. Instead, it\u2019s drown with other debug messages.", "issue_status": "Closed", "issue_reporting_time": "2018-03-15T18:04:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "419": {"issue_url": "https://github.com/scrapy/scrapy/issues/3171", "issue_id": "#3171", "issue_summary": "Log with error level instead of debug when reaching max retry times", "issue_description": "ariasuni commented on Mar 15, 2018\nThere\u2019s no easy/not hackish way to log an error when reaching max retry times with standard scrapy RetryMiddleware. It\u2019s useful for me to be able to see right away if a page I tried to crawl has not been downloaded.\nI think it\u2019s sensible to change this line to log to error level instead:\nscrapy/scrapy/downloadermiddlewares/retry.py\nLine 89 in 6cc6bbb\n logger.debug(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\", \n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-03-15T17:49:19Z", "fixed_by": "#3566", "pull_request_summary": "[MRG+1] change _retry() in RetryMiddleware log from debug to error", "pull_request_description": "Contributor\nBurnzZ commented on Jan 4, 2019 \u2022\nedited by Gallaecio\nProposal/Suggestion for #3171\nFixes #3171", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-28T20:53:26Z", "files_changed": [["2", "scrapy/downloadermiddlewares/retry.py"]]}, "420": {"issue_url": "https://github.com/scrapy/scrapy/issues/3170", "issue_id": "#3170", "issue_summary": "On JavaScript paging", "issue_description": "runpython commented on Mar 15, 2018 \u2022\nedited\nI have a problem that I have never been able to solve, and I am very annoyed\nThere are no relevant contents in the https://docs.scrapy.org/en/latest/\nThat's the way it is.\nI'm going to scrapy this site http://q.10jqka.com.cn/\nFor example, I want to take the stock code under the stock market\nThe XPath parameter is similar to this\ntoxpath=response.xpath('//div[@id=\"maincont\"]/table[@Class=\"m-table m-pager-table\"]/tbody/tr').extract()\nfor listxpath in toxpath:\nshno=response.listxpath('./td[1]/text()').extract()\nshcode=response.listxpath('./td[2]/a/text()').extract()\nshname=response.listxpath('./td[3]/a/text()').extract()\nshprice=response.listxpath('./td[4]/text()').extract()\nBut I was in trouble when I was paging\nThe site's paging is a dynamic JavaScript\na code like this\n<a class=\"changePage\" page=\"2\" href=\"javascript:void(0);\">\u4e0b\u4e00\u9875</a>\nSo I can't scrapy the relevant content. How do I do it?\nI hope to be able to help you, thank you.", "issue_status": "Closed", "issue_reporting_time": "2018-03-15T08:48:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "421": {"issue_url": "https://github.com/scrapy/scrapy/issues/3169", "issue_id": "#3169", "issue_summary": "\"New HTTP/2 download handler\" project help for GSoC", "issue_description": "hsumerf commented on Mar 15, 2018\nHi Scrapy commpunity!\nI saw \"New HTTP/2 download handler\" idea for Gsoc 2018 list which i found much interesting to work because it is in my interest field of networking,\nbefore My questions i would like to introduce myself, I'm a student of University of karachi and completing my BS in computer Sciences and in 3rd year\nof university.I have knowledge of Python,C,jave,C#,HTML,CSS,javascript but my favourite languages are python and C,in which i have developed some\nprojects data datarecovery(in C, formats recovered MP3,JPG,PNG,PDF) and in python i have worked on sentiment analyzer(by using nltk with twitter API),\nwebapplication shares trading website (on flask as a assignment), photos and data extractors(developed in scrapy these spiders).\nAs you describe we have to work on http/2 for this project I read many articles and reading 1 book of Daniel Stenberg this to understand the concepts of http/2 and what are pros and cons of this new version of http.Now What I have learned so far is that:\nIn HTTP /1.1 we use textual frames but in http/2 we use binary because it is much more efficient to parse,more compact on wire,less error-prone.\n2)HTTP/1.1 has a problem of head-of-line blocking, tried to resolve this issue by pipelining but couldn't solve practically this problem,so now http/2 has solved this issue by multiplexing.\n3)HTTP/2 supports server push i.e. which sent all necessary files of a webrequest simultaneously.\n4)HTTP/2 use HPACK compression which use huffman encoding, by this we get less data in stream over network and save our precious bytes.\n5)HTTP/2 use application-layer protocol negotiation(ALPN) i.e. it sends the potocols list to server in hello message to choose the desire protocols, which reduce the additional round trips.\nAll these things can increase the speed of downloading web pages and all media files.\n1)But now I want to ask you, are my concepts right regarding this http/2? give me feedback regarding this please.\n2)As far as i understand that, it is also necessary for server to work upon HTTP/2 if we want to use HTTP/2 advantages, so how we will counter if does not support HTTP/2 and check which HTTP is this?\n3)HTTP/2 does not support telnet, so how we will encounter this problem too?\n4)please suggest me bug through which i can learn code level concepts by patching that bug and can show that patch in my proposal of GSoC.\n5)Guide me please more, how to start working for this proposal buy suggesting me files of scrapy project and other concepts which i should learn to start work upon this project.\nThanks in advance! :)", "issue_status": "Closed", "issue_reporting_time": "2018-03-15T01:43:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "422": {"issue_url": "https://github.com/scrapy/scrapy/issues/3168", "issue_id": "#3168", "issue_summary": "scrapy shell exception Occured", "issue_description": "newthis commented on Mar 13, 2018 \u2022\nedited\nWhen I execute \"scrapy shell scrapy shell https://crates.io/api/v1/crates?page=1&per_page=100\"\uff0cexception occured as below:\n2018-03-13 21:00:15 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n2018-03-13 21:00:15 [scrapy.core.engine] INFO: Spider opened\n2018-03-13 21:00:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://crates.io/api/v1/crates?page=1> (failed 1 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2018-03-13 21:00:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://crates.io/api/v1/crates?page=1> (failed 2 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2018-03-13 21:00:17 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://crates.io/api/v1/crates?page=1> (failed 3 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 150, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 90, in _run_print_help\nfunc(*a, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 157, in _run_command\ncmd.run(args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/shell.py\", line 73, in run\nshell.start(url=url, redirect=not opts.no_redirect)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 48, in start\nself.fetch(url, spider, redirect=redirect)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 115, in fetch\nreactor, self._schedule, request, spider)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\nresult.raiseException()\nFile \"\", line 2, in raiseException\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\nAnd the pip freeze result is:\nadium-theme-ubuntu==0.3.4\napt-xapian-index==0.45\nasn1crypto==0.24.0\nattrs==17.4.0\nAutomat==0.6.0\ncffi==1.11.5\nchardet==2.0.1\ncolorama==0.2.5\ncommand-not-found==0.3\nconstantly==15.1.0\ncryptography==2.1.4\ncssselect==1.0.3\ndebtagshw==0.1\ndefer==1.0.6\ndirspec==13.10\nduplicity==0.6.23\nenum34==1.1.6\nhtml5lib==0.999\nhttplib2==0.8\nhyperlink==18.0.0\nidna==2.6\nincremental==17.5.0\nipaddress==1.0.19\nlockfile==0.8\nlxml==3.3.3\noauthlib==0.6.1\noneconf==0.3.7.14.4.1\nPAM==0.4.2\nparsel==1.4.0\npexpect==3.1\nPillow==2.3.0\npiston-mini-client==0.7.5\npyasn1==0.4.2\npyasn1-modules==0.2.1\npycparser==2.18\npycrypto==2.6.1\npycups==1.9.66\nPyDispatcher==2.0.5\npygobject==3.12.0\npymongo==3.6.0\npyOpenSSL==17.4.0\npyserial==2.6\npysmbc==1.0.14.1\npython-apt===0.9.3.5ubuntu2\npython-debian===0.1.21-nmu2ubuntu2\npyxdg==0.25\nqueuelib==1.5.0\nreportlab==3.0\nrequests==2.2.1\nScrapy==1.5.0\nservice-identity==17.0.0\nsessioninstaller==0.0.0\nsix==1.5.2\nsoftware-center-aptd-plugins==0.0.0\nsystem-service==0.1.6\nTwisted==13.1.0\nTwisted-Core==13.2.0\nTwisted-Web==13.2.0\nunity-lens-photos==1.0\nurllib3==1.7.1\nw3lib==1.19.0\nxdiagnose===3.6.3build2\nzope.interface==4.0.5\nopenssl version on ubuntu is OpenSSL 1.0.2g 1 Mar 2016\uff0c\non windows ,the same openssl exception occured, \"scrapy shell scrapy shell https://crates.io/api/v1/crates?page=1&per_page=100\"", "issue_status": "Closed", "issue_reporting_time": "2018-03-13T13:05:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "423": {"issue_url": "https://github.com/scrapy/scrapy/issues/3167", "issue_id": "#3167", "issue_summary": "twisted.python.failure", "issue_description": "Svickie7 commented on Mar 12, 2018\nScrapy : 1.5.0\nlxml : 4.1.1.0\nlibxml2 : 2.9.5\ncssselect : 1.0.3\nparsel : 1.4.0\nw3lib : 1.19.0\nTwisted : 17.9.0\nPython : 2.7.6 (default, Nov 10 2013, 19:24:18) [MSC v.1500 32 bit (Intel)]\npyOpenSSL : 17.6.0.dev0 (OpenSSL 1.1.0g 2 Nov 2017)\ncryptography : 2.1.4\nPlatform : Windows-7-6.1.7601-SP1\n2018-03-12 21:20:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.farmerscompress.com/> (referer: None)\n2018-03-12 21:20:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST https://www.farmerscompress.com/ProcessUser.aspx> (failed 1 times): []\n2018-03-12 21:20:50 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST https://www.farmerscompress.com/ProcessUser.aspx> (failed 2 times): []\n2018-03-12 21:20:50 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <POST https://www.farmerscompress.com/ProcessUser.aspx> (failed 3 tim\nes): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection\nlost.>]\n2018-03-12 21:20:50 [scrapy.core.scraper] ERROR: Error downloading <POST https://www.farmerscompress.com/ProcessUser.aspx>: [<twisted.python.failure.F\nailure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-03-12 21:20:51 [scrapy.core.engine] INFO: Closing spider (finished)\n2018-03-12 21:20:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:", "issue_status": "Closed", "issue_reporting_time": "2018-03-12T15:56:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "424": {"issue_url": "https://github.com/scrapy/scrapy/issues/3156", "issue_id": "#3156", "issue_summary": "GSOC 2018:Want to contribute to Scrapy in GSOC 2018", "issue_description": "ayushmankoul commented on Mar 7, 2018\nHello Mentors and Developers,\nI am Ayushman Koul, a student at GCET Jammu.I went through the Scrapy GSoC 2018 ideas page and found all the projects to be very interesting and want to contribute to Scrapy performance improvement and Scrapy spider auto repair projects.I've used scrapy for my personal projects, which is why I'd love to have an opportunity to contribute meaningfully this summer!\nYou can know more about me on:\nLinkedin: https://www.linkedin.com/in/ayushman-koul-a70350128/\nGitHub: https://github.com/ayushmankoul\nI have started on picking up an issue and familiarizing myself with the codebase.Please help me how can I fix this bug (#3077) and also have given my feedback on comments.@cathalgarvey, I would really appreciate any guidance on fixing this bug (#3077) and I would like to try my hand more by implementing a small new feature and/or resolving bugs.\nIf there is anything else that might be useful to know while I'm researching this project please let me know!\nThanking you.\nRegards,\nAyushman Koul", "issue_status": "Closed", "issue_reporting_time": "2018-03-07T15:21:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "425": {"issue_url": "https://github.com/scrapy/scrapy/issues/3154", "issue_id": "#3154", "issue_summary": "[GSoC 2018] Guidance request for Idea \"Scrapy Spider autoRepair\"", "issue_description": "thisisayush commented on Mar 4, 2018 \u2022\nedited\nHi,\nI am Ayush, B.Tech (CSE), 2nd Year, from Amity University, Noida. I came across the idea of automatically generating xpaths from scapshots and became interested at the same moment in the project and would love to work for the same.\nI have worked with scrapy in the previous in a team, scraping news websites and storing information [1]. I have been developing in python since last one year and have done various projects in Django, Flask, StaticJinja, etc mostly on web development.\nI am highly interested on working in this project and would like to discuss over the idea.\n@cathalgarvey , I would really appreciate any guidance on pre-requisites for GSoC 2018 and this project as a part of GSoC.\n[1] https://github.com/thisisayush/scrape\nRegards,\nAyush Agarwal\ncontact@thisisayush.com | thisisayush.com", "issue_status": "Closed", "issue_reporting_time": "2018-03-04T16:33:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "426": {"issue_url": "https://github.com/scrapy/scrapy/issues/3146", "issue_id": "#3146", "issue_summary": "root logger's level is always set to NOTSET", "issue_description": "joelkim commented on Feb 27, 2018\nCurrently, the log level of a root logger is always automatically re-set to NOTSET level in install_scrapy_root_handler function call if I add handlers to root logger:\nscrapy/scrapy/utils/log.py\nLine 110 in 108f8c4\n logging.root.setLevel(logging.NOTSET) \nCan anybody explain why this level reset is required?\nThis issue is relevant to issue #2352, #2149, I think.", "issue_status": "Closed", "issue_reporting_time": "2018-02-27T08:09:00Z", "fixed_by": "#3960", "pull_request_summary": "Update documentation for logging manually", "pull_request_description": "Contributor\nthernstig commented on Aug 15, 2019 \u2022\nedited\nUsage of basicConfig() together with crawlerRunner is not recommended.\nUpdate documentation to highlight this fact.\nCloses #2149, closes #2352, and closes #3146", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-12T11:17:50Z", "files_changed": [["14", "docs/topics/logging.rst"]]}, "427": {"issue_url": "https://github.com/scrapy/scrapy/issues/3144", "issue_id": "#3144", "issue_summary": "Issue in FromRequest", "issue_description": "Svickie7 commented on Feb 27, 2018 \u2022\nedited\nIt return the same login page\nit is not able to redirect the next page..\nI have all tried to call http://schedule.amacotton.com/transportation.php in scrap_page method..it return 302\nimport scrapy\nclass ExampleSpider(scrapy.Spider):\nname = 'example'\nstart_urls = ['http://schedule.amacotton.com/']\nLOGIN_URL = 'http://schedule.amacotton.com/login.php'\nURL = 'http://schedule.amacotton.com/transportation.php'\ndef parse(self, response):\n    my_data = {'user': 'abc', 'pass': 'abc', 'plant': '1'}\n    yield scrapy.FormRequest(\n                url = self.LOGIN_URL,\n                headers = {\n                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n                    'Accept-Encoding': 'gzip, deflate',\n                    'Accept-Language': 'en-US,en;q=0.5',\n                    'Connection': 'keep-alive',\n                    'Content-Type': 'application/x-www-form-urlencoded',\n                    'Host': 'schedule.amacotton.com',\n                    'Referer': 'http://schedule.amacotton.com/login.php',\n                    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:53.0) Gecko/20100101 Firefox/53.0',\n                },\n                method = 'POST',\n                formdata = my_data,\n                callback = self.scrap_page,\n                dont_filter = True\n        )\n\ndef scrap_page(self, response):\n    print response.body\n2018-02-27 11:23:49 [scrapy.core.engine] INFO: Spider opened\n2018-02-27 11:23:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2018-02-27 11:23:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n2018-02-27 11:23:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://schedule.amacotton.com/> (referer: None)\n2018-02-27 11:23:50 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://schedule.amacotton.com/login.php> (referer: http://schedule.amacotton.com/l\nogin.php)\n<title> Login </title>\nUserName:\nPassword:\n\nAmarillo Cotton Warehouse\nTulia", "issue_status": "Closed", "issue_reporting_time": "2018-02-27T06:20:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "428": {"issue_url": "https://github.com/scrapy/scrapy/issues/3143", "issue_id": "#3143", "issue_summary": "Python 3.7 support", "issue_description": "Member\nlopuhin commented on Feb 26, 2018\nThe goal is to add python 3.7 to travis and pass all tests, the first beta was already released at the end of January.", "issue_status": "Closed", "issue_reporting_time": "2018-02-26T16:25:06Z", "fixed_by": "#3326", "pull_request_summary": "Python 3.7 support", "pull_request_description": "Member\nlopuhin commented on Jul 9, 2018 \u2022\nedited\nContinue @patiences work on python 3.7 compatibility from #3150, fixes #3143. Thanks a lot @patiences!\nIt's likely that we'll release scrapy 1.6 before new twisted version with twisted/twisted#966 is released (sorry for making the PR stall), so in this PR I work around this issue, so that scrapy at least works, although without telnet console support yet.\nAlso pytest is updated, so that tox -e py37 can collect the tests (2.9 was released more than 2 years ago).\nWIP as I want to check how tests work on travis early (there are still a few failures due to PEP 479 left which didn't show up in #3150).", "pull_request_status": "Merged", "issue_fixed_time": "2018-07-11T12:36:56Z", "files_changed": [["4", ".travis.yml"], ["2", "requirements-py3.txt"], ["8", "scrapy/extensions/telnet.py"], ["17", "scrapy/utils/iterators.py"], ["4", "tests/requirements-py3.txt"], ["6", "tests/test_crawler.py"], ["7", "tests/test_utils_log.py"], ["4", "tox.ini"]]}, "429": {"issue_url": "https://github.com/scrapy/scrapy/issues/3141", "issue_id": "#3141", "issue_summary": "permission denied when I install scrapy via conda", "issue_description": "mayouf commented on Feb 26, 2018\nHi everyone,\nIn order to start a scrapy project, I just set an ubuntu 16.04 VM. I installed ST3, anaconda 3, smartgit and chrome. That's all.\nSince that, I tried to install scrapy by typing a simple:\nconda install scrapy\nand it gives me an error that I could not troubleshoot:\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: failed\nERROR conda.core.link:_execute(481): An error occurred while installing package 'defaults::constantly-15.1.0-py36_0'.\nPermissionError(13, 'Permission denied')\nAttempting to roll back.\nRolling back transaction: done\nPermissionError(13, 'Permission denied')\nI did not find that much on this error. But the strangest part, is that I've done this installation a thousand time and I never experience this error.\nAnyone here had this issue before ?\nRegards,\nMic", "issue_status": "Closed", "issue_reporting_time": "2018-02-26T15:20:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "430": {"issue_url": "https://github.com/scrapy/scrapy/issues/3140", "issue_id": "#3140", "issue_summary": "Round Robin Domain Crawling second scheduler to improve performance", "issue_description": "tianhuil commented on Feb 26, 2018\nPropose merging a DomainScheduler implemented in https://github.com/tianhuil/domain_scheduler into scrapy. It scrapes in a domain-smart way: by round-robin cycling through the domains. This has two benefits:\nSpreading out load on the target servers instead of hitting the server with many requests at once\nReducing delays caused by server-throttling or scrapy's own CONCURRENT_REQUESTS_PER_IP restrictions. Empirical testing has shown this to be quite effective.\nIt implements the solution proposed in #1802 (comment) which found similar performance improvements. Original proposal was first posted #1802 and #2474.\nNote: It requires more than just using SCHEDULER_PRIORITY_QUEUE as it needs an API change to the queue (passing in a non-integer key, i.e. the domain, to a new round robin queue). Therefore, it is dependent on first merging scrapy/queuelib#21.\nI'm happy to setup a PR to refactor Scheduler to allow for both DomainScheduler and Scheduler to exist in scrapy as there is significant code overlap if there's interest.", "issue_status": "Closed", "issue_reporting_time": "2018-02-26T13:14:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "431": {"issue_url": "https://github.com/scrapy/scrapy/issues/3139", "issue_id": "#3139", "issue_summary": "FormRequest.from_response() clickdata ignores input[type=image]", "issue_description": "Nadya24 commented on Feb 26, 2018\nNo way to click image inputs now, see here.", "issue_status": "Closed", "issue_reporting_time": "2018-02-26T06:05:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "432": {"issue_url": "https://github.com/scrapy/scrapy/issues/3138", "issue_id": "#3138", "issue_summary": "GSoC 2018: Scrapy spider autorepair", "issue_description": "Contributor\nvirmht commented on Feb 25, 2018 \u2022\nedited\nHi! I have already introduced myself here.\nI am interested in working on the project: \"Scrapy spider autorepair\". As suggested by you, @cathalgarvey , I have read about Scrapely, its API, how it works, and have a decent understanding of its source code.\nI have also read 2 research papers cited on the Github Scrapely page.\nAs per my understanding, scrapely has the following drawbacks when comapared to what is required as stated on the GSoC 2018 ideas page:\nScrapely currently does not output rules in the form of XPaths or CSS Selectors.\nScrapely currently requires the structure of the test page to remain same or at least similar to the page on which it was trained. In our case, (as described on the ideas page), the content remains same but the structure can change. Currently scrapely looks at the order of elements in the prefix and suffix of the element to be extracted. Hence, it might produce wrong results if the order changes quite drastically but the content remains same.\nOne way to deal with issue 2. is to do the following: instead of looking at the order of elements in the prefix and suffix of the element to be extracted(which scrapely does), we need to look at the information or the content of the page around the region of interest and figure out its path in the new page. For example:\n**Old version of page:**\n<parent>\n               <child>ABC</child>\n               <child>XYZ</child>\n</parent>\n\n**New version of page:**\n<parent>\n               <child>XYZ</child>\n               <child>ABC</child>\n</parent>\nfor each element in old version:\n        value = element.value // example: value of first child element = 'ABC'\n        position = find position of value in new version // position of 'ABC' in new version = /parent/child[2]\n        return position\nI want to start writing my GSoC 2018 proposal for this project. @cathalgarvey can you provide some more info about the tasks that should be completed ?\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2018-02-25T12:31:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "433": {"issue_url": "https://github.com/scrapy/scrapy/issues/3136", "issue_id": "#3136", "issue_summary": "Discussion regarding gsoc", "issue_description": "Contributor\nyashrsharma44 commented on Feb 23, 2018 \u2022\nedited\nHello, I opened this issue, regarding some discussions of unittests. I am planning to participate in Gsoc , and while I am going through the code base, I am getting hang of most of the code, except unittests. This is not something that there are issues with the codebase ; my main issue is that testing for asynchronous programming is something new for me, so I wanted some advices : 1. How to go about learning unittests ? 2. How do I apply my learning in writing code so that it is utilised with unittests ? 3. While applying Test Driven Development( which is quite new for me), how do I go about bringing new features, if I need to rewrite the previous tests and test-suites from scratch ?\nEDIT : I am planning for the Asyncio Project of GSoC, so if specific comments for this project are there, you are most welcome.", "issue_status": "Closed", "issue_reporting_time": "2018-02-22T18:45:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "434": {"issue_url": "https://github.com/scrapy/scrapy/issues/3130", "issue_id": "#3130", "issue_summary": "Gsoc-discussion", "issue_description": "videetssinghai commented on Feb 21, 2018\nHello,\nI am interested in working on the project \"Scrapy performance improvement\". Can anyone give some brief idea about which area to target exactly?", "issue_status": "Closed", "issue_reporting_time": "2018-02-20T18:40:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "435": {"issue_url": "https://github.com/scrapy/scrapy/issues/3128", "issue_id": "#3128", "issue_summary": "[suggest ] `response.follow` should raise a exception when called on None or an empty string, instead of crawling the current page again", "issue_description": "Contributor\nNewUserHa commented on Feb 17, 2018\nresponse.follow will raise a exception when url='' or none in stead of crawl the (base) page itself again.\nnone will use follow to crawl the source(base) page again right? all parsers will be passed without warning if that way.\nthanks", "issue_status": "Closed", "issue_reporting_time": "2018-02-16T23:38:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "436": {"issue_url": "https://github.com/scrapy/scrapy/issues/3121", "issue_id": "#3121", "issue_summary": "Can't set settings with ENV SCRAPY_SETTINGS_MODULE", "issue_description": "marcuslind90 commented on Feb 13, 2018\nI'm running Scrapy in Docker with an .env file where I define the SCRAPY_SETTINGS_MODULE to decide which settings file I want to load. This throws ModuleNotFoundError, even if you point to the existing settings.py file and use the default value which will be myproject.settings.\nI have replicated it with a brand new 1.5.0 project.", "issue_status": "Closed", "issue_reporting_time": "2018-02-13T10:27:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "437": {"issue_url": "https://github.com/scrapy/scrapy/issues/3120", "issue_id": "#3120", "issue_summary": "Write items to MongoDB", "issue_description": "Fingalzzz commented on Feb 12, 2018\nHi I'm new to scrapy and I'm reading official document, Write items to MongoDB has an example of how to write items into MongoDB. But why we need to get items from settings, it seems useless to me. Anyone could tell me why and show me a simple example of what items could be. Thanks.\nimport pymongo\n\nclass MongoPipeline(object):\n\n    collection_name = 'scrapy_items'\n\n    def __init__(self, mongo_uri, mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'),\n            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def close_spider(self, spider):\n        self.client.close()\n\n    def process_item(self, item, spider):\n        self.db[self.collection_name].insert_one(dict(item))\n        return item", "issue_status": "Closed", "issue_reporting_time": "2018-02-12T14:46:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "438": {"issue_url": "https://github.com/scrapy/scrapy/issues/3118", "issue_id": "#3118", "issue_summary": "ERROR IN EXECUTION PYTHON FILE", "issue_description": "ProfessorV commented on Feb 12, 2018\nFile \"C:\\Users****\\Desktop\\root.py\", line 154, in host = m.group(1)\nAttributeError: 'NoneType' object has no attribute 'group'", "issue_status": "Closed", "issue_reporting_time": "2018-02-11T23:43:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "439": {"issue_url": "https://github.com/scrapy/scrapy/issues/3117", "issue_id": "#3117", "issue_summary": "DLL load failed: %1 is not a valid Win32 application", "issue_description": "Zoher15 commented on Feb 11, 2018\nHello,\nI have been using scrapy for a while and this did not happen a month ago. I have tried everything possible but this error is not going away. Help would be appreciated.\nbest,\nZoher", "issue_status": "Closed", "issue_reporting_time": "2018-02-11T16:18:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "440": {"issue_url": "https://github.com/scrapy/scrapy/issues/3116", "issue_id": "#3116", "issue_summary": "Contributing to Scrapy in GSoC 2018", "issue_description": "Contributor\nvirmht commented on Feb 9, 2018\nHello Mentors and Developers,\nI am Viral Mehta, a student at BITS Pilani, Hyderabad Campus. I went through the Scrapy GSoC 2018 ideas page and found all the projects to be very interesting.\nI found the projects Scrapy performance improvement and Scrapy spider autorepair, particularly interesting.\nI would love to contribute to Scrapy and participate in GSoC 2018.\nYou can know more about me on:\nLinkedin: https://in.linkedin.com/in/viral-mehta-b76093102\nGitHub: https://github.com/virmht\nI do have some open source experience and I would like to try my hand by implementing a small new feature and/or resolving bugs.\nI will be extremely thankful if someone could guide me regarding the same.\nThanking you.\nRegards,\nViral Mehta.", "issue_status": "Closed", "issue_reporting_time": "2018-02-09T08:34:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "441": {"issue_url": "https://github.com/scrapy/scrapy/issues/3114", "issue_id": "#3114", "issue_summary": "Can't paginate a ASP.NET WebForms website", "issue_description": "Urahara commented on Feb 7, 2018 \u2022\nedited\nSo i have a site that is made using ASP.NET (WebForms).\nI was able to log in and even load a page by filtering a comboBox but when i try to paginate i receive:\n179|error|500|The page is performing an async postback but the ScriptManager.SupportsPartialRendering property is set to false. Ensure that the property is set to true during an async postback.|\nIs there any workaround to crawl this type of site? I see many params on requests i even try send them all but without success. Is scrapy capable of scrapy this? Or is better i will try with Selenium?", "issue_status": "Closed", "issue_reporting_time": "2018-02-07T16:06:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "442": {"issue_url": "https://github.com/scrapy/scrapy/issues/3112", "issue_id": "#3112", "issue_summary": "better way to ignore next parse and use next next parse if its page doesn't exist.", "issue_description": "Contributor\nNewUserHa commented on Feb 7, 2018\ndef parse..\n  next_page = href\n  item={..}\n  response.follow(href, self.b, mata={'item': item})\ndef b..\n  ..\n  item= response..\n  response.follow(...meta={'item': item}\ndef c...\ntheir results have to be chained into one item from the first method, but how to jump to next method if a next_page of one method doesn't exist. Is there a best way to achieve except making a dict consist of next_page:parse and poping called method out one by one?\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2018-02-07T13:51:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "443": {"issue_url": "https://github.com/scrapy/scrapy/issues/3110", "issue_id": "#3110", "issue_summary": "Is there better way to call Scrapy programatically?", "issue_description": "Urahara commented on Feb 6, 2018 \u2022\nedited\nI have a server that have endpoints that call Scrapy using this below syntax, that i copied from reddit and use since then.\nprocess = Popen(\n    ['scrapy', 'crawl', 'product_list', '-s', 'LOG_ENABLED=False', '-t', 'json', '-o' '-',\n        '-a', 'username={}'.format(username),\n        '-a', 'password={}'.format(password),\n        stdout=PIPE, stderr=PIPE)\n\nstdout, _ = process.communicate()\n\nstdout.decode(\"utf-8\")\n\n# parse string to json, code ommited.\nMy doubt is if have a better way to call my crawl and get the data returned. The above example that i use works but don't seens a good code.\nUPDATE: I trying using CrawlerProcess but i don't know how to get items:\nimport scrapy\nfrom scrapy.crawler import  CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\nprocess = CrawlerProcess(get_project_settings())\nprocess.crawl('product_list', **{ 'username' : 'test', 'password' : '123456'})\nprocess.start()", "issue_status": "Closed", "issue_reporting_time": "2018-02-06T17:57:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "444": {"issue_url": "https://github.com/scrapy/scrapy/issues/3109", "issue_id": "#3109", "issue_summary": "Extract_WebFormFields", "issue_description": "anonymousReaper commented on Feb 5, 2018\nHi,\nCan we install this crawler using Kali Linux? Also, can we extract the web form field label names from a URL and the sub-links through the main domain, If not, can you suggest one?\nThank you!", "issue_status": "Closed", "issue_reporting_time": "2018-02-05T16:13:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "445": {"issue_url": "https://github.com/scrapy/scrapy/issues/3108", "issue_id": "#3108", "issue_summary": "Scrapy crawl stalls and doesn't raise TimeoutError, prints logstats every minute", "issue_description": "sheri528 commented on Feb 5, 2018\nI got log like these every minute, and the process never quite itself unless I do \"kill -9\".\n2018-02-05 16:38:21 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 0 pages/min), scraped 115 items (at 0 items/min)\n2018-02-05 16:39:21 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 0 pages/min), scraped 115 items (at 0 items/min)\n2018-02-05 16:40:21 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 0 pages/min), scraped 115 items (at 0 items/min)\n2018-02-05 16:41:21 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 0 pages/min), scraped 115 items (at 0 items/min)\n2018-02-05 16:42:21 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 0 pages/min), scraped 115 items (at 0 items/min)\nThen I use gdb to track the process, I got these(part of them)\n#0  0x00007f0eb65d6903 in __epoll_wait_nocancel () from /lib64/libc.so.6\n#1  0x00007f0ea9b04123 in pyepoll_poll (self=0x21c2180, args=<optimized out>, kwds=<optimized out>) at /usr/src/debug/Python-2.7.5/Modules/selectmodule.c:1005\n#2  0x00007f0eb72aabb0 in call_function (oparg=<optimized out>, pp_stack=0x7ffebc5e2c40) at /usr/src/debug/Python-2.7.5/Python/ceval.c:4408\n#3  PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x3356680, for file /usr/lib64/python2.7/site-packages/twisted/internet/epollreactor.py, line 379, in doPoll (self=<EPollReactor(waker=<_UnixWaker(i=25, fi>, reactor=<...>, o=26) at remote 0x2714cd0>, _poller=<select.epoll at remote 0x21c2180>, threadpoolShutdownID=('shutdown', ('during', <instancemethod at remote 0x32674b0>, ()medCalls=[], _pendingTimedCalls=[<DelayedCall(resetter=<instancemethod at remote 0x32fcfa0>, seconds=<built-in function time>, args=(...), canceller=<instancemethod at remote  func=<LoopingCall(a=(...), interval=5, clock=<...>, f=<instancemethod at remote 0x32670a0>, running=True, kw={}, starttime=<float at remote 0x2ab0708>, call=<...>, _runAtStarller=None, callbacks=[]) at remote 0x32e4200>) at remote 0x32dde50>, time=<float at remote 0x39af480>, cancelled=0, called=0) at remote 0x368cdd0>, <DelayedCall(resetter=<instncated), throwflag=throwflag@entry=0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3040\n#4  0x00007f0eb72acefd in PyEval_EvalCodeEx (co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=<optimized out>, argcount=2, kws=0x333f5b0, kwcount=0, \n    closure=closure@entry=0x0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3640\n#5  0x00007f0eb72aa3fc in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffebc5e2e50, func=<optimized out>) at /usr/src/debug/Python-2.7\n#6  call_function (oparg=<optimized out>, pp_stack=0x7ffebc5e2e50) at /usr/src/debug/Python-2.7.5/Python/ceval.c:4429\n#7  PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x333f410, for file /usr/lib64/python2.7/site-packages/twisted/internet/base.py, line 1207, in mainLoop (self=<EPollReactor(waker=<_UnixWaker(i=25, fileno=actor=<...>, o=26) at remote 0x2714cd0>, _poller=<select.epoll at remote 0x21c2180>, threadpoolShutdownID=('shutdown', ('during', <instancemethod at remote 0x32674b0>, (), {})lls=[], _pendingTimedCalls=[<DelayedCall(resetter=<instancemethod at remote 0x32fcfa0>, seconds=<built-in function time>, args=(...), canceller=<instancemethod at remote 0x361=<LoopingCall(a=(...), interval=5, clock=<...>, f=<instancemethod at remote 0x32670a0>, running=True, kw={}, starttime=<float at remote 0x2ab0708>, call=<...>, _runAtStart=TruNone, callbacks=[]) at remote 0x32e4200>) at remote 0x32dde50>, time=<float at remote 0x39af480>, cancelled=0, called=0) at remote 0x368cdd0>, <DelayedCall(resetter=<instancemncated), throwflag=throwflag@entry=0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3040\n#8  0x00007f0eb72acefd in PyEval_EvalCodeEx (co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=<optimized out>, argcount=1, kws=0x333ce60, kwcount=0, \n    closure=closure@entry=0x0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3640\n#9  0x00007f0eb72aa3fc in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffebc5e3060, func=<optimized out>) at /usr/src/debug/Python-2.7\n#10 call_function (oparg=<optimized out>, pp_stack=0x7ffebc5e3060) at /usr/src/debug/Python-2.7.5/Python/ceval.c:4429\n#11 PyEval_EvalFrameEx (\n    f=f@entry=Frame 0x333ccd0, for file /usr/lib64/python2.7/site-packages/twisted/internet/base.py, line 1195, in run (self=<EPollReactor(waker=<_UnixWaker(i=25, fileno=<func=<...>, o=26) at remote 0x2714cd0>, _poller=<select.epoll at remote 0x21c2180>, threadpoolShutdownID=('shutdown', ('during', <instancemethod at remote 0x32674b0>, (), {})), _j], _pendingTimedCalls=[<DelayedCall(resetter=<instancemethod at remote 0x32fcfa0>, seconds=<built-in function time>, args=(...), canceller=<instancemethod at remote 0x36134b0>pingCall(a=(...), interval=5, clock=<...>, f=<instancemethod at remote 0x32670a0>, running=True, kw={}, starttime=<float at remote 0x2ab0708>, call=<...>, _runAtStart=True, _d callbacks=[]) at remote 0x32e4200>) at remote 0x32dde50>, time=<float at remote 0x39af480>, cancelled=0, called=0) at remote 0x368cdd0>, <DelayedCall(resetter=<instancemethodncated), throwflag=throwflag@entry=0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3040\n#12 0x00007f0eb72acefd in PyEval_EvalCodeEx (co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=<optimized out>, argcount=1, kws=0x2f2d9b0, kwcount=1, \n    closure=closure@entry=0x0) at /usr/src/debug/Python-2.7.5/Python/ceval.c:3640\n#13 0x00007f0eb72aa3fc in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffebc5e3270, func=<optimized out>) at /usr/src/debug/Python-2.7\n#14 call_function (oparg=<optimized out>, pp_stack=0x7ffebc5e3270) at /usr/src/debug/Python-2.7.5/Python/ceval.c:4429\nTrack the last call by twisted in doPoll, I got timeout=5.\n(gdb) py-locals\nself = <EPollReactor(waker=<_UnixWaker(i=25, fileno=<function at remote 0x1ab8d70>, reactor=<...>, o=26) at remote 0x2714cd0>, _poller=<select.epoll at remote 0x21c2180>, threadpoolShutdownID=('shutdown', ('during', <instancemethod at remote 0x32674b0>, (), {})), _justStopped=False, _newTimedCalls=[], _pendingTimedCalls=[<DelayedCall(resetter=<instancemethod at remote 0x32fcfa0>, seconds=<built-in function time>, args=(...), canceller=<instancemethod at remote 0x36134b0>, delayed_time=0, kw={}, func=<LoopingCall(a=(...), interval=5, clock=<...>, f=<instancemethod at remote 0x32670a0>, running=True, kw={}, starttime=<float at remote 0x2ab0708>, call=<...>, _runAtStart=True, _deferred=<Deferred(_canceller=None, callbacks=[]) at remote 0x32e4200>) at remote 0x32dde50>, time=<float at remote 0x39af480>, cancelled=0, called=0) at remote 0x368cdd0>, <DelayedCall(resetter=<instancemethod at remote 0x36512d0>, seconds=<built-in function time>, args=(...), canceller=<instancemethod at remote 0x3f8a730>, delayed_time=0, kw={}...(truncated)\ntimeout = <float at remote 0x34bc670>\n(gdb) print timeout\n$1 = 5\nAnd also,\nscrapy==1.5.0\ntwisted==16.4.1\nOS==centOs, 3.10.0-693.11.1.el7.x86_64\nSo,why the process doesn't quite itself when timeout does have a value? How to fix it?", "issue_status": "Closed", "issue_reporting_time": "2018-02-05T09:03:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "446": {"issue_url": "https://github.com/scrapy/scrapy/issues/3107", "issue_id": "#3107", "issue_summary": "can't crawl all url", "issue_description": "Linoonphan commented on Feb 5, 2018\nScrapy crawl the page is not complete, in the rules of the write can only crawl part of the url, but can not crawl all, the same rules to crawl a page alone can successfully crawl, I do not know what the problem.", "issue_status": "Closed", "issue_reporting_time": "2018-02-05T02:27:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "447": {"issue_url": "https://github.com/scrapy/scrapy/issues/3106", "issue_id": "#3106", "issue_summary": "Discussion regarding Asyncio", "issue_description": "Contributor\nyashrsharma44 commented on Feb 3, 2018\nI have been going through the GSOC projects of last summer, and I was interested in asyncio prototype. Now, there was one issue that came after I searched through the details of asyncio. Asyncio does not have any built in abstraction for error handling, unlike Twisted, so that means up a lot of work regarding error handling which is quite error prone when it is handled manually. So I wanted to discuss as to how would the developers would go about the error handling of asyncio ?", "issue_status": "Closed", "issue_reporting_time": "2018-02-03T12:38:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "448": {"issue_url": "https://github.com/scrapy/scrapy/issues/3104", "issue_id": "#3104", "issue_summary": "How to get all the items (via Signal API)", "issue_description": "lolobosse commented on Feb 1, 2018\nHello there,\nI wrote the following code:\n process = CrawlerProcess()\n results = []\n\n def crawler_results(parse_result):\n      results.append(parse_result)\n\n # The line stuff are some params, not interesting ;)\n process.crawl(BASpider, self.server, line[0], line[2], line[1])\n for p in process.crawlers:\n     p.signals.connect(crawler_results, signal=scrapy.signals.item_dropped)\n process.start()\nBut the method crawler_results is never triggered and I do not understand why. Is it not supposed to work like this? (I mean, it's not working with item_scrapped or engine_started either) I just get no events triggered\nElse, how would your retrieve the items of a spider with CrawlerProcess? (having a pipe is not something handy because I need to send all the data at once).", "issue_status": "Closed", "issue_reporting_time": "2018-01-31T18:46:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "449": {"issue_url": "https://github.com/scrapy/scrapy/issues/3103", "issue_id": "#3103", "issue_summary": "twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost", "issue_description": "Svickie7 commented on Jan 31, 2018 \u2022\nedited\n-- coding: utf-8 --\nimport scrapy\nclass FarmersSpider(scrapy.Spider):\nname = 'farmers'\nallowed_domains = ['www.farmerscompress.com']\nlogin_url = 'https://www.farmerscompress.com/ProcessUser.aspx'\nstart_urls = [login_url]\ndef parse(self, response):\n yield scrapy.FormRequest(url=self.login_url,\n  formdata={'T1': 'user', 'T2': 'pass', 'B2': 'Login'},\n  callback=self.after_login)\n\ndef after_login(self, response):\n tes = response.css('#Label1').extract()\n yield {'tes': 'tes', 'tes': tes}\n2018-01-31 22:50:30 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2018-01-31 22:50:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.farmerscompress.com/robots.txt> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-31 22:50:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.farmerscompress.com/robots.txt> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-31 22:50:32 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://www.farmerscompress.com/robots.txt> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-31 22:50:32 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://www.farmerscompress.com/robots.txt>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-31 22:50:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.farmerscompress.com/ProcessUser.aspx> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-31 22:50:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.farmerscompress.com/ProcessUser.aspx> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-31 22:50:33 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://www.farmerscompress.com/ProcessUser.aspx> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-31 22:50:34 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.farmerscompress.com/ProcessUser.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-31 22:50:34 [scrapy.core.engine] INFO: Closing spider (finished)\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2018-01-31T17:26:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "450": {"issue_url": "https://github.com/scrapy/scrapy/issues/3102", "issue_id": "#3102", "issue_summary": "Possible bug in response.xpath()", "issue_description": "mohdhashimpa commented on Jan 31, 2018 \u2022\nedited\nThere was an issue in parsing \"title\" from the HTML file attached.\ntest.txt\nSample HTML :\n<html>\n   <head>\n      <div></div>\n      <title>Gas safety - 10 tips that could save your life</title>\n   </head>\n   <body>\n   </body>\n</html>\nresponse.xpath('/html/head/title/text()').extract()\nThe above was returing empty array.\nBut title was successfully parsed from the following expression\nresponse.xpath('//title/text()').extract()\nI have reproduced this issue in lxml using the following code.\nfrom lxml import etree\nwith open('./test.txt', 'r') as test_html:\n  file_contents = test_html.read()\n  tree = etree.HTML(file_contents)\n  print('via head specified xpath expression : ',tree.xpath('/html/head/title/text()'))\n  print('via take every title xpath expression : ',tree.xpath('//title/text()'))\nSeems the issue is caused by the div tag.\nIf that tag is removed, it works properly.\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2018-01-31T13:16:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "451": {"issue_url": "https://github.com/scrapy/scrapy/issues/3099", "issue_id": "#3099", "issue_summary": "DEBUG: Retrying", "issue_description": "Svickie7 commented on Jan 30, 2018 \u2022\nedited\nclass Someweb(scrapy.Spider):\nname = 'someweb'\nstart_urls = ['http://www.someweb.com//']\ndef parse(self, response):\n    return scrapy.FormRequest.from_response(\n        response,\n        formxpath = '//*[@name=\"B2\"]',\n        formdata={'T1': '123', 'T2': '123'},\n        callback=self.after_login\n    )\n    \ndef after_login(self, response):\n    # check login succeed before going on\n    yield {\"res\" : response , \"hi\" : \"hi\"}\n    if \"authentication failed\" in response.body:\n        self.logger.error(\"Login failed\")\n        return    \n<f_orm method=\"POST\" action=\"https://www.someweb.com/ProcessUser.aspx\">\n<l_abel>\nUSER ID:\n<in_put name=\"T1\" size=\"10\" type=\"text\">\nPASSWORD:\n<in_put name=\"T2\" size=\"10\" type=\"password\">\n<inp_ut value=\"Login\" name=\"B2\" type=\"submit\">\n</la_bel>\n</f_orm>\n2018-01-30 15:56:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on\n2018-01-30 15:56:50 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://www.somweb.com/robots.txt> (referer: None)\n2018-01-30 15:56:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.somweb.com//> (referer: None)\n2018-01-30 15:56:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST https://www.someweb.com/ProcessUser.aspx> (failed 1 times): []\n2018-01-30 15:56:51 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST https://www.someweb.com/ProcessUser.aspx> (failed 2 times): []\n2018-01-30 15:56:52 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <POST https://www.someweb.com/ProcessUser.aspx> (failed 3 tim\nes): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection\nlost.>]\n2018-01-30 15:56:52 [scrapy.core.scraper] ERROR: Error downloading <POST https://www.someweb.com/ProcessUser.aspx>: [<twisted.python.failure.F\nailure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]", "issue_status": "Closed", "issue_reporting_time": "2018-01-30T10:37:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "452": {"issue_url": "https://github.com/scrapy/scrapy/issues/3098", "issue_id": "#3098", "issue_summary": "Error in extract data in span tag", "issue_description": "Svickie7 commented on Jan 29, 2018 \u2022\nedited\nthis in my website inspect element\n\"\"\"\"\"\"<span id=\"price_MP000000001881735\"> \u20b9999</span>\"\"\"\"\"\"\"\"\"\"\"\nin my scrapy shell\nww.css('.old > .priceFormat > span::text').extract()[0]\nu' \\u20b91499'\nww.xpath('//p[@Class=\"old\"]/span[@Class=\"priceFormat\"]/span').extract_first()\nu' \\u20b91499'\nww.xpath('//p[@Class=\"old\"]/span[@Class=\"priceFormat\"]/span/text()').extract_first()\nu' \\u20b91499'\nI need a result like\n\u20b9999 or 999", "issue_status": "Closed", "issue_reporting_time": "2018-01-29T13:50:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "453": {"issue_url": "https://github.com/scrapy/scrapy/issues/3097", "issue_id": "#3097", "issue_summary": "`item_urls` is not defined in example", "issue_description": "tobiasfielitz commented on Jan 29, 2018\nhere in def parse", "issue_status": "Closed", "issue_reporting_time": "2018-01-29T03:58:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "454": {"issue_url": "https://github.com/scrapy/scrapy/issues/3094", "issue_id": "#3094", "issue_summary": "Wrong file to log", "issue_description": "edurenye commented on Jan 26, 2018\nI'm running multiple CrawlerRunner to crawl parallel sites, each CrawlerRunner has configured a different LOG_FILE.\nWhen I run it, all the files just have logged the initialization of the Spider, until the line [scrapy.core.engine] INFO: Spider opened\nthen all log into the same file.\nI expect that each Spider logs into his configured file.", "issue_status": "Closed", "issue_reporting_time": "2018-01-26T11:00:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "455": {"issue_url": "https://github.com/scrapy/scrapy/issues/3093", "issue_id": "#3093", "issue_summary": "Cannot override default Connection header", "issue_description": "RavenHustler commented on Jan 26, 2018\nCannot override the default Connection: close header of scrapy requests. I want to scrape a site for which I need to send Connection: keep-alive headers and remove the default Connection: close header.\nTried using custom settings, custom headers and default headers to change Connection: close to Connection: keep-alive but it instead merges and sends two Connection headers.\nIt works in scrapy shell but not when used in a spider.\nI used Fiddler to check the headers scrapy was sending. Please use Fiddler or any other packet sniffing tool to check the headers as it works fine in scrapy shell.\nBelow is the different things I tried:\nUsing custom settings:\ncustom_settings = {\n        'DEFAULT_REQUEST_HEADERS': {\n            'Connection': 'keep-alive'\n        }\n    }\nUsing custom headers:\nheaders = {\"Connection\": \"keep-alive\"}\n\nfor url in urls:\n        yield scrapy.Request(url=url, callback=self.parse, headers=headers)\nChanging default header settings:\nDEFAULT_REQUEST_HEADERS = {\n  'Connection': 'keep-alive',\n}", "issue_status": "Closed", "issue_reporting_time": "2018-01-26T07:46:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "456": {"issue_url": "https://github.com/scrapy/scrapy/issues/3091", "issue_id": "#3091", "issue_summary": "DEBUG: Crawled (407)", "issue_description": "Svickie7 commented on Jan 25, 2018\nfetch(\"http://wwww.google.com/\")\n2018-01-25 18:46:23 [scrapy.core.engine] INFO: Spider opened\n2018-01-25 18:46:27 [scrapy.core.engine] DEBUG: Crawled (407) <GET http://wwww.g\noogle.com/> (referer: None)", "issue_status": "Closed", "issue_reporting_time": "2018-01-25T13:18:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "457": {"issue_url": "https://github.com/scrapy/scrapy/issues/3088", "issue_id": "#3088", "issue_summary": "install scrapy failed on Mac OS X High Sierra", "issue_description": "kintela commented on Jan 22, 2018\nHi\nI'm trying to install Scarpy in Mac OSX High Sierra after install python by Homebrew but always start with this warning:\n\"The directory '/Users/kintela/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\"\nI have checked that the current user \"Kintela\" is owner and has read/write permissons in that folders.\nAfter the process i get this error\nInstalling collected packages: six, w3lib, cssselect, lxml, parsel, attrs, pyasn1, pyasn1-modules, service-identity, queuelib, PyDispatcher, constantly, incremental, hyperlink, Automat, Twisted, scrapy\nFound existing installation: six 1.4.1\nDEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\nUninstalling six-1.4.1:\nException:\nTraceback (most recent call last):\nFile \"/Library/Python/2.7/site-packages/pip/basecommand.py\", line 215, in main\nstatus = self.run(options, args)\nFile \"/Library/Python/2.7/site-packages/pip/commands/install.py\", line 342, in run\nprefix=options.prefix_path,\nFile \"/Library/Python/2.7/site-packages/pip/req/req_set.py\", line 778, in install\nrequirement.uninstall(auto_confirm=True)\nFile \"/Library/Python/2.7/site-packages/pip/req/req_install.py\", line 754, in uninstall\npaths_to_remove.remove(auto_confirm)\nFile \"/Library/Python/2.7/site-packages/pip/req/req_uninstall.py\", line 115, in remove\nrenames(path, new_path)\nFile \"/Library/Python/2.7/site-packages/pip/utils/init.py\", line 267, in renames\nshutil.move(old, new)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 302, in move\ncopy2(src, real_dst)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 131, in copy2\ncopystat(src, dst)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 103, in copystat\nos.chflags(dst, st.st_flags)\nOSError: [Errno 1] Operation not permitted: '/tmp/pip-M8QFCu-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info'\nAny idea please?\nRegards", "issue_status": "Closed", "issue_reporting_time": "2018-01-22T12:41:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "458": {"issue_url": "https://github.com/scrapy/scrapy/issues/3087", "issue_id": "#3087", "issue_summary": "how to use Scrapy post the multipart/form-data to the api?", "issue_description": "mtianyan commented on Jan 21, 2018 \u2022\nedited\nI can\u2018t find the useful method to post the multipart/form-data to the api. I ask the question in the stackoverflow\nbut I can't get a good answer that can help me.\nhttps://stackoverflow.com/questions/48343098/how-to-use-scrapy-post-the-multipart-form-data-to-the-api", "issue_status": "Closed", "issue_reporting_time": "2018-01-21T05:21:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "459": {"issue_url": "https://github.com/scrapy/scrapy/issues/3085", "issue_id": "#3085", "issue_summary": "Scrapy with Splash and Rotating Proxies - requests aren't made via Splash", "issue_description": "zinyosrim commented on Jan 19, 2018\nI'm trying to use scrapy with splash and rotating proxies. Here's my settings.py:\nROBOTSTXT_OBEY = False\nBOT_NAME = 'mybot'\nSPIDER_MODULES = ['myproject.spiders']\nNEWSPIDER_MODULE = 'myproject.spiders'\nLOG_LEVEL = 'INFO'\nUSER_AGENT = 'Mozilla/5.0'\n\n# JSON file pretty formatting\nFEED_EXPORT_INDENT = 4\n\n# Suppress dataloss warning messages of scrapy downloader\nDOWNLOAD_FAIL_ON_DATALOSS = False \nDOWNLOAD_DELAY = 1.25  \n\n# Enable or disable spider middlewares\nSPIDER_MIDDLEWARES = {\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\n}\n\n# Enable or disable downloader middlewares\nDOWNLOADER_MIDDLEWARES = {\n    'rotating_proxies.middlewares.RotatingProxyMiddleware': 610,\n    'rotating_proxies.middlewares.BanDetectionMiddleware': 620,\n    'scrapy_splash.SplashCookiesMiddleware': 723,\n    'scrapy_splash.SplashMiddleware': 725,\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n}\n\n# Splash settings\nHTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\nSPLASH_URL = 'http://localhost:8050'\nI'm setting the ROTATING_PROXY_LIST in my spider:\nproxy_list = re.findall(r'(\\d*\\.\\d*\\.\\d*\\.\\d*\\:\\d*)\\b', requests.get(\"https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list.txt\").text)\ncustom_settings = {'ROTATING_PROXY_LIST': proxy_list}\nI started splash with\ndocker run -p 8050:8050 scrapinghub/splash\nBut, when running the crawler I don't see any requests going through Splash. How can I fix this?\nThanks\nZin", "issue_status": "Closed", "issue_reporting_time": "2018-01-19T11:48:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "460": {"issue_url": "https://github.com/scrapy/scrapy/issues/3084", "issue_id": "#3084", "issue_summary": "Why not support more S3 storage configuration in setting file when using Filespipline in release version?", "issue_description": "Anderson-Liu commented on Jan 19, 2018 \u2022\nedited\nHi. I meet a problem after compare the source code in this Github repo with the code installed inside my machine's site-packages directory when I try to custom aws S3 endpoint url in setting file . I found that the code relate this feature had been commit about 11 months ago in master branch and you can check this one or this one for more detail.\nMy installed scrapy is the latest one(1.5.0) and I also run pip show to confirm that.\nName: Scrapy\nVersion: 1.5.0\nSummary: A high-level Web Crawling and Web Scraping framework\nHome-page: https://scrapy.org\nAuthor: Pablo Hoffman\nAuthor-email: pablo@pablohoffman.com\nLicense: BSD\nLocation: /home/liuhongda1/.pyenv/versions/3.6.4/lib/python3.6/site-packages\nRequires: queuelib, pyOpenSSL, lxml, Twisted, PyDispatcher, w3lib, six, service-identity, parsel, cssselect\nAnd in my installed package's code(which under site-packages/scrapy) I found code like this:\nclass S3FilesStore(object):\n\n    AWS_ACCESS_KEY_ID = None\n    AWS_SECRET_ACCESS_KEY = None\n\n    POLICY = 'private'  # Overriden from settings.FILES_STORE_S3_ACL in\n                        # FilesPipeline.from_settings.\n    HEADERS = {\n        'Cache-Control': 'max-age=172800',\n    }\n\n    def __init__(self, uri):\n        self.is_botocore = is_botocore()\n        if self.is_botocore:\n            import botocore.session\n            session = botocore.session.get_session()\n            self.s3_client = session.create_client(\n                's3', aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY)\n        else:\n            from boto.s3.connection import S3Connection\n            self.S3Connection = S3Connection\n        assert uri.startswith('s3://')\n        self.bucket, self.prefix = uri[5:].split('/', 1)\nrather than the code inside Github repo's master branch:\nclass S3FilesStore(object):\n    AWS_ACCESS_KEY_ID = None\n    AWS_SECRET_ACCESS_KEY = None\n    AWS_ENDPOINT_URL = None\n    AWS_REGION_NAME = None\n    AWS_USE_SSL = None\n    AWS_VERIFY = None\n\n    POLICY = 'private'  # Overriden from settings.FILES_STORE_S3_ACL in\n                        # FilesPipeline.from_settings.\n    HEADERS = {\n        'Cache-Control': 'max-age=172800',\n    }\n\n    def __init__(self, uri):\n        self.is_botocore = is_botocore()\n        if self.is_botocore:\n            import botocore.session\n            session = botocore.session.get_session()\n            self.s3_client = session.create_client(\n                's3',\n                aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n                endpoint_url=self.AWS_ENDPOINT_URL,\n                region_name=self.AWS_REGION_NAME,\n                use_ssl=self.AWS_USE_SSL,\n                verify=self.AWS_VERIFY\n            )\n        else:\n            from boto.s3.connection import S3Connection\n            self.S3Connection = S3Connection\n        assert uri.startswith('s3://')\n        self.bucket, self.prefix = uri[5:].split('/', 1)\n     ......\n    @classmethod\n    def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['FILES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n\n        store_uri = settings['FILES_STORE']\n        return cls(store_uri, settings=settings)\nSo, my problem is why not add those code into release version package ? @kmike", "issue_status": "Closed", "issue_reporting_time": "2018-01-19T10:47:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "461": {"issue_url": "https://github.com/scrapy/scrapy/issues/3081", "issue_id": "#3081", "issue_summary": "Not Scraping", "issue_description": "Harish120896 commented on Jan 18, 2018\nscrapy shell https://www.zomato.com\n2018-01-18 18:27:33 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n2018-01-18 18:27:33 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g 2 Nov 2017), cryptography 2.1.4, Platform Linux-4.13.0-26-generic-x86_64-with-Ubuntu-16.04-xenial\n2018-01-18 18:27:33 [scrapy.crawler] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2018-01-18 18:27:33 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.memusage.MemoryUsage',\n'scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.telnet.TelnetConsole']\n2018-01-18 18:27:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2018-01-18 18:27:33 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2018-01-18 18:27:33 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2018-01-18 18:27:33 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6028\n2018-01-18 18:27:33 [scrapy.core.engine] INFO: Spider opened\n^C2018-01-18 18:30:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.zomato.com> (failed 1 times): User timeout caused connection failure: Getting https://www.zomato.com took longer than 180.0 seconds.", "issue_status": "Closed", "issue_reporting_time": "2018-01-18T13:03:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "462": {"issue_url": "https://github.com/scrapy/scrapy/issues/3080", "issue_id": "#3080", "issue_summary": "DEBUG: Retrying <GET https://www.zomato.com/india> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]", "issue_description": "Harish120896 commented on Jan 18, 2018\nWhen i try to crawl this site its shows the following error:\n2018-01-18 11:38:11 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.zomato.com/india> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-18 11:38:30 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://www.zomato.com/india> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-18 11:38:31 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.zomato.com/india>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2018-01-18 11:38:31 [scrapy.core.engine] INFO: Closing spider (finished)\n2018-01-18 11:38:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 3,\n'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,\n'downloader/request_bytes': 1485,\n'downloader/request_count': 3,\n'downloader/request_method_count/GET': 3,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2018, 1, 18, 6, 8, 31, 579117),\n'log_count/CRITICAL': 16,\n'log_count/DEBUG': 4,\n'log_count/ERROR': 1,\n'log_count/INFO': 7,\n'retry/count': 2,\n'retry/max_reached': 1,\n'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,\n'scheduler/dequeued': 3,\n'scheduler/dequeued/memory': 3,\n'scheduler/enqueued': 3,\n'scheduler/enqueued/memory': 3,\n'start_time': datetime.datetime(2018, 1, 18, 6, 7, 32, 562915)}\n2018-01-18 11:38:31 [scrapy.core.engine] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2018-01-18T06:12:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "463": {"issue_url": "https://github.com/scrapy/scrapy/issues/3079", "issue_id": "#3079", "issue_summary": "Connection Lost in a non-clean fashion for some URLs on a particular domain, but not others", "issue_description": "FredEnglish commented on Jan 16, 2018 \u2022\nedited\nI have created a basic spider to scrape a small group of job listings from totaljobs.com. I have set up the spider with a single start URL, to bring up the list of jobs I am interested in. From there, I launch a separate request for each page of the results. Within each of these requests, I launch a separate request calling back to a different parse method, to handle the individual job URLs.\nWhat I'm finding is that the start URL and all of the results page requests are handled fine - scrapy connects to the site and returns the page content. However, when it attempts to follow the URLs for each individual job page, scrapy isn't able to form a connection. Within my log file, it states:\n[<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nI'm afraid that I don't have a huge amount of programming experience or knowledge of internet protocols etc. so please forgive me for not being able to provide more information on what might be going on here. I have tried changing the TLS connection type; updating to the latest version of scrapy, twisted and OpenSSL; rolling back to previous versions of scrapy, twisted and OpenSSL; rolling back the cryptography version, creating a custom Context Factory and trying various browser agents and proxies. I get the same outcome every time: whenever the URL relates to a specific job page, scrapy cannot connect and I get the above log file output.\nIt may be likely that I am overlooking something very obvious to seasoned scrapers, that is preventing me from connecting with scrapy. I have tried following some of the the advice in these threads:\n#1429\npsf/requests#4458\n#2717\nHowever, some of it is a bit over my head e.g. how to update cipher lists etc. I presume that it is some kind of certification issue, but then again scrapy is able to connect to other URLs on that domain, so I don't know.\nThe code that I've been using to test this is very basic, but here it is anyway:\nimport scrapy\n\nclass Test(scrapy.Spider):\n \n\n start_urls = [\n     'https://www.totaljobs.com/job/welder/jark-wakefield-job79229824'\n     ,'https://www.totaljobs.com/job/welder/elliott-wragg-ltd-job78969310'\n     ,'https://www.totaljobs.com/job/welder/exo-technical-job79019672'\n     ,'https://www.totaljobs.com/job/welder/exo-technical-job79074694'\n      ]\n \n name = \"test\"\n\n def parse(self, response):\n  print 'aaaa'\n                yield {'a': 1}\nThe URLs in the above code are not being connected to successfully.\nThe URLs in the below code are being connected to successfully.\nimport scrapy\n\nclass Test(scrapy.Spider):\n \n\n start_urls = [\n     'https://www.totaljobs.com/jobs/permanent/welder/in-uk'\n     ,'https://www.totaljobs.com/jobs/permanent/mig-welder/in-uk'\n     ,'https://www.totaljobs.com/jobs/permanent/tig-welder/in-uk'\n      ]\n \n name = \"test\"\n\n def parse(self, response):\n  print 'aaaa'\n                yield {'a': 1}\nIt'd be great if someone could replicate this behavior (or not as the case may be) and let me know. Please let me know if I should submit additional details. I apologise, if I have overlooked something really obvious. I am using:\nWindows 7 64 bit\nPython 2.7\nscrapy version 1.5.0\ntwisted version 17.9.0\nopenSSL version 17.5.0\nlxml version 4.1.1", "issue_status": "Closed", "issue_reporting_time": "2018-01-16T14:21:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "464": {"issue_url": "https://github.com/scrapy/scrapy/issues/3078", "issue_id": "#3078", "issue_summary": "Scrapy how to limit the breadth? ?", "issue_description": "thsheep commented on Jan 16, 2018 \u2022\nedited\nI'm very sorry to bother you.\nHow to limit the breadth in Scrapy?\nOr how to achieve?\nPlease enlighten me. thank you very much\n@kmike @redapple\nalready solved", "issue_status": "Closed", "issue_reporting_time": "2018-01-16T09:50:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "465": {"issue_url": "https://github.com/scrapy/scrapy/issues/3076", "issue_id": "#3076", "issue_summary": "How can I add a spider to the scrapy engine after the project is already started", "issue_description": "xuexingdong commented on Jan 15, 2018 \u2022\nedited\nAs the title.", "issue_status": "Closed", "issue_reporting_time": "2018-01-15T09:44:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "466": {"issue_url": "https://github.com/scrapy/scrapy/issues/3074", "issue_id": "#3074", "issue_summary": "Cxfreeze+python3.4+scrapy1.4 failed to bundle to executables(AttributeError: module object has no attribute '_fix_up_module')", "issue_description": "Caleb-Wade commented on Jan 12, 2018\nAfter I failed to bundle .py document into .exe document by pyinstaller, I tried cxfreeze. Similar error happened. Someting went wrong when importing scrapy module, and AttributeError: module object has no attribute '_fix_up_module' appeared in the command window. Would someone tell me what happened. Can I make it? Thks!", "issue_status": "Closed", "issue_reporting_time": "2018-01-12T12:56:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "467": {"issue_url": "https://github.com/scrapy/scrapy/issues/3073", "issue_id": "#3073", "issue_summary": "Pyinstaller+Python3.4+Scrapy1.4 Failed to bundled exe (ImportError:No module named \"XXXX\")", "issue_description": "Caleb-Wade commented on Jan 12, 2018\nI have tried to bundle my spider .py into .exe, but weeks pasted, I didn't make it. It pointed that some modules were missing. Even I used --hidden-import to put the missing modules into my bundled exe, it still didn't work. It's amazing that I can see the modules in the HTML document under build file. Pls help me ,Thks!", "issue_status": "Closed", "issue_reporting_time": "2018-01-12T12:30:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "468": {"issue_url": "https://github.com/scrapy/scrapy/issues/3071", "issue_id": "#3071", "issue_summary": "Why using popen can't start spider", "issue_description": "linglaiyao1314 commented on Jan 12, 2018\nI need using subprocess.Popen to start spider , but can't work.\nif I type the command in shell:\nnohup scrapy crawl helloworld --logfile log.txt &\nit workers well.\nBut if I using subprocess:\ncommand = [\"nohup\", \"scrapy\", \"crawl\", \"helloworld\", \"--logfile\", \"log.log\", \"&\"] subprocess.Popen(command)\nit can't work , and output:\nscrapy crawl [options] <spider> crawl: error: running 'scrapy crawl' with more than one spider is no longer supported\nso, how can I slove this problem?", "issue_status": "Closed", "issue_reporting_time": "2018-01-12T07:46:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "469": {"issue_url": "https://github.com/scrapy/scrapy/issues/3070", "issue_id": "#3070", "issue_summary": "Pipelines don't get applied on items when running spider with `scrapy parse`", "issue_description": "roboostify commented on Jan 11, 2018\nI usually test my spiders with the command scrapy parse --spider=myspider -c parse_whatever <url_to_scrape>.\nSo far it worked out well, but when I want to apply a pipeline on my items it just doesn't work, at least it seems like the pipeline is skipped during execution, however the log says that the pipeline is enabled.\nSo for example, I have the following Item, Spider and Pipeline to test the issue:\nItem:\nclass MyItem(scrapy.Item):\n    status = scrapy.Field()\n    date_created = scrapy.Field()\n    site = scrapy.Field()\nSpider:\nclass TestSpider(Spider):\n    name = 'test'\n    start_urls = ['http://www.example.com',]\n\n    def parse(self, response):\n        item = ProductPriceItem()\n        yield item\nPipeline:\nclass DefaultValuePipeline(object):\n    def process_item(self, item, spider):\n        item.setdefault('status', 'New')\n        item.setdefault('date_created', '01/12/2018')\n        item.setdefault('site',  'example.com')\n\n        return item\nAnd my problem is when I execute the spider as:\nscrapy parse --spider=test -c parse http://example.com\nIt will output:\n# Scraped Items  ------------------------------------------------------------\n[{}]\nWhile using:\nscrapy crawl test\nWill output:\n2018-01-11 12:05:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.example.com>\n{'date_created': '01/12/2018',\n 'site': 'example.com',\n 'status': 'New'}\nAs expected.\nAm I missing something or it's a bug?", "issue_status": "Closed", "issue_reporting_time": "2018-01-11T11:08:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "470": {"issue_url": "https://github.com/scrapy/scrapy/issues/3069", "issue_id": "#3069", "issue_summary": "about the signal retry_complete", "issue_description": "lifei1245 commented on Jan 11, 2018\nI didn't find the singnal in the singnal list,how can I use it", "issue_status": "Closed", "issue_reporting_time": "2018-01-11T10:30:14Z", "fixed_by": "#3668", "pull_request_summary": "Remove the unexisting retry_complete signal from the documentation", "pull_request_description": "Member\nGallaecio commented on Mar 8, 2019\nFixes #3069", "pull_request_status": "Merged", "issue_fixed_time": "2019-03-14T17:16:29Z", "files_changed": [["2", "docs/topics/downloader-middleware.rst"], ["4", "scrapy/downloadermiddlewares/retry.py"]]}, "471": {"issue_url": "https://github.com/scrapy/scrapy/issues/3068", "issue_id": "#3068", "issue_summary": "scrapy always Starting new HTTP connection after crawl finished", "issue_description": "3xp10it commented on Jan 11, 2018\nI reopen #3066 here,there are more details here.", "issue_status": "Closed", "issue_reporting_time": "2018-01-11T01:14:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "472": {"issue_url": "https://github.com/scrapy/scrapy/issues/3067", "issue_id": "#3067", "issue_summary": "scrapy.Request doesnt wait for loop", "issue_description": "theduman commented on Jan 11, 2018 \u2022\nedited by kmike\nI fetch url and other parameters from database and use them in for loop and pass to scrapy.request() function but when i run the code i succesfully pass first element only. Scrapy can't get other elements of list. Here is my code\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    custom_settings = {\n        'CONCURRENT_REQUESTS': 1,\n    }\n    def start_requests(self):\n        sourceArr = []\n        connection = db.connect()\n        try:\n            with connection.cursor() as cursor:\n                # Read a single record\n                sql = \"SELECT `code`, `url`, `target`,`next` FROM `source`\"\n                cursor.execute(sql)\n                result = cursor.fetchall()\n        finally:\n            connection.close()\n        print(result)\n        for i in result:\n            source = Source(i['code'], i['url'], i['target'], i['next'])\n            sourceArr.append(source)\n        print(sourceArr)\n        #for url in urls:\n           #yield scrapy.Request(url=url, callback=self.parse)\n        for s in sourceArr:\n            print(s.target)\n            yield scrapy.Request(url=s.url, meta={'target': s.target, 'next': s.next})\n            print(\"slept\")\n\n\n    def parse(self, response):\n        titlearr = []\n        count = 0\n        print(response.meta)\n        for title in response.css(response.meta['target']):\n            count += 1\n            titlearr.append(title.css('p a::text').extract_first())\n            #yield {'title': title.css('p a::text').extract_first()}\n        print(\"total count \" + str(count))\n        print(titlearr)\n        for next_page in response.css(response.meta['next']):\n            yield response.follow(next_page, self.parse)\nWhen i print response.meta i got target and next values for only first item. List contains more than 1 item. How can i make scrapy.Request() function wait for the next element to run?", "issue_status": "Closed", "issue_reporting_time": "2018-01-10T19:09:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "473": {"issue_url": "https://github.com/scrapy/scrapy/issues/3066", "issue_id": "#3066", "issue_summary": "scrapy always Starting new HTTP connection after crawl finished", "issue_description": "3xp10it commented on Jan 10, 2018 \u2022\nedited\nAfter my spider has crawled all the urls,scrapy doesn't stoped,how to stop it after crawl finished?\nThe start url is http://http://192.168.139.28/dvwa.\nHere is the output info:\n 'retry/reason_count/504 Gateway Time-out': 2,\n 'scheduler/dequeued': 82,\n 'scheduler/dequeued/memory': 82,\n 'scheduler/enqueued': 82,\n 'scheduler/enqueued/memory': 82,\n 'splash/execute/request_count': 40,\n 'splash/execute/response_count/200': 38,\n 'splash/execute/response_count/400': 1,\n 'splash/execute/response_count/504': 3,\n 'start_time': datetime.datetime(2018, 1, 10, 6, 36, 4, 298146)}\n  2018-01-10 14:37:48 [scrapy.core.engine] INFO: Spider closed (finished)\n  2018-01-10 14:38:41 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:38:41 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:39:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:39:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:40:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:40:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:41:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:41:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:42:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:42:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:43:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:43:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:44:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:44:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:45:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:45:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:46:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:46:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:47:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:47:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:48:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:48:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:49:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:49:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:50:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:50:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:51:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:51:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:52:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:52:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  2018-01-10 14:53:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 192.168.139.28\n  2018-01-10 14:53:42 [urllib3.connectionpool] DEBUG: http://192.168.139.28:80 \"GET / HTTP/1.1\" 200 3041\n  ...", "issue_status": "Closed", "issue_reporting_time": "2018-01-10T07:17:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "474": {"issue_url": "https://github.com/scrapy/scrapy/issues/3063", "issue_id": "#3063", "issue_summary": "How can I set the spider name dynamicly?", "issue_description": "xuexingdong commented on Jan 5, 2018\nI want to store my spider name in database, and load it like this, but why it doesn't work?\nprocess = CrawlerProcess(get_project_settings())\n    for spider_config in spiders_configs:\n        process.crawl(XXXSpider(spider_config.name))\n    process.start()", "issue_status": "Closed", "issue_reporting_time": "2018-01-05T10:30:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "475": {"issue_url": "https://github.com/scrapy/scrapy/issues/3060", "issue_id": "#3060", "issue_summary": "HTTPERROR_ALLOWED_CODES can't parse response with 301", "issue_description": "kmagonski commented on Jan 3, 2018\nIn scrapy.downloadermiddlewares.redirect.RedirectMiddleware#process_response\ndosn't get anything from settings like HTTPERROR_ALLOWED_CODES only in HttpErrorMiddleware we have\nhandle_httpstatus_list.", "issue_status": "Closed", "issue_reporting_time": "2018-01-02T18:46:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "476": {"issue_url": "https://github.com/scrapy/scrapy/issues/3057", "issue_id": "#3057", "issue_summary": "configure_logging unable to handle GBK", "issue_description": "Contributor\nNewUserHa commented on Dec 31, 2017\nI added\n`import logging\nfrom scrapy.utils.log import configure_logging\nconfigure_logging(install_root_handler=False)\nlogging.basicConfig(\nfilename='log.txt',\nformat='%(levelname)s: %(message)s',\nlevel=logging.INFO\n)`\nfrom scrapy documents blow my class define.\nthen:\n--- Logging error ---\nTraceback (most recent call last):\nFile \"C:\\Program Files\\Python35\\lib\\logging_init_.py\", line 982, in emit\nstream.write(msg)\nUnicodeEncodeError: 'gbk' codec can't encode character '\\ufe0f' in position 190: illegal multibyte sequence\nCall stack:\n....\nFile \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\core\\scraper.py\", line 237, in _itemproc_finished\nlogger.log(*logformatter_adapter(logkws), extra={'spider': spider})\nMessage: 'Scraped from %(src)s\\r\\n%(item)s'\nArguments: {'src': <200 http://...>, 'item': {'date': '12-02', 'floor': '...\\n \u7535\\ufe0f', 'pics': ['...', ...]}}\nI also have 'LOG_LEVEL': 'INFO' in the spider.\nI googled and have no idea how to fix this.\nany help, please?\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2017-12-31T08:18:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "477": {"issue_url": "https://github.com/scrapy/scrapy/issues/3054", "issue_id": "#3054", "issue_summary": "Request serialization should fail for non-picklable objects", "issue_description": "Member\nelacuesta commented on Dec 29, 2017\nThe Pickle-based disk queues silently serialize requests that shouldn't be serialized in Python<=3.5. I found this problem when dumping a request with an ItemLoader object in its meta dict. Python 3.6 fails in this line with TypeError: can't pickle HtmlElement objects, because the loader contains a Selector, which in turns contains an HtmlElement object.\nI tested this using the https://github.com/scrapinghub/scrapinghub-stack-scrapy repository, and found that pickle.loads(pickle.dumps(selector)) doesn't fail, but generates a broken object.\nPython 2.7, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3)\nroot@04bfc6cf84cd:/# scrapy version -v\nScrapy    : 1.3.3\nlxml      : 3.7.2.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 16.6.0\nPython    : 2.7.14 (default, Dec 12 2017, 16:55:09) - [GCC 4.9.2]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\nroot@04bfc6cf84cd:/# scrapy shell \"http://example.org\"\n2017-12-29 16:49:27 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n(...)\n>>> from six.moves import cPickle as pickle\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\n>>> response.selector.css('a')\n[<Selector xpath=u'descendant-or-self::a' data=u'<a href=\"http://www.iana.org/domains/exa'>]\n>>> s2.css('a')\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 227, in css\n    return self.xpath(self._css2xpath(query))\n  File \"/usr/local/lib/python2.7/site-packages/parsel/selector.py\", line 203, in xpath\n    **kwargs)\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\nAssertionError: invalid Element proxy at 140144569743064\nPython 3.5, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\nroot@1945e2154919:/# scrapy version -v\nScrapy    : 1.3.3\nlxml      : 3.7.2.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 16.6.0\nPython    : 3.5.4 (default, Dec 12 2017, 16:43:39) - [GCC 4.9.2]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\nroot@1945e2154919:/# scrapy shell \"http://example.org\"\n2017-12-29 16:52:37 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n(...)\n>>> from six.moves import cPickle as pickle\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\n>>> response.selector.css('a')\n[<Selector xpath='descendant-or-self::a' data='<a href=\"http://www.iana.org/domains/exa'>]\n>>> s2.css('a')\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 227, in css\n    return self.xpath(self._css2xpath(query))\n  File \"/usr/local/lib/python3.5/site-packages/parsel/selector.py\", line 203, in xpath\n    **kwargs)\n  File \"src/lxml/lxml.etree.pyx\", line 1584, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:59349)\n  File \"src/lxml/xpath.pxi\", line 257, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:170478)\n  File \"src/lxml/apihelpers.pxi\", line 19, in lxml.etree._assertValidNode (src/lxml/lxml.etree.c:16482)\nAssertionError: invalid Element proxy at 139862544625976\nPython 3.6, Scrapy 1.3.3 (https://github.com/scrapinghub/scrapinghub-stack-scrapy/tree/branch-1.3-py3)\nroot@43e690443ca7:/# scrapy version -v\nScrapy    : 1.3.3\nlxml      : 3.7.2.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 16.6.0\nPython    : 3.6.4 (default, Dec 21 2017, 01:35:12) - [GCC 4.9.2]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\nPlatform  : Linux-4.9.44-linuxkit-aufs-x86_64-with-debian-8.10\nroot@43e690443ca7:/# scrapy shell \"http://example.org\"\n2017-12-29 16:54:49 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n(...)\n>>> from six.moves import cPickle as pickle\n>>> s2 = pickle.loads(pickle.dumps(response.selector, protocol=2))\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\nTypeError: can't pickle HtmlElement objects\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2017-12-29T17:17:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "478": {"issue_url": "https://github.com/scrapy/scrapy/issues/3052", "issue_id": "#3052", "issue_summary": "Item.fields missing values, while dict(Item.items()) not.", "issue_description": "chinsyo commented on Dec 27, 2017\nPython 3.6.4\nmacOS 10.13.2\n>>> import scrapy\n>>> class CrawlItem(scrapy.Item):\n...     title = scrapy.Field()\n...     link = scrapy.Field()\n... \n>>> article = CrawlItem(title=\"Nice to meet you\", link=\"google.com\")\n>>> article.fields\n{'link': {}, 'title': {}}\n>>> type(article.fields)\n<class 'dict'>\n>>> article.fields.keys()\ndict_keys(['link', 'title'])\n>>> article.fields.values()\ndict_values([{}, {}])\n>>> dict(article.fields)\n{'link': {}, 'title': {}}\n>>> article.items()\nItemsView({'link': 'google.com', 'title': 'Nice to meet you'})\n>>> type(article.items())\n<class 'collections.abc.ItemsView'>\n>>> dict(article.items()).keys()\ndict_keys(['title', 'link'])\n>>> dict(article.items()).values()\ndict_values(['Nice to meet you', 'google.com'])", "issue_status": "Closed", "issue_reporting_time": "2017-12-27T02:33:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "479": {"issue_url": "https://github.com/scrapy/scrapy/issues/3046", "issue_id": "#3046", "issue_summary": "Item loader missing values from base item", "issue_description": "Member\nstummjr commented on Dec 21, 2017\nItemLoaders behave oddly when they get a pre-populated item as an argument and get_output_value() gets called for one of the pre-populated fields before calling load_item().\nCheck this out:\n>>> from scrapy.loader import ItemLoader\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\n>>> loader = ItemLoader(item)\n>>> loader.load_item()\n{'summary': 'foo bar', 'url': 'http://example.com'}\n\n# so far, so good... what about now?\n>>> item = {'url': 'http://example.com', 'summary': 'foo bar'}\n>>> loader = ItemLoader(item)\n>>> loader.get_output_value('url')\n[]\n>>> loader.load_item()\n{'summary': 'foo bar', 'url': []}\nThere are 2 unexpected behaviors in this snippet (at least from my point of view):\n1) loader.get_output_value() doesn't return the pre-populated values, even though they end up in the final item.\nIt seems to be like this on purpose, though. The get_output_value() method only queries the _local_values defaultdict (here).\n2) once we call loader.get_output_value('url'), that field is not included in the load_item() result anymore.\nThis one doesn't look right, IMHO.\nIt happens because when we call loader.get_output_value('url') for the first time, such value is not available on _local_values, and so a new entry in the _local_values defaultdict will be created with an empty list on it (here). Then, when loader.load_item() gets called, these lines overwrite the current value from the internal item because the value returned by get_output_value() is [] and not None.\nAny thoughts on this?", "issue_status": "Closed", "issue_reporting_time": "2017-12-21T01:30:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "480": {"issue_url": "https://github.com/scrapy/scrapy/issues/3044", "issue_id": "#3044", "issue_summary": "Can I get some guidance for outputting scraped data and images to a GCP bucket?", "issue_description": "zinyosrim commented on Dec 19, 2017\nmanaged S3 through settings.py but still struggling with Google cloud.\nThanks\nZin", "issue_status": "Closed", "issue_reporting_time": "2017-12-19T09:55:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "481": {"issue_url": "https://github.com/scrapy/scrapy/issues/3043", "issue_id": "#3043", "issue_summary": "scrapy coredump", "issue_description": "wangjingpei01 commented on Dec 18, 2017 \u2022\nedited\nit's in centos env, when we exec scrapy fetch url or we scrapy crawl ourproject, it always coredump, following is the result:(help-wanted)\n2017-12-18 12:03:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 578,\n'downloader/request_count': 2,\n'downloader/request_method_count/GET': 2,\n'downloader/response_bytes': 33201,\n'downloader/response_count': 2,\n'downloader/response_status_count/200': 1,\n'downloader/response_status_count/302': 1,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2017, 12, 18, 4, 3, 12, 341518),\n'log_count/DEBUG': 1,\n'log_count/INFO': 1,\n'memusage/max': 43687936,\n'memusage/startup': 43687936,\n'response_received_count': 1,\n'scheduler/dequeued': 2,\n'scheduler/dequeued/memory': 2,\n'scheduler/enqueued': 2,\n'scheduler/enqueued/memory': 2,\n'start_time': datetime.datetime(2017, 12, 18, 4, 3, 8, 952436)}\nSegmentation fault (core dumped)\nsince I can't parse the core file, so I don't know how to fix this prob, please hlep, thanks", "issue_status": "Closed", "issue_reporting_time": "2017-12-18T04:14:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "482": {"issue_url": "https://github.com/scrapy/scrapy/issues/3042", "issue_id": "#3042", "issue_summary": "Is it one pipeline instance shared by all spiders?", "issue_description": "holymonson commented on Dec 17, 2017 \u2022\nedited\nAs Pipeline implements pipeline.open_spider(spider) and pipeline.close_spider(spider), I assume that there is one pipeline instance shared by all spiders, instead of one pipeline instance owned by each spider, when runing multiple spiders in the same process.\nNow I want to setup a db client in pipepline for all spiders shared instead of one client for each spider, but there is no open_pipeline and close_pipeline implemented.\nSo, is the statement above correct? Would you consider implementing open_pipeline and close_pipeline? Or open_crawler close_crawler?", "issue_status": "Closed", "issue_reporting_time": "2017-12-17T04:22:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "483": {"issue_url": "https://github.com/scrapy/scrapy/issues/3037", "issue_id": "#3037", "issue_summary": "Unable to build scrapy based project due to invalid environment marker error", "issue_description": "rgupta2 commented on Dec 12, 2017 \u2022\nedited\nI am unable to build one of my projects written few months ago using Scrapy. Its showing following dependency error -\nDownloading/unpacking cryptography>=2.1.4 (from pyOpenSSL->Scrapy==1.4.0->-r /app/requirements.txt (line 3))\n  Running setup.py (path:/tmp/pip_build_root/cryptography/setup.py) egg_info for package cryptography\n    error in cryptography setup command: Invalid environment marker: platform_python_implementation != 'PyPy'\n    Complete output from command python setup.py egg_info:\n    error in cryptography setup command: Invalid environment marker: platform_python_implementation != 'PyPy'\nAs I have no control over Scrapy's dependencies and I have been using Scrapy 1.4.0, latest release, alongwith Python 3, I am not sure what are my options now?\nFollowing is the command I use to install the dependencies\npip3 install -r requirements.txt\nFollowing is what my requirements.txt looks like\nlxml==3.8.0\npsycopg2==2.7.3.1\nScrapy==1.4.0\nTwisted==17.5.0\nvalidators==0.12.0\ntweepy==3.5.0\nI also posted the same on Stackoverflow.", "issue_status": "Closed", "issue_reporting_time": "2017-12-12T06:41:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "484": {"issue_url": "https://github.com/scrapy/scrapy/issues/3036", "issue_id": "#3036", "issue_summary": "File downloaded by pipeline are blank", "issue_description": "exotfboy commented on Dec 12, 2017\nI have deployed a spider in my remote server, and I am using FilePipeline to download images for item, however I found that the downloaded image have size of zero, which is not expected.\nThen I test the project in my local machine, it worked. So I think maybe my remote server has been banned, however when I tried wget .. I can download the image normally, and I also tried scrapy sheel image_src it still work.\nNow I have no idea what's going on , I just want to intercept the response of the request re-sent by the pipeline, is it possible?", "issue_status": "Closed", "issue_reporting_time": "2017-12-12T02:38:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "485": {"issue_url": "https://github.com/scrapy/scrapy/issues/3035", "issue_id": "#3035", "issue_summary": "Python3.3 support and requirements without it", "issue_description": "Member\nDigenis commented on Dec 11, 2017 \u2022\nedited\nScrapy still supports py3.3 (at least according to its trove classifiers in setup.py and the CI conf)\nbut some of its dependencies dropped support some time ago.\nhttps://github.com/pyca/service_identity/blob/master/CHANGELOG.rst#backward-incompatible-changes-1\nhttps://github.com/pyca/cryptography/blob/master/CHANGELOG.rst#20---2017-07-17\nThis caused some problems when testing scrapy daemon with py3.3,\nwhich was resolved by installing the enum-compat virtual package\nThere are several options here.\nscrapy1.5 can drop support for python3.3,\nscrapy1.4 can restrict the max versions for those dependencies\nand enum-compat can become a requirement,\nalthough there may be more things broken.\nI didn't figure out why the python3.3 build for scrapy doesn't fail\nbut here is a failed scrapyd build https://travis-ci.org/scrapy/scrapyd/jobs/299029712", "issue_status": "Closed", "issue_reporting_time": "2017-12-11T17:06:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "486": {"issue_url": "https://github.com/scrapy/scrapy/issues/3034", "issue_id": "#3034", "issue_summary": "Line-ends (unnecessary blank lines) problem in CSV export on Windows", "issue_description": "Contributor\nReLLL commented on Dec 9, 2017\nCSV export on Windows create unnecessary blank lines after each line.\nYou can fix the problem just by adding\nnewline=''\nas parameter to io.TextIOWrapper in the init method of the CsvItemExporter class in scrapy.exporters\nDetails are over here:\nhttps://stackoverflow.com/questions/39477662/scrapy-csv-file-has-uniform-empty-rows/43394566#43394566\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-12-09T13:03:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "487": {"issue_url": "https://github.com/scrapy/scrapy/issues/3033", "issue_id": "#3033", "issue_summary": "Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\esmer\\AppData\\Local\\Temp\\pip-build-spxq918q\\Twisted\\", "issue_description": "aldaaga commented on Dec 8, 2017\nHi,\nI'm trying to install to install Scrapy on Python 3.6.3 and I already installed Anaconda.\nI'm receiving an error as below:\nCommand \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\esmer\\AppData\\Local\\Temp\\pip-build-spxq918q\\Twisted\\\nI'm not sure what's the problem as didn't do anything rather than that.\nI have had installed and uninstalled Python, Anaconda & Scrapy though.\nDoes anyone have any clue? Thank you!", "issue_status": "Closed", "issue_reporting_time": "2017-12-07T20:36:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "488": {"issue_url": "https://github.com/scrapy/scrapy/issues/3032", "issue_id": "#3032", "issue_summary": "Exception raises on scrapy shell.", "issue_description": "0xfede7c8 commented on Dec 8, 2017\nRunning:\nscrapy shell \"https://pcsupport.lenovo.com/ar/en/\"\nRises:\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 149, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 89, in _run_print_help\nfunc(*a, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 156, in _run_command\ncmd.run(args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/shell.py\", line 73, in run\nshell.start(url=url, redirect=not opts.no_redirect)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 48, in start\nself.fetch(url, spider, redirect=redirect)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 115, in fetch\nreactor, self._schedule, request, spider)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\nresult.raiseException()\nFile \"\", line 2, in raiseException\nAttributeError: 'module' object has no attribute 'OP_SINGLE_ECDH_USE'\nI can help fixing it if you can give me some insight on what can be happening. Thanks.", "issue_status": "Closed", "issue_reporting_time": "2017-12-07T18:44:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "489": {"issue_url": "https://github.com/scrapy/scrapy/issues/3031", "issue_id": "#3031", "issue_summary": "unknown key error !", "issue_description": "Masoudtfn commented on Dec 6, 2017 \u2022\nedited\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2017-12-06T08:05:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "490": {"issue_url": "https://github.com/scrapy/scrapy/issues/3029", "issue_id": "#3029", "issue_summary": "Invalid DNS pattern", "issue_description": "cp2587 commented on Dec 3, 2017\nHello,\nWe are using https proxies to crawl some website and sometimes i get the following stack trace:\nError during info_callback\nTraceback (most recent call last):\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 315, in dataReceived\n    self._checkHandshakeStatus()\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 235, in _checkHandshakeStatus\n    self._tlsConnection.do_handshake()\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1442, in do_handshake\n    result = _lib.SSL_do_handshake(self._ssl)\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 933, in wrapper\n    callback(Connection._reverse_mapping[ssl], where, return_code)\n--- <exception caught here> ---\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1102, in infoCallback\n    return wrapped(connection, where, ret)\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/scrapy/core/downloader/tls.py\", line 67, in _identityVerifyingInfoCallback\n    verifyHostname(connection, self._hostnameASCII)\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 44, in verify_hostname\n    cert_patterns=extract_ids(connection.get_peer_certificate()),\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/pyopenssl.py\", line 73, in extract_ids\n    ids.append(DNSPattern(n.getComponent().asOctets()))\n  File \"/home/wirebot/.virtualenvs/cayzn_tracking.scraping/local/lib/python2.7/site-packages/service_identity/_common.py\", line 156, in __init__\n    \"Invalid DNS pattern {0!r}.\".format(pattern)\nservice_identity.exceptions.CertificateError: Invalid DNS pattern '194.167.13.105'.\nI think this issue is somewhat similar to #2092 and this 'invalid DNS pattern' error should be caught similarly as the 'Invalid DNS-ID'. What do you think ?\nIn the meantime, how can i catch it myself and silent it ?", "issue_status": "Closed", "issue_reporting_time": "2017-12-03T15:33:18Z", "fixed_by": "#3166", "pull_request_summary": "catch CertificateError in tls verification", "pull_request_description": "Member\nlucywang000 commented on Mar 12, 2018 \u2022\nedited\nThis should fix #3029 (downlader errors when certificate is issued for ips instead of domains)", "pull_request_status": "Merged", "issue_fixed_time": "2018-03-14T14:47:58Z", "files_changed": [["15", "scrapy/core/downloader/tls.py"], ["21", "tests/keys/localhost-ip.gen.README"], ["20", "tests/keys/localhost.ip.crt"], ["28", "tests/keys/localhost.ip.key"], ["13", "tests/test_downloader_handlers.py"]]}, "491": {"issue_url": "https://github.com/scrapy/scrapy/issues/3022", "issue_id": "#3022", "issue_summary": "When i'm using proxy to scrape - sometimes i got timeouterror", "issue_description": "AndrewMishchenko commented on Nov 28, 2017 \u2022\nedited\nI'm trying to scrape one https site. Also i build spider for it and the problem is next:\nIt is work perfect without proxy. It scrape 100% pages and very fast.\nWhen i'm trying to scrape through proxy(crawlera and etc) through time it raise timeout(at all works not stable) and i don't know what to do with this.\nHeders i'm using:\n'Host': '.....,\n'Connection': 'keep-alive',\n'Pragma': 'no-cache',\n'Cache-Control': 'no-cache',\n'User-Agent': 'Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)',\n'DNT': '1',\n'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8',\n'Accept-Encoding': 'gzip, deflate, br',\n'Accept-Language': 'en-US,en;q=0.8',\n'Upgrade-Insecure-Requests': '1',\n'X-Requested-With': 'XMLHttpRequest',\n'Referer': '....',\nAlso i was trying to add 'DOWNLOADER_CLIENT_TLS_METHOD': 'TLSv1.2' - it doesn't help!\nand write something like:\nclass SClientContextFactory(ScrapyClientContextFactory):\ndef init(self, method=SSL.TLSv1_2_METHOD):\nself.method = method\nI'm 100 percent sure that the problem is not in the proxy\nCan someone help me?", "issue_status": "Closed", "issue_reporting_time": "2017-11-28T11:44:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "492": {"issue_url": "https://github.com/scrapy/scrapy/issues/3021", "issue_id": "#3021", "issue_summary": "lxml error after upgrading to scrapy 1.4.0 (python 3.6)", "issue_description": "Beenhakker commented on Nov 25, 2017 \u2022\nedited\nHi,\nSince I have upgraded to scrapy 1.4.0 today, I get lxml import errors on Ubuntu 16.10 with Python 3.6:\nroot@5d52e8462fc2:/# scrapy\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 7, in\nfrom scrapy.cmdline import execute\nFile \"/usr/local/lib/python3.6/dist-packages/scrapy/init.py\", line 34, in\nfrom scrapy.spiders import Spider\nFile \"/usr/local/lib/python3.6/dist-packages/scrapy/spiders/init.py\", line 10, in\nfrom scrapy.http import Request\nFile \"/usr/local/lib/python3.6/dist-packages/scrapy/http/init.py\", line 11, in\nfrom scrapy.http.request.form import FormRequest\nFile \"/usr/local/lib/python3.6/dist-packages/scrapy/http/request/form.py\", line 11, in\nimport lxml.html\nFile \"/usr/local/lib/python3.6/dist-packages/lxml/html/init.py\", line 54, in\nfrom .. import etree\nImportError: /usr/local/lib/python3.6/dist-packages/lxml/etree.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _PyErr_FormatFromCause", "issue_status": "Closed", "issue_reporting_time": "2017-11-24T19:27:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "493": {"issue_url": "https://github.com/scrapy/scrapy/issues/3019", "issue_id": "#3019", "issue_summary": "from_crawler not documented in Spider/Downloader Middleware", "issue_description": "Jesse-Bakker commented on Nov 23, 2017\nThe from_crawler class method is not documented in the class documentation for Spider- and DowloaderMiddleware, which makes it slightly difficult to find unless you know what you're looking for. I propose adding this, including the description in the Item Pipeline class docs.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-11-23T14:16:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "494": {"issue_url": "https://github.com/scrapy/scrapy/issues/3018", "issue_id": "#3018", "issue_summary": "Is RetryMiddleware rescheduling failed requests?", "issue_description": "flaiming commented on Nov 23, 2017 \u2022\nedited\nIn RetryMiddleware docs there is line\nFailed pages are collected on the scraping process and rescheduled at the end, once the spider has finished crawling all regular (non failed) pages.\nBut I don't see anything like it in code https://github.com/scrapy/scrapy/blob/master/scrapy/downloadermiddlewares/retry.py when there is for example TimeoutError exception raised more than max_retry_times. It just throws request away, which is not what docs are saying. Is it a bug or mistake in docs?", "issue_status": "Closed", "issue_reporting_time": "2017-11-23T10:47:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "495": {"issue_url": "https://github.com/scrapy/scrapy/issues/3015", "issue_id": "#3015", "issue_summary": "How to use scrapy to crawl angularjs websites?", "issue_description": "emy-lee commented on Nov 21, 2017\nI need a way to get ALL odds of ALL events of this bookmaker: https://www.snai.it/sport\nI am using Scrapy+Splash to get the first javascript-loaded content of the site. But to get all other odds, I have to click \"Spagna-LigaSpagnola\", \"Italia->Serie A\", etc.\nHow can I do that ?", "issue_status": "Closed", "issue_reporting_time": "2017-11-21T13:57:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "496": {"issue_url": "https://github.com/scrapy/scrapy/issues/3014", "issue_id": "#3014", "issue_summary": "scrapy Failed to establish a new connection: [Errno 111] Connection refused',))", "issue_description": "guangbaowan commented on Nov 20, 2017 \u2022\nedited\nHi, We are deploying the scrapy on docker (scrapyd (1.1.0), scrapyd-client (1.0.1))\nwe restart(cronjob) the job by send message to 'http://0.0.0.0:6800/cancel.json' and got\nTraceback (most recent call last):\nFile \"stop_spiders.py\", line 31, in\nmain()\nFile \"stop_spiders.py\", line 8, in main\njobs = requests.get(job_url)\nFile \"/usr/local/lib/python2.7/site-packages/requests/api.py\", line 67, in get\nreturn request('get', url, params=params, **kwargs)\nFile \"/usr/local/lib/python2.7/site-packages/requests/api.py\", line 53, in request\nreturn session.request(method=method, url=url, **kwargs)\nFile \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\nresp = self.send(prep, **send_kwargs)\nFile \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\nr = adapter.send(request, **kwargs)\nFile \"/usr/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\nraise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='0.0.0.0', port=6800): Max retries exceeded with url: /listjobs.json?project=chaos (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7fa3b8b71b10>: Failed to establish a new connection: [Errno 111] Connection refused',))", "issue_status": "Closed", "issue_reporting_time": "2017-11-20T04:53:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "497": {"issue_url": "https://github.com/scrapy/scrapy/issues/3010", "issue_id": "#3010", "issue_summary": "Multiple Item exporters in ItemPipeline, only the first one work", "issue_description": "Textcat commented on Nov 18, 2017\nimport re\nimport os\nfrom scrapy import signals\nfrom scrapy.exporters import CsvItemExporter\n\nclass multipleOutputTestPipeline(object):\n    def __init__(self):\n        self.exporters = {}\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        pipeline = cls()\n        crawler.signals.connect(pipeline.spider_opened, signals.spider_opened)\n        crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)\n        return pipeline\n\n    def spider_opened(self, spider):\n        pass\n\n    def spider_closed(self, spider):\n        for exporter in self.exporters.values():\n            exporter.finish_exporting()\n    def process_item(self, item, spider):\n        #if the exporter already exists, export the item directly\n        if item[\"start_url\"] in self.exporters:\n            self.exporters[item[\"start_url\"]].export_item(item)\n        else:\n            #if the exporter doesn't exist, create one and start it\n            catname = item[\"start_url\"][-1] + \".csv\"\n            thefile = open('/Users/apple/Desktop/multipleFileOutputTest/' + catname, 'w+b')\n\n            self.exporters[item[\"start_url\"]] = CsvItemExporter(thefile)\n            self.exporters[item[\"start_url\"]].start_exporting()\n            self.exporters[item[\"start_url\"]].export_item(item)\n\n\n        return item\nI'm trying to export items to multiple CSV files, the names of these files are based on the start_url, the problem is that the exported files except the first one are empty.", "issue_status": "Closed", "issue_reporting_time": "2017-11-18T06:59:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "498": {"issue_url": "https://github.com/scrapy/scrapy/issues/3009", "issue_id": "#3009", "issue_summary": "Missing max_concurrent_requests in documentation", "issue_description": "ahivert commented on Nov 17, 2017\nI wanted to set CONCURRENT_REQUESTS to 1 but only for a spider to process request one by one with the correct order. But it's not worked when I defined it in spider.\nThen, I found an option that seems to work in my case : max_concurrent_requests (present in scrapy code but not in documentation).\nIs there Any reason why this option is not present in documentation ?\nIs there another solution to limit concurrent requests of a spider or a domain ?", "issue_status": "Closed", "issue_reporting_time": "2017-11-17T11:32:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "499": {"issue_url": "https://github.com/scrapy/scrapy/issues/3008", "issue_id": "#3008", "issue_summary": "trying to make a files and youtube downloader", "issue_description": "monikarazdan commented on Nov 17, 2017 \u2022\nedited by kmike\nhello, I'm new to python and trying to make files and youtube downloader but while making that I have a problem with setting the youtube URL by the user at runtime(plz someone kindly correct me where I'm wrong)\nfrom __future__ import unicode_literals\nimport youtube_dl\nfrom youtube_dl import YoutubeDL\nimport urllib\nimport shutil\nimport os \n\n\nu_input=int(input(\"enter url:\"))\nydl_opts = {}\nurl=(u_input)\nwith youtube_dl.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(url)\n    info_dict=ydl.extract_info(url[0], download=False)\n    video_title = info_dict.get('title', None)\n    print(video_title)\nprint(\"DONE!\")\nscript_dir = os.path.dirname(os.path.realpath(__file__))\nprint(script_dir+\"\\\\\" +video_title)](url)", "issue_status": "Closed", "issue_reporting_time": "2017-11-17T10:25:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "500": {"issue_url": "https://github.com/scrapy/scrapy/issues/3007", "issue_id": "#3007", "issue_summary": "How do I package the Scrapy project into exe?", "issue_description": "panjunbing commented on Nov 16, 2017\ni try to use pyinstaller,but failed", "issue_status": "Closed", "issue_reporting_time": "2017-11-16T12:03:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "501": {"issue_url": "https://github.com/scrapy/scrapy/issues/3006", "issue_id": "#3006", "issue_summary": "Allow the option to remove headers being title case or lower case.", "issue_description": "Contributor\nIAlwaysBeCoding commented on Nov 16, 2017\nSo, I spent the last 6 hours trying to get through a website only to find out the problem was with scrapy.\nThere is this website( I can't name the url) that checks for a special csrftoken in the header. Well to my surprise Scrapy has hard-coded the headers so it always titles them.\nSo my specific header is this one:\n\"X-csrftoken\" but scrapy titles it to X-Csrftoken so the website cannot detect it.\nWe should allow in a setting to be able to remove this bug/feature because at the moment the only way I can edit it is by going into Scrapy source code.", "issue_status": "Closed", "issue_reporting_time": "2017-11-16T02:33:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "502": {"issue_url": "https://github.com/scrapy/scrapy/issues/3005", "issue_id": "#3005", "issue_summary": "The behavior of parsing multiple redirect between Scrapy and requests is not the same", "issue_description": "kingname commented on Nov 15, 2017\nThere is a website. When I visit it, in Chrome I can find that It will follow 7 redirects before I get the destination page. between these redirects, the website will write some Cookies and Headers items into the Browser.\nIf I use requests to get the beginning page, everything is fine and I can get the source html of the destination page. However, when I use Scrapy, the website will find my spider and block me.\nI thought it must because there are some difference between requests and Scrapy in the behavior of redirect. Do you know what the difference really is?", "issue_status": "Closed", "issue_reporting_time": "2017-11-15T13:00:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "503": {"issue_url": "https://github.com/scrapy/scrapy/issues/3004", "issue_id": "#3004", "issue_summary": "Caching from a webdriver to scrapy cache", "issue_description": "mmx64bits commented on Nov 15, 2017\nAfter setting a download middleware that uses Selenium webdriver to get website content:\nclass SeleniumMiddleware(object):\nclass SeleniumMiddleware(object):\n    def process_request(self, request, spider):=\n        driver = webdriver.Chrome()\n        driver.get(request.url)\n        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'a')))\n\n        body = driver.page_source\n        url = driver.current_url\n        driver.quit()\n        return HtmlResponse(url, body=body, encoding='utf-8', request=request)\nwith the settings below on the spider\n    custom_settings = dict(\n        BOT_NAME=\"x1\"\n        , ROBOTSTXT_OBEY = False\n        , AUTOTHROTTLE_TARGET_CONCURRENCY=0.1\n        , HTTPCACHE_ENABLED=True\n        , HTTPCACHE_EXPIRATION_SECS=0\n        , HTTPCACHE_DIR='httpcache'\n        , HTTPCACHE_IGNORE_HTTP_CODES=[]\n        , HTTPCACHE_STORAGE='scrapy.extensions.httpcache.FilesystemCacheStorage'\n        , DOWNLOADER_MIDDLEWARES = {\n            'mySpider.middlewares.SeleniumMiddleware': 1000,\n        },\n    )\nThe spider works well and scrapy caches the web pages and yields the items:\n    # ...\n    def parse(self, response):\n        options = response.xpath(\"//select[@id='specialism']/option/@value\").extract()\n        for option in options:\n            if option != \"\":\n                option = option.replace(\" \", \"+\")\n\n                url = \"{}?searchTerm=&specialism={}\".format(response.url, option)\n                yield Request(url, self.parse_page)\n    # ...\nwhen relaunching the crawl from the cache an error is logged:\nraise NotSupported(\"Response content isn't text\") at line: response.xpath(...)\nwhen debugging the response is of type: <class 'scrapy.http.response.Response'>\nand the response.body displays correctly the html page\nGoing back to the cache, scrapy seems to store scraped web pages as raw HTML without compression. Whereas, it caches the same page as a binary gzipped file if we were using scrapy without the selenium middleware.<class 'scrapy.http.response.Response'>\nNote: i changed the middleware order from 534 to 1000 in order to be sure the HTTP cache middleware processes the webdriver response.\nCan't find any clue on solving this error", "issue_status": "Closed", "issue_reporting_time": "2017-11-15T09:32:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "504": {"issue_url": "https://github.com/scrapy/scrapy/issues/3003", "issue_id": "#3003", "issue_summary": "Unexpected behavior while yielding an item.", "issue_description": "abeagomez commented on Nov 15, 2017\nI have a .csv files and I need to fill some of my items with data from the .csv and some other with data from a list of URLs I will built using info from the csv. For an item to be send to the pipeline (and then written to the data base), seems like I need to yield it. I have been trying different approaches and so far, I don't know if this is a bug in Scrapy.\nIf I try yielding the items at every level, like this (commented lines means that I also tried that way):\ndef start_requests(self):\n          #yield self.parse_csv()\n          for item in self.parse_csv():\n              yield item\n\ndef parse_csv(self):\n        directory = os.path.join(os.path.dirname(__file__),\"example.csv\")\n        with open(directory) as data:\n            reader = csv.DictReader(data)\n            for row in reader:\n                #yield self.item_filler(row)\n                for item in self.item_filler(row):\n                    yield item\n\ndef item_filler(self, row, pin):\n        item = ExampleItem()\n        item['some_field'] = row['some_field']\n        yield prop_item\nI get the following exception:\n2017-11-14 13:10:40 [twisted] CRITICAL: Unhandled Error\nTraceback (most recent call last):\n  [.... ... .... tracebacks .... ..... ....]\n    raise AttributeError(name)\nexceptions.AttributeError: dont_filter\nIf I try without the \"yields\" in the start_request and the parse functions, like this:\ndef start_requests(self):\n          print(\"before parse\")\n          self.parse_csv()\n          print(\"after parse\")\n        \ndef parse_csv(self):\n        directory = os.path.join(os.path.dirname(__file__),\"example.csv\")\n        with open(directory) as data:\n            reader = csv.DictReader(data)\n            print(\"Before cycle\")\n            for row in reader:\n                self.item_filler(row)\n            print(\"After cycle\")\n\ndef item_filler(self, row, pin):\n        item = ExampleItem()\n        item['some_field'] = row['some_field']\n        print(\"yielding\")\n        yield prop_item\nI get something like:\n[...]\n2017-11-14 13:31:21 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\nbefore parse\nBefore cycle\nAfter cycle\nafter parse\n2017-11-14 13:31:26 [scrapy.core.engine] INFO: Closing spider (finished)\n[...]\nNo exceptions or errors, the item_filler function never was executed.\nI'm trying to figure out how to solve this issue. Is this the expected behavior? Is this a bug? Thanks in advance.", "issue_status": "Closed", "issue_reporting_time": "2017-11-14T18:43:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "505": {"issue_url": "https://github.com/scrapy/scrapy/issues/3002", "issue_id": "#3002", "issue_summary": "Error in scraping zomato.com", "issue_description": "paritosh16 commented on Nov 14, 2017\nWhen I run scrapy shell https://www.zomato.com, I get the following warning:\n:0: UserWarning: You do not have a working installation of the service_identity module: 'No \nmodule named cryptography.x509'.  Please install it from \n<https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are \nsatisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, \nTwisted can perform only rudimentary TLS client hostname verification.  Many valid \ncertificate/hostname mappings may be rejected.\nfollowed by another warning:\n2017-11-13 15:11:37 [scrapy.core.downloader.tls] WARNING: Remote certificate is not valid for \nhostname \"www.zomato.com\"; u'*.zomato.com'!=u'www.zomato.com'\nafter which it exits without scrapping any webpage. If I run the same command with https://github.com it works as it should.\nThe environment details are:\nMacOS - v10.13\nPython - v3.6\nScrapy - v1.4.0\nTwisted - 17.9.0\nservice_identity - v17.0.0\npyopnssl - 17.2.0\nI have kind of hit a dead-end and cannot debug this.\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2017-11-13T20:17:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "506": {"issue_url": "https://github.com/scrapy/scrapy/issues/2999", "issue_id": "#2999", "issue_summary": "mention dont_merge_cookies in CookiesMiddleware docs", "issue_description": "Member\nkmike commented on Nov 9, 2017\nCurrently dont_merge_cookies is only documented in https://doc.scrapy.org/en/latest/topics/request-response.html#request-meta-special-keys (in 'Request' docs actually). I think we should mention it in CookiesMiddleware docs as well.\n//cc @whalebot-helmsman", "issue_status": "Closed", "issue_reporting_time": "2017-11-09T11:18:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "507": {"issue_url": "https://github.com/scrapy/scrapy/issues/2998", "issue_id": "#2998", "issue_summary": "dont_merge_cookies docs are incomplete", "issue_description": "Member\nkmike commented on Nov 9, 2017 \u2022\nedited\ndont_merge_cookies docs are incomplete: they say that\nWhen some site returns cookies (in a response) those are stored in the cookies for that domain and will be sent again in future requests. That\u2019s the typical behaviour of any regular web browser. However, if, for some reason, you want to avoid merging with existing cookies you can instruct Scrapy to do so by setting the dont_merge_cookies key to True in the Request.meta.\nBut this flag not only prevents merging of cookies, but also prevents sending of them:\nscrapy/scrapy/downloadermiddlewares/cookies.py\nLine 27 in b8870ee\n def process_request(self, request, spider): \nMaybe a separate issue, but it'd be nice to have separate flags for sending and merging of cookies.\n//cc @whalebot-helmsman\n\ud83d\udc4d 3\n\ud83d\udc40 1", "issue_status": "Closed", "issue_reporting_time": "2017-11-09T11:17:08Z", "fixed_by": "#4028", "pull_request_summary": "Clarify the effects of dont_merge_cookies", "pull_request_description": "Member\nGallaecio commented on Sep 19, 2019\nFixes #2998", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-20T13:51:37Z", "files_changed": [["22", "docs/topics/request-response.rst"]]}, "508": {"issue_url": "https://github.com/scrapy/scrapy/issues/2997", "issue_id": "#2997", "issue_summary": "filepipeline handle urls with \"?\" wrong", "issue_description": "Contributor\nNewUserHa commented on Nov 9, 2017\nlike http://www.hebpr.cn/WebbuilderMIS/attach/attachdown.jspx?attachGuid=c986d80b-4d9d-4a50-950f-fea61b828dda&dataSourceName=EpointHbztbCMS\nand how can I only replace file_path method of FilesPipeline to make my custom pipeline work?\nI googled many times but no answer\nthanks", "issue_status": "Closed", "issue_reporting_time": "2017-11-08T22:01:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "509": {"issue_url": "https://github.com/scrapy/scrapy/issues/2994", "issue_id": "#2994", "issue_summary": "scrapy FormRequest ignore Content-Type other values", "issue_description": "andywubit commented on Nov 8, 2017\nThe following code\nsuper(FormRequest, self).__init__(*args, **kwargs)\nif formdata:\n    items = formdata.items() if isinstance(formdata, dict) else formdata\n    querystr = _urlencode(items, self.encoding)\n    if self.method == 'POST':\n        self.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\n        self._set_body(querystr)\n    else:\n        self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)\nAlthough the FormRequest base class Request sets the request header based on the value of the headers band, such as setting Content-Type = \"application / json; charset = utf-8\", the code\n self.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\nsets the Content- Type value back to \"application / x -www-form-urlencoded \". He did not consider headers own Content-Type situation.", "issue_status": "Closed", "issue_reporting_time": "2017-11-08T09:29:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "510": {"issue_url": "https://github.com/scrapy/scrapy/issues/2993", "issue_id": "#2993", "issue_summary": "Scrapy Selector ignored elements", "issue_description": "ghost commented on Nov 8, 2017 \u2022\nedited by ghost\nProblem\nBoth response.css and response.xpath ignored a lot of element after a special div element\nHow to re-trigger the problem\nscrapy shell \"https://redsea.com/en/apple-iphone-x-64gb-silver.html\"\nresponse.css(\".page-title\") or response.xpath(\"\"\"/html/body/div[2]/div[2]\"\"\") both returns an empty array\nWhat I found is wrong\nI tried to extract the body element using response.css(\"body\")\nThis is what it extracted\n* This is the original html\n* Where had the elements below the highlighted line gone?\nThis question is originated from stackoverflow\nOriginal Question", "issue_status": "Closed", "issue_reporting_time": "2017-11-07T21:05:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "511": {"issue_url": "https://github.com/scrapy/scrapy/issues/2992", "issue_id": "#2992", "issue_summary": "Why scrapy stats 's time zone is not consistent with log's time", "issue_description": "syncml commented on Nov 3, 2017\nhi,\nWhy scrapy stats 's time zone is not consistent with log's time.\nThere are any variable to set time zone?\nthanks\n2017-11-03 13:56:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n......\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2017, 11, 3, 5, 56, 35, 981583),\n....\n 'start_time': datetime.datetime(2017, 11, 3, 5, 44, 10, 93653)}\n2017-11-03 13:56:35 [scrapy.core.engine] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2017-11-03T06:42:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "512": {"issue_url": "https://github.com/scrapy/scrapy/issues/2990", "issue_id": "#2990", "issue_summary": "pypy tests fail on CI", "issue_description": "Member\nkmike commented on Nov 1, 2017\nIt seems pyenv changed something again, so pypy installation fails; all recent PRs are red. //cc @lopuhin :)", "issue_status": "Closed", "issue_reporting_time": "2017-10-31T21:56:57Z", "fixed_by": "#2991", "pull_request_summary": "Fix PyPy build", "pull_request_description": "Member\nlopuhin commented on Nov 1, 2017 \u2022\nedited\nFixes #2990\nWe'll see if it fixes it.", "pull_request_status": "Merged", "issue_fixed_time": "2017-11-02T13:59:21Z", "files_changed": [["14", ".travis.yml"]]}, "513": {"issue_url": "https://github.com/scrapy/scrapy/issues/2987", "issue_id": "#2987", "issue_summary": "Scrapy response status lines without a Reason phrase", "issue_description": "jiayunyan commented on Oct 31, 2017\nC:\\Users\\PycharmProjects\\untitled\\first>scrapy crawl yjy\n2017-10-31 22:12:22 [scrapy] INFO: Scrapy 1.1.0rc3 started (bot: first)\n2017-10-31 22:12:22 [scrapy] INFO: Overridden settings: {'AUTOTHROTTLE_ENABLED': True, 'BOT_NAME': 'first', 'CONCURRENT_REQUESTS_PER_DOMAIN': 16, 'CONCURRENT_REQUESTS_PER_IP': 16, 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'first.spiders', 'SPIDER_MODULES': ['first.spiders'], 'TELNETCONSOLE_ENABLED': False, 'USER_AGENT': 'first (+http://www.baidu.com)'}\n2017-10-31 22:12:23 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.logstats.LogStats',\n'scrapy.extensions.throttle.AutoThrottle']\n2017-10-31 22:12:25 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-10-31 22:12:25 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-10-31 22:12:25 [scrapy] INFO: Enabled item pipelines:\n['first.pipelines.FirstPipeline']\n2017-10-31 22:12:25 [scrapy] INFO: Spider opened\n2017-10-31 22:12:25 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-10-31 22:12:25 [scrapy] ERROR: Error downloading <GET http://www.baidu.com/>\nTraceback (most recent call last):\nFile \"e:\\python\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1384, in inlineCallbacks\nresult = result.throwExceptionIntoGenerator(g)\nFile \"e:\\python\\lib\\site-packages\\twisted\\python\\failure.py\", line 408, in throwExceptionIntoGenerator\nreturn g.throw(self.type, self.value, self.tb)\nFile \"e:\\python\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\ndefer.returnValue((yield download_func(request=request,spider=spider)))\nFile \"e:\\python\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 45, in mustbe_deferred\nresult = f(*args, **kw)\nFile \"e:\\python\\lib\\site-packages\\scrapy\\core\\downloader\\handlers_init.py\", line 65, in download_request\nreturn handler.download_request(request, spider)\nFile \"e:\\python\\lib\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 60, in download_request\nreturn agent.download_request(request)\nFile \"e:\\python\\lib\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 264, in download_request\nmethod, to_bytes(url, encoding='ascii'), headers, bodyproducer)\nFile \"e:\\python\\lib\\site-packages\\twisted\\web\\client.py\", line 1655, in request\nparsedURI.originForm)\nFile \"e:\\python\\lib\\site-packages\\twisted\\web\\client.py\", line 1432, in _requestWithEndpoint\nd = self._pool.getConnection(key, endpoint)\nFile \"e:\\python\\lib\\site-packages\\twisted\\web\\client.py\", line 1318, in getConnection\nreturn self._newConnection(key, endpoint)\nFile \"e:\\python\\lib\\site-packages\\twisted\\web\\client.py\", line 1330, in _newConnection\nreturn endpoint.connect(factory)\nFile \"e:\\python\\lib\\site-packages\\twisted\\internet\\endpoints.py\", line 903, in connect\nEndpointReceiver, self._hostText, portNumber=self._port\nFile \"e:\\python\\lib\\site-packages\\twisted\\internet_resolver.py\", line 189, in resolveHostName\nonAddress = self._simpleResolver.getHostByName(hostName)\nFile \"e:\\python\\lib\\site-packages\\scrapy\\resolver.py\", line 21, in getHostByName\nd = super(CachingThreadedResolver, self).getHostByName(name, timeout)\nFile \"e:\\python\\lib\\site-packages\\twisted\\internet\\base.py\", line 276, in getHostByName\ntimeoutDelay = sum(timeout)\nTypeError: 'float' object is not iterable\n2017-10-31 22:12:25 [scrapy] INFO: Closing spider (finished)\n2017-10-31 22:12:25 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 1,\n'downloader/exception_type_count/builtins.TypeError': 1,\n'downloader/request_bytes': 207,\n'downloader/request_count': 1,\n'downloader/request_method_count/GET': 1,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2017, 10, 31, 14, 12, 25, 541215),\n'log_count/ERROR': 1,\n'log_count/INFO': 7,\n'scheduler/dequeued': 1,\n'scheduler/dequeued/memory': 1,\n'scheduler/enqueued': 1,\n'scheduler/enqueued/memory': 1,\n'start_time': datetime.datetime(2017, 10, 31, 14, 12, 25, 126226)}\n2017-10-31 22:12:25 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2017-10-31T14:16:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "514": {"issue_url": "https://github.com/scrapy/scrapy/issues/2981", "issue_id": "#2981", "issue_summary": "Long time response processing cause other concurrent requests timeout", "issue_description": "tobads commented on Oct 28, 2017\nI want to obtain some data from a website, so I wrote a Scrapy project\nCode below:\nsettings.py\nCONCURRENT_REQUESTS = 4\nDOWNLOAD_TIMEOUT = 5\nTestSpiders.py\nimport scrapy\nfrom datetime import datetime\nfrom scrapy.linkextractors import LinkExtractor\n\nclass TestSpider(scrapy.Spider):\n    name = \"Test\"\n    allowed_domains = [\"scrapy.org\"]\n    start_urls = [\"https://doc.scrapy.org\"]\n\n    def parse(self, response):\n        time_start = datetime.now()\n        \n        links = LinkExtractor(allow=()).extract_links(response)\n        for link in links:\n            yield scrapy.http.Request(url = link.url)\n            \n        print \"parse method time consumed: \", (datetime.now() - time_start).total_seconds(), \"seconds\\n\"\nOutput\n2017-10-28 16:50:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/1.4/topics/signals.html> (referer: https://doc.scrapy.org/en/1.4/intro/overview.html)\n2017-10-28 16:50:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/latest/_static/selectors-sample1.html> (referer: None)\nparse method time consumed:  0.106 seconds\n\n2017-10-28 16:50:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/1.4/news.html> (referer: https://doc.scrapy.org/en/1.4/intro/overview.html)\n2017-10-28 16:50:29 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite request to 'example.com': <GET http://example.com/image1.html>\nparse method time consumed:  0.003 seconds\n\nparse method time consumed:  0.072 seconds\n\nparse method time consumed:  0.357 seconds\n\n2017-10-28 16:50:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.scrapy.org/en/master/genindex.html> (referer: https://docs.scrapy.org/en/master/)\n2017-10-28 16:50:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/1.4/topics/djangoitem.html#topics-djangoitem> (referer: https://doc.scrapy.org/en/1.4/news.html)\nparse method time consumed:  0.044 seconds\n\nparse method time consumed:  0.347 seconds\n\n2017-10-28 16:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.scrapy.org/en/master/versioning.html> (referer: https://docs.scrapy.org/en/master/)\nparse method time consumed:  0.051 seconds\n\n2017-10-28 16:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.scrapy.org/en/master/py-modindex.html> (referer: https://docs.scrapy.org/en/master/)\nparse method time consumed:  0.073 seconds\nAs you can see, everything is fine.\nBut now I want to improve this project, not only obtaining data, but also parsing it, storing it, using it.\nAnyway, it's a time-consuming job.\nSo TestSpiders.py need to be changed a little bit\nTestSpiders.py\nimport time\nimport scrapy\nfrom datetime import datetime\nfrom scrapy.linkextractors import LinkExtractor\n\nclass TestSpider(scrapy.Spider):\n    name = \"Test\"\n    allowed_domains = [\"scrapy.org\"]\n    start_urls = [\"https://doc.scrapy.org\"]\n\n    def parse(self, response):\n        time_start = datetime.now()\n        \n        links = LinkExtractor(allow=()).extract_links(response)\n        for link in links:\n            yield scrapy.http.Request(url = link.url)\n            \n        time.sleep(10) #simulating time-consuming job\n        print \"parse method time consumed: \", (datetime.now() - time_start).total_seconds(), \"seconds\\n\"\nOutput\n2017-10-28 17:00:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/latest/> (referer: https://doc.scrapy.org/en/latest/)\n2017-10-28 17:00:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/latest/topics/webservice.html> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/latest/topics/webservice.html took longer than 5.0 seconds..\n2017-10-28 17:00:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/latest/topics/telnetconsole.html> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/latest/topics/telnetconsole.html took longer than 5.0 seconds..\n2017-10-28 17:00:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/latest/topics/email.html> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/latest/topics/email.html took longer than 5.0 seconds..\nparse method time consumed:  10.071 seconds\n\n2017-10-28 17:00:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/master/> (referer: https://doc.scrapy.org/en/latest/)\n2017-10-28 17:00:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/xpath-tutorial/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/xpath-tutorial/ took longer than 5.0 seconds..\n2017-10-28 17:00:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.9/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.9/ took longer than 5.0 seconds..\n2017-10-28 17:00:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.10.3/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.10.3/ took longer than 5.0 seconds..\nparse method time consumed:  10.054 seconds\n\n2017-10-28 17:00:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/0.12/> (referer: https://doc.scrapy.org/en/latest/)\n2017-10-28 17:00:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.14/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.14/ took longer than 5.0 seconds..\n2017-10-28 17:00:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.16/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.16/ took longer than 5.0 seconds..\n2017-10-28 17:00:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.18/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.18/ took longer than 5.0 seconds..\nparse method time consumed:  10.028 seconds\nOops, so many Retrying.\nYou can easily find the pattern, always a Crawled and then 3 Retrying.\nWhy?\nBecause 3 + 1 = 4, and CONCURRENT_REQUESTS = 4 in settings.py\nSo when spider processing a response, other 3 concurrent_requests are timeout.\nSo, I am curious, while spider processing a response, what other concurrent_requests are doing?\nAre they still receive data from server? If so, when all data was received, they should generate a response, and wait for spider to finish processing current response, and then send response to spider. In this case, timeout shouldn't happen.\nIf they are not receive data from server? Why not?", "issue_status": "Closed", "issue_reporting_time": "2017-10-28T09:28:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "515": {"issue_url": "https://github.com/scrapy/scrapy/issues/2980", "issue_id": "#2980", "issue_summary": "How to clear all data which come from website and reset spider?", "issue_description": "kingname commented on Oct 27, 2017 \u2022\nedited\nI am crawling 100 urls in scrapy. I have found that if I start scrapy - crawl 1 url - kill scrapy, then I can crawl every url in a short time. However, if I crawl all the 100 url in the same scrapy process, then the website will find me and refuse me.\nTherefore, the website must write some datas to my spider, so the latter request will fail. Is there some method to reset the status of spider just like restart the scrapy process? So that I can reset the status of spider as soon as I finish crawling every url.", "issue_status": "Closed", "issue_reporting_time": "2017-10-27T09:42:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "516": {"issue_url": "https://github.com/scrapy/scrapy/issues/2979", "issue_id": "#2979", "issue_summary": "Page not found!!! \"\"\"http://doc.scrapy.org/_static/selectors-sample1.html\"\"\"\"", "issue_description": "omi10859 commented on Oct 27, 2017\nExample page under Using selectors with XPath link taken for example \"(http://doc.scrapy.org/_static/selectors-sample1.html)\" is not working", "issue_status": "Closed", "issue_reporting_time": "2017-10-27T04:33:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "517": {"issue_url": "https://github.com/scrapy/scrapy/issues/2975", "issue_id": "#2975", "issue_summary": "Issue with running scrapy spider from script.", "issue_description": "mahecode commented on Oct 25, 2017 \u2022\nedited by kmike\nHi ! I am trying to run this code\nimport csv\nimport scrapy\nclass SouqSpider(scrapy.Spider):\n    name = \"Souq\"  # Name of the Spider, required value\n    start_urls = [\"http://deals.souq.com/ae-en/\"]  # The starting url, Scrapy will request this URL in parse\n\n    # Entry point for the spider\n    def parse(self, response):\n        for href in response.css('.sp-cms_unit--ttlattr(href)'):\n            url = href.extract()\n            yield scrapy.Request(url, callback=self.parse_item)\n\n    # Method for parsing a product page\n    def parse_item(self, response):\n        original_price = -1\n        savings=0\n        discounted = False\n        seller_rating = response.css('.vip-product-infoats .inline-block small::text'). extract()[0]\n        seller_rating = int(filter(unicode.isdigit,seller_rating))\n\n        # Not all deals are discounted\n        if response.css('.vip-product-infobhead::text').extract():\n            original_price = response.css('.vip-product-infobhead::text').extract()[0].replace(\"AED\", \"\")\n            discounted = True\n            savings = response.css('.vip-product-infossage .noWrap::text').extract()[0].replace(\"AED\", \"\")\n        yield {\n            'Title': response.css('.product-title:text').extract()[0],\n            'Category': response.css('.product-title span a+ a::text').extract()[0],\n            'OriginalPrice': original_price,\n            'CurrentPrice': response.css('.vip-product-infoice::text').extract()[0].replace(u\"\\xa0\", \"\"),\n            'Discounted': discounted,\n            'Savings': savings,\n            'SoldBy': response.css('.vip-product-infoats a::text').extract()[0],\n            'SellerRating': seller_rating,\n            'Url': response.url\n        }\nHowever , i run this script i get this following error\nclass SouqSpider(scrapy.Spider): AttributeError: 'module' object has no attribute 'Spider'\nanyone who can fix this error", "issue_status": "Closed", "issue_reporting_time": "2017-10-25T14:24:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "518": {"issue_url": "https://github.com/scrapy/scrapy/issues/2974", "issue_id": "#2974", "issue_summary": "can't support string.format()", "issue_description": "3114318094 commented on Oct 24, 2017 \u2022\nedited\ncan't support string.format()\nlike this:\nstart_urls=[f'{i}/{j}' for i in range(1,10) for j in range(1, 10)]\nbut it work well in python3.6 or ipython,python3.5 failed.\nThis is a new feature in python3.6.\nenvironment: python3.6 scrapy-1.4.0", "issue_status": "Closed", "issue_reporting_time": "2017-10-24T12:56:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "519": {"issue_url": "https://github.com/scrapy/scrapy/issues/2971", "issue_id": "#2971", "issue_summary": "ImportError: cannot import name '_unquotepath'", "issue_description": "younesherlock commented on Oct 21, 2017\nHello,\nWhen I run scrapy shell 'https://www.google.com', here's what I get:\n`\nTraceback (most recent call last):\nFile \"/usr/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\nFile \"/usr/lib64/python3.4/site-packages/scrapy/__init__.py\", line 34, in <module>\n    from scrapy.spiders import Spider\nFile \"/usr/lib64/python3.4/site-packages/scrapy/spiders/__init__.py\", line 10, in <module>\n    from scrapy.http import Request\nFile \"/usr/lib64/python3.4/site-packages/scrapy/http/__init__.py\", line 10, in <module>\n    from scrapy.http.request import Request\nFile \"/usr/lib64/python3.4/site-packages/scrapy/http/request/__init__.py\", line 13, in <module>\n    from scrapy.utils.url import escape_ajax\nFile \"/usr/lib64/python3.4/site-packages/scrapy/utils/url.py\", line 15, in <module>\n    from w3lib.url import _safe_chars, _unquotepath\nImportError: cannot import name '_unquotepath'\n`\nAnd here are some information about my packages:\n`\nlinux-hcjz:/home/user/Projects/books # pip show scrapy\nName: Scrapy\nVersion: 1.4.0\nSummary: A high-level Web Crawling and Web Scraping framework\nHome-page: http://scrapy.org\nAuthor: Pablo Hoffman\nAuthor-email: pablo@pablohoffman.com\nLicense: BSD\nLocation: /usr/lib64/python3.4/site-packages\nRequires: pyOpenSSL, lxml, queuelib, PyDispatcher, service-identity, Twisted, cssselect, six, w3lib, parsel\nlinux-hcjz:/home/user/Projects/books # pip freeze scrapy\napparmor==2.10.2\nasn1crypto==0.23.0\nattrs==17.2.0\nAutomat==0.6.0\nbottle==0.12.8\ncertifi==2017.7.27.1\ncffi==1.11.2\nclick==6.7\nconstantly==15.1.0\ncoverage==4.3.4\ncryptography==2.1.1\ncssselect==1.0.1\ncupshelpers==1.0\ndaemonocle==1.0.1\ndecorator==4.1.2\nhyperlink==17.3.1\nidna==2.6\nincremental==17.5.0\nipython==6.2.1\nipython-genutils==0.2.0\njedi==0.11.0\nLibAppArmor==2.10.2\nlxml==4.1.0\nnose==1.3.7\nolefile==0.44\nonedrive-d==1.1.0.dev0\npackaging==16.8\nparsel==1.2.0\nparso==0.1.0\npexpect==4.2.1\npickleshare==0.7.4\nPillow==4.2.1\nprompt-toolkit==1.0.15\npsutil==5.4.0\nptyprocess==0.5.2\npy==1.4.30\npyasn1==0.3.7\npyasn1-modules==0.1.5\npycparser==2.18\npycrypto==2.6.1\npycups==1.9.72\npycurl==7.19.5.1\nPyDispatcher==2.0.5\nPygments==2.2.0\npygobject==3.20.1\npyOpenSSL==17.3.0\npyparsing==2.2.0\npyserial==3.4\npysmbc==1.0.15.4\nqueuelib==1.4.2\nrequests==2.7.0\nScrapy==1.4.0\nSend2Trash==1.4.1\nservice-identity==17.0.0\nsimplegeneric==0.8.1\nsimplejson==3.8.0\nsix==1.11.0\ntraitlets==4.3.2\nTwisted==17.9.0\ntyping==3.6.2\nurllib3==1.22\nw3lib==1.18.0\nwcwidth==0.1.7\nzope.interface==4.4.3\nlinux-hcjz:/home/user/Projects/books # \n`", "issue_status": "Closed", "issue_reporting_time": "2017-10-21T13:50:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "520": {"issue_url": "https://github.com/scrapy/scrapy/issues/2970", "issue_id": "#2970", "issue_summary": "Override \"Connection\" in headers will result in two \"Connection\" in headers.", "issue_description": "tuangeek commented on Oct 21, 2017 \u2022\nedited\nIf I try to override the \"Connection\" in the headers with either custome_settings\ncustom_settings = {\n        'DEFAULT_REQUEST_HEADERS': {\n            'Connection' : 'Keep-Alive'\n        }\n}\nor within the request headers\nheaders = {}\n headers['Connection'] : 'Keep-Alive'\n headers['User-Agent'] : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n\nfor url in urls:\n     yield scrapy.Request(url, headers=headers, callback = self.parse)\nIt will result with a request with two \"Connection\"s in the header.\nGET http://example.com HTTP/1.1\nConnection: close\nConnection: Keep-Alive\nUser-Agent: Scrapy/1.4.0 (+http://scrapy.org)\nAccept-Encoding: gzip,deflate", "issue_status": "Closed", "issue_reporting_time": "2017-10-20T23:46:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "521": {"issue_url": "https://github.com/scrapy/scrapy/issues/2969", "issue_id": "#2969", "issue_summary": "DOWNLOAD_DELAY and ctrl-c", "issue_description": "netcaf commented on Oct 20, 2017\nHello,\nIf we set DOWNLOAD_DELAY for a list of urls,\nIt may wait too long to terminate the job for the first cancel operation.\nCan we improve it?\nFor example, wake up the sleeping spider(DOWNLOAD_DELAY) and let it respond to cancel operation.", "issue_status": "Closed", "issue_reporting_time": "2017-10-20T10:23:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "522": {"issue_url": "https://github.com/scrapy/scrapy/issues/2968", "issue_id": "#2968", "issue_summary": "ImportError when running script", "issue_description": "addisonwebb commented on Oct 20, 2017\nDisclaimer: I am very new to the Python world so this issue might be caused by me not understanding how the pieces fit together. \ud83d\ude43\nBackground\nI have create a couple Spiders and I would like to start them using a script. (vs having to start them individually from the command line) I tried to follow the example in the Scrapy docs here. I am confident both of my spiders work as I would expect. When I run them like this, scrapy crawl data_1, I get the behavior I expect.\nWhen I run the script I get this error:\n$ python crawlScript.py \nTraceback (most recent call last):\n  File \"crawlScript.py\", line 3, in <module>\n    import scrapy\nImportError: No module named scrapy\nAny help figuring out the issue would be greatly appreciated!\nScript Content\n#!/usr/bin/env python\n\nimport scrapy\n\nfrom scrapy.crawler import CrawlerProcess\n\n# import spiders\nfrom spiders.data_1 import Data1Spider\n\nprocess = CrawlerProcess(get_project_settings())\n\nprocess.crawl(Data1Spider)\nprocess.start()\nProject Structure\nMyScraper\n|____.DS_Store\n|______init__.py\n|______init__.pyc\n|____crawlScript.py     <--- my script\n|____items.py\n|____items.pyc\n|____middlewares.py\n|____pipelines.py\n|____pipelines.pyc\n|____settings.py\n|____settings.pyc\n|____spiders\n| |____.DS_Store\n| |______init__.py\n| |______init__.pyc\n| |____data_1.py        <--- spider file 1\n| |____data_1.pyc\n| |____data_2.py       <--- spider file 2\n| |____data_2.pyc\nOther Info\nPython version: 2.7.10\nScrapy version: 1.4.0\nOS: macOS 10.12.6", "issue_status": "Closed", "issue_reporting_time": "2017-10-20T04:56:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "523": {"issue_url": "https://github.com/scrapy/scrapy/issues/2967", "issue_id": "#2967", "issue_summary": "<a> element has no href attribute", "issue_description": "AntonYuzhakov commented on Oct 18, 2017 \u2022\nedited\nSo, why does scrapy raise ValueError when you gave <a> tag without href to response.follow(), why not just ignore those?\nAs far as my understanding goes a tags without href is still completely valid, so this behaviour looks strange to me.\nThe href attribute on a and area elements is not required; when those elements do not have href attributes they do not create hyperlinks.", "issue_status": "Closed", "issue_reporting_time": "2017-10-18T17:49:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "524": {"issue_url": "https://github.com/scrapy/scrapy/issues/2966", "issue_id": "#2966", "issue_summary": "ERROR: Unable to send mail: To=['wgm@163.com'] Cc=[] Subject=\"Scrapy Error Alarm\" Attachs=0- object of type 'module' has no len()", "issue_description": "pattywgm commented on Oct 18, 2017\nI caught this error when I wanted to send email with scrapy.mail.MailSender.\n[twisted] CRITICAL: Unhandled Error\nTraceback (most recent call last):\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/python/log.py\", line 103, in callWithLogger\nreturn callWithContext({\"system\": lp}, func, *args, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/python/log.py\", line 86, in callWithContext\nreturn context.call({ILogContext: newCtx}, func, *args, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/python/context.py\", line 122, in callWithContext\nreturn self.currentContext().callWithContext(ctx, func, *args, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/python/context.py\", line 85, in callWithContext\nreturn func(*args,**kw)\n--- ---\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/posixbase.py\", line 597, in _doReadOrWrite\nwhy = selectable.doRead()\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/tcp.py\", line 208, in doRead\nreturn self._dataReceived(data)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/tcp.py\", line 214, in _dataReceived\nrval = self.protocol.dataReceived(data)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/tls.py\", line 330, in dataReceived\nself._flushReceiveBIO()\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/tls.py\", line 295, in _flushReceiveBIO\nProtocolWrapper.dataReceived(self, bytes)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/policies.py\", line 120, in dataReceived\nself.wrappedProtocol.dataReceived(data)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/basic.py\", line 571, in dataReceived\nwhy = self.lineReceived(line)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/mail/smtp.py\", line 995, in lineReceived\nwhy = self._okresponse(self.code, b'\\n'.join(self.resp))\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/mail/smtp.py\", line 1044, in smtpState_to\nreturn self.smtpState_toOrData(0, b'')\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/mail/smtp.py\", line 1062, in smtpState_toOrData\nself.sendLine(b'RCPT TO:' + quoteaddr(self.lastAddress))\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/mail/smtp.py\", line 179, in quoteaddr\nres = email.utils.parseaddr(addr)\nFile \"/usr/lib/python2.7/email/utils.py\", line 214, in parseaddr\naddrs = _AddressList(addr).addresslist\nFile \"/usr/lib/python2.7/email/_parseaddr.py\", line 457, in init\nself.addresslist = self.getaddrlist()\nFile \"/usr/lib/python2.7/email/_parseaddr.py\", line 217, in getaddrlist\nwhile self.pos < len(self.field):\nexceptions.TypeError: object of type 'module' has no len()\nThe environment is scrapy 1.4.0 , twisted 17.5.0.\nBut it was succeed in my own pc with scrapy 1.4.0 , twisted 13.1.0.", "issue_status": "Closed", "issue_reporting_time": "2017-10-18T09:57:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "525": {"issue_url": "https://github.com/scrapy/scrapy/issues/2965", "issue_id": "#2965", "issue_summary": "How to scrapy duplicate url even USE_BLOOM = True ?", "issue_description": "mapleflow commented on Oct 17, 2017\nmulti page with one attachments url\npage url should use bloom_filter, it work ok.\nattachment url may same, but i want rename it hash(page_url)\nit seem block, can scrapy.Request ignore bloom in this condition ?", "issue_status": "Closed", "issue_reporting_time": "2017-10-17T03:52:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "526": {"issue_url": "https://github.com/scrapy/scrapy/issues/2961", "issue_id": "#2961", "issue_summary": "scrapy spider process blocked", "issue_description": "lxtsina commented on Oct 13, 2017 \u2022\nedited\nwhen i debug the spider process ,it show (self._sock.recv() ? blocked):\n(gdb) py-bt\n#5 Frame 0x3c68ab0, for file /usr/lib64/python2.7/socket.py, line 476, in readline (self=<_fileobject at remote 0x3bb92d0>, size=65537, buf=<cStringIO.StringO at remote 0x3bca1f0>, buf_len=0)\ndata = self._sock.recv(self._rbufsize)\n#9 Frame 0x3c688b0, for file /usr/lib64/python2.7/httplib.py, line 400, in _read_status (self=<HTTPResponse(fp=<_fileobject at remote 0x3bb92d0>, status='UNKNOWN', will_close='UNKNOWN', chunk_left='UNKNOWN', length='UNKNOWN', strict=0, reason='UNKNOWN', version='UNKNOWN', debuglevel=0, msg=None, chunked='UNKNOWN', _method='POST') at remote 0x3bc8d40>)\nline = self.fp.readline(_MAXLINE + 1)\n......\n(gdb) info threads\nId Target Id Frame\n1 Thread 0x7f125c92f740 (LWP 29841) \"python2\" 0x00007f125c142a9b in __libc_recv (fd=19, buf=buf@entry=0x3c70b44, n=n@entry=8192, flags=flags@entry=0) at ../sysdeps/unix/sysv/linux/x86_64/recv.c:33\n(gdb) py-list\n471 self._rbuf.write(buf.read())\n472 return rv\n473 self._rbuf = StringIO() # reset _rbuf. we consume it via buf.\n474 while True:\n475 try:\n476 data = self._sock.recv(self._rbufsize) #blocked here\n477 except error, e:\n478 if e.args[0] == EINTR:\nthe parse spider source file ,i have add timeout like this(why it not exit?) :\nclass SinbotSpider(scrapy.Spider):\nname = \"sinbot\"\nallowed_domains = []\ndef __init__(self, start_url, tid, timeout=2400, **kwargs):\n    super(SinbotSpider, self).__init__()\n\n    self.logger.debug('TIMEOUT=%s' %timeout)\n    self.logger.debug('the param is %s' % start_url)\n    self.start_url = start_url\n    self.tid = tid\n    self.timeout = int(timeout)\n    ....\ndef parse(self, response):\n    cur_time = time.time()\n    dif_time = cur_time - self.start_time\n    if dif_time - self.timeout > 0:\n        self.logger.info('time exceed the time limit.')\n        return\n     ....", "issue_status": "Closed", "issue_reporting_time": "2017-10-13T03:40:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "527": {"issue_url": "https://github.com/scrapy/scrapy/issues/2959", "issue_id": "#2959", "issue_summary": "How to connect to the spider singnal in middleware", "issue_description": "exotfboy commented on Oct 11, 2017\nI have to connect to the spider singnal in my middleware, and I use this at the moment:\nclass XXXMiddleware(object):\n    def __init__(self):\n        dispatcher.connect(self.spider_closed, signals.spider_closed)\nWhile I got the warning the Importing from scrapy.xlib.pydispatch is deprecated, ... and use from_crawler` instead.\nAlso I found this closed issue, and this is the suggested alternative:\n@classmethod\ndef from_crawler(cls, crawler, *args, **kwargs):\n    spider = super(MySpider, cls).from_crawler(crawler, *args, **kwargs)\n    crawler.signals.connect(spider.spider_opened, signals.spider_opened)\n    return spider\nHowever in my case, I can not access the spider in the middleware, and I do not think middleware should be coupled with spider .\nSo how to connect to the spider singnal in middleware?", "issue_status": "Closed", "issue_reporting_time": "2017-10-11T00:55:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "528": {"issue_url": "https://github.com/scrapy/scrapy/issues/2949", "issue_id": "#2949", "issue_summary": "SplashFormRequest.from_response executes same request for multiple form parameters.", "issue_description": "nehakansal commented on Oct 3, 2017\nHi,\nI am using SplashFormRequest.from_response() method to submit a form multiple times with different parameters. The formdata is passed on correctly it seems, but eventually when the request is executed, only the first set of parameters are used for the subsequent requests too. I ran a test on google to submit 3 different parameters on the search form and get the same issue. I am pasting the relevant piece of code and part of the output log from the crawl. Any idea why this might be happening? Thanks.\nThis is part of the parse method.\n            for formd in formparams:\n              print('===== Submitting FORM ===== ' + json.dumps(formd))\n              yield SplashFormRequest.from_response(\n                response,  \n                formdata=formd,\n                method=GET,\n                callback=self.parse,\n                meta=meta, **kwargs)\nOutput log\n2017-10-02 23:38:02 [scrapy.core.engine] INFO: Spider opened\n2017-10-02 23:38:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\nREQUEST === <GET http://google.com>\nREQUEST === <GET http://www.google.com via http://127.0.0.1:8050/execute>\n2017-10-02 23:38:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.google.com via http://127.0.0.1:8050/execute> (referer: None)\n2017-10-02 23:38:10 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/?gws_rd=ssl>\n<CDRItem: _id: '78EA5573E093F9030B1F945E84B59B51D405C360BB4C421ED93A66976195BC2C', url: 'https://www.google.com/?gws_rd=ssl', timestamp_crawl: '2017-10-03T06:38:10.971013Z'>\n===== Submitting FORM ===== {\"q\": \"happy\"}\nREQUEST === <GET http://google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy>\n===== Submitting FORM ===== {\"q\": \"sad\"}\n2017-10-02 23:38:10 [py.warnings] WARNING: /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy_splash/middleware.py:184: UserWarning: Unexpected request.meta['splash']['_replaced_args']\n  warnings.warn(\"Unexpected request.meta['splash']['_replaced_args']\")\n  \nREQUEST === <GET http://google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=sad>\nREQUEST === <GET https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy via http://127.0.0.1:8050/execute>\nREQUEST === <GET https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy via http://127.0.0.1:8050/execute>\n2017-10-02 23:38:10 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy via http://127.0.0.1:8050/execute>\n===== Submitting FORM ===== {\"q\": \"neutral\"}\nREQUEST === <GET http://google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=neutral>\nREQUEST === <GET https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy via http://127.0.0.1:8050/execute>\n2017-10-02 23:38:11 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy via http://127.0.0.1:8050/execute>\nREQUEST === <GET https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy via http://127.0.0.1:8050/execute>\n2017-10-02 23:38:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy via http://127.0.0.1:8050/execute> (referer: None)\n2017-10-02 23:38:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy>\n<CDRItem: _id: 'B31AF63FD579B31F9A18BE78F30F845C21DD5F7DE36838FE87CED836E5783BEB', url: 'https://www.google.com/search?source=hp&oq=&gs_l=&btnG=Search&q=happy', timestamp_crawl: '2017-10-03T06:38:22.662764Z'>\n2017-10-02 23:38:22 [scrapy.core.engine] INFO: Closing spider (finished)", "issue_status": "Closed", "issue_reporting_time": "2017-10-03T07:01:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "529": {"issue_url": "https://github.com/scrapy/scrapy/issues/2948", "issue_id": "#2948", "issue_summary": "[twisted] CRITICAL: Unhandled error in Deferred", "issue_description": "nqthiep commented on Oct 1, 2017\nToday, I suddenly encountered a bug related to Twisted.\nI tried reinstalling the latest version of Scrapy and Twisted.\nBut the problem is still not fixed.\nDetailed error as below, anyone knows how to fix it.\n2017-10-01 22:46:02 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: DataInput)\n2017-10-01 22:46:02 [scrapy.utils.log] INFO: Overridden settings: {'FEED_EXPORT_ENCODING': 'utf-8', 'BOT_NAME': 'DataInput', 'SPIDER_MODULES': ['DataInput.spiders'], 'NEWSPIDER_MODULE': 'DataInput.spiders'}\n2017-10-01 22:46:03 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.logstats.LogStats']\nUnhandled error in Deferred:\n2017-10-01 22:46:03 [twisted] CRITICAL: Unhandled error in Deferred:\n\nTemporarily disabling observer LegacyLogObserverWrapper(<bound method PythonLoggingObserver.emit of <twisted.python.log.PythonLoggingObserver object at 0x000001A33286EA20>>) due to exception: [Failure instance: Traceback: <class 'RecursionError'>: maximum recursion depth exceeded\nC:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\commands\\crawl.py:57:run\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\internet\\defer.py:965:__del__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\logger\\_logger.py:181:failure\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\logger\\_logger.py:135:emit\n--- <exception caught here> ---\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\logger\\_observer.py:131:__call__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\logger\\_legacy.py:93:__call__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\log.py:595:emit\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\logger\\_legacy.py:154:publishToNewObserver\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\logger\\_stdlib.py:113:__call__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:539:getTracebackObject\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:116:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:111:__init__\nC:\\Program Files\\Python35\\lib\\site-packages\\twisted\\python\\failure.py:136:__init__\n]\n\nProcess finished with exit code 0", "issue_status": "Closed", "issue_reporting_time": "2017-10-01T15:52:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "530": {"issue_url": "https://github.com/scrapy/scrapy/issues/2945", "issue_id": "#2945", "issue_summary": "how call spider from spider", "issue_description": "seozed commented on Sep 27, 2017 \u2022\nedited\ni need start a spider from a running spider.\nbut show this error.\nwhat should I do\nmyspider.py\nimport scrapy\nfrom twisted.internet import reactor\nfrom scrapy.crawler import CrawlerRunner\n\nclass MySpider1(scrapy.Spider):\n    name = \"myspider1\"\n    allowed_domains = []\n    start_urls = ['http://cn.bing.com/']\n\n    def parse(self, response):\n        # Some run conditions\n        if 'Bing' in response.text:\n\n            runner = CrawlerRunner()\n            runner.crawl(MySpider2)\n            d = runner.join()\n            d.addBoth(lambda _: reactor.stop())\n            reactor.run()\n\n\nclass MySpider2(scrapy.Spider):\n    \"\"\"do someting\"\"\"\n    pass\nerror info\n['base.pipelines.BasePipeline']\n2017-09-27 13:40:54 [scrapy.core.engine] INFO: Spider opened\n2017-09-27 13:40:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-09-27 13:40:54 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-09-27 13:40:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://cn.bing.com/> (referer: None)\n2017-09-27 13:40:55 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.corestats.CoreStats']\nUnhandled error in Deferred:\n2017-09-27 13:40:55 [twisted] CRITICAL: Unhandled error in Deferred:\n\n2017-09-27 13:40:55 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1301, in _inlineCallbacks\n    result = g.send(result)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\crawler.py\", line 71, in crawl\n    self.spider = self._create_spider(*args, **kwargs)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\crawler.py\", line 94, in _create_spider\n    return self.spidercls.from_crawler(self, *args, **kwargs)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 50, in from_crawler\n    spider = cls(*args, **kwargs)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\scrapy\\spiders\\__init__.py\", line 29, in __init__\n    raise ValueError(\"%s must have a name\" % type(self).__name__)\nValueError: MySpider2 must have a name\n2017-09-27 13:40:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://cn.bing.com/> (referer: None)\nTraceback (most recent call last):\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\twisted\\internet\\defer.py\", line 653, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"D:\\scripts\\spider\\SpiderBase\\base\\spiders\\testsMyspider.py\", line 19, in parse\n    reactor.run()\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\twisted\\internet\\base.py\", line 1242, in run\n    self.startRunning(installSignalHandlers=installSignalHandlers)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\twisted\\internet\\base.py\", line 1222, in startRunning\n    ReactorBase.startRunning(self)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\twisted\\internet\\base.py\", line 728, in startRunning\n    raise error.ReactorAlreadyRunning()\ntwisted.internet.error.ReactorAlreadyRunning\n2017-09-27 13:40:55 [scrapy.core.engine] INFO: Closing spider (shutdown)", "issue_status": "Closed", "issue_reporting_time": "2017-09-27T05:47:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "531": {"issue_url": "https://github.com/scrapy/scrapy/issues/2943", "issue_id": "#2943", "issue_summary": "Scrapy ImagePipeline ignore images from specific host", "issue_description": "marcuslind90 commented on Sep 25, 2017\nI have an issue where my crawler is ignoring images from a specific host. It does not download any images and it does not throw any error or log messages. If I replace the image path with any other image path from any other source, it works fine with exactly the same config and code.\nI put up a public repo here that reproduce it:\nhttps://github.com/marcuslind90/scrapy_error\nAs you can see in the example spider, I hardcode 2 paths like this:\nloader.add_value('image_urls', ['https://media.fastighetsbyran.se/22943836.jpg', 'http://hemmon.com/house.jpg'])\nThe first image is ignored (and any other images on that host) while the second image is downloaded.\nAnyone recognize this issue? Could it have something to do with headers? Also, I believe that the host is programmatically generating the images because you can append a ?Bredd=<pixels> parameter that will automatically resize the picture. Perhaps this could be part of it as well.", "issue_status": "Closed", "issue_reporting_time": "2017-09-25T15:07:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "532": {"issue_url": "https://github.com/scrapy/scrapy/issues/2941", "issue_id": "#2941", "issue_summary": "Running spider via CrawlerRunner from script gives Error: ReactorNotRestartable", "issue_description": "ArionMiles commented on Sep 23, 2017\nOk so I've given it a lot of try, asked most pythonistas I know, even on stackoverflow here: https://stackoverflow.com/questions/46346863/twisted-reactor-not-restarting-in-scrapy\nBut I cannot for the life of me figure this out. I'm trying to execute the spider via a telegram bot, using the python-telegram-bot API wrapper.\nI'm running Python 2.7 on Windows 10.\nThis is my code:\nfrom twisted.internet import reactor\nfrom scrapy import cmdline\nfrom telegram.ext import Updater, CommandHandler, MessageHandler, Filters, RegexHandler\nimport logging\nimport os\nimport ConfigParser\nimport json\nimport textwrap\nfrom MIS.spiders.moodle_spider import MySpider\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import CrawlerRunner, CrawlerProcess\nfrom scrapy.utils.log import configure_logging\n\n\n# Read settings from config file\nconfig = ConfigParser.RawConfigParser()\nconfig.read('./spiders/creds.ini')\nTOKEN = config.get('BOT', 'TOKEN')\n#APP_NAME = config.get('BOT', 'APP_NAME')\n#PORT = int(os.environ.get('PORT', '5000'))\nupdater = Updater(TOKEN)\n\n# Setting Webhook\n#updater.start_webhook(listen=\"0.0.0.0\",\n#                      port=PORT,\n#                      url_path=TOKEN)\n#updater.bot.setWebhook(APP_NAME + TOKEN)\n\nlogging.basicConfig(format='%(asctime)s -# %(name)s - %(levelname)s - %(message)s',level=logging.INFO)\n\ndispatcher = updater.dispatcher\n\ndef doesntRun(bot, update):\n    configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n    runner = CrawlerRunner({\n        'FEED_FORMAT' : 'json',\n        'FEED_URI' : 'output.json'\n        })\n\n    d = runner.crawl(MySpider)\n    d.addBoth(lambda _: reactor.stop()) # this line is supposed to restart the reactor, right?\n    reactor.run(installSignalHandlers=0) # the script will block here until the crawling is finished\n\n    with open(\"./output.json\", 'r') as file:\n        contents = file.read()\n        a_r = json.loads(contents)\n        AM = a_r[0]['AM']\n        ...\n        ...\n\n        message_template = textwrap.dedent(\"\"\"\n                AM: {AM}\n                ...\n                \"\"\")\n        messageContent = message_template.format(AM=AM, ...)\n        bot.sendMessage(chat_id=update.message.chat_id, text=messageContent)\n\n\n# Handlers\ntest_handler = CommandHandler('doesntRun', doesntRun)\n\n# Dispatchers\ndispatcher.add_handler(test_handler)\n\nupdater.start_polling()\nupdater.idle()\nPlease provide insight on how I can restart the reactor, this is bugging me from a couple of days.", "issue_status": "Closed", "issue_reporting_time": "2017-09-23T16:27:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "533": {"issue_url": "https://github.com/scrapy/scrapy/issues/2940", "issue_id": "#2940", "issue_summary": "Give `BaseDupeFilter` access to spider-object", "issue_description": "Chratho commented on Sep 23, 2017\nI am in a situation where a single item gets defined over a sequence of multiple pages, passing values between the particular callbacks using the meta-dict. I believe this is a common approach among scrapy-users.\nHowever, it feels like this approach is difficult to get right. With the default implementation of RFPDupefilter, my callback-chain is teared apart quite easy, as fingerprints don't take the meta-dict into account. The corresponding requests are thrown away, the information in the meta-dict which made this request unique is lost.\nI have currently implemented by own meta-aware DupeFilter, but I am still facing the problem that it lacks access to the specific spider in use - and only the Spider really knows the meta-attributes that make a request unique. I could now take it a step further and implement my own scheduler, but I'm afraid that all these custom extensions make my code very brittle wrt future versions of scrapy.", "issue_status": "Closed", "issue_reporting_time": "2017-09-23T15:03:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "534": {"issue_url": "https://github.com/scrapy/scrapy/issues/2938", "issue_id": "#2938", "issue_summary": "scrapy to follow external link with one depth only", "issue_description": "fatagun commented on Sep 21, 2017\nImagine I am crawling foo.com. foo.com has several internal links to itself, and it has some external links like:\nfoo.com/hello\nfoo.com/contact\nbar.com\nholla.com\nI would like scrapy to crawl all the internal links but also only one depth for external links such as I want scrapy to go to bar.com or holla.com but I dont want it to go any other link within bar.com so only depth of one.\nis this possible? What would be the config for this case?\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2017-09-21T10:49:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "535": {"issue_url": "https://github.com/scrapy/scrapy/issues/2937", "issue_id": "#2937", "issue_summary": "IPython version incompatible problem in tests of python 2.7", "issue_description": "Contributor\ngrammy-jiang commented on Sep 21, 2017\nThe version of IPython in tests/requirements.txt is not fixed to less then 6.0, so when testing is running under python 2.7, there will be an error on the requirement installing stage.\nPlease refer to Installation \u2014 IPython 6.2.0 documentation:\nThis documentation covers IPython versions 6.0 and higher. Beginning with version 6.0, IPython stopped supporting compatibility with Python versions lower than 3.3 including all versions of Python 2.7.\nIf you are looking for an IPython version compatible with Python 2.7, please use the IPython 5.x LTS release and refer to its documentation (LTS is the long term support release).\nSo it should be like this in tests/requirements.txt:\nipython<6.0", "issue_status": "Closed", "issue_reporting_time": "2017-09-21T08:53:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "536": {"issue_url": "https://github.com/scrapy/scrapy/issues/2936", "issue_id": "#2936", "issue_summary": "Scrapy vs Django + Django Q + Gunicorn", "issue_description": "iagent commented on Sep 21, 2017 \u2022\nedited\nI am currently using scrapy for an application involving broad crawling.\nOne of my teammates contested to use a combination of django + gunicorn in place of Scrapy as most of the team is conversant with Django and its ORM. The number of websites involved are to the tune of 100K. He suggests to use Gunicorn if we want concurrency and Django Q for queueing requests.\nI am not comfortable with the idea as Django and Scrapy are built for completely different problems but not sure how can I make a strong argument.\nCould you help me out here? My understanding is that queueing at each stage of crawling and scraping sort of helps in performance. As Scrapy takes care of each stage, even if we temporarily drop performance concern, wouldn't it involve more code writing?", "issue_status": "Closed", "issue_reporting_time": "2017-09-21T07:50:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "537": {"issue_url": "https://github.com/scrapy/scrapy/issues/2934", "issue_id": "#2934", "issue_summary": "PyPy build failing on Travis", "issue_description": "Member\nlopuhin commented on Sep 20, 2017\nlxml fails to install: https://travis-ci.org/scrapy/scrapy/jobs/277021716", "issue_status": "Closed", "issue_reporting_time": "2017-09-20T11:25:08Z", "fixed_by": "#2935", "pull_request_summary": "Update pypy version regexp to get last release", "pull_request_description": "Member\nlopuhin commented on Sep 20, 2017 \u2022\nedited\nPyPy changed naming convention since 5.8 release, not it's called pypy2.7-x.x.x\nThis fixes #2934", "pull_request_status": "Merged", "issue_fixed_time": "2017-09-20T12:59:47Z", "files_changed": [["2", ".travis.yml"]]}, "538": {"issue_url": "https://github.com/scrapy/scrapy/issues/2932", "issue_id": "#2932", "issue_summary": "Structure of XmlExportPipeline example class in docs is confusing", "issue_description": "Contributor\ncolinmorris commented on Sep 19, 2017\nThe Exporters section of the docs give an example pipeline class called XmlExportPipeline. It defines methods spider_opened and spider_closed.\nI found this confusing because the pipeline docs describe methods called open_spider and close_spider, with seemingly the same semantics. At first I thought the example methods were typo'd, until I saw that they were explicitly connected to the corresponding signals in from_crawler.\nI'm guessing this is the \"old way\" of doing it, and the example predates the existence of the open_spider and close_spider methods?\nUnless there's a reason to do it this way, I think the example would be clearer (and shorter) if it used the built-in names.", "issue_status": "Closed", "issue_reporting_time": "2017-09-18T23:24:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "539": {"issue_url": "https://github.com/scrapy/scrapy/issues/2930", "issue_id": "#2930", "issue_summary": "ImportError:No module named py.zhihuuser.zhihuuser.items. But it was exist.", "issue_description": "youyiqin commented on Sep 18, 2017\n\nI donot know how to make it work.thanks \ud83d\ude22", "issue_status": "Closed", "issue_reporting_time": "2017-09-18T12:32:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "540": {"issue_url": "https://github.com/scrapy/scrapy/issues/2928", "issue_id": "#2928", "issue_summary": "LinkExtractor is not ignoring .m4v extension (video) by default", "issue_description": "Contributor\ndjunzu commented on Sep 18, 2017\nBy chance I found out that LinkExtractor is not ignoring the video extension m4v in the same way it is ignoring other video formats.\nhttps://en.wikipedia.org/wiki/M4V", "issue_status": "Closed", "issue_reporting_time": "2017-09-17T18:46:03Z", "fixed_by": "#2929", "pull_request_summary": "[MRG+1] Add m4v extension to IGNORED_EXTENSIONS in LinkExtractor.", "pull_request_description": "Contributor\ndjunzu commented on Sep 18, 2017\nAdd m4v extension (video) to IGNORED_EXTENSIONS in LinkExtractor so it can be ignored by default in the same way other video formats are already ignored.\nfix #2928", "pull_request_status": "Merged", "issue_fixed_time": "2017-10-26T14:35:32Z", "files_changed": [["2", "scrapy/linkextractors/__init__.py"]]}, "541": {"issue_url": "https://github.com/scrapy/scrapy/issues/2927", "issue_id": "#2927", "issue_summary": "Unhelpful log message from core.downloader.handlers.http11", "issue_description": "Contributor\ndjunzu commented on Sep 18, 2017\nConsider the two log messages:\n2017-09-17 10:52:27 [scrapy.core.downloader.handlers.http11] WARNING: Expected response size (42276161) larger than download warn size (33554432).\n2017-09-17 10:54:18 [scrapy.core.downloader.handlers.http11] WARNING: Received more bytes than download warn size (33554432) in request <GET https://example.com>.\nWhile the second one can be useful, the first one is not helpful at all because there is no info about which request it is about. The first message should indicate the request it refers to in the same way the second message does.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-09-17T18:37:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "542": {"issue_url": "https://github.com/scrapy/scrapy/issues/2926", "issue_id": "#2926", "issue_summary": "Call antoher spider (spider B in project B) in the current spider (spider A in project A)", "issue_description": "xiaoyuan1998 commented on Sep 17, 2017 \u2022\nedited\nHello Guys,\nTo solve the problem of proxy, i programmed spider A in project A which helps to crawl free proxy ip in a website and then put them in a csv file:\nscrapy runspider getproxy_inmyfile.py -o proxylist.csv\nThen i did my spider B in project B to crawl the information.\nSpider B will check and get a proxy ip in my customised download middware. if the proxy ip is invalid, middware will delete this ip in the proxy csv file. if it is effective, i will use this ip as a proxy,\nMy issue is that:\nwhen the proxy csv file is empty (all proxy ip are invalid and deleted)\ni would like to run spider A again to provisioning the csv file.\nI only find the tutorial of running multiple spiders in one project in the site of doc https://docs.scrapy.org/en/latest/ but i do not know how to run another spider in another project.\nThanks in advance for your help.", "issue_status": "Closed", "issue_reporting_time": "2017-09-17T14:29:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "543": {"issue_url": "https://github.com/scrapy/scrapy/issues/2925", "issue_id": "#2925", "issue_summary": "HtmlResponse check in _requests_to_follow method", "issue_description": "Igor4uk commented on Sep 14, 2017\nIs it necessary to check for HtmlResponse here https://github.com/scrapy/scrapy/blob/master/scrapy/spiders/crawl.py#L57 ?\nMy site start return content-type:application/json and scrapy recognise it like a TextResponse and finish parsing after the first request to it", "issue_status": "Closed", "issue_reporting_time": "2017-09-14T16:22:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "544": {"issue_url": "https://github.com/scrapy/scrapy/issues/2920", "issue_id": "#2920", "issue_summary": "CONCURRENT_REQUESTS_PER_DOMAIN seem not work", "issue_description": "treemore commented on Sep 11, 2017 \u2022\nedited\nthis is my scrapy file\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass Bug1Spider(scrapy.Spider):\n    name = 'bug1'\n    allowed_domains = ['google.com']\n    start_urls = ['https://www.google.com/search?q=scrapy' for x in range(0, 20, 1)]\n\n    def parse(self, response):\n        self.log(\"*\" * 100)\n        pass\nsettings.py\nDOWNLOAD_DELAY = 10\n# The download delay setting will honor only one of:\nCONCURRENT_REQUESTS_PER_DOMAIN = 20\n# here AUTOTHROTTLE_ENABLED set True or False not affect the result\nAUTOTHROTTLE_ENABLED = False\nwhen i execute the shell below\nscrapy crawl bug1\ni notice that the start_urls not CONCURRENT request.\ni set CONCURRENT_REQUESTS_PER_DOMAIN=20. i assume will fetch 20 request at the same time, but the console look like the request is fetch one by one . request one and than wait 10 seconds and start the next one.\nmaybe DOWNLOAD_DELAY>0 -> CONCURRENT_REQUES TS_PER_DOMAIN=1 ?\njust like https://stackoverflow.com/questions/37461327/scrapy-concurrent-requests-ignored-when-download-delay-set\nbut i can not find that in any document.\nmy scrapy version is Version: 1.4.0\nTwisted version is Version: 17.5.0", "issue_status": "Closed", "issue_reporting_time": "2017-09-10T19:14:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "545": {"issue_url": "https://github.com/scrapy/scrapy/issues/2914", "issue_id": "#2914", "issue_summary": "Python 3.6 type hinting exception", "issue_description": "IaroslavR commented on Sep 6, 2017\nGot error with code:\ndef strip_text(data) -> list:\n    if isinstance(data, str):\n        return [data.strip()]\n    else:\n        return [s.strip() for s in data if s.strip()]\nTraceback (most recent call last):\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\nyield next(it)\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\nfor x in result:\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in\nreturn (_set_referer(r) for r in result or ())\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in\nreturn (r for r in result or () if _filter(r))\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in\nreturn (r for r in result or () if _filter(r))\nFile \"/home/elruso/projects/shopify/shopify/spiders/countdown.py\", line 40, in level_2\nil.add_value('url', response.url)\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/loader/init.py\", line 77, in add_value\nself._add_value(field_name, value)\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/loader/init.py\", line 91, in _add_value\nprocessed_value = self._process_input_value(field_name, value)\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/loader/init.py\", line 150, in _process_input_value\nreturn proc(value)\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/loader/processors.py\", line 45, in call\nwrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/loader/processors.py\", line 45, in\nwrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/loader/common.py\", line 10, in wrap_loader_context\nif 'loader_context' in get_func_args(function):\nFile \"/home/elruso/.virtualenvs/3.6.1/lib/python3.6/site-packages/scrapy/utils/python.py\", line 201, in get_func_args\nfunc_args, _, _, _ = inspect.getargspec(func)\nFile \"/usr/lib/python3.6/inspect.py\", line 1072, in getargspec\nraise ValueError(\"Function has keyword-only parameters or annotations\"\nValueError: Function has keyword-only parameters or annotations, use getfullargspec() API which can support them", "issue_status": "Closed", "issue_reporting_time": "2017-09-06T11:30:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "546": {"issue_url": "https://github.com/scrapy/scrapy/issues/2913", "issue_id": "#2913", "issue_summary": "Scrapy spider fails to terminate after finishing web scrape", "issue_description": "jeremyjordan commented on Sep 4, 2017\nI am running a spider with Scrapy but after it finishes crawling it can't seem to terminate. Log stats just recursively report that it is scraping 0 pages/minute. When I try to quit with Ctrl-C, it fails to shut down gracefully and I have to quit forcefully with Ctrl-C again. Any clue what is happening?\nAfter completing a scrape, I just get output like this:\n2017-08-24 11:13:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\n2017-08-24 11:14:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\n2017-08-24 11:15:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\n2017-08-24 11:16:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\n2017-08-24 11:17:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\n2017-08-24 11:18:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\n2017-08-24 11:19:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\n2017-08-24 11:20:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\n2017-08-24 11:21:45 [scrapy.extensions.logstats] INFO: Crawled 60 pages (at 0 pages/min), scraped 54 items (at 0 items/min)\nwhich continues indefinitely.\nMy spider goes to a page that contains a list of links over multiple pages. It visits the first page, extracts the links (using the request meta trick to pass some information along while following the link), and then goes to the next page of links.\nA second parser extracts information from the individual pages.\nI don't see any error messages, and the job performs successfully; it just fails to end. This is a problem because I would like to use a script to call the job to run multiple times on different pages (same structure, different information), but the since the first job never finishes I can't ever get to the next set of pages to scrape.\nThe parse(self, response) method yields two types of information.\nFor each link on the page, visit the page to extract more information.\n request = scrapy.Request(item['url'], callback=self.parse_transcript)\n request.meta['item'] = item\n yield request\nIf there is another page of links, get link and increment page number by 1 using regex.\n while data['count'] > 0:\n     next_page = re.sub('(?<=page=)(\\d+)', lambda x: str(int(x.group(0)) + 1), response.url) \n     yield Request(next_page)\nI checked the engine status using the telnet extension. I'm not sure how to interpret this information though.\n>>> est()\nExecution engine status\n\ntime()-engine.start_time                        : 10746.1215799\nengine.has_capacity()                           : False\nlen(engine.downloader.active)                   : 0\nengine.scraper.is_idle()                        : False\nengine.spider.name                              : transcripts\nengine.spider_is_idle(engine.spider)            : False\nengine.slot.closing                             : <Deferred at 0x10d8fda28>\nlen(engine.slot.inprogress)                     : 4\nlen(engine.slot.scheduler.dqs or [])            : 0\nlen(engine.slot.scheduler.mqs)                  : 0\nlen(engine.scraper.slot.queue)                  : 0\nlen(engine.scraper.slot.active)                 : 4\nengine.scraper.slot.active_size                 : 31569\nengine.scraper.slot.itemproc_size               : 0\nengine.scraper.slot.needs_backout()             : False\nI tried raising an exception to close the spider after it reached the end of the links, but this prematurely stopped the spider from being able to visit all of the links that were scrapped. Further, the engine still appeared to hang after closing the spider.\nwhile data['count'] > 0:\n    next_page = re.sub('(?<=page=)(\\d+)', lambda x: str(int(x.group(0)) + 1), response.url)\n    yield Request(next_page)\nelse:\n    raise CloseSpider('End of transcript history has been reached.')\nI also tried using the CLOSESPIDER_TIMEOUT extension, but to no avail. The spider appears to close properly, but the engine remains idling indefinitely.\n2017-08-30 11:20:44 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 9 pages/min), scraped 42 items (at 9 items/min)\n2017-08-30 11:23:44 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 0 pages/min), scraped 42 items (at 0 items/min)\n2017-08-30 11:24:44 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 0 pages/min), scraped 42 items (at 0 items/min)\n2017-08-30 11:25:44 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)\n2017-08-30 11:25:44 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 0 pages/min), scraped 42 items (at 0 items/min)\n2017-08-30 11:28:44 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 0 pages/min), scraped 42 items (at 0 items/min)\n2017-08-30 11:29:44 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 0 pages/min), scraped 42 items (at 0 items/min)\n2017-08-30 11:32:44 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 0 pages/min), scraped 42 items (at 0 items/min)\n^C2017-08-30 11:33:31 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force\n2017-08-30 11:41:44 [scrapy.extensions.logstats] INFO: Crawled 48 pages (at 0 pages/min), scraped 42 items (at 0 items/min)\n^C2017-08-30 11:45:52 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown", "issue_status": "Closed", "issue_reporting_time": "2017-09-04T17:45:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "547": {"issue_url": "https://github.com/scrapy/scrapy/issues/2908", "issue_id": "#2908", "issue_summary": "how should I use cookie in FilesPipeline?", "issue_description": "lcz1688 commented on Aug 31, 2017\nhello,how should I use cookie in FilesPipeline? For example,when filesPipeline download a video in page A,it need the cookie of page A\uff0cbut spider has sent a requset to page B,and the cookie has changed.How can I solve the problem?Besides,if the video address has a short term of validity,what should I do?", "issue_status": "Closed", "issue_reporting_time": "2017-08-31T09:11:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "548": {"issue_url": "https://github.com/scrapy/scrapy/issues/2907", "issue_id": "#2907", "issue_summary": "Scrapy is not populating correctly response.request", "issue_description": "rlucca commented on Aug 31, 2017\nHi there,\nI was expecting the response.request.url to be the same url of the original request here. Or I'm thinking wrong? I saw this problem when I was doing some fixes in one of my spiders.\n$ scrapy shell 'http://www.maneyonline.com'\n2017-08-30 18:23:58 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n2017-08-30 18:23:58 [scrapy.utils.log] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'EDITOR': '/usr/bin/vim', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2017-08-30 18:23:58 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2017-08-30 18:23:58 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-08-30 18:23:58 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-08-30 18:23:58 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-08-30 18:23:58 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-08-30 18:23:58 [scrapy.core.engine] INFO: Spider opened\n2017-08-30 18:23:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.tandfonline.com/> from <GET http://www.maneyonline.com>\n2017-08-30 18:23:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.tandfonline.com/?cookieSet=1> from <GET http://www.tandfonline.com/>\n2017-08-30 18:23:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://www.tandfonline.com/> from <GET http://www.tandfonline.com/?cookieSet=1>\n2017-08-30 18:24:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.tandfonline.com/> (referer: None)\n2017-08-30 18:24:00 [traitlets] DEBUG: Using default logger\n2017-08-30 18:24:00 [traitlets] DEBUG: Using default logger\n[s] Available Scrapy objects:\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7feb48de3b90>\n[s]   item       {}\n[s]   request    <GET http://www.maneyonline.com>\n[s]   response   <200 http://www.tandfonline.com/>\n[s]   settings   <scrapy.settings.Settings object at 0x7feb48de3990>\n[s]   spider     <DefaultSpider 'default' at 0x7feb48736610>\n[s] Useful shortcuts:\n[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n[s]   shelp()           Shell help (print this help)\n[s]   view(response)    View response in a browser\nIn [1]: print request.url\nhttp://www.maneyonline.com\n\nIn [2]: print response.url\nhttp://www.tandfonline.com/\n\nIn [3]: print response.request.url\nhttp://www.tandfonline.com/", "issue_status": "Closed", "issue_reporting_time": "2017-08-30T21:30:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "549": {"issue_url": "https://github.com/scrapy/scrapy/issues/2905", "issue_id": "#2905", "issue_summary": "An error occurred while connecting: [Failure instance: Traceback: <class 'ValueError'>: filedescriptor out of range in select()", "issue_description": "sseyboth commented on Aug 29, 2017\nI'm trying crawl ~200k sites, only the home pages. In the beginning the crawl works fine but the logs quickly fill up with the following errors:\n2017-08-29 11:18:55,131 - scrapy.core.scraper - ERROR - Error downloading <GET http://axo-suit.eu>\nTraceback (most recent call last):\nFile \"venv/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1384, in _inlineCallbacks\nresult = result.throwExceptionIntoGenerator(g)\nFile \"venv/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\nreturn g.throw(self.type, self.value, self.tb)\nFile \"venv/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\ndefer.returnValue((yield download_func(request=request,spider=spider)))\ntwisted.internet.error.ConnectError: An error occurred while connecting: [Failure instance: Traceback: <class 'ValueError'>: filedescriptor out of range in select()\nvenv/lib/python3.6/site-packages/twisted/internet/base.py:1243:run\nvenv/lib/python3.6/site-packages/twisted/internet/base.py:1255:mainLoop\nvenv/lib/python3.6/site-packages/twisted/internet/selectreactor.py:106:doSelect\nvenv/lib/python3.6/site-packages/twisted/internet/selectreactor.py:88:_preenDescriptors\n--- ---\nvenv/lib/python3.6/site-packages/twisted/internet/selectreactor.py:85:_preenDescriptors\n].\nlsof shows that the process indeed has >1024 open network connections, which I believe is the limit for select().\nI set CONCURRENT_REQUESTS = 100 and REACTOR_THREADPOOL_MAXSIZE = 20 based on https://doc.scrapy.org/en/latest/topics/broad-crawls.html.\nNot sure how the crawl ends up with so many open connections. Maybe it's leaking filedescriptors somewhere?\nI'm using:\nPython 3.6.2\nScrapy 1.4.0\nTwisted 17.5.0\nmacOS Sierra 10.12.6\nHappy to provide more info as needed.", "issue_status": "Closed", "issue_reporting_time": "2017-08-29T11:51:33Z", "fixed_by": "#4294", "pull_request_summary": "Specify Twisted reactor (TWISTED_REACTOR setting)", "pull_request_description": "Member\nelacuesta commented 8 days ago \u2022\nedited\nI've been thinking about this since #2905 (comment), but at the time I didn't quite know where the changes should go. Then came @wRAR's awesome asyncio PR (#4010) and made it clear, I'm definitely standing on the shoulders of giants here \ud83d\ude47\nAt the moment I cannot think of many actual use cases besides the one from the above issue, but I think this would be a cool option to have.\nUpdate: This PR now removes the ASYNCIO_REACTOR setting (not yet released), merging its functionality with the TWISTED_REACTOR one (by setting TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor)\nUpdate 2: closes #2905", "pull_request_status": "Merged", "issue_fixed_time": "2020-02-06T17:42:35Z", "files_changed": [["11", "docs/faq.rst"], ["7", "docs/topics/broad-crawls.rst"], ["45", "docs/topics/settings.rst"], ["7", "pytest.ini"], ["19", "scrapy/crawler.py"], ["4", "scrapy/settings/default_settings.py"], ["17", "scrapy/utils/asyncio.py"], ["4", "scrapy/utils/defer.py"], ["11", "scrapy/utils/log.py"], ["35", "scrapy/utils/reactor.py"], ["3", "tests/CrawlerProcess/asyncio_enabled_no_reactor.py"], ["3", "tests/CrawlerProcess/asyncio_enabled_reactor.py"], ["13", "tests/CrawlerProcess/twisted_reactor_asyncio.py"], ["13", "tests/CrawlerProcess/twisted_reactor_poll.py"], ["13", "tests/CrawlerProcess/twisted_reactor_select.py"], ["10", "tests/test_commands.py"], ["47", "tests/test_crawler.py"], ["4", "tests/test_utils_asyncio.py"]]}, "550": {"issue_url": "https://github.com/scrapy/scrapy/issues/2904", "issue_id": "#2904", "issue_summary": "RFPDupeFilter request fingerprint can not be saved", "issue_description": "mahaoyang commented on Aug 29, 2017 \u2022\nedited\nscrapy.dupefilters.RFPDupeFilter\nline 47 - 53\ndef request_seen(self, request):\n    fp = self.request_fingerprint(request)\n    if fp in self.fingerprints:\n        return True\n    self.fingerprints.add(fp)\n    if self.file:\n        self.file.write(fp + os.linesep)\nThe file.write() function only write this to cahce . So when I restart the spider, all fingerprints gone.\nI change it to\nclass SeenURLFilter(RFPDupeFilter):\n\"\"\"A dupe filter that considers the URL\"\"\"\ndef __init__(self, path=None, debug=False):\n    RFPDupeFilter.__init__(self, path, debug)\n\ndef request_seen(self, request):\n    fp = request.url\n    if fp in self.fingerprints:\n        return True\n    self.fingerprints.add(fp)\n    if self.file:\n        self.file.write(fp + os.linesep)\n        self.file.flush()", "issue_status": "Closed", "issue_reporting_time": "2017-08-29T07:01:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "551": {"issue_url": "https://github.com/scrapy/scrapy/issues/2903", "issue_id": "#2903", "issue_summary": "Multi crawlers at the same time might conflict over scrapy.signal", "issue_description": "dingld commented on Aug 28, 2017\nscrapy.signal module is globally imported in all related modules, so it is shared among all the crawlers running in the same process.\nScanning the source code of scrapyrt, an extra if-condition was added to the receiver to isolate each crawler.\n   if spider is self.crawler.spider:\nBut this way looks redundant, would it be better to construct the signal object as a member variable per crawler?", "issue_status": "Closed", "issue_reporting_time": "2017-08-28T09:50:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "552": {"issue_url": "https://github.com/scrapy/scrapy/issues/2902", "issue_id": "#2902", "issue_summary": "Why png format image is saved after the jpg format", "issue_description": "fengfangqian commented on Aug 28, 2017\ni custom a CustomImagesPipeline but save png pic wrong like this please help me\n\n2017-08-28 11:14:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://pp.myapp.com/ma_icon/0/icon_12057190_21093090_1431637865/256#icon> (referer: None)\n2017-08-28 11:14:23 [scrapy.pipelines.files] DEBUG: File (downloaded): Downloaded file from <GET http://pp.myapp.com/ma_icon/0/icon_12057190_21093090_1431637865/256#icon> referred in\n12bdace74812a55c4365a7ad49dea0721242a807 jpg 44444444\n12bdace74812a55c4365a7ad49dea0721242a807 jpg 44444444\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: STREAM 'IHDR' 16 13\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: STREAM 'cHRM' 41 32\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: 'cHRM' 41 32 (unknown)\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: STREAM 'PLTE' 85 768\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: STREAM 'tRNS' 865 1\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: STREAM 'bKGD' 878 1\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: 'bKGD' 878 1 (unknown)\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: STREAM 'tEXt' 891 50\n2017-08-28 11:14:23 [PIL.PngImagePlugin] DEBUG: STREAM 'IDAT' 953 10120\nPNG png 3333333333", "issue_status": "Closed", "issue_reporting_time": "2017-08-28T03:22:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "553": {"issue_url": "https://github.com/scrapy/scrapy/issues/2899", "issue_id": "#2899", "issue_summary": "Can not extract second <body> by xpath or css in multiple <body> page.", "issue_description": "qwIvan commented on Aug 25, 2017 \u2022\nedited\nsuch as this\nThis page contain 3 <body> tags, but I can only extract the first one by response.xpath('body') or response.css('*')", "issue_status": "Closed", "issue_reporting_time": "2017-08-25T07:13:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "554": {"issue_url": "https://github.com/scrapy/scrapy/issues/2896", "issue_id": "#2896", "issue_summary": "SpiderMiddleware.process_spider_exception() did not handle exception of spider", "issue_description": "RonaldinhoL commented on Aug 24, 2017\nwhen all spider middware's process_spider_input() has finish, scrapy call Scraper.call_spider(), then call the spider' callback, but it just return a generater, in Scraper.handle_spider_output() it iter the generater, then spider raise exception\uff0c scrapy direct call Scraper.handle_spider_error(), SpiderMiddleware have no chance to handle the exception. it just handle the exception raise by SpiderMiddleware.process_spider_input() .", "issue_status": "Closed", "issue_reporting_time": "2017-08-24T02:28:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "555": {"issue_url": "https://github.com/scrapy/scrapy/issues/2891", "issue_id": "#2891", "issue_summary": "ur'strings' are a Syntax Error in Python 3", "issue_description": "Contributor\ncclauss commented on Aug 21, 2017\nYou can have u'strings' or you can have r'strings' but you can not have ur'strings' in Python 3.\nflake8 testing of https://github.com/scrapy/scrapy on Python 3.6.2\n$ flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics\n./docs/conf.py:194:51: E901 SyntaxError: invalid syntax\n  ('index', 'Scrapy.tex', ur'Scrapy Documentation',\n                                                  ^\n\n./docs/utils/linkfix.py:23:72: E901 SyntaxError: invalid syntax\nline_re = re.compile(ur'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))')\n                                                                       ^", "issue_status": "Closed", "issue_reporting_time": "2017-08-21T15:54:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "556": {"issue_url": "https://github.com/scrapy/scrapy/issues/2890", "issue_id": "#2890", "issue_summary": "Custom settings do not showup in the Log so i am not sure if it is applied.", "issue_description": "archfch2 commented on Aug 21, 2017 \u2022\nedited\nI use scrapy 1.3 According to documentation I can declare settings for each specific spider simple enough. Here is my code\nclass BhSpider(scrapy.Spider): \n      name = \"code\"  \n      custom_settings = {'CONCURRENT_REQUESTS' : '20',                   \n                         'FEED_EXPORT_FIELDS' :['price', 'stock','partnumber','sku', 'name' ,'manufacture','attribute','distributor','upc','descr', 'p_url','main_image','images']}     \nallowed_domains = *** \nBut according to logs project settings are used. How do i know project settings are overcomed by this specific spider settings?here is the log\n2017-08-20 13:47:51 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: pc) 2017-08-20 13:47:51 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'pc.spiders' , 'FEED_URI': 'test_output1.csv', 'CONCURRENT_REQUESTS': 200, 'SPIDER_MODULES': ['pc.spiders'], 'BO T_NAME': 'pc', 'USER_AGENT': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) C hrome/59.0.3071.115 Safari/537.36', 'FEED_FORMAT': 'csv', 'FEED_EXPORT_FIELDS': ['price', 'stock', 'partnumber', 'sku', 'name', 'manufacture', 'attribute', 'distributor', 'upc', 'descr']} 2017-08-20 13:47:51 [scrapy.middleware] INFO: Enabled extensions:\nIn the output file Export fields seem to be correct( as in custom settings, though log does not show that). How can i know that specific settings are enabled?\n-- | --", "issue_status": "Closed", "issue_reporting_time": "2017-08-21T06:09:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "557": {"issue_url": "https://github.com/scrapy/scrapy/issues/2888", "issue_id": "#2888", "issue_summary": "Scrapy returns importError: can't import etreee for windows", "issue_description": "ajwad605 commented on Aug 21, 2017\nI am having issue with using Scrapy on windows. I use Anaconda on my Windows and installed scrapy following the instructions on documentation. When I tried to use scrapy on the cmd its failed as it could not import etree. I am not sure if the issue is with my paths or something else. Would appreciate any advice!", "issue_status": "Closed", "issue_reporting_time": "2017-08-20T22:35:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "558": {"issue_url": "https://github.com/scrapy/scrapy/issues/2886", "issue_id": "#2886", "issue_summary": "ItemLoader.add_value()/add_xpath() should append (not replace) a value to the same key.", "issue_description": "zaazbb commented on Aug 19, 2017\nmy code:\nimport scrapy\nfrom scrapy.loader import ItemLoader\n\nclass Product(scrapy.Item):\n    name = scrapy.Field()\n    test = scrapy.Field()\n\nloader = ItemLoader(Product())\nloader.add_value('name', '1111111111')\nloader.add_value('name', '222222222')\nloader.add_value('test', 'aaaaaaaaa')\nitem = loader.load_item()\nprint(item)\n\nloader = ItemLoader(item)\nloader.add_value('name', '333333333')\nitem = loader.load_item()\nprint(item)\noutput:\n{'name': ['1111111111', '222222222'], 'test': ['aaaaaaaaa']}\n{'name': ['333333333'], 'test': ['aaaaaaaaa']}\nI expect '333333333' append to the key 'name', but it replace the value.\nMy spider need get lots of urls to download from different pages, so, i need append values to the same key, in different page paser.\nWhat can i do, to do that??\nthank you for your replay.", "issue_status": "Closed", "issue_reporting_time": "2017-08-19T03:55:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "559": {"issue_url": "https://github.com/scrapy/scrapy/issues/2885", "issue_id": "#2885", "issue_summary": "Asynchronously query database for next URL to request", "issue_description": "apnewberry commented on Aug 18, 2017 \u2022\nedited\nThanks for all your great work on the Scrapy project -- I'm a huge fan!\nI have a remote database containing a job queue of URLs I want to scrape. In order to distribute the work efficiently, I want the Scrapy process to asynchronously query the database for the next URL to scrape.\nThe approach I found most natural was to yield Deferred database queries from start_requests, as in the code below. I'm using alchimia, which combines sqlalchemy with Twisted. However, I get an exception indicating that start_requests can't yield a Deferred.\n\"\"\"Asynchronously get next url to request.\"\"\"\n\nimport os\nimport scrapy\nimport alchimia\nfrom sqlalchemy import create_engine\nfrom twisted.internet import reactor\nfrom twisted.internet.defer import Deferred\n\n\ndb_url = os.environ['PGURL']\nengine = create_engine(\n    db_url, reactor=reactor, strategy=alchimia.TWISTED_STRATEGY\n)\n\n\nclass JobQueueSpider(scrapy.Spider):\n\n    def start_requests(self):\n\n        while True:\n\n            d = Deferred()\n            d.addCallbacks(engine.execute, 'pop_next_url()')\n            d.addCallbacks(alchimia.engine.TwistedResultProxy.scalar)\n            d.addCallback(scrapy.Request)\n            yield d\nAn alternative approach is\nyield fake requests to localhost from start_requests\nuse a downloader middleware to intercept each fake request and replace it with a Deferred query to the database for the next URL\nadd a callback to the database query to make a scrapy request from it\nthis approach works, but it seems quite roundabout.\nAm I missing something? Is there a clean way to asynchronously, one-at-a-time query a database for the next URL to request? If not, might it be changed so start_requests can yield Deferreds?\nThanks again!\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-08-17T19:43:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "560": {"issue_url": "https://github.com/scrapy/scrapy/issues/2883", "issue_id": "#2883", "issue_summary": "Feature Suggest: add the ability to insert meta fields when calling from `scrapy parse`", "issue_description": "Contributor\nIAlwaysBeCoding commented on Aug 14, 2017\nI find myself doing a lot of scrapy parse when testing out my parsing methods on the spiders that I develop.\nHowever, there is this one thing that I really hate that scrapy doesn't have.\nThe ability to insert meta fields coming from the response.meta when using it from the scrapy parse command.\nFor example, say I'm scraping an ecommerce site.\nA method such as parse_product_detail_page can have the category_id meta key coming from the previous Request which is not available unless scrapy crawl was used.\nI'm doing a lot of commenting out my parse functions like this\n#category_id = response.meta.get('category_id', None)\ncategory_id = 3183\nAll because I cannot pass category_id into meta when calling from scrapy parse.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-08-14T17:14:20Z", "fixed_by": "#2957", "pull_request_summary": "[MRG+1] Scrapy Command: add --meta/-m to the \"parse\" command to pass additional meta data into the request", "pull_request_description": "Contributor\nIAlwaysBeCoding commented on Oct 10, 2017 \u2022\nedited\nThis should fix issue 2883 which I opened a couple of months ago.\nThe gist of this commit is to allow the ability to pass extra meta data onto a request.\nExample:\nscrapy parse --callback=parse_category_page --meta='{\"category\" : \"electronics\", \"sub-category\" : \"tablets\"}' \"https://some-ecommerce-website.com/electronics-38df9s9/tablets-3fsfs8\"\n... some spider code\n\ndef parse_category_page(self, response):\n    category = response.meta.get('category', None)\n    sub_category = response.meta.get('sub-category', None)\n\n    # Do some parsing here \n    yield {'product_name' : 'ipad' , 'category' : category, 'sub_category' : sub_category}\n\n... more spider code\nThe idea is that as you start testing out your spider individual parsing methods, you want to inject custom values to test individual pages.\nI currently do a lot of scraping with ecommerce, food review sites, even job boards. I need to test randomly picked pages, and different categories.\nAs everybody knows, hard coding values like @Digenis suggested is not the right way to do things.\nI also did a little bit of refactoring that might need some additional testing. @kmike do you know how I can build tests to test out the 2 new refactored methods that I created: process_spider_arguments and process_request_meta ?", "pull_request_status": "Merged", "issue_fixed_time": "2017-11-29T19:26:51Z", "files_changed": [["3", "docs/topics/commands.rst"], ["23", "scrapy/commands/parse.py"], ["39", "tests/test_command_parse.py"]]}, "561": {"issue_url": "https://github.com/scrapy/scrapy/issues/2880", "issue_id": "#2880", "issue_summary": "HttpErrorMiddleware should log as error not as debug", "issue_description": "umrashrf commented on Aug 11, 2017\nSo the middleware is called httperror but it logs failed requests as debug and not error.\nAdd a setting called REQUEST_SEVERITY_LEVEL and based on it log failed request as error?", "issue_status": "Closed", "issue_reporting_time": "2017-08-11T07:45:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "562": {"issue_url": "https://github.com/scrapy/scrapy/issues/2878", "issue_id": "#2878", "issue_summary": "About the RedirectMiddleware\u2014\u2014safe_url_string(response.headers['location'])", "issue_description": "mahaoyang commented on Aug 10, 2017 \u2022\nedited\nWhen I got a long redirect_url\uff0cthe RedirectMiddleware \u2014\u2014\u2014\u2014at code line 71th\nlocation = safe_url_string(response.headers['location'])\nwill strip \u201chttps://\u201d to \u201cps://\u201d\nand then got an scrapy schema error", "issue_status": "Closed", "issue_reporting_time": "2017-08-10T11:30:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "563": {"issue_url": "https://github.com/scrapy/scrapy/issues/2877", "issue_id": "#2877", "issue_summary": "how to change ip with scrapy when one network card have 10 ip?", "issue_description": "yongliangliu commented on Aug 10, 2017\nmy linux network card have 10 ip, how to change i with scrapy ,could you tell me ?", "issue_status": "Closed", "issue_reporting_time": "2017-08-10T03:36:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "564": {"issue_url": "https://github.com/scrapy/scrapy/issues/2874", "issue_id": "#2874", "issue_summary": "The program does not close automatically\uff1f", "issue_description": "lovebaicai commented on Aug 7, 2017\nScrapy-redis framework, redis stored xxx: requests have been crawled finished, but the program is still running, how to automatically stop the program, rather than has been running?\n2017-08-07 09:17:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-08-07 09:18:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\nPlease help me.Ths!", "issue_status": "Closed", "issue_reporting_time": "2017-08-07T06:17:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "565": {"issue_url": "https://github.com/scrapy/scrapy/issues/2872", "issue_id": "#2872", "issue_summary": "how can i generate item in list page and detail page separately?", "issue_description": "vivianjia714 commented on Aug 7, 2017\nfor ex,\nin list page i will get some item[''] to generate, the item need to be looped in....\nthen i will yield a request to a new detail page to get other item[''] to generate,\nhow can i handle this situition?", "issue_status": "Closed", "issue_reporting_time": "2017-08-07T03:50:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "566": {"issue_url": "https://github.com/scrapy/scrapy/issues/2871", "issue_id": "#2871", "issue_summary": "why scrapy_splash don't extract link addr by rules list??? need help.", "issue_description": "jekoie commented on Aug 4, 2017 \u2022\nedited by redapple\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy_splash import SplashRequest\n\nclass FangSpider(CrawlSpider):\n    name = 'fang'\n    rules = [Rule(LinkExtractor(allow=(r'/loupan/pg\\d+/') ), callback='parse_item', follow=True , process_request='splash_request')]\n\n    def start_requests(self):\n        urls = ['http://sz.fang.lianjia.com/loupan/']\n        yield SplashRequest(urls[0], args={'wait': 0.5} )\n\n    def splash_request(self, request):\n       return SplashRequest(request.url, args={'wait': 0.5})\n\n    def parse_start_url(self, response):\n        print '------start', type(response), response.url\n\n    def parse_item(self, response):\n        print '----------parse', type(response), response.url\nLogs are here:\nD:\\Demo\\scrapy\\fang>scrapy crawl fang\n2017-08-04 14:45:55 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: fang)\n2017-08-04 14:45:55 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'fang.spiders', 'ROBOTSTXT_OBEY': True, 'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter', 'SPIDER_MODULES': ['fang.spiders'], 'BOT_NAME': 'fang', 'HT\nTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage'}\n2017-08-04 14:45:55 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2017-08-04 14:45:55 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy_splash.SplashCookiesMiddleware',\n 'scrapy_splash.SplashMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-08-04 14:45:55 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy_splash.SplashDeduplicateArgsMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-08-04 14:45:55 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-08-04 14:45:55 [scrapy.core.engine] INFO: Spider opened\n2017-08-04 14:45:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-08-04 14:45:55 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-08-04 14:45:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sz.fang.lianjia.com/robots.txt> (referer: None)\n2017-08-04 14:45:55 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://192.168.99.100:8050/robots.txt> (referer: None)\n2017-08-04 14:45:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://sz.fang.lianjia.com/loupan/ via http://192.168.99.100:8050/render.html> (referer: None)\n------start <class 'scrapy_splash.response.SplashTextResponse'> http://sz.fang.lianjia.com/loupan/\n2017-08-04 14:45:57 [scrapy.core.engine] INFO: Closing spider (finished)\n2017-08-04 14:45:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 1120,\n 'downloader/request_count': 3,\n 'downloader/request_method_count/GET': 2,\n 'downloader/request_method_count/POST': 1,\n 'downloader/response_bytes': 118767,\n 'downloader/response_count': 3,\n 'downloader/response_status_count/200': 2,\n 'downloader/response_status_count/404': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2017, 8, 4, 6, 45, 57, 454000),\n 'log_count/DEBUG': 4,\n 'log_count/INFO': 7,\n 'response_received_count': 3,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'splash/render.html/request_count': 1,\n 'splash/render.html/response_count/200': 1,\n 'start_time': datetime.datetime(2017, 8, 4, 6, 45, 55, 736000)}\n2017-08-04 14:45:57 [scrapy.core.engine] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2017-08-04T06:49:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "567": {"issue_url": "https://github.com/scrapy/scrapy/issues/2870", "issue_id": "#2870", "issue_summary": "Xpath yields empty list when iterating through sibling nodes using a wildcard", "issue_description": "huangwaylon commented on Aug 4, 2017\nI'm iterating over the sibling nodes of an element. However, I'm having trouble using the xpath function to extract the data within. I seem to always obtain an empty list.\n# Determine if the cast members are listed in a table or unordered list.\ncast_siblings = response.xpath('//h2/span[starts-with(@id,\"Cast\")]/../following-sibling::*')\nfor sibling in cast_siblings:\n desc_str = \"\".join(sibling.xpath('./descendant::text()').extract())\n if \"Trailer\" in desc_str or \"Film Festival\" in desc_str or \"Comment\" in desc_str:\n  break\n\n # Extract the name to get the type of the node.\n node_type = sibling.xpath('name()').extract_first()\n if node_type == 'ul':\n  cast_list = sibling.xpath('./ul[1]/li')\n  self.parse_list_into_cast_table(cast_list, _id, conn)\n elif node_type == 'table':\n  cast_tables = sibling.xpath('./table')\n  self.parse_table_into_cast_table(cast_tables, _id, conn)\n else:\n  print \"Unknown cast element type! \" + _id\nIn the above code, I keep going through the sibling nodes until I hit an element which contains \"Trailer\" \"Film Festival\" or \"Comments\" somewhere in the text.\nOtherwise, for each sibling node, I first get its name to find out what kind of node it is - I'm interested in tables and unordered lists.\nHowever, the part that fails is when I call sibling.xpath('./table') and/or sibling.xpath('ul[1]/li').\nThe resulting object is always an empty list.\nI have used scrapy shell and examined the list of selectors that I get from cast_siblings. Each selector seems valid and when I extract them, I can see the inner elements. I just can't get to them using xpath.\nHere's an excerpt from my console showing the empty list result:\n>>> a = response.xpath('//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*')\n>>> a\n[<Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<h3> <span class=\"mw-headline\" id=\"Sudo_'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table border=\"0\" cellspacing=\"3\" cellpa'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table border=\"0\" cellspacing=\"3\" cellpa'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<h3> <span class=\"mw-headline\" id=\"Maruy'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table border=\"0\" cellspacing=\"3\" cellpa'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<h3> <span class=\"mw-headline\" id=\"Azumi'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table border=\"0\" cellspacing=\"3\" cellpa'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table border=\"0\" cellspacing=\"3\" cellpa'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table border=\"0\" cellspacing=\"3\" cellpa'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<h3> <span class=\"mw-headline\" id=\"Prese'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table border=\"0\" cellspacing=\"3\" cellpa'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<p><b>Additional Cast Members:</b>\\n</p>'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<ul><li> <a href=\"/Yukari_Ito\" title=\"Yu'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<h2> <span class=\"mw-headline\" id=\"Trail'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<p>\\n<script type=\"text/javascript\" src=\"'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<div id=\"mediaplayer\"></div>'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<script type=\"text/javascript\">\\n  jwplay'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<h2> <span class=\"mw-headline\" id=\"Comme'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<br>'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<div id=\"comment-outer\"><hr><div id=\"com'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<div align=\"center\"></div>'>, <Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table cellpadding=\"2\" style=\"border:0px'>]\n>>> a[0]\n<Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<h3> <span class=\"mw-headline\" id=\"Sudo_'>\n>>> a[1]\n<Selector xpath='//h2/span[starts-with(@id,\"Cas\")]/../following-sibling::*' data=u'<table border=\"0\" cellspacing=\"3\" cellpa'>\n>>> a[1].xpath('./table')\n[]\nThis is really strange since a[1] in the above example clearly shows that a table is contained within the selector.", "issue_status": "Closed", "issue_reporting_time": "2017-08-04T03:57:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "568": {"issue_url": "https://github.com/scrapy/scrapy/issues/2868", "issue_id": "#2868", "issue_summary": "why crawspider couldnot find corresponding href link by rules list???", "issue_description": "jekoie commented on Aug 3, 2017\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\n\nclass FangSpider(CrawlSpider):\n    name = 'fang'\n    start_urls = ['http://sz.fang.lianjia.com/loupan/']\n\n    rules = [Rule(LinkExtractor(allow=(r'/loupan/pg\\d+/') ), callback='parse_item', follow=True) ]\n\n    def parse_start_url(self, response):\n        print 'start', response.url\n\n\n    def parse_item(self, response):\n        print 'parse', response.url\nIn parse_item() function, the spdier could't find similar /loupang/pg3 link addr?", "issue_status": "Closed", "issue_reporting_time": "2017-08-03T09:31:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "569": {"issue_url": "https://github.com/scrapy/scrapy/issues/2863", "issue_id": "#2863", "issue_summary": "SyntaxError: invalid syntax when installing from pip, python3.5", "issue_description": "gajop commented on Jul 28, 2017 \u2022\nedited\n$ pip install scrappy\nCollecting scrappy\n  Using cached Scrappy-0.3.0.alpha.4.tar.gz\nCollecting guessit (from scrappy)\n  Using cached guessit-2.1.4.tar.gz\nCollecting tvdb_api (from scrappy)\n  Using cached tvdb_api-1.10.tar.gz\nCollecting hachoir-metadata (from scrappy)\n  Using cached hachoir-metadata-1.3.3.tar.gz\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n      File \"<string>\", line 1, in <module>\n      File \"/tmp/pip-build-e218slx9/hachoir-metadata/setup.py\", line 65\n        except OSError, err:\n                      ^\n    SyntaxError: invalid syntax\n    \n    ----------------------------------------\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-e218slx9/hachoir-metadata/\nEdit: it installs fine on python 2.7", "issue_status": "Closed", "issue_reporting_time": "2017-07-28T15:35:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "570": {"issue_url": "https://github.com/scrapy/scrapy/issues/2862", "issue_id": "#2862", "issue_summary": "DeprecationWarning: inspect.getargspec()", "issue_description": "Contributor\nandreip commented on Jul 28, 2017\nHi,\nI've tried searching for a similar problem but couldn't find a discussion in scrapy about it, so starting one.\nbuild  \u2502   func_args, _, _, _ = inspect.getargspec(func)\nbuild  \u2502 /usr/local/lib/python3.6/site-packages/scrapy/utils/python.py:201: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\nWe've stumbled into this log a LOT, actually getting spammed by it currently when we execute some twisted.trial.unittest tests where each unittest spins up a CrawlerRunner for our live tests that do exactly what unittests do but with live data fetching.\nCould we leave the current behavior for py2 but port the getargspec to getfullargspec as is suggested in the warning? I see latter for py3 includes additional return values, but should be a quick port. Would you be ok with a patch? Thanks", "issue_status": "Closed", "issue_reporting_time": "2017-07-28T15:02:34Z", "fixed_by": "#2864", "pull_request_summary": "Use getfullargspec under the scenes for py3 to stop DeprecationWarning", "pull_request_description": "Contributor\nandreip commented on Jul 29, 2017\nfixes #2862\nI also ran tox for the tests, but there are slight differences for the packages used inside the library, so in tests/requirements.txt:\nnetlib can be easily updated to 0.17 to support both py2 and py3\nmitmproxy on the other hand can't w/o some modifications; its version 0.18+ that supports both py3 and py2 doesn't contain libmproxy that's used in tests/test_proxy_connect.py. So yea, related to #2545 actually, so nevermind.\nOther than that issue, everything worked fine, no errors for py2 nor py3.\nAlso I tested the branch in our project and the DeprecationWarning has disappeared now :).\n\ud83c\udf89 1", "pull_request_status": "Merged", "issue_fixed_time": "2017-08-01T14:14:43Z", "files_changed": [["27", "scrapy/utils/python.py"]]}, "571": {"issue_url": "https://github.com/scrapy/scrapy/issues/2861", "issue_id": "#2861", "issue_summary": "High amount of Request to add per parsed page blocks", "issue_description": "wpxgit commented on Jul 28, 2017\nI've a crawl where every subpage has 300+ links on it.\nIt slows down after a minute or so / sometimes it crawls 0 pages / min.\nI've configured 10 concurrent_requests and 10 processed items. That means up to 3.000 yields per 10 processed items...\nAs far as i can see the parse function takes 70+ seconds per page.\nThat long time results from yielding these 300+ links and each of the links seems to wait until the engine? or something similar has done one task and is ready to process the new yield request?\nAdding the requests to scheduler don't take long so it seems to me that the yield is waiting for something other.\nAny hints?\nIs it possible to yield the requests in bulk instead of yielding each on its own.\nIs it possible to add them to scheduler without yielding them?\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2017-07-28T12:27:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "572": {"issue_url": "https://github.com/scrapy/scrapy/issues/2860", "issue_id": "#2860", "issue_summary": "There is a problem with scrapy 1.4 document", "issue_description": "sunxiaozheng commented on Jul 28, 2017 \u2022\nedited by kmike\nhttps://doc.scrapy.org/en/1.4/topics/loaders.html\nl.add_xpath('name', '//div[@Class=\"product_title\"]') should be l.add_xpath('title', '//div[@Class=\"product_title\"]')\nthe code below:\nfrom scrapy.loader import ItemLoader\nfrom myproject.items import Product\n\ndef parse(self, response):\n    l = ItemLoader(item=Product(), response=response)\n    l.add_xpath('name', '//div[@class=\"product_name\"]')\n    l.add_xpath('name', '//div[@class=\"product_title\"]')\n    l.add_xpath('price', '//p[@id=\"price\"]')\n    l.add_css('stock', 'p#stock]')\n    l.add_value('last_updated', 'today') # you can also use literal values\n    return l.load_item()\n\ud83d\ude04 1", "issue_status": "Closed", "issue_reporting_time": "2017-07-28T08:52:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "573": {"issue_url": "https://github.com/scrapy/scrapy/issues/2859", "issue_id": "#2859", "issue_summary": "ERROR: Error processing {'image_url': [u'https://imgsa.baidu.com/forum/w%3D580/sign=b78ca5208dd6277fe912323018391f63/80048326cffc1e17ba0fbdaa4090f603728de922.jpg',", "issue_description": "youleihuang commented on Jul 28, 2017 \u2022\nedited by redapple\nI want to download some photo from this website, when I get the photo's url I find there are some problems ,some photo can download ,other can not.\n#coding:utf-8\nfrom scrapy.spiders import Spider\nfrom scrapy.selector import Selector\nfrom jianshu.items import JianshuItem\nimport scrapy\nfrom scrapy.crawler  import  CrawlerProcess\n\n\nclass jiansider(Spider):\n name = \"jiantu\"\n allowed_domains = []\n start_urls= [\n \"https://tieba.baidu.com/p/5227563995\"\n ]\n\n\n def parse(self, response):\n\n  sel = Selector(response)\n  sites = sel.xpath('//div/img/@src').extract()\n  item = JianshuItem()\n  item['image_url'] = response.xpath('//div/img/@src').extract()\n  for url in item['image_url']:\n\n\n  \n   list_photo = url.split('.')\n   photo_type = list_photo[len(list_photo)-1]\n   print photo_type\n   if photo_type != 'jpg':\n    #print url\n    item['image_url'].remove(url)\n    #print \"delete1\"\n\n  \n  yield item\n  total_page = response.xpath('//span[@class=\"red\"]/text()').extract()\n  now_page = response.xpath('//li/span[@class=\"tP\"]/text()').extract()\n  tpa=total_page[len(total_page)-1]\n  npa=now_page[len(now_page)-1]\n  tpage= int(tpa)\n  npage= int (npa)\n  starturls = 'https://tieba.baidu.com/p/5227563995?pn='\n  if npage != tpage:\n   npage = npage+1\n   new_url = '%s%s'%(starturls,npage)\n   print \"new_url is ------------\"\n   print new_url  \n  if new_url:\n   yield  scrapy.Request(new_url,callback = self.parse)\n  2017-07-27 16:39:41 [scrapy.core.scraper] ERROR: Error processing {'image_url': [u'https://imgsa.baidu.com/forum/w%3D580/sign=b78ca5208dd6277fe912323018391f63/80048326cffc1e17ba0fbdaa4090f603728de922.jpg',\n               u'https://gsp0.baidu.com/5aAHeD3nKhI2p27j8IqW0jdnxx1xbK/tb/editor/images/client/image_emoticon12.png',\n               u'https://gsp0.baidu.com/5aAHeD3nKhI2p27j8IqW0jdnxx1xbK/tb/editor/images/client/image_emoticon1.png',\n               u'https://gsp0.baidu.com/5aAHeD3nKhI2p27j8IqW0jdnxx1xbK/tb/editor/images/client/image_emoticon27.png',\n               u'https://gsp0.baidu.com/5aAHeD3nKhI2p27j8IqW0jdnxx1xbK/tb/editor/images/client/image_emoticon28.png',\n               u'//tb2.bdstatic.com/tb/static-pb/img/loading_69032b0.gif',\n               u'//tb2.bdstatic.com/tb/static-pb/img/loading_69032b0.gif',\n               u'https://imgsa.baidu.com/forum/w%3D580/sign=57f4bac7865494ee87220f111df5e0e1/faac203fb80e7becf0cd3899252eb9389a506be4.jpg',\n               u'https://imgsa.baidu.com/forum/w%3D580/sign=5dbaf84b798da9774e2f86238051f872/41af5bafa40f4bfbecc7a52c094f78f0f63618db.jpg',\n               u'//tb2.bdstatic.com/tb/static-pb/img/loading_69032b0.gif',\n               u'https://gsp0.baidu.com/5aAHeD3nKhI2p27j8IqW0jdnxx1xbK/tb/editor/images/client/image_emoticon15.png',\n               u'//tb2.bdstatic.com/tb/static-pb/img/loading_69032b0.gif',\n               u'//tb2.bdstatic.com/tb/static-pb/img/loading_69032b0.gif',\n               u'//tb2.bdstatic.com/tb/static-pb/img/loading_69032b0.gif',\n               u'https://gsp0.baidu.com/5aAHeD3nKhI2p27j8IqW0jdnxx1xbK/tb/editor/images/client/image_emoticon25.png',\n               u'https://imgsa.baidu.com/forum/pic/item/8af90823dd54564e654a0ea2b9de9c82d0584fa8.jpg',\n               u'//tb2.bdstatic.com/tb/static-pb/img/loading_69032b0.gif',\n               u'//tb1.bdstatic.com/tb/cms/dalibao.png']}\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 653, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/Users/youleihuang/pythontestku/jianshu/jianshu/pipelines.py\", line 25, in process_item\n    conn = urllib.urlopen(image_url)\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.py\", line 87, in urlopen\n    return opener.open(url)\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.py\", line 213, in open\n    return getattr(self, name)(url)\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.py\", line 467, in open_file\n    return self.open_ftp(url)\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.py\", line 550, in open_ftp\n    ftpwrapper(user, passwd, host, port, dirs)\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.py\", line 877, in __init__\n    self.init()\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.py\", line 886, in init\n    self.ftp.connect(self.host, self.port, self.timeout)\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ftplib.py\", line 135, in connect\n    self.sock = socket.create_connection((self.host, self.port), self.timeout)\n  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.py\", line 575, in create_connection\n    raise err\nIOError: [Errno ftp error] [Errno 60] Operation timed out", "issue_status": "Closed", "issue_reporting_time": "2017-07-28T00:00:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "574": {"issue_url": "https://github.com/scrapy/scrapy/issues/2856", "issue_id": "#2856", "issue_summary": "Scrapy crawl spider didnt see links on ubuntu 16.04 but work fine with macOS sierra 10.12.6", "issue_description": "komuher commented on Jul 27, 2017 \u2022\nedited by redapple\nSo Im trying to scrapy opineo.pl everything works fine on mac. But when I'm trying to run exactly same script with exactly same packages installed on ubuntu he just did not see any links on site.\nEvery spider works fine on ubuntu (23 unique sites) except this for opineo\n(which work fine on macos).\nubuntu version:\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\nLogs\n2017-07-27 15:33:15 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: duck)\n2017-07-27 15:33:15 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'duck', 'DOWNLOAD_DELAY': 0.5, 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter', 'FEED_EXPORT_ENCODING': 'utf-8', 'NEWSPIDER_MODULE': 'scraper.spiders', 'REDIRECT_ENABLED': False, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'SPIDER_MODULES': ['scraper.spiders'], 'USER_AGENT': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:10.0.2) Gecko/20100101 Firefox/10.0.2'}\n2017-07-27 15:33:15 [py.warnings] WARNING: /home/scrapy/anaconda3/lib/python3.6/site-packages/scrapy_sentry/extensions.py:10: ScrapyDeprecationWarning: Module `scrapy.log` has been deprecated, Scrapy now relies on the builtin Python library for logging. Read the updated logging entry in the documentation to learn more.\n  from scrapy import signals, log\n\n2017-07-27 15:33:15 [raven.base.Client] DEBUG: Configuring Raven for host: <raven.conf.remote.RemoteConfig object at 0x6bcb95219b70>\n2017-07-27 15:33:15 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.logstats.LogStats',\n 'scraper.sentry.CustomErrors']\n2017-07-27 15:33:15 [opineo] INFO: Reading start URLs from redis key 'opineo:start_urls' (batch size: 16, encoding: utf-8\n2017-07-27 15:33:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scraper.middlewares.SplashyRequestMiddleware',\n 'scrapy_splash.SplashCookiesMiddleware',\n 'scrapy_splash.SplashMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-07-27 15:33:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scraper.middlewares.DisqusParser',\n 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scraper.middlewares.SaveLogs',\n 'scraper.middlewares.SaveItems',\n 'scraper.middlewares.GenderGuessingMiddleware',\n 'scraper.middlewares.ValidationMiddleware',\n 'scraper.middlewares.ParentUIDMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scraper.middlewares.ReviewMiddleware',\n 'scraper.middlewares.SaveAllLinksMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-07-27 15:33:15 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-07-27 15:33:15 [scrapy.core.engine] INFO: Spider opened\n2017-07-27 15:33:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-07-27 15:33:15 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-07-27 15:33:15 [opineo] DEBUG: Read 1 requests from 'opineo:start_urls'\n2017-07-27 15:33:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.opineo.pl/> (referer: None)\n2017-07-27 15:33:16 [root] INFO: review middleware 2017-07-27 15:33:16.174427\n2017-07-27 15:33:16 [root] INFO: save all links middleware 2017-07-27 15:33:16.176983\n2017-07-27 15:33:16 [root] INFO: RESPONSE STATUS= 200, response url = https://www.opineo.pl/\n2017-07-27 15:33:16 [root] INFO: add 0 new links to database\n2017-07-27 15:33:16 [root] INFO: https://www.opineo.pl/ review lvl = 6, start lvl = 6\n2017-07-27 15:33:16 [root] INFO: {'download_timeout': 180.0, 'download_slot': 'www.opineo.pl', 'download_latency': 0.2444303035736084, 'depth': 0}\n...\n\n2017-07-27 15:34:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)\n2017-07-27 15:35:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-07-27 15:36:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n^C2017-07-27 15:37:05 [scrapy.crawler] INFO: Received SIG_SETMASK, shutting down gracefully. Send again to force\n2017-07-27 15:37:05 [scrapy.core.engine] INFO: Closing spider (shutdown)\n2017-07-27 15:37:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 258,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 155,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2017, 7, 27, 13, 37, 5, 767638),\n 'log_count/DEBUG': 4,\n 'log_count/INFO': 23,\n 'log_count/WARNING': 1,\n 'response_received_count': 1,\n 'scheduler/dequeued/redis': 1,\n 'scheduler/enqueued/redis': 1,\n 'start_time': datetime.datetime(2017, 7, 27, 13, 33, 15, 665473)}\n2017-07-27 15:37:05 [scrapy.core.engine] INFO: Spider closed (shutdown)\nlogs from scrapy shell ubuntu:\nscrapy shell https://www.opineo.pl/\n2017-07-27 15:45:05 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n2017-07-27 15:45:05 [scrapy.utils.log] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\n2017-07-27 15:45:05 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole']\n2017-07-27 15:45:06 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-07-27 15:45:06 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-07-27 15:45:06 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-07-27 15:45:06 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-07-27 15:45:06 [scrapy.core.engine] INFO: Spider opened\n2017-07-27 15:45:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.opineo.pl/> (referer: None)\n2017-07-27 15:45:06 [traitlets] DEBUG: Using default logger\n2017-07-27 15:45:06 [traitlets] DEBUG: Using default logger\n[s] Available Scrapy objects:\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7427209c67b8>\n[s]   item       {}\n[s]   request    <GET https://www.opineo.pl/>\n[s]   response   <200 https://www.opineo.pl/>\n[s]   settings   <scrapy.settings.Settings object at 0x7427182cab00>\n[s]   spider     <DefaultSpider 'default' at 0x74271803a0b8>\n[s] Useful shortcuts:\n[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n[s]   shelp()           Shell help (print this help)\n[s]   view(response)    View response in a browser\nIn [1]: print(response.xpath('//a/@href').extract())\n[]\nIn [2]: print(response.xpath('(//a/@href)').extract())\n[]\nShell on MacOS:\nscrapy shell https://www.opineo.pl/\n2017-07-27 15:46:55 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrapybot)\n2017-07-27 15:46:55 [scrapy.utils.log] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}\n2017-07-27 15:46:55 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole']\n2017-07-27 15:46:55 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-07-27 15:46:55 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-07-27 15:46:55 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-07-27 15:46:55 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-07-27 15:46:55 [scrapy.core.engine] INFO: Spider opened\n2017-07-27 15:46:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.opineo.pl/> (referer: None)\n2017-07-27 15:46:56 [traitlets] DEBUG: Using default logger\n2017-07-27 15:46:56 [traitlets] DEBUG: Using default logger\n[s] Available Scrapy objects:\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n[s]   crawler    <scrapy.crawler.Crawler object at 0x10c4e4cc0>\n[s]   item       {}\n[s]   request    <GET https://www.opineo.pl/>\n[s]   response   <200 https://www.opineo.pl/>\n[s]   settings   <scrapy.settings.Settings object at 0x10e733a20>\n[s]   spider     <DefaultSpider 'default' at 0x10e9c2f60>\n[s] Useful shortcuts:\n[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n[s]   shelp()           Shell help (print this help)\n[s]   view(response)    View response in a browser\nIn [1]: print(response.xpath('//a/@href').extract())\n['/agd.html', '/apteka.html', '/artykuly-zoologiczne.html', '/biuro.html', '/dla-dzieci.html', '/dom.html', '/erotyka.html', '/film-i-muzyka.html', '/fotografia.html', '/gry.html', '/hobby.html', '/komputery.html', '/ksiazki.html', '/militaria.html', '/motoryzacja.html', '/odziez-obuwie-i-dodatki.html', '/ogrod.html', '/pozostale.html', '/rtv.html', '/sport.html', '/telefony.html', '/zdrowie-i-uroda.html', '/zegarki-i-bizuteria.html', '/zywnosc.html', '/p/agd.html', '/p/zdrowie.html', '/p/biuro.html', '/p/budowa-i-remont.html', '/p/dla-dzieci.html', '/p/dom-i-wnetrze.html', '/p/film-i-muzyka.html', '/p/foto.html', '/p/gry.html', '/p/komputery.html', '/p/ksiazki.html', '/p/militaria.html', '/p/motoryzacja.html', '/p/moda.html', '/p/ogrod-i-narzedzia.html', '/p/rtv.html', '/p/sport.html', '/p/telefony.html', '/p/uroda.html', '/p/zegarki-i-bizuteria.html', '/p/zwierzeta-i-hobby.html', '/p/delikatesy.html', '/firmy/biuro.html', '/firmy/budownictwo.html', '/firmy/dom-i-ogrod.html', '/firmy/doradztwo.html', '/firmy/edukacja.html', '/firmy/ekologia.html', '/firmy/finanse-i-ubezpieczenia.html', '/firmy/firmy-kurierskie.html', '/firmy/fotografia.html', '/firmy/handel.html', '/firmy/hobby.html', '/firmy/instytucje.html', '/firmy/komputery.html', '/firmy/kultura-i-rozrywka.html', '/firmy/media.html', '/firmy/medycyna.html', '/firmy/motoryzacja.html', '/firmy/nieruchomosci.html', '/firmy/odziez-i-obuwie.html', '/firmy/poligrafia.html', '/firmy/posrednictwo-pracy.html', '/firmy/pozostale.html', '/firmy/prawo.html', '/firmy/przemysl.html', '/firmy/reklama-i-marketing.html', '/firmy/rolnictwo-i-lesnictwo.html', '/firmy/serwis.html', '/firmy/sport.html', '/firmy/telekomunikacja.html', '/firmy/transport.html', '/firmy/turystyka-i-wypoczynek.html', '/firmy/uslugi.html', '/firmy/zdrowie-i-uroda.html', '/firmy/zywnosc-i-gastronomia.html', '/', '/e-sklepy/', '/firmy/', '#', '#', '/i/dodaj-e-sklep-lub-firme&type=shop', '/i/dodaj-e-sklep-lub-firme&type=firm', '/?administracja=1&red=%2F', 'javascript:void();', 'javascript:void();', 'javascript:void();', '/opinie/ecentrum-kelton-pl', '/opinie/centrumpapieru-pl', '/opinie/centrumpapieru-pl', '/opinie/centersport-pl', '/opinie/centersport-pl', '/opinie/lens-shop-pl', '/opinie/lens-shop-pl', '/opinie/dcfoto-pl', '/opinie/dcfoto-pl', '/opinie/winezja-pl', '/opinie/winezja-pl', '/opinie/lokikoki-pl', '/opinie/lokikoki-pl', '/opinie/dekoracjadomu-pl', '/opinie/dekoracjadomu-pl', '/opinie/centrumdruku-com-pl', '/opinie/centrumdruku-com-pl', '/opinie/mixmedia-pl', '/opinie/mixmedia-pl', '/opinie/eobuwie-pl', '/opinie/decathlon-pl', '/opinie/decathlon-pl', '/opinie/apteka-melissa-pl', '/opinie/apteka-melissa-pl', '/opinie/krakvet-pl', '/opinie/krakvet-pl', '/opinie/morele-net', '/opinie/morele-net', '/opinie/zooart-com-pl', '/opinie/zooart-com-pl', '/opinie/empik-com', '/opinie/empik-com', '/opinie/wapteka-pl', '/opinie/wapteka-pl', '/opinie/manada-pl', '/opinie/manada-pl', '/opinie/wittchen', '/opinie/wittchen', 'javascript:void();', 'javascript:void();', 'javascript:void();', '/opinie-5589252-vordon-4-5.html', '/opinie-12862866-vordon-gps-7-europa.html', '/opinie-12862866-vordon-gps-7-europa.html', '/opinie-24595648-vordon-ht-869v2.html', '/opinie-24595648-vordon-ht-869v2.html', '/opinie-41978012-vordon-rg1-czarny.html', '/opinie-41978012-vordon-rg1-czarny.html', '/opinie-43042266-kamera-cofania-vordon-4smdpl.html', '/opinie-43042266-kamera-cofania-vordon-4smdpl.html', '/opinie-9940754-kamera-cofania-vordon-4smdpl.html', '/opinie-9940754-kamera-cofania-vordon-4smdpl.html', '/opinie-14594031-vordon-4smdpl-k0704.html', '/opinie-14594031-vordon-4smdpl-k0704.html', '/opinie-24595667-vordon-ht-175bt.html', '/opinie-24595667-vordon-ht-175bt.html', '/opinie-21372477-puppi-otulacz-welniany-baby-mint-v2-ot-os-bm.html', '/opinie-21372477-puppi-otulacz-welniany-baby-mint-v2-ot-os-bm.html', '/opinie-24829823-quazi-sandaly-2343-czarny-skora-naturalna-zamsz-skora-natura.html', '/opinie-24829823-quazi-sandaly-2343-czarny-skora-naturalna-zamsz-skora-natura.html', '/opinie-41978012-vordon-rg1-czarny.html', '/opinie-16685505-gaia-576.html', '/opinie-16685505-gaia-576.html', '/opinie-24595667-vordon-ht-175bt.html', '/opinie-24595667-vordon-ht-175bt.html', '/opinie-24595648-vordon-ht-869v2.html', '/opinie-24595648-vordon-ht-869v2.html', '/opinie-14594031-vordon-4smdpl-k0704.html', '/opinie-14594031-vordon-4smdpl-k0704.html', '/opinie-13383130-royal-canin-8-kg.html', '/opinie-13383130-royal-canin-8-kg.html', '/opinie-19963058-lavazza-12x-crema-e-aroma-1kg.html', '/opinie-19963058-lavazza-12x-crema-e-aroma-1kg.html', '/opinie-20287051-adidas-adidas-superstar.html', '/opinie-20287051-adidas-adidas-superstar.html', '/opinie-15355077-caprice-9-28314-26.html', '/opinie-15355077-caprice-9-28314-26.html', '/opinie-11006508-big-star-s274492-bialy.html', '/opinie-11006508-big-star-s274492-bialy.html', 'javascript:void();', 'javascript:void();', 'javascript:void();', '/opinie/www-czajkapodroze-pl', '/opinie/kabaretowebilety-pl', '/opinie/kabaretowebilety-pl', '/opinie/nkantor-pl', '/opinie/nkantor-pl', '/opinie/easyprotect-pl', '/opinie/easyprotect-pl', '/opinie/interviewme-pl', '/opinie/interviewme-pl', '/opinie/www-ekantor-pl', '/opinie/www-ekantor-pl', '/opinie/styronet-pl', '/opinie/styronet-pl', '/opinie/kantoria-com', '/opinie/kantoria-com', '/opinie/goldem-sp-z-o-o', '/opinie/goldem-sp-z-o-o', '/opinie/cinkciarz-pl', '/opinie/cinkciarz-pl', '/opinie/internetowykantor-pl', '/opinie/cinkciarz-pl', '/opinie/cinkciarz-pl', '/opinie/dpd-com-pl', '/opinie/dpd-com-pl', '/opinie/dhl-com-pl', '/opinie/dhl-com-pl', '/opinie/poczta-polska-pl', '/opinie/poczta-polska-pl', '/opinie/epaka-pl', '/opinie/epaka-pl', '/opinie/gls-poland-com', '/opinie/gls-poland-com', '/opinie/furgonetka-pl', '/opinie/furgonetka-pl', '/opinie/ups-com', '/opinie/ups-com', '/opinie/fedex-com', '/opinie/fedex-com', '/opinie/telekarma', '/email/9120266', '/opinie/telekarma', '/opinie/pancernik-eu', '/email/9120264', '/opinie/pancernik-eu', '/opinie/internetowykantor-pl', '/email/9120262', '/opinie/internetowykantor-pl', '/opinie/internetowykantor-pl', '/email/9120259', '/opinie/internetowykantor-pl', '/opinie/krakvet-pl', '/email/9120258', '/opinie/krakvet-pl', '/opinie/fedex-com', '/email/9120257', '/opinie/fedex-com', 'http://www.iai-shop.com/', 'http://sklepolandia.pl', 'http://www.skapiec.pl', 'http://www.web-market.pl', 'http://www.cstore.pl/', 'http://www.smartbay.pl', 'http://www.shoper.pl/', 'http://www.iai-shop.com/', 'http://loopa.eu', 'http://www.selly.pl/', 'http://www.mysklep.pl/', 'http://www.swistak.pl', 'http://www.goshop.pl/', 'http://muratordom.pl', 'http://www.iai-shop.com/', 'http://abstore.pl', 'http://www.icomarch24.pl/oferta/produkty/isklep24', 'http://redcart.pl/', 'http://www.atomstore.pl/', 'http://pretia.pl/', 'http://www.i-sklep.pl/', 'http://www.iai-shop.com/', 'http://www.nokaut.pl', 'http://www.sklep-szybko.pl/', 'http://gieldaelektryczna.pl/', 'http://www.rzetelnafirma.pl/', 'http://www.sklepy-internetowe.pl', '/i/dla-uzytkownika/zasady-pisania-opinii', '/i/dla-uzytkownika/pytania-faq', '/i/dla-uzytkownika/polityka-prywatnosci', '/i/dla-uzytkownika/regulamin-dla-uzytkownikow-serwisu', '/i/dla-e-sklepow/jak-zaczac-wspolprace', '/i/dla-e-sklepow/oferta', '/i/dla-e-sklepow/program-slucham-swoich-klientow', '/i/dla-e-sklepow/system-wiarygodne-opinie', '/i/dla-e-sklepow/regulamin-dla-sklepow', '/i/dla-firm/jak-zaczac-wspolprace', '/i/dla-firm/oferta', '/i/dla-firm/regulamin-dla-firm', '/i/dla-firm/program-slucham-swoich-klientow', '/i/dla-e-sklepow/znak-jakosci-q', '/i/dla-e-sklepow/program-slucham-swoich-klientow', 'https://www.youtube.com/user/SerwisOpineo', 'https://plus.google.com/+OpineoPlinfo', '/i/o-nas', '/i/aktualnosci', '/i/dla-prasy/informacje-prasowe', '/i/partnerzy', '/i/kariera', '/i/mapa-strony', '/i/kontakt', '#', '/i/informacje-o-ciastkach']\npackages mac:\ntwisted                   17.5.0                   py36_0\nlxml                      3.7.3                    py36_0\nparsel                    1.2.0                    py36_0\nw3lib                     1.17.0                   py36_0\npyopenssl                 17.0.0                   py36_0\ncryptography              1.8.1                    py36_0\nubuntu packages:\ntwisted                   17.5.0                   py36_0\npyopenssl                 17.0.0                   py36_0\nparsel                    1.2.0                    py36_0\nw3lib                     1.17.0                   py36_0\ncryptography              1.8.1                    py36_0\nlxml                      3.7.3                    py36_0", "issue_status": "Closed", "issue_reporting_time": "2017-07-27T14:03:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "575": {"issue_url": "https://github.com/scrapy/scrapy/issues/2855", "issue_id": "#2855", "issue_summary": "The order problem of start_request and scheduler queue", "issue_description": "guodeliang commented on Jul 27, 2017 \u2022\nedited\n        while not self._needs_backout(spider):\n            **if not self._next_request_from_scheduler(spider):**\n                break\n\n        if slot.start_requests and not self._needs_backout(spider):\n            try:\n                **request = next(slot.start_requests)**\n            except StopIteration:\n                slot.start_requests = None\n            except Exception:\n                slot.start_requests = None\n                logger.error('Error while obtaining start requests',\n                             exc_info=True, extra={'spider': spider})\n            else:\n                self.crawl(request, spider)\nWhen I started running more than two scrapy, start_requests confused me.\nI thought it run start_requests function first\uff0c and check the source code\u3002 I found out I was wrong\u3002\nMy judgment in start_requests was a complete failure,\nTo avoid two scrapy application running two times seed links, my start_requests code is as follows\uff1a\nBut It's nothing, because of the order of the funcitons\n    def start_requests(self):\n        request = self.crawler.engine.slot.scheduler.next_request()\n        if request:\n            yield request\n            return\n        else:\n            self.logger.info(\"\u8c03\u5ea6\u961f\u5217\u4e3a\u7a7a\uff0c\u7eed\u6dfb\u52a0\u91c7\u96c6\u79cd\u5b50\")\n\n        for media_id in self.media_list:\n            toutiao_as_value = self.get_as_cp()\n            url = media_home.format(media_id=media_id, as_param=toutiao_as_value)\n            request = Request(url, dont_filter=True,\n                              headers=self.headers,\n                              callback=self.parse_media_homepage)\n\n            self.crawler.engine.slot.scheduler.enqueue_request(request)\n\n        yield self.crawler.engine.slot.scheduler.next_request()\n    pass\n\ud83d\udc4e 1\n\ud83d\ude15 2", "issue_status": "Closed", "issue_reporting_time": "2017-07-27T09:44:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "576": {"issue_url": "https://github.com/scrapy/scrapy/issues/2853", "issue_id": "#2853", "issue_summary": "SitemapSpider support rel=\"alternate\" works not as expected", "issue_description": "Contributor\njenya commented on Jul 26, 2017 \u2022\nedited\nThis functionality was added here: #360\nAccording to this https://github.com/scrapy/scrapy/blob/master/scrapy/spiders/sitemap.py#L46 sitemap_alternate_links used for sitemap type sitemapindex, but not urlset, which is common use case for rel=\"alternate\".\nI don't think this is as expected, because that issue, docs and tests (which in fact tests xml parser, but not sitemap parser) in PR talks about urlset.\nPlease correct me if I'm wrong. Otherwise - I will prepare PR.", "issue_status": "Closed", "issue_reporting_time": "2017-07-26T13:49:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "577": {"issue_url": "https://github.com/scrapy/scrapy/issues/2851", "issue_id": "#2851", "issue_summary": "[idea] Add some non-standard but commonly used HTTP status codes for retrying", "issue_description": "Contributor\nstarrify commented on Jul 26, 2017\nCloudflare is being used widely, which has has introduced its non-standard HTTP status codes. E.g.\nError 522: Connection timed out\nError 524: A timeout occurred\nIn practice, I've seen several occurrences of 522 and/or 524 status codes generated by Cloudflare. Usually a user may simply override setting RETRY_HTTP_CODES to include these values.\nHere I suppose it might be good to include these two codes into scrapy.settings.default_settings.RETRY_HTTP_CODES.\nSee also:\nList of Croudflare error codes\nList of HTTP status codes (Wikipedia) -> Unofficial codes -> Cloudflare", "issue_status": "Closed", "issue_reporting_time": "2017-07-26T06:29:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "578": {"issue_url": "https://github.com/scrapy/scrapy/issues/2850", "issue_id": "#2850", "issue_summary": "IOError: cannot identify image file <cStringIO.StringI object at 0x0000000005796E00>", "issue_description": "MaGuiSen commented on Jul 25, 2017\ni got this problem when i use the ImagesPipeline to download some image\uff08but some image can be download\uff09...such as this image url:\nhttp://mmbiz.qpic.cn/mmbiz_png/NmpHEAE5bXl1WYvWBOQBhDeB7Dar8JhIr4tHxouT9AbhZJO7wF1lv2bCUU1UX0YPtU70OUF7RRfnMQpubRJYpA/0?wx_fmt=png\nthis image can`t be download.\ni think it is because the image type is JFIF\uff0cand the ImagesPipeline is not support .\ni dont know how to Solve this problem, please help me", "issue_status": "Closed", "issue_reporting_time": "2017-07-25T12:07:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "579": {"issue_url": "https://github.com/scrapy/scrapy/issues/2846", "issue_id": "#2846", "issue_summary": "Show warning if SPIDER_MODULES is not set", "issue_description": "decentral1se commented on Jul 24, 2017\nFollowing from scrapinghub/shub#293 (comment).\nI'm trying to find the right place to show a warning for new users where they have not set the SPIDER_MODULES settings and cannot figure out how to run their spider (or get it registered on Scrapy Cloud).\nFor context, I wrote my spider from scratch without the startproject command.", "issue_status": "Closed", "issue_reporting_time": "2017-07-24T11:33:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "580": {"issue_url": "https://github.com/scrapy/scrapy/issues/2844", "issue_id": "#2844", "issue_summary": "Redirect 308 missing", "issue_description": "maugch commented on Jul 22, 2017\nI did a check on the RedirectMiddleware and noticed that code 308 is missing. Is there a reason for that?\nSome websites don't update their sitemap and have a long list of 308 from http to https.\n(side note: is there a way to add \"s\" before a link is scraped?)", "issue_status": "Closed", "issue_reporting_time": "2017-07-22T10:30:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "581": {"issue_url": "https://github.com/scrapy/scrapy/issues/2843", "issue_id": "#2843", "issue_summary": "'scrapy startproject tutorial' return a traceback", "issue_description": "ljh19930603 commented on Jul 21, 2017\nC:\\Users\\CB34681308\\OneDrive\\scrapy>scrapy startproject tutorial\nTraceback (most recent call last):\nFile \"c:\\python27\\lib\\runpy.py\", line 174, in _run_module_as_main\n\"main\", fname, loader, pkg_name)\nFile \"c:\\python27\\lib\\runpy.py\", line 72, in run_code\nexec code in run_globals\nFile \"C:\\Python27\\Scripts\\scrapy.exe_main.py\", line 5, in\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\cmdline.py\", line 9, in\nfrom scrapy.crawler import CrawlerProcess\nFile \"c:\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 7, in\nfrom twisted.internet import reactor, defer\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\reactor.py\", line 38, in\nfrom twisted.internet import default\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\default.py\", line 56, in\ninstall = _getInstallFunction(platform)\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\default.py\", line 50, in getInstallFunction\nfrom twisted.internet.selectreactor import install\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 18, in\nfrom twisted.internet import posixbase\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\posixbase.py\", line 18, in\nfrom twisted.internet import error, udp, tcp\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet\\tcp.py\", line 28, in\nfrom twisted.internet.newtls import (\nFile \"c:\\python27\\lib\\site-packages\\twisted\\internet_newtls.py\", line 21, in\nfrom twisted.protocols.tls import TLSMemoryBIOFactory, TLSMemoryBIOProtocol\nFile \"c:\\python27\\lib\\site-packages\\twisted\\protocols\\tls.py\", line 41, in\nfrom OpenSSL.SSL import Error, ZeroReturnError, WantReadError\nFile \"c:\\python27\\lib\\site-packages\\OpenSSL_init.py\", line 8, in\nfrom OpenSSL import rand, crypto, SSL\nFile \"c:\\python27\\lib\\site-packages\\OpenSSL\\crypto.py\", line 12, in\nfrom cryptography import x509\nFile \"c:\\python27\\lib\\site-packages\\cryptography\\x509_init.py\", line 9, in\nfrom cryptography.x509.base import (\nFile \"c:\\python27\\lib\\site-packages\\cryptography\\x509\\base.py\", line 16, in\nfrom cryptography.x509.extensions import Extension, ExtensionType\nFile \"c:\\python27\\lib\\site-packages\\cryptography\\x509\\extensions.py\", line 13, in\nfrom asn1crypto.keys import PublicKeyInfo\nFile \"c:\\python27\\lib\\site-packages\\asn1crypto\\keys.py\", line 22, in\nfrom ._elliptic_curve import (\nFile \"c:\\python27\\lib\\site-packages\\asn1crypto_elliptic_curve.py\", line 51, in\nfrom ._int import inverse_mod\nFile \"c:\\python27\\lib\\site-packages\\asn1crypto_int.py\", line 56, in\nfrom ._perf._big_num_ctypes import libcrypto\nFile \"c:\\python27\\lib\\site-packages\\asn1crypto_perf_big_num_ctypes.py\", line 31, in\nlibcrypto_path = find_library('crypto')\nFile \"c:\\python27\\lib\\ctypes\\util.py\", line 53, in find_library\nfname = os.path.join(directory, name)\nFile \"c:\\python27\\lib\\ntpath.py\", line 85, in join\nresult_path = result_path + p_path\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 1: ordinal not in range(128)", "issue_status": "Closed", "issue_reporting_time": "2017-07-21T08:04:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "582": {"issue_url": "https://github.com/scrapy/scrapy/issues/2840", "issue_id": "#2840", "issue_summary": "Iterate `Selector` with index", "issue_description": "iShawnWang commented on Jul 19, 2017\nI try to iterate with Selector, and i wan't index within the loop :\n    def parseTopicTable(self, response):\n        table = response.xpath('//*[@id=\"Main\"]/div[2]')\n        cells = table.xpath('//div[@class=\"cell item\"]')\n        for cell in cells:\n            yield self.parseTopic(cell, `i need index here`)\nand i try this, but i got an error\n        for index, cell in cells:\n            yield self.parseTopic(cell, index)  # got error here ~\nso what's the right way to get index when loop the Selector", "issue_status": "Closed", "issue_reporting_time": "2017-07-19T13:09:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "583": {"issue_url": "https://github.com/scrapy/scrapy/issues/2839", "issue_id": "#2839", "issue_summary": "crawler.engine.slot's rapid reduction", "issue_description": "ru1996 commented on Jul 19, 2017 \u2022\nedited\nHi, there.I am in trouble when trying scrapy, I hope you can help me.\nScrapy (1.4.0)\nscrapy-redis (0.6.8)\nI set CONCURRENT_REQUESTS=200.And when it run, I use telnet to find len(crawler.engine.slot.inprogress)=134.\nI guess this may be related to the memory(but my memory is a lot of surplus).Can someone explain this?Thanks.\nThen, when it has being running for a while(about one hour).I find thelen(crawler.engine.slot.inprogress)=80.I read the document and get that is beacuse python memory management.\nBut how can i solve this?Because its reduction affected the crawler rate.\nHere is my thought:\nAdd an extensions: When the number drops to the half, call CrawlerProcess()._graceful_stop_reactor.And use supervisor restart it.\nIs there any other solutions?", "issue_status": "Closed", "issue_reporting_time": "2017-07-19T02:46:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "584": {"issue_url": "https://github.com/scrapy/scrapy/issues/2836", "issue_id": "#2836", "issue_summary": "Undefined name '__class__' on Python 2 only", "issue_description": "Contributor\ncclauss commented on Jul 19, 2017\nThe code quoted is embedded in a contains WithClassRef which this code does not have.\nFlake8 testing of https://github.com/scrapy/scrapy on Python 2.7.13\n$ flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics\n./tests/test_item.py:273:28: F821 undefined name '__class__'\n                    return __class__\n                           ^", "issue_status": "Closed", "issue_reporting_time": "2017-07-18T21:02:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "585": {"issue_url": "https://github.com/scrapy/scrapy/issues/2834", "issue_id": "#2834", "issue_summary": "Root log not logging to file after updating from 1.2 to 1.4", "issue_description": "tonyyherb commented on Jul 18, 2017 \u2022\nedited\nI have\nd = {\n        'LOG_LEVEL': logging.DEBUG,\n        'LOG_FORMAT': '%(levelname)s %(asctime)s %(name)s: %(message)s',\n        'LOG_FILE': 'scrapyroot'\n}\nconfigure_logging(d)\nI am using CrawlerRunner(get_project_settings()) to run spiders. The setting above used to log the scrapy root logs to file 'scrapyroot'.\nAfter updating to 1.4, scrapy root logs are printed to console; the file 'scrapyroot' is created but empty.", "issue_status": "Closed", "issue_reporting_time": "2017-07-18T14:33:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "586": {"issue_url": "https://github.com/scrapy/scrapy/issues/2833", "issue_id": "#2833", "issue_summary": "telnet console abnormal", "issue_description": "wangtua1 commented on Jul 18, 2017\nwhen I typed \u2018\u2019telnet localhost 6023\u2018\u2019,it replies like this \"escape chararctor is ']'\", and the program itself stops crawling data from the web.I can't figure out what is the matter.", "issue_status": "Closed", "issue_reporting_time": "2017-07-18T07:07:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "587": {"issue_url": "https://github.com/scrapy/scrapy/issues/2832", "issue_id": "#2832", "issue_summary": "spider.logger not working in ProjectPipeline.open_spider(self, spider)", "issue_description": "aidiss commented on Jul 17, 2017\nThe logs are not there when I try to print them.\nspider.logger it works fine when called in process_item or close_spider methods.\nAny clues?", "issue_status": "Closed", "issue_reporting_time": "2017-07-17T13:46:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "588": {"issue_url": "https://github.com/scrapy/scrapy/issues/2831", "issue_id": "#2831", "issue_summary": "Explicit message for NotImplementedError when parse callback not defined", "issue_description": "Contributor\nredapple commented on Jul 17, 2017\nMotivation: https://stackoverflow.com/questions/45113478/scrapy-request-returns-notimplementederror\nProposal: add an explicit message saying what's missing in the spider class definition.\nAlthough obvious for experienced Scrapy users, NotImplementedError for missing parse can be obscure for newcomers.\nAlso, NotImplementedError is not mentioned in the docs for this situation.", "issue_status": "Closed", "issue_reporting_time": "2017-07-17T12:52:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "589": {"issue_url": "https://github.com/scrapy/scrapy/issues/2824", "issue_id": "#2824", "issue_summary": "DOC python 3 compatible module for memory profiling", "issue_description": "Member\nlopuhin commented on Jul 13, 2017\nGuppy (mentioned here https://doc.scrapy.org/en/latest/topics/leaks.html) is python 2 only AFAIK, so it would be nice to provide a python 3 compatible alternative, e.g. pympler.readthedocs.io", "issue_status": "Closed", "issue_reporting_time": "2017-07-13T12:34:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "590": {"issue_url": "https://github.com/scrapy/scrapy/issues/2822", "issue_id": "#2822", "issue_summary": "No Module found Issue!", "issue_description": "Pac23 commented on Jul 12, 2017\nPS C:\\Windows\\system32> scrapy shell\n2017-07-12 09:30:40 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n2017-07-12 09:30:40 [scrapy.utils.log] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilte\nr', 'LOGSTATS_INTERVAL': 0}\n2017-07-12 09:30:40 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.telnet.TelnetConsole']\nTraceback (most recent call last):\nFile \"c:\\program files (x86)\\python36-32\\lib\\runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec)\nFile \"c:\\program files (x86)\\python36-32\\lib\\runpy.py\", line 85, in run_code\nexec(code, run_globals)\nFile \"C:\\Program Files (x86)\\Python36-32\\Scripts\\scrapy.exe_main.py\", line 9, in\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\cmdline.py\", line 149, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\cmdline.py\", line 89, in _run_print_help\nfunc(*a, **kw)\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\cmdline.py\", line 156, in _run_command\ncmd.run(args, opts)\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\commands\\shell.py\", line 67, in run\ncrawler.engine = crawler._create_engine()\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\crawler.py\", line 102, in create_engine\nreturn ExecutionEngine(self, lambda : self.stop())\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\core\\engine.py\", line 69, in init\nself.downloader = downloader_cls(crawler)\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader_init.py\", line 88, in init\nself.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\middleware.py\", line 58, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\middleware.py\", line 34, in from_settings\nmwcls = load_object(clspath)\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 44, in load_object\nmod = import_module(module)\nFile \"c:\\program files (x86)\\python36-32\\lib\\importlib_init.py\", line 126, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 978, in _gcd_import\nFile \"\", line 961, in _find_and_load\nFile \"\", line 950, in _find_and_load_unlocked\nFile \"\", line 655, in _load_unlocked\nFile \"\", line 678, in exec_module\nFile \"\", line 205, in _call_with_frames_removed\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\downloadermiddlewares\\retry.py\", line 20, in <module\nfrom twisted.web.client import ResponseFailed\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\twisted\\web\\client.py\", line 42, in\nfrom twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\twisted\\internet\\endpoints.py\", line 41, in\nfrom twisted.internet.stdio import StandardIO, PipeAddress\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\twisted\\internet\\stdio.py\", line 30, in\nfrom twisted.internet import _win32stdio\nFile \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\twisted\\internet_win32stdio.py\", line 9, in\nimport win32api\nModuleNotFoundError: No module named 'win32api'\nScrapy was working fine on win 7,ever since i switched to win 10 this error pop's up.", "issue_status": "Closed", "issue_reporting_time": "2017-07-12T04:02:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "591": {"issue_url": "https://github.com/scrapy/scrapy/issues/2821", "issue_id": "#2821", "issue_summary": "Catching exceptions on start urls of CrawlSpider", "issue_description": "madspark commented on Jul 11, 2017 \u2022\nedited\nHere is the problem that I am having:\nI want to crawl some URLs with CrawlSpider and I expect some of them to give DNS lookup errors.\nI want to catch these errors and handle them gracefully (log them, etc).\nFor a generic spider, this can be done as shown in the documentation:\nhttps://doc.scrapy.org/en/latest/topics/request-response.html?highlight=failure.check#using-errbacks-to-catch-exceptions-in-request-processing\nFor sublinks of my initial pages, it can be done as suggested on stackoverflow:\nhttps://stackoverflow.com/questions/35866873/scrapy-get-website-with-error-dns-lookup-failed\nBut for the initial URL visited (i.e. before rules are used for extraction), I can't figure out a way to gracefully catch errors. Best I can do is write a downloader middleware, which can at least log them.\nThe following seemed promising, but does not work - I overwrote the start_requests function to include callback and errback in the initial request. This catches the errors, but it also makes the CrawlSpider not attempt to parse subpages for valid requests. I am actually not sure if this is a bug or if I am missing something.\nCan anyone point out if there is an obvious approach that I am missing to catching all request download errors (including DNS lookup errors) using CrawlSpider?", "issue_status": "Closed", "issue_reporting_time": "2017-07-11T17:57:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "592": {"issue_url": "https://github.com/scrapy/scrapy/issues/2820", "issue_id": "#2820", "issue_summary": "How to use the Rule in CrawlSpider to track the response that Splash returns", "issue_description": "thsheep commented on Jul 9, 2017\nI would like to use Rule to track the Splash rendering Response!\nBut using SplashRequest, Rule does not take effect.\nThen use the rule of the process_request, re-set the Request object URL, written into the Splash HTTP API request.\n`class MySpider(CrawlSpider):\nname = 'innda'\n\ndef start_requests(self):\n    yield Request(url)\n    \n\nrules = (\n    Rule(LinkExtractor(allow=('node_\\d+\\.htm',)), process_request='splash_request', follow=True),\n    Rule(LinkExtractor(allow=('content_\\d+\\.htm',)), callback=\"one_parse\")\n)\n\n\ndef splash_request(self, request):\n    request = request.replace(url=RENDER_HTML_URL + request.url)\n    return request\n`\nBut the relative path URL is replaced by the Splash url.\nWhat is the solution\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-07-09T15:56:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "593": {"issue_url": "https://github.com/scrapy/scrapy/issues/2818", "issue_id": "#2818", "issue_summary": "Pipeline sending data after spider_closed and opened again.", "issue_description": "ghost commented on Jul 7, 2017\nI am not sure where's the issue:\njayzeng/scrapy-elasticsearch#63", "issue_status": "Closed", "issue_reporting_time": "2017-07-07T00:31:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "594": {"issue_url": "https://github.com/scrapy/scrapy/issues/2817", "issue_id": "#2817", "issue_summary": "Downloader middleware not being loaded", "issue_description": "ocelot2123 commented on Jul 6, 2017\nfrom https://stackoverflow.com/questions/44939830/scrapy-downloader-middleware-not-being-loaded\nNot sure if this is a problem on my end or on scrapy's end but I can't figure out what is causing this because everything else is being loaded properly", "issue_status": "Closed", "issue_reporting_time": "2017-07-06T06:05:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "595": {"issue_url": "https://github.com/scrapy/scrapy/issues/2815", "issue_id": "#2815", "issue_summary": "Invalid Hostname when \"_\" in domain", "issue_description": "ghost commented on Jul 4, 2017\n2017-07-04 16:25:43 [scrapy.core.scraper] ERROR: Error downloading <GET http://wodnik_forum.orq.pl/> Traceback (most recent call last): File \"c:\\users\\bukowa\\vritualenv2\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1384, in _inlineCallbacks result = result.throwExceptionIntoGenerator(g) File \"c:\\users\\bukowa\\vritualenv2\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator return g.throw(self.type, self.value, self.tb) File \"c:\\users\\bukowa\\vritualenv2\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request defer.returnValue((yield download_func(request=request,spider=spider))) ValueError: invalid hostname: www.bg_2014.team360.pl\nValueError: invalid hostname: www.bg_2014.team360.pl\nValueError: invalid hostname: www.wolfs_rain.wxv.pl\nValueError: invalid hostname: wodnik_forum.orq.pl\nValueError: invalid hostname: kolo_pzw_miejskie_konskie.wedkuje.pl\nValueError: invalid hostname: bieganie_swiebo.e-fora.pl\nValueError: invalid hostname: rzymskie_koloseum.lovetotravel.pl\nValueError: invalid hostname: zbuntowana_anielica.story.gala.pl\nValueError: invalid hostname: forum_origami.bo.pl\nValueError: invalid hostname: dfk_forum.gbbsoft.pl\nValueError: invalid hostname: wodnik_forum.orq.pl", "issue_status": "Closed", "issue_reporting_time": "2017-07-04T14:50:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "596": {"issue_url": "https://github.com/scrapy/scrapy/issues/2813", "issue_id": "#2813", "issue_summary": "Debug Visual Studio Code", "issue_description": "livertonoliveira commented on Jul 4, 2017\nIs there any way to use Visual Studio Code as Debugger with Scrapy ?", "issue_status": "Closed", "issue_reporting_time": "2017-07-04T03:47:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "597": {"issue_url": "https://github.com/scrapy/scrapy/issues/2811", "issue_id": "#2811", "issue_summary": "DNSCACHE_ENABLED=False not working", "issue_description": "Contributor\nredapple commented on Jul 3, 2017\nOriginally reported by @softwarevamp on StackOverflow:\nWhen i run scrapy shell with DNSCACHE_ENABLED=False got\nKeyError: 'dictionary is empty'\ntwisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.mydomain.com.\n    2017-07-03 03:09:12 [twisted] CRITICAL: while looking up www.mydomain.com with <scrapy.resolver.CachingThreadedResolver object at 0x3fd0050>\n    Traceback (most recent call last):\n      File \"/usr/lib64/python2.7/site-packages/twisted/internet/defer.py\", line 653, in _runCallbacks\n        current.result = callback(current.result, *args, **kw)\n      File \"/usr/lib64/python2.7/site-packages/scrapy/resolver.py\", line 29, in _cache_result\n        dnscache[name] = result\n      File \"/usr/lib64/python2.7/site-packages/scrapy/utils/datatypes.py\", line 305, in __setitem__\n        self.popitem(last=False)\n      File \"/usr/lib64/python2.7/collections.py\", line 159, in popitem\n        raise KeyError('dictionary is empty')\n    KeyError: 'dictionary is empty'\n    2017-07-03 03:09:12 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET //www.mydomain.com/> (failed 3 times): DNS lookup failed: no results for hostname lookup: www.mydomain.com.\n    Traceback (most recent call last):\n      File \"/usr/bin/scrapy\", line 11, in <module>\n        sys.exit(execute())\n      File \"/usr/lib64/python2.7/site-packages/scrapy/cmdline.py\", line 149, in execute\n        _run_print_help(parser, _run_command, cmd, args, opts)\n      File \"/usr/lib64/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n        func(*a, **kw)\n      File \"/usr/lib64/python2.7/site-packages/scrapy/cmdline.py\", line 156, in _run_command\n        cmd.run(args, opts)\n      File \"/usr/lib64/python2.7/site-packages/scrapy/commands/shell.py\", line 73, in run\n        shell.start(url=url, redirect=not opts.no_redirect)\n      File \"/usr/lib64/python2.7/site-packages/scrapy/shell.py\", line 48, in start\n        self.fetch(url, spider, redirect=redirect)\n      File \"/usr/lib64/python2.7/site-packages/scrapy/shell.py\", line 115, in fetch\n        reactor, self._schedule, request, spider)\n      File \"/usr/lib64/python2.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n        result.raiseException()\n      File \"<string>\", line 2, in raiseException\n    twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.mydomain.com.\nAny thoughts welcome", "issue_status": "Closed", "issue_reporting_time": "2017-07-03T14:10:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "598": {"issue_url": "https://github.com/scrapy/scrapy/issues/2806", "issue_id": "#2806", "issue_summary": "error filedescriptor out of range in select()", "issue_description": "echoocking commented on Jun 29, 2017\n2017-06-29 22:21:36 [crawler.download_middlewares.RetryMiddleware] DEBUG: Gave up retrying <GET https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv986&productId=1540024&score=0&sortType=6&page=88&pageSize=10&isShadowSku=0> (failed 3 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <type 'exceptions.ValueError'>: filedescriptor out of range in select()\ni use python 2.7 and scrapy 1.3.3\nif you want ,i can upload my code\nif you are interest in,and want more information ,please email:echooc@outlook.com \ud83d\udc4d", "issue_status": "Closed", "issue_reporting_time": "2017-06-29T14:28:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "599": {"issue_url": "https://github.com/scrapy/scrapy/issues/2805", "issue_id": "#2805", "issue_summary": "Error in LinkExtractor", "issue_description": "mohit0749 commented on Jun 28, 2017 \u2022\nedited\nif allowed_domains occurred in url(in path or query) it also crawl that url.\ne.g:\nallowed_domains =[\"www.rankwatch.com\"]\nwhen i run LinkExtractor it also crawl :\n{\"url\": \"https://www.facebook.com/login.php?skip_api_login=1&api_key=864596490301578&signed_next=1&next=https%3A%2F%2Fwww.facebook.com%2Fv2.4%2Fdialog%2Foauth%3Fredirect_uri%3Dhttps%253A%252F%252Fwww.rankwatch.com%252Flearning%252Fuser%252Fsimple-fb-connect%252Freturn%26state%3Daa68bcc193612233a0511106f69f9203%26scope%3Demail%26client_id%3D864596490301578%26ret%3Dlogin%26sdk%3Dphp-sdk-4.0.23%26logger_id%3Df7320741-707c-2f7e-1947-390b18a93e99&cancel_url=https%3A%2F%2Fwww.rankwatch.com%2Flearning%2Fuser%2Fsimple-fb-connect%2Freturn%3Ferror%3Daccess_denied%26error_code%3D200%26error_description%3DPermissions%2Berror%26error_reason%3Duser_denied%26state%3Daa68bcc193612233a0511106f69f9203%23_%3D_&display=page&locale=en_GB&logger_id=f7320741-707c-2f7e-1947-390b18a93e99\"},", "issue_status": "Closed", "issue_reporting_time": "2017-06-28T10:01:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "600": {"issue_url": "https://github.com/scrapy/scrapy/issues/2804", "issue_id": "#2804", "issue_summary": "Strange errors and freezes (strange connections handling?)", "issue_description": "ghost commented on Jun 26, 2017 \u2022\nedited by ghost\n2017-06-25 18:21:19 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://forum.wincenter.pl/> (failed 3 times): Connection was refused by other side: 111: Connection refused. 2017-06-25 18:21:19 [scrapy.core.scraper] ERROR: Error downloading <GET http://forum.wincenter.pl/> Traceback (most recent call last): File \"/usr/local/lib/python3.5/site-packages/twisted/internet/defer.py\", line 1384, in _inlineCallbacks result = result.throwExceptionIntoGenerator(g) File \"/usr/local/lib/python3.5/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator return g.throw(self.type, self.value, self.tb) File \"/usr/local/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request defer.returnValue((yield download_func(request=request,spider=spider))) twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.\n\n\n\n\nThen scraper freezes for 30seconds - 1 minute and continues to crawl, then, after few websites, other or the same errors happen.\nShouldnt the site be skipped and crawl should continue?", "issue_status": "Closed", "issue_reporting_time": "2017-06-25T18:35:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "601": {"issue_url": "https://github.com/scrapy/scrapy/issues/2797", "issue_id": "#2797", "issue_summary": "Use Signals API", "issue_description": "Headmaster11 commented on Jun 20, 2017\nI am not sure, how to user signals API. For example, I have a spider\nfrom scrapy.xlib.pydispatch import dispatcher\n\nclass PageSpider(CrawlSpider):\n\n    def __init__(self):\n        dispatcher.connect(self.spider_closed, signals.spider_closed)\n        # some code\n\n    def spider_closed(self, spider):\n        # some code\nAnd I have a control script\ns = get_project_settings()\nprocess = CrawlerProcess(s)\nfor spider in spider_list:\n    process.crawl(spider)\nprocess.start()\nCan I use SignalManager to catch if spider is closed? Or this is impossible?", "issue_status": "Closed", "issue_reporting_time": "2017-06-20T09:43:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "602": {"issue_url": "https://github.com/scrapy/scrapy/issues/2796", "issue_id": "#2796", "issue_summary": "error while trying to execute, need your HELP!", "issue_description": "begimai commented on Jun 17, 2017 \u2022\nedited\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 148, in execute\ncmd.crawler_process = CrawlerProcess(settings)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 243, in init\nsuper(CrawlerProcess, self).init(settings)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 134, in init\nself.spider_loader = _get_spider_loader(settings)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 330, in _get_spider_loader\nreturn loader_cls.from_settings(settings.frozencopy())\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 61, in from_settings\nreturn cls(settings)\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 25, in init\nself._load_all_spiders()\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 47, in _load_all_spiders\nfor module in walk_modules(name):\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 71, in walk_modules\nsubmod = import_module(fullpath)\nFile \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/init.py\", line 37, in import_module\nimport(name)\nFile \"/Users/admin/Downloads/scr-news-master/news/spiders/24kg.py\", line 3, in\nfrom base import BaseSpider\nFile \"/Users/admin/Downloads/scr-news-master/news/spiders/base.py\", line 2, in\nfrom record import Record\nImportError: No module named record", "issue_status": "Closed", "issue_reporting_time": "2017-06-17T06:54:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "603": {"issue_url": "https://github.com/scrapy/scrapy/issues/2795", "issue_id": "#2795", "issue_summary": "Error with yield from on scrapy.Request object", "issue_description": "reyman commented on Jun 16, 2017 \u2022\nedited\nHi,\nI try to use some imbricated/recursive scrapy.Request(...) (scrapy 1.4.0) using the new yield from keyword of python 3.3, without success.\nI test my algorithm with simple example and fixed data, it works, but i cannot reproduce the same algorithm using yield from on Scrapy Request :\n2017-06-16 15:20:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://api.flightradar24.com/common/v1/airport.json?code=VAR&plugin%5C%5B%5C%5D=&plugin-setting%5C%5Bschedule%5C%5D%5C%5Bmode%5C%5D=&plugin-setting%5C%5Bschedule%5C%5D%5C%5Btimestamp%5C%5D=1493251200&page=1&limit=100&token=> (referer: https://www.flightradar24.com/data/airports/bulgaria)\nTraceback (most recent call last):\n  File \"/home/reyman/.pyenv/versions/venv352/lib/python3.5/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/home/reyman/.pyenv/versions/venv352/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/home/reyman/.pyenv/versions/venv352/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/home/reyman/.pyenv/versions/venv352/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/reyman/.pyenv/versions/venv352/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/reyman/Projets/Flight-Scrapping/flight/flight_project/spiders/AirportsSpider.py\", line 209, in parse_schedule\n    yield from scrapy.Request(url, self.parse_schedule, meta={'airport_item': item, 'airport_urls': urls, 'i': i + 1})\nTypeError: 'Request' object is not iterable\n2017-06-16 15:20:45 [scrapy.core.engine] INFO: Closing spider (finished)\n2017-06-16 15:20:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\nYou could find the original script here , do you have an idea to help me on this point ?", "issue_status": "Closed", "issue_reporting_time": "2017-06-16T13:29:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "604": {"issue_url": "https://github.com/scrapy/scrapy/issues/2794", "issue_id": "#2794", "issue_summary": "scrapy can't run", "issue_description": "zangree commented on Jun 16, 2017\nTraceback (most recent call last):\nFile \"e:\\python27\\lib\\runpy.py\", line 162, in _run_module_as_main\n\"main\", fname, loader, pkg_name)\nFile \"e:\\python27\\lib\\runpy.py\", line 72, in run_code\nexec code in run_globals\nFile \"E:\\Python27\\Scripts\\scrapy.exe_main.py\", line 5, in\nFile \"e:\\python27\\lib\\site-packages\\scrapy\\cmdline.py\", line 9, in\nfrom scrapy.crawler import CrawlerProcess\nFile \"e:\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 7, in\nfrom twisted.internet import reactor, defer\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet\\reactor.py\", line 38, in\nfrom twisted.internet import default\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet\\default.py\", line 56, in\ninstall = _getInstallFunction(platform)\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet\\default.py\", line 50, in _getInstallFunction\nfrom twisted.internet.selectreactor import install\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 18, in\nfrom twisted.internet import posixbase\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet\\posixbase.py\", line 18, in\nfrom twisted.internet import error, udp, tcp\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet\\tcp.py\", line 28, in\nfrom twisted.internet._newtls import (\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet_newtls.py\", line 21, in\nfrom twisted.protocols.tls import TLSMemoryBIOFactory, TLSMemoryBIOProtocol\nFile \"e:\\python27\\lib\\site-packages\\twisted\\protocols\\tls.py\", line 63, in\nfrom twisted.internet._sslverify import _setAcceptableProtocols\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet_sslverify.py\", line 157, in\nverifyHostname, VerificationError = selectVerifyImplementation()\nFile \"e:\\python27\\lib\\site-packages\\twisted\\internet_sslverify.py\", line 140, in selectVerifyImplementation\nfrom service_identity import VerificationError\nFile \"e:\\python27\\lib\\site-packages\\service_identity_init.py\", line 7, in\nfrom . import cryptography, pyopenssl\nFile \"e:\\python27\\lib\\site-packages\\service_identity\\cryptography.py\", line 9, in\nfrom cryptography.x509 import (\nFile \"e:\\python27\\lib\\site-packages\\cryptography\\x509_init.py\", line 9, in\nfrom cryptography.x509.base import (\nFile \"e:\\python27\\lib\\site-packages\\cryptography\\x509\\base.py\", line 16, in\nfrom cryptography.x509.extensions import Extension, ExtensionType\nFile \"e:\\python27\\lib\\site-packages\\cryptography\\x509\\extensions.py\", line 13, in\nfrom asn1crypto.keys import PublicKeyInfo\nFile \"e:\\python27\\lib\\site-packages\\asn1crypto\\keys.py\", line 22, in\nfrom ._elliptic_curve import (\nFile \"e:\\python27\\lib\\site-packages\\asn1crypto_elliptic_curve.py\", line 51, in\nfrom ._int import inverse_mod\nFile \"e:\\python27\\lib\\site-packages\\asn1crypto_int.py\", line 56, in\nfrom ._perf._big_num_ctypes import libcrypto\nFile \"e:\\python27\\lib\\site-packages\\asn1crypto_perf_big_num_ctypes.py\", line 31, in\nlibcrypto_path = find_library('crypto')\nFile \"e:\\python27\\lib\\ctypes\\util.py\", line 54, in find_library\nfname = os.path.join(directory, name)\nFile \"e:\\python27\\lib\\ntpath.py\", line 85, in join\nresult_path = result_path + p_path\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 7: ordinal not in range(128)\nWhen I update scrapy from 1.3.3 to 1.4.0, this Error happed. Could someone help to fix it? Thank you.", "issue_status": "Closed", "issue_reporting_time": "2017-06-16T01:55:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "605": {"issue_url": "https://github.com/scrapy/scrapy/issues/2790", "issue_id": "#2790", "issue_summary": "ImportError: No module named settings", "issue_description": "kirollosd commented on Jun 14, 2017\nHello guys, I'm suffering from this error\n  File \"/Library/Python/2.7/site-packages/scrapy/cmdline.py\", line 161, in <module>\n    execute()\n  File \"/Library/Python/2.7/site-packages/scrapy/cmdline.py\", line 108, in execute\n    settings = get_project_settings()\n  File \"/Users/kdawod/.virtualenvs/python2.7/lib/python2.7/site-packages/scrapy/utils/project.py\", line 68, in get_project_settings\n    settings.setmodule(settings_module_path, priority='project')\n  File \"/Users/kdawod/.virtualenvs/python2.7/lib/python2.7/site-packages/scrapy/settings/__init__.py\", line 292, in setmodule\n    module = import_module(module)\n  File \"/usr/local/Cellar/python/2.7.12_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: No module named settings \nit's only happening while running in pycharm, however, it was working just fine the previous day.\nI would be glad if you can help me figure out this issue and prevent it from happening in the feature.", "issue_status": "Closed", "issue_reporting_time": "2017-06-14T13:48:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "606": {"issue_url": "https://github.com/scrapy/scrapy/issues/2787", "issue_id": "#2787", "issue_summary": "Mail Sender", "issue_description": "Ndiithi commented on Jun 13, 2017\nWhen trying to send an attachment from MailSender.send() , the attachs argument takes a tuple. If one is passing a single tuple eg, attachs=(\"header\", \"text/plain\", myFileObject) or attachs=((\"header\", \"text/plain\", myFileObject)) , there's a ValueError: exception, only works when passing params in this way: attachs=((\"header\", \"text/plain\", myFileObject),)", "issue_status": "Closed", "issue_reporting_time": "2017-06-13T16:32:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "607": {"issue_url": "https://github.com/scrapy/scrapy/issues/2785", "issue_id": "#2785", "issue_summary": "[idea] Supporting also `link` tags in `Response.follow`", "issue_description": "Contributor\nstarrify commented on Jun 13, 2017\nCurrently the Response.follow method supports Selector objects whose root tag are <a>. It'll be nice to support also link tags.\nRelated code: here\nSample code:\n# Now\nfor href in response.css('link[rel=next]::attr(href)').extract():\n    yield response.follow(href)\n\n# When the proposed feature is available\nfor link_sel in response.css('link[rel=next]'):\n    yield response.follow(link_sel)", "issue_status": "Closed", "issue_reporting_time": "2017-06-13T11:27:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "608": {"issue_url": "https://github.com/scrapy/scrapy/issues/2783", "issue_id": "#2783", "issue_summary": "\"Failed to create process.\" on Windows.", "issue_description": "Krasiu commented on Jun 11, 2017\nI've read lots of threads on stackoverflow regarding this issue. For most people it was problem with spaces in path.\nThat is not the case with me. The weird thing is that after fresh installation of Anaconda/Miniconda and successful instalation of scrapy, I can always do ONE action (whether it's running the spider, creating new project, etc.)\nNext try, even though I haven't changed anything results in:\nfailed to create process.\nC:\\Users\\Micha\u0142\\cellink>C:\\Miniconda3\\Scripts\\scrapy crawl cellink\n2017-06-11 12:32:48 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: cellink)\n2017-06-11 12:32:48 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'cellink', 'NEWSPIDER_MODULE': 'cellink.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['cellink.spiders']}\n2017-06-11 12:32:48 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n....\n/* Succesfull run */\nImmedietely after:\nC:\\Users\\Micha\u0142\\cellink>C:\\Miniconda3\\Scripts\\scrapy crawl cellink\nfailed to create process.", "issue_status": "Closed", "issue_reporting_time": "2017-06-11T11:56:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "609": {"issue_url": "https://github.com/scrapy/scrapy/issues/2782", "issue_id": "#2782", "issue_summary": "Do not fire close_spider when using ImagesPipeline", "issue_description": "Samarpitr commented on Jun 10, 2017 \u2022\nedited\nI am downloading the images by using ImagesPipeline and want to fire a script on the closing of the spider but scrapy close_spider not performing the its functionality.", "issue_status": "Closed", "issue_reporting_time": "2017-06-10T11:56:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "610": {"issue_url": "https://github.com/scrapy/scrapy/issues/2779", "issue_id": "#2779", "issue_summary": "canonicalize at LinkExtractor works incorrectly", "issue_description": "Contributor\njenya commented on Jun 7, 2017\nFirst:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/linkextractors/lxmlhtml.py#L110\nAnd then:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/linkextractors/lxmlhtml.py#L40\nSo, when canonicalize = False (current default value), it will run canonicalize_url, but it shouldn't:\n:param canonicalize: canonicalize each extracted url (using w3lib.url.canonicalize_url)", "issue_status": "Closed", "issue_reporting_time": "2017-06-07T13:57:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "611": {"issue_url": "https://github.com/scrapy/scrapy/issues/2776", "issue_id": "#2776", "issue_summary": "how can i get index from the parse_detail", "issue_description": "jzfan commented on Jun 5, 2017 \u2022\nedited\nI parse a list page, and the details, i got a random data, how can i get them ordered like the list page display? here is my code:\ndef parse(self, response):\n     for url in response.css('.entry-content > ul > li > a::attr(href)').extract():\n            yield scrapy.Request(url=url, callback=self.parse_details)\n\ndef parse_details(self, response):\n          yield {\n   'entry': response.css('h1.entry-title::text').extract_first(),\n   'content': response.css('div.entry-content').extract_first()\n  }", "issue_status": "Closed", "issue_reporting_time": "2017-06-05T08:57:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "612": {"issue_url": "https://github.com/scrapy/scrapy/issues/2774", "issue_id": "#2774", "issue_summary": "Don`t fetch url", "issue_description": "tonal commented on Jun 2, 2017\nUrl http://www.tehnosad.ru/subcategory/?id=584&producers[]=524 do not fetch from scrapy.\nBat httpie and chrome get Ok.\nHow to correct it?\n$ scrapy fetch --headers 'http://www.tehnosad.ru/subcategory/?id=584&producers[]=524'\n2017-06-02 13:49:44 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: remains)\n2017-06-02 13:49:44 [scrapy.utils.log] INFO: Overridden settings: {'RETRY_TIMES': 5, 'SPIDER_MODULES': ['remains.spiders'], 'FEED_URI': './%(name)s.csv', 'HTTPCACHE_EXPIRATION_SECS': 600, 'RETRY_HTTP_CODES': [400, 408, 420, 500, 502, 503, 504], 'BOT_NAME': 'remains', 'FEED_FORMAT': 'csv', 'AUTOTHROTTLE_ENABLED': True, 'HTTPCACHE_STORAGE': 'scrapy.contrib.httpcache.FilesystemCacheStorage', 'NEWSPIDER_MODULE': 'remains.spiders', 'HTTPCACHE_DIR': './httpcache', 'DOWNLOADER_CLIENTCONTEXTFACTORY': 'remains.ssl_context.CustomClientContextFactory'}\n2017-06-02 13:49:44 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.throttle.AutoThrottle']\n2017-06-02 13:49:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-06-02 13:49:44 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-06-02 13:49:44 [scrapy.middleware] INFO: Enabled item pipelines:\n['remains.pipelines.RemainsPipeline']\n2017-06-02 13:49:44 [scrapy.core.engine] INFO: Spider opened\n2017-06-02 13:49:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-06-02 13:49:44 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:49:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:49:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:10 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:18 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:22 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:29 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:36 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:40 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-06-02 13:50:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:50:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:51:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:51:13 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:51:20 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:51:26 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:51:32 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:51:39 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:51:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-06-02 13:51:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524> from <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>\n2017-06-02 13:51:54 [scrapy.downloadermiddlewares.redirect] DEBUG: Discarding <GET http://www.tehnosad.ru/subcategory/?id=584&producers%5B%5D=524>: max redirections reached\n2017-06-02 13:51:54 [scrapy.core.engine] INFO: Closing spider (finished)\n2017-06-02 13:51:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 9196,\n 'downloader/request_count': 21,\n 'downloader/request_method_count/GET': 21,\n 'downloader/response_bytes': 14284,\n 'downloader/response_count': 21,\n 'downloader/response_status_count/301': 21,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2017, 6, 2, 6, 51, 54, 206787),\n 'log_count/DEBUG': 21,\n 'log_count/INFO': 9,\n 'memusage/max': 50880512,\n 'memusage/startup': 49733632,\n 'scheduler/dequeued': 21,\n 'scheduler/dequeued/memory': 21,\n 'scheduler/enqueued': 21,\n 'scheduler/enqueued/memory': 21,\n 'start_time': datetime.datetime(2017, 6, 2, 6, 49, 44, 629809)}\n2017-06-02 13:51:54 [scrapy.core.engine] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2017-06-02T07:02:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "613": {"issue_url": "https://github.com/scrapy/scrapy/issues/2772", "issue_id": "#2772", "issue_summary": "crawl the urls for failure", "issue_description": "ChengjinWu commented on Jun 1, 2017\nWhen I crawl this article\uff1a\n<a href=\"\nhttp://tech.ifeng.com/a/20170526/44626856_0.shtml\n\" target=\"_blank\">\nAlphaGo\u4ee5\u4e00\u654c\u4e94\u8f7b\u677e\u83b7\u80dc\uff0c\u9762\u5bf9\u81ea\u5df1\u65f6\u7ec8\u5c1d\u8d25\u7ee9\n</a>\nI will get the wrong urls\uff1a\nhttp://tech.ifeng.com/%0Ahttp://tech.ifeng.com/a/20170127/44537081_0.shtml%0A%0A%0A%0A", "issue_status": "Closed", "issue_reporting_time": "2017-06-01T07:29:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "614": {"issue_url": "https://github.com/scrapy/scrapy/issues/2771", "issue_id": "#2771", "issue_summary": "how crawl post url", "issue_description": "fengfangqian commented on Jun 1, 2017\ncurl -H \"Content-Type:application/json\" -d '{\"body\":{\"pkgNameList\":[\"com.sinyee.babybus.show\"],\"sdkVersionCode\":0,\"oriUrl\":\"http://xxxxxx\"},\"head\":{\"businessId\":\"test\",\"callbackPara\":\"callback01\",\"nonce\":1804289383,\"timestamp\":1429951077}}' \"http://maapi.3g.qq.com:8080/v1/getAppDetailBatch?output=json&signature=8b259a5eb9d41741a520ce196af5ac86\"\nhow can i crawl like this url", "issue_status": "Closed", "issue_reporting_time": "2017-06-01T05:57:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "615": {"issue_url": "https://github.com/scrapy/scrapy/issues/2770", "issue_id": "#2770", "issue_summary": "ERROR: Spider error processing <GET https://blog.scrapinghub.com> (referer: None)", "issue_description": "mazh91 commented on Jun 1, 2017\nHello. I'm getting this error after going through the site tutorial:\n2017-05-31 22:00:38 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n2017-05-31 22:00:38 [scrapy] INFO: Optional features available: ssl, http11, boto\n2017-05-31 22:00:38 [scrapy] INFO: Overridden settings: {}\n2017-05-31 22:00:38 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n2017-05-31 22:00:38 [boto] DEBUG: Retrieving credentials from metadata server.\n2017-05-31 22:00:39 [boto] ERROR: Caught exception reading instance data\nTraceback (most recent call last):\nFile \"/usr/lib/python2.7/dist-packages/boto/utils.py\", line 210, in retry_url\nr = opener.open(req, timeout=timeout)\nFile \"/usr/lib/python2.7/urllib2.py\", line 429, in open\nresponse = self._open(req, data)\nFile \"/usr/lib/python2.7/urllib2.py\", line 447, in _open\n'_open', req)\nFile \"/usr/lib/python2.7/urllib2.py\", line 407, in _call_chain\nresult = func(*args)\nFile \"/usr/lib/python2.7/urllib2.py\", line 1228, in http_open\nreturn self.do_open(httplib.HTTPConnection, req)\nFile \"/usr/lib/python2.7/urllib2.py\", line 1198, in do_open\nraise URLError(err)\nURLError:\n2017-05-31 22:00:39 [boto] ERROR: Unable to read instance data, giving up\n2017-05-31 22:00:39 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2017-05-31 22:00:39 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2017-05-31 22:00:39 [scrapy] INFO: Enabled item pipelines:\n2017-05-31 22:00:39 [scrapy] INFO: Spider opened\n2017-05-31 22:00:39 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-05-31 22:00:39 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-05-31 22:00:40 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com> (referer: None)\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'Deploy your Scrapy Spiders from GitHub'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'Looking Back at 2016'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'How to Increase Sales with Online Reputation Management'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'How to Build your own Price Monitoring Tool'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'How You Can Use Web Data to Accelerate Your Startup'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'An Introduction to XPath: How to Get Started'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'Why Promoting Open Data Increases Economic Opportunities'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'Interview: How Up Hail uses Scrapy to Increase Transparency'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'How to Run Python Scripts in Scrapy Cloud'}\n2017-05-31 22:00:40 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'Embracing the Future of Work: How To Communicate Remotely'}\n2017-05-31 22:00:40 [scrapy] ERROR: Spider error processing <GET https://blog.scrapinghub.com> (referer: None)\nTraceback (most recent call last):\nFile \"/usr/lib/python2.7/dist-packages/scrapy/utils/defer.py\", line 102, in iter_errback\nyield next(it)\nFile \"/usr/lib/python2.7/dist-packages/scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\nfor x in result:\nFile \"/usr/lib/python2.7/dist-packages/scrapy/spidermiddlewares/referer.py\", line 22, in\nreturn (_set_referer(r) for r in result or ())\nFile \"/usr/lib/python2.7/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in\nreturn (r for r in result or () if _filter(r))\nFile \"/usr/lib/python2.7/dist-packages/scrapy/spidermiddlewares/depth.py\", line 54, in\nreturn (r for r in result or () if _filter(r))\nFile \"/home/milad/Desktop/contact_scraper.py\", line 13, in parse\nyield response.follow(next_page, self.parse)\nAttributeError: 'HtmlResponse' object has no attribute 'follow'\n2017-05-31 22:00:40 [scrapy] INFO: Closing spider (finished)\n2017-05-31 22:00:40 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 218,\n'downloader/request_count': 1,\n'downloader/request_method_count/GET': 1,\n'downloader/response_bytes': 11455,\n'downloader/response_count': 1,\n'downloader/response_status_count/200': 1,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2017, 6, 1, 2, 0, 40, 839833),\n'item_scraped_count': 10,\n'log_count/DEBUG': 13,\n'log_count/ERROR': 3,\n'log_count/INFO': 7,\n'response_received_count': 1,\n'scheduler/dequeued': 1,\n'scheduler/dequeued/memory': 1,\n'scheduler/enqueued': 1,\n'scheduler/enqueued/memory': 1,\n'spider_exceptions/AttributeError': 1,\n'start_time': datetime.datetime(2017, 6, 1, 2, 0, 39, 752881)}\n2017-05-31 22:00:40 [scrapy] INFO: Spider closed (finished)\nHere is my source code:\nimport scrapy\n\nclass BlogSpider(scrapy.Spider):\n    name = 'blogspider'\n    start_urls = ['https://blog.scrapinghub.com']\n\n    def parse(self, response):\n        for title in response.css('h2.entry-title'):\n            yield {'title': title.css('a ::text').extract_first()}\n\n        for next_page in response.css('div.prev-post > a'):\n            yield response.follow(next_page, self.parse)\nI'm using Python 2.7.12 on Linux. Any helpful comments appreciated!", "issue_status": "Closed", "issue_reporting_time": "2017-06-01T02:15:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "616": {"issue_url": "https://github.com/scrapy/scrapy/issues/2768", "issue_id": "#2768", "issue_summary": "error for https sites", "issue_description": "DANIYELS commented on May 31, 2017 \u2022\nedited by redapple\nHi! I can't crawl https websites; http sites work just fine. I tried updating the scrapy.cfg file with DOWNLOADER_CLIENTCONTEXTFACTORY='testproject.CustomContext.CustomClientContextFactory' and creating the CustomContext.py file in the project directory, but it didn't solve the problem. See console log below.\nI am grateful for solutions!\nC:\\Users\\UserX\\Documents\\Python Scripts\\Test\\tutorials> **scrapy version -v**\nScrapy    : 1.3.3\nlxml      : 3.7.2.0\nlibxml2   : 2.9.4\ncssselect : 1.0.0\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2k  26 Jan 2017)\nPlatform  : Windows-2012ServerR2-6.3.9600-SP0\nC:\\Users\\UserX\\Documents\\Python Scripts\\Test\\tutorials> scrapy shell 'https://www.stackoverflow.com'\n\n2017-05-31 13:18:28 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorials)\n2017-05-31 13:18:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorials', 'DUPEFILTER_CLASS': 'scrapy.\ndupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'NEWSPIDER_MODULE': 'tutorials.spiders', 'ROBOTSTXT_OBEY': True, 'S\nPIDER_MODULES': ['tutorials.spiders']}\n2017-05-31 13:18:28 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole']\n2017-05-31 13:18:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-05-31 13:18:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-05-31 13:18:28 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-05-31 13:18:28 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-05-31 13:18:28 [scrapy.core.engine] INFO: Spider opened\n2017-05-31 13:18:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stackoverflow.com/robots.txt>\n(failed 1 times): Connection was refused by other side: 10061: No connection could be made because the target machine ac\ntively refused it..\n2017-05-31 13:18:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stackoverflow.com/robots.txt>\n(failed 2 times): Connection was refused by other side: 10061: No connection could be made because the target machine ac\ntively refused it..\n2017-05-31 13:18:31 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://www.stackoverflow.com/robo\nts.txt> (failed 3 times): Connection was refused by other side: 10061: No connection could be made because the target ma\nchine actively refused it..\n2017-05-31 13:18:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://www.stackoverflow.com\n/robots.txt>: Connection was refused by other side: 10061: No connection could be made because the target machine active\nly refused it..\nTraceback (most recent call last):\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299,\nin _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, i\nn throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\",\nline 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: No connection could be made\nbecause the target machine actively refused it..\n2017-05-31 13:18:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stackoverflow.com> (failed 1 t\nimes): Connection was refused by other side: 10061: No connection could be made because the target machine actively refu\nsed it..\n2017-05-31 13:18:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stackoverflow.com> (failed 2 t\nimes): Connection was refused by other side: 10061: No connection could be made because the target machine actively refu\nsed it..\n2017-05-31 13:18:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://www.stackoverflow.com> (fa\niled 3 times): Connection was refused by other side: 10061: No connection could be made because the target machine activ\nely refused it..\nTraceback (most recent call last):\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\Scripts\\scrapy-script.py\", line 5, in <module>\n    sys.exit(scrapy.cmdline.execute())\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 142, in execut\ne\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 88, in _run_pr\nint_help\n    func(*a, **kw)\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 149, in _run_c\nommand\n    cmd.run(args, opts)\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scrapy\\commands\\shell.py\", line 73, in\nrun\n    shell.start(url=url, redirect=not opts.no_redirect)\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scrapy\\shell.py\", line 48, in start\n    self.fetch(url, spider, redirect=redirect)\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scrapy\\shell.py\", line 115, in fetch\n    reactor, self._schedule, request, spider)\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\twisted\\internet\\threads.py\", line 122,\n in blockingCallFromThread\n    result.raiseException()\n  File \"C:\\Users\\UserX\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\twisted\\python\\failure.py\", line 372, i\nn raiseException\n    raise self.value.with_traceback(self.tb)\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: No connection could be made\nbecause the target machine actively refused it..\nC:\\Users\\UserX\\Documents\\Python Scripts\\Test\\tutorials> scrapy shell 'http://www.stackoverflow.com'\n\n2017-05-31 13:18:47 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorials)\n2017-05-31 13:18:47 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorials', 'DUPEFILTER_CLASS': 'scrapy.\ndupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'NEWSPIDER_MODULE': 'tutorials.spiders', 'ROBOTSTXT_OBEY': True, 'S\nPIDER_MODULES': ['tutorials.spiders']}\n2017-05-31 13:18:47 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole']\n2017-05-31 13:18:48 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-05-31 13:18:48 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-05-31 13:18:48 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-05-31 13:18:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-05-31 13:18:48 [scrapy.core.engine] INFO: Spider opened\n2017-05-31 13:18:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://stackoverflow.com/ro\nbots.txt> from <GET http://www.stackoverflow.com/robots.txt>\n2017-05-31 13:18:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://stackoverflow.com/r\nobots.txt> from <GET http://stackoverflow.com/robots.txt>\n2017-05-31 13:18:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://stackoverflow.com/robots.txt> (referer: None)\n\n2017-05-31 13:18:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://stackoverflow.com/>\nfrom <GET http://www.stackoverflow.com>\n2017-05-31 13:18:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://stackoverflow.com/r\nobots.txt> from <GET http://stackoverflow.com/robots.txt>\n2017-05-31 13:18:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://stackoverflow.com/robots.txt> (referer: None)\n\n2017-05-31 13:18:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://stackoverflow.com/>\n from <GET http://stackoverflow.com/>\n2017-05-31 13:18:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://stackoverflow.com/> (referer: None)\n2017-05-31 13:18:49 [traitlets] DEBUG: Using default logger\n2017-05-31 13:18:49 [traitlets] DEBUG: Using default logger\n[s] Available Scrapy objects:\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n[s]   crawler    <scrapy.crawler.Crawler object at 0x000000ED0AEDB8D0>\n[s]   item       {}\n[s]   request    <GET http://www.stackoverflow.com>\n[s]   response   <200 https://stackoverflow.com/>\n[s]   settings   <scrapy.settings.Settings object at 0x000000ED0C022908>\n[s]   spider     <DefaultSpider 'default' at 0xed0c2b9cc0>\n[s] Useful shortcuts:\n[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n[s]   shelp()           Shell help (print this help)\n[s]   view(response)    View response in a browser\nIn [1]:", "issue_status": "Closed", "issue_reporting_time": "2017-05-31T11:23:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "617": {"issue_url": "https://github.com/scrapy/scrapy/issues/2766", "issue_id": "#2766", "issue_summary": "Cryptic traceback for non-callable callback", "issue_description": "Contributor\nredapple commented on May 30, 2017\nOriginally from https://stackoverflow.com/questions/44259172/scrapy-twisted-internet-defer-defgen-return-exception\nWhen a scrapy.Request is created with a callback that is a string (and not a callable),\ncallback (callable) \u2013 the function that will be called with the response of this request (once its downloaded) as its first parameter.\nTwisted chokes with a confusing twisted.internet.defer._DefGen_Return exception traceback.\nThe error of using a string for a callback comes from allowing a string in CrawlSpider rules.\ncallback is a callable or a string (in which case a method from the spider object with that name will be used) to be called for each link extracted with the specified link_extractor.\nSuggestion\neither allow callback to be string and matched with a spider method, like in CrawlSpider rules\nor fail earlier in scrapy.Request.__init__() if a non-None callback is not callable\nHow to reproduce\n$ scrapy version -v\nScrapy    : 1.4.0\nlxml      : 3.7.3.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.2.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 3.6.0+ (default, Feb 24 2017, 17:40:01) - [GCC 6.2.0 20161005]\npyOpenSSL : 17.0.0 (OpenSSL 1.0.2g  1 Mar 2016)\nPlatform  : Linux-4.8.0-53-generic-x86_64-with-debian-stretch-sid\n\n\n$ cat noncallable/spiders/example.py \n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = 'example'\n    start_urls = ['http://example.com/']\n\n    def parse(self, response):\n        yield scrapy.Request('http://httpbin.org/get?q=1', callback='parse_item')\n\n    def parse_item(self, response):\n        pass\n\n\n$ scrapy crawl example\n2017-05-30 16:04:47 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: noncallable)\n(...)\n2017-05-30 16:04:48 [scrapy.core.engine] INFO: Spider opened\n2017-05-30 16:04:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-05-30 16:04:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-05-30 16:04:48 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://example.com/robots.txt> (referer: None)\n2017-05-30 16:04:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.com/> (referer: None)\n2017-05-30 16:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/robots.txt> (referer: None)\n2017-05-30 16:04:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/get?q=1> (referer: http://example.com/)\n2017-05-30 16:04:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://httpbin.org/get?q=1> (referer: http://example.com/)\nTraceback (most recent call last):\n  File \"/home/paul/.virtualenvs/scrapy14/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1301, in _inlineCallbacks\n    result = g.send(result)\n  File \"/home/paul/.virtualenvs/scrapy14/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\n  File \"/home/paul/.virtualenvs/scrapy14/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1278, in returnValue\n    raise _DefGen_Return(val)\ntwisted.internet.defer._DefGen_Return: <200 http://httpbin.org/get?q=1>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/paul/.virtualenvs/scrapy14/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\n    result = f(*args, **kw)\n  File \"/home/paul/.virtualenvs/scrapy14/lib/python3.6/site-packages/scrapy/core/spidermw.py\", line 49, in process_spider_input\n    return scrape_func(response, request, spider)\n  File \"/home/paul/.virtualenvs/scrapy14/lib/python3.6/site-packages/scrapy/core/scraper.py\", line 146, in call_spider\n    dfd.addCallbacks(request.callback or spider.parse, request.errback)\n  File \"/home/paul/.virtualenvs/scrapy14/lib/python3.6/site-packages/twisted/internet/defer.py\", line 303, in addCallbacks\n    assert callable(callback)\nAssertionError\n2017-05-30 16:04:49 [scrapy.core.engine] INFO: Closing spider (finished)\n2017-05-30 16:04:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 893,\n 'downloader/request_count': 4,\n 'downloader/request_method_count/GET': 4,\n 'downloader/response_bytes': 2816,\n 'downloader/response_count': 4,\n 'downloader/response_status_count/200': 3,\n 'downloader/response_status_count/404': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2017, 5, 30, 14, 4, 49, 477327),\n 'log_count/DEBUG': 5,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'memusage/max': 45879296,\n 'memusage/startup': 45879296,\n 'request_depth_max': 1,\n 'response_received_count': 4,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'spider_exceptions/AssertionError': 1,\n 'start_time': datetime.datetime(2017, 5, 30, 14, 4, 48, 184220)}\n2017-05-30 16:04:49 [scrapy.core.engine] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2017-05-30T14:16:34Z", "fixed_by": "#2769", "pull_request_summary": "[MRG+1] Add verification to check if Request callback is callable", "pull_request_description": "Member\nstummjr commented on May 31, 2017\nThis PR intends to fix #2766.\nWhen users pass a string to Request.__init__'s callback parameter, they end up getting a Twisted exception, which doesn't help to uncover this, which is a rather simple issue.\nThis PR changes that, so that users get a proper message when passing something that's not callable to the callback and errback parameters.", "pull_request_status": "Merged", "issue_fixed_time": "2017-07-24T18:14:33Z", "files_changed": [["7", "scrapy/commands/parse.py"], ["4", "scrapy/http/request/__init__.py"], ["20", "tests/test_http_request.py"], ["6", "tests/test_utils_reqser.py"]]}, "618": {"issue_url": "https://github.com/scrapy/scrapy/issues/2761", "issue_id": "#2761", "issue_summary": "Images min dimensions cause unhandled exception", "issue_description": "xhujerr commented on May 29, 2017\nWhen I enable in settings.py\nIMAGES_MIN_HEIGHT = 350\nIMAGES_MIN_WIDTH = 350\nI start getting unhandled exceptions:\nUnhandled error in Deferred:\nFailure: scrapy.pipelines.images.ImageException: Image too small (62x59 < 350x350)\nThe exception looks like this:\n2017-05-28 19:06:43 [scrapy.pipelines.files] WARNING: File (error): Error processing file from <GET http://example.com/images/0.jpg> referred in <http://www.example.com/>: Image too small (600x200 < 350x350)\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/scrapy/pipelines/files.py\", line 356, in media_downloaded\n    checksum = self.file_downloaded(response, request, info)\n  File \"/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py\", line 98, in file_downloaded\n    return self.image_downloaded(response, request, info)\n  File \"/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py\", line 102, in image_downloaded\n    for path, image, buf in self.get_images(response, request, info):\n  File \"/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py\", line 120, in get_images\n    (width, height, self.min_width, self.min_height))\nImageException: Image too small (600x200 < 350x350)\nWhen I comment except statements:\nhttps://github.com/scrapy/scrapy/blob/1.3/scrapy/pipelines/files.py#L364\nhttps://github.com/scrapy/scrapy/blob/1.3/scrapy/pipelines/files.py#L372\nthe exceptions aren't unhandled anymore.\nIt seems to me like a bug. Isn't is? Or am I doing something wrong?\nIt may be something similar to:\n#1884\nbut in my case scrapy really crashes\npython-w3lib: 1.17.0-1exp1\npython-scrapy: 1.3.3-1exp1\nboth from a debian package\nPython 2.7.10+\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2017-05-28T19:34:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "619": {"issue_url": "https://github.com/scrapy/scrapy/issues/2759", "issue_id": "#2759", "issue_summary": "Document CloseSpider extension better", "issue_description": "Contributor\nredapple commented on May 26, 2017\nSee #2748 (comment)\n@kmike 's details:\nWhen CLOSERSPIDER_ITEMCOUNT limit is reached, Scrapy stops the spider gracefully; it means requests which are currently in the downloader queue (up to CONCURRENT_REQUEST requests) are still processed. This is the expected behavior.", "issue_status": "Closed", "issue_reporting_time": "2017-05-26T10:21:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "620": {"issue_url": "https://github.com/scrapy/scrapy/issues/2758", "issue_id": "#2758", "issue_summary": "scrapy runspider do not take effect of \"custom_settings\"", "issue_description": "clouds56 commented on May 25, 2017\nsample code:\n# myspider.py\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    custom_settings = {\n        'LOG_LEVEL': 'INFO',\n        'FEED_FORMAT': 'jsonlines',\n    }\n    start_urls = [\"https://www.example.com\"]\n    def parse(self, response):\n        pass\nrun with scrapy runspider mysipder.py, it shows:\n2017-05-25 23:22:13 [scrapy.utils.log] INFO: Scrapy 1.3.1 started (bot: scrapybot)\n2017-05-25 23:22:13 [scrapy.utils.log] INFO: Overridden settings: {}\n2017-05-25 23:22:13 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n'scrapy.extensions.feedexport.FeedExporter',\n'scrapy.extensions.logstats.LogStats']\n2017-05-25 23:22:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-05-25 23:22:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\nnotice that in \"[scrapy.utils.log] INFO: Overridden settings: {}\", the custom_settings doesn't take effects.", "issue_status": "Closed", "issue_reporting_time": "2017-05-25T15:52:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "621": {"issue_url": "https://github.com/scrapy/scrapy/issues/2754", "issue_id": "#2754", "issue_summary": "Scrapy can't crawl webpage with for loop", "issue_description": "kennedykan commented on May 24, 2017\nI have managed to extract some data with the following program. I tried to include a for loop to reduce the work to add different URLs to start_URLs, but seem not working. How can I make it work?\nimport scrapy\n\nclass JPItem(scrapy.Item):\nbest_answer = scrapy.Field()\nquestion_content = scrapy.Field()\nquestion_title = scrapy.Field()\n\nclass JPSpider(scrapy.Spider):\n\nname = \"jp\"\nallowed_domains = ['detail.chiebukuro.yahoo.co.jp']\n\nfor x in range (12174467750,12174467759):\n    start_urls = [\n        'https://detail.chiebukuro.yahoo.co.jp/qa/question_detail/q' + str(x),\n    ]\n\n    def parse(self, response):\n        item = JPItem()\n\n        item['question_title'] = response.css(\"div.mdPstd.mdPstdQstn.sttsRslvd.clrfx div.ttl h1::text\").extract_first()\n        item['question_content'] = ''.join([i for i in response.css(\"div.mdPstd.mdPstdQstn.sttsRslvd.clrfx div.ptsQes p::text\").extract()])\n        item['best_answer'] = ''.join([i for i in response.css(\"div.mdPstd.mdPstdBA.othrAns.clrfx div.ptsQes p.queTxt::text\").extract()])\n\n        yield item", "issue_status": "Closed", "issue_reporting_time": "2017-05-24T04:30:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "622": {"issue_url": "https://github.com/scrapy/scrapy/issues/2753", "issue_id": "#2753", "issue_summary": "MediaPipeline doesn't send request to downloader middleware.", "issue_description": "cdz620 commented on May 22, 2017\ndescription\nscrapy version: Scrapy 1.3.3\nI have user_agent and ip_proxy downloader middleware. It seem no work When MediaPipeline send request to downloader.\nsettings.py :\nDOWNLOADER_MIDDLEWARES = {\n    'xxx.middlewares.user_agent.UserAgentPoolMiddleware': 1,\n    'xxx.retry.RetryMiddleware': None,\n    'xxx.middlewares.retry.RetryMiddleware': 550,\n    'xxx.middlewares.ip_proxy.IPProxyMiddleware': 600,\n}\ncode :\nclass ImagePipeline(FilesPipeline):\n    FILES_URLS_FIELD = \"image_urls\"\n    FILES_RESULT_FIELD = 'images'\n    def get_media_requests(self, item, info):\n        return [Request(x) for x in item.get(self.files_urls_field, [])]\nQuestion\nDoes MediaPipeline data flow not comply with the scrapy general data flow \uff1f\nps\nI'm not good at English, I hope I have made it clear.", "issue_status": "Closed", "issue_reporting_time": "2017-05-22T08:21:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "623": {"issue_url": "https://github.com/scrapy/scrapy/issues/2752", "issue_id": "#2752", "issue_summary": "Can't find the middleware function process_request.", "issue_description": "ramwin commented on May 22, 2017\nIt seems that the middleware function process_request had been removed in documentation for the latest 1.4 version scrapy. How can I change the params of a request just before it was sent.", "issue_status": "Closed", "issue_reporting_time": "2017-05-22T03:39:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "624": {"issue_url": "https://github.com/scrapy/scrapy/issues/2748", "issue_id": "#2748", "issue_summary": "CLOSESPIDER_ITEMCOUNT in scrapy.extensions.closespider.CloseSpider not working !", "issue_description": "scila1996 commented on May 18, 2017 \u2022\nedited\nI'm want limit a number of item scraped . I Enabled success this extensions \"scrapy.extensions.closespider.CloseSpider\" And SETTINGS Like this\nsettings.py\nEXTENSIONS = { 'scrapy.extensions.closespider.CloseSpider': 1 }\nmy spiders\ncustom_settings = { \"CLOSESPIDER_ITEMCOUNT\" : 1 }\nbut not working and and logging ...\n{'data': 'abc'}\n2017-05-18 09:40:31 [scrapy.core.engine] INFO: Closing spider (closespider_itemcount)\n2017-05-18 09:40:31 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tinbatdongsan.com/>\n{'data': 'abc'}\n2017-05-18 09:40:31 [scrapy.core.scraper] DEBUG: Scraped from <200 http://tinbatdongsan.com/>\n{'data': 'abc'}\nHelp me ! Thank you so much !", "issue_status": "Closed", "issue_reporting_time": "2017-05-18T02:50:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "625": {"issue_url": "https://github.com/scrapy/scrapy/issues/2747", "issue_id": "#2747", "issue_summary": "I can't use the webservice(https://github.com/scrapy-plugins/scrapy-jsonrp)", "issue_description": "swumqw commented on May 18, 2017\nCan someone tell me how to use scrapy-jsonrpc?I want to use the webservice to control scrapy.(https://github.com/scrapy-plugins/scrapy-jsonrp) I modified the configuration according to the document, but I could not access http://localhost:6080/crawler....\na part of \"setting.py\":\nJSONRPC_ENABLED = True\nEXTENSIONS = {\n'scrapy_jsonrpc.webservice.WebService': 500,\n}\ni use python3.5,and scrapy 1.3.2\nif you know the problem,could you please answer me?Thank you very much...", "issue_status": "Closed", "issue_reporting_time": "2017-05-18T01:15:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "626": {"issue_url": "https://github.com/scrapy/scrapy/issues/2743", "issue_id": "#2743", "issue_summary": "connection pooling do not work when using proxy", "issue_description": "jdxin0 commented on May 17, 2017 \u2022\nedited\nScrapy create a new TCP4ClientEndpoint for each request when using proxy in ScrapyAgent while ProxyAgent(twisted) use key = (\"http-proxy\", self._proxyEndpoint) as connection pool key.\nIt causes creating new connection for each request when using proxy\uff0c\nwill get errno99: cannot assign requested address when all ports has been used (socket TIME_WAIT).\nscrapy/core/downloader/handlers/http11.py\nclass ScrapyAgent(object):\n    def _get_agent(self, request, timeout):\n        bindaddress = request.meta.get('bindaddress') or self._bindAddress\n        proxy = request.meta.get('proxy')\n        if proxy:\n            _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxyHost = to_unicode(proxyHost)\n            omitConnectTunnel = b'noconnect' in proxyParams\n            if  scheme == b'https' and not omitConnectTunnel:\n                proxyConf = (proxyHost, proxyPort,\n                             request.headers.get(b'Proxy-Authorization', None))\n                return self._TunnelingAgent(reactor, proxyConf,\n                    contextFactory=self._contextFactory, connectTimeout=timeout,\n                    bindAddress=bindaddress, pool=self._pool)\n            else:\n                endpoint = TCP4ClientEndpoint(reactor, proxyHost, proxyPort,\n                    timeout=timeout, bindAddress=bindaddress)\n                return self._ProxyAgent(endpoint)\n\n        return self._Agent(reactor, contextFactory=self._contextFactory,\n            connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)\ntwisted/web/client.py\n@implementer(IAgent)\nclass ProxyAgent(_AgentBase):\n    \"\"\"\n    An HTTP agent able to cross HTTP proxies.\n\n    @ivar _proxyEndpoint: The endpoint used to connect to the proxy.\n\n    @since: 11.1\n    \"\"\"\n\n    def __init__(self, endpoint, reactor=None, pool=None):\n        if reactor is None:\n            from twisted.internet import reactor\n        _AgentBase.__init__(self, reactor, pool)\n        self._proxyEndpoint = endpoint\n\n\n    def request(self, method, uri, headers=None, bodyProducer=None):\n        \"\"\"\n        Issue a new request via the configured proxy.\n        \"\"\"\n        # Cache *all* connections under the same key, since we are only\n        # connecting to a single destination, the proxy:\n        key = (\"http-proxy\", self._proxyEndpoint)\n\n        # To support proxying HTTPS via CONNECT, we will use key\n        # (\"http-proxy-CONNECT\", scheme, host, port), and an endpoint that\n        # wraps _proxyEndpoint with an additional callback to do the CONNECT.\n        return self._requestWithEndpoint(key, self._proxyEndpoint, method,\n                                         URI.fromBytes(uri), headers,\n                                         bodyProducer, uri)", "issue_status": "Closed", "issue_reporting_time": "2017-05-17T04:03:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "627": {"issue_url": "https://github.com/scrapy/scrapy/issues/2742", "issue_id": "#2742", "issue_summary": "AttributeError: 'NoneType' object has no attribute 'fields'", "issue_description": "hyanx commented on May 16, 2017\nwhen I crawl some data happened the issue of under. I don't known I made what's wrong?\n2017-05-16 21:03:33 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7f2d0db8e610>> Traceback (most recent call last): File \"/root/myproject/scrapy/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred result = f(*args, **kw) File \"/root/myproject/scrapy/local/lib/python2.7/site-packages/pydispatch/robustapply.py\", line 55, in robustApply return receiver(*arguments, **named) File \"/root/myproject/scrapy/local/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 221, in item_scraped slot.exporter.export_item(item) File \"/root/myproject/scrapy/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 210, in export_item values = list(self._build_row(x for _, x in fields)) File \"/root/myproject/scrapy/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 214, in _build_row for s in values: File \"/root/myproject/scrapy/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 210, in <genexpr> values = list(self._build_row(x for _, x in fields)) File \"/root/myproject/scrapy/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 63, in _get_serialized_fields field_iter = six.iterkeys(item.fields) AttributeError: 'NoneType' object has no attribute 'fields'", "issue_status": "Closed", "issue_reporting_time": "2017-05-16T13:28:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "628": {"issue_url": "https://github.com/scrapy/scrapy/issues/2739", "issue_id": "#2739", "issue_summary": "[doc] Sample code out-dated in documents", "issue_description": "Contributor\nstarrify commented on May 16, 2017 \u2022\nedited\nSample code of Here is an example that runs multiple spiders simultaneously: is believed to be out-dated and now not working.\nHere's a minimal example:\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass MySpider1(scrapy.Spider):\n    # Your first spider definition\n    name = 'spider1'\n\nclass MySpider2(scrapy.Spider):\n    # Your second spider definition\n    name = 'spider2'\n\nprocess = CrawlerProcess()\nprocess.crawl(MySpider1)\nprocess.crawl(MySpider2)\nprocess.start() # the script will block here until all crawling jobs are finished\nAnd here's the stack trace when invoking it using scrapy runspider test.py (using Scrapy v1.3.3):\nTraceback (most recent call last):\n  File \"/usr/sbin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/usr/lib/python2.7/site-packages/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/usr/lib/python2.7/site-packages/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/usr/lib/python2.7/site-packages/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"/usr/lib/python2.7/site-packages/scrapy/commands/runspider.py\", line 89, in run\n    self.crawler_process.start()\n  File \"/usr/lib/python2.7/site-packages/scrapy/crawler.py\", line 280, in start\n    reactor.run(installSignalHandlers=False)  # blocking call\n  File \"/usr/lib/python2.7/site-packages/twisted/internet/base.py\", line 1242, in run\n    self.startRunning(installSignalHandlers=installSignalHandlers)\n  File \"/usr/lib/python2.7/site-packages/twisted/internet/base.py\", line 1222, in startRunning\n    ReactorBase.startRunning(self)\n  File \"/usr/lib/python2.7/site-packages/twisted/internet/base.py\", line 730, in startRunning\n    raise error.ReactorNotRestartable()\ntwisted.internet.error.ReactorNotRestartable", "issue_status": "Closed", "issue_reporting_time": "2017-05-15T20:07:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "629": {"issue_url": "https://github.com/scrapy/scrapy/issues/2736", "issue_id": "#2736", "issue_summary": "Scrapy with Python MPI module", "issue_description": "abhisharma7 commented on May 11, 2017\nI am using Scrapy from almost 1 years I started it in a very naive way but now I started building up queues for different crawling request, but the biggest challenge I am facing is its lack of scalability it can only run on one machine with 32 threads. Does anyone implemented a solution which could scale Scrapy to multiple machines with one machine act as a master node.\nI am writing a script to integrate MPI library on top of Scrapy so that it could be run on cluster. If you guys are aware with any of the existing solution please share.\nThanks\nAbhishek", "issue_status": "Closed", "issue_reporting_time": "2017-05-10T21:33:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "630": {"issue_url": "https://github.com/scrapy/scrapy/issues/2735", "issue_id": "#2735", "issue_summary": "Functions don't work as item processors in Python 3", "issue_description": "Contributor\nwRAR commented on May 10, 2017\nThe documentation says \"processors are just callable objects, which are called with the data to be parsed, and return a parsed value. So you can use any function as input or output processor\" and this works in Python 2:\ndef proc(i):\n    return i\n\nclass FooItemLoader(ItemLoader):\n    foo_in = proc\nBut FooItemLoader.foo_in is a bound method in Python 3 so it receives a self argument and not just the field value.", "issue_status": "Closed", "issue_reporting_time": "2017-05-10T16:08:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "631": {"issue_url": "https://github.com/scrapy/scrapy/issues/2734", "issue_id": "#2734", "issue_summary": "response.meta['item'] has changed", "issue_description": "nukc commented on May 10, 2017\ncode:\n# import ...\nfrom scrapy.spiders.crawl import CrawlSpider\nfrom scrapy.http import Request\n\nclass ArticleSpider(CrawlSpider):\n    # code.....\n\n    def start_requests(self):\n        yield Request(url=self.setting_url,\n                      callback=self.parse_setting)  \n\n    def parse_setting(self, response):\n        # code.....\n        for uid in users:\n            yield Request(url=self.user_url % uid,\n                          callback=self.parse_user,\n                          meta={'user_id': uid, \"count\": count})\n\n    def parse_user(self, response):\n        data = json.loads(response.text)['data']\n        item = Article()\n        item['user_id'] = response.meta['user_id']\n\n        # code.....\n\n        count = response.meta['count']\n        print(\"username = %s\" % data['name'])\n\n        yield Request(url=self.start_url % (item['user_id'], count),\n                      callback=self.parse_item,\n                      meta={'item': item})\n\n    def parse_item(self, response):\n        data = json.loads(response.text)\n        data = data['data']\n\n        item = response.meta['item']\n\n        for cell in data:\n            if cell['has_video'] is False:\n                item['id'] = cell['item_id']\n                # code...\n                print(\"parse_item--------%s\" % item['id'])\n                yield Request(url=self.content_url % item['id'],\n                              callback=self.parse_content,\n                              meta={'item': item})\n\n    def parse_content(self, response):\n        item = response.meta['item']\n        print(\"parse_content--------%s\" % item['id'])\n        content = json.loads(response.text)['data']['content']\n        item['content'] = content\n        yield item\nitem is right in parse_item, but item has changed in parse_content, there are a lot of the same items in\nparse_content\nlog:\nparse_item--------6418342558736335361\nparse_item--------6418286349643678209\nparse_item--------6418284239581610498\nparse_item--------6418295974489227777\nparse_item--------6418034829111067138\nparse_item--------6418293518971699714\nparse_item--------6417979940523737601\nparse_item--------6417981035316773377\nparse_item--------6402028097360626178\nparse_item--------6418431607643832833\nparse_item--------6418431597376176642\nparse_item--------6418415582135190018\nparse_item--------6418415570441470466\nparse_item--------6418415567752921602\nparse_item--------6418400026266960385\nparse_item--------6418399974828016129\nparse_item--------6418399827670860290\nparse_item--------6418399798528836097\nparse_item--------6418399798004548097\nparse_item--------6418409949893755393\nparse_item--------6418400361773531650\nparse_item--------6418390433314898433\nparse_item--------6418048335759802881\nparse_item--------6417830522428326401\nparse_item--------6417351676604187137\nparse_item--------6416922409391096322\nparse_item--------6414183060027212290\nparse_item--------6418404844897305089\nparse_item--------6418403936591413762\nparse_item--------6418361360911958530\nparse_item--------6418360522529309186\nparse_item--------6418329324931252738\nparse_item--------6418150524058599937\nparse_item--------6418082048350618114\nparse_item--------6418048844008784385\nparse_item--------6417991904180306433\nparse_item--------6411412992780403202\nparse_item--------6416114889642738177\nparse_item--------6417330059622744577\nparse_item--------6417681599206261250\nparse_item--------6418128879294087682\nparse_item--------6417674545829773825\nparse_item--------6417673373089792514\nparse_item--------6417682781802856962\nparse_item--------6417672227549872642\nparse_item--------6417394300459418113\nparse_item--------6418419045539774977\nparse_item--------6418281614052164098\nparse_item--------6417915107245818370\nparse_item--------6417708495272935938\nparse_item--------6417537333049950722\nparse_item--------6417351560539406849\nparse_item--------6416430286560559618\nparse_item--------6416224528753492482\nparse_item--------6416060406762045953\nparse_item--------6416054120792195585\nparse_item--------6418332079066448385\nparse_item--------6418329433404342785\nparse_item--------6417945603132621313\nparse_item--------6417944451234136577\nparse_item--------6418402985272934914\nparse_item--------6418401409477116417\nparse_item--------6418401281844445698\nparse_item--------6418401155897885185\nparse_item--------6418382959304245761\nparse_item--------6418382957760741889\nparse_item--------6418030742470656514\nparse_item--------6418019819144086017\nparse_item--------6418113665009975809\nparse_item--------6418113772384158210\nparse_item--------6418383427241771521\nparse_item--------6417985925728436738\nparse_item--------6417638036871315969\nparse_item--------6418393940222804482\nparse_item--------6418390984832320001\nparse_item--------6418137921240433153\nparse_item--------6418013655916347905\nparse_item--------6418010737435738625\nparse_item--------6417976118363554305\nparse_item--------6417968722933187074\nparse_item--------6417651330243887617\nparse_item--------6417602714796032514\nparse_item--------6417722539400757762\nparse_item--------6418343702871147010\nparse_item--------6418342645881373185\nparse_item--------6418341865510142465\nparse_item--------6418340769089716738\nparse_item--------6418340303010267650\nparse_item--------6418339311539716609\nparse_item--------6418337020082389505\nparse_item--------6418336611213246978\nparse_item--------6418335688655110658\nparse_item--------6411470870673359362\nparse_item--------6418047333631525377\nparse_item--------6418050097505894913\nparse_item--------6418058875357889025\nparse_item--------6417936940816925185\nparse_item--------6417673720365580802\nparse_item--------6417677126643745281\nparse_item--------6417645913178636802\nparse_item--------6417391467400004098\n\nparse_content--------6417394300459418113\nparse_content--------6417981035316773377\nparse_content--------6417981035316773377\nparse_content--------6417394300459418113\nparse_content--------6417981035316773377\nparse_content--------6416922409391096322\nparse_content--------6417981035316773377\nparse_content--------6416922409391096322\nparse_content--------6417394300459418113\nparse_content--------6417394300459418113\nparse_content--------6417944451234136577\nparse_content--------6417944451234136577\nparse_content--------6416054120792195585\nparse_content--------6417944451234136577\nparse_content--------6416054120792195585\nparse_content--------6416054120792195585\nparse_content--------6418113772384158210\nparse_content--------6418113772384158210\nparse_content--------6418113772384158210\nparse_content--------6418113772384158210\nparse_content--------6417391467400004098\nparse_content--------6418113772384158210\nparse_content--------6418113772384158210\nparse_content--------6417391467400004098\nparse_content--------6417391467400004098\nparse_content--------6417391467400004098\nparse_content--------6417391467400004098\nparse_content--------6417391467400004098\nparse_content--------6418335688655110658\nparse_content--------6417391467400004098\nparse_content--------6418335688655110658\nparse_content--------6417391467400004098\nparse_content--------6417391467400004098\nparse_content--------6418335688655110658\nparse_content--------6418335688655110658\nparse_content--------6418335688655110658\nparse_content--------6418335688655110658\nparse_content--------6418335688655110658\nparse_content--------6418335688655110658\nparse_content--------6418335688655110658\nparse_content--------6417602714796032514\nparse_content--------6418335688655110658\nparse_content--------6417602714796032514\nparse_content--------6417602714796032514\nparse_content--------6417602714796032514\nparse_content--------6417602714796032514\nparse_content--------6417602714796032514\nparse_content--------6417602714796032514\nparse_content--------6417638036871315969\nparse_content--------6417602714796032514\nparse_content--------6417602714796032514\nparse_content--------6417638036871315969\nparse_content--------6418113772384158210\nparse_content--------6418113772384158210\nparse_content--------6417638036871315969\nparse_content--------6418113772384158210\nparse_content--------6417944451234136577\nparse_content--------6418113772384158210\nparse_content--------6416054120792195585\nparse_content--------6416054120792195585\nparse_content--------6416054120792195585\nparse_content--------6416054120792195585\nparse_content--------6416054120792195585\nparse_content--------6416054120792195585\nparse_content--------6416054120792195585\nparse_content--------6417394300459418113\nparse_content--------6417394300459418113\nparse_content--------6417394300459418113\nparse_content--------6417394300459418113\nparse_content--------6417394300459418113\nparse_content--------6417394300459418113\nparse_content--------6417991904180306433\nparse_content--------6417991904180306433\nparse_content--------6417991904180306433\nparse_content--------6417991904180306433\nparse_content--------6417991904180306433\nparse_content--------6417991904180306433\nparse_content--------6417991904180306433\nparse_content--------6416922409391096322\nparse_content--------6417991904180306433\nparse_content--------6417991904180306433\nparse_content--------6417991904180306433\nparse_content--------6418048335759802881\nparse_content--------6418048335759802881\nparse_content--------6418048335759802881\nparse_content--------6418399798004548097\nparse_content--------6418399798004548097\nparse_content--------6418399798004548097\nparse_content--------6418399798004548097\nparse_content--------6418399798004548097\nparse_content--------6418048335759802881\nparse_content--------6418399798004548097\nparse_content--------6418399798004548097\nparse_content--------6418399798004548097\nparse_content--------6418399798004548097\nparse_content--------6402028097360626178\nparse_content--------6417981035316773377\nparse_content--------6417981035316773377\nparse_content--------6417981035316773377\nparse_content--------6418399798004548097\nparse_content--------6417981035316773377\n\nspider_closed\nAlthough I have other solutions, but I want to know why this problem.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-05-10T10:17:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "632": {"issue_url": "https://github.com/scrapy/scrapy/issues/2731", "issue_id": "#2731", "issue_summary": "how can i create png thumbs", "issue_description": "fengfangqian commented on May 9, 2017\nhi,\n When a document is png pp but the suffix is how to generate png pss thumbnail, and now he unified generated jpg how can I set up to generate png", "issue_status": "Closed", "issue_reporting_time": "2017-05-09T09:14:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "633": {"issue_url": "https://github.com/scrapy/scrapy/issues/2730", "issue_id": "#2730", "issue_summary": "[Bug]Lost <th> and content", "issue_description": "pek77 commented on May 8, 2017 \u2022\nedited\nLost and content.\nWhen I crawled a table named \"\u57fa\u672c\u6982\u51b5\" on this site: http://fund.eastmoney.com/f10/jbgk_000001.html\nI found the tr List length is 9, not 10. The th named \"\u6700\u9ad8\u7533\u8d2d\u8d39\u7387\" and \"\u6700\u9ad8\u8d4e\u56de\u8d39\u7387\" were lost.\nI think this is a bug.\nThe spider code is as follows:\nimport scrapy\nimport requests\nimport re\nimport json\nfrom scrapy.selector import HtmlXPathSelector\nfrom fund.items import FundItem\nimport logging\n\nfundHead = []\nfundContent = []\nclass fundSpider(scrapy.Spider):\n    name = \"fund\"\n    allowed_domains = ['fund.eastmoney.com']\n    fundJS = requests.get('http://fund.eastmoney.com/js/fundcode_search.js')\n    jstext = fundJS.text\n    start_urls = [\"http://fund.eastmoney.com/f10/jbgk_000001.html\"]\n    #    \"http://fund.eastmoney.com/f10/jbgk_003816.html\",\n    start = jstext.index(\"[\")\n    end = jstext.index(\";\")\n    jstext=jstext[start:end];\n    fundJSON = json.loads(jstext)\n    j = 0;\n    # for obj in fundJSON:\n    #     start_urls.append(\"http://fund.eastmoney.com/f10/jbgk_\"+obj[0]+\".html\");\n    #     #if (j>3):\n    #     break\n    #    # j= j+1;\n    #\n\n    def parse(self, response):\n        print(\"++++++++++++++++++++\")\n        i = 0\n        fund = [];\n        print len(response.xpath(\"//table[@class='info w790']/tr\"))\n        thList =response.xpath(\"//table[@class='info w790']/tr/th\")\n        for th in thList:\n            #print tr.extract()\n            #if (len(fundHead) < len(thList)):\n            print th.xpath(\"text()\").extract()[0].encode('utf-8')\n                #fundHead.append(th.xpath(\"text()\").extract()[0])\n            td = response.xpath(\"//table[@class='info w790']/tr/td\")[i]\n            if(len(td.xpath(\"text()\"))!=0):\n                print td.xpath(\"text()\").extract()[0].encode('utf-8')\n                #fund.append(td.xpath(\"text()\").extract()[0])\n            elif (len(td.xpath(\"a\"))!=0):\n                print td.xpath(\"a/text()\").extract()[0].encode('utf-8')\n                #fund.append(td.xpath(\"a/text()\").extract()[0])\n            else :\n                print \"\"\n                #fund.append(\" \")\n            i = i+1\n        print(\"++++++++++++++++++++\")", "issue_status": "Closed", "issue_reporting_time": "2017-05-08T13:19:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "634": {"issue_url": "https://github.com/scrapy/scrapy/issues/2728", "issue_id": "#2728", "issue_summary": "DNSLookupError raises on request cyrillic domains", "issue_description": "shoreward commented on May 7, 2017 \u2022\nedited\nHello\nI had an error when requesting Cyrillic domains\nscrapy shell '\u0448\u0430\u043d\u0442\u0438-\u0448\u0430\u043d\u0442\u0438.\u0440\u0444'\n  File \"/www/.venv/lib/python2.7/site-packages/scrapy/shell.py\", line 115, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/www/.venv/lib/python2.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<string>\", line 2, in raiseException\ntwisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: xn----7sbb4ac0ad0be6cf.xn--p1ai.", "issue_status": "Closed", "issue_reporting_time": "2017-05-07T11:59:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "635": {"issue_url": "https://github.com/scrapy/scrapy/issues/2727", "issue_id": "#2727", "issue_summary": "Bring back docstring comments in CrawlSpider", "issue_description": "sseyboth commented on May 6, 2017\nCan we please bring back the docstring comments in CrawlSpider that were removed in e2290a5? The code has zero comments now. Just spent two hours pulling my hair out trying to pass extra params down the crawler. Until I found the removed docstrings in this commit that where hugely helpful :) Thanks!", "issue_status": "Closed", "issue_reporting_time": "2017-05-06T06:08:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "636": {"issue_url": "https://github.com/scrapy/scrapy/issues/2725", "issue_id": "#2725", "issue_summary": "Item Pipeline code", "issue_description": "MalikRumi commented on May 5, 2017\nI wanted to get a deeper understanding of the inner workings of the Scrapy pipeline. However, when I looked at the github repo, I found file, image, and media pipeline code that talked about network connections and downloads. But the Item Pipeline, as the docs say, is for\nAfter an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.\nWhere is that code? Is some of this code not open sourced? I see an issue #2633\nsuggesting a base class. Is the entire pipeline implementation wholly up to the programmer? Then why are we required to use process_item()? What about the Item Pipelines setting that sets the sequence in which the components are called? Where is that code and how does it work? What about scoping, state, and variables?", "issue_status": "Closed", "issue_reporting_time": "2017-05-05T16:18:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "637": {"issue_url": "https://github.com/scrapy/scrapy/issues/2723", "issue_id": "#2723", "issue_summary": "How to scrapy a web with login form in JavaScript?", "issue_description": "wenxzhen commented on May 4, 2017 \u2022\nedited\nDear all,\nAny idea to do the crawling on a web but need to login first? even worse, we need to provide the verification code before submitting the login form in JavaScript?\n<form action=\"\" onsubmit=\"return login();\">\n              <input type=\"hidden\" id=\"usertype\" name=\"usertype\" />\n                     <ul>\n                         <li>\n                            <input type=\"text\" name=\"typename\" id=\"typename\" class=\"selectBg ac_input\" />\n                         </li>\n                            </ul>\n</form>\nThanks,", "issue_status": "Closed", "issue_reporting_time": "2017-05-04T11:37:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "638": {"issue_url": "https://github.com/scrapy/scrapy/issues/2722", "issue_id": "#2722", "issue_summary": "Crawl AngularJS based website", "issue_description": "anuragmishra1 commented on May 2, 2017\nI am able crawl some pages but some pages are taking time to load because DOM is not fully rendered so that I am not able to crawl it. Can anyone have solution for this?\nThanks in advance", "issue_status": "Closed", "issue_reporting_time": "2017-05-02T12:05:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "639": {"issue_url": "https://github.com/scrapy/scrapy/issues/2719", "issue_id": "#2719", "issue_summary": "Misleading information in scrapy", "issue_description": "Contributor\ncsalazar commented on Apr 27, 2017\nHi guys, I want to notify three misleading issues in scrapy information.\nFirst one, when a site sets two cookies, response.headers shows the key Set-Cookie with the data of the last key as a string.\n{\n    'Set-Cookie':` 'laravel_session=...; expires=Thu, 27-Apr-2017 14:18:16 GMT; Max-Age=7200; path=/; httponly',\n    [...]\n}\nHowever, I can use response.headers.getlist('Set-Cookie') to get both two cookies, but I think the information returned by response.headers is misleading since it shows the value as a string and not as a list. It's rational to think that due to dictionary nature when there are two keys Set-Cookie the last will override the former one, then people will guess it's a problem of Scrapy handling cookies.\nThe second issue is that custom_settings in a spider won't be shown in the list of Overriden settings in scrapy log. I've created a vanilla project and with this spider as example:\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = 'example'\n    allowed_domains = ['example.com']\n    start_urls = ['http://example.com/']\n    custom_settings = {\n        'COOKIES_ENABLED': False\n    }\n\n    def parse(self, response):\n        pass\nWhen I run the spider, the output is:\n2017-04-27 09:33:42 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: set)\n2017-04-27 09:33:42 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'set', 'NEWSPIDER_MODULE': 'set.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['set.spiders']}\nCOOKIES_ENABLED doesn't appears on the list but it's actually False in the logic, then it's just an information issue. I was in a project where COOKIES_ENABLED=False was set in settings.py, then when I set COOKIES_ENABLED=True in a spider it continued to show COOKIES_ENABLED=False in the spider log, making a confusion if custom_settings were already taken in consideration or not.\nThe last one is that I'm using master branch, with version 1.3.3 already released and scrapy version command shows that I'm using 1.3.2.\nI could make a pull request, just let me know if there's some technical reason of that behavior in any of the three issues.\n--\nscrapy version -v output:\n$ scrapy version -v\nScrapy    : 1.3.2\nlxml      : 3.7.3.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 3.6.1 |Continuum Analytics, Inc.| (default, Mar 22 2017, 19:54:23) - [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npyOpenSSL : 17.0.0 (OpenSSL 1.0.2k  26 Jan 2017)\nPlatform  : Linux-4.4.0-75-generic-x86_64-with-debian-stretch-sid", "issue_status": "Closed", "issue_reporting_time": "2017-04-27T12:48:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "640": {"issue_url": "https://github.com/scrapy/scrapy/issues/2717", "issue_id": "#2717", "issue_summary": "TLS handshake failure", "issue_description": "povilasb commented on Apr 25, 2017\nI have this simple spider:\nimport scrapy\n\n\nclass FailingSpider(scrapy.Spider):\n    name = 'Failing Spider'\n    start_urls = ['https://www.skelbiu.lt/']\n\n    def parse(self, response: scrapy.http.Response) -> None:\n        pass\nOn debian 9 it fails with:\n2017-04-25 19:01:39 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.skelbiu.lt/>\nTraceback (most recent call last):\n  File \"/home/povilas/projects/skelbiu-scraper/pyenv/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/home/povilas/projects/skelbiu-scraper/pyenv/lib/python3.6/site-packages/twisted/python/failure.py\", line 393, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/home/povilas/projects/skelbiu-scraper/pyenv/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'sslv3 alert handshake failure')]>]\nOn debian 8 it works well.\nAnd \"https://www.skelbiu.lt\" is the only target I can reproduce the problem.\nSome more context:\n$ pyenv/bin/pip freeze\nasn1crypto==0.22.0\nattrs==16.3.0\nAutomat==0.5.0\ncffi==1.10.0\nconstantly==15.1.0\ncryptography==1.8.1\ncssselect==1.0.1\nfuncsigs==0.4\nidna==2.5\nincremental==16.10.1\nlxml==3.7.3\nmock==1.3.0\npackaging==16.8\nparsel==1.1.0\npbr==3.0.0\npy==1.4.33\npyasn1==0.2.3\npyasn1-modules==0.0.8\npycparser==2.17\nPyDispatcher==2.0.5\nPyHamcrest==1.8.5\npyOpenSSL==17.0.0\npyparsing==2.2.0\npytest==2.7.2\nqueuelib==1.4.2\nScrapy==1.3.3\nservice-identity==16.0.0\nsix==1.10.0\nTwisted==17.1.0\nw3lib==1.17.0\nzope.interface==4.4.0\n\n$ dpkg --get-selections | grep libssl\nlibssl-dev:amd64                                install\nlibssl-doc                                      install\nlibssl1.0.2:amd64                               install\nlibssl1.1:amd64                                 install\nlibssl1.1:i386                                  install\n\n\n$ apt-cache show libssl1.1\nPackage: libssl1.1\nSource: openssl\nVersion: 1.1.0e-1\nAny ideas what I should look for? :)\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2017-04-25T16:23:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "641": {"issue_url": "https://github.com/scrapy/scrapy/issues/2716", "issue_id": "#2716", "issue_summary": "Cannot select with .css selector or xpath selector?", "issue_description": "kassiansun commented on Apr 24, 2017\nPage\nhttp://libproject.hkbu.edu.hk/was40/search?lang=cht&channelid=47953\nHow to Reproduce\nRun scrapy shell: scrapy shell \"http://libproject.hkbu.edu.hk/was40/search?lang=cht&channelid=47953\"\nBoth xpath('//*[@id=\"gallery\"]/table/tbody/tr[12]/td/table/tbody/tr/td[3]/table/tbody/tr[2]/td/table/tbody/tr/td[4]') and css('#gallery > table > tbody > tr:nth-child(20) > td > table > tbody > tr > td:nth-child(3) > table > tbody > tr:nth-child(2) > td > table > tbody > tr > td:nth-child(4)') will not select the second column of first row(contains \u963f\u9b4f<br>Awei), is this a bug of scrapy?\nxpath/css copied from chrome developer tool.", "issue_status": "Closed", "issue_reporting_time": "2017-04-24T09:10:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "642": {"issue_url": "https://github.com/scrapy/scrapy/issues/2715", "issue_id": "#2715", "issue_summary": "Inconsistency while crawling few URLS. ( out of 115 urls i was able to crawl over 108).", "issue_description": "rpadma commented on Apr 24, 2017\nLog messages for one such URL which I am not able to crawl\nG:\\webc\\tut>scrapy crawl painting\n2017-04-24 03:59:32 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: tut)\n2017-04-24 03:59:32 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tut.spiders', 'SPIDER_MODULES': ['tut.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tut'}\n2017-04-24 03:59:32 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n'scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.corestats.CoreStats']\n2017-04-24 03:59:32 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-04-24 03:59:32 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n'scrapy.spidermiddlewares.referer.RefererMiddleware',\n'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-04-24 03:59:32 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-04-24 03:59:32 [scrapy.core.engine] INFO: Spider opened\n2017-04-24 03:59:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-04-24 03:59:32 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n2017-04-24 03:59:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.saatchiart.com/robots.txt> (referer: None)\n2017-04-24 03:59:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.saatchiart.com/art/Painting-silence-waters-13/891427/3273220/view> (referer: None)\n2017-04-24 03:59:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.saatchiart.com/art/Painting-silence-waters-13/891427/3273220/view> (referer: None)\nTraceback (most recent call last):\nFile \"C:\\Users\\Rohit\\AppData\\Roaming\\Python\\Python27\\site-packages\\twisted\\internet\\defer.py\", line 653, in _runCallbacks\ncurrent.result = callback(current.result, *args, **kw)\nFile \"G:\\webc\\tut\\tut\\spiders\\Paintings_spider.py\", line 40, in parse\nf.write(str(paintingname)+\",\"+str((\"\".join(str(e).replace(\"\\n\",\"\") for e in authorname).replace(\",\",\"\").replace(\"[u'\",\"\")).replace(\"']\",\"\"))+\",\"+str(authorcountry)+\",\"+str(sizeheight).replace(\"H\",\"\")+\",\"+str(sizeWidth).replace(\"W\",\"\")+\",\"+str(sizedepth)+\",\"+str(((price).replace(\",\",\"\")).replace(\"\\n\",\"\")).replace(\"$\",\"\")+\",\"+str(views)+\",\"+str(favorites)+\",\"+str(publisheddate)+\",\"+str(modifieddate)+\",\"+str(\" \".join(str(e).replace(\"\\n\",\"\") for e in paintingtype)).replace(\"\\n\",\"\")+\",\"+str(paintingimage)+\",\"+str(Authorurl)+\"\\n\")\nFile \"G:\\webc\\tut\\tut\\spiders\\Paintings_spider.py\", line 40, in\nf.write(str(paintingname)+\",\"+str((\"\".join(str(e).replace(\"\\n\",\"\") for e in authorname).replace(\",\",\"\").replace(\"[u'\",\"\")).replace(\"']\",\"\"))+\",\"+str(authorcountry)+\",\"+str(sizeheight).replace(\"H\",\"\")+\",\"+str(sizeWidth).replace(\"W\",\"\")+\",\"+str(sizedepth)+\",\"+str(((price).replace(\",\",\"\")).replace(\"\\n\",\"\")).replace(\"$\",\"\")+\",\"+str(views)+\",\"+str(favorites)+\",\"+str(publisheddate)+\",\"+str(modifieddate)+\",\"+str(\" \".join(str(e).replace(\"\\n\",\"\") for e in paintingtype)).replace(\"\\n\",\"\")+\",\"+str(paintingimage)+\",\"+str(Authorurl)+\"\\n\")\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\xdf' in position 9: ordinal not in range(128)\n2017-04-24 03:59:33 [scrapy.core.engine] INFO: Closing spider (finished)\n2017-04-24 03:59:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 553,\n'downloader/request_count': 2,\n'downloader/request_method_count/GET': 2,\n'downloader/response_bytes': 26470,\n'downloader/response_count': 2,\n'downloader/response_status_count/200': 2,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2017, 4, 24, 7, 59, 33, 946000),\n'log_count/DEBUG': 3,\n'log_count/ERROR': 1,\n'log_count/INFO': 7,\n'response_received_count': 2,\n'scheduler/dequeued': 1,\n'scheduler/dequeued/memory': 1,\n'scheduler/enqueued': 1,\n'scheduler/enqueued/memory': 1,\n'spider_exceptions/UnicodeEncodeError': 1,\n'start_time': datetime.datetime(2017, 4, 24, 7, 59, 32, 691000)}\n2017-04-24 03:59:33 [scrapy.core.engine] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2017-04-24T08:04:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "643": {"issue_url": "https://github.com/scrapy/scrapy/issues/2712", "issue_id": "#2712", "issue_summary": "Scrapy logging inside thread only show record name as scrapy, no more.", "issue_description": "vionemc commented on Apr 16, 2017\nSo I run Scrapy inside a Thread object, like this:\nclass myThread(threading.Thread):\n    #omitted\n    \n def start(self, spiders):\n  settings = get_project_settings()\n  settings['LOG_ENABLED'] = False\n  process = CrawlerProcess(settings)\n  for spider in spiders:\n   process.crawl(spider)\n\n  process.start()\nBut the log becomes like this:\n2017-04-16 07:13:48 [scrapy] INFO: Spider opened\n2017-04-16 07:13:48 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-04-16 07:13:48 [root] INFO: Using crawlera at http://proxy.crawlera.com:8010?noconnect (user: ae3151d...)\n2017-04-16 07:13:48 [root] INFO: CrawleraMiddleware: disabling download delays on Scrapy side to optimize delays introduced by Crawlera. To avoid this behaviour you can use the CRAWLERA_PRESERVE_DELAY setting but keep in mind that this may slow down the crawl significantly\n2017-04-16 07:13:48 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-04-16 07:13:48 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2017-04-16 07:13:48 [scrapy] INFO: Enabled downloader middlewares:\nPlease notice [scrapy] instead of [scrapy.middlewares] etc. It makes it hard for me to filter the log.", "issue_status": "Closed", "issue_reporting_time": "2017-04-16T00:23:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "644": {"issue_url": "https://github.com/scrapy/scrapy/issues/2709", "issue_id": "#2709", "issue_summary": "A spider might still get closed even if it does have scheduled some requests in a `spider_idle` handle", "issue_description": "Contributor\nstarrify commented on Apr 12, 2017\nRegarding the spider_idle signal, the doc says:\nIf the idle state persists after all handlers of this signal have finished, the engine starts closing the spider. After the spider has finished closing, the spider_closed signal is sent.\nYou can, for example, schedule some requests in your spider_idle handler to prevent the spider from being closed.\nHowever, the spider may still close even when it has scheduled some requests within a spider_idle handler, if all such requests are filtered due to duplication.\nIn this case, the engine would still have self.spider_is_idle(spider) evaluated to True after finishing all spider_idle handlers.\nRelated code in scrapy/core/engine.py\nBelow is an example illustrating this issue. In this example, one may expect the spider to send three requests, but with the current version of Scrapy (05ce1296) there'll be only one request.\n# coding: utf8\n\nimport scrapy\nimport scrapy.signals\n\n\nclass TestSpider(scrapy.Spider):\n\n    name = 'test'\n    urls = None\n\n    def start_requests(self):\n        self.urls = [\n            'http://httpbin.org/get?foo=bar',\n            'http://httpbin.org/get?bar=baz',\n            'http://httpbin.org/get?baz=foo',\n            'http://httpbin.org/get?baz=foo',\n        ]\n        self.crawler.signals.connect(self.on_idle, scrapy.signals.spider_idle)\n        return []\n\n    def on_idle(self, spider):\n        if self.urls:\n            url = self.urls.pop()\n            self.crawler.engine.crawl(scrapy.Request(url), self)\n\n    def parse(self, response):\n        pass\n(Save it to a.py and use scrapy runspider a.py to test.)\nMy suggestion is to have ExecutionEngine._spider_idle updated to make sure the spider does not get closed, as long as there's any request scheduled within a spider_idle handler.", "issue_status": "Closed", "issue_reporting_time": "2017-04-12T13:05:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "645": {"issue_url": "https://github.com/scrapy/scrapy/issues/2708", "issue_id": "#2708", "issue_summary": "Not able to run Scrapy shell - TypeError", "issue_description": "shashank-sharma commented on Apr 12, 2017 \u2022\nedited\nI was following one of your scrapy tutorials and I am stuck with this error. I am not sure what's wrong.\nThis is the error which i am getting:\nvipul@shashank:~/Mythical/Scrapy/tutorial$ scrapy shell 'http://quotes.toscrape.com/page/1/'\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/usr/local/lib/python3.4/dist-packages/scrapy/cmdline.py\", line 6, in <module>\n    import pkg_resources\n  File \"/usr/local/lib/python3.4/dist-packages/pkg_resources/__init__.py\", line 72, in <module>\n    import packaging.requirements\n  File \"/usr/local/lib/python3.4/dist-packages/packaging/requirements.py\", line 59, in <module>\n    MARKER_EXPR = originalTextFor(MARKER_EXPR())(\"marker\")\nTypeError: __call__() missing 1 required positional argument: 'name'\nvipul@shashank:~/Mythical/Scrapy/tutorial$ python --version\nPython 2.7.6", "issue_status": "Closed", "issue_reporting_time": "2017-04-12T11:02:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "646": {"issue_url": "https://github.com/scrapy/scrapy/issues/2706", "issue_id": "#2706", "issue_summary": "Dupefilters MemoryError", "issue_description": "dsandesari commented on Apr 12, 2017\nVersion:\nScrapy    : 1.1.1\nlxml      : 3.6.1.0\nlibxml2   : 2.9.1\nTwisted   : 16.3.0\nPython    : 2.7.6 (default, Jun 22 2015, 17:58:13) - [GCC 4.8.2]\npyOpenSSL : 0.13 (OpenSSL 1.0.1f 6 Jan 2014)\nPlatform  : Linux-3.13.0-92-generic-x86_64-with-Ubuntu-14.04-trusty\nException:\n2017-04-07 07:00:21 [scrapy] DEBUG: Crawled (200) <GET http://www.xyz.com/people_directory/professional_profile/B-2-14893> (referer: http://www.xyz.com/people_directory/professional_profile/B-1-149)\n2017-04-07 07:00:21 [scrapy] DEBUG: Scraped from <200 http://www.xyz.com/people_directory/professional_profile/B-2-14893>\n{'url': 'http://www.xyz.com/people_directory/professional_profile/B-2-14893'}\n2017-04-07 07:00:21 [scrapy] ERROR: Spider error processing <GET http://www.xyz.com/people_directory/professional_profile/B-2-14893> (referer: http://www.xyz.com/people_directory/professional_profile/B-1-149)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\nGeneratorExit\nException RuntimeError: 'generator ignored GeneratorExit' in <generator object iter_errback at 0x7f767e516370> ignored\nUnhandled error in Deferred:\n2017-04-07 07:00:21 [twisted] CRITICAL: Unhandled error in Deferred:\n\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1194, in run\n    self.mainLoop()\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1203, in mainLoop\n    self.runUntilCurrent()\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 825, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py\", line 671, in _tick\n    taskObj._oneWorkUnit()\n--- <exception caught here> ---\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py\", line 517, in _oneWorkUnit\n    result = next(self._iterator)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py\", line 63, in <genexpr>\n    work = (callable(elem, *args, **named) for elem in iterable)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/scraper.py\", line 183, in _process_spidermw_output\n    self.crawler.engine.crawl(request=output, spider=spider)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 209, in crawl\n    self.schedule(request, spider)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 215, in schedule\n    if not self.slot.scheduler.enqueue_request(request):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/scheduler.py\", line 54, in enqueue_request\n    if not request.dont_filter and self.df.request_seen(request):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/dupefilters.py\", line 51, in request_seen\n    self.fingerprints.add(fp)\nexceptions.MemoryError: \n2017-04-07 07:00:21 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py\", line 517, in _oneWorkUnit\n    result = next(self._iterator)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py\", line 63, in <genexpr>\n    work = (callable(elem, *args, **named) for elem in iterable)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/scraper.py\", line 183, in _process_spidermw_output\n    self.crawler.engine.crawl(request=output, spider=spider)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 209, in crawl\n    self.schedule(request, spider)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 215, in schedule\n    if not self.slot.scheduler.enqueue_request(request):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/scheduler.py\", line 54, in enqueue_request\n    if not request.dont_filter and self.df.request_seen(request):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/dupefilters.py\", line 51, in request_seen\n    self.fingerprints.add(fp)\nMemoryError", "issue_status": "Closed", "issue_reporting_time": "2017-04-11T19:15:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "647": {"issue_url": "https://github.com/scrapy/scrapy/issues/2704", "issue_id": "#2704", "issue_summary": "\u63a8\u8350\u4e00\u4e2aScrapy\u7684\u9879\u76ee", "issue_description": "xiyouMc commented on Apr 10, 2017\n\u6e90\u7801\u5730\u5740", "issue_status": "Closed", "issue_reporting_time": "2017-04-10T03:10:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "648": {"issue_url": "https://github.com/scrapy/scrapy/issues/2703", "issue_id": "#2703", "issue_summary": "Incompatible library version: etree.cpython-35m-darwin.so", "issue_description": "bozzmob commented on Apr 9, 2017\nI installed scrapy. Tried to run the first sample program from here. I am getting the following error every time I run the command.\nbozzmob$ scrapy runspider quotes-scraper.py -o quotes-output.json\nTraceback (most recent call last):\n  File \"/Users/satjagan/miniconda3/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/Users/satjagan/miniconda3/lib/python3.5/site-packages/scrapy/__init__.py\", line 34, in <module>\n    from scrapy.spiders import Spider\n  File \"/Users/satjagan/miniconda3/lib/python3.5/site-packages/scrapy/spiders/__init__.py\", line 10, in <module>\n    from scrapy.http import Request\n  File \"/Users/satjagan/miniconda3/lib/python3.5/site-packages/scrapy/http/__init__.py\", line 11, in <module>\n    from scrapy.http.request.form import FormRequest\n  File \"/Users/satjagan/miniconda3/lib/python3.5/site-packages/scrapy/http/request/form.py\", line 9, in <module>\n    import lxml.html\n  File \"/Users/satjagan/miniconda3/lib/python3.5/site-packages/lxml/html/__init__.py\", line 54, in <module>\n    from .. import etree\nImportError: dlopen(/Users/satjagan/miniconda3/lib/python3.5/site-packages/lxml/etree.cpython-35m-darwin.so, 2): Library not loaded: @rpath/libxml2.2.dylib\n  Referenced from: /Users/satjagan/miniconda3/lib/python3.5/site-packages/lxml/etree.cpython-35m-darwin.so\n  Reason: Incompatible library version: etree.cpython-35m-darwin.so requires version 12.0.0 or later, but libxml2.2.dylib provides version 10.0.0\nI tried lot of solutions on stackoverflow(one, two, etc.,) and other github issues(one, two, etc.,). None of them helped. Please let me know how to fix this.", "issue_status": "Closed", "issue_reporting_time": "2017-04-08T19:49:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "649": {"issue_url": "https://github.com/scrapy/scrapy/issues/2698", "issue_id": "#2698", "issue_summary": "Invalid JSON output when nothing is crawled", "issue_description": "arvindpdmn commented on Apr 4, 2017\nJSON output is okay in normal cases when URLs are crawled. But in some cases, my application logic determines that nothing needs to be crawled. In such a case, a stray [ appears on the output.", "issue_status": "Closed", "issue_reporting_time": "2017-04-04T09:24:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "650": {"issue_url": "https://github.com/scrapy/scrapy/issues/2697", "issue_id": "#2697", "issue_summary": "Scraping semantic data: perhaps an extension", "issue_description": "arvindpdmn commented on Apr 4, 2017\nThis is a feature request.\nI understand that it's possible to add custom extensions. I am proposing extension(s) to scrape semantic data. To my knowledge, many web pages use one of these:\nMicrodata\nOpengraph\nParsley\nTypically, the data extracted will include date of publishing, author name, site name, title, main image URL, etc. Since not all pages on the web are using semantic markup, the extension can also have a feature to create custom mapping: css/xpath selection mapped to relevant data.\nWhat do you think? Any pointers on how to create a custom extension? Thx.", "issue_status": "Closed", "issue_reporting_time": "2017-04-04T06:00:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "651": {"issue_url": "https://github.com/scrapy/scrapy/issues/2696", "issue_id": "#2696", "issue_summary": "Selector.re method interprets HTML escape characters and fails JSON decoding", "issue_description": "osjerick commented on Apr 4, 2017\nThis piece of code fails:\nimport json, re\nfrom scrapy.selector import Selector\n\nsample_text = \"\"\"\n<script type=\"text/javascript\">\n   pageProductData = {\"bulletAttributes\":[\"Mid rise.\",\"Slim through the hip and thigh.\",\"Skinny leg opening.\",\"Inseam: regular 28.5&#34;/72 cm, tall 31&#34;/77 cm, petite 26&#34;/66 cm.\",\"Model is wearing a regular Gap size 27.\"]};\n</script>\"\"\"\ncode_selector = Selector(text=sample_text)\nvar_value = code_selector.xpath('//script').re_first(r'pageProductData = ({.*?});')\nparsed_data = json.loads(var_value)\nJSONDecodeError: Expecting ',' delimiter: line 1 column 112 (char 111)\nHere, var_value is:\n'{\"bulletAttributes\":[\"Mid rise.\",\"Slim through the hip and thigh.\",\"Skinny leg opening.\",\"Inseam: regular 28.5\"/72 cm, tall 31\"/77 cm, petite 26\"/66 cm.\",\"Model is wearing a regular Gap size 27.\"]}'\nThe problem is because the &#34; HTML character is interpreted (\") within the extracted string by the re method, and this damage the string before the JSON decoding.\nAnd this doesn't:\nimport json, re\nfrom scrapy.selector import Selector\n\nsample_text = \"\"\"\n<script type=\"text/javascript\">\n   pageProductData = {\"bulletAttributes\":[\"Mid rise.\",\"Slim through the hip and thigh.\",\"Skinny leg opening.\",\"Inseam: regular 28.5&#34;/72 cm, tall 31&#34;/77 cm, petite 26&#34;/66 cm.\",\"Model is wearing a regular Gap size 27.\"]};\n</script>\"\"\"\ncode_selector = Selector(text=sample_text)\nvar_value = re.search(r'pageProductData = ({.*?});', code_selector.extract())\nparsed_data = json.loads(var_value.group(1))\nThe extracted value here is:\n'{\"bulletAttributes\":[\"Mid rise.\",\"Slim through the hip and thigh.\",\"Skinny leg opening.\",\"Inseam: regular 28.5&#34;/72 cm, tall 31&#34;/77 cm, petite 26&#34;/66 cm.\",\"Model is wearing a regular Gap size 27.\"]}'\nscrapy version -v:\nScrapy    : 1.3.3\nlxml      : 3.6.4.0\nlibxml2   : 2.9.4\ncssselect : 1.0.0\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 3.5.3 |Anaconda custom (64-bit)| (default, Feb 22 2017, 21:28:42) [MSC v.1900 64 bit (AMD64)]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2j  26 Sep 2016)\nPlatform  : Windows-7-6.1.7601-SP1", "issue_status": "Closed", "issue_reporting_time": "2017-04-04T00:21:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "652": {"issue_url": "https://github.com/scrapy/scrapy/issues/2694", "issue_id": "#2694", "issue_summary": "Unhandled error in Deferred: - but what error?", "issue_description": "mohmad-null commented on Apr 1, 2017\nUpgraded from Scrapy 1.2.2 to 1.3.3. Twisted 17.x\nPython 2.7.x on Windows.\nSince upgrading, if I have a problem in my spider, i.e. my spider classes init gets a KeyError because I made a mistake, I see:\n\"Unhandled error in Deferred:\"\nAnd that's it. There is no traceback. I had to edit Crawler.py, Line 76, and use the traceback module to see the content of the error.\nTo the best of my knowledge I don't believe I'm doing anything to the logger (apart from a getLogger(name) call), and I can't see anything in the release notes that would indicate why this is.\nAs you can imagine, it makes debugging very difficult.", "issue_status": "Closed", "issue_reporting_time": "2017-04-01T17:12:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "653": {"issue_url": "https://github.com/scrapy/scrapy/issues/2693", "issue_id": "#2693", "issue_summary": "Enable DOWNLOADER_MIDDLEWARES has error for mac", "issue_description": "littlestar1998 commented on Apr 1, 2017 \u2022\nedited by pawelmhm\n> scrapy version -v\n\nScrapy    : 1.3.0\nlxml      : 3.7.2.0\nlibxml2   : 2.9.4\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.16.0\nTwisted   : 16.6.0\nPython    : 2.7.10 (default, Jul 30 2016, 19:40:32) - [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2j  26 Sep 2016)\nPlatform  : Darwin-16.4.0-x86_64-i386-64bit\nPython 2.7.10\nBut i disable DOWNLOADER_MIDDLEWARES on settings.py it's ok not error;\nenable\nDOWNLOADER_MIDDLEWARES = {\n    'tutorial.middlewares.MyCustomDownloaderMiddleware': 543,\n}\non settings.py\nrun again has error\ndemo\nxiaoxingxingdeMacBook-Pro:tutorial xiaoxingxing$ scrapy crawl dmoz\n2017-04-01 17:49:34 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tutorial)\n2017-04-01 17:49:34 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial'}\n2017-04-01 17:49:34 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\nUnhandled error in Deferred:\n2017-04-01 17:49:34 [twisted] CRITICAL: Unhandled error in Deferred:\n\n2017-04-01 17:49:34 [twisted] CRITICAL:\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\n    result = g.send(result)\n  File \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 90, in crawl\n    six.reraise(*exc_info)\n  File \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 72, in crawl\n    self.engine = self._create_engine()\n  File \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 97, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/Library/Python/2.7/site-packages/scrapy/core/engine.py\", line 69, in __init__\n    self.downloader = downloader_cls(crawler)\n  File \"/Library/Python/2.7/site-packages/scrapy/core/downloader/__init__.py\", line 88, in __init__\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"/Library/Python/2.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/Library/Python/2.7/site-packages/scrapy/middleware.py\", line 34, in from_settings\n    mwcls = load_object(clspath)\n  File \"/Library/Python/2.7/site-packages/scrapy/utils/misc.py\", line 49, in load_object\n    raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name))\nNameError: Module 'tutorial.middlewares' doesn't define any object named 'MyCustomDownloaderMiddleware'", "issue_status": "Closed", "issue_reporting_time": "2017-04-01T09:54:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "654": {"issue_url": "https://github.com/scrapy/scrapy/issues/2692", "issue_id": "#2692", "issue_summary": "adding HTTP/2 support for better result.", "issue_description": "shankarj67 commented on Mar 30, 2017\nNow twisted supports only HTTP/0 or HTTP/1, Is there any way we can add hyper to integrate scrapy\nwith HTTP/2.", "issue_status": "Closed", "issue_reporting_time": "2017-03-30T16:06:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "655": {"issue_url": "https://github.com/scrapy/scrapy/issues/2689", "issue_id": "#2689", "issue_summary": "Parse HTML Pages with VUE.js templates", "issue_description": "ferasodh commented on Mar 28, 2017\nI'm new to scrapy and I tried to scrap some pages that uses VUE.js. When getting response I get html tags with VUE.js placeholders instead of real values. Is there a way to parse the page and get real values? any ideas on this?\nThanks,", "issue_status": "Closed", "issue_reporting_time": "2017-03-27T20:15:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "656": {"issue_url": "https://github.com/scrapy/scrapy/issues/2687", "issue_id": "#2687", "issue_summary": "weird callback function for crawlspider problem", "issue_description": "quietcoding commented on Mar 27, 2017 \u2022\nedited\nHi,\nI was attempting to scrape data from a website and export it to a csv file.\nHowever, the issue arose when I attempted to use a function inside another function when a callback function was being used.\nExample:\nClass Spider(CrawlSpider):\n// initial code...\nrules = (Rule(LinkExtractor(callback='funcA')))\ndef funcA(self, response):\nfuncB()\nfuncB(self, response):\n// do stuff\nThe issue arose when I attempted to log the csv data inside funcB which funcA called. I didn't get any errors or problems, but it would not write unless the csv writer was directly inside funcA. What's more weird was being able to print(statements) inside funcB but not get a csv writer or even item pipeline to work from funcB. Once I cut out funcB completely and just threw it all into funcA, it worked fine. Can anyone help me investigate this weird problem?", "issue_status": "Closed", "issue_reporting_time": "2017-03-27T02:04:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "657": {"issue_url": "https://github.com/scrapy/scrapy/issues/2686", "issue_id": "#2686", "issue_summary": "custom_settings in spider won't work for LOG_FILE", "issue_description": "vionemc commented on Mar 25, 2017\nIt works okay for the other settings, but it doesn't work for LOG_FILE. The spider keeps writing to the console instead of to the log file.", "issue_status": "Closed", "issue_reporting_time": "2017-03-25T06:43:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "658": {"issue_url": "https://github.com/scrapy/scrapy/issues/2684", "issue_id": "#2684", "issue_summary": "image download keep original name", "issue_description": "chriseugenerodriguez commented on Mar 24, 2017\nHi,\nI am really interested in knowing to remove sha1 and keep original name of image downloaded using scrapy. I am using this to mass audit website and optimize it with gulp for easy convert and replace. Please advise.", "issue_status": "Closed", "issue_reporting_time": "2017-03-24T17:48:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "659": {"issue_url": "https://github.com/scrapy/scrapy/issues/2679", "issue_id": "#2679", "issue_summary": "How can I know whether the response is from cache or not", "issue_description": "eadren commented on Mar 23, 2017\nHow can I know whether the response is from cache or not", "issue_status": "Closed", "issue_reporting_time": "2017-03-23T14:08:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "660": {"issue_url": "https://github.com/scrapy/scrapy/issues/2677", "issue_id": "#2677", "issue_summary": "Duplicate Content-Length header for POST requests with empty body", "issue_description": "Contributor\nredapple commented on Mar 23, 2017\nOriginally reported on StackOverflow.\nHTTP requests with POST method and no request body are sent with 2 Content-Length: 0 headers.\nReproducible with Scrapy 1.3.3 and Twisted 17.1:\n$ scrapy version -v\nScrapy    : 1.3.3\nlxml      : 3.7.3.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 2.7.12 (default, Nov 19 2016, 06:48:10) - [GCC 5.4.0 20160609]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2g  1 Mar 2016)\nPlatform  : Linux-4.8.0-41-generic-x86_64-with-Ubuntu-16.10-yakkety\n$ scrapy shell\n>>> fetch(scrapy.Request('http://httpbin.org/post', method='POST'))\n2017-03-23 10:58:34 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://httpbin.org/post> (referer: None)\n(note that httpbin.org/post output does not show duplicate headers, but the Wireshark capture does)\nWireshark sniffing:\nPOST /post HTTP/1.1\nContent-Length: 0\nContent-Length: 0\nAccept-Language: en\nAccept-Encoding: gzip,deflate\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Scrapy/1.3.3 (+http://scrapy.org)\nHost: httpbin.org\n\nHTTP/1.1 200 OK\nConnection: keep-alive\nServer: gunicorn/19.7.1\nDate: Thu, 23 Mar 2017 09:58:34 GMT\nContent-Type: application/json\nAccess-Control-Allow-Origin: *\nAccess-Control-Allow-Credentials: true\nContent-Length: 458\nVia: 1.1 vegur\n\n{\n  \"args\": {}, \n  \"data\": \"\", \n  \"files\": {}, \n  \"form\": {}, \n  \"headers\": {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \n    \"Accept-Encoding\": \"gzip,deflate\", \n    \"Accept-Language\": \"en\", \n    \"Connection\": \"close\", \n    \"Content-Length\": \"0\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"Scrapy/1.3.3 (+http://scrapy.org)\"\n  }, \n  \"json\": null, \n  \"origin\": \"89.84.122.217\", \n  \"url\": \"http://httpbin.org/post\"\n}\nThis is due to Twisted's twisted/twisted#670 since v17.1 and Scrapy's #1800 since Scrapy 1.1.0", "issue_status": "Closed", "issue_reporting_time": "2017-03-23T10:03:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "661": {"issue_url": "https://github.com/scrapy/scrapy/issues/2674", "issue_id": "#2674", "issue_summary": "Scrapy crawler spider doesn't follow links", "issue_description": "lzqbegin commented on Mar 22, 2017 \u2022\nedited\nFor this, I used example in Scrapy crawl spider example: http://doc.scrapy.org/en/latest/topics/spiders.html\nI want to get links from a web page and follow them to parse table with statistics, but somehow I don't see that any links would be grabbed and followed to web page that has data. Here is my script:\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.item import Item, Field\n\nclass CNNVDItem(Item):\n    title = Field()\n\nclass CNNVDSpider(CrawlSpider):\n    name = 'cnnvd'\n    allowed_domains = 'www.cnnvd.org.cn'\n    start_urls = [\n        'http://www.cnnvd.org.cn/vulnerability/index/p/1/',\n    ]\n\n    rules = (\n        Rule(LinkExtractor(allow=r'\\?&amp;p=\\d{1,5}'), follow=True, ),\n        Rule(LinkExtractor(allow=r'/vulnerability/show/cv_id/(\\d+)'), callback='parse_detail', follow=True, ),\n    )\n\n    def parse_detail(self, response):\n        item = CNNVDItem()\n        title = response.xpath('//table[@class=\"details\"]/tr[1]/td[2]/text()').extract()[0]\n        item['title'] = title\n        yield item\nAnd here I have response from terminal:\n2017-03-15 11:06:41 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: scrapybot)\n2017-03-15 11:06:41 [scrapy.utils.log] INFO: Overridden settings: {}\n2017-03-15 11:06:41 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2017-03-15 11:06:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-03-15 11:06:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-03-15 11:06:41 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-03-15 11:06:41 [scrapy.core.engine] INFO: Spider opened\n2017-03-15 11:06:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-03-15 11:06:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-03-15 11:06:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.cnnvd.org.cn/vulnerability/index/p/1/> (referer: None)\n2017-03-15 11:06:42 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite request to 'www.cnnvd.org.cn': <GET http://www.cnnvd.org.cn/vulnerability/show/cv_id/2017030912>\n2017-03-15 11:06:42 [scrapy.core.engine] INFO: Closing spider (finished)\n2017-03-15 11:06:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 238,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 35358,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2017, 3, 15, 15, 6, 42, 833131),\n 'log_count/DEBUG': 3,\n 'log_count/INFO': 7,\n 'offsite/domains': 1,\n 'offsite/filtered': 20,\n 'request_depth_max': 1,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2017, 3, 15, 15, 6, 41, 450096)}\n2017-03-15 11:06:42 [scrapy.core.engine] INFO: Spider closed (finished)\nWhat I am doing, wrong here? Any ideas would be great help.\nI'm running Scrapy version 1.3.2 on Python 2.7.5", "issue_status": "Closed", "issue_reporting_time": "2017-03-22T08:26:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "662": {"issue_url": "https://github.com/scrapy/scrapy/issues/2673", "issue_id": "#2673", "issue_summary": "SplashTextResponse mismatch with HtmlResponse in CrawlerSpider", "issue_description": "SmartAI commented on Mar 22, 2017\nI use splash plugin to render html in CrawlerSpider, but seems the rendered response type is SplashTextResponse, but in CrawlerSpider implementation _requests_to_follow only\nallow HtmlResponse\nthis is the code link\nSo can I just remove the condition?\nif not isinstance(response, HtmlResponse):\n            return", "issue_status": "Closed", "issue_reporting_time": "2017-03-22T07:21:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "663": {"issue_url": "https://github.com/scrapy/scrapy/issues/2665", "issue_id": "#2665", "issue_summary": "Getting issue in deploy Scrapy project on Scrapy Cloud", "issue_description": "rubhanazeem commented on Mar 19, 2017\nI am trying to deploy scrapy app on scrapy cloud. But after deploy, Spider is not available to run. After deploy I am getting this status.\n{\"project\": 169397, \"version\": \"f79741f-master\", \"spiders\": 0, \"status\": \"ok\"}\nOn running scrapy list, it is showing spider.", "issue_status": "Closed", "issue_reporting_time": "2017-03-18T21:01:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "664": {"issue_url": "https://github.com/scrapy/scrapy/issues/2662", "issue_id": "#2662", "issue_summary": "why Request(url can not be response.url)", "issue_description": "ReZeroS commented on Mar 18, 2017\nI have a question, anyone can help me?\nmy code is here:\nhttp://paste.ubuntu.com/24200006/\nthe question is why the call_back will not run when the Request has a args is response.url", "issue_status": "Closed", "issue_reporting_time": "2017-03-18T07:15:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "665": {"issue_url": "https://github.com/scrapy/scrapy/issues/2661", "issue_id": "#2661", "issue_summary": "Scrapy docs: 'make htmlview' does not open the webbrowser", "issue_description": "Contributor\nharshasrinivas commented on Mar 18, 2017\nAfter compiling the scrapy documentation, make htmlview does not open the web browser.\n>>> webbrowser.open('build/html/index.html') # does not work\n>>> webbrowser.open('file:///path/to/the/index.html') # works\nIt works when I give the full path to the build/html/index.html file.\nI have tested this using Python 2.7 and 3.6 on MacOS.\nI believe this is an issue specific to MacOS. A minor change to the webbrowser.open() command in the Makefile should fix this bug. Shall I go ahead and fix this?", "issue_status": "Closed", "issue_reporting_time": "2017-03-17T20:29:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "666": {"issue_url": "https://github.com/scrapy/scrapy/issues/2660", "issue_id": "#2660", "issue_summary": "SelectorList method __nonzero__ is documented, but not implemented.", "issue_description": "djbirke commented on Mar 17, 2017\nSelectorList method __nonzero__ in documentation\nPython 3.6.0 |Continuum Analytics, Inc.| (default, Dec 23 2016, 12:22:00) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import scrapy\n>>> print(scrapy.__version__, dir(scrapy.selector.SelectorList), sep='\\n')\n1.3.2\n['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', 'append', 'clear', 'copy', 'count', 'css', 'extend', 'extract', 'extract_first', 'extract_unquoted', 'index', 'insert', 'pop', 're', 're_first', 'remove', 'reverse', 'select', 'sort', 'x', 'xpath']", "issue_status": "Closed", "issue_reporting_time": "2017-03-17T06:44:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "667": {"issue_url": "https://github.com/scrapy/scrapy/issues/2656", "issue_id": "#2656", "issue_summary": "sos filepipelines 302", "issue_description": "fengfangqian commented on Mar 16, 2017\nhi\nwhen i setting file_urls \"http://m.baidu.com/api?action=redirect&token=kpyysd&from=1014090y&type=app&dltype=new&refid=2650327114&tj=soft_5845028_88031597_%E8%AF%AD%E9%9F%B3%E6%90%9C%E7%B4%A2&refp=action_search&blink=da5b687474703a2f2f7265736765742e39312e636f6d2f536f66742f436f6e74726f6c6c65722e617368783f616374696f6e3d646f776e6c6f61642674706c3d312669643d34313034393931c658&crversion=1\"\nthis url redirect 3 times so when i use scrap download it the scrapy retrun 302 how can i setting it can working ? please help me!", "issue_status": "Closed", "issue_reporting_time": "2017-03-16T10:27:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "668": {"issue_url": "https://github.com/scrapy/scrapy/issues/2654", "issue_id": "#2654", "issue_summary": "Support for ptpython REPL", "issue_description": "Contributor\nGranitosaurus commented on Mar 15, 2017\nhttps://github.com/jonathanslenders/ptpython\nptpython is the new fancy repl shell (like ipython, bpython) and it would be nice to have support for it.", "issue_status": "Closed", "issue_reporting_time": "2017-03-15T07:47:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "669": {"issue_url": "https://github.com/scrapy/scrapy/issues/2652", "issue_id": "#2652", "issue_summary": "Amazon Scraping", "issue_description": "ihusny commented on Mar 14, 2017\nI want to create a web scrapping on amazon product page, not on the individual page but on the search result page.\nI tried with this but it isn't returning any results.\ndef parse(self, response):\n    for href in response.css('ul li.s-result-item celwidget::attr(href)'):\n        print href", "issue_status": "Closed", "issue_reporting_time": "2017-03-14T03:11:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "670": {"issue_url": "https://github.com/scrapy/scrapy/issues/2648", "issue_id": "#2648", "issue_summary": "twisted version 17 incompatibility?", "issue_description": "nicolaskruchten commented on Mar 13, 2017\nI just installed scrapy via conda and scrapy shell http://nicolas.kruchten.com/ returns the following stack trace:\nTraceback (most recent call last):\n  File \"/Users/nicolas/miniconda2/envs/scrapy/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/Users/nicolas/miniconda2/envs/scrapy/lib/python2.7/site-packages/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/nicolas/miniconda2/envs/scrapy/lib/python2.7/site-packages/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/nicolas/miniconda2/envs/scrapy/lib/python2.7/site-packages/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/nicolas/miniconda2/envs/scrapy/lib/python2.7/site-packages/scrapy/commands/shell.py\", line 73, in run\n    shell.start(url=url, redirect=not opts.no_redirect)\n  File \"/Users/nicolas/miniconda2/envs/scrapy/lib/python2.7/site-packages/scrapy/shell.py\", line 48, in start\n    self.fetch(url, spider, redirect=redirect)\n  File \"/Users/nicolas/miniconda2/envs/scrapy/lib/python2.7/site-packages/scrapy/shell.py\", line 115, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/Users/nicolas/miniconda2/envs/scrapy/lib/python2.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<string>\", line 2, in raiseException\nTypeError: 'float' object is not iterable\nI noticed that conda installs scrapy version 1.1 so I upgraded to scrapy 1.3 which didn't solve the issue.\nI then noticed that conda installs twisted version 17.1.0 so I downgraded to twisted 16.6.0 and that resolved the issue. I tried this after having read through the closed issue #2461\nTo summarize:\nscrapy 1.1 + twisted 17.1.0 = error\nscrapy 1.3 + twisted 17.1.0 = error\nscrapy 1.3 + twisted 16.6.0 = works\nFor reference, I'm using Python 2.7.13 on macOS and here is the output of pip freeze when I finally got things to work:\nattrs==15.2.0\nAutomat==0.5.0\ncffi==1.9.1\nconstantly==15.1.0\ncryptography==1.7.1\ncssselect==1.0.0\nenum34==1.1.6\nidna==2.2\nincremental==16.10.1\nipaddress==1.0.18\nlxml==3.7.3\nparsel==1.1.0\npyasn1==0.1.9\npyasn1-modules==0.0.8\npycparser==2.17\nPyDispatcher==2.0.5\npyOpenSSL==16.2.0\nqueuelib==1.4.2\nScrapy==1.3.0\nservice-identity==16.0.0\nsix==1.10.0\nTwisted==16.6.0\nw3lib==1.16.0\nzope.interface==4.3.3\nI would be happy to try various other permutations of versions to try to narrow things down, if that would be helpful :)", "issue_status": "Closed", "issue_reporting_time": "2017-03-13T13:25:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "671": {"issue_url": "https://github.com/scrapy/scrapy/issues/2647", "issue_id": "#2647", "issue_summary": "After adding request flags subclasses of logformatter that rely on 'flags' format string are broken", "issue_description": "Contributor\npawelmhm commented on Mar 13, 2017 \u2022\nedited\n#2082 added flags to request but it also renamed formatting string key from flags to response_flags/request_flags\nCRAWLEDMSG = u\"Crawled (%(status)s) %(request)s (referer: %(referer)s)%(flags)s\"\n +CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\" \nScrapy allows you to override logformatter and this is what I have in my project. I have logformatter looking rouhgly like this\n# dirbot/logf.py\nfrom scrapy.logformatter import LogFormatter\n\n\nclass CustomLogFormatter(LogFormatter):\n    def crawled(self, request, response, spider):\n        kwargs = super(CustomLogFormatter, self).crawled(\n            request, response, spider)\n        kwargs['msg'] = (\n            u\"Crawled (%(status)s) %(request)s \"\n            u\"(referer: %(referer)s, latency: %(latency).2f s)%(flags)s\"\n        )\n        kwargs['args']['latency'] = response.meta.get('download_latency', 0)\n        return kwargs\nnow if you enable it in settings LOG_FORMATTER = 'dirbot.logf.CustomLogFormatter' and try to run it with recent master you'll get KeyError\n2017-03-13 14:15:26 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 851, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 724, in format\n    return fmt.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 464, in format\n    record.message = record.getMessage()\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 328, in getMessage\n    msg = msg % self.args\nKeyError: u'flags'\nLogged from file engine.py, line 238\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 851, in emit\n    msg = self.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 724, in format\n    return fmt.format(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 464, in format\n    record.message = record.getMessage()\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 328, in getMessage\n    msg = msg % self.args\nKeyError: u'flags'\nLogged from file engine.py, line 238\n2017-03-13 14:15:27 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/>\nSo this change that renamed flags to response_flags/request_flags seems backward incompatible.", "issue_status": "Closed", "issue_reporting_time": "2017-03-13T13:19:18Z", "fixed_by": "#2649", "pull_request_summary": "[MRG+2] [logformatter] 'flags' format spec backward compatibility", "pull_request_description": "Contributor\npawelmhm commented on Mar 13, 2017\npass 'flags' kwarg to logger so that it is compatible with old format of CRAWLEDMSG. fixes #2647", "pull_request_status": "Merged", "issue_fixed_time": "2017-03-21T09:51:32Z", "files_changed": [["2", "scrapy/logformatter.py"], ["25", "tests/test_logformatter.py"]]}, "672": {"issue_url": "https://github.com/scrapy/scrapy/issues/2642", "issue_id": "#2642", "issue_summary": "allow to set RETRY_TIMES per-request", "issue_description": "Member\nkmike commented on Mar 11, 2017\nI think it'd be nice to allow set RETRY_TIMES per-request using a request.meta key (\"max_retry_times\"?) - some requests could be more important than others.", "issue_status": "Closed", "issue_reporting_time": "2017-03-10T20:14:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "673": {"issue_url": "https://github.com/scrapy/scrapy/issues/2638", "issue_id": "#2638", "issue_summary": "Scrapy will fill all slots with the same, waiting domain", "issue_description": "milescrawford commented on Mar 10, 2017 \u2022\nedited\nIf you have a polite DOWNLOAD_DELAY and CONCURRENT_REQUESTS_PER_DOMAIN, Scrapy will happily fill up all of its slots with requests going to a single domain, and essentially sit idle.\nA crawl will last at least as long as it takes to crawl the most numerous site, but when waiting for a particular site, Scrapy ignores possible concurrent activity which prolongs the crawl a lot.\nScrapy has all the information required to re-queue requests going to a domain which is already waiting on delays or concurrency limits. Could it be extended to do this?", "issue_status": "Closed", "issue_reporting_time": "2017-03-10T00:25:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "674": {"issue_url": "https://github.com/scrapy/scrapy/issues/2635", "issue_id": "#2635", "issue_summary": "how should image pipeline react to 3** redirects?", "issue_description": "Contributor\npawelmhm commented on Mar 9, 2017 \u2022\nedited\nCurrently when image download encounters redirect download fails. This is probably because of this line that sets handle_httpstatus_all. As a result image download handler gets to process response with status 3** and usually empty body, it is not able to download image, it fails. Is this expected behavior? Why not follow redirect in case like this?", "issue_status": "Closed", "issue_reporting_time": "2017-03-09T09:56:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "675": {"issue_url": "https://github.com/scrapy/scrapy/issues/2634", "issue_id": "#2634", "issue_summary": "Stale contrib paragraph in contribution docs", "issue_description": "Contributor\njorenham commented on Mar 9, 2017 \u2022\nedited\nBy my knowledge contrib has been depcrecated since 1.0.0.\nThere still is some contribution documentation about it.\nhttps://doc.scrapy.org/en/master/contributing.html#scrapy-contrib", "issue_status": "Closed", "issue_reporting_time": "2017-03-09T09:17:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "676": {"issue_url": "https://github.com/scrapy/scrapy/issues/2629", "issue_id": "#2629", "issue_summary": "scrapy bench needs to be more complex", "issue_description": "Member\nParth-Vader commented on Mar 7, 2017 \u2022\nedited\nThe current benchmarking is done on a very simple page, where the bot just follows all the links. This does not actually give a correct scraping speed for a standard web page.\nIt could contain several images as well, along with some restrictions so that the benchmarking suite could give a realistic approximation of the scraping speed.\nAny other ideas?\nping @dangra", "issue_status": "Closed", "issue_reporting_time": "2017-03-06T18:50:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "677": {"issue_url": "https://github.com/scrapy/scrapy/issues/2626", "issue_id": "#2626", "issue_summary": "lots of warnings `RuntimeWarning: Could not load referrer policy ''` when running tests", "issue_description": "Member\nkmike commented on Mar 6, 2017 \u2022\nedited\nCheck tox -e py36 -- tests/test_crawl.py -s output. @redapple is it a testing issue, or a problem with the warning? I've first noticed it in https://travis-ci.org/scrapy/scrapy/jobs/208185821 output.", "issue_status": "Closed", "issue_reporting_time": "2017-03-06T14:47:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "678": {"issue_url": "https://github.com/scrapy/scrapy/issues/2623", "issue_id": "#2623", "issue_summary": "Breadth-first crawl not working", "issue_description": "beeker1121 commented on Mar 5, 2017\nHi,\nI'm trying to perform a broad crawl of the web with Scrapy in breadth-first order.\nThe issue I'm running into is that after a few seconds of the crawl running, it seems to get stuck on just one or two domains instead of continuing down the list of seed URLs. I would expect it to either continue through the entire list of separate domains, and/or crawl multiple domains concurrently.\nOutput of scrapy version -v:\nScrapy    : 1.3.2\nlxml      : 3.4.0.0\nlibxml2   : 2.9.1\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 2.7.9 (default, Jun 29 2016, 13:08:31) - [GCC 4.9.2]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1t  3 May 2016)\nPlatform  : Linux-3.16.0-4-amd64-x86_64-with-debian-8.7\nHere is the main code for my spider:\n# MySpider defines our Scrapy spider.\nclass MySpider(CrawlSpider):\n    name = \"MyBot\"\n    rules = (\n        Rule(LinkExtractor(deny_domains=deny_domains), callback='parse_item', follow=True),\n    )\n\n    def start_requests(self):\n        # Open the seed.csv file.\n        seed_path = os.path.dirname(os.path.realpath(__file__)) + '/seed.csv'\n        f = open(seed_path)\n        csv_rdr = csv.reader(f)\n\n        for row in csv_rdr:\n            url = 'http://' + row[0].strip()\n            yield Request(url=url, callback=self.parse)\n\n    def parse_start_url(self, response):\n        return self.parse_item(response)\n\n    def parse_item(self, response):\n        print '!!!!!!!!!!!!! Parsing: %s !!!!!!!!!!!!!' % response.url\n\n        # Check the Content-Type.\n        if is_content_type_ok(response.headers.getlist('Content-Type')):\n            # Yield data here\n            yield {}\nHere is my settings.py file:\nBOT_NAME = 'MyBot'\n\nSPIDER_MODULES = ['crawler.spiders']\nNEWSPIDER_MODULE = 'crawler.spiders'\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\nUSER_AGENT = 'BotTest (+https://test.com)'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Broad crawl settings.\nCONCURRENT_REQUESTS = 128\nREACTOR_THREADPOOL_MAXSIZE = 30\nCONCURRENT_REQUESTS_PER_DOMAIN = 1\nCONCURRENT_REQUESTS_PER_IP = 1\nCOOKIES_ENABLED = False\nRETRY_ENABLED = False\nDOWNLOAD_TIMEOUT = 3\nLOG_LEVEL = 'INFO'\nAJAXCRAWL_ENABLED = True\nHTTPCACHE_ENABLED = False\nLOGSTATS_INTERVAL = 10\n\n# Disable auto throttling.\nAUTOTHROTTLE_ENABLED = False\n\n# Set delay.\nRANDOMIZE_DOWNLOAD_DELAY = False\nDOWNLOAD_DELAY = 2\n\n# Set max download size to 2MB\nDOWNLOAD_MAXSIZE = 2 * 1024 * 1024\nDOWNLOAD_WARNSIZE = 0\n\n# Set scheduler to BFO.\nDEPTH_PRIORITY = 1\nSCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'\nSCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'\nAnd here is example output from a crawl:\n$:~/crawler$ scrapy crawl MyBot -s JOBDIR=crawls/crawl-1 -o output.csv -t csv\n2017-03-04 22:56:19 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: MyBot)\n2017-03-04 22:56:19 [scrapy.utils.log] INFO: Overridden settings: {'FEED_URI': 'output.csv', 'AJAXCRAWL_ENABLED': True, 'COOKIES_ENABLED': False, 'FEED_FORMAT': 'csv', 'DOWNLOAD_DELAY': 2, 'LOG_LEVEL': 'INFO', 'RETRY_ENABLED': False, 'CONCURRENT_REQUESTS_PER_IP': 1, 'DOWNLOAD_TIMEOUT': 3, 'LOGSTATS_INTERVAL': 10, 'DEPTH_PRIORITY': 1, 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue', 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue', 'CONCURRENT_REQUESTS': 128, 'RANDOMIZE_DOWNLOAD_DELAY': False, 'DOWNLOAD_WARNSIZE': 0, 'SPIDER_MODULES': ['crawler.spiders'], 'BOT_NAME': 'Bot', 'NEWSPIDER_MODULE': 'crawler.spiders', 'ROBOTSTXT_OBEY': True, 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'REACTOR_THREADPOOL_MAXSIZE': 30, 'DOWNLOAD_MAXSIZE': 2097152}\n2017-03-04 22:56:19 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.spiderstate.SpiderState']\n2017-03-04 22:56:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-03-04 22:56:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-03-04 22:56:19 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-03-04 22:56:19 [scrapy.core.engine] INFO: Spider opened\n2017-03-04 22:56:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n!!!!!!!!!!!!! Parsing: http://imgur.com !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://api.imgur.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://www.sina.com.cn/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://twitter.com/imgur !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://www.msn.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://www.ask.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://www.apple.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://www.bbc.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://www.adobe.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://wordpress.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://www.tumblr.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://www.reddit.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://www.yahoo.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://www.imdb.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://www.aliexpress.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://www.youtube.com/ !!!!!!!!!!!!!\n\n... seconds later ...\n\n!!!!!!!!!!!!! Parsing: https://www.paypal.com/us/home !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://stackoverflow.blog/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://www.amazon.co.jp/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/privacy !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/tags !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://login.live.com/login.srf?wa=wsignin1.0&rpsnv=13&ct=1488668190&rver=6.4.6456.0&wp=MBI_SSL_SHARED&wreply=https:%2F%2Fmail.live.com%2Fdefault.aspx%3Frru%3Dinbox&lc=1033&id=64855&mkt=en-US&cbcxt=mai !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/upload !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/vidgif !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackexchange.com/sites !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/tour !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/memegen !!!!!!!!!!!!!\n2017-03-04 22:56:39 [scrapy.extensions.logstats] INFO: Crawled 165 pages (at 90 pages/min), scraped 19 items (at 36 items/min)\n!!!!!!!!!!!!! Parsing: http://meta.stackoverflow.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://imgur.com/signin?invokedBy=regularSignIn !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: https://imgur.com/register?invokedBy=regularSignIn !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/help !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/company/about !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/hot/viral !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackexchange.com/ !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/new/viral !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/?tab=interesting !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Funny !!!!!!!!!!!!!\n2017-03-04 22:56:49 [scrapy.extensions.logstats] INFO: Crawled 175 pages (at 60 pages/min), scraped 25 items (at 36 items/min)\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/The_More_You_Know !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/?tab=featured !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Science_and_Tech !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/?tab=hot !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Gaming !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/?tab=week !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/?tab=month !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Eat_What_You_Want !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/questions/42602342/java-unlimited-cryptography-extension-doesnt-work !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Aww !!!!!!!!!!!!!\n2017-03-04 22:56:59 [scrapy.extensions.logstats] INFO: Crawled 185 pages (at 60 pages/min), scraped 42 items (at 102 items/min)\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Inspiring !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/questions/tagged/authentication !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/questions/tagged/hdfs !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Awesome !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/questions/tagged/kerberos !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Creativity !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/questions/tagged/jce !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/The_Great_Outdoors !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://imgur.com/t/Storytime !!!!!!!!!!!!!\n!!!!!!!!!!!!! Parsing: http://stackoverflow.com/users/3593261/user3593261 !!!!!!!!!!!!!\n2017-03-04 22:57:09 [scrapy.extensions.logstats] INFO: Crawled 195 pages (at 60 pages/min), scraped 48 items (at 36 items/min)\nThanks, let me know if you need any other information!", "issue_status": "Closed", "issue_reporting_time": "2017-03-05T01:17:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "679": {"issue_url": "https://github.com/scrapy/scrapy/issues/2621", "issue_id": "#2621", "issue_summary": "Scrapy installation in windows.", "issue_description": "rpadma commented on Mar 4, 2017\nI am getting following error while installing Scrapy :\nRequirement already satisfied: scrapy in c:\\users\\rohit\\appdata\\roaming\\python\\python27\\site-packages\nRequirement already satisfied: service-identity in c:\\python27\\lib\\site-packages (from scrapy)\nRequirement already satisfied: parsel>=1.1 in c:\\users\\rohit\\appdata\\roaming\\python\\python27\\site-packages (from scrapy)\nRequirement already satisfied: six>=1.5.2 in c:\\python27\\lib\\site-packages (from scrapy)\nRequirement already satisfied: lxml in c:\\python27\\lib\\site-packages (from scrapy)\nRequirement already satisfied: Twisted>=13.1.0 in c:\\users\\rohit\\appdata\\roaming\\python\\python27\\site-packages (from scrapy)\nRequirement already satisfied: cssselect>=0.9 in c:\\python27\\lib\\site-packages (from scrapy)\nRequirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\rohit\\appdata\\roaming\\python\\python27\\site-packages (from scrapy)\nRequirement already satisfied: queuelib in c:\\python27\\lib\\site-packages (from scrapy)\nRequirement already satisfied: w3lib>=1.15.0 in c:\\python27\\lib\\site-packages (from scrapy)\nRequirement already satisfied: pyOpenSSL in c:\\python27\\lib\\site-packages (from scrapy)\nRequirement already satisfied: pyasn1 in c:\\python27\\lib\\site-packages (from service-identity->scrapy)\nRequirement already satisfied: attrs in c:\\python27\\lib\\site-packages (from service-identity->scrapy)\nRequirement already satisfied: pyasn1-modules in c:\\python27\\lib\\site-packages (from service-identity->scrapy)\nRequirement already satisfied: Automat>=0.3.0 in c:\\users\\rohit\\appdata\\roaming\\python\\python27\\site-packages (from Twisted>=13.1.0->scrapy)\nRequirement already satisfied: constantly>=15.1 in c:\\users\\rohit\\appdata\\roaming\\python\\python27\\site-packages (from Twisted>=13.1.0->scrapy)\nRequirement already satisfied: incremental>=16.10.1 in c:\\users\\rohit\\appdata\\roaming\\python\\python27\\site-packages (from Twisted>=13.1.0->scrapy)\nRequirement already satisfied: zope.interface>=3.6.0 in c:\\users\\rohit\\appdata\\roaming\\python\\python27\\site-packages (from Twisted>=13.1.0->scrapy)\nRequirement already satisfied: cryptography>=1.3.4 in c:\\python27\\lib\\site-packages (from pyOpenSSL->scrapy)\nRequirement already satisfied: setuptools in c:\\python27\\lib\\site-packages (from zope.interface>=3.6.0->Twisted>=13.1.0->scrapy)\nRequirement already satisfied: cffi>=1.4.1 in c:\\python27\\lib\\site-packages (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nRequirement already satisfied: idna>=2.0 in c:\\python27\\lib\\site-packages (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nRequirement already satisfied: ipaddress in c:\\python27\\lib\\site-packages (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nRequirement already satisfied: enum34 in c:\\python27\\lib\\site-packages (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nRequirement already satisfied: appdirs>=1.4.0 in c:\\python27\\lib\\site-packages (from setuptools->zope.interface>=3.6.0->Twisted>=13.1.0->scrapy)\nRequirement already satisfied: packaging>=16.8 in c:\\python27\\lib\\site-packages (from setuptools->zope.interface>=3.6.0->Twisted>=13.1.0->scrapy)\nRequirement already satisfied: pycparser in c:\\python27\\lib\\site-packages (from cffi>=1.4.1->cryptography>=1.3.4->pyOpenSSL->scrapy)\nRequirement already satisfied: pyparsing in c:\\python27\\lib\\site-packages (from packaging>=16.8->setuptools->zope.interface>=3.6.0->Twisted>=13.1.0->scrapy)\nI have only one python version installed (2.7) , OS : Windows 10.\nCan someone help me with this?", "issue_status": "Closed", "issue_reporting_time": "2017-03-04T04:44:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "680": {"issue_url": "https://github.com/scrapy/scrapy/issues/2610", "issue_id": "#2610", "issue_summary": "grabbing the content of a comment under an instagram post using scrapy", "issue_description": "monajalal commented on Mar 2, 2017 \u2022\nedited\n-Original stackoverflow post here http://stackoverflow.com/questions/42545020/how-to-get-the-followers-of-a-person-as-well-as-comments-under-the-photos-in-ins\nAs you see, the following json has number of followers as well as number of comments but how can I access the data within each comment as well as ID of followers so I could crawl into them?\n{\n    \"logging_page_id\": \"profilePage_20327023\", \n    \"user\": {\n        \"biography\": null, \n        \"blocked_by_viewer\": false, \n        \"connected_fb_page\": null, \n        \"country_block\": false, \n        \"external_url\": null, \n        \"external_url_linkshimmed\": null, \n        \"followed_by\": {\n            \"count\": 2585\n        }, \n        \"followed_by_viewer\": false, \n        \"follows\": {\n            \"count\": 561\n        }, \n        \"follows_viewer\": false, \n        \"full_name\": \"LeAnne Barengo\", \n        \"has_blocked_viewer\": false, \n        \"has_requested_viewer\": false, \n        \"id\": \"20327023\", \n        \"is_private\": false, \n        \"is_verified\": false, \n        \"media\": {\n            \"count\": 1904, \n            \"nodes\": [\n                {\n                    \"__typename\": \"GraphImage\", \n                    \"caption\": \"The coop was literally blown away. #wtf #sicktomystomach\", \n                    \"code\": \"BRHDfFHAUg3\", \n                    \"comments\": {\n                        \"count\": 18\n                    }, \n                    \"comments_disabled\": false, \n                    \"date\": 1488402905, \n                    \"dimensions\": {\n                        \"height\": 1080, \n                        \"width\": 1080\n                    }, \n                    \"display_src\": \"https://scontent.cdninstagram.com/t51.2885-15/e35/16908727_1139679066131441_6607786783801344000_n.jpg\", \n                    \"id\": \"1461151934034561079\", \n                    \"is_video\": false, \n                    \"likes\": {\n                        \"count\": 46\n                    }, \n                    \"owner\": {\n                        \"id\": \"20327023\"\n                    }, \n                    \"thumbnail_src\": \"https://scontent.cdninstagram.com/t51.2885-15/s640x640/sh0.08/e35/16908727_1139679066131441_6607786783801344000_n.jpg\"\nHere's the code related to this:\nimport scrapy\nprint(scrapy.__file__)\nimport json\nfrom instagram.items import UserItem\nfrom instagram.items import PostItem\nfrom scrapy.spider import BaseSpider as Spider\n\nclass InstagramSpider(Spider):\n\n    name = 'instagramspider'\n    allowed_domains = ['instagram.com']\n    start_urls = []\n\n    def __init__(self):\n        #self.start_urls = [\"https://www.instagram.com/_spataru/?__a=1\"]\n        #self.start_urls = [\"https://www.instagram.com/mona_of_green_gables/?__a=1\"]\n        self.start_urls = [\"https://www.instagram.com/ducks_love_sun/?__a=1\"]\n    def parse(self, response):\n        #get the json file\n        json_response = {}\n        try:\n            json_response = json.loads(response.body_as_unicode())\n            print json.dumps(json_response, indent=4, sort_keys=True)\n\n        except:\n            self.logger.info('%s doesnt exist', response.url)\n            pass\n        if json_response[\"user\"][\"is_private\"]:\n            return;\n        #check if the username even worked\n        try:\n            json_response = json_response[\"user\"]\n\n            item = UserItem()\n\n\n            #get User Info\n            item[\"username\"] = json_response[\"username\"]\n            item[\"follows_count\"] = json_response[\"follows\"][\"count\"]\n            item[\"followed_by_count\"] = json_response[\"followed_by\"][\"count\"]\n            item[\"is_verified\"] = json_response[\"is_verified\"]\n            item[\"biography\"] = json_response.get(\"biography\")\n            item[\"external_link\"] = json_response.get(\"external_url\")\n            item[\"full_name\"] = json_response.get(\"full_name\")\n            item[\"posts_count\"] = json_response.get(\"media\").get(\"count\")\n\n            #interate through each post\n            item[\"posts\"] = []\n\n            json_response = json_response.get(\"media\").get(\"nodes\")\n            if json_response:\n                for post in json_response:\n                    items_post = PostItem()\n                    items_post[\"code\"]=post[\"code\"]\n                    items_post[\"likes\"]=post[\"likes\"][\"count\"]\n                    items_post[\"caption\"]=post[\"caption\"]\n                    items_post[\"thumbnail\"]=post[\"thumbnail_src\"]\n                    item[\"posts\"].append(dict(items_post))\n\n            return itemimport scrapy\nprint(scrapy.__file__)\nimport json\nfrom instagram.items import UserItem\nfrom instagram.items import PostItem\nfrom scrapy.spider import BaseSpider as Spider\n\nclass InstagramSpider(Spider):\n\n    name = 'instagramspider'\n    allowed_domains = ['instagram.com']\n    start_urls = []\n\n    def __init__(self):\n        #self.start_urls = [\"https://www.instagram.com/_spataru/?__a=1\"]\n        #self.start_urls = [\"https://www.instagram.com/mona_of_green_gables/?__a=1\"]\n        self.start_urls = [\"https://www.instagram.com/ducks_love_sun/?__a=1\"]\n    def parse(self, response):\n        #get the json file\n        json_response = {}\n        try:\n            json_response = json.loads(response.body_as_unicode())\n            print json.dumps(json_response, indent=4, sort_keys=True)\n\n        except:\n            self.logger.info('%s doesnt exist', response.url)\n            pass\n        if json_response[\"user\"][\"is_private\"]:\n            return;\n        #check if the username even worked\n        try:\n            json_response = json_response[\"user\"]\n\n            item = UserItem()\n\n\n            #get User Info\n            item[\"username\"] = json_response[\"username\"]\n            item[\"follows_count\"] = json_response[\"follows\"][\"count\"]\n            item[\"followed_by_count\"] = json_response[\"followed_by\"][\"count\"]\n            item[\"is_verified\"] = json_response[\"is_verified\"]\n            item[\"biography\"] = json_response.get(\"biography\")\n            item[\"external_link\"] = json_response.get(\"external_url\")\n            item[\"full_name\"] = json_response.get(\"full_name\")\n            item[\"posts_count\"] = json_response.get(\"media\").get(\"count\")\n\n            #interate through each post\n            item[\"posts\"] = []\n\n            json_response = json_response.get(\"media\").get(\"nodes\")\n            if json_response:\n                for post in json_response:\n                    items_post = PostItem()\n                    items_post[\"code\"]=post[\"code\"]\n                    items_post[\"likes\"]=post[\"likes\"][\"count\"]\n                    items_post[\"caption\"]=post[\"caption\"]\n                    items_post[\"thumbnail\"]=post[\"thumbnail_src\"]\n                    item[\"posts\"].append(dict(items_post))\n\n            return item\n        except:\n            self.logger.info(\"Error during parsing %s\", response.url)\n        except:\n            self.logger.info(\"Error during parsing %s\", response.url)\nWhen I use something like print json.dumps(json_response[\"user\"][\"media\"][\"nodes\"][1][\"comments\"][1], indent=4, sort_keys=True)\nI get this error:\nmona@pascal:~/computer_vision/instagram/instagram$ scrapy crawl instagramspider\n2017-03-01 20:38:39-0600 [scrapy] INFO: Scrapy 0.14.4 started (bot: instagram)\n2017-03-01 20:38:39-0600 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, MemoryUsage, SpiderState\n/usr/lib/python2.7/dist-packages/scrapy/__init__.pyc\n2017-03-01 20:38:40-0600 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, RedirectMiddleware, CookiesMiddleware, HttpCompressionMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2017-03-01 20:38:40-0600 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2017-03-01 20:38:40-0600 [scrapy] DEBUG: Enabled item pipelines: \n2017-03-01 20:38:40-0600 [instagramspider] INFO: Spider opened\n2017-03-01 20:38:40-0600 [instagramspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-03-01 20:38:40-0600 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n2017-03-01 20:38:40-0600 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n2017-03-01 20:38:40-0600 [instagramspider] DEBUG: Crawled (200) <GET https://www.instagram.com/ducks_love_sun/?__a=1> (referer: None)\nmonamona\n2017-03-01 20:38:40-0600 [instagramspider] ERROR: Spider error processing <GET https://www.instagram.com/ducks_love_sun/?__a=1>\n    Traceback (most recent call last):\n      File \"/usr/local/lib/python2.7/dist-packages/Twisted-13.1.0-py2.7-linux-x86_64.egg/twisted/internet/base.py\", line 1201, in mainLoop\n        self.runUntilCurrent()\n      File \"/usr/local/lib/python2.7/dist-packages/Twisted-13.1.0-py2.7-linux-x86_64.egg/twisted/internet/base.py\", line 824, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/usr/local/lib/python2.7/dist-packages/Twisted-13.1.0-py2.7-linux-x86_64.egg/twisted/internet/defer.py\", line 380, in callback\n        self._startRunCallbacks(result)\n      File \"/usr/local/lib/python2.7/dist-packages/Twisted-13.1.0-py2.7-linux-x86_64.egg/twisted/internet/defer.py\", line 488, in _startRunCallbacks\n        self._runCallbacks()\n    --- <exception caught here> ---\n      File \"/usr/local/lib/python2.7/dist-packages/Twisted-13.1.0-py2.7-linux-x86_64.egg/twisted/internet/defer.py\", line 575, in _runCallbacks\n        current.result = callback(current.result, *args, **kw)\n      File \"/home/mona/computer_vision/instagram/instagram/instagram/spiders/spider.py\", line 31, in parse\n        self.logger.info('%s doesnt exist', response.url)\n    exceptions.AttributeError: 'InstagramSpider' object has no attribute 'logger'\n\n2017-03-01 20:38:40-0600 [instagramspider] INFO: Closing spider (finished)\n2017-03-01 20:38:40-0600 [instagramspider] INFO: Dumping spider stats:\n    {'downloader/request_bytes': 223,\n     'downloader/request_count': 1,\n     'downloader/request_method_count/GET': 1,\n     'downloader/response_bytes': 2985,\n     'downloader/response_count': 1,\n     'downloader/response_status_count/200': 1,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2017, 3, 2, 2, 38, 40, 460277),\n     'scheduler/memory_enqueued': 1,\n     'spider_exceptions/AttributeError': 1,\n     'start_time': datetime.datetime(2017, 3, 2, 2, 38, 40, 206785)}\n2017-03-01 20:38:40-0600 [instagramspider] INFO: Spider closed (finished)\n2017-03-01 20:38:40-0600 [scrapy] INFO: Dumping global stats:\n    {'memusage/max': 120844288, 'memusage/startup': 120844288}\nwhile for print json.dumps(json_response[\"user\"][\"media\"][\"nodes\"][1][\"comments\"], indent=4, sort_keys=True)\nI get\n{\n    \"count\": 19\n}", "issue_status": "Closed", "issue_reporting_time": "2017-03-02T02:44:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "681": {"issue_url": "https://github.com/scrapy/scrapy/issues/2606", "issue_id": "#2606", "issue_summary": "Scapry does not work with Twisted version 17.1.0", "issue_description": "DarrenWickham commented on Mar 1, 2017\n2017-02-28 16:00:20 [scrapy] ERROR: Error downloading\nTraceback (most recent call last):\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py\", line 45, in mustbe_deferred\nresult = f(*args, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/init.py\", line 41, in download_request\nreturn handler(request, spider)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/http11.py\", line 44, in download_request\nreturn agent.download_request(request)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/core/downloader/handlers/http11.py\", line 211, in download_request\nd = agent.request(method, url, headers, bodyproducer)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/client.py\", line 1631, in request\nparsedURI.originForm)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/client.py\", line 1408, in _requestWithEndpoint\nd = self._pool.getConnection(key, endpoint)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/client.py\", line 1294, in getConnection\nreturn self._newConnection(key, endpoint)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/client.py\", line 1306, in _newConnection\nreturn endpoint.connect(factory)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/endpoints.py\", line 1958, in connect\nself._wrapperFactory(protocolFactory)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/endpoints.py\", line 788, in connect\nEndpointReceiver, self._hostText, portNumber=self._port\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/_resolver.py\", line 174, in resolveHostName\nonAddress = self._simpleResolver.getHostByName(hostName)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/resolver.py\", line 21, in getHostByName\nd = super(CachingThreadedResolver, self).getHostByName(name, timeout)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 276, in getHostByName\ntimeoutDelay = sum(timeout)\nTypeError: 'float' object is not iterable\nDowngrading to Twisted 16.6.0 solved the issue for now", "issue_status": "Closed", "issue_reporting_time": "2017-02-28T18:42:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "682": {"issue_url": "https://github.com/scrapy/scrapy/issues/2605", "issue_id": "#2605", "issue_summary": "Unable to override settings inside spider from spider constructor using CrawlerProcess.crawl()", "issue_description": "manisoftwartist commented on Feb 28, 2017\nI am trying to programatically call a spider through a script. I an unable to override the settings through the constructor using CrawlerProcess. Let me illustrate this with the default spider for scraping quotes from the official scrapy site (last code snippet at official scrapy quotes example spider).\nclass QuotesSpider(Spider):\n\nname = \"quotes\"\n\ndef __init__(self, somestring, *args, **kwargs):\n    super(QuotesSpider, self).__init__(*args, **kwargs)\n    self.somestring = somestring\n    self.custom_settings = kwargs\n\n\ndef start_requests(self):\n    urls = [\n        'http://quotes.toscrape.com/page/1/',\n        'http://quotes.toscrape.com/page/2/',\n    ]\n    for url in urls:\n        yield Request(url=url, callback=self.parse)\n\ndef parse(self, response):\n    for quote in response.css('div.quote'):\n        yield {\n            'text': quote.css('span.text::text').extract_first(),\n            'author': quote.css('small.author::text').extract_first(),\n            'tags': quote.css('div.tags a.tag::text').extract(),\n        }\nHere is the script through which I try to run the quotes spider\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.settings import Settings\n\n    def main():\n\n    proc = CrawlerProcess(get_project_settings())\n\n    custom_settings_spider = \\\n    {\n        'FEED_URI': 'quotes.csv',\n        'LOG_FILE': 'quotes.log'\n    }\n    proc.crawl('quotes', 'dummyinput', **custom_settings_spider)\n    proc.start()", "issue_status": "Closed", "issue_reporting_time": "2017-02-28T14:53:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "683": {"issue_url": "https://github.com/scrapy/scrapy/issues/2604", "issue_id": "#2604", "issue_summary": "log path to http cache", "issue_description": "Member\nkmike commented on Feb 28, 2017\nWhat do you think about logging a full path to HTTP cache when HttpCacheMiddleware is instantiated? It may help to debug issues like #2601.", "issue_status": "Closed", "issue_reporting_time": "2017-02-28T10:36:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "684": {"issue_url": "https://github.com/scrapy/scrapy/issues/2601", "issue_id": "#2601", "issue_summary": "How to delete Cache ?", "issue_description": "cristimocean commented on Feb 28, 2017 \u2022\nedited\nSorry for asking here but I could not find help anywhere else.\nUsually to delete cache I just delete .scrapy folder in project directory.\nBut now I have a spider for which this doesn't work. Somehow I get the cached responses (I tested by turning off the internet so I am 100% sure). But I don't understand where they are coming from since my .scrapy folder is deleted.\nP.S. I NEED to delete cache , I can't just deactivate it or set it to expire because of the way I build my spiders.", "issue_status": "Closed", "issue_reporting_time": "2017-02-27T21:45:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "685": {"issue_url": "https://github.com/scrapy/scrapy/issues/2597", "issue_id": "#2597", "issue_summary": "How to call 3rd party function in parse() callback.", "issue_description": "hanjihun commented on Feb 27, 2017 \u2022\nedited by kmike\nWhat I want to implement is:\ndef parse(self, response):\n    param = {}\n    self.send_request(self, param)\n\ndef send_request(self, param):\n    url = \"www.sample.com/auto/\"\n    yield FormRequest(url, callback=self.parse_auto, formdata=param, method=\"POST\")\n\ndef parse_auto(self, response):\n    ...\nIn the above code, the self.send_request(self, param) function does not work. I am on right way?", "issue_status": "Closed", "issue_reporting_time": "2017-02-26T22:17:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "686": {"issue_url": "https://github.com/scrapy/scrapy/issues/2591", "issue_id": "#2591", "issue_summary": "Pip install missing modules", "issue_description": "kontur commented on Feb 24, 2017\nAfter a fresh venv pip install I get this (last line) of an error message, when running $ scrapy\nImportError: No module named cryptography.hazmat.bindings.openssl.binding\nThe original pip install ended with an error message. Then removing scrapy, and then reinstalling ended without error.", "issue_status": "Closed", "issue_reporting_time": "2017-02-23T20:28:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "687": {"issue_url": "https://github.com/scrapy/scrapy/issues/2589", "issue_id": "#2589", "issue_summary": "Headers.to_unicode_dict() fails with badly encoded values", "issue_description": "Contributor\nrmax commented on Feb 23, 2017\nHere is an example:\nn [21]: response.headers\n{b'Accept-Ranges': b'bytes',\n b'Cache-Control': b'max-age=604800',\n b'Content-Type': b'text/html',\n b'Date': b'Wed, 22 Feb 2017 04:03:39 GMT',\n b'Etag': b'\"801764d9a9acf1:0\"',\n b'Last-Modified': b'Mon, 06 Jan 2014 06:38:03 GMT',\n b'Public-Key-Pins': b'pin-sha256=\\x94SPKI_digest#1\"; pin-sha256=\"SPKI_digest#2\"; max-age=31536000',\n b'Server': b'Microsoft-IIS/7.5',\n b'Strict-Transport-Security': b'max-age=31536000; includeSubDomains',\n b'Vary': b'Accept-Encoding,Accept-Encoding',\n b'X-Powered-By': b'ASP.NET',\n b'X-Powered-By-Plesk': b'PleskWin'}\n\nIn [22]: response.headers.to_unicode_dict()\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-22-259fb34b3193> in <module>()\n----> 1 response.headers.to_unicode_dict()\n\n<env>/lib/python3.5/site-packages/scrapy/http/headers.py in to_unicode_dict(self)\n     87             (to_unicode(key, encoding=self.encoding),\n     88              to_unicode(b','.join(value), encoding=self.encoding))\n---> 89             for key, value in self.items())\n     90\n     91     def __copy__(self):\n\n<env>/lib/python3.5/site-packages/scrapy/utils/datatypes.py in __init__(self, seq)\n    191         super(CaselessDict, self).__init__()\n    192         if seq:\n--> 193             self.update(seq)\n    194\n    195     def __getitem__(self, key):\n\n<env>/lib/python3.5/site-packages/scrapy/utils/datatypes.py in update(self, seq)\n    227         seq = seq.items() if isinstance(seq, dict) else seq\n    228         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n--> 229         super(CaselessDict, self).update(iseq)\n    230\n    231     @classmethod\n\n<env>/lib/python3.5/site-packages/scrapy/utils/datatypes.py in <genexpr>(.0)\n    226     def update(self, seq):\n    227         seq = seq.items() if isinstance(seq, dict) else seq\n--> 228         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n    229         super(CaselessDict, self).update(iseq)\n    230\n\n<env>/lib/python3.5/site-packages/scrapy/http/headers.py in <genexpr>(.0)\n     87             (to_unicode(key, encoding=self.encoding),\n     88              to_unicode(b','.join(value), encoding=self.encoding))\n---> 89             for key, value in self.items())\n     90\n     91     def __copy__(self):\n\n<env>/lib/python3.5/site-packages/scrapy/utils/python.py in to_unicode(text, encoding, errors)\n    105     if encoding is None:\n    106         encoding = 'utf-8'\n--> 107     return text.decode(encoding, errors)\n    108\n    109\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x94 in position 11: invalid start byte", "issue_status": "Closed", "issue_reporting_time": "2017-02-23T06:18:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "688": {"issue_url": "https://github.com/scrapy/scrapy/issues/2586", "issue_id": "#2586", "issue_summary": "DataLoss error when content-length doesn't match actual body size", "issue_description": "Contributor\nrmax commented on Feb 23, 2017\nThe example website below returns a Content-Length header larger than the actual response size. The webserver itself is broken (or misconfigured) because it returns the Content-Length from the uncompressed body while it sends the compressed body.\n$ scrapy shell \"http://www.ebk-gruppe.com/\"  --set RETRY_ENABLED=0\n2017-02-23 00:26:18 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: scrapybot)\n2017-02-23 00:26:18 [scrapy.utils.log] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'RETRY_ENABLED': '0', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2017-02-23 00:26:18 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.memusage.MemoryUsage']\n2017-02-23 00:26:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-02-23 00:26:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-02-23 00:26:19 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-02-23 00:26:19 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6025\n2017-02-23 00:26:19 [scrapy.core.engine] INFO: Spider opened\nTraceback (most recent call last):\n  File \"<snip>/bin/scrapy\", line 11, in <module>\n    load_entry_point('Scrapy', 'console_scripts', 'scrapy')()\n  File \"<snip>/scrapy/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"<snip>/scrapy/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"<snip>/scrapy/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"<snip>/scrapy/scrapy/commands/shell.py\", line 73, in run\n    shell.start(url=url, redirect=not opts.no_redirect)\n  File \"<snip>/scrapy/scrapy/shell.py\", line 48, in start\n    self.fetch(url, spider, redirect=redirect)\n  File \"<snip>/scrapy/scrapy/shell.py\", line 115, in fetch\n    reactor, self._schedule, request, spider)\n  File \"<snip>/twisted/src/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<snip>/twisted/src/twisted/python/failure.py\", line 372, in raiseException\n    raise self.value.with_traceback(self.tb)\ntwisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]", "issue_status": "Closed", "issue_reporting_time": "2017-02-23T04:29:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "689": {"issue_url": "https://github.com/scrapy/scrapy/issues/2584", "issue_id": "#2584", "issue_summary": "Compare two scaped items for similarity", "issue_description": "fbuchinger commented on Feb 23, 2017\nHi, I'm using scrapy to scrape a few historical HTML documents, which are ordered by date. later, I want to order the documents by their publication date and them in pairs of 2 for similarity. Is there an elegant way to do this in scrapy?\nCurrently I'm trying to do this in my item pipeline (open the exported csv file in the close_spider method, parse it again, sort it, do the comparison). This feels rather clumsy. Is there a better way?", "issue_status": "Closed", "issue_reporting_time": "2017-02-22T22:05:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "690": {"issue_url": "https://github.com/scrapy/scrapy/issues/2582", "issue_id": "#2582", "issue_summary": "response.follow_all or SelectorList.follow_all shortcut", "issue_description": "Member\nkmike commented on Feb 22, 2017 \u2022\nedited\nWhat do you think about adding response.follow_all shortcut, which returns a list of requests? This is inspired by this note in docs:\nresponse.follow(response.css('li.next a')) is not valid because response.css returns a list-like object with selectors for all results, not a single selector. A for loop like in the example above, or response.follow(response.css('li.next a')[0]) is fine.\nSo instead of\nfor href in response.css('li.next a::attr(href)'):\n    yield response.follow(href, callback=self.parse)\nusers would be able to write (in Python 3)\nyield from response.follow_all(response.css('li.next a::attr(href)'), self.parse)\nWe can also add 'css' and 'xpath' support to it, as keyword arguments; it would shorten the code to this:\nyield from response.follow_all(css='li.next a::attr(href)', callback=self.parse)\n(this is a follow-up to #1940 and #2540)\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2017-02-22T14:03:32Z", "fixed_by": "#4057", "pull_request_summary": "Response.follow_all", "pull_request_description": "Member\nelacuesta commented on Oct 2, 2019\nFixes #2582", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-22T21:15:25Z", "files_changed": [["34", "docs/intro/tutorial.rst"], ["4", "docs/topics/request-response.rst"], ["70", "scrapy/http/response/__init__.py"], ["106", "scrapy/http/response/text.py"], ["0", "...ta/link_extractor/sgml_linkextractor.html", "\u2192", "...le_data/link_extractor/linkextractor.html"], ["25", "tests/sample_data/link_extractor/linkextractor_no_href.html"], ["130", "tests/test_http_response.py"], ["2", "tests/test_linkextractors.py"]]}, "691": {"issue_url": "https://github.com/scrapy/scrapy/issues/2579", "issue_id": "#2579", "issue_summary": "Why the downloaded file numbers is not equal numbers of url's line in my log file?", "issue_description": "hwypengsir commented on Feb 21, 2017\nPlatform: debian8 + python3.6 + scrapy 1.3.2.\nHere is a simple scrapy script to download all the us stock quote.\nPlease to download the 7z file on webpage.\nhttps://drive.google.com/open?id=0B9BpilWzmmMCRGQ0RF8xWWx3ZVU\nTo extract it with 7z.\n7z x urls.7z -o/home\nThe sample data /home/urls.csv can be tested.\nTo save the below scrapy script as /home/quote.py\nimport scrapy\nimport csv\n\nCONCURRENT_REQUESTS = 3\nCONCURRENT_REQUESTS_PER_SPIDER = 3\nCLOSESPIDER_PAGECOUNT = 100000\nCLOSESPIDER_TIMEOUT = 36000\nDOWNLOAD_DELAY = 10\nRETRY_ENABLED = False\nCOOKIES_ENABLED = False\nRETRY_ENABLED = True\nRETRY_TIMES = 1\nCOOKIES_ENABLED = False\n\ndownloaded = open('/home/downloaded.csv','w')\n\nclass TestSpider(scrapy.Spider):\n    def __init__(self, *args, **kw):\n        self.timeout = 10\n\n    name = \"quote\"\n    allowed_domains = [\"chart.yahoo.com\"]\n    csvfile = open('/home/urls.csv')\n    reader = csv.reader(csvfile)\n    rows = [row[0] for row in reader]\n    start_urls = rows\n\n    def parse(self, response):\n        content = response.body\n        target = response.url\n        filename = target.split(\"=\")[1]\n        open('/home/data/'+filename+'.csv', 'wb').write(content)\n        downloaded.write(target+\"\\n\")\nThe last two lines in /home/quote.py is important,\nopen('/home/data/'+filename+'.csv', 'wb').write(content) to open a file and save the data into the file.\nThe following ,downloaded.write(target+\"\\n\"),it is to write a log to describe which url was downloaded instantly.\nTo execute the spider with:\nscrapy runspider  /home/quote.py\nIn my opinion the numbers--all downloaded files is equal to line numbers of url in /home/downloaded.csv.\nls  /home/data |wc -l\n6012\nwc /home/downloaded.csv\n6124\nWhy two numbers here aren't equal?\nPlease to test on your plarform and tell me the two numbers.", "issue_status": "Closed", "issue_reporting_time": "2017-02-21T02:25:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "692": {"issue_url": "https://github.com/scrapy/scrapy/issues/2576", "issue_id": "#2576", "issue_summary": "MEMUSAGE_REPORT doesn't seem to do anything", "issue_description": "Member\nkmike commented on Feb 20, 2017\nMoved from #2187 (comment)", "issue_status": "Closed", "issue_reporting_time": "2017-02-20T14:20:03Z", "fixed_by": "#2741", "pull_request_summary": "[FIX #2576] cleanup: removed unused MEMUSAGE_REPORT", "pull_request_description": "Contributor\neliat123 commented on May 16, 2017\nfix #2576\nSigned-off-by: Eli Atzaba eliat123@gmail.com", "pull_request_status": "Merged", "issue_fixed_time": "2017-05-17T08:48:00Z", "files_changed": [["1", "docs/topics/extensions.rst"], ["13", "docs/topics/settings.rst"], ["1", "scrapy/extensions/memusage.py"], ["1", "scrapy/settings/default_settings.py"]]}, "693": {"issue_url": "https://github.com/scrapy/scrapy/issues/2575", "issue_id": "#2575", "issue_summary": "document brotlipy requirement somewhere", "issue_description": "Member\nkmike commented on Feb 20, 2017\nA followup to #2535 - this feature is not mentioned in docs.", "issue_status": "Closed", "issue_reporting_time": "2017-02-20T14:07:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "694": {"issue_url": "https://github.com/scrapy/scrapy/issues/2574", "issue_id": "#2574", "issue_summary": "ftp_user and ftp_password meta keys are not documented", "issue_description": "Member\nkmike commented on Feb 20, 2017\nftp_user and ftp_password meta keys are not documented in https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-meta; it'd be nice to add them. See also: #2343\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-02-20T14:01:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "695": {"issue_url": "https://github.com/scrapy/scrapy/issues/2573", "issue_id": "#2573", "issue_summary": "Sharing of website-specific configuration files", "issue_description": "Member\nParth-Vader commented on Feb 18, 2017\nA new section in the official documentation could be added where people could share their configuration files that they used to successfully scrape data from a specific website (by successful, I mean not getting banned and getting a good speed.)\nThis way, it would be easier for people without any prior knowledge of HTML, Python or Shell, could easily use scrapy to get data from those specific sites.", "issue_status": "Closed", "issue_reporting_time": "2017-02-18T11:12:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "696": {"issue_url": "https://github.com/scrapy/scrapy/issues/2571", "issue_id": "#2571", "issue_summary": "Chunked", "issue_description": "sivanbil commented on Feb 17, 2017 \u2022\nedited\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2017-02-17T02:44:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "697": {"issue_url": "https://github.com/scrapy/scrapy/issues/2568", "issue_id": "#2568", "issue_summary": "finish relocations", "issue_description": "Member\nkmike commented on Feb 16, 2017\nWhat do you think about removing shims for moved/renamed modules from #1063? Scrapy 1.0 release was 20 months ago, and upgrade path for users is clear.", "issue_status": "Closed", "issue_reporting_time": "2017-02-15T23:41:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "698": {"issue_url": "https://github.com/scrapy/scrapy/issues/2563", "issue_id": "#2563", "issue_summary": "How can I install scrapy on python3.6?", "issue_description": "driftluo commented on Feb 14, 2017 \u2022\nedited\nI use the newest Python on Centos 7, and a dedicated virtualenv\n(ENV) [luoc@study ~ ]$ lsb_release -a\nLSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch\nDistributor ID: CentOS\nDescription: CentOS Linux release 7.3.1611 (Core) \nRelease: 7.3.1611\nCodename: Core\n\n(ENV) [luoc@study ~ ]$ python --version\nPython 3.6.0\nWhen I install scrapy, the error\n(ENV) [luoc@study ~ ]$ pip install scrapy\nCollecting scrapy\n  Using cached Scrapy-1.3.2-py2.py3-none-any.whl\nCollecting cssselect>=0.9 (from scrapy)\n  Using cached cssselect-1.0.1-py2.py3-none-any.whl\nRequirement already satisfied: six>=1.5.2 in ./ENV/lib/python3.6/site-packages (from scrapy)\nCollecting Twisted>=13.1.0 (from scrapy)\n  Could not find a version that satisfies the requirement Twisted>=13.1.0 (from scrapy) (from versions: )\nNo matching distribution found for Twisted>=13.1.0 (from scrapy)\n\n(ENV) [luoc@study ~ ]$ pip install Twisted\nCollecting Twisted\n  Could not find a version that satisfies the requirement Twisted (from versions: )\nNo matching distribution found for Twisted\nthe Twisted can't find a version, so how can I do it\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-02-14T13:51:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "699": {"issue_url": "https://github.com/scrapy/scrapy/issues/2560", "issue_id": "#2560", "issue_summary": "BrowserLikeContextFactory not available in some conditions", "issue_description": "unode commented on Feb 14, 2017\nWhile tracing the error that lead to #2555 I tried the workaround mentioned in the documentation without success.\nThis code lives incontextfactory.py but was not reachable as the import was failing on my system due to #2555.\nThis file is a large try/except block with many potential points of failure and it's likely to trip other users in the future.\nThat said, could this be refactored to provide a fallback for BrowserLikeContextFactory or otherwise reduce the scope of the try/except to avoid breaking the API?\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-02-13T18:35:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "700": {"issue_url": "https://github.com/scrapy/scrapy/issues/2555", "issue_id": "#2555", "issue_summary": "Scrapy incompatible with twisted 17.1.0 on TLS/SSL enabled websites.", "issue_description": "unode commented on Feb 13, 2017\nReferencing #2479\nRunning pip install scrapy on a new virtualenv got me Twisted 17.1.0.\nQuite a lot of HTTPS websites started to fail (e.g. google) with a twisted error SSL23_GET_SERVER_HELLO - tlsv1 alert internal error.\nThe main cause of the SSL23 error is that code introduced after #1794 on core/downloader/tls.py imports _maybeSetHostNameIndication which has been removed from _sslverify.py.\nThis code is also wrapped on a large try: ... except ImportError: pass which silences the actual cause of the problem.\nManually downgrading twisted to 16.0.0 solved the problem.\nAlso to avoid these incompatibilities shouldn't scrapy on pypi be limited to known working versions of dependencies?\nScrapy    : 1.3.1\nlxml      : 3.7.2.0\nlibxml2   : 2.9.3\ncssselect : 1.0.1\nparsel    : 1.1.0\nw3lib     : 1.17.0\nTwisted   : 17.1.0\nPython    : 3.4.5 (default, Jan 24 2017, 17:55:08) - [GCC 4.9.3]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2j  26 Sep 2016)\nPlatform  : Linux-4.6.0-sabayon-x86_64-Intel-R-_Core-TM-_i7-4710MQ_CPU_@_2.50GHz-with-gentoo-2.2", "issue_status": "Closed", "issue_reporting_time": "2017-02-13T09:58:19Z", "fixed_by": "#2558", "pull_request_summary": "Fixed compatibility with twisted 17+", "pull_request_description": "Member\nkmike commented on Feb 13, 2017 \u2022\nedited\nI've replaced try/except with explicit version checks and ported code to be compatible with Twisted 17+. This should fix #2555.\nFor the record, issue was caused by https://twistedmatrix.com/trac/ticket/8916.\nAlso, we no longer support Ubuntu 12.04, so its tox environment is replaced with Ubuntu 14.04 environment (we still don't run it on Travis though).", "pull_request_status": "Merged", "issue_fixed_time": "2017-02-13T17:48:13Z", "files_changed": [["28", "scrapy/core/downloader/tls.py"], ["12", "tox.ini"]]}, "701": {"issue_url": "https://github.com/scrapy/scrapy/issues/2554", "issue_id": "#2554", "issue_summary": "Authenticated URL results in DNS lookup failure", "issue_description": "p8a commented on Feb 11, 2017 \u2022\nedited\nCrawling of authenticated urls ( http://:@domain.com) fails with DNS lookup failure:\n2017-02-10 14:48:37 [scrapy] INFO: Spider opened\n2017-02-10 14:48:37 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-02-10 14:48:37 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-02-10 14:48:37 [scrapy] DEBUG: Retrying <GET http://user:password@foobar.com/context/latest.txt> (failed 1 times): DNS lookup failed: address 'user:password@foobar.com' not found: [Errno 8] nodename nor servname provided, or not known.\n2017-02-10 14:48:37 [scrapy] DEBUG: Retrying <GET http://user:password@foobar.com/context/latest.txt> (failed 2 times): DNS lookup failed: address 'user:password@foobar.com' not found: [Errno 8] nodename nor servname provided, or not known.\n2017-02-10 14:48:37 [scrapy] DEBUG: Gave up retrying <GET http://user:password@foobar.com/context/latest.txt> (failed 3 times): DNS lookup failed: address 'user:password@foobar.com' not found: [Errno 8] nodename nor servname provided, or not known.\n2017-02-10 14:48:37 [scrapy] ERROR: Error downloading <GET http://user:password@foobar.com/context/latest.txt>: DNS lookup failed: address 'user:password@foobar.com' not found: [Errno 8] nodename nor servname provided, or not known.\n2017-02-10 14:48:37 [scrapy] INFO: Closing spider (finished)\nI'm using scrapy 1.1.2\nIs there a different way/option to make this kind of URLs work ?\nThanks", "issue_status": "Closed", "issue_reporting_time": "2017-02-10T19:56:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "702": {"issue_url": "https://github.com/scrapy/scrapy/issues/2553", "issue_id": "#2553", "issue_summary": "CsvItemExporter fails on py3", "issue_description": "Member\nelacuesta commented on Feb 10, 2017\nfrom scrapy.exporters import CsvItemExporter\nwith open('temp.csv', 'w') as f:\n    exporter = CsvItemExporter(f)\n    exporter.start_exporting()\n    exporter.export_item({'a': 'b'})\n    exporter.finish_exporting()\nThe previous snippet works fine on python 2, however the following error appears when running on python 3:\nTraceback (most recent call last):\n  File \"exporter.py\", line 5, in <module>\n    exporter.export_item({'a': 'b'})\n  File \"/Users/eugenio/.../venv/lib/python3.5/site-packages/scrapy/exporters.py\", line 206, in export_item\n    self._write_headers_and_set_fields_to_export(item)\n  File \"/Users/eugenio/.../venv/lib/python3.5/site-packages/scrapy/exporters.py\", line 230, in _write_headers_and_set_fields_to_export\n    self.csv_writer.writerow(row)\nTypeError: write() argument must be str, not bytes\nAm I missing something? Thanks!", "issue_status": "Closed", "issue_reporting_time": "2017-02-10T14:27:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "703": {"issue_url": "https://github.com/scrapy/scrapy/issues/2552", "issue_id": "#2552", "issue_summary": "scrapy.Request no init error on invalid url", "issue_description": "Contributor\npawelmhm commented on Feb 9, 2017 \u2022\nedited\nI stumbled on some weird issue, spider got some invalid url, but instead of crashing loudly when trying to create scrapy.Request() with invalid url it just silently ignored this error. Sample to reproduce\nfrom scrapy.spiders import Spider\nfrom scrapy import Request\n\n\nclass DmozSpider(Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n    ]\n\n    def parse(self, response):\n        invalid_url = \"/container.productlist.productslist.productthumbnail.articledetaillink.layerlink:open-layer/0/CLASSIC/-1/WEB$007cARBO$007c13263065/null$007cDisplay$0020Product$002f111499$002fAil$0020blanc$007c?t:ac=13263065\"\n        yield Request(invalid_url)\nthis generates following output:\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Spider opened\n2017-02-09 12:21:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-02-09 12:21:04 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n2017-02-09 12:21:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2017-02-09 12:21:04 [scrapy.core.engine] INFO: Closing spider (finished)\nthere is no information about trying to generate this Request with invalid_url, no stacktrace, no error info from middleware. Why?", "issue_status": "Closed", "issue_reporting_time": "2017-02-09T11:24:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "704": {"issue_url": "https://github.com/scrapy/scrapy/issues/2550", "issue_id": "#2550", "issue_summary": "A small flaw about the code of example 1.3.0", "issue_description": "Youwei-Xiao commented on Feb 9, 2017\n\nIn this example, I found out that I cannot crawl author name. After I checked, I figured out that the format of HTML of author in http://quotes.toscrape.com/tag/humor/ is different. We can see:\nSo, to solve is, we need to code like this:\n\nThanks a lot. That's not a bug or problem but a small flaw. : )", "issue_status": "Closed", "issue_reporting_time": "2017-02-09T05:37:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "705": {"issue_url": "https://github.com/scrapy/scrapy/issues/2549", "issue_id": "#2549", "issue_summary": "Many log messages from ImagesPipeline", "issue_description": "podolskyi commented on Feb 9, 2017 \u2022\nedited\nI'm using ImagesPipeline and store images to S3 and see many log messages.\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (1): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (2): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (3): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (4): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (5): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (6): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (7): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (8): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (9): ***.s3.amazonaws.com\n2017-02-09 02:09:41 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTPS connection (10): ***.s3.amazonaws.com\nScrapy log level INFO. any way to prevent to generate logs to console?", "issue_status": "Closed", "issue_reporting_time": "2017-02-09T00:16:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "706": {"issue_url": "https://github.com/scrapy/scrapy/issues/2545", "issue_id": "#2545", "issue_summary": "enable test_proxy_connect in Python 3", "issue_description": "Member\nkmike commented on Feb 8, 2017\nmitmproxy is ported to Python 3 (in fact, it is now Python3-only), so it'd be nice to enable test_proxy_connect.py in Python 3.", "issue_status": "Closed", "issue_reporting_time": "2017-02-08T07:38:11Z", "fixed_by": "#4114", "pull_request_summary": "Remove py2 tests", "pull_request_description": "Contributor\nwRAR commented on Oct 31, 2019 \u2022\nedited by Gallaecio\nThis removes Python2-only tests and updates test_proxy_connect.py to new mitmproxy.\nFixes #2545.", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-21T13:22:10Z", "files_changed": [["10", "conftest.py"], ["1", "pytest.ini"], ["3", "tests/py3-ignores.txt", "\u2192", "tests/ignores.txt"], ["2", "tests/requirements-py3.txt"], ["23", "tests/test_downloader_handlers.py"], ["233", "tests/test_linkextractors_deprecated.py"], ["97", "tests/test_proxy_connect.py"], ["29", "tests/test_utils_python.py"], ["20", "tests/test_webclient.py"]]}, "707": {"issue_url": "https://github.com/scrapy/scrapy/issues/2541", "issue_id": "#2541", "issue_summary": "scrapy xpath parses xml wrong", "issue_description": "light4 commented on Feb 7, 2017 \u2022\nedited\nSoftware version\nScrapy 1.3.0, Python 2.7.13, lxml==3.7.2\nSummary\nwhen I use scrapy shell test this page, I found that use response.xpath('//description/*') can not get the right content.\nxml source at gist\nTest steps\nscrapy shell -s USER_AGENT='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.41 Safari/537.36' http://www.finishline.com/store/browse/gadgets/productLookupXML.jsp\\?productId\\=prod1120206\nuse scrapy xpath\nIn [0]: description = ''.join(response.xpath('//description/*').extract())\n\nIn [1]: print(description)\n<p>Meet the Women's Nike Air Max Thea Mid Running Shoes. She is lighter than ever, durable as ever, and as comfortable as ever--and now, she comes in a mid-top silhouette. She is everything you could want in a running shoe and now she's better than ever with the addition of a seamless, molded leather upper. </p><p> In addition to the molded leather upper, the lelastic at the ankle provides a secure fit that is easy to put on and take off. The midsole features injected Phylon for a great cushiony feel and a visible Air-Sole unit to absorb shock, providing a forgiving, easeful feel in every foot strike. She'll go easy on you, but don't feel like you have to reciprocate. </p><p>FEATURES:</p><ul><li>UPPER: Molded leather </li> <li>MIDSOLE: Injected Phylon with Air-Sole unit <li>  <li>OUTSOLE: Rubber </li> IMPORTED</li></li>\n        <salesText/>\n        <bogo>false</bogo>\n        <otherdetails/>\n        <colors><color selected=\"true\" colorId=\"859550-600\" thumbnail=\"http://images.finishline.com/is/image/FinishLine/859550_600?$Thumbnail$\">Night Maroon/Sail</color>\n                                <color selected=\"false\" colorId=\"859550-400\" thumbnail=\"http://images.finishline.com/is/image/FinishLine/859550_400?$Thumbnail$\">Obsidian/Sail/Bright Grape</color>\n                                <color selected=\"false\" colorId=\"859550-200\" thumbnail=\"http://images.finishline.com/is/image/FinishLine/859550_200?$Thumbnail$\">Ale Brown/Sail/Velvet Brown</color>\n                                <color selected=\"false\" colorId=\"859550-001\" thumbnail=\"http://images.finishline.com/is/image/FinishLine/859550_001?$Thumbnail$\">Black/Sail/Reflect Silver</color></colors>\n        <sizes><size sku=\"2241540\" colorId=\"859550-600\" available=\"false\">5.5</size>\n                                <size sku=\"2241541\" colorId=\"859550-600\" available=\"true\">6.0</size>\n                                <size sku=\"2241542\" colorId=\"859550-600\" available=\"true\">6.5</size>\n                                <size sku=\"2241543\" colorId=\"859550-600\" available=\"true\">7.0</size>\n                                <size sku=\"2241544\" colorId=\"859550-600\" available=\"true\">7.5</size>\n                                <size sku=\"2241545\" colorId=\"859550-600\" available=\"true\">8.0</size>\n                                <size sku=\"2241546\" colorId=\"859550-600\" available=\"true\">8.5</size>\n                                <size sku=\"2241547\" colorId=\"859550-600\" available=\"true\">9.0</size>\n                                <size sku=\"2241548\" colorId=\"859550-600\" available=\"true\">9.5</size>\n                                <size sku=\"2241549\" colorId=\"859550-600\" available=\"true\">10.0</size>\n                                <size sku=\"2241550\" colorId=\"859550-600\" available=\"false\">11.0</size>\n                                <size sku=\"2241551\" colorId=\"859550-400\" available=\"false\">5.5</size>\n                                <size sku=\"2241552\" colorId=\"859550-400\" available=\"true\">6.0</size>\n                                <size sku=\"2241553\" colorId=\"859550-400\" available=\"true\">6.5</size>\n                                <size sku=\"2241554\" colorId=\"859550-400\" available=\"true\">7.0</size>\n                                <size sku=\"2241555\" colorId=\"859550-400\" available=\"true\">7.5</size>\n                                <size sku=\"2241556\" colorId=\"859550-400\" available=\"true\">8.0</size>\n                                <size sku=\"2241557\" colorId=\"859550-400\" available=\"true\">8.5</size>\n                                <size sku=\"2241558\" colorId=\"859550-400\" available=\"true\">9.0</size>\n                                <size sku=\"2241559\" colorId=\"859550-400\" available=\"true\">9.5</size>\n                                <size sku=\"2241560\" colorId=\"859550-400\" available=\"true\">10.0</size>\n                                <size sku=\"2241561\" colorId=\"859550-400\" available=\"false\">11.0</size>\n                                <size sku=\"2242952\" colorId=\"859550-200\" available=\"false\">5.5</size>\n                                <size sku=\"2242953\" colorId=\"859550-200\" available=\"true\">6.0</size>\n                                <size sku=\"2242954\" colorId=\"859550-200\" available=\"true\">6.5</size>\n                                <size sku=\"2242955\" colorId=\"859550-200\" available=\"true\">7.0</size>\n                                <size sku=\"2242956\" colorId=\"859550-200\" available=\"true\">7.5</size>\n                                <size sku=\"2242957\" colorId=\"859550-200\" available=\"true\">8.0</size>\n                                <size sku=\"2242958\" colorId=\"859550-200\" available=\"true\">8.5</size>\n                                <size sku=\"2242959\" colorId=\"859550-200\" available=\"true\">9.0</size>\n                                <size sku=\"2242960\" colorId=\"859550-200\" available=\"true\">9.5</size>\n                                <size sku=\"2242961\" colorId=\"859550-200\" available=\"true\">10.0</size>\n                                <size sku=\"2242962\" colorId=\"859550-200\" available=\"false\">11.0</size>\n                                <size sku=\"2242963\" colorId=\"859550-001\" available=\"false\">5.5</size>\n                                <size sku=\"2242964\" colorId=\"859550-001\" available=\"true\">6.0</size>\n                                <size sku=\"2242965\" colorId=\"859550-001\" available=\"true\">6.5</size>\n                                <size sku=\"2242966\" colorId=\"859550-001\" available=\"true\">7.0</size>\n                                <size sku=\"2242967\" colorId=\"859550-001\" available=\"true\">7.5</size>\n                                <size sku=\"2242968\" colorId=\"859550-001\" available=\"true\">8.0</size>\n                                <size sku=\"2242969\" colorId=\"859550-001\" available=\"true\">8.5</size>\n                                <size sku=\"2242970\" colorId=\"859550-001\" available=\"true\">9.0</size>\n                                <size sku=\"2242971\" colorId=\"859550-001\" available=\"false\">9.5</size>\n                                <size sku=\"2242972\" colorId=\"859550-001\" available=\"true\">10.0</size>\n                                <size sku=\"2242973\" colorId=\"859550-001\" available=\"false\">11.0</size></sizes>\n       <alternateviews><alternateviews colorId=\"859550-600\">http://images.finishline.com/is/image/FinishLine/859550_600_P1?$Thumbnail$</alternateviews>\n\n\n      <alternateviews colorId=\"859550-600\">http://images.finishline.com/is/image/FinishLine/859550_600_P2?$Thumbnail_3quarter$</alternateviews>\n\n\n      <alternateviews colorId=\"859550-600\">http://images.finishline.com/is/image/FinishLine/859550_600_P3?$Thumbnail_fb$</alternateviews>\n\n\n      <alternateviews colorId=\"859550-600\">http://images.finishline.com/is/image/FinishLine/859550_600_P4?$Thumbnail$</alternateviews>\n\n\n      <alternateviews colorId=\"859550-600\">http://images.finishline.com/is/image/FinishLine/859550_600_P5?$Thumbnail_fb$</alternateviews>\n\n\n      <alternateviews colorId=\"859550-600\">http://images.finishline.com/is/image/FinishLine/859550_600_P6?$Thumbnail$</alternateviews>\n\n\n      <alternateviews colorId=\"859550-600\">http://images.finishline.com/is/image/FinishLine/859550_600_P7\n?$Thumbnail$</alternateviews></alternateviews>\n        <startDate/>\n        <isShoe>true</isShoe>\n        <ratingImage/>\n        <totalReview/>\n        <greyOutAddToCartButtons><greyOutAddToCartButton colorId=\"859550-600\">false</greyOutAddToCartButton>\n                       <greyOutAddToCartButton colorId=\"859550-400\">false</greyOutAddToCartButton>\n                       <greyOutAddToCartButton colorId=\"859550-200\">false</greyOutAddToCartButton>\n                       <greyOutAddToCartButton colorId=\"859550-001\">false</greyOutAddToCartButton></greyOutAddToCartButtons>\n    </ul>\nuse lxml xpath\nIn [4]: from lxml.html import tostring, fromstring\n\nIn [5]: test = fromstring(response.body)\n\nIn [10]: description2 = '\\n'.join([tostring(i) for i in test.xpath('//description/*')])\n\nIn [11]: print(description2)\n<p>Meet the Women's Nike Air Max Thea Mid Running Shoes. She is lighter than ever, durable as ever, and as comfortable as ever--and now, she comes in a mid-top silhouette. She is everything you could want in a running shoe and now she's better than ever with the addition of a seamless, molded leather upper. </p>\n<p> In addition to the molded leather upper, the lelastic at the ankle provides a secure fit that is easy to put on and take off. The midsole features injected Phylon for a great cushiony feel and a visible Air-Sole unit to absorb shock, providing a forgiving, easeful feel in every foot strike. She'll go easy on you, but don't feel like you have to reciprocate. </p>\n<p>FEATURES:</p>\n<ul><li>UPPER: Molded leather </li> <li>MIDSOLE: Injected Phylon with Air-Sole unit </li><li>  </li><li>OUTSOLE: Rubber </li> IMPORTED</ul>\n\nIn [12]: test.xpath('//description/*')\nOut[12]:\n[<Element p at 0x111aafdb8>,\n <Element p at 0x111bcb100>,\n <Element p at 0x111bcb368>,\n <Element ul at 0x111bcb470>]\nUpdate\nuse the same text from gist, TextResponse parses ok, XmlResponse parses wrong.\nfrom scrapy.http import TextResponse, XmlResponse\n\nurl = 'http://test.com'\nbody = b''  # text from gist\nresponse = TextResponse(url=url, body=body, encoding='utf-8')\nresponsex = XmlResponse(url=url, body=body, encoding='utf-8')\nresponse.xpath('//description/*').extract()\nresponsex.xpath('//description/*').extract()\nseems like it's a bug of scrapy/parsel.", "issue_status": "Closed", "issue_reporting_time": "2017-02-07T07:55:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "708": {"issue_url": "https://github.com/scrapy/scrapy/issues/2527", "issue_id": "#2527", "issue_summary": "POST method not preserved while performing a 302 redirect", "issue_description": "Contributor\nstarrify commented on Feb 2, 2017 \u2022\nedited\nBelow are sample results from scrapy shell:\n>>> import scrapy\n>>> scrapy.__version__\nu'1.3.0'\n>>> fetch(scrapy.Request('http://httpbin.org/status/302', method='POST'))\n2017-02-02 22:03:31 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://httpbin.org/redirect/1> from <POST http://httpbin.org/status/302>\n2017-02-02 22:03:31 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://httpbin.org/get> from <GET http://httpbin.org/redirect/1>\n2017-02-02 22:03:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/get> (referer: None)\nfrom which you could see that Scrapy sent a new GET request after having received 302 from a POST request.\nRFC 2616 says:\nNote: RFC 1945 and RFC 2068 specify that the client is not allowed to change the method on the redirected request. However, most existing user agent implementations treat 302 as if it were a 303 response, performing a GET on the Location field-value regardless of the original request method. The status codes 303 and 307 have been added for servers that wish to make unambiguously clear which kind of reaction is expected of the client.\nThus I guess we may need to have the redirect middleware updated if we are to stick strictly to the RFC.", "issue_status": "Closed", "issue_reporting_time": "2017-02-02T14:14:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "709": {"issue_url": "https://github.com/scrapy/scrapy/issues/2526", "issue_id": "#2526", "issue_summary": "Scrapy ignores proxy credentials when using \"proxy\" meta key", "issue_description": "vezunch1k commented on Feb 2, 2017\nCode yield Request(link, meta={'proxy': 'http://user:password@ip:port\u2019}) ignores user:password.\nProblem is solved by using header \"Proxy-Authorization\" with base64, but it is better to implement it inside Scrapy.", "issue_status": "Closed", "issue_reporting_time": "2017-02-02T13:48:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "710": {"issue_url": "https://github.com/scrapy/scrapy/issues/2524", "issue_id": "#2524", "issue_summary": "Cannot add custom command", "issue_description": "nihn commented on Feb 1, 2017 \u2022\nedited\nHello,\nI followed https://doc.scrapy.org/en/latest/topics/commands.html#custom-project-commands to add my custom command. I've set COMMANDS_MODULE=linkedin_scraper.commands. Unfortunately no new commands are present in scrapy -h output nor in scrapy commands.\nI've done some debugging and it seems that in _get_commands_dict function custom settings are not available yet so Scrapy does not know anything about my custom commands module.\nI have this project structure:\n.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 linkedin_scraper\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 __pycache__\n\u2502   \u251c\u2500\u2500 commands\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 people_search.py\n\u2502   \u251c\u2500\u2500 settings.py\n\u2502   \u2514\u2500\u2500 spiders\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 __pycache__\n\u2502       \u2514\u2500\u2500 people_search.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 run.sh\n\u251c\u2500\u2500 scrapy.cfg\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 tests.sh\nI tired with Scrapy 1.3.0 and 1.2.2.", "issue_status": "Closed", "issue_reporting_time": "2017-02-01T08:51:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "711": {"issue_url": "https://github.com/scrapy/scrapy/issues/2523", "issue_id": "#2523", "issue_summary": "S3 compatible API services", "issue_description": "drozzy commented on Feb 1, 2017 \u2022\nedited\nCan we use Feed exporter for S3 to store the items in an s3 compatible service, like Minio or Riak CS?\nI tried setting the FEED_URI to my local instance of minio http://192.168.1.111:9000/minio/mybucket/%(name)s.json\nbut scrapy complains that \"Unknown feed storage shceme: http\".\nIf I try changing it to s3://192.168.1.111:9000/minio/mybucket/%(name)s.json it tries to pre-pend it to amazon:\nCertificateError: hostname 192.168.1.111.s3.amazonaws.com doesn't match either of '*.s3.amazonaws.com', 's3.amazon.com'\nAny thoughts? Am I doing something wrong?", "issue_status": "Closed", "issue_reporting_time": "2017-02-01T03:58:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "712": {"issue_url": "https://github.com/scrapy/scrapy/issues/2522", "issue_id": "#2522", "issue_summary": "How to get spider.state when start_requests?", "issue_description": "yanpeipan commented on Jan 31, 2017\nin callback, state work well.", "issue_status": "Closed", "issue_reporting_time": "2017-01-31T16:39:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "713": {"issue_url": "https://github.com/scrapy/scrapy/issues/2520", "issue_id": "#2520", "issue_summary": "Scrapy missing ajax requests", "issue_description": "lavorato commented on Jan 31, 2017\nI created some Scrapy crawler with Selenium to paginate the Google Reviews window. My code works well on my machine (Mac OS) but when I send to my Linux AWS instance the behavior of pagination is not the same. I guess that scrapy are missing the ajax requests of each pagination.\nThe pagination loads in a intermitted way, loading some pages but not all like in my machine. Do you guys have any idea what is happening?", "issue_status": "Closed", "issue_reporting_time": "2017-01-30T21:17:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "714": {"issue_url": "https://github.com/scrapy/scrapy/issues/2518", "issue_id": "#2518", "issue_summary": "AttributeError: 'FeedExporter' object has no attribute 'slot'", "issue_description": "fausterjames commented on Jan 30, 2017 \u2022\nedited\nI have this simple spider, when I call scrapy crawl dataspider it works fine and prints the item in the output :\nimport json\nfrom scrapy.spiders import Spider\n\nclass dataspider(Spider):\n    name='dataspider'\n    start_urls=('https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL',)\n    def parse(self, response):\n        j=json.loads( response.body.decode('utf-8') )\n        yield j['matches'][1]\nOutputs :\n{'t': 'AAPL', 'n': 'Apple Inc.', 'e': 'NASDAQ', 'id': '22144'}\nHowever as soon as I try to save the item in a file using scrapy crawl dataspider -o out.json I get this error :\nAttributeError: 'FeedExporter' object has no attribute 'slot'\nFull Traceback is :\n$ scrapy crawl dataspider -o ./test.json\n2017-01-30 14:32:06 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: googlefinance)\n2017-01-30 14:32:06 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'googlefinance', 'CONCURRENT_REQUESTS': 100, 'CONCURRENT_REQUESTS_PER_DOMAIN': 100, 'DNS_TIMEOUT': 30, 'DOWNLOAD_TIMEOUT': 30, 'FEED_FORMAT': 'json', 'FEED_URI': './test.json', 'NEWSPIDER_MODULE': 'googlefinance.spiders', 'RETRY_HTTP_CODES': [500, 502, 503, 504, 400, 403, 404, 408], 'RETRY_TIMES': 30, 'SPIDER_MODULES': ['googlefinance.spiders'], 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; FSL 7.0.6.01001)'}\n2017-01-30 14:32:06 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats']\n2017-01-30 14:32:06 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2017-01-30 14:32:06 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2017-01-30 14:32:06 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2017-01-30 14:32:06 [scrapy.core.engine] INFO: Spider opened\n2017-01-30 14:32:06 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.open_spider of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 187, in open_spider\n    uri = self.urifmt % self._get_uri_params(spider)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 262, in _get_uri_params\n    params[k] = getattr(spider, k)\n  File \"/usr/lib/python3.6/site-packages/scrapy/spiders/__init__.py\", line 36, in logger\n    logger = logging.getLogger(self.name)\n  File \"/usr/lib/python3.6/logging/__init__.py\", line 1813, in getLogger\n    return Logger.manager.getLogger(name)\n  File \"/usr/lib/python3.6/logging/__init__.py\", line 1167, in getLogger\n    raise TypeError('A logger name must be a string')\nTypeError: A logger name must be a string\n2017-01-30 14:32:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2017-01-30 14:32:06 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2017-01-30 14:32:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL> (referer: None)\n2017-01-30 14:32:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL>\n{'t': 'AAPL', 'n': 'Apple Inc.', 'e': 'NASDAQ', 'id': '22144'}\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 217, in item_scraped\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL>\n{'t': 'AAPL', 'n': 'APPLE INC CEDEAR(REPR 1/10 SHR)', 'e': 'BCBA', 'id': '640373807586235'}\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 217, in item_scraped\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL>\n{'t': 'AAPL', 'n': 'Apple', 'e': 'SWX', 'id': '268194557752272'}\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 217, in item_scraped\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL>\n{'t': 'AVSPY', 'n': 'NASDAQ OMX Alpha AAPL vs. SPY Index', 'e': 'INDEXNASDAQ', 'id': '3139928'}\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 217, in item_scraped\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL>\n{'t': 'AAPL34', 'n': 'APPLE DRN', 'e': 'BVMF', 'id': '486420404817650'}\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 217, in item_scraped\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL>\n{'t': 'AAPL', 'n': 'APPLE COMPUTER INC', 'e': 'BMV', 'id': '119565461895124'}\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 217, in item_scraped\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL>\n{'t': 'AAPL-EUR', 'n': 'Apple', 'e': 'SWX', 'id': '706336206708362'}\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 217, in item_scraped\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.google.com/finance/match?matchtype=matchall&ei=UVlPWNmDEYm_U7SqgvAH&q=AAPL>\n{'t': 'AAPL-USD', 'n': 'Apple', 'e': 'SWX', 'id': '1009743014824088'}\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 217, in item_scraped\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.core.engine] INFO: Closing spider (finished)\n2017-01-30 14:32:07 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method FeedExporter.close_spider of <scrapy.extensions.feedexport.FeedExporter object at 0x7ff68de97ef0>>\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python3.6/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/usr/lib/python3.6/site-packages/scrapy/extensions/feedexport.py\", line 198, in close_spider\n    slot = self.slot\nAttributeError: 'FeedExporter' object has no attribute 'slot'\n2017-01-30 14:32:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 309,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 761,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2017, 1, 30, 13, 32, 7, 192220),\n 'item_scraped_count': 8,\n 'log_count/DEBUG': 10,\n 'log_count/ERROR': 10,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2017, 1, 30, 13, 32, 6, 846350)}\n2017-01-30 14:32:07 [scrapy.core.engine] INFO: Spider closed (finished)))\nAny idea what the problem is ?", "issue_status": "Closed", "issue_reporting_time": "2017-01-30T12:31:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "715": {"issue_url": "https://github.com/scrapy/scrapy/issues/2517", "issue_id": "#2517", "issue_summary": "Boolean setting on command line fails", "issue_description": "zwessels commented on Jan 29, 2017\nWhen I try to run my crawler from the command line with the command:\nscrapy crawl -sROBOTSTXT_OBEY=True mycrawler\nThen I get the following error message:\n`2017-01-29 00:24:24 [scrapy] INFO: Scrapy 1.2.1 started (bot: crawler)\n2017-01-29 00:24:24 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'crawler.spiders', 'ROBOTSTXT_OBEY': 'True', 'SPIDER_MODULES': ['crawler.spiders'], 'BOT_NAME': 'crawler'}\n2017-01-29 00:24:24 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n'scrapy.extensions.telnet.TelnetConsole',\n'scrapy.extensions.corestats.CoreStats']\nUnhandled error in Deferred:\n2017-01-29 00:24:24 [twisted] CRITICAL: Unhandled error in Deferred:\n2017-01-29 00:24:24 [twisted] CRITICAL:\nTraceback (most recent call last):\nFile \"/home/santa/.local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1260, in _inlineCallbacks\nresult = g.send(result)\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/crawler.py\", line 90, in crawl\nsix.reraise(*exc_info)\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/crawler.py\", line 72, in crawl\nself.engine = self._create_engine()\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/crawler.py\", line 97, in _create_engine\nreturn ExecutionEngine(self, lambda _: self.stop())\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 68, in init\nself.downloader = downloader_cls(crawler)\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/core/downloader/init.py\", line 88, in init\nself.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/middleware.py\", line 36, in from_settings\nmw = mwcls.from_crawler(crawler)\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/downloadermiddlewares/robotstxt.py\", line 33, in from_crawler\nreturn cls(crawler)\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/downloadermiddlewares/robotstxt.py\", line 24, in init\nif not crawler.settings.getbool('ROBOTSTXT_OBEY'):\nFile \"/home/santa/.local/lib/python2.7/site-packages/scrapy/settings/init.py\", line 129, in getbool\nreturn bool(int(self.get(name, default)))\nValueError: invalid literal for int() with base 10: 'True'`", "issue_status": "Closed", "issue_reporting_time": "2017-01-28T22:31:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "716": {"issue_url": "https://github.com/scrapy/scrapy/issues/2514", "issue_id": "#2514", "issue_summary": "Something wrong with ''Missing scheme in request url: %s' % self._url'", "issue_description": "yutiansut commented on Jan 27, 2017 \u2022\nedited\nWell, I meet some problems, but I don't know how to solve it. It seems that there are something wrong with my request url from the scrapy cmd callback\n raise ValueError('Missing scheme in request url: %s' % self._url)\nValueError: Missing scheme in request url: h\nmy start url is\nstart_urls = 'https://xueqiu.com/people'\nBut i have checked the spider from many times, and still don't know how to solve this\nI used the selenium + PhantomJS as the middleware, codes below are my spider's code,could you help me?\nIPSpider.py\nimport scrapy\nfrom scrapy.spiders import Spider  \n# from scrapy.http import Request  \nfrom scrapy.selector import Selector  \nfrom influenceSpider.items import InfluencespiderItem\nfrom influenceSpider.middlewares import InfluencespiderSpiderMiddleware\nimport json\nimport base64\nimport requests\nfrom selenium import webdriver  \nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nimport getCookies as cok\n\nclass IpspiderSpider(Spider):\n    #\n    name = \"IPSpider\"\n    allowed_domains = [\"xueqiu.com\"] \n    item = [ ]\n    start_urls = 'https://xueqiu.com/people'\n    t = cok.getCookies()\n    cookies = t.getCookiesFromXueqiu('xueqiu');\n    print cookies\n    print '===start requests==='\n    print start_urls\n    def parse(self, response):\n        print self.start_urls\n        print self.cookies\n        spider = Spider(self,name=\"IPSpider\")\n        InfluencespiderSpiderMiddleware.process_request(self,self.start_urls,spider,self.cookies)\nmiddleware.py\nfrom selenium import webdriver  \nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nfrom scrapy.http import HtmlResponse\nimport time  \nimport random\nimport base64\n\nclass InfluencespiderSpiderMiddleware(object):\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    def process_request(self, request, spider,cookies):\n        print \"PhantomJS is starting...\"\n        dcap = dict(DesiredCapabilities.PHANTOMJS)\n        dcap[\"phantomjs.page.settings.userAgent\"] = (\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0 \")\n        driver = webdriver.PhantomJS(executable_path='D:/Projects/Projects/craw/xueqiu/influenceSpider/influenceSpider/phantomjs.exe', desired_capabilities=dcap)\n print 'before get'\n        driver.add_cookie(cookies)\n        driver.get(request.url)\n        print 'after get'\n        time.sleep(1)\n        js = \"var q=document.documentElement.scrollTop=10000\"\n        driver.execute_script(js) #\u53ef\u6267\u884cjs\uff0c\u6a21\u4eff\u7528\u6237\u64cd\u4f5c\u3002\u6b64\u5904\u4e3a\u5c06\u9875\u9762\u62c9\u81f3\u6700\u5e95\u7aef\u3002\n        time.sleep(3)\n        body = driver.page_source\n        print(\"\u8bbf\u95ee\"+request.url)\n        return HtmlResponse(driver.current_url, body=body, encoding='utf-8', request=request)\nthanks anyway", "issue_status": "Closed", "issue_reporting_time": "2017-01-27T00:33:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "717": {"issue_url": "https://github.com/scrapy/scrapy/issues/2513", "issue_id": "#2513", "issue_summary": "twisted error in log", "issue_description": "whwq2012 commented on Jan 25, 2017\n2017-01-19 14:41:21 [twisted] CRITICAL: Unhandled Error\nTraceback (most recent call last):\nFailure: twisted.internet.error.ConnectionDone: Connection was closed cleanly.\nThis is error, I check my code, and I think the error that from scrapy\nscrapy version:1.30\nos:centos7.0\npython version: 2.7.11", "issue_status": "Closed", "issue_reporting_time": "2017-01-25T08:07:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "718": {"issue_url": "https://github.com/scrapy/scrapy/issues/2511", "issue_id": "#2511", "issue_summary": "Python 3.6 Item inheritance fails", "issue_description": "jeffcjohnson commented on Jan 24, 2017\nWhen trying to use inheritance on Item I get\nTypeError: __class__ set to <class '__main__.SpecialItem'> defining 'SpecialItem' as <class '__main__.SpecialItem'>\nSee more detail here:\nscrapy-plugins/scrapy-djangoitem#18\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2017-01-24T17:21:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "719": {"issue_url": "https://github.com/scrapy/scrapy/issues/2508", "issue_id": "#2508", "issue_summary": "listjobs.json response does not contain parameters", "issue_description": "sieira commented on Jan 23, 2017\nIs there a way to retrieve the parameters of a scheduled task?\nCurrently\n$ curl http://localhost:6800/schedule.json -d project=myproject -d spider=somespider -d setting=DOWNLOAD_DELAY=2 -d arg1=val1\nReturns\n{\"status\": \"ok\",\n \"pending\": [{\"id\": \"78391cc0fcaf11e1b0090800272a6d06\", \"spider\": \"somespider\"}]\n}\nI wonder if there is a way to get:\n{\"status\": \"ok\",\n \"pending\": [{\n    \"id\": \"78391cc0fcaf11e1b0090800272a6d06\",\n    \"spider\": \"somespider\",\n    \"setting\": {\"DOWNLOAD_DELAY\": 2},\n    \"arg1\": \"val1\"\n}]\n}", "issue_status": "Closed", "issue_reporting_time": "2017-01-23T14:23:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "720": {"issue_url": "https://github.com/scrapy/scrapy/issues/2506", "issue_id": "#2506", "issue_summary": "Cannot install scrapy on windows", "issue_description": "top1st commented on Jan 21, 2017\nc:\\Python27\\Scripts>pip2.7 install scrapy\nCollecting scrapy\nUsing cached Scrapy-1.3.0-py2.py3-none-any.whl\nCollecting service-identity (from scrapy)\nUsing cached service_identity-16.0.0-py2.py3-none-any.whl\nCollecting cssselect>=0.9 (from scrapy)\nUsing cached cssselect-1.0.1-py2.py3-none-any.whl\nCollecting queuelib (from scrapy)\nUsing cached queuelib-1.4.2-py2.py3-none-any.whl\nCollecting parsel>=0.9.5 (from scrapy)\nUsing cached parsel-1.1.0-py2.py3-none-any.whl\nCollecting w3lib>=1.15.0 (from scrapy)\nUsing cached w3lib-1.16.0-py2.py3-none-any.whl\nCollecting lxml (from scrapy)\nUsing cached lxml-3.7.2.tar.gz\nCollecting pyOpenSSL (from scrapy)\nUsing cached pyOpenSSL-16.2.0-py2.py3-none-any.whl\nCollecting six>=1.5.2 (from scrapy)\nUsing cached six-1.10.0-py2.py3-none-any.whl\nCollecting Twisted>=13.1.0 (from scrapy)\nUsing cached Twisted-16.6.0.tar.bz2\nCollecting PyDispatcher>=2.0.5 (from scrapy)\nUsing cached PyDispatcher-2.0.5.tar.gz\nCollecting pyasn1-modules (from service-identity->scrapy)\nUsing cached pyasn1_modules-0.0.8-py2.py3-none-any.whl\nCollecting attrs (from service-identity->scrapy)\nUsing cached attrs-16.3.0-py2.py3-none-any.whl\nCollecting pyasn1 (from service-identity->scrapy)\nUsing cached pyasn1-0.1.9-py2.py3-none-any.whl\nCollecting cryptography>=1.3.4 (from pyOpenSSL->scrapy)\nDownloading cryptography-1.7.1-cp27-cp27m-win32.whl (900kB)\n100% |################################| 901kB 652kB/s\nCollecting zope.interface>=3.6.0 (from Twisted>=13.1.0->scrapy)\nDownloading zope.interface-4.3.3-cp27-cp27m-win32.whl (135kB)\n100% |################################| 143kB 302kB/s\nCollecting constantly>=15.1 (from Twisted>=13.1.0->scrapy)\nUsing cached constantly-15.1.0-py2.py3-none-any.whl\nCollecting incremental>=16.10.1 (from Twisted>=13.1.0->scrapy)\nUsing cached incremental-16.10.1-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): setuptools>=11.3 in c:\\python27\\lib\\site-packages (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nCollecting enum34 (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nDownloading enum34-1.1.6-py2-none-any.whl\nCollecting ipaddress (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nDownloading ipaddress-1.0.18-py2-none-any.whl\nCollecting idna>=2.0 (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nUsing cached idna-2.2-py2.py3-none-any.whl\nCollecting cffi>=1.4.1 (from cryptography>=1.3.4->pyOpenSSL->scrapy)\nDownloading cffi-1.9.1-cp27-cp27m-win32.whl (145kB)\n100% |################################| 153kB 306kB/s\nCollecting pycparser (from cffi>=1.4.1->cryptography>=1.3.4->pyOpenSSL->scrapy)\nUsing cached pycparser-2.17.tar.gz\nInstalling collected packages: enum34, ipaddress, pyasn1, six, idna, pycparser, cffi, cryptography, pyOpenSSL, pyasn1-modules, attrs, service-identity, cssselect, queuelib, w3lib, lxml, parsel, zope.interface, constantly, incremental, Twisted, PyDispatcher, scrapy\nRunning setup.py install for pycparser ... done\nRunning setup.py install for lxml ... error\nComplete output from command c:\\python27\\python.exe -u -c \"import setuptools, tokenize;file='c:\\users\\top1st\\appdata\\local\\temp\\pip-build-zypkvu\\lxml\\setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record c:\\users\\top1st\\appdata\\local\\temp\\pip-xw2rdw-record\\install-record.txt --single-version-externally-managed --compile:\nBuilding lxml version 3.7.2.\nBuilding without Cython.\nERROR: 'xslt-config' is not recognized as an internal or external command,\noperable program or batch file.\n** make sure the development packages of libxml2 and libxslt are installed **\n\nUsing build configuration of libxslt\nrunning install\nrunning build\nrunning build_py\ncreating build\ncreating build\\lib.win32-2.7\ncreating build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\builder.py -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\cssselect.py -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\doctestcompare.py -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\ElementInclude.py -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\pyclasslookup.py -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\sax.py -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\usedoctest.py -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\_elementpath.py -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\__init__.py -> build\\lib.win32-2.7\\lxml\ncreating build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\__init__.py -> build\\lib.win32-2.7\\lxml\\includes\ncreating build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\builder.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\clean.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\defs.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\diff.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\ElementSoup.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\formfill.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\html5parser.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\soupparser.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\usedoctest.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\_diffcommand.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\_html5builder.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\_setmixin.py -> build\\lib.win32-2.7\\lxml\\html\ncopying src\\lxml\\html\\__init__.py -> build\\lib.win32-2.7\\lxml\\html\ncreating build\\lib.win32-2.7\\lxml\\isoschematron\ncopying src\\lxml\\isoschematron\\__init__.py -> build\\lib.win32-2.7\\lxml\\isoschematron\ncopying src\\lxml\\lxml.etree.h -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\lxml.etree_api.h -> build\\lib.win32-2.7\\lxml\ncopying src\\lxml\\includes\\c14n.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\config.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\dtdvalid.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\etreepublic.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\htmlparser.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\relaxng.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\schematron.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\tree.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\uri.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\xinclude.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\xmlerror.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\xmlparser.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\xmlschema.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\xpath.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\xslt.pxd -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\etree_defs.h -> build\\lib.win32-2.7\\lxml\\includes\ncopying src\\lxml\\includes\\lxml-version.h -> build\\lib.win32-2.7\\lxml\\includes\ncreating build\\lib.win32-2.7\\lxml\\isoschematron\\resources\ncreating build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\rng\ncopying src\\lxml\\isoschematron\\resources\\rng\\iso-schematron.rng -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\rng\ncreating build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\ncopying src\\lxml\\isoschematron\\resources\\xsl\\RNG2Schtrn.xsl -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\ncopying src\\lxml\\isoschematron\\resources\\xsl\\XSD2Schtrn.xsl -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\ncreating build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\ncopying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_abstract_expand.xsl -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\ncopying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_dsdl_include.xsl -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\ncopying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_message.xsl -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\ncopying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_schematron_skeleton_for_xslt1.xsl -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\ncopying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\iso_svrl_for_xslt1.xsl -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\ncopying src\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\\readme.txt -> build\\lib.win32-2.7\\lxml\\isoschematron\\resources\\xsl\\iso-schematron-xslt1\nrunning build_ext\nbuilding 'lxml.etree' extension\nerror: Microsoft Visual C++ 9.0 is required (Unable to find vcvarsall.bat). Get it from http://aka.ms/vcpython27\n\n----------------------------------------\nCommand \"c:\\python27\\python.exe -u -c \"import setuptools, tokenize;file='c:\\users\\top1st\\appdata\\local\\temp\\pip-build-zypkvu\\lxml\\setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record c:\\users\\top1st\\appdata\\local\\temp\\pip-xw2rdw-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in c:\\users\\top1st\\appdata\\local\\temp\\pip-build-zypkvu\\lxml\nYou are using pip version 8.1.1, however version 9.0.1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.", "issue_status": "Closed", "issue_reporting_time": "2017-01-21T13:22:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "721": {"issue_url": "https://github.com/scrapy/scrapy/issues/2501", "issue_id": "#2501", "issue_summary": "scrapy view <url> raise exc in v1.3.0", "issue_description": "wingyiu commented on Jan 19, 2017 \u2022\nedited\n(py35) wingyiu@mbp101:~$scrapy view http://www.scrapy.org\n2017-01-19 22:13:54 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: scrapybot)\n2017-01-19 22:13:54 [scrapy.utils.log] INFO: Overridden settings: {}\nTraceback (most recent call last):\n  File \"/Users/user/venv/py35/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/Users/user/venv/py35/lib/python3.5/site-packages/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/user/venv/py35/lib/python3.5/site-packages/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/user/venv/py35/lib/python3.5/site-packages/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/user/venv/py35/lib/python3.5/site-packages/scrapy/commands/fetch.py\", line 58, in run\n    if not opts.no_redirect:\nAttributeError: 'Values' object has no attribute 'no_redirect'", "issue_status": "Closed", "issue_reporting_time": "2017-01-19T14:16:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "722": {"issue_url": "https://github.com/scrapy/scrapy/issues/2492", "issue_id": "#2492", "issue_summary": "Scrapy + splash: no module named scrapy_splash", "issue_description": "tituskex commented on Jan 11, 2017\nHi,\nI'm trying to learn how to work with splash for scrapy. I'm doing this tutorial: https://github.com/scrapy-plugins/scrapy-splash.\nI've created a scrapy project which I added to this post. When I run $ scrapy crawl spider1 everything works fine. However, when I add: DOWNLOADER_MIDDLEWARES = { 'scrapy_splash.SplashCookiesMiddleware': 723, 'scrapy_splash.SplashMiddleware': 725, 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810, }\nI get a message saying: ModuleNotFoundError: No module named 'scrapy_splash'. I've checked whether I have scrapy_splash installed with:\n$ User-MacBook-Air:tScraper username$ pip3 show scrapy_splash\nName: scrapy-splash\nVersion: 0.7.1\nSummary: JavaScript support for Scrapy using Splash\nHome-page: https://github.com/scrapy-plugins/scrapy-splash\nAuthor: Mikhail Korobov\nAuthor-email: kmike84@gmail.com\nLicense: BSD\nLocation: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\nRequires:\nI've tried to import scrapy_splash into my spider script and into my settings script. If I do so, I get a message saying:\nraise KeyError(\"Spider not found: {}\".format(spider_name))\nKeyError: 'Spider not found: spider1'\nMy script: tScraper.zip\nDoes anyone know how to fix this issue?", "issue_status": "Closed", "issue_reporting_time": "2017-01-11T11:45:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "723": {"issue_url": "https://github.com/scrapy/scrapy/issues/2491", "issue_id": "#2491", "issue_summary": "TLS connection fails through HTTPS proxy after CONNECT tunnel is established", "issue_description": "aiportal commented on Jan 11, 2017 \u2022\nedited by redapple\nI set proxy by this code:\nclass HttpProxyMiddleware(object):\n    def process_request(self, request, spider):\n        request.meta['proxy'] = 'https://127.0.0.1:8787'\nIt's error is:\nscrapy.core.downloader.handlers.http11.TunnelError: Could not open CONNECT tunnel with proxy 127.0.0.1:8787\nThen I test the proxy by requests:\nresp = requests.get('https://......', proxies={'https': 'https://127.0.0.1:8787'})\nIt' work!\nSo, what is it happen?", "issue_status": "Closed", "issue_reporting_time": "2017-01-11T03:09:17Z", "fixed_by": "#2495", "pull_request_summary": "[MRG] Buffer CONNECT response bytes from proxy until all HTTP headers are received", "pull_request_description": "Contributor\nredapple commented on Jan 13, 2017\nFixes #2491\nSee #2491 (comment) for rationale.\nA simple fix, but ideally, we'd want to use a proper HTTP parser.\nSomething for another PR I believe.", "pull_request_status": "Merged", "issue_fixed_time": "2017-02-20T20:44:36Z", "files_changed": [["11", "scrapy/core/downloader/handlers/http11.py"]]}, "724": {"issue_url": "https://github.com/scrapy/scrapy/issues/2490", "issue_id": "#2490", "issue_summary": "Scrapyd spider connecting to a random host.", "issue_description": "Cesped commented on Jan 10, 2017 \u2022\nedited\nRecently installed Scrapy and Scrapyd on Ubuntu 14.04.5 LTS.\nAfter starting any spider in Scrapyd, Scrapy opens one or multiple connections to urls (mostly \"colocrossing.com\") not belonging to the spider's allowed_domains. lsof -i:80 outputs:\n$ lsof -i:80\nCOMMAND   PID     USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\npython   8658   scrapy   15u  IPv4 4189971      0t0  TCP localhost:42217->xx.xx.xx.xx-host.colocrossing.com:http (SYN_SENT)\npython  12767   scrapy   14u  IPv4 4188973      0t0  TCP localhost:41640->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  12767   scrapy   15u  IPv4 4189757      0t0  TCP localhost:46526->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  12767   scrapy   17u  IPv4 4189616      0t0  TCP localhost:53453->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\nThen:\n$ ps -aux | grep 8658\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nscrapy    8658  6.0  5.5 699820 449332 ?       Sl   03:43  29:43 /usr/bin/python -m scrapyd.runner crawl my_spider -a _job=a31a45d8d6a611e6beef00163e48492c\nOne spider might have multiple connections like this:\n$ lsof -i:80 | grep 27279\nCOMMAND   PID     USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\npython  27279   scrapy   14u  IPv4 4199890      0t0  TCP localhost:48256->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   16u  IPv4 4197326      0t0  TCP localhost:40035->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   17u  IPv4 4200900      0t0  TCP localhost:52125->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   18u  IPv4 4203466      0t0  TCP localhost:49432->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   19u  IPv4 4195886      0t0  TCP localhost:56941->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   22u  IPv4 4199342      0t0  TCP localhost:34607->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   23u  IPv4 4203035      0t0  TCP localhost:47392->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   25u  IPv4 4203052      0t0  TCP localhost:48560->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   26u  IPv4 4203194      0t0  TCP localhost:47006->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   27u  IPv4 4198839      0t0  TCP localhost:35547->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   28u  IPv4 4203082      0t0  TCP localhost:49981->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   29u  IPv4 4201208      0t0  TCP localhost:54259->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   31u  IPv4 4203109      0t0  TCP localhost:54151->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   32u  IPv4 4202316      0t0  TCP localhost:58475->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   33u  IPv4 4204651      0t0  TCP localhost:56938->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   34u  IPv4 4203880      0t0  TCP localhost:55858->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   35u  IPv4 4205155      0t0  TCP localhost:50273->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   36u  IPv4 4205169      0t0  TCP localhost:42232->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   37u  IPv4 4205188      0t0  TCP localhost:54358->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\npython  27279   scrapy   38u  IPv4 4204011      0t0  TCP localhost:54375->xx.xx.xx.xx-host.colocrossing.com:http (ESTABLISHED)\nIs this related to scrapy/scrapyd? If so, is it a normal behavior?\nScrapy version:\n$ scrapy version -v\nScrapy    : 1.3.0\nlxml      : 3.6.4.0\nlibxml2   : 2.9.1\ncssselect : 1.0.0\nparsel    : 1.1.0\nw3lib     : 1.16.0\nTwisted   : 16.4.0\nPython    : 2.7.6 (default, Oct 26 2016, 20:30:19) - [GCC 4.8.4]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1f 6 Jan 2014)\nPlatform  : Linux-3.13.0-24-generic-x86_64-with-Ubuntu-14.04-trusty", "issue_status": "Closed", "issue_reporting_time": "2017-01-10T12:39:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "725": {"issue_url": "https://github.com/scrapy/scrapy/issues/2487", "issue_id": "#2487", "issue_summary": "an error import in scrapy/core/downloader/handlers/http11.py", "issue_description": "zonglinyue commented on Jan 10, 2017\nan error import at line 16 is that\n\"from twisted.web.client import Agent, ProxyAgent, ResponseDone,\nHTTPConnectionPool\"\nthere's none class or method in twisted.web.client named HTTPConnectionPool,is it \"HTTP11ClientProtocol\" in \"twisted.web._newclient\" or \"HTTPConnectionPool\" in \"urllib3\"", "issue_status": "Closed", "issue_reporting_time": "2017-01-10T08:39:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "726": {"issue_url": "https://github.com/scrapy/scrapy/issues/2486", "issue_id": "#2486", "issue_summary": "The order of install failed", "issue_description": "MarcSteven commented on Jan 9, 2017\nI try to install via shell ,do this order $pip install Scrapy,but It cannot install ...My version of Python is 2.7.1.......", "issue_status": "Closed", "issue_reporting_time": "2017-01-09T05:44:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "727": {"issue_url": "https://github.com/scrapy/scrapy/issues/2479", "issue_id": "#2479", "issue_summary": "Scrapy is incompatible with Twisted 16.7.0rc1", "issue_description": "Contributor\nredapple commented on Jan 4, 2017\nSee Twisted 16.7.0rc1 release annoucement for new features and changes.\ntwisted/twisted#624 seems to be causing trouble (may not be the only one).\n#2461 is also happening with 16.7.0rc1", "issue_status": "Closed", "issue_reporting_time": "2017-01-04T13:44:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "728": {"issue_url": "https://github.com/scrapy/scrapy/issues/2475", "issue_id": "#2475", "issue_summary": "Recommend Anaconda when installing Scrapy on Windows", "issue_description": "Contributor\nredapple commented on Jan 3, 2017\nSuccesfully using pip on Windows for Scrapy (and lxml and Twisted) is far from easy.\nAnaconda/Miniconda seems to work much smoother.\nLet's recommend Anaconda first for Windows users in the installation page, and leave the pip-way for the brave.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2017-01-03T12:58:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "729": {"issue_url": "https://github.com/scrapy/scrapy/issues/2474", "issue_id": "#2474", "issue_summary": "Optimised dequeing", "issue_description": "mohmad-null commented on Jan 3, 2017\nA feature request.\nI've got my settings set up so that Scrapy is being nice and polite to any given site. For example:\n# Very high\nCONCURRENT_REQUESTS = 128\n\nCONCURRENT_REQUESTS_PER_IP = 3\nDOWNLOAD_DELAY = 2\n\nAUTOTHROTTLE_ENABLED = True\nAUTOTHROTTLE_TARGET_CONCURRENCY = 3\nAUTOTHROTTLE_START_DELAY = 1\nAUTOTHROTTLE_MAX_DELAY = 15\n\n#because I want to try and focus on breadth - I'm mostly making few a requests to many many domains\n#With a few domains getting many requests\nDEPTH_PRIORITY = 1\nSCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'\nSCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'\nAs you can see, for any given IP, scrapy is going to be slow, but in theory scrapy can handle many different IP's at once - that's why CONCURRENT_REQUESTS is very high. The problem is, it doesn't seem to.\nConsider the following hypothetical queue (assume all domains are on different IPs):\nexample.com/1\nexample.com/2\nexample.com/3\nexample.com/4\nexample.com/5\nexample.com/6\nexample.com/7\nexample.com/8\ngoogle.example/1\ngoogle.example/2\ngoogle.example/3\ngoogle.example/4\ngoogle.example/5\ngoogle.example/6\ngoogle.example/7\nblue.example\nyellow.example\nred.example\ngreen.example\npurple.example\npink.example\nblank.example\nAssuming scrapy is parsing the above top-down (and ignoring whether we're doing FIFO or reverse), what seems to happen is that scrapy will scrape all of the example.com pages first, then the google.example ones, in both instances hitting the per-ip/autothrottle limits and waiting for them to clear down.\nHowever, the optimal way to do the above list is to note all the different domains, and then try and do them all concurrently (within the limit of CONCURRENT_REQUESTS). This doesn't appear to happen at present.\n(At least, I'm guessing that's the reason I'm seeing very slow crawl rates, a low number of tcp/ip threads, as well as negligible CPU usage (<20% of a single core), and almost non-existent network usage (about 1MB per min - 130k bits/s!!!))\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2017-01-02T19:32:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "730": {"issue_url": "https://github.com/scrapy/scrapy/issues/2467", "issue_id": "#2467", "issue_summary": "Quotes in html text are parsed incorrectly", "issue_description": "jacklol13 commented on Dec 26, 2016\nThis is a simple IPython test case:\nIn [1]: from scrapy.selector import Selector\n\nIn [2]: Selector(text='<div x=\"It\\x22s\"', type=\"html\")\nOut[2]: <Selector xpath=None data=u'<html><body><div x=\"It\" s></div></body><'>\nThe data in the selector shows <div x=\"It\" s> instead of <div x=\"It\\\"s\"> or <div x=\"It&quot;s\">\nThis has been tested in Scrapy 1.2.2", "issue_status": "Closed", "issue_reporting_time": "2016-12-25T23:44:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "731": {"issue_url": "https://github.com/scrapy/scrapy/issues/2465", "issue_id": "#2465", "issue_summary": "Half the time stats.get_value('finish_time') returns None", "issue_description": "theotheo commented on Dec 24, 2016 \u2022\nedited\nHi there! I'm trying to add time of scraping to stats. So I create Extension class with spider_closed callback:\n    def spider_closed(self, spider):\n        stats = self.stats\n        ...\n        finish_time = stats.get_value('finish_time')\nSurprisingly half the time stats.get_value('finish_time') returns None, but at the same time in the dump from scrapy.statscollectors finish_time is ok.\n2016-12-23 21:25:30 hp spider[28062] DEBUG finish_time None\n2016-12-23 21:25:30 hp scrapy.utils.signal[28062] ERROR Error caught on signal handler: <bound method Ext.spider_closed of <spider.Ext object at 0x7f7358c33400>>\nTraceback (most recent call last):\n  File \"/home/i/.local/lib/python3.5/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/home/i/.local/lib/python3.5/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/home/i/LABS/FAQ/spider.py\", line 117, in spider_closed\n    delta = finish_time - start_time\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'datetime.datetime'\n2016-12-23 21:25:30 hp scrapy.statscollectors[28062] INFO Dumping Scrapy stats:\n{'downloader/request_bytes': 5481,\n 'downloader/request_count': 17,\n 'downloader/request_method_count/GET': 17,\n 'downloader/response_bytes': 101049,\n 'downloader/response_count': 17,\n 'downloader/response_status_count/200': 10,\n 'downloader/response_status_count/302': 1,\n 'downloader/response_status_count/404': 6,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 12, 23, 18, 25, 30, 724220),\n 'item_scraped_count': 9,\n 'log_count/DEBUG': 21,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 11,\n 'request_depth_max': 1,\n 'response_received_count': 11,\n 'scheduler/dequeued': 17,\n 'scheduler/dequeued/memory': 17,\n 'scheduler/enqueued': 17,\n 'scheduler/enqueued/memory': 17,\n 'start_time': datetime.datetime(2016, 12, 23, 18, 25, 26, 649006)}\nWhat I do wrong? Is it a bug?", "issue_status": "Closed", "issue_reporting_time": "2016-12-23T18:45:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "732": {"issue_url": "https://github.com/scrapy/scrapy/issues/2463", "issue_id": "#2463", "issue_summary": "A lot of dead links to scrapy's readthedocs pages", "issue_description": "vovanz commented on Dec 23, 2016\nLink to online documentation from scrapy website is dead: http://doc.scrapy.org/en/1.2\nWhen you google something like scrapy spiders top 3 links are all dead.\nWhen I was submitting this issue github suggested me to read guidelines for contributing. I wish I could do it, but the link is also dead:\nThe guidelines for contributing are available here: http://doc.scrapy.org/en/master/contributing.html", "issue_status": "Closed", "issue_reporting_time": "2016-12-23T15:02:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "733": {"issue_url": "https://github.com/scrapy/scrapy/issues/2461", "issue_id": "#2461", "issue_summary": "TypeError: 'float' object is not iterable (on Twisted dev + Scrapy dev)", "issue_description": "Contributor\nrmax commented on Dec 22, 2016 \u2022\nedited\nThis happens on Twisted trunk and with latest Scrapy master.\n$ scrapy shell http://localhost:8081/\n2016-12-22 12:52:01 [scrapy.utils.log] INFO: Scrapy 1.2.2 started (bot: scrapybot)\n2016-12-22 12:52:01 [scrapy.utils.log] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2016-12-22 12:52:01 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-12-22 12:52:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-12-22 12:52:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-12-22 12:52:01 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2016-12-22 12:52:01 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-12-22 12:52:01 [scrapy.core.engine] INFO: Spider opened\nTraceback (most recent call last):\n  File \"/Users/rolando/miniconda3/envs/dev/bin/scrapy\", line 11, in <module>\n    load_entry_point('Scrapy', 'console_scripts', 'scrapy')()\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/commands/shell.py\", line 71, in run\n    shell.start(url=url)\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/shell.py\", line 47, in start\n    self.fetch(url, spider)\n  File \"/Users/rolando/Projects/sh/scrapy/scrapy/shell.py\", line 112, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/Users/rolando/Projects/gh/twisted/src/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/Users/rolando/Projects/gh/twisted/src/twisted/python/failure.py\", line 372, in raiseException\n    raise self.value.with_traceback(self.tb)\nTypeError: 'float' object is not iterable\n(Pdb) w\n  /Users/rolando/Projects/sh/scrapy/scrapy/utils/defer.py(45)mustbe_deferred()\n-> result = f(*args, **kw)\n  /Users/rolando/Projects/sh/scrapy/scrapy/core/downloader/handlers/__init__.py(65)download_request()\n-> return handler.download_request(request, spider)\n  /Users/rolando/Projects/sh/scrapy/scrapy/core/downloader/handlers/http11.py(61)download_request()\n-> return agent.download_request(request)\n  /Users/rolando/Projects/sh/scrapy/scrapy/core/downloader/handlers/http11.py(286)download_request()\n-> method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n  /Users/rolando/Projects/gh/twisted/src/twisted/web/client.py(1601)request()\n-> parsedURI.originForm)\n  /Users/rolando/Projects/gh/twisted/src/twisted/web/client.py(1378)_requestWithEndpoint()\n-> d = self._pool.getConnection(key, endpoint)\n  /Users/rolando/Projects/gh/twisted/src/twisted/web/client.py(1264)getConnection()\n-> return self._newConnection(key, endpoint)\n  /Users/rolando/Projects/gh/twisted/src/twisted/web/client.py(1276)_newConnection()\n-> return endpoint.connect(factory)\n  /Users/rolando/Projects/gh/twisted/src/twisted/internet/endpoints.py(779)connect()\n-> EndpointReceiver, self._hostText, portNumber=self._port\n  /Users/rolando/Projects/gh/twisted/src/twisted/internet/_resolver.py(174)resolveHostName()\n-> onAddress = self._simpleResolver.getHostByName(hostName)\n  /Users/rolando/Projects/sh/scrapy/scrapy/resolver.py(21)getHostByName()\n-> d = super(CachingThreadedResolver, self).getHostByName(name, timeout)\n> /Users/rolando/Projects/gh/twisted/src/twisted/internet/base.py(276)getHostByName()\n-> timeoutDelay = sum(timeout)\nAfter digging, I found out that the addition of DNS_TIMEOUT was not effective at all: 85aa3c7#diff-92d881d6568986904888f43c885240e2L13\nPreviously, on Twisted <=16.6.0, the method getHostByName was always called with a default timeout: https://github.com/twisted/twisted/blob/twisted-16.6.0/src/twisted/internet/base.py#L565-L573\nBut now, on Twisted trunk, the method is called without a timeout parameter: https://github.com/twisted/twisted/blob/trunk/src/twisted/internet/_resolver.py#L174\nThis makes the caching resolver to use the default value timeout=60.0 which causes the error: https://github.com/twisted/twisted/blob/twisted-16.6.0/src/twisted/internet/base.py#L259-L268", "issue_status": "Closed", "issue_reporting_time": "2016-12-22T16:58:37Z", "fixed_by": "#2496", "pull_request_summary": "[MRG] Enforce DNS resolution timeout", "pull_request_description": "Contributor\nredapple commented on Jan 13, 2017\nFixes #2461\nAs @rolando noticed, DNS_TIMEOUT setting was never actually used by Scrapy with 14<=Twisted<=16.6, because a (1, 3, 11, 45) tuple was always passed to the DNS resolver by Twisted internally:\nSuccessfully installed twisted-14.0.2\n$ scrapy fetch --headers https://www.example.com -s DNS_TIMEOUT=15\n2017-01-13 16:06:33 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: scrapybot)\n2017-01-13 16:06:33 [scrapy.utils.log] INFO: Overridden settings: {'DNS_TIMEOUT': '15'}\n(...)\nCachingThreadedResolver.getHostByName(name='www.example.com', timeout=(1, 3, 11, 45))\n2017-01-13 16:06:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.example.com> (referer: None)\n\n\nSuccessfully installed twisted-15.5.0\n$ scrapy fetch --headers https://www.example.com -s DNS_TIMEOUT=15\n2017-01-13 16:07:05 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: scrapybot)\n2017-01-13 16:07:05 [scrapy.utils.log] INFO: Overridden settings: {'DNS_TIMEOUT': '15'}\n(...)\nCachingThreadedResolver.getHostByName(name='www.example.com', timeout=(1, 3, 11, 45))\n2017-01-13 16:07:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.example.com> (referer: None)\n\n\nSuccessfully installed twisted-16.6.0\n$ scrapy fetch --headers https://www.example.com -s DNS_TIMEOUT=15\n2017-01-13 16:07:20 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: scrapybot)\n2017-01-13 16:07:20 [scrapy.utils.log] INFO: Overridden settings: {'DNS_TIMEOUT': '15'}\n(...)\nCachingThreadedResolver.getHostByName(name='www.example.com', timeout=(1, 3, 11, 45))\n2017-01-13 16:07:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.example.com> (referer: None)\nAnd Twisted 16.7 actually stops passing this default value, but a tuple (or list) of ints is expected.", "pull_request_status": "Merged", "issue_fixed_time": "2017-02-02T16:13:59Z", "files_changed": [["7", "scrapy/resolver.py"]]}, "734": {"issue_url": "https://github.com/scrapy/scrapy/issues/2459", "issue_id": "#2459", "issue_summary": "can't use \"scrapy\" after scrapy installed", "issue_description": "zonglinyue commented on Dec 21, 2016\nI have installed scrapy successed\nInstalling collected packages: scrapy\nSuccessfully installed scrapy-1.2.2\npip show Scrapy\nName: Scrapy\nVersion: 1.2.2\nSummary: A high-level Web Crawling and Web Scraping framework\nHome-page: http://scrapy.org\nAuthor: Pablo Hoffman\nAuthor-email: pablo@pablohoffman.com\nLicense: BSD\nLocation: /usr/local/python3.5/lib/python3.5/site-packages\nRequires: service-identity, Twisted, six, parsel, cssselect, PyDispatcher, queuelib, lxml, pyOpenSSL, w3lib\nbut when i use scrapy ,\"bash: scrapy: command not found\".is that i should \"ln -s *** scrapy\"? please tell me which is the target file,thank you!", "issue_status": "Closed", "issue_reporting_time": "2016-12-21T15:26:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "735": {"issue_url": "https://github.com/scrapy/scrapy/issues/2453", "issue_id": "#2453", "issue_summary": "Scrapy asynchronous behaviour : how to bypass it and wait for a result to perform a new Request ?", "issue_description": "younesherlock commented on Dec 18, 2016\nHello,\nMy code aims to scrape a webpage (the one in start_urls) to retrieve a number in a \"Show More\" button.\nThis number will then be used as a parameter to request a URL that will return a JSON that contains data + a number. This last number will be used as a parameter to request the URL that will return a JSON that contains data + a number, etc.. The process goes on until the JSON return empty data + a number. When the data is empty, the scraper should stop.\nCould you give me an example of a code that does a similar thing ?", "issue_status": "Closed", "issue_reporting_time": "2016-12-18T10:52:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "736": {"issue_url": "https://github.com/scrapy/scrapy/issues/2452", "issue_id": "#2452", "issue_summary": "Image Background converting to green.", "issue_description": "mkaya93 commented on Dec 15, 2016 \u2022\nedited\nHi,\nProblem is I'm downloading images with crawler but they are transparency. So image is looking like this.\nBut it should look like this.\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2016-12-15T15:18:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "737": {"issue_url": "https://github.com/scrapy/scrapy/issues/2451", "issue_id": "#2451", "issue_summary": "Does the Item Pipeline drops Items if scrapy runs at 99% CPU? I'm experiencing Scrapinghub 10-20 times difference on the item count vs running it on my own vps.", "issue_description": "Contributor\nIAlwaysBeCoding commented on Dec 15, 2016 \u2022\nedited\nI don't know if this is the right place to post something like this, but this is driving me nuts.\nI'm testing out different cloud providers where I am planning on running a Scrapy cluster(with proxies) I'm building, and something has been pulling my hair trying to understand what is going on. I feel like I'm going in circles.\nThe main points that I think are contributing to this weird bug is that: I have 4-7 depth request level, and that the tiny boxes am running Scrapy are maxing out at 99% CPU.\nHere is the code:\n# -*- coding: utf-8 -*-\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf8')\n\n\nimport datetime\nimport json\nimport urllib\nimport re\nimport time\n\nfrom scrapy import Spider\nfrom scrapy.http import Request\nfrom scrapy.selector import Selector\nfrom scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import TakeFirst, MapCompose, Join, SelectJmes\n\nfrom novoaurum.items import ProductInfo\n\n\nclass ProductInfo(Item):\n\n    title = Field()\n    brand = Field()\n    product_url = Field()\n    page_url = Field()\n    manufacturer = Field()\n    manufacturer_number = Field()\n    upc = Field()\n    sku = Field()\n    list_price = Field()\n    retail_price = Field()\n    is_out_of_stock = Field()\n    category = Field()\n    items_count = Field()\n    break_down_price = Field()\n    deal_title = Field()\n    is_clearance = Field()\n    scraped_date = Field()\n\nclass NoneEmpty(object):\n    def __call__(self,values):\n        if values is None:\n            return ''\n        elif isinstance(values, (list, tuple)):\n            if len(values) == 1 and values[0] == '':\n                return ''\n        return TakeFirst()(values)\n\ndef strip_spaces(raw_str):\n    if isinstance(raw_str, (unicode, str)):\n        clean = raw_str.strip()\n        return clean.replace('\\n', ' ').replace('\\t','')\n    else:\n        return raw_str\n\n\ndef default_missing_keys(item, default_value, except_keys=[]):\n\n    missing_keys = list(set(item.fields.keys()) - set(item.keys()))\n    for missing_key in missing_keys:\n        if except_keys:\n            if missing_key not in except_keys:\n                item[missing_key] = default_value\n        else:\n            item[missing_key] = default_value\n\nclass Toysrus(Spider):\n    name = \"toysrus\"\n    allowed_domains = [\"toysrus.com\"]\n    start_urls = (\n        'http://www.toysrus.com/products/tru-index.jsp',\n    )\n\n\n    def __init__(self, *args, **kwargs):\n        self.mode = kwargs.get('mode', 'normal')\n        self.jmesquery = kwargs.get('jmesquery',\n        \"deals[?contains(title, 'buy') || contains(title, 'free')].[url, title]\")\n        self.upcs = []\n\n    def parse(self, response):\n\n        if self.mode == 'normal':\n            brand_urls = response.xpath('//td[@class=\"idxColumn\" and @style=\"padding-top: 0px\"][1]//a/@href').extract()\n\n            for brand_url in brand_urls:\n                yield Request(response.urljoin(brand_url),\n                            callback=self.parse_browse_page)\n\n        elif self.mode == 'todaysdeals':\n            yield Request('http://www.toysrus.com/shop/index.jsp?categoryId=3395098#viewalldeals',\n                          callback=self.parse_todaysdeals)\n\n        elif self.mode == 'clearance':\n            yield Request('http://www.toysrus.com/family/index.jsp?categoryId=13131514',\n                          meta={\n                              'is_clearance' : True\n                          },\n                          callback=self.parse_browse_page)\n\n    def browse_page(self, response):\n\n        items_count = response.meta.get('items_count', '')\n        brand = response.meta.get('brand', '')\n        break_down_price = response.meta.get('break_down_price', '')\n        deal_title = response.meta.get('deal_title', '')\n        is_clearance = response.meta.get('is_clearance', False)\n\n        for p in response.xpath('//div[@class=\"prodloop-thumbnail\"]/a/@href').extract():\n\n            yield Request(response.urljoin(p),\n                            meta={\n                                'page_url' : response.url,\n                                'break_down_price' : break_down_price,\n                                'items_count' : items_count,\n                                'deal_title': deal_title,\n                                'is_clearance': is_clearance,\n                                'brand' : brand\n                            },\n                            callback=self.parse_product_page)\n\n    def parse_todaysdeals(self, response):\n\n        found_json_url = response.meta.get('found_json_url', False)\n\n        def extract_todays_deals(html):\n            find_json_url = re.search(r\"(?<=jsonURL = ')(.*?)(?=')\", html)\n            if find_json_url:\n                return find_json_url.group(0)\n\n        if not found_json_url:\n            url = extract_todays_deals(response.body)\n\n            if url:\n                yield Request(url,\n                              meta={'found_json_url' : True},\n                              callback=self.parse_todaysdeals)\n        else:\n            if self.jmesquery:\n                get_deals = SelectJmes(self.jmesquery)\n                for deal in get_deals(json.loads(response.body)):\n                    yield Request(\n                        response.urljoin(deal[0].replace('&amp;','&')),\n                        meta={\n                            'deal_title' : deal[1]\n                        },\n                        callback=self.parse_browse_page)\n\n    def parse_browse_page_less_500(self, response):\n        category_id = response.xpath('//input[@name=\"categoryId\"]/@value').extract_first()\n        brand = response.xpath('//h1[@id=\"TRUFamilyBrandTitle\"]/text()').extract_first()\n        deal_title = response.meta.get('deal_title', '')\n        total_count = response.meta.get('total_count', '')\n        is_clearance = response.meta.get('is_clearance', False)\n\n        query = {\n                's' : 'A-UnitRank',\n                'searchSort' : 'TRUE',\n                'categoryId' : category_id,\n                'ppg' : 500\n        }\n\n        url = '{}?{}'.format('http://www.toysrus.com/family/index.jsp',urllib.urlencode(query))\n        yield Request(url,\n                        meta={\n                            'page_url' : response.url,\n                            'break_down_price' : '',\n                            'items_count' : total_count,\n                            'deal_title': deal_title,\n                            'is_clearance': is_clearance,\n                            'brand' : brand,\n                        },\n                        callback=self.browse_page)\n\n    def parse_browse_page_break_down_prices(self, response):\n        break_down_prices = response.xpath('//div[@id=\"module_Price\"]//*[@class=\"filter_multiselectAttrib\"]').extract()\n        brand = response.meta.get('brand', '')\n        deal_title = response.meta.get('deal_title', '')\n        is_clearance = response.meta.get('is_clearance', False)\n\n        for b in break_down_prices:\n            div_sel = Selector(text=b)\n            url = div_sel.xpath('//a[not(contains(text(),\"more...\"))]/@href').extract_first()\n            url = '{}ppg=500'.format(response.urljoin(url[2:]))\n            b_text = div_sel.xpath('//a[not(contains(text(),\"more...\"))]/text()').extract_first()\n            items_count = div_sel.xpath('//span[@class=\"count\"]/text()').extract_first()\n            if items_count:\n                items_count = items_count.replace(')','').replace('(','')\n\n            yield Request(url,\n                            meta={\n                                'break_down_price' : b_text,\n                                'items_count' : items_count,\n                                'deal_title': deal_title,\n                                'is_clearance': is_clearance,\n                                'brand' : brand\n                            },\n                            callback=self.browse_page)\n\n    def parse_browse_page_featured_categories(self, response):\n        featured_categories = response.xpath('//div[@id=\"featured-category\"]//a[@class=\"featured-category-link\"]/@href').extract()\n\n        for featured_category in featured_categories:\n            yield Request(response.urljoin(featured_category),\n                          callback=self.parse_browse_page)\n\n    def parse_browse_page(self, response):\n\n        deal_title = response.meta.get('deal_title', '')\n        is_clearance = response.meta.get('is_clearance', False)\n        has_featured_categories = response.xpath('//div[@id=\"featured-category\"]')\n        brand = response.xpath('//h1[@id=\"TRUFamilyBrandTitle\"]/text()').extract_first()\n        total_results = response.xpath('//div[@class=\"showingText\"]/text()').extract_first()\n\n        if total_results:\n            get_total_results = re.search(r'\\s*(?<=of)\\s*(.*?)\\s*(?=results)', total_results.strip())\n\n            if get_total_results:\n\n                total_count = int(get_total_results.group(0))\n\n                if total_count >= 499:\n                    response.meta['brand'] = brand\n                    for item_or_request in self.parse_browse_page_break_down_prices(response):\n                        yield item_or_request\n                else:\n                    response.meta['total_count'] = total_count\n                    for item_or_request in self.parse_browse_page_less_500(response):\n                        yield item_or_request\n\n        elif has_featured_categories:\n            for item_or_request in self.parse_browse_page_featured_categories(response):\n                yield item_or_request\n\n    def parse_product_page(self, response):\n\n        page_url = response.meta.get('page_url', None)\n        break_down_price = response.meta.get('break_down_price', '')\n        items_count = response.meta.get('items_count', None)\n        is_clearance = response.meta.get('is_clearance', False)\n        brand = response.meta.get('brand', '')\n        deal_title = response.meta.get('deal_title', '')\n        loader = ItemLoader(ProductInfo(), Selector(response))\n        loader.default_input_processor = MapCompose(strip_spaces)\n        loader.default_output_processor = NoneEmpty()\n        loader.add_xpath('title', '//div[@id=\"lTitle\"]/h1/text()')\n        loader.add_value('product_url', response.url)\n        loader.add_value('page_url', page_url)\n        loader.add_value('brand', brand)\n        loader.add_xpath('manufacturer','//div[@id=\"lTitle\"]//li[@class=\"first\"]/h3/label/text()')\n        loader.add_xpath('manufacturer_number', '//div[@id=\"AddnInfo\"]//p[//label[contains(text(),\"Manufacturer\")]]/text()')\n        loader.add_xpath('upc', '//div[@id=\"AddnInfo\"]//p[@class=\"upc\" or @class=\"upc hidden\"]//span/text()')\n        loader.add_xpath('sku', '//div[@id=\"AddnInfo\"]//p[@class=\"skuText\" or @class=\"skuText hidden\"]//span/text()')\n        loader.add_xpath('retail_price', '//div[@id=\"price\"]//li[@class=\"retail fl \"]//span/text()', Join(''))\n        loader.add_xpath('retail_price', '//div[@id=\"price\"]//li[@class=\"retail fl withLP\"]//span/text()', Join(''))\n        loader.add_xpath('list_price', '//div[@id=\"price\"]//li[@class=\"list fl\"]/span/text()')\n        loader.add_xpath('list_price', '//div[@id=\"price\"]//li[@class=\"list\"]/span/text()')\n        loader.add_xpath('is_out_of_stock', '//div[@id=\"productOOS\"]')\n        loader.add_xpath('category','//div[@id=\"breadCrumbs\"]/a[@class=\"breadcrumb\"]/text()', TakeN(1))\n        loader.add_value('items_count', items_count)\n        loader.add_value('break_down_price', break_down_price)\n        loader.add_value('deal_title', deal_title)\n        loader.add_value('is_clearance', is_clearance)\n        loader.add_value(\n            'scraped_date',\n            datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n        )\n\n        item = loader.load_item()\n\n        if 'is_out_of_stock' in item:\n            item['is_out_of_stock'] = 'out of stock'\n        else:\n            item['is_out_of_stock'] = 'in stock'\n\n        default_missing_keys(item=item, default_value='')\n\n        if 'upc' in item:\n            if item['upc'] not in self.upcs:\n                self.upcs.append(item['upc'])\n                yield item\nI tested running this same spider code 2 different cloud providers:\nScaleway dedicated 2.99 euro a month a 2gb and 4 core but ARM based, and DigitalOcean 512mb x86_64 droplet.\nI'm getting vastly different results of items scraped compared if I ran the same scraper on ScrapingHub.\nScaleway gives me around 300-900ish items from doing 24k requests, DigitalOcean gives me between 9k-15k while Scrapinghub gives me 15kish.\nHere is the catch, all of them have 10-20+/- of the same requests count running the same scraper on mode=\"normal\" so basically just a simple scrapy crawl toysrus.\nThe bug that I have noticed is that once it gets up to 99% CPU , there will be NO MORE ITEMS scraped even if its at 20% of the current scrape, even though all of the requests will actually get scraped and yielded as well.\nThe main difference between this spider and the others I have built on the past is the request depth. I can't figure out why Scrapy is literally stopping processing items after 10-20% of the scrape. It LITERALLY STOPS. I have logged into the telnet console and watched with my own eyes every 15 seconds the counts on stats._stats and it scrapes all of the items at the beginning and then it stops around 10-20% of the current scrape. As soon as the cpu is pegged at 99%, Scrapy will start having this weird effect of basically dropping everything on the pipeline. This is a huge bug, that should be documented. Also, my scraper produces absolutely 0 exceptions, and I've gone through the nohup.out log to see anything weird and all I can see is that the items stop appearing after 10-20% of the scrape. It just shows me a bunch of product urls that it went too, but no items are shown.\nThe count difference makes me wonder that Scrapy might have a huge bug when pushed to its limits.\nI get noticeable less items scraped on Scaleway 2.99 dedicated server than DigitalOcean if I run it with this command:scrapy crawl toysrus -o items.csv -t csv, as in like 30-60% less items.\nSomething is going on, and I think something inside Scrapy is dropping items, because I'm using absolutely no custom middlewares, pipelines just the defaults.", "issue_status": "Closed", "issue_reporting_time": "2016-12-15T05:13:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "738": {"issue_url": "https://github.com/scrapy/scrapy/issues/2446", "issue_id": "#2446", "issue_summary": "Downloader Middleware", "issue_description": "Talefairy commented on Dec 13, 2016\nHi\uff1a\n\u6211\u5728\u4f7f\u7528scrapy1.2.2\u7248\u672c\u65f6\uff0c\u6ca1\u6709\u627e\u5230\u8fd9\u4e2ascrapy.downloadermiddlewares.DownloaderMiddleware\u662f\u600e\u4e48\u56de\u4e8b\uff1f", "issue_status": "Closed", "issue_reporting_time": "2016-12-13T07:29:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "739": {"issue_url": "https://github.com/scrapy/scrapy/issues/2445", "issue_id": "#2445", "issue_summary": "install error", "issue_description": "ketankr9 commented on Dec 12, 2016\nThe error given out when runnung sudo pip install scrapy on ubuntu16.04\nCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-rfOc5T/cryptography/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-GxhQ75-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-rfOc5T/cryptography/", "issue_status": "Closed", "issue_reporting_time": "2016-12-12T12:55:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "740": {"issue_url": "https://github.com/scrapy/scrapy/issues/2441", "issue_id": "#2441", "issue_summary": "ERROR : Scrapy 1.2.2 requires Python 2.7", "issue_description": "dhermanus commented on Dec 12, 2016\nHi All, After some tinkering on my Centos VPN I've finally got scrapy to be installed.\nHowever when I tried to run scrapy on the console. It says\nScrapy 1.2.2 requires Python 2.7\nHowever you can see below that I've installed Python 2,7 already and I've added alias to my\nbashrc file to use Python 2,7 as the default version for python.\nWhy am I still getting that error ?\nPlease help... Thanks.\n[root@vnode01 dhermanus]# python -V\nPython 2.7.8\n[root@vnode01 dhermanus]# scrapy\nScrapy 1.2.2 requires Python 2.7\n[root@vnode01 dhermanus]#", "issue_status": "Closed", "issue_reporting_time": "2016-12-11T21:01:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "741": {"issue_url": "https://github.com/scrapy/scrapy/issues/2440", "issue_id": "#2440", "issue_summary": "Why I get the wrong source code of the page ?", "issue_description": "woshichuanqilz commented on Dec 11, 2016 \u2022\nedited\nI use the scrapy shell http://sanguoshaenglish.blogspot.com/2010/07/liu-bei.html to debug the scrapy\nbut when I use the command response.body to output the content the result I get is this. Obviously it's wrong. Why's that?\n'<!doctype html><html lang=\"zh\"><head><title>\\xe5\\xa4\\x9a\\xe6\\x80\\x81 | \\xe8\\xaf\\xa6\\xe7\\xbb\\x86\\xe6\\xb5\\x81\\xe9\\x87\\x8f\\xe7\\xbb\\x9f\\xe8\\xae\\xa1</title><meta charset=\"utf-8\"><meta name=\"description\" content=\"\\xe5\\xa4\\x9a\\xe6\\x80\\x81\\xef\\xbc\\x88duotai.org\\xef\\xbc\\x89\\xe6\\x98\\xaf\\xe5\\x9f\\xba\\xe4\\xba\\x8e HTTP \\xe4\\xbb\\xa3\\xe7\\x90\\x86\\xe7\\x9a\\x84\\xe8\\xb7\\xa8\\xe5\\x9f\\x9f\\xe7\\xbd\\x91\\xe7\\xbb\\x9c\\xe5\\x8a\\xa0\\xe9\\x80\\x9f\\xe6\\x9c\\x8d\\xe5\\x8a\\xa1\\xe3\\x80\\x82\"><meta name=\"viewport\" content=\"\"><meta name=\"renderer\" content=\"webkit\"><meta http-equiv=\"mobile-agent\" content=\"format=html5\"><link rel=\"shortcut icon\" href=\"/favicon.ico\"></head><body><div id=\"root\"></div><script type=\"text/javascript\" src=\"/static/vendor.6a7fbcebf016c04cea05.js?6a7fbcebf016c04cea05\"></script><script type=\"text/javascript\" src=\"/static/app.6a7fbcebf016c04cea05.js?6a7fbcebf016c04cea05\"></script></body></html>'\nThe page is like this", "issue_status": "Closed", "issue_reporting_time": "2016-12-11T14:30:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "742": {"issue_url": "https://github.com/scrapy/scrapy/issues/2439", "issue_id": "#2439", "issue_summary": "How to use the -a option to pass a parameter to scrapy?", "issue_description": "woshichuanqilz commented on Dec 11, 2016\nI read the doc and find the command line should be like this.\nscrapy runspider getspecificimg.py -a ip='lizhe'\nAnd my spider code is like this :\nclass GetImage(scrapy.Spider):\n    name = 'ImageSpider'\n    start_urls = ['https://www.pexels.com/']\n\n# Get the input argument\n    # NameNeedSearch = InputPara\n    NameNeedSearch = ip\nBut the result I get means that the ip isn't defined why?", "issue_status": "Closed", "issue_reporting_time": "2016-12-11T08:27:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "743": {"issue_url": "https://github.com/scrapy/scrapy/issues/2438", "issue_id": "#2438", "issue_summary": "Yet another ssl problem", "issue_description": "yunmanger1 commented on Dec 11, 2016\nCan't parse site. While everything works within browsers and curl.\nscrapy fetch --pdb https://lsm.kz/rss\nDEBUG 2016-12-11 12:22:00,799 base 7005 140536583235328 Configuring Raven for host: <raven.conf.remote.RemoteConfig object at 0x47e3110>\nINFO 2016-12-11 12:22:01,714 logstats 7005 140536583235328 Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-12-11 12:22:01 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\nDEBUG 2016-12-11 12:22:01,718 telnet 7005 140536583235328 Telnet console listening on 127.0.0.1:6023\nDEBUG 2016-12-11 12:22:02,276 retry 7005 140536583235328 Retrying <GET https://lsm.kz/rss> (failed 1 times): 500 Internal Server Error\nDEBUG 2016-12-11 12:22:02,386 retry 7005 140536583235328 Retrying <GET https://lsm.kz/rss> (failed 2 times): 500 Internal Server Error\nDEBUG 2016-12-11 12:22:02,490 retry 7005 140536583235328 Gave up retrying <GET https://lsm.kz/rss> (failed 3 times): 500 Internal Server Error\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<rss version=\"2.0\">\n  <channel>\n    <title>LS \u2014 \u0424\u0418\u041d\u0410\u041d\u0421\u041e\u0412\u042b\u0419 \u0416\u0423\u0420\u041d\u0410\u041b</title>\n    <link>https://lsm.kz/</link>\n    <description>\u041d\u0430\u0448\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0432\u0441\u0435\u043c \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u0430\u043c \u0440\u044b\u043d\u043a\u0430 \u0432\u044b\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0441\u0432\u043e\u0435 \u043c\u043d\u0435\u043d\u0438\u0435 \u043f\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430\u043c, \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u044f\u0449\u0438\u043c, \u043a\u0430\u043a \u0432 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0435, \u0442\u0430\u043a \u0438 \u043d\u0430 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u043e\u043c \u0440\u044b\u043d\u043a\u0435.</description>\n    <language>ru-RU</language>\n    <copyright>(c) \u0412\u0441\u0435 \u043f\u0440\u0430\u0432\u0430 \u0437\u0430\u0449\u0438\u0449\u0435\u043d\u044b - LS \u2014 \u0424\u0418\u041d\u0410\u041d\u0421\u041e\u0412\u042b\u0419 \u0416\u0423\u0420\u041d\u0410\u041b</copyright>\n    <pubDate>Sun, 11 Dec 2016 06:22:02 +0000</pubDate>\n    <lastBuildDate>Sun, 11 Dec 2016 06:22:02 +0000</lastBuildDate>\n    <ttl>60</ttl>\n    <item>\n      <title>\u041d\u0435\u0444\u0442\u0435\u0434\u043e\u0431\u044b\u0432\u0430\u044e\u0449\u0438\u0435 \u0441\u0442\u0440\u0430\u043d\u044b \u0441\u043e\u0433\u043b\u0430\u0441\u0438\u043b\u0438\u0441\u044c \u0441\u043d\u0438\u0437\u0438\u0442\u044c \u0434\u043e\u0431\u044b\u0447\u0443 \u0441\u044b\u0440\u044c\u044f</title>\n      <link>https://lsm.kz/neft-opek-ne-opek</link>\n      <description>\u0412 \u0412\u0435\u043d\u0435 \u0437\u0430\u0432\u0435\u0440\u0448\u0438\u043b\u0430\u0441\u044c \u0432\u0441\u0442\u0440\u0435\u0447\u0430 \u041e\u041f\u0415\u041a</description>\n      <guid isPermaLink=\"true\">https://lsm.kz/neft-opek-ne-opek</guid>\n      <pubDate>Sat, 10 Dec 2016 15:45:00 +0000</pubDate>\n    </item>\n    <item>\n      <title>\u041a\u0430\u043a \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u0441\u044f \u0434\u043e\u0431\u044b\u0447\u0430 \u043d\u0435\u0444\u0442\u0438 \u0432 2017 \u0433\u043e\u0434\u0443. \u0418\u043d\u0444\u043e\u0433\u0440\u0430\u0444\u0438\u043a\u0430</title>\n      <link>https://lsm.kz/naskol-ko-opek-i-ne-opek-snizyat-dobychu-nefti-v-2017-godu-infografika</link>\n      <description>\u0421\u0442\u0440\u0430\u043d\u044b \u041e\u041f\u0415\u041a \u0434\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u043b\u0438\u0441\u044c \u043e \u0441\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u0438 \u0434\u043e\u0431\u044b\u0447\u0438 \u043d\u0430 1,2 \u043c\u043b\u043d \u0431\u0430\u0440\u0440\u0435\u043b\u0435\u0439 \u0432 \u0434\u0435\u043d\u044c \u0438 \u043f\u043e\u043f\u0440\u043e\u0441\u0438\u043b\u0438 \u0443 \u0434\u0440\u0443\u0433\u0438\u0445 \u0441\u0442\u0440\u0430\u043d \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0442\u044c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043d\u0430 600 \u0442\u044b\u0441. \u0431\u0430\u0440\u0440\u0435\u043b\u0435\u0439 \u0432 \u0434\u0435\u043d\u044c</description>\n      <guid isPermaLink=\"true\">https://lsm.kz/naskol-ko-opek-i-ne-opek-snizyat-dobychu-nefti-v-2017-godu-infografika</guid>\n      <pubDate>Sat, 10 Dec 2016 11:01:00 +0000</pubDate>\n    </item>\n    <item>\n      <title>\u041f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442 \u0422\u0443\u0440\u0446\u0438\u0438 \u043f\u043e\u0441\u0435\u0442\u0438\u0442 \u041a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d</title>\n      <link>https://lsm.kz/prezident-turcii-priedet-s-vizitom-v-kazahstan</link>\n      <description>\u0420\u0435\u0434\u0436\u0435\u043f \u0422\u0430\u0439\u0438\u043f \u042d\u0440\u0434\u043e\u0433\u0430\u043d \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u0442 \u0432\u0441\u0442\u0440\u0435\u0447\u0443 \u0441 \u041d\u0430\u0437\u0430\u0440\u0431\u0430\u0435\u0432\u044b\u043c \u043e\u0434\u0438\u043d \u043d\u0430 \u043e\u0434\u0438\u043d \u0438 \u0432 \u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u043d\u043e\u043c \u0441\u043e\u0441\u0442\u0430\u0432\u0435</description>\n      <guid isPermaLink=\"true\">https://lsm.kz/prezident-turcii-priedet-s-vizitom-v-kazahstan</guid>\n      <pubDate>Sat, 10 Dec 2016 10:16:00 +0000</pubDate>\n    </item>\n  </channel>\n</rss>\n\nINFO 2016-12-11 12:22:02,598 statscollectors 7005 140536583235328 Dumping Scrapy stats:\n{'downloader/request_bytes': 567,\n 'downloader/request_count': 3,\n 'downloader/request_method_count/GET': 3,\n 'downloader/response_bytes': 7875,\n 'downloader/response_count': 3,\n 'downloader/response_status_count/500': 3,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 12, 11, 6, 22, 2, 597473),\n 'log_count/INFO': 1,\n 'response_received_count': 1,\n 'scheduler/dequeued': 3,\n 'scheduler/dequeued/memory': 3,\n 'scheduler/enqueued': 3,\n 'scheduler/enqueued/memory': 3,\n 'start_time': datetime.datetime(2016, 12, 11, 6, 22, 1, 715674)}\n2016-12-11 12:22:02 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 567,\n 'downloader/request_count': 3,\n 'downloader/request_method_count/GET': 3,\n 'downloader/response_bytes': 7875,\n 'downloader/response_count': 3,\n 'downloader/response_status_count/500': 3,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 12, 11, 6, 22, 2, 597473),\n 'log_count/INFO': 1,\n 'response_received_count': 1,\n 'scheduler/dequeued': 3,\n 'scheduler/dequeued/memory': 3,\n 'scheduler/enqueued': 3,\n 'scheduler/enqueued/memory': 3,\n 'start_time': datetime.datetime(2016, 12, 11, 6, 22, 1, 715674)}\nJumping into debugger for post-mortem of exception '(-1, 'Unexpected EOF')':\n> /var/www/.virtualenvs/news/local/lib/python2.7/site-packages/OpenSSL/SSL.py(1167)_raise_ssl_error()\n-> raise SysCallError(-1, \"Unexpected EOF\")\n(Pdb) \nMy versions:\nscrapy version -v\nDEBUG 2016-12-11 12:22:34,346 base 7035 140565107422976 Configuring Raven for host: <raven.conf.remote.RemoteConfig object at 0x3a1a110>\nScrapy    : 1.2.2\nlxml      : 3.7.0.0\nlibxml2   : 2.7.8\ncssselect : 1.0.0\nparsel    : 1.1.0\nw3lib     : 1.16.0\nTwisted   : 16.6.0\nPython    : 2.7.3 (default, Jun 22 2015, 19:33:41) - [GCC 4.6.3]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.1 14 Mar 2012)\nPlatform  : Linux-3.13.0-32-generic-x86_64-with-Ubuntu-12.04-precise", "issue_status": "Closed", "issue_reporting_time": "2016-12-11T06:24:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "744": {"issue_url": "https://github.com/scrapy/scrapy/issues/2431", "issue_id": "#2431", "issue_summary": "Scrapy gives HTML response different from requests.get", "issue_description": "fjdu commented on Dec 8, 2016 \u2022\nedited\nscrapy version: 1.1.2\npython version: 2.7.12\nplatform: Mac OS X 10.11.6\nThe issue:\nFor the url given in the following minimum working example, the HTML text in the response from scrapy is different from the one obtained with requests.get. The latter seems to be the correct one. It seems scrapy somehow duplicates part of the response html. This does not happen for all sites.\nSee the attached file for the two different html files. Or you may run the following code to generate them.\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom twisted.internet import reactor\nimport requests\n\nurl = 'http://training.sac.net.cn/cms/flkcalone.htm?myId=4028d0ee57ec28180157f55059b87209'\n\nheaders = {\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n    'Accept-Encoding': 'gzip, deflate, sdch',\n    'Accept-Language': 'en-US,en;q=0.8,zh-CN;q=0.6,zh;q=0.4,zh-TW;q=0.2',\n    'Cache-Control': 'max-age=0',\n    'Connection': 'keep-alive',\n    'Upgrade-Insecure-Requests': '1',\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36',\n    }\n\n\nclass Test(scrapy.Spider):\n    name = 'test'\n\n    def start_requests(self):\n        request = scrapy.Request(url=url, callback=self.parse, headers=headers)\n        yield request\n\n    def parse(self, response):\n        with open('response_with_scrapy.html', 'w') as f:\n            f.write(response.text.encode('utf-8'))\n\n\nif __name__ == '__main__':\n    with open('response_with_requests.html', 'w') as f:\n        f.write(requests.get(url, headers=headers).text.encode('utf-8'))\n\n    runner = CrawlerRunner()\n    runner.crawl(Test)\n    d = runner.join()\n    d.addBoth(lambda _: reactor.stop())\n    reactor.run()\ntwo_responses.zip", "issue_status": "Closed", "issue_reporting_time": "2016-12-08T04:48:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "745": {"issue_url": "https://github.com/scrapy/scrapy/issues/2430", "issue_id": "#2430", "issue_summary": "`scrapy version` fails if `SPIDER_MODULES` points to non-existent module", "issue_description": "nathanielford commented on Dec 8, 2016 \u2022\nedited by redapple\nWhen executing scrapy version on the command line inside a scrapy project, an ImportError: No module named 'spiders' can occur if the settings.py file that is found by scrapy contains, in this case SPIDER_MODULES = ['spiders'] and there is no spiders module in the project.\nEssentially, any execution of scrapy picks up the settings file, which will report a rather generic error (googling for it will return a number of unrelated issues) if there is an issue in that file.\nI created a StackOverflow question about this that can be found here with a bit more detail:\nhttp://stackoverflow.com/questions/41028605/importerror-no-module-named-spiders/41028628?noredirect=1#comment69264795_41028628\nSample traceback:\n$ scrapy version\nTraceback (most recent call last):\n  File \"/Users/nathanielford/virtualenvironments/crawler/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/site-packages/scrapy/cmdline.py\", line 141, in execute\n    cmd.crawler_process = CrawlerProcess(settings)\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/site-packages/scrapy/crawler.py\", line 238, in __init__\n    super(CrawlerProcess, self).__init__(settings)\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/site-packages/scrapy/crawler.py\", line 129, in __init__\n    self.spider_loader = _get_spider_loader(settings)\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/site-packages/scrapy/crawler.py\", line 325, in _get_spider_loader\n    return loader_cls.from_settings(settings.frozencopy())\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/site-packages/scrapy/spiderloader.py\", line 33, in from_settings\n    return cls(settings)\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/site-packages/scrapy/spiderloader.py\", line 20, in __init__\n    self._load_all_spiders()\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/site-packages/scrapy/spiderloader.py\", line 28, in _load_all_spiders\n    for module in walk_modules(name):\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/site-packages/scrapy/utils/misc.py\", line 63, in walk_modules\n    mod = import_module(path)\n  File \"/Users/nathanielford/virtualenvironments/crawler/lib/python3.5/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 956, in _find_and_load_unlocked\nImportError: No module named 'spiders'\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-12-07T23:19:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "746": {"issue_url": "https://github.com/scrapy/scrapy/issues/2426", "issue_id": "#2426", "issue_summary": "runspider failed on ubuntu", "issue_description": "kingflight commented on Dec 4, 2016 \u2022\nedited\nI followed the tutorial to make a simple spider on ubuntu and when I ran scrapy runspider quotes_spider.py -o quotes.json I got this error message:\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 9, in <module>\n    from scrapy.crawler import CrawlerProcess\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 7, in <module>\n    from twisted.internet import reactor, defer\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/reactor.py\", line 38, in <module>\n    from twisted.internet import default\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/default.py\", line 56, in <module>\n    install = _getInstallFunction(platform)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/default.py\", line 44, in _getInstallFunction\n    from twisted.internet.epollreactor import install\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/epollreactor.py\", line 24, in <module>\n    from twisted.internet import posixbase\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/posixbase.py\", line 18, in <module>\n    from twisted.internet import error, udp, tcp\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/tcp.py\", line 28, in <module>\n    from twisted.internet._newtls import (\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/_newtls.py\", line 21, in <module>\n    from twisted.protocols.tls import TLSMemoryBIOFactory, TLSMemoryBIOProtocol\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/protocols/tls.py\", line 65, in <module>\n    from twisted.internet._sslverify import _setAcceptableProtocols\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/_sslverify.py\", line 204, in <module>\n    verifyHostname, VerificationError = _selectVerifyImplementation(OpenSSL)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/_sslverify.py\", line 179, in _selectVerifyImplementation\n    from service_identity import VerificationError\n  File \"/usr/local/lib/python2.7/dist-packages/service_identity/__init__.py\", line 7, in <module>\n    from . import pyopenssl\n  File \"/usr/local/lib/python2.7/dist-packages/service_identity/pyopenssl.py\", line 14, in <module>\n    from .exceptions import SubjectAltNameWarning\n  File \"/usr/local/lib/python2.7/dist-packages/service_identity/exceptions.py\", line 21, in <module>\n    @attr.s\nAttributeError: 'module' object has no attribute 's'\nTwisted and service_identity are installed by pip install twisted and pip install service_identity.\nPython verison is 2.7.6", "issue_status": "Closed", "issue_reporting_time": "2016-12-04T12:03:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "747": {"issue_url": "https://github.com/scrapy/scrapy/issues/2425", "issue_id": "#2425", "issue_summary": "scrapy is not working with Single Page Web App like AngularJs and React", "issue_description": "rahullsinha commented on Dec 3, 2016 \u2022\nedited\ni am trying to scrape data from web app , but scrappy is not working. when i opened website in scrappy shell and i used \"view(response)\" command , but website is not loading contents.", "issue_status": "Closed", "issue_reporting_time": "2016-12-03T16:46:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "748": {"issue_url": "https://github.com/scrapy/scrapy/issues/2424", "issue_id": "#2424", "issue_summary": "SSL handshake failure", "issue_description": "briehanlombaard commented on Dec 3, 2016\nHi,\nI'm getting a handshake error for the sites listed below:\n2016-12-03 00:02:19 [scrapy] ERROR: Error downloading <GET https://apnews.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-12-03 00:03:25 [scrapy] ERROR: Error downloading <GET https://techcrunch.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-12-03 00:03:53 [scrapy] ERROR: Error downloading <GET https://medium.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]\n2016-12-03 00:05:08 [scrapy] ERROR: Error downloading <GET https://theintercept.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-12-03 00:06:32 [scrapy] ERROR: Error downloading <GET https://www.opendemocracy.net/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]\n2016-12-03 00:07:55 [scrapy] ERROR: Error downloading <GET https://www.rt.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-12-03 00:19:53 [scrapy] ERROR: Error downloading <GET https://www.thestar.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]\n2016-12-03 00:58:42 [scrapy] ERROR: Error downloading <GET https://www.cnet.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]\nWhat's strange is that it works if I try each one of those sites individually using scrapy shell so I might be doing something wrong.\nHere's some information about my environment:\n$ scrapy version -v\nScrapy    : 1.2.1\nlxml      : 3.6.4.0\nlibxml2   : 2.9.4\nTwisted   : 16.6.0\nPython    : 2.7.12 (default, Jul  1 2016, 15:12:24) - [GCC 5.4.0 20160609]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)\nPlatform  : Linux-3.13.0-52-generic-x86_64-with-Ubuntu-16.04-xenial\nAny ideas where I can look to troubleshoot the problem?", "issue_status": "Closed", "issue_reporting_time": "2016-12-03T14:11:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "749": {"issue_url": "https://github.com/scrapy/scrapy/issues/2420", "issue_id": "#2420", "issue_summary": "String value for order of Scrapy component", "issue_description": "vshlapakov commented on Dec 1, 2016\nIf Scrapy component order is defined as a string, it leads to undefined behaviour on Python 2 and to the following errors on Python 3:\nFile \"/usr/local/lib/python3.5/site-packages/scrapy/middleware.py\", line 58, in from_crawler\n return cls.from_settings(crawler.settings, crawler)\nFile \"/usr/local/lib/python3.5/site-packages/scrapy/middleware.py\", line 29, in from_settings\n mwlist = cls._get_mwlist_from_settings(settings)\nFile \"/usr/local/lib/python3.5/site-packages/scrapy/core/spidermw.py\", line 21, in _get_mwlist_from_settings\n return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\nFile \"/usr/local/lib/python3.5/site-packages/scrapy/utils/conf.py\", line 47, in build_component_list\n return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]\nbuiltins.TypeError: unorderable types: str() < int()\nMy guess that 1) order of a Scrapy component should be stated as of integer type (or None) and there should be a check somewhere, 2) or the sorting logic should be fixed.", "issue_status": "Closed", "issue_reporting_time": "2016-12-01T12:28:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "750": {"issue_url": "https://github.com/scrapy/scrapy/issues/2416", "issue_id": "#2416", "issue_summary": "Strange behavior when trying to override user agent", "issue_description": "MatBl commented on Nov 29, 2016\nMore info here: http://stackoverflow.com/questions/40848302/scrapy-overriding-user-agent-doesnt-work\nTo keep it short: If I try to change the user agent (via settings, command line, spider, even default settings, doesn't matter where), the parser stops outputting data. It only works if the user agent starts with \"Scrapy .....\". So:\nscrapy crawl myspider -o myspider.csv -t csv -s USER_AGENT=\"Scrapy Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\" -> works.\nscrapy crawl myspider -o myspider.csv -t csv -s USER_AGENT=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\" -> doesn't work.\nDoes anybody see the logic in this?\nScrapy : 1.2.1\nlxml : 3.6.4.0\nlibxml2 : 2.9.4\nTwisted : 16.5.0\nPython : 2.7.12 (default, Nov 19 2016, 06:48:10) - [GCC 5.4.0 20160609]\npyOpenSSL : 16.2.0 (OpenSSL 1.0.2g 1 Mar 2016)\nPlatform : Linux-4.4.0-45-generic-x86_64-with-Ubuntu-16.04-xenial", "issue_status": "Closed", "issue_reporting_time": "2016-11-29T06:11:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "751": {"issue_url": "https://github.com/scrapy/scrapy/issues/2414", "issue_id": "#2414", "issue_summary": "SCHEDULER settings not documented", "issue_description": "mohmad-null commented on Nov 26, 2016\nThe FAQ mentions:\nSCHEDULER_DISK_QUEUE and SCHEDULER_MEMORY_QUEUE with regards to changing between FIFO/LIFO.\nNeither of these appears to be documention except for this single reference. They're not on the settings page.\nAlso, the link to wikipedia for \"LIFO\" is actually to a disambiguation page, which doesn't help if you don't know what it is in the first place. :-) (I know it's Last In First Out, but others may not).", "issue_status": "Closed", "issue_reporting_time": "2016-11-25T21:39:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "752": {"issue_url": "https://github.com/scrapy/scrapy/issues/2413", "issue_id": "#2413", "issue_summary": "Chunked decoder in 'CHUNK_LENGTH' state, still expecting more data to get to 'FINISHED' state", "issue_description": "duangy commented on Nov 25, 2016\nTraceback (most recent call last):\nFile \"/home/env/lib/python3.4/site-packages/twisted/internet/defer.py\", line 1183, in _inlineCallbacks\nresult = result.throwExceptionIntoGenerator(g)\nFile \"/home/env/lib/python3.4/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\nreturn g.throw(self.type, self.value, self.tb)\nFile \"/home/env/lib/python3.4/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\ndefer.returnValue((yield download_func(request=request,spider=spider)))\ntwisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'CHUNK_LENGTH' state, still expecting more data to get to 'FINISHED' state.>]\nDid Anyone meet the same problem\uff0c and have any solution?\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-11-25T06:23:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "753": {"issue_url": "https://github.com/scrapy/scrapy/issues/2412", "issue_id": "#2412", "issue_summary": "ImportError: No module named web", "issue_description": "cmdbdu commented on Nov 25, 2016 \u2022\nedited\n$ scrapy --version\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 7, in\nfrom scrapy.cmdline import execute\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/init.py\", line 34, in\nfrom scrapy.spiders import Spider\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/spiders/init.py\", line 10, in\nfrom scrapy.http import Request\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/http/init.py\", line 11, in\nfrom scrapy.http.request.form import FormRequest\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/http/request/form.py\", line 14, in\nfrom scrapy.utils.response import get_base_url\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/response.py\", line 10, in\nfrom twisted.web import http\nImportError: No module named web\nhow \uff1f\nand the os info\n$ uname -a\nLinux ubuntu 3.13.0-24-generic #46-Ubuntu SMP Thu Apr 10 19:11:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\nPython 2.7.6\nUbuntu server 14.04", "issue_status": "Closed", "issue_reporting_time": "2016-11-25T04:26:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "754": {"issue_url": "https://github.com/scrapy/scrapy/issues/2408", "issue_id": "#2408", "issue_summary": "301 Moved Permanently", "issue_description": "liangpu597 commented on Nov 23, 2016\nFollow the doc I write this code with shell :\nscrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html response.selector.xpath('//title/text()')\nhowever:\nis suggested:\n[<Selector xpath='//title/text()' data=u'301 Moved Permanently'>]\nbut i can open :http://doc.scrapy.org/en/latest/_static/selectors-sample1.html\nwith my brower.\nwhy ?", "issue_status": "Closed", "issue_reporting_time": "2016-11-23T13:31:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "755": {"issue_url": "https://github.com/scrapy/scrapy/issues/2407", "issue_id": "#2407", "issue_summary": "SelectJmes fails to parse the right way due to arg_to_iter() turning the output into a list.", "issue_description": "Contributor\nIAlwaysBeCoding commented on Nov 23, 2016 \u2022\nedited\nHere is an example of what I'm talking about:\nimport json\nfrom scrapy import Item\nfrom scrapy.loader import ItemLoader\nfrom scrapy.loader.processors import SelectJmes\nstring = r'''{\n  \"type\": [\n    \"http://schema.org/Physician\"\n  ], \n  \"properties\": {\n    \"url\": [\n      \"http://www.vitals.com/doctors/Dr_Patrick_B_Wright.html\"\n    ], \n    \"aggregateRating\": [\n      {\n        \"type\": [\n          \"http://schema.org/AggregateRating\"\n        ], \n        \"properties\": {\n          \"worstRating\": [\n            \"0\"\n          ], \n          \"reviewCount\": [\n            \"1\"\n          ], \n          \"bestRating\": [\n            \"5\"\n          ], \n          \"ratingValue\": [\n            \"4.3\"\n          ], \n          \"ratingCount\": [\n            \"6\"\n          ]\n        }\n      }\n    ], \n    \"medicalspecialty\": [\n      {\n        \"type\": [\n          \"http://schema.org/MedicalSpecialty\"\n        ], \n        \"properties\": {\n          \"name\": [\n            \"Pediatric Surgery\"\n          ]\n        }\n      }\n    ], \n    \"name\": [\n      \"Dr. Patrick Wright MD\"\n    ], \n    \"address\": [\n      {\n        \"type\": [\n          \"http://schema.org/PostalAddress\"\n        ], \n        \"properties\": {\n          \"addressLocality\": [\n            \"Jackson\"\n          ], \n          \"addressRegion\": [\n            \"MS\"\n          ], \n          \"streetAddress\": [\n            \"2500 N State St\"\n          ], \n          \"postalCode\": [\n            \"39216\"\n          ], \n          \"telephone\": [\n            \"(601) 984-2150\"\n          ]\n        }\n      }\n    ]\n  }\n}'''\n\ndata = json.loads(string)\n\n#This will work fine\nproc = SelectJmes('properties.address[0].properties.postalCode[0]')\n\n\n#However, if its used in an input processor it will fail and will need to add `[0]` before the query in order to work.\n\nclass SomeItem(Item):\n     postal_code = Field(\n          input_processor = SelectJmes('properties.address[0].properties.postalCode[0]')\n     )\n\nloader = ItemLoader(SomeItem())\nloader.add_value('postal_code', data)\nitem = loader.load_item()\nThe only way to make it work will be to always add [0] to the JmesQuery when done inside an input or output field processor.\n#This will work\n[0].properties.address[0].properties.postalCode[0]\nShould we document this odd behaviour due to arg_to_iter doing this?", "issue_status": "Closed", "issue_reporting_time": "2016-11-23T11:42:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "756": {"issue_url": "https://github.com/scrapy/scrapy/issues/2406", "issue_id": "#2406", "issue_summary": "I don't know how to install this", "issue_description": "lksy0217 commented on Nov 23, 2016 \u2022\nedited by redapple\n    running build_ext\n    building 'lxml.etree' extension\n    error: Unable to find vcvarsall.bat\n\n    ----------------------------------------\nCommand \"c:\\users\\jongho\\appdata\\local\\programs\\python\\python35\\python.exe -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\JongHo\\\\AppData\\\\Local\\\\Temp\\\\pip-build-yh509eor\\\\lxml\\\\setup.py';f=g\netattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record C:\\Users\\JongHo\\AppData\\Local\\Temp\\pip-629uwerr-record\\in\nstall-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\JongHo\\AppData\\Local\\Temp\\pip-build-yh509eor\\lxml\\", "issue_status": "Closed", "issue_reporting_time": "2016-11-23T06:56:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "757": {"issue_url": "https://github.com/scrapy/scrapy/issues/2405", "issue_id": "#2405", "issue_summary": "CLOSESPIDER_ERRORCOUNT doesn't respect (count) errors in pipelines/middlewares", "issue_description": "jamb0ss commented on Nov 23, 2016 \u2022\nedited\nThis param (CLOSESPIDER_ERRORCOUNT) only works for exceptions in spider's callbacks, but doesn't work for item pipelines, downloader middlewares etc.\nCould you mention this (expected?) behavior in docs?\nAlso I'm wondering, what is a conventional way to stop spider on any exception in middlewares/pipelines?", "issue_status": "Closed", "issue_reporting_time": "2016-11-23T05:48:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "758": {"issue_url": "https://github.com/scrapy/scrapy/issues/2402", "issue_id": "#2402", "issue_summary": "[twisted] CRITICAL: Unhandled error in Deferred", "issue_description": "Lampere1021 commented on Nov 22, 2016 \u2022\nedited by redapple\n[root@VM_50_138_centos tutorial]#  scrapy crawl dmoz\n2016-11-22 17:08:19 [scrapy] INFO: Scrapy 1.2.1 started (bot: tutorial)\n2016-11-22 17:08:19 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'tutorial'}\n2016-11-22 17:08:19 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-11-22 17:08:19 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-11-22 17:08:19 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-11-22 17:08:19 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-11-22 17:08:19 [scrapy] INFO: Spider opened\nUnhandled error in Deferred:\n2016-11-22 17:08:19 [twisted] CRITICAL: Unhandled error in Deferred:\n\n2016-11-22 17:08:19 [twisted] CRITICAL:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/Twisted-16.6.0rc1-py2.7-linux-x86_64.egg/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\n    result = g.send(result)\n  File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.2.1-py2.7.egg/scrapy/crawler.py\", line 90, in crawl\n    six.reraise(*exc_info)\n  File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.2.1-py2.7.egg/scrapy/crawler.py\", line 74, in crawl\n    yield self.engine.open_spider(self.spider, start_requests)\nImportError: No module named _sqlite3", "issue_status": "Closed", "issue_reporting_time": "2016-11-22T09:09:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "759": {"issue_url": "https://github.com/scrapy/scrapy/issues/2397", "issue_id": "#2397", "issue_summary": "ImagesPipeline does not follow redirection", "issue_description": "dolohow commented on Nov 16, 2016\nI am using the following settings:\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 500,\n}\n\nITEM_PIPELINES = {\n    'scrapy.pipelines.images.ImagesPipeline': 1,\n}\nimage_urls contains a link that further redirect to a different url\nFile (code: 302): Error downloading file from <GET \nhttp://xxx.xxx/1023019-01.jpg> referred in <None>\nThe Location header as well as status code are present.", "issue_status": "Closed", "issue_reporting_time": "2016-11-16T13:52:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "760": {"issue_url": "https://github.com/scrapy/scrapy/issues/2390", "issue_id": "#2390", "issue_summary": "Sitemap spider not robust against wrong sitemap URLs in robots.txt", "issue_description": "Contributor\nredapple commented on Nov 9, 2016\nThe \"specs\" do say that the URL should be a \"full URL\":\nYou can specify the location of the Sitemap using a robots.txt file. To do this, simply add the following line including the full URL to the sitemap:\nSitemap: http://www.example.com/sitemap.xml\nBut some robots.txt use relative ones.\nExample: http://www.asos.com/robots.txt\nUser-agent: *\nSitemap: /sitemap.ashx\nSitemap: http://www.asos.com/sitemap.xml\nDisallow: /basket/\n(...)\nSpider:\nfrom scrapy.spiders import SitemapSpider\n\n\nclass TestSpider(SitemapSpider):\n    name = \"test\"\n    sitemap_urls = [\n        'http://www.asos.com/robots.txt',\n    ]\n\n    def parse(self, response):\n        self.logger.info('parsing %r' % response.url)\nLogs:\n$ scrapy runspider spider.py\nLinux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'\n2016-11-09 17:46:19 [scrapy] INFO: Scrapy 1.2.1 started (bot: scrapybot)\n(...)\n2016-11-09 17:46:19 [scrapy] DEBUG: Crawled (200) <GET http://www.asos.com/robots.txt> (referer: None)\n2016-11-09 17:46:19 [scrapy] ERROR: Spider error processing <GET http://www.asos.com/robots.txt> (referer: None)\nTraceback (most recent call last):\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spiders/sitemap.py\", line 36, in _parse_sitemap\n    yield Request(url, callback=self._parse_sitemap)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/http/request/__init__.py\", line 25, in __init__\n    self._set_url(url)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/http/request/__init__.py\", line 57, in _set_url\n    raise ValueError('Missing scheme in request url: %s' % self._url)\nValueError: Missing scheme in request url: /sitemap.ashx\n2016-11-09 17:46:19 [scrapy] INFO: Closing spider (finished)\n2016-11-09 17:46:19 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 291,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 1857,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 11, 9, 16, 46, 19, 332383),\n 'log_count/DEBUG': 2,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'spider_exceptions/ValueError': 1,\n 'start_time': datetime.datetime(2016, 11, 9, 16, 46, 19, 71714)}\n2016-11-09 17:46:19 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-11-09T16:58:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "761": {"issue_url": "https://github.com/scrapy/scrapy/issues/2389", "issue_id": "#2389", "issue_summary": "gzip-Content-Encoded sitemap.xml.gz is not parsed", "issue_description": "Contributor\nredapple commented on Nov 9, 2016\nFrom https://stackoverflow.com/questions/40499288/python-scrapy-sitemapspider-callbacks-not-being-called\nSitemap spider for http://www.newegg.com/Siteindex_USA.xml fails to parse sub-sitemaps files.\nExcerpt from sitemap file:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<sitemapindex xmlns=\"http://www.google.com/schemas/sitemap/0.9\">\n<sitemap>\n<loc>http://www.newegg.com/Sitemap/USA/newegg_sitemap_store01.xml.gz</loc>\n<lastmod>2016-11-06</lastmod>\n</sitemap>\n<sitemap>\n<loc>http://www.newegg.com/Sitemap/USA/newegg_sitemap_product01.xml.gz</loc>\n<lastmod>2016-11-06</lastmod>\n</sitemap>\n...\nExample spider:\nimport scrapy\n\n\nclass NeweggSpider(scrapy.spiders.SitemapSpider):\n    name = \"newegg\"\n    allowed_domains = [\"newegg.com\"]\n    sitemap_urls = ['http://www.newegg.com/Siteindex_USA.xml']\n\n    def parse(self, response):\n        self.logger.info('parsing %r' % response.url)\nWith scrapy 1.2, you get the following exceptions:\n$ scrapy runspider spider.py \n2016-11-09 15:53:10 [scrapy] INFO: Scrapy 1.2.1 started (bot: scrapybot)\n(...)\n2016-11-09 15:53:10 [scrapy] INFO: Spider opened\n2016-11-09 15:53:10 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-11-09 15:53:10 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-11-09 15:53:11 [scrapy] DEBUG: Crawled (200) <GET http://www.newegg.com/Siteindex_USA.xml> (referer: None)\n2016-11-09 15:53:11 [scrapy] DEBUG: Crawled (200) <GET http://www.newegg.com/Sitemap/USA/newegg_sitemap_store01.xml.gz> (referer: http://www.newegg.com/Siteindex_USA.xml)\n2016-11-09 15:53:11 [scrapy] ERROR: Spider error processing <GET http://www.newegg.com/Sitemap/USA/newegg_sitemap_store01.xml.gz> (referer: http://www.newegg.com/Siteindex_USA.xml)\nTraceback (most recent call last):\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/spiders/sitemap.py\", line 44, in _parse_sitemap\n    s = Sitemap(body)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/utils/sitemap.py\", line 17, in __init__\n    rt = self._root.tag\nAttributeError: 'NoneType' object has no attribute 'tag'\nThe root cause seem to be that the server sends .xml.gz files, and gzip-encodes them:\nGET /Sitemap/USA/newegg_sitemap_product29.xml.gz HTTP/1.1\nAccept-Language: en\nAccept-Encoding: gzip,deflate\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Scrapy/1.2.1 (+http://scrapy.org)\nHost: www.newegg.com\nReferer: http://www.newegg.com/Siteindex_USA.xml\n\nHTTP/1.1 200 OK\nContent-Type: application/x-gzip\nLast-Modified: Tue, 08 Nov 2016 18:44:56 GMT\nAccept-Ranges: bytes\nETag: \"471fb033f039d21:0\"\nServer: NEWEGG\nx-server-id: 106\nX-Served-By: 40015\nX-Ver: 08161601\nx-newegg-flow: MISS\nX-newegg-index: 0\nAccept-Ranges: bytes\nX-Frame-Options: SAMEORIGIN\nVary: Accept-Encoding\nContent-Encoding: gzip\nExpires: Wed, 09 Nov 2016 14:15:46 GMT\nCache-Control: max-age=0, no-cache, no-store\nPragma: no-cache\nDate: Wed, 09 Nov 2016 14:15:46 GMT\nTransfer-Encoding:  chunked\nConnection: keep-alive\nConnection: Transfer-Encoding\nSo scrapy should first gunzip the body to get that \"raw\" .xml.gz file, and then gunzip again to get a valid XML sitemap file.\nBut HttpCompressionMiddleware does not touch the response since is_gzipped() returns True, leaving it for subsequent layers to interpret.\n\u2764\ufe0f 1", "issue_status": "Closed", "issue_reporting_time": "2016-11-09T15:07:59Z", "fixed_by": "#2391", "pull_request_summary": "[MRG] Always decompress Content-Encoding: gzip at HttpCompression stage", "pull_request_description": "Contributor\nredapple commented on Nov 10, 2016\nLet SitemapSpider handle decoding of .xml.gz files if necessary.\nFixes #2389\nThe change here is to always decompress responses with Content-Encoding: gzip, whatever Content-Type says, and contrary to #193 (comment), #660, #2065\nTherefore, SitemapSpider still has to decode \"real\" .gz files/content if the HTTP response was not \"Content-Encoded\", relying on gzip magic number instead of response headers.\nI believe it follows RFC 7231 better:\nThe \"Content-Encoding\" header field indicates what content codings\nhave been applied to the representation, beyond those inherent in the\nmedia type, and thus what decoding mechanisms have to be applied in\norder to obtain data in the media type referenced by the Content-Type\nheader field. Content-Encoding is primarily used to allow a\nrepresentation's data to be compressed without losing the identity of\nits underlying media type.\n#951 (comment) also hinted at something like this.\nI think it can also fix #2162 though I need to test that.", "pull_request_status": "Merged", "issue_fixed_time": "2017-03-07T11:56:54Z", "files_changed": [["4", "scrapy/downloadermiddlewares/httpcompression.py"], ["18", "scrapy/spiders/sitemap.py"], ["4", "scrapy/utils/gz.py"], ["61", "tests/test_downloadermiddleware_httpcompression.py"], ["4", "tests/test_spider.py"]]}, "762": {"issue_url": "https://github.com/scrapy/scrapy/issues/2381", "issue_id": "#2381", "issue_summary": "Tests sometimes fail on Travis", "issue_description": "Member\nkmike commented on Nov 7, 2016\nException:\n___ ImagesPipelineTestCaseCustomSettings.test_custom_settings_for_subclasses ___\n\nself = <tests.test_pipeline_images.ImagesPipelineTestCaseCustomSettings testMethod=test_custom_settings_for_subclasses>\n    def test_custom_settings_for_subclasses(self):\n        \"\"\"\n            If there are custom settings for subclass and NO class attributes, pipeline should use custom\n            settings.\n            \"\"\"\n        class UserDefinedImagePipeline(ImagesPipeline):\n            pass\n    \n        prefix = UserDefinedImagePipeline.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = UserDefinedImagePipeline.from_settings(Settings(settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n>           self.assertNotEqual(custom_value, self.default_pipeline_settings[pipe_attr])\n/home/travis/build/scrapy/scrapy/tests/test_pipeline_images.py:354: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nself = <tests.test_pipeline_images.ImagesPipelineTestCaseCustomSettings testMethod=test_custom_settings_for_subclasses>\nfirst = 90, second = 90, msg = None\n\n    def assertNotEqual(self, first, second, msg=None):\n        \"\"\"\n            Fail the test if C{first} == C{second}.\n    \n            @param msg: if msg is None, then the failure message will be\n            '%r == %r' % (first, second)\n            \"\"\"\n        if not first != second:\n>           raise self.failureException(msg or '%r == %r' % (first, second))\nE           FailTest: 90 == 90", "issue_status": "Closed", "issue_reporting_time": "2016-11-07T13:04:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "763": {"issue_url": "https://github.com/scrapy/scrapy/issues/2379", "issue_id": "#2379", "issue_summary": "est() method with telnet access crash because of py3", "issue_description": "rolele commented on Nov 6, 2016 \u2022\nedited\nI run a spider in console (1) and try to connect using telnet access on console (2).\nI run the command est() on console (2) to get back some useful metrics\n>>> est()\nHere is the stack, I am pretty sure it is related to the py3 migration on console (1)\n2016-11-06 10:40:06 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-11-06 10:40:16 [scrapy] INFO: Crawled 3 pages (at 18 pages/min), scraped 1 items (at 6 items/min)\nq2016-11-06 10:40:26 [scrapy] INFO: Crawled 6 pages (at 18 pages/min), scraped 3 items (at 12 items/min)\nUnhandled Error\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/python/log.py\", line 101, in callWithLogger\n    return callWithContext({\"system\": lp}, func, *args, **kw)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/python/log.py\", line 84, in callWithContext\n    return context.call({ILogContext: newCtx}, func, *args, **kw)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/python/context.py\", line 118, in callWithContext\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/python/context.py\", line 81, in callWithContext\n    return func(*args,**kw)\n--- <exception caught here> ---\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/internet/posixbase.py\", line 597, in _doReadOrWrite\n    why = selectable.doRead()\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/internet/tcp.py\", line 208, in doRead\n    return self._dataReceived(data)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/internet/tcp.py\", line 214, in _dataReceived\n    rval = self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/telnet.py\", line 636, in dataReceived\n    self.applicationDataReceived(b''.join(appDataBuffer))\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/telnet.py\", line 988, in applicationDataReceived\n    self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/telnet.py\", line 1035, in dataReceived\n    self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/insults/insults.py\", line 537, in dataReceived\n    self.terminalProtocol.keystrokeReceived(ch, None)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/recvline.py\", line 220, in keystrokeReceived\n    m()\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/recvline.py\", line 369, in handle_RETURN\n    return RecvLine.handle_RETURN(self)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/recvline.py\", line 287, in handle_RETURN\n    self.lineReceived(line)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/manhole.py\", line 235, in lineReceived\n    more = self.interpreter.push(line)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/manhole.py\", line 87, in push\n    source = \"\\n\".join(self.buffer)\nbuiltins.TypeError: sequence item 0: expected str instance, bytes found\n\n2016-11-06 10:40:33 [twisted] CRITICAL: Unhandled Error\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/python/log.py\", line 101, in callWithLogger\n    return callWithContext({\"system\": lp}, func, *args, **kw)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/python/log.py\", line 84, in callWithContext\n    return context.call({ILogContext: newCtx}, func, *args, **kw)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/python/context.py\", line 118, in callWithContext\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/python/context.py\", line 81, in callWithContext\n    return func(*args,**kw)\n--- <exception caught here> ---\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/internet/posixbase.py\", line 597, in _doReadOrWrite\n    why = selectable.doRead()\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/internet/tcp.py\", line 208, in doRead\n    return self._dataReceived(data)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/internet/tcp.py\", line 214, in _dataReceived\n    rval = self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/telnet.py\", line 636, in dataReceived\n    self.applicationDataReceived(b''.join(appDataBuffer))\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/telnet.py\", line 988, in applicationDataReceived\n    self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/telnet.py\", line 1035, in dataReceived\n    self.protocol.dataReceived(data)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/insults/insults.py\", line 537, in dataReceived\n    self.terminalProtocol.keystrokeReceived(ch, None)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/recvline.py\", line 220, in keystrokeReceived\n    m()\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/recvline.py\", line 369, in handle_RETURN\n    return RecvLine.handle_RETURN(self)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/recvline.py\", line 287, in handle_RETURN\n    self.lineReceived(line)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/manhole.py\", line 235, in lineReceived\n    more = self.interpreter.push(line)\n  File \"/usr/local/lib/python3.5/dist-packages/twisted/conch/manhole.py\", line 87, in push\n    source = \"\\n\".join(self.buffer)\nbuiltins.TypeError: sequence item 0: expected str instance, bytes found\n\n2016-11-06 10:40:36 [scrapy] INFO: Crawled 8 pages (at 12 pages/min), scraped 5 items (at 12 items/min)\nI get disconnected from the telnet console (1)\n>>> est()Connection closed by foreign host.\nI tried other command like engine but still got the same result\n>>> engineConnection closed by foreign host.\nscrapy version\nScrapy 1.2.1\ninstalled using pip3 install scrapy\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2016-11-06T10:46:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "764": {"issue_url": "https://github.com/scrapy/scrapy/issues/2377", "issue_id": "#2377", "issue_summary": "Invalid Codepoint in robots.txt", "issue_description": "mohmad-null commented on Nov 2, 2016 \u2022\nedited\nDuring my scraping I had this error get thrown.\nPages that triggered it:\nhttps://www.gpo.gov:/robots.txt\nhttps://www.gpo.gov:/fdsys/pkg/FR-2006-12-28/html/E6-22242.htm\nhttps://www.gpo.gov:/fdsys/pkg/FR-2006-07-20/html/E6-11541.htm\nhttps://www.gpo.gov:/fdsys/pkg/FR-2006-05-10/html/06-4319.htm\nhttps://www.gpo.gov:/fdsys/pkg/FR-2006-08-02/html/E6-12432.htm\nPython 2.7.12\nScrapy 1.2.1\n2016-11-02 15:30:04 [scrapy] ERROR: Error downloading <GET https://www.gpo.gov:/robots.txt>: Codepoint U+003A at position 4 of u'gov:' not allowed\nTraceback (most recent call last):\n  File \"c:\\python27_internet\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1183, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"c:\\python27_internet\\lib\\site-packages\\twisted\\python\\failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"c:\\python27_internet\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\n  File \"c:\\python27_internet\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 45, in mustbe_deferred\n    result = f(*args, **kw)\n  File \"c:\\python27_internet\\lib\\site-packages\\scrapy\\core\\downloader\\handlers\\__init__.py\", line 65, in download_request\n    return handler.download_request(request, spider)\n  File \"c:\\python27_internet\\lib\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 60, in download_request\n    return agent.download_request(request)\n  File \"c:\\python27_internet\\lib\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 285, in download_request\n    method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n  File \"c:\\python27_internet\\lib\\site-packages\\twisted\\web\\client.py\", line 1596, in request\n    endpoint = self._getEndpoint(parsedURI)\n  File \"c:\\python27_internet\\lib\\site-packages\\twisted\\web\\client.py\", line 1580, in _getEndpoint\n    return self._endpointFactory.endpointForURI(uri)\n  File \"c:\\python27_internet\\lib\\site-packages\\twisted\\web\\client.py\", line 1456, in endpointForURI\n    uri.port)\n  File \"c:\\python27_internet\\lib\\site-packages\\scrapy\\core\\downloader\\contextfactory.py\", line 59, in creatorForNetloc\n    return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext())\n  File \"c:\\python27_internet\\lib\\site-packages\\twisted\\internet\\_sslverify.py\", line 1198, in __init__\n    self._hostnameBytes = _idnaBytes(hostname)\n  File \"c:\\python27_internet\\lib\\site-packages\\twisted\\internet\\_sslverify.py\", line 86, in _idnaBytes\n    return idna.encode(text)\n  File \"c:\\python27_internet\\lib\\site-packages\\idna\\core.py\", line 355, in encode\n    result.append(alabel(label))\n  File \"c:\\python27_internet\\lib\\site-packages\\idna\\core.py\", line 276, in alabel\n    check_label(label)\n  File \"c:\\python27_internet\\lib\\site-packages\\idna\\core.py\", line 253, in check_label\n    raise InvalidCodepoint('Codepoint {0} at position {1} of {2} not allowed'.format(_unot(cp_value), pos+1, repr(label)))\nInvalidCodepoint: Codepoint U+003A at position 4 of u'gov:' not allowed", "issue_status": "Closed", "issue_reporting_time": "2016-11-02T16:33:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "765": {"issue_url": "https://github.com/scrapy/scrapy/issues/2375", "issue_id": "#2375", "issue_summary": "ImportError: No module named 'twisted.mail.smtp'", "issue_description": "duangy commented on Nov 2, 2016\nI use scrapy in python3.5, when I try to send a mail by MailSender, it's crashed.\n  File \"/home/guangyao/Documents/Code/financer_crawl/env/lib/python3.5/site-packages/scrapy/mail.py\", line 85, in send\n    dfd = self._sendmail(rcpts, msg.as_string())\n  File \"/home/guangyao/Documents/Code/financer_crawl/env/lib/python3.5/site-packages/scrapy/mail.py\", line 108, in _sendmail\n    from twisted.mail.smtp import ESMTPSenderFactory\nImportError: No module named 'twisted.mail.smtp'", "issue_status": "Closed", "issue_reporting_time": "2016-11-02T14:32:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "766": {"issue_url": "https://github.com/scrapy/scrapy/issues/2374", "issue_id": "#2374", "issue_summary": "DepthMiddleware docs should mention request.meta['depth']", "issue_description": "Member\nkmike commented on Nov 2, 2016\nrequest.meta['depth'] is set by DepthMiddleware, but its docs don't mention that. I think it'd be nice to add this information.", "issue_status": "Closed", "issue_reporting_time": "2016-11-02T14:14:02Z", "fixed_by": "#2963", "pull_request_summary": "Add note about request.meta['depth'] in DepthMiddleware", "pull_request_description": "Contributor\ndjunzu commented on Oct 17, 2017\nUpdate DepthMiddleware docs with request.meta['depth'].\ncloses #2374\ncloses #2455", "pull_request_status": "Merged", "issue_fixed_time": "2017-10-26T11:15:37Z", "files_changed": [["10", "docs/topics/spider-middleware.rst"]]}, "767": {"issue_url": "https://github.com/scrapy/scrapy/issues/2373", "issue_id": "#2373", "issue_summary": "Wikipedia robots.txt raises exceptions", "issue_description": "mohmad-null commented on Nov 2, 2016\nI'm scraping a page which in turn links to wikipedia.\nBut the wikipedia robots.txt is creating some errors/exceptions as below.\nPython 2.7.12\nScrapy 1.2.1\n2016-11-02 13:13:18 [scrapy] DEBUG: Crawled (200) <GET https://en.wikipedia.org/robots.txt> (referer: None)\n2016-11-02 13:13:18 [py.warnings] WARNING: C:\\Python27\\lib\\urllib.py:1303: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n  return ''.join(map(quoter, s))\n\n2016-11-02 13:13:18 [scrapy] ERROR: Error downloading <GET http://en.wikipedia.org/robots.txt>: u'\\xd8'\nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 587, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\downloadermiddlewares\\robotstxt.py\", line 97, in _parse_robots\n    rp.parse(body.splitlines())\n  File \"C:\\Python27\\lib\\robotparser.py\", line 120, in parse\n    entry.rulelines.append(RuleLine(line[1], False))\n  File \"C:\\Python27\\lib\\robotparser.py\", line 174, in __init__\n    self.path = urllib.quote(path)\n  File \"C:\\Python27\\lib\\urllib.py\", line 1303, in quote\n    return ''.join(map(quoter, s))\nKeyError: u'\\xd8'", "issue_status": "Closed", "issue_reporting_time": "2016-11-02T13:18:27Z", "fixed_by": "#2388", "pull_request_summary": "[MRG+1] Parse robots.txt content as native str", "pull_request_description": "Contributor\nredapple commented on Nov 9, 2016 \u2022\nedited\nFixes #2373\nSimpler fix than #2385", "pull_request_status": "Merged", "issue_fixed_time": "2016-12-01T20:43:23Z", "files_changed": [["8", "scrapy/downloadermiddlewares/robotstxt.py"], ["16", "tests/test_downloadermiddleware_robotstxt.py"]]}, "768": {"issue_url": "https://github.com/scrapy/scrapy/issues/2371", "issue_id": "#2371", "issue_summary": "image pipline failed with scrapy.pipelines.files.FileException", "issue_description": "geemaple commented on Oct 30, 2016 \u2022\nedited\n[u'https://imgs.bipush.com/article/cover/201610/21/104313114422.jpg?imageView2/1/w/800/h/600/imageMogr2/strip/interlace/1/quality/85/format/jpg', u'https://imgs.bipush.com/article/content/201610/29/58146895b66e78.93663001.jpg', u'https://imgs.bipush.com/article/content/201610/29/58146896002550.90536333.jpg', u'https://imgs.bipush.com/article/content/201610/29/581468964c1c62.50438304.jpg', u'https://imgs.bipush.com/article/content/201610/29/581468972bc645.06245956.jpg', u'https://imgs.bipush.com/article/content/201610/29/581468976a3144.56422697.jpg', u'https://imgs.bipush.com/article/content/201610/29/58146897a30785.81365307.jpg', u'https://imgs.bipush.com/article/content/201610/29/58146897d18ef1.53961958.jpg', u'https://imgs.bipush.com/article/content/201610/29/58146898083c73.31449922.jpg', u'https://imgs.bipush.com/article/content/201610/29/58146898df6dc9.24235456.jpg']\n[(False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >), (False, <twisted.python.failure.Failure scrapy.pipelines.files.FileException: >)]", "issue_status": "Closed", "issue_reporting_time": "2016-10-30T10:27:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "769": {"issue_url": "https://github.com/scrapy/scrapy/issues/2367", "issue_id": "#2367", "issue_summary": "Extracting text in custom tags use Scrapy", "issue_description": "hzxie commented on Oct 28, 2016\nThis is a web page for a paper. There're some useful information to extract.\nI want to extract title, author and abstract of this paper. So I wrote following code:\nclass PublicationSpider(scrapy.Spider):\n    name = \"publications\"\n    start_urls = [\n        'https://www.ncbi.nlm.nih.gov/pubmed/15721472',\n    ]\n\n    def parse(self, response):\n        for publication in response.css('div.rprt.abstract'):\n            yield {\n                'title': publication.css('h1::text').extract_first(),\n                'author': publication.css('div.auths > a::text').extract(),\n                'abstract': publication.css('div.abstr abstracttext::text').extract(),\n                'doi': publication.css('div.aux a::text').extract_first(),\n            }\nUnfortunately, the code above could not return the right content. It seems that the abstracttext element cannot be recognized by Scrapy.\nI tried to replace abstracttext with h4, and the script went well.\nSo how can I extract the content in abstracttext?\nSee more: http://stackoverflow.com/questions/40241637/extracting-text-in-custom-tags-use-scrapy/", "issue_status": "Closed", "issue_reporting_time": "2016-10-28T05:53:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "770": {"issue_url": "https://github.com/scrapy/scrapy/issues/2366", "issue_id": "#2366", "issue_summary": "How can I post a request that had both data and cookies", "issue_description": "tarjintor commented on Oct 27, 2016\nHi,everyone\nI need scrapy a website,and It used aspx,\nat first ,I used selenium,and it works fine,but too slow\n,so I analysis the request head in chrome dev,\nand I found how to structure the head to get the content I need,but it need post both data and cookies,\nI used requests and it works fine,but I need to do it in scrapy\nI googled before,but it seems if you use Request(...),it can't post data, on the other hand ,if you use Formrequest,it can't have cookies\nNow,I'm confused,and don't know what to do\nIs that I misunderstood the meaning of these two method?\nHelp please!", "issue_status": "Closed", "issue_reporting_time": "2016-10-27T15:01:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "771": {"issue_url": "https://github.com/scrapy/scrapy/issues/2364", "issue_id": "#2364", "issue_summary": "FeedExported error when using sub-items", "issue_description": "birla commented on Oct 27, 2016 \u2022\nedited\nI'm creating a Scrapy.Item with the following structure:\nItem1\nproperty (string)\nproperty (string)\nproperty (Item2[])\n(Item2)\n(Item2)\n....\nNow the console's debug log displays the resulting JSON structure property, however in the export I get the following error:\n2016-10-27 15:17:55 [scrapy] ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7f0edde6ebd0>>\nTraceback (most recent call last):\n  File \"/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 149, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 221, in item_scraped\n    slot.exporter.export_item(item)\n  File \"/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 116, in export_item\n    itemdict = dict(self._get_serialized_fields(item))\n  File \"/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 75, in _get_serialized_fields\n    value = self.serialize_field(field, field_name, item[field_name])\n  File \"/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 47, in serialize_field\n    return serializer(value)\n  File \"/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/item.py\", line 52, in __init__\n    for k, v in six.iteritems(dict(*args, **kwargs)):\nValueError: dictionary update sequence element #0 has length 12; 2 is required\nItem2 has a 12 properties. As far as I understand it is being raised while converting Item2[] into a dict object. Also, if I run the crawl without exporting the code works fine.", "issue_status": "Closed", "issue_reporting_time": "2016-10-27T08:39:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "772": {"issue_url": "https://github.com/scrapy/scrapy/issues/2363", "issue_id": "#2363", "issue_summary": "Quoted url Location header fails to redirect", "issue_description": "Tarliton commented on Oct 26, 2016\nHello,\nI'm trying to crawl a website that returns a Location url in the header that is quoted. When that happens the downloadermiddleware_redirect fails to build the correct url.\nThis little test helps to explain:\n    def test_quoted_location(self):\n        req = Request('http://scrapytest.org/first')\n        utf8_location = u'http%3A//scrapytest.org/a\u00e7\u00e3o'.encode('utf-8')  # header using quoted UTF-8 encoding\n        resp = Response('http://scrapytest.org/first', headers={'Location': utf8_location}, status=302)\n        req_result = self.mw.process_response(req, resp, self.spider)\n        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'\n        self.assertEquals(perc_encoded_utf8_url, req_result.url)\nit fails:\n===================================================================== FAILURES =====================================================================\n___________________________________________________ RedirectMiddlewareTest.test_quoted_location ____________________________________________________\n\nself = <tests.test_downloadermiddleware_redirect.RedirectMiddlewareTest testMethod=test_quoted_location>\n\n    def test_quoted_location(self):\n        req = Request('http://scrapytest.org/first')\n        utf8_location = u'http%3A//scrapytest.org/a\u00e7\u00e3o'.encode('utf-8')  # header using UTF-8 encoding\n        resp = Response('http://scrapytest.org/first', headers={'Location': utf8_location}, status=302)\n        req_result = self.mw.process_response(req, resp, self.spider)\n        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'\n>       self.assertEquals(perc_encoded_utf8_url, req_result.url)\nE       AssertionError: 'http://scrapytest.org/a%C3%A7%C3%A3o' != 'http://scrapytest.org/http%3A//scrapytest.org/a%C3%A7%C3%A3o'\n\n/home/aurumdev/repos/scrapy/tests/test_downloadermiddleware_redirect.py:177: AssertionError\n======================================================= 1 failed, 19 passed in 0.29 seconds ========================================================\nThe quoted url comes from the website.\nIs that a bug?\nThank you", "issue_status": "Closed", "issue_reporting_time": "2016-10-26T17:39:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "773": {"issue_url": "https://github.com/scrapy/scrapy/issues/2362", "issue_id": "#2362", "issue_summary": "Unhelpful traces when there's an error in the pipeline", "issue_description": "mohmad-null commented on Oct 26, 2016\nI had a pretty basic error in my pipeline (i'd forgotten to import os).\nHowever, rather than get the customary NameError: name 'os' is not defined, I instead receive a long, drawn-out Twisted error:\n2016-10-26 16:34:15 [scrapy] INFO: Closing spider (shutdown)\nUnhandled error in Deferred:\n2016-10-26 16:34:15 [twisted] CRITICAL: Unhandled error in Deferred:\n\n\nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 57, in run\n    self.crawler_process.crawl(spname, **opts.spargs)\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\crawler.py\", line 163, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\crawler.py\", line 167, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1331, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1183, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"C:\\Python27\\lib\\site-packages\\twisted\\python\\failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\crawler.py\", line 87, in crawl\n    yield self.engine.close()\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 100, in close\n    return self._close_all_spiders()\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 340, in _close_all_spiders\n    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 298, in close_spider\n    dfd = slot.close()\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 44, in close\n    self._maybe_fire_closing()\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 51, in _maybe_fire_closing\n    self.heartbeat.stop()\n  File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\task.py\", line 202, in stop\n    assert self.running, (\"Tried to stop a LoopingCall that was \"\nexceptions.AssertionError: Tried to stop a LoopingCall that was not running.\n2016-10-26 16:34:15 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1183, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"C:\\Python27\\lib\\site-packages\\twisted\\python\\failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\crawler.py\", line 87, in crawl\n    yield self.engine.close()\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 100, in close\n    return self._close_all_spiders()\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 340, in _close_all_spiders\n    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 298, in close_spider\n    dfd = slot.close()\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 44, in close\n    self._maybe_fire_closing()\n  File \"C:\\Python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 51, in _maybe_fire_closing\n    self.heartbeat.stop()\n  File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\task.py\", line 202, in stop\n    assert self.running, (\"Tried to stop a LoopingCall that was \"\nAssertionError: Tried to stop a LoopingCall that was not running.\nWithout useful traceroutes this is going to make it very difficult to develop a spider using pipelines.\nCould scrapy be made to more transparently handle failures in pipelines please? Thanks.\nScrapy 1.2", "issue_status": "Closed", "issue_reporting_time": "2016-10-26T15:37:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "774": {"issue_url": "https://github.com/scrapy/scrapy/issues/2360", "issue_id": "#2360", "issue_summary": "Scrapy alert when scraping result suspiciously shows a lot of missing data", "issue_description": "vionemc commented on Oct 26, 2016\nOne of the biggest worries of web scraping is when the site changes and the script should be changed adapting to the site's change. It will be great if Scrapy can show and send an alert when those abnormalities show up. Does Scrapy has any plan of applying such feature?", "issue_status": "Closed", "issue_reporting_time": "2016-10-26T04:11:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "775": {"issue_url": "https://github.com/scrapy/scrapy/issues/2357", "issue_id": "#2357", "issue_summary": "engine_started called multi times in one process, if multi spiders exists.", "issue_description": "ultragis commented on Oct 23, 2016 \u2022\nedited by redapple\nMy Extension code is:\nclass SpiderStatsExtension(object):\n    @classmethod\n    def from_crawler(cls, crawler):\n        if not hasattr(cls, '__instance'):\n            setattr(cls, '__instance', cls())\n        ext = getattr(cls, '__instance')\n        crawler.signals.connect(ext.engine_started, signal=signals.engine_started)\n        crawler.signals.connect(ext.engine_stopped, signal=signals.engine_stopped)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(ext.spider_error, signal=signals.spider_error)\n        return ext\nMy process start code is:\nprocess = CrawlerProcess(get_project_settings())\nspiders = process.spider_loader.list()\nfor spider in spiders:\n    process.crawl(process.spider_loader.load(spider))\nprocess.start()\nI found the from_crawler called multi times if my project have multi spiders.\nIs it expectant design?\nI need once call for on process to record stats info, how can i do for it?\nthanks.", "issue_status": "Closed", "issue_reporting_time": "2016-10-23T02:07:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "776": {"issue_url": "https://github.com/scrapy/scrapy/issues/2356", "issue_id": "#2356", "issue_summary": "twisted.internet.error with bindaddress with scrapy.Request", "issue_description": "dingld commented on Oct 22, 2016 \u2022\nedited by redapple\nPython3.5 with latest Scrapy\nI was trying to set the bindaddress with Request.meta , whatever (host, port) I chose, the program always complained about\nTraceback (most recent call last):\n  File \"/Users/ld/.virtualenvs/scrapy/lib/python3.5/site-packages/twisted/internet/defer.py\", line 1126, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/Users/ld/.virtualenvs/scrapy/lib/python3.5/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/Users/ld/.virtualenvs/scrapy/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\ntwisted.internet.error.ConnectBindError: Couldn't bind: 48: Address already in use.\nIs there any approach to that?", "issue_status": "Closed", "issue_reporting_time": "2016-10-22T14:10:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "777": {"issue_url": "https://github.com/scrapy/scrapy/issues/2352", "issue_id": "#2352", "issue_summary": "how to let the log only log ERROR level information", "issue_description": "shyandsy commented on Oct 21, 2016 \u2022\nedited\nI tried lots of way to use the ERROR level, but still all information was wrote into log file\npython 3.5 + scrapy 1.2\ndmoz_spider.py\n\nconfigure_logging(install_root_handler=False)\nlogging.basicConfig(filename='log.txt', format='%(levelname)s: %(message)s', level=logging.ERROR)\nsettings.py\n\n...................\nLOG_LEVEL = \"ERROR\"\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-10-21T16:17:49Z", "fixed_by": "#3960", "pull_request_summary": "Update documentation for logging manually", "pull_request_description": "Contributor\nthernstig commented on Aug 15, 2019 \u2022\nedited\nUsage of basicConfig() together with crawlerRunner is not recommended.\nUpdate documentation to highlight this fact.\nCloses #2149, closes #2352, and closes #3146", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-12T11:17:50Z", "files_changed": [["14", "docs/topics/logging.rst"]]}, "778": {"issue_url": "https://github.com/scrapy/scrapy/issues/2348", "issue_id": "#2348", "issue_summary": "Promote conda-forge channel for conda package manager", "issue_description": "Contributor\nredapple commented on Oct 21, 2016 \u2022\nedited\nScrapy now has it's own feedstock on conda-forge.\nLet's advertize conda-forge channel in the docs instead of the \"scrapinghub\" channel.", "issue_status": "Closed", "issue_reporting_time": "2016-10-21T11:41:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "779": {"issue_url": "https://github.com/scrapy/scrapy/issues/2347", "issue_id": "#2347", "issue_summary": "Random ImagesPipeline test failure", "issue_description": "Contributor\nredapple commented on Oct 21, 2016\nTravis CI sometimes fails builds because of a ImagesPipeline test failure.\nExample failed build: https://travis-ci.org/scrapy/scrapy/jobs/169472958\n=================================== FAILURES ===================================\n___ ImagesPipelineTestCaseCustomSettings.test_custom_settings_for_subclasses ___\nself = <tests.test_pipeline_images.ImagesPipelineTestCaseCustomSettings testMethod=test_custom_settings_for_subclasses>\n    def test_custom_settings_for_subclasses(self):\n        \"\"\"\n            If there are custom settings for subclass and NO class attributes, pipeline should use custom\n            settings.\n            \"\"\"\n        class UserDefinedImagePipeline(ImagesPipeline):\n            pass\n\n        prefix = UserDefinedImagePipeline.__name__.upper()\n        settings = self._generate_fake_settings(prefix=prefix)\n        user_pipeline = UserDefinedImagePipeline.from_settings(Settings(settings))\n        for pipe_attr, settings_attr in self.img_cls_attribute_names:\n            # Values from settings for custom pipeline should be set on pipeline instance.\n            custom_value = settings.get(prefix + \"_\" + settings_attr)\n>           self.assertNotEqual(custom_value, self.default_pipeline_settings[pipe_attr])\n/home/travis/build/scrapy/scrapy/tests/test_pipeline_images.py:354: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nself = <tests.test_pipeline_images.ImagesPipelineTestCaseCustomSettings testMethod=test_custom_settings_for_subclasses>\nfirst = 90, second = 90, msg = None\n    def assertNotEqual(self, first, second, msg=None):\n        \"\"\"\n            Fail the test if C{first} == C{second}.\n\n            @param msg: if msg is None, then the failure message will be\n            '%r == %r' % (first, second)\n            \"\"\"\n        if not first != second:\n>           raise self.failureException(msg or '%r == %r' % (first, second))\nE           twisted.trial.unittest.FailTest: 90 == 90\n/home/travis/build/scrapy/scrapy/.tox/py35/lib/python3.5/site-packages/twisted/trial/_synctest.py:468: FailTest\n== 1 failed, 1396 passed, 15 skipped, 12 xfailed, 1 xpassed in 423.24 seconds ==\nERROR: InvocationError: '/home/travis/build/scrapy/scrapy/.tox/py35/bin/py.test --cov=scrapy --cov-report= scrapy tests'\n___________________________________ summary ____________________________________\nERROR:   py35: commands failed", "issue_status": "Closed", "issue_reporting_time": "2016-10-21T10:40:54Z", "fixed_by": "#2427", "pull_request_summary": "TST: Randomize IMAGES_EXPIRES above 90 days", "pull_request_description": "Contributor\nredapple commented on Dec 6, 2016\nHoping to fix #2347", "pull_request_status": "Merged", "issue_fixed_time": "2016-12-06T18:03:09Z", "files_changed": [["2", "tests/test_pipeline_images.py"]]}, "780": {"issue_url": "https://github.com/scrapy/scrapy/issues/2342", "issue_id": "#2342", "issue_summary": "Anonymous FTP download fails with KeyError: ftp_user", "issue_description": "Contributor\nredapple commented on Oct 20, 2016 \u2022\nedited\nTested with Scrapy 1.2.0\n$ scrapy version -v\nScrapy    : 1.2.0\nlxml      : 3.6.4.0\nlibxml2   : 2.9.4\nTwisted   : 16.4.1\nPython    : 2.7.12 (default, Jul  1 2016, 15:12:24) - [GCC 5.4.0 20160609]\npyOpenSSL : 16.1.0 (OpenSSL 1.0.2g  1 Mar 2016)\nPlatform  : Linux-4.4.0-43-generic-x86_64-with-Ubuntu-16.04-xenial\n\n\n$ scrapy shell ftp://ftp.eu.metabrainz.org/pub/musicbrainz/data/fullexport/20161019-001816/MD5SUMS\n2016-10-20 16:01:27 [scrapy] INFO: Scrapy 1.2.0 started (bot: scrapybot)\n(...)\n2016-10-20 16:01:27 [scrapy] INFO: Spider opened\nTraceback (most recent call last):\n  File \"/home/paul/.virtualenvs/scrapy12/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/commands/shell.py\", line 71, in run\n    shell.start(url=url)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/shell.py\", line 47, in start\n    self.fetch(url, spider)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/shell.py\", line 112, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<string>\", line 2, in raiseException\nKeyError: 'ftp_user'", "issue_status": "Closed", "issue_reporting_time": "2016-10-20T14:05:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "781": {"issue_url": "https://github.com/scrapy/scrapy/issues/2340", "issue_id": "#2340", "issue_summary": "Always blocked with no information", "issue_description": "Sraw commented on Oct 20, 2016\nI use scrapy startproject XXXX to init a project and build my spider.\nAnd then everything is fine but blocked at a random step:\n 2016-10-20 15:49:54 [scrapy] DEBUG: Crawled (200) <GET http://xxx.xxx.com/thread-2109141-1-27.html> (referer: http://xxx.xxx.com/forum-1286-27.html) ['partial']\nThen it's totally blocked and no information output. Especially, when I tried to use double Ctrl+C to cancel the program, it doesn't work.", "issue_status": "Closed", "issue_reporting_time": "2016-10-20T08:10:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "782": {"issue_url": "https://github.com/scrapy/scrapy/issues/2335", "issue_id": "#2335", "issue_summary": "Add a sample middleware to startproject's template", "issue_description": "Member\neliasdorneles commented on Oct 18, 2016 \u2022\nedited by redapple\nIt will be nice to have a middleware template inside the template project to serve as an example for people that want to use it.", "issue_status": "Closed", "issue_reporting_time": "2016-10-18T17:38:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "783": {"issue_url": "https://github.com/scrapy/scrapy/issues/2326", "issue_id": "#2326", "issue_summary": "ssl error: tlsv1 alert internal error", "issue_description": "cp2587 commented on Oct 17, 2016 \u2022\nedited\nHello,\nThank you for all the work on this library.\nWe recently started to scrape some website using https using proxies (crawlera & proxymesh) but with proxymesh we always get the following error:\n[('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert internal error'), ('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]\nWhen using no proxy or crawlera, we are getting a correct response.\nUsing curl with proxymesh also seems to work...\nDo you have any idea on what's happening ? I can provide the url website in private if need be.\nscrapy version: 1.0.5", "issue_status": "Closed", "issue_reporting_time": "2016-10-17T08:10:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "784": {"issue_url": "https://github.com/scrapy/scrapy/issues/2325", "issue_id": "#2325", "issue_summary": "address \"'http:\" not found: [Errno 11001] getaddrinfo failed.", "issue_description": "LancelotHolmes commented on Oct 14, 2016\nWhen I try the sample code of scrapy tutorial in Creating a project, Extracting data,after type\nscrapy shell 'http://quotes.toscrape.com/page/1/'\nI got a exception like\ntwisted.internet.error.DNSLookupError: DNS lookup failed: address \"'http:\" not found: [Errno 11001] getaddrinfo failed.\nand my operating system is windows with python 2.7", "issue_status": "Closed", "issue_reporting_time": "2016-10-14T07:30:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "785": {"issue_url": "https://github.com/scrapy/scrapy/issues/2324", "issue_id": "#2324", "issue_summary": "sslv3 alert handshake failure", "issue_description": "bhagatsajan0073 commented on Oct 12, 2016 \u2022\nedited by redapple\nError: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]", "issue_status": "Closed", "issue_reporting_time": "2016-10-12T18:05:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "786": {"issue_url": "https://github.com/scrapy/scrapy/issues/2323", "issue_id": "#2323", "issue_summary": "`LxmlLinkExtractor` fails handling unicode netlocs in Python2", "issue_description": "Contributor\nstarrify commented on Oct 12, 2016\nAffected version:\ndc1f9ad\nAffected Python version:\nPython 2 only\nSteps to reproduce:\n>>> import scrapy.http\n>>> response = scrapy.http.TextResponse(url='http://foo.com', body=u'<a href=\"http://foo\\u263a\">', encoding='utf8')\n>>> response.css('a::attr(href)').extract()\n[u'http://foo\\u263a']\n>>> import scrapy.linkextractors\n>>> extractor = scrapy.linkextractors.LinkExtractor()\n>>> extractor.extract_links(response)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/tmp/virtualenv/src/scrapy/scrapy/linkextractors/lxmlhtml.py\", line 111, in extract_links\n    all_links.extend(self._process_links(links))\n  File \"/tmp/virtualenv/src/scrapy/scrapy/linkextractors/__init__.py\", line 104, in _process_links\n    link.url = canonicalize_url(urlparse(link.url))\n  File \"/tmp/virtualenv/lib/python2.7/site-packages/w3lib/url.py\", line 354, in canonicalize_url\n    parse_url(url), encoding=encoding)\n  File \"/tmp/virtualenv/lib/python2.7/site-packages/w3lib/url.py\", line 298, in _safe_ParseResult\n    netloc = parts.netloc.encode('idna')\n  File \"/tmp/virtualenv/lib/python2.7/encodings/idna.py\", line 164, in encode\n    result.append(ToASCII(label))\n  File \"/tmp/virtualenv/lib/python2.7/encodings/idna.py\", line 76, in ToASCII\n    label = nameprep(label)\n  File \"/tmp/virtualenv/lib/python2.7/encodings/idna.py\", line 21, in nameprep\n    newlabel.append(stringprep.map_table_b2(c))\n  File \"/usr/lib64/python2.7/stringprep.py\", line 197, in map_table_b2\n    b = unicodedata.normalize(\"NFKC\", al)\nTypeError: normalize() argument 2 must be unicode, not str", "issue_status": "Closed", "issue_reporting_time": "2016-10-12T17:10:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "787": {"issue_url": "https://github.com/scrapy/scrapy/issues/2321", "issue_id": "#2321", "issue_summary": "Decoding of \"Location\" header on redirects using latin-1 can be wrong", "issue_description": "Contributor\nredapple commented on Oct 12, 2016 \u2022\nedited\nWeb servers should use encoded URLs in their \"Location\" headers, but they don't always do.\nThis website for example, for this URL http://www.yjc.ir/fa/news/1815565/\nredirects to www.yjc.ir/fa/news/1815565/\u0627\u0639\u0632\u0627\u0645-\u0643\u0648\u0647\u0646\u0648\u0631\u062f\u0627\u0646-\u0627\u064a\u0631\u0627\u0646\u064a-\u0628\u0647-\u0643\u064a\u0644\u064a\u0645\u0627\u0646\u062c\u0627\u0631\u0648\nbut the bytes received are UTF-8 encoded, and not percent-escaped:\n'Location': ['/fa/news/1815565/\\xd8\\xa7\\xd8\\xb9\\xd8\\xb2\\xd8\\xa7\\xd9\\x85-\\xd9\\x83\\xd9\\x88\\xd9\\x87\\xd9\\x86\\xd9\\x88\\xd8\\xb1\\xd8\\xaf\\xd8\\xa7\\xd9\\x86-\\xd8\\xa7\\xd9\\x8a\\xd8\\xb1\\xd8\\xa7\\xd9\\x86\\xd9\\x8a-\\xd8\\xa8\\xd9\\x87-\\xd9\\x83\\xd9\\x8a\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd8\\xa7\\xd9\\x86\\xd8\\xac\\xd8\\xa7\\xd8\\xb1\\xd9\\x88']\nRedirectMiddleware decodes the header as \"latin1\" (this is new in Scrapy 1.1) and issues a request to http://www.yjc.ir/fa/news/1815565/%C3%98%C2%A7%C3%98%C2%B9%C3%98%C2%B2%C3%98%C2%A7%C3%99%C2%85-%C3%99%C2%83%C3%99%C2%88%C3%99%C2%87%C3%99%C2%86%C3%99%C2%88%C3%98%C2%B1%C3%98%C2%AF%C3%98%C2%A7%C3%99%C2%86-%C3%98%C2%A7%C3%99%C2%8A%C3%98%C2%B1%C3%98%C2%A7%C3%99%C2%86%C3%99%C2%8A-%C3%98%C2%A8%C3%99%C2%87-%C3%99%C2%83%C3%99%C2%8A%C3%99%C2%84%C3%99%C2%8A%C3%99%C2%85%C3%98%C2%A7%C3%99%C2%86%C3%98%C2%AC%C3%98%C2%A7%C3%98%C2%B1%C3%99%C2%88\nwhich is not correct.\ncurl -i \"http://www.yjc.ir/fa/news/1815565/\" and wget http://www.yjc.ir/fa/news/1815565/ handle it just fine and correctly follow http://www.yjc.ir/fa/news/1815565/%D8%A7%D8%B9%D8%B2%D8%A7%D9%85-%D9%83%D9%88%D9%87%D9%86%D9%88%D8%B1%D8%AF%D8%A7%D9%86-%D8%A7%D9%8A%D8%B1%D8%A7%D9%86%D9%8A-%D8%A8%D9%87-%D9%83%D9%8A%D9%84%D9%8A%D9%85%D8%A7%D9%86%D8%AC%D8%A7%D8%B1%D9%88\n(curl fixed the issue not too long ago )\nThanks @stav for reporting!", "issue_status": "Closed", "issue_reporting_time": "2016-10-12T10:48:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "788": {"issue_url": "https://github.com/scrapy/scrapy/issues/2320", "issue_id": "#2320", "issue_summary": "Document headers methods such as getlist()", "issue_description": "Contributor\nredapple commented on Oct 11, 2016\nIt appears that current documentation does not mention how you can get values from HTTP Headers object.\nThings like response.headers.getlist('Set-Cookie') to get all cookies are not explained (see #2319)", "issue_status": "Closed", "issue_reporting_time": "2016-10-11T09:11:42Z", "fixed_by": "#2327", "pull_request_summary": "[MRG+1] Added documentation about accessing header values", "pull_request_description": "Contributor\nbopace commented on Oct 18, 2016\nAdds documentation to resolve #2320", "pull_request_status": "Merged", "issue_fixed_time": "2016-10-20T07:05:14Z", "files_changed": [["8", "docs/topics/request-response.rst"]]}, "789": {"issue_url": "https://github.com/scrapy/scrapy/issues/2319", "issue_id": "#2319", "issue_summary": "missed the same key in response.headers", "issue_description": "bfdcq commented on Oct 11, 2016\nIn browser, the response headers is\nServer:nginx\nSet-Cookie:_m_h5_tk=f8baf9999427b2eb4ffa13bc2b5c04b7_1476178540157; Domain=taobao.com; Expires=Tue, 18-Oct-2016 08:14:40 GMT; Path=/\nSet-Cookie:_m_h5_tk_enc=84ac09db726b4c8fabd07de36ac46cb8; Domain=taobao.com; Expires=Tue, 18-Oct-2016 08:14:40 GMT; Path=/\nThere are two 'Set-Cookie'\nIn scrapy, when I crawled this page, the response.headers is\n'Server': 'nginx',\n'Set-Cookie': '_m_h5_tk_enc=6e7808eac6473ca8e15df0442824097a; Domain=taobao.com; Expires=Tue, 18-Oct-2016 08:17:45 GMT; Path=/',\nThere is only one 'Set-Cookie'\nHow can I get all the 'Set-Cookie'?\nurl = 'https://api.m.taobao.com/h5/mtop.taobao.ocean.quest.list.pc/1.0/?appKey=12574478&t=14761675779&sign=41a5e059e35832b3e815ae8b537617cc&api=mtop.taobao.ocean.quest.list.pc&v=1.0&type=jsonp&dataType=jsonp&callback=mtopjsonp1&data=%7B%22itemId%22%3A%22536016491947%22%7D'", "issue_status": "Closed", "issue_reporting_time": "2016-10-11T08:30:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "790": {"issue_url": "https://github.com/scrapy/scrapy/issues/2317", "issue_id": "#2317", "issue_summary": "Cannot save the item into cvs or json", "issue_description": "iBotasky commented on Oct 10, 2016 \u2022\nedited\n@redapple Hi, I need some help!!\nI got a problem with scrapy version 1.2.0\nHere is my code , it is very simple:\n# -*- coding:utf-8 -*-\nimport scrapy\nfrom scrapy.spiders import CrawlSpider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\nfrom StudyDemo.items import StudydemoItem\nimport urllib\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nclass StudyDemoSpider(CrawlSpider):\n    name = 'StudyDemo'\n    start_urls = ['http://www.qiushibaike.com/hot/page/1']\n\n    def parse(self, response):\n        soup = BeautifulSoup(response.text)\n        articles = soup.select(\".article\")\n        for article in articles:\n            item = StudydemoItem()\n            author = article.select(\".author a h2\")[0].get_text()\n            content = article.select(\".content span\")[0].get_text()\n            like = article.select(\".stats-vote i\")[0].get_text()\n            comment = article.select(\".stats-comments i\")[0].get_text()\n            item['author'] = author\n            item['content'] = content\n            item['likeNum'] = like\n            item['commentNum'] = comment\n            yield item\nIn the setting.py:\nFEED_URI=u'Scrapy/qiubai.csv'\nFEED_FORMAT='CSV'\nAnd I run scrapy crawl StudyDemo\nIt was success, and create a csv file in Scrapy dir, but the file is ZeroByte.\nAnd I tryed run scrapy crawl StudyDemo -o qiubai.json it was the same as csv, it create a qiubai.json file ,but the file is ZeroByte too .\nAnd I can get All The Result success in Terminal.\nI don't kown what's the matter with my Code.\nNeed some help!", "issue_status": "Closed", "issue_reporting_time": "2016-10-10T02:45:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "791": {"issue_url": "https://github.com/scrapy/scrapy/issues/2316", "issue_id": "#2316", "issue_summary": "Export to 2 locations", "issue_description": "rolele commented on Oct 8, 2016\nI am using scrapyd and a FEED_URI to save in a ftp endpoint.\nEverything works fine but I would like the items to be also saved locally using the default scrapyd file location: eg. items='file:///var/lib/scrapyd/items/properties/distr/ca3636168d4711e69d740242ac110007.jl'\nRight now, all the links to the items on the scrapyd webUI return a 404.", "issue_status": "Closed", "issue_reporting_time": "2016-10-08T11:28:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "792": {"issue_url": "https://github.com/scrapy/scrapy/issues/2312", "issue_id": "#2312", "issue_summary": "Does scrapy on Windows/Python3.5+ support the FTP download handler yet?", "issue_description": "zeluspudding commented on Oct 7, 2016\nAccording to this ~1yr old update, scrapy still didn't support the FTP download handler on windows. Does it support it now?", "issue_status": "Closed", "issue_reporting_time": "2016-10-06T21:42:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "793": {"issue_url": "https://github.com/scrapy/scrapy/issues/2311", "issue_id": "#2311", "issue_summary": "SSL error", "issue_description": "ricoxor commented on Oct 7, 2016\nI have lot of errors with SSL websites.\nFor exemple, when I call : scrapy shell https://subscribe.wsj.com/printpack/\nI have this error :\n2016-10-06 22:15:40 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6043\n2016-10-06 22:15:40 [scrapy] INFO: Spider opened\n2016-10-06 22:15:40 [scrapy] DEBUG: Retrying <GET https://subscribe.wsj.com/printpack/> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]\n2016-10-06 22:15:40 [scrapy] DEBUG: Retrying <GET https://subscribe.wsj.com/printpack/> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]\n2016-10-06 22:15:40 [scrapy] DEBUG: Gave up retrying <GET https://subscribe.wsj.com/printpack/> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/shell.py\", line 71, in run\n    shell.start(url=url)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 47, in start\n    self.fetch(url, spider)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 112, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<string>\", line 2, in raiseException\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]\nHow fix that ?", "issue_status": "Closed", "issue_reporting_time": "2016-10-06T20:17:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "794": {"issue_url": "https://github.com/scrapy/scrapy/issues/2310", "issue_id": "#2310", "issue_summary": "Scrapy don't stop with rabbitMQ", "issue_description": "ricoxor commented on Oct 6, 2016\nI'm using RabbitMQ with Scrapyand when the spider finished, the Job on Scrapyd stay on Running status.\nCan someone help me to fix that ?\n2016-10-06 14:05:27 [scrapy] INFO: Scrapy 1.2.0 started (bot: Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html))\n2016-10-06 14:05:27 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'dirbot.spiders', 'FEED_URI': 'file:///home/CrawlerNDD/items/default/expired_one/2ac82f2a8bbd11e687360cc47ac3ab56.jl', 'LOG_LEVEL': 'INFO', 'DUPEFILTER_CLASS': 'dirbot.custom_filters.BLOOMDupeFilter', 'SPIDER_MODULES': ['dirbot.spiders'], 'BOT_NAME': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)', 'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.FifoMemoryQueue', 'LOG_FILE': 'logs/default/expired_one/2ac82f2a8bbd11e687360cc47ac3ab56.log', 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleFifoDiskQueue'}\n2016-10-06 14:05:27 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-10-06 14:05:27 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-10-06 14:05:27 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-10-06 14:05:27 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-10-06 14:05:27 [scrapy] INFO: Spider opened\n2016-10-06 14:05:27 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-10-06 14:05:28 [root] WARNING: domain\n2016-10-06 14:05:28 [root] WARNING: domain\n2016-10-06 14:05:28 [root] WARNING: domain\n2016-10-06 14:05:28 [root] WARNING: domain\n2016-10-06 14:05:28 [root] WARNING: domain\n2016-10-06 14:05:28 [root] WARNING: domain\n2016-10-06 14:05:28 [root] WARNING: domain\n2016-10-06 14:05:28 [root] WARNING: domain\n2016-10-06 14:05:48 [root] WARNING: domain\n2016-10-06 14:05:48 [root] WARNING: domain\n2016-10-06 14:05:48 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 162839,\n 'downloader/request_count': 525,\n 'downloader/request_method_count/GET': 525,\n 'downloader/response_bytes': 12053814,\n 'downloader/response_count': 525,\n 'downloader/response_status_count/200': 486,\n 'downloader/response_status_count/301': 5,\n 'downloader/response_status_count/404': 34,\n 'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2016, 10, 6, 12, 5, 48, 212386),\n 'log_count/INFO': 8,\n 'log_count/WARNING': 242,\n 'offsite/domains': 3814,\n 'offsite/filtered': 10063,\n 'request_depth_max': 4,\n 'response_received_count': 520,\n 'scheduler/dequeued': 525,\n 'scheduler/dequeued/memory': 525,\n 'scheduler/enqueued': 2263,\n 'scheduler/enqueued/memory': 2263,\n 'start_time': datetime.datetime(2016, 10, 6, 12, 5, 27, 717486)}\n2016-10-06 14:05:48 [scrapy] INFO: Spider closed (shutdown)\n2016-10-06 14:05:48 [root] WARNING: domain\n2016-10-06 14:05:48 [root] WARNING: domain\n2016-10-06 14:05:48 [root] WARNING: domain", "issue_status": "Closed", "issue_reporting_time": "2016-10-06T13:15:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "795": {"issue_url": "https://github.com/scrapy/scrapy/issues/2304", "issue_id": "#2304", "issue_summary": "TypeError: normalize() argument 2 must be unicode, not str", "issue_description": "ricoxor commented on Oct 5, 2016\nMy crawler do lot of errors like this :\n2016-10-04 21:21:24 [scrapy] ERROR: Spider error processing <GET http://xxxx.com/> (referer:)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spiders/crawl.py\", line 78, in _parse_response\n    for request_or_item in self._requests_to_follow(response):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spiders/crawl.py\", line 56, in _requests_to_follow\n    links = [lnk for lnk in rule.link_extractor.extract_links(response)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/linkextractors/lxmlhtml.py\", line 111, in extract_links\n    all_links.extend(self._process_links(links))\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/linkextractors/__init__.py\", line 104, in _process_links\n    link.url = canonicalize_url(urlparse(link.url))\n  File \"/usr/local/lib/python2.7/dist-packages/w3lib/url.py\", line 354, in canonicalize_url\n    parse_url(url), encoding=encoding)\n  File \"/usr/local/lib/python2.7/dist-packages/w3lib/url.py\", line 298, in _safe_ParseResult\n    netloc = parts.netloc.encode('idna')\n  File \"/usr/lib/python2.7/encodings/idna.py\", line 164, in encode\n    result.append(ToASCII(label))\n  File \"/usr/lib/python2.7/encodings/idna.py\", line 76, in ToASCII\n    label = nameprep(label)\n  File \"/usr/lib/python2.7/encodings/idna.py\", line 21, in nameprep\n    newlabel.append(stringprep.map_table_b2(c))\n  File \"/usr/lib/python2.7/stringprep.py\", line 197, in map_table_b2\n    b = unicodedata.normalize(\"NFKC\", al)\nTypeError: normalize() argument 2 must be unicode, not str\nCan someone help me to fix that ?\nThank's", "issue_status": "Closed", "issue_reporting_time": "2016-10-04T19:23:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "796": {"issue_url": "https://github.com/scrapy/scrapy/issues/2301", "issue_id": "#2301", "issue_summary": "AttributeError when using cookiejar in request.meta attribute", "issue_description": "songshixuan commented on Oct 3, 2016\nFile \"/usr/lib/python3.4/http/cookiejar.py\", line 1103, in return_ok_secure\nif cookie.secure and request.type != \"https\":\nAttributeError: 'WrappedRequest' object has no attribute 'type'\nThen I annotated the line, the error disappear. The function cause the exception is:\ndef return_ok_secure(self, cookie, request):\n#if cookie.secure and request.type != \"https\":\n# _debug(\" secure cookie with non-secure request\")\n# return False\nreturn True", "issue_status": "Closed", "issue_reporting_time": "2016-10-03T13:41:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "797": {"issue_url": "https://github.com/scrapy/scrapy/issues/2299", "issue_id": "#2299", "issue_summary": "Genspider command prepends an inconvenient 'www' in start_urls", "issue_description": "Member\nstummjr commented on Oct 1, 2016\nAlmost every time I use the genspider command, I end up removing the www prefix from the value that this command generates for start_urls.\nFor example, this command:\n$ scrapy genspider quotes quotes.toscrape.com\ngenerates this spider:\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    allowed_domains = [\"quotes.toscrape.com\"]\n    start_urls = (\n        'http://www.quotes.toscrape.com/',\n    )\n    ...\nIMHO, there's no need to add www to the start_urls, because it might be more annoying than helpful to people using genspider.\nThoughts on it?", "issue_status": "Closed", "issue_reporting_time": "2016-10-01T12:31:43Z", "fixed_by": "#2307", "pull_request_summary": "genspider: removing www. from starturl templates", "pull_request_description": "Member\neLRuLL commented on Oct 5, 2016 \u2022\nedited\nfixes #2299", "pull_request_status": "Merged", "issue_fixed_time": "2016-10-12T13:32:18Z", "files_changed": [["4", "scrapy/templates/spiders/basic.tmpl"], ["2", "scrapy/templates/spiders/crawl.tmpl"], ["2", "scrapy/templates/spiders/csvfeed.tmpl"], ["2", "scrapy/templates/spiders/xmlfeed.tmpl"]]}, "798": {"issue_url": "https://github.com/scrapy/scrapy/issues/2291", "issue_id": "#2291", "issue_summary": "CrawlSpider with Rules not working", "issue_description": "sixfingers76 commented on Sep 28, 2016 \u2022\nedited by redapple\nHi,\nI am using Scrapy version 1.1.2. It is somenthing weird, because depending on the Rules configuration, the callback function is called or not...\nMy rules:\nrules = (\n        Rule(LinkExtractor(allow=('.*balsamo'))), # get category page\n        Rule(LinkExtractor(allow=(\".*\\/\\d+.*\"),restrict_xpaths=('//*[@id=\"center_column\"]/div[3]')), callback='category'), # get product pages links in the category page\n)\nIf I remove the restrict_xpaths filter, the callback function is called, but when I use restrict_xpaths, the callback function is never called, even when the ouput shows the right crawled pages as \"crawled\". This is the output resulting from the above rules.\n2016-09-28 19:00:57 [scrapy] DEBUG: Crawled (200) GET http://www.example.es/robots.txt (referer: None)\n2016-09-28 19:00:57 [scrapy] DEBUG: Crawled (200) GET http://www.example.es (referer: None)\n2016-09-28 19:00:58 [scrapy] DEBUG: Crawled (200) GET http://www.example.es/75-balsamo (referer: http://www.example.es)\n2016-09-28 19:00:58 [scrapy] DEBUG: Filtered duplicate request: GET http://www.example.es/75-balsamo - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n2016-09-28 19:01:01 [scrapy] DEBUG: Crawled (200) GET http://www.example.es/varios/233-relajante-balsamo-aromatico-dream-6-g-ha-tha.html (referer: http://www.example.es/75-balsamo)\n2016-09-28 19:01:01 [scrapy] DEBUG: Crawled (200) GET http://www.example.es/balsamo/459-balsamo-de-tigre-rojo-21-ml-varios.html (referer: http://www.example.es/75-balsamo)\n2016-09-28 19:01:01 [scrapy] DEBUG: Crawled (200) GET http://www.example.es/varios/235-refrescante-bio-balsamo-hierbas-classic-6-g-ha-tha.html (referer: http://www.example.es/75-balsamo)\n2016-09-28 19:01:01 [scrapy] DEBUG: Crawled (200) GET http://www.example.es/varios/234-tranquilizador-balsamo-aromatico-balance-ha-tha.html (referer: http://www.example.es/75-balsamo)\n2016-09-28 19:01:01 [scrapy] DEBUG: Crawled (200) GET http://www.example.es/balsamo/460-balsamo-de-tigre-blanco-21-ml-varios.html (referer: http://www.example.es/75-balsamo)\n2016-09-28 19:01:01 [scrapy] INFO: Closing spider (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-09-28T17:21:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "799": {"issue_url": "https://github.com/scrapy/scrapy/issues/2290", "issue_id": "#2290", "issue_summary": "Scrapy should follow redirects on scrapy shell by default", "issue_description": "Member\neliasdorneles commented on Sep 28, 2016 \u2022\nedited by redapple\nScrapy shell doesn't follow redirects when you do fetch('http://google.com'), but it does if you do fetch(scrapy.Request('http://google.com'))`.\nI realize this is a historic behavior, but I'd argue that it breaks the expectations for most users, since the most common expectation is that Scrapy would try to do what a browser would do, so I think we should change it.\nIn my opinion, not following the redirect should be the exceptional behavior, and so that's what should require the user to build a scrapy.Request object.\nI think ideally, we would be able to pass a keyword argument to fetch, like:\nfetch('http://google.com', redirect=False)\nAnd only for that case it would set handle_httpstatus_all to True as explained by @redapple here: #2177 (comment)\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2016-09-28T15:32:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "800": {"issue_url": "https://github.com/scrapy/scrapy/issues/2289", "issue_id": "#2289", "issue_summary": "Priority value, Higher or Lower?", "issue_description": "pykler commented on Sep 27, 2016 \u2022\nedited by redapple\nThere is a contradiction in the documentation.\nhttp://doc.scrapy.org/en/latest/topics/settings.html?highlight=priority#depth-priority\nvs\nhttp://doc.scrapy.org/en/latest/topics/request-response.html?highlight=priority#scrapy.http.Request\nThe first says a positive number will mean lower priority. The latter says a higher number means a higher priority. So what is it?", "issue_status": "Closed", "issue_reporting_time": "2016-09-27T14:16:10Z", "fixed_by": "#3670", "pull_request_summary": "Clarify the documentation of DEPTH_PRIORITY further", "pull_request_description": "Member\nGallaecio commented on Mar 8, 2019\nFixes #2289", "pull_request_status": "Merged", "issue_fixed_time": "2019-03-14T17:10:43Z", "files_changed": [["16", "docs/topics/settings.rst"]]}, "801": {"issue_url": "https://github.com/scrapy/scrapy/issues/2288", "issue_id": "#2288", "issue_summary": "scrapy spider finished crawling by the wayside", "issue_description": "Leocodefocus commented on Sep 27, 2016 \u2022\nedited by redapple\nI would like to use my spider to crawl about 80000 pages and parse their information,but it finished without error messages,here is my code:\n# -*- coding: utf-8 -*-\nfrom cved\n\netails.items import testItem\nimport scrapy\nimport urlparse\nfrom scrapy.selector import Selector\nfrom scrapy.linkextractors import LinkExtractor\nimport re\nimport string\nimport pymongo\nimport dbconfig\nimport requests\nfrom scrapy import log\nfrom bs4 import BeautifulSoup\nimport extralib\nimport datetime\nimport threading\nclass cvedetails(scrapy.Spider):\n    #3217\n    name = \"cvedetails\"\n    allowed_domains = ['www.cvedetails.com']\n    #start_urls = ['http://www.cvedetails.com/vendor-search.php?search=Omron']#,'http://www.cvedetails.com/vendor-search.php?search=NI']\n    link_extract = LinkExtractor()\n    def start_requests(self):\n        self.conn = self.connDB()\n        r = requests.get(\"http://www.cvedetails.com/browse-by-date.php\")\n        bsObj = BeautifulSoup(r.text)\n        date_trs = bsObj.find(\"table\",{\"id\":\"maintable\"}).find(\"div\",{\"id\":\"contentdiv\"}).find(\"table\",{\"class\":\"stats\"}).findAll(\"tr\",{\"onmouseover\":\"this.style.background='#F5F4B8'\"})\n        for date_tr in date_trs:\n            tds = date_tr.findAll('td')\n            for td_s in tds:\n                a_href = td_s.find('a')\n                if a_href != None:\n                    if 'href' in a_href.attrs:\n                        if re.compile(r'^(/vulnerability-list/year-)(\\d){4}/month-(\\d){1,2}/[A-Za-z]+(\\.html)$').match(a_href['href']):\n                            url = urlparse.urljoin(\"http://www.cvedetails.com/\",a_href['href'])\n                            print url\n                            yield scrapy.Request(url, callback=self.parse,\n                                                errback=self.errback_httpbin,\n                                                dont_filter=True)\n\n    def errback_httpbin(self,response):\n        self.logger.info('Got failed response from {}'.format(response.url))\n    def connDB(self):\n        connection=pymongo.MongoClient(\n        dbconfig.MONGODB_SERVER,\n        dbconfig.MONGODB_PORT\n        )\n        db=connection[dbconfig.MONGODB_DB]\n        return db[dbconfig.MONGODB_COLLECTION_CVEDETAILSHTML]\n\n    def insertHtml(self,link,html):\n        htmls = {\"link\":link,\"html\":html}\n        count = self.conn.find({\"link\":htmls['link']}).count()\n        if count < 1:\n            self.conn.insert(dict(htmls))\n    def parse(self,response):\n        bsObj = BeautifulSoup(response.body)\n        tdd = bsObj.find(\"table\",{\"id\":\"maintable\"}).findAll(\"tr\")[1].find(\"td\",{\"align\":\"left\",\"valign\":\"top\"})\n        trs = tdd.find(\"div\",{\"id\":\"contentdiv\"}).find(\"div\",{\"id\":\"searchresults\"}).find(\"table\",{\"class\":\"searchresults sortable\",\"id\":\"vulnslisttable\"}).findAll(\"tr\",{\"class\":\"srrowns\"})\n        for s_tr in trs:\n            a_href = s_tr.find(\"td\",{\"class\":\"num\"}).next_sibling.next_sibling.find(\"a\")\n            url = urlparse.urljoin(\"http://www.cvedetails.com/\",a_href['href'])\n            yield scrapy.Request(url, callback=self.parse_item,\n                                            errback=self.errback_httpbin,\n                                            dont_filter=True)\n\n        if re.compile(r'.*(/vulnerability-list/year-)(\\d){4}/month-(\\d){1,2}/[A-Za-z]+(\\.html)$').match(response.url):\n            all_a = tdd.find(\"div\",{\"id\":\"contentdiv\"}).find(\"div\",{\"id\":\"pagingb\",\"class\":\"paging\"}).findAll(lambda tag: 'href' in tag.attrs and tag['title'] != 'Go to page 1')\n            for aa in all_a:\n                url = urlparse.urljoin(\"http://www.cvedetails.com/\",aa['href'])\n                yield scrapy.Request(url, callback=self.parse,\n                                            errback=self.errback_httpbin,\n                                            dont_filter=True)\n\n    def parse_item(self,response):\n            cve_item = testItem()\n            cve_item = extralib.initialfunc(cve_item)\n            cve_item['link'] = response.url\n\n            t = threading.Thread(target=self.insertHtml,args={response.url,response.body})\n            t.start()\n            all_text = Selector(text=response.body)\n            item_text = all_text.xpath(\"//table[@id='maintable']/tr[2]/td[2]/div[@id='contentdiv']/table/tr[1]/td[@id='cvedetails']\").extract()[0]\n            #parse description\n            description = Selector(text=item_text).xpath(\"//td/div[@class='cvedetailssummary']/text()\").extract()[0]\n            cve_item['description'].append(description)\n            # parse date\n            date_note = Selector(text=item_text).xpath(\"//td/div[@class='cvedetailssummary']/span[@class='datenote']/text()\").extract()[0]\n            cve_item['publish_time'] = extralib.parseTime(date_note.strip().replace(' ','').split('\\t')[0].split(':')[1])\n            cve_item['commit_time'] = extralib.parseTime(date_note.strip().replace(' ','').split('\\t')[1].split(':')[1])\n            cve_item['found_time'] = cve_item['commit_time']\n            cve_item['update_time'] = datetime.datetime.now()\n            # parse items\n            vul_cvss_vultype = Selector(text=item_text).xpath(\"//td/table[2]/tr/td[1]/table[@id='cvssscorestable']/tr\").extract()\n            for tr in vul_cvss_vultype:\n                tr_name = Selector(text=tr).xpath(\"//tr/th/text()\").extract()[0].strip()\n                if tr_name == \"CVSS Score\":\n                    cve_item['cvss_score'] = Selector(text=tr).xpath(\"//tr/td/div[@class='cvssbox']/text()\").extract()[0]\n                elif tr_name == \"Confidentiality Impact\":\n                    cve_item['confidentiality_impact'] = Selector(text=tr).xpath(\"//tr/td/span[1]/text()\").extract()[0].strip()\n                elif tr_name == \"Integrity Impact\":\n                    cve_item['integrity_impact'] = Selector(text=tr).xpath(\"//tr/td/span[1]/text()\").extract()[0].strip()\n                elif tr_name == \"Availability Impact\":\n                    cve_item['availability_impact'] = Selector(text=tr).xpath(\"//tr/td/span[1]/text()\").extract()[0].strip()\n                elif tr_name == \"Access Complexity\":\n                    cve_item['access_complexity'] = Selector(text=tr).xpath(\"//tr/td/span[1]/text()\").extract()[0].strip()\n                elif tr_name == \"Authentication\":\n                    cve_item['authentication'] = Selector(text=tr).xpath(\"//tr/td/span[1]/text()\").extract()[0].strip()\n                elif tr_name == \"Gained Access\":\n                    cve_item['gained_access'] = Selector(text=tr).xpath(\"//tr/td/span[1]/text()\").extract()[0].strip()\n                elif tr_name == \"Vulnerability Type(s)\":\n                    cve_item['vul_type'] = Selector(text=tr).xpath(\"//tr/td/span/text()\").extract()\n            # parse products\n            pro_trs = Selector(text=item_text).xpath(\"//td/table[@id='vulnprodstable']/tr\").extract()\n            products = []\n            for tr in pro_trs:\n                try:\n                    tr_num = Selector(text=tr).xpath(\"//tr/td[@class='num']/text()\").extract()[0].strip()\n                    if tr_num != \"#\":\n                        product = {}\n                        product['type']=Selector(text=tr).xpath(\"//tr/td[2]/text()\").extract()[0].strip()\n                        product['vendor']=Selector(text=tr).xpath(\"//tr/td[3]/a/text()\").extract()[0].strip()\n                        product['name']=Selector(text=tr).xpath(\"//tr/td[4]/a/text()\").extract()[0].strip()\n                        product['version']=Selector(text=tr).xpath(\"//tr/td[5]/text()\").extract()[0].strip()\n                        product['update']=Selector(text=tr).xpath(\"//tr/td[6]/text()\").extract()[0].strip()\n                        products.append(product)\n                except:\n                    continue\n            cve_item['products']=products\n            # parse reference\n            references = Selector(text=item_text).xpath(\"//td/table[@id='vulnrefstable']/tr/td/a/@href\").extract()\n            cve_item['reference']=references\n            cve_item['source_id']=response.url.split('/')[-2]\n            yield cve_item", "issue_status": "Closed", "issue_reporting_time": "2016-09-27T10:55:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "802": {"issue_url": "https://github.com/scrapy/scrapy/issues/2286", "issue_id": "#2286", "issue_summary": "scrapy parse does not run start_requests hook", "issue_description": "rolele commented on Sep 24, 2016\nI would like to try my spider but I am redefining the start-Requests method to route the request to my splash server to run the js\n    def start_requests(self):\n        for url in self.start_urls:\n            yield SplashRequest(url, self.parse, args={'wait': 0.5})\nbut parse does not run this hook\nI tried adding the callback option -c but it does not do\nscrapy parse --pipelines -c start_requests -c parse -d 2 -v --spider aaa http://example.com\nHow can I run parse (to test a page) while being able to run the js hook?", "issue_status": "Closed", "issue_reporting_time": "2016-09-24T01:51:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "803": {"issue_url": "https://github.com/scrapy/scrapy/issues/2283", "issue_id": "#2283", "issue_summary": "Add note that CrawlerProcess.crawl() needs a crawler or spider class, not a spider instance", "issue_description": "Contributor\nredapple commented on Sep 23, 2016 \u2022\nedited\nEven if it's in the docs,\ncrawler_or_spidercls (Crawler instance, Spider subclass or string) \u2013 already created crawler, or a spider class or spider\u2019s name inside the project to create it\nusers still miss sometimes that .crawl() expects a Spider class (or a crawler) but not a Spider instance.\ne.g. http://stackoverflow.com/q/39639568\nI overlooked the fact that the class itself has to be passed as an argument, and not an instance of that class.\nThis is quite subtle on the eye but makes all the difference\nThis ticket is to make it a bit more prominent.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-09-23T09:13:23Z", "fixed_by": "#3610", "pull_request_summary": "[MRG+1] Check that spidercls arguments in scrapy.crawler classes are not spid\u2026", "pull_request_description": "Member\nGallaecio commented on Feb 1, 2019\n\u2026er objects\nI think this is a better approach to fix #2283. Instead of trying to make the documentation more obvious, have the code fail gracefully with a useful error message.\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2019-03-15T07:25:16Z", "files_changed": [["13", "scrapy/crawler.py"], ["13", "tests/test_crawler.py"], ["2", "tests/test_downloadermiddleware_httpproxy.py"]]}, "804": {"issue_url": "https://github.com/scrapy/scrapy/issues/2278", "issue_id": "#2278", "issue_summary": "URLs vs Requests in at Architecture overview page", "issue_description": "Member\nkmike commented on Sep 22, 2016\nI think the architecture overview in Scrapy master docs doesn't explain correctly the difference between URLs and Requests. It says that spider returns URLs, and Engine converts them to Requests, but this is confusing - spider returns Requests. There are also \"URLs\" on the chart, but Scrapy passes Request instances almost everywhere, not URLs.\nOld explanation (before #2165) didn't have this issue; as it is a regression I added '1.2' milestone.", "issue_status": "Closed", "issue_reporting_time": "2016-09-22T14:18:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "805": {"issue_url": "https://github.com/scrapy/scrapy/issues/2277", "issue_id": "#2277", "issue_summary": "Python3| FTP download handler", "issue_description": "mateuszdargacz commented on Sep 22, 2016 \u2022\nedited by redapple\nIs ftp download handler on roadmap? what are the issues related with porting it to Py3.x?", "issue_status": "Closed", "issue_reporting_time": "2016-09-22T10:10:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "806": {"issue_url": "https://github.com/scrapy/scrapy/issues/2276", "issue_id": "#2276", "issue_summary": "can LinkExtractor extract scrapy.link with node info", "issue_description": "zhengbomo commented on Sep 22, 2016\nthe html is like below, i want to extract the link /example/category/pg{page}/, but the scrapy.link does not contains the node info(currentPage and totalPage), how can i extract the link with the node info\n<div class=\"page-box\">\n     <div page-url=\"/example/category/pg{page}/\"\n             totalPage=\"35\"\n             currentPage=\"1\"             \n     </div>\n</div>", "issue_status": "Closed", "issue_reporting_time": "2016-09-22T02:37:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "807": {"issue_url": "https://github.com/scrapy/scrapy/issues/2274", "issue_id": "#2274", "issue_summary": "tool to dump the current project settings after bootstrap", "issue_description": "casertap commented on Sep 21, 2016\nI think it would be great if scrapy embedded a tool like scrapy parseor scrapy shell to dump all the setting of the current project after bootstrap (at the time it reach the spider code).\nI think this tool could be used in the future to post relevant information while opening an issue on github.\nIs there anything like it already implemented in scrapy?", "issue_status": "Closed", "issue_reporting_time": "2016-09-21T12:13:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "808": {"issue_url": "https://github.com/scrapy/scrapy/issues/2272", "issue_id": "#2272", "issue_summary": "Accept single values for \"to\" and \"cc\" arguments when sending mails", "issue_description": "Contributor\nredapple commented on Sep 20, 2016 \u2022\nedited\nFollow up to #2271 (comment)\nWhat about checking for strings and raising an error?\nPassing a string is an easy mistake to make.\nOr maybe we can support passing a single string.", "issue_status": "Closed", "issue_reporting_time": "2016-09-20T15:53:34Z", "fixed_by": "#2331", "pull_request_summary": "[MRG+1] Fixes issue #2272 using arg_to_iter() to wrap single values and list() to\u2026", "pull_request_description": "Contributor\nmoisesguimaraes commented on Oct 18, 2016 \u2022\nedited by redapple\n(fixes #2272) using arg_to_iter() to wrap single values and list() to avoid consuming from generators.", "pull_request_status": "Merged", "issue_fixed_time": "2016-12-07T15:08:18Z", "files_changed": [["10", "docs/topics/email.rst"], ["6", "scrapy/mail.py"], ["18", "tests/test_mail.py"]]}, "809": {"issue_url": "https://github.com/scrapy/scrapy/issues/2265", "issue_id": "#2265", "issue_summary": "Support .format() compatible FEED_URI", "issue_description": "Contributor\nredapple commented on Sep 19, 2016\nI wanted to add a new %(date)s for FEED_URI but then I thought we may as well upgrade the formatting support to the more modern format string syntax.\nWhat do you think?", "issue_status": "Closed", "issue_reporting_time": "2016-09-19T15:42:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "810": {"issue_url": "https://github.com/scrapy/scrapy/issues/2264", "issue_id": "#2264", "issue_summary": "Idea: warn users when trying to use TextResponse functionality with plain Response", "issue_description": "Member\neliasdorneles commented on Sep 19, 2016 \u2022\nedited\nCurrently, if we try to use TextResponse functionality like response.text or css()/xpath() methods with a plain Response (e.g. in case of binary content), we get an AttributeError:\n>>> response.css\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-1-7d6e256164d4> in <module>()\n----> 1 response.css\n\nAttributeError: 'Response' object has no attribute 'css'\n>>> response.xpath\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-2-4f61f6e9fc6e> in <module>()\n----> 1 response.xpath\n\nAttributeError: 'Response' object has no attribute 'xpath'\n>>> response.text\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-3-be6a4a00df5e> in <module>()\n----> 1 response.text\n\nAttributeError: 'Response' object has no attribute 'text'\nWould it make sense to add a few methods/properties to explain what's going on for new users?\nI was thinking instead of AttributeError, a better behavior could be a ValueError with a message giving a bit more context.\nSo, in plain Response, we could have:\ndef css(self, *args, **kw):\n    raise ValueError('Response content is not text')\n\ndef xpath(self, *args, **kw):\n    raise ValueError('Response content is not text')\n\n@property\ndef text(self, *args, **kw):\n    raise ValueError('Response content is not text')\nThis would be nice, because we'd had to explain fewer things when teaching people about responses and also about using .css and .xpath methods.\nWhat do you think?", "issue_status": "Closed", "issue_reporting_time": "2016-09-19T14:54:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "811": {"issue_url": "https://github.com/scrapy/scrapy/issues/2262", "issue_id": "#2262", "issue_summary": "Better explain middleware orders and processing directions", "issue_description": "Contributor\nredapple commented on Sep 19, 2016\nSee #2202 (comment)", "issue_status": "Closed", "issue_reporting_time": "2016-09-19T10:30:22Z", "fixed_by": "#2329", "pull_request_summary": "[MRG+1] Better explain middleware orders and processing directions", "pull_request_description": "Contributor\njosericardo commented on Oct 18, 2016\nIt should fix #2262", "pull_request_status": "Merged", "issue_fixed_time": "2016-10-20T07:04:19Z", "files_changed": [["15", "docs/topics/architecture.rst"], ["6", "docs/topics/downloader-middleware.rst"], ["7", "docs/topics/spider-middleware.rst"]]}, "812": {"issue_url": "https://github.com/scrapy/scrapy/issues/2261", "issue_id": "#2261", "issue_summary": "[Python3]When I export data to a JSON file, How can I change the codec to UTF-8?", "issue_description": "LokiSharp commented on Sep 18, 2016\nWhen I use scrapy crawl xxx -o xxx.json -t json to export a JSON file, it just give me some Unicode like u'H\\ufffd\\ufffdftsitz'. It defaults to export with Unicode, And I can't change it.\nI found this method [http://stackoverflow.com/questions/9181214/scrapy-text-encoding](scrape text encoding), But it does not work in Scrapy with Python3.", "issue_status": "Closed", "issue_reporting_time": "2016-09-18T16:07:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "813": {"issue_url": "https://github.com/scrapy/scrapy/issues/2259", "issue_id": "#2259", "issue_summary": "Bad dict key in tutorial", "issue_description": "Contributor\nwaynelovely commented on Sep 17, 2016\nThe use of 'title' as a dict key results in an error during the tutorial.\nfrom tutorial.items import QuoteItem\nitem = QuoteItem()\nitem['text'] = 'Some random quote'\nitem['title']\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/item.py\", line 56, in getitem\nreturn self._values[key]\nKeyError: 'title'", "issue_status": "Closed", "issue_reporting_time": "2016-09-17T11:53:39Z", "fixed_by": "#2260", "pull_request_summary": "Fix a dict key in the tutorial", "pull_request_description": "Contributor\nwaynelovely commented on Sep 17, 2016\nfixes #2259\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2016-09-17T12:19:21Z", "files_changed": [["2", "docs/intro/tutorial.rst"]]}, "814": {"issue_url": "https://github.com/scrapy/scrapy/issues/2255", "issue_id": "#2255", "issue_summary": "Mention StackOverflow as a support channel", "issue_description": "Contributor\nredapple commented on Sep 16, 2016\nStackOverflow is missing in http://doc.scrapy.org/en/latest/index.html#getting-help\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-09-16T14:27:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "815": {"issue_url": "https://github.com/scrapy/scrapy/issues/2254", "issue_id": "#2254", "issue_summary": "[Python3.5.2][Scrapy 1.1.2] ImportError: No module named 'sgmllib'", "issue_description": "LokiSharp commented on Sep 16, 2016\nThe sgmllib module has been removed in Python 3. What can i do to use the SgmlLinkExtractor?\n/Users/LokiSharp/wikiSpider/wikiSpider/spiders/articleSpider.py:4: ScrapyDeprecationWarning: Module `scrapy.contrib.linkextractors` is deprecated, use `scrapy.linkextractors` instead\n  from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n/Users/LokiSharp/wikiSpider/wikiSpider/spiders/articleSpider.py:4: ScrapyDeprecationWarning: Module `scrapy.contrib.linkextractors.sgml` is deprecated, use `scrapy.linkextractors.sgml` instead\n  from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/cmdline.py\", line 141, in execute\n    cmd.crawler_process = CrawlerProcess(settings)\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/crawler.py\", line 238, in __init__\n    super(CrawlerProcess, self).__init__(settings)\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/crawler.py\", line 129, in __init__\n    self.spider_loader = _get_spider_loader(settings)\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/crawler.py\", line 325, in _get_spider_loader\n    return loader_cls.from_settings(settings.frozencopy())\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/spiderloader.py\", line 33, in from_settings\n    return cls(settings)\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/spiderloader.py\", line 20, in __init__\n    self._load_all_spiders()\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/spiderloader.py\", line 28, in _load_all_spiders\n    for module in walk_modules(name):\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/utils/misc.py\", line 71, in walk_modules\n    submod = import_module(fullpath)\n  File \"/usr/local/Cellar/python3/3.5.2_1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 665, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n  File \"/Users/LokiSharp/wikiSpider/wikiSpider/spiders/articleSpider.py\", line 4, in <module>\n    from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/contrib/linkextractors/sgml.py\", line 7, in <module>\n    from scrapy.linkextractors.sgml import *\n  File \"/usr/local/lib/python3.5/site-packages/scrapy/linkextractors/sgml.py\", line 6, in <module>\n    from sgmllib import SGMLParser\nImportError: No module named 'sgmllib'", "issue_status": "Closed", "issue_reporting_time": "2016-09-16T13:54:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "816": {"issue_url": "https://github.com/scrapy/scrapy/issues/2253", "issue_id": "#2253", "issue_summary": "Document how to customize path for written files with media pipelines", "issue_description": "Contributor\nredapple commented on Sep 16, 2016\nMotivation: #923\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2016-09-16T10:42:50Z", "fixed_by": "#3609", "pull_request_summary": "Document FilesPipeline.file_path and ImagesPipeline.file_path", "pull_request_description": "Member\nGallaecio commented on Feb 1, 2019 \u2022\nedited\nFixes #2253\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-04T17:05:47Z", "files_changed": [["60", "docs/topics/media-pipeline.rst"]]}, "817": {"issue_url": "https://github.com/scrapy/scrapy/issues/2251", "issue_id": "#2251", "issue_summary": "[Python 2] scrapy crawl does not work with non-ASCII spider names", "issue_description": "Contributor\nredapple commented on Sep 16, 2016 \u2022\nedited\nI don't know if this is an already documented issue, but when a project has a spider with a non-ASCII name, scrapy crawl fails with error KeyError: 'Spider not found: ara\\xc3\\xb1a'\nThe following fails with Python 2 (but works with Python 3):\n$ scrapy version -v\nScrapy    : 1.1.2\nlxml      : 3.6.4.0\nlibxml2   : 2.9.4\nTwisted   : 16.4.0\nPython    : 2.7.11+ (default, Apr 17 2016, 14:00:29) - [GCC 5.3.1 20160413]\npyOpenSSL : 16.1.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)\nPlatform  : Linux-4.4.0-36-generic-x86_64-with-Ubuntu-16.04-xenial\n\n\n$ scrapy list\nara\u00f1a\n\n$ cat unicodename/spiders/example.py\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = u\"ara\u00f1a\"\n    allowed_domains = [\"example.com\"]\n    start_urls = (\n        'http://www.example.com/',\n    )\n\n    def parse(self, response):\n        yield {\"url\": response.url}\n\n$ scrapy crawl ara\u00f1a\n2016-09-15 22:46:35 [scrapy] INFO: Scrapy 1.1.2 started (bot: unicodename)\n2016-09-15 22:46:35 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'unicodename.spiders', 'SPIDER_MODULES': ['unicodename.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'unicodename'}\nTraceback (most recent call last):\n  File \"/home/paul/.virtualenvs/scrapy11.py2/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/home/paul/.virtualenvs/scrapy11.py2/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/paul/.virtualenvs/scrapy11.py2/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/home/paul/.virtualenvs/scrapy11.py2/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"/home/paul/.virtualenvs/scrapy11.py2/local/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 57, in run\n    self.crawler_process.crawl(spname, **opts.spargs)\n  File \"/home/paul/.virtualenvs/scrapy11.py2/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 162, in crawl\n    crawler = self.create_crawler(crawler_or_spidercls)\n  File \"/home/paul/.virtualenvs/scrapy11.py2/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 190, in create_crawler\n    return self._create_crawler(crawler_or_spidercls)\n  File \"/home/paul/.virtualenvs/scrapy11.py2/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 194, in _create_crawler\n    spidercls = self.spider_loader.load(spidercls)\n  File \"/home/paul/.virtualenvs/scrapy11.py2/local/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 43, in load\n    raise KeyError(\"Spider not found: {}\".format(spider_name))\nKeyError: 'Spider not found: ara\\xc3\\xb1a'\nwith Python 3:\n$ scrapy version -v\nScrapy    : 1.1.2\nlxml      : 3.6.4.0\nlibxml2   : 2.9.4\nTwisted   : 16.4.0\nPython    : 3.5.1+ (default, Mar 30 2016, 22:46:26) - [GCC 5.3.1 20160330]\npyOpenSSL : 16.1.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)\nPlatform  : Linux-4.4.0-36-generic-x86_64-with-Ubuntu-16.04-xenial\n\n\n$ scrapy list\nara\u00f1a\n\n\n$ scrapy crawl ara\u00f1a\n2016-09-15 22:48:10 [scrapy] INFO: Scrapy 1.1.2 started (bot: unicodename)\n2016-09-15 22:48:10 [scrapy] INFO: Overridden settings: {'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'unicodename', 'SPIDER_MODULES': ['unicodename.spiders'], 'NEWSPIDER_MODULE': 'unicodename.spiders'}\n2016-09-15 22:48:11 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole']\n2016-09-15 22:48:11 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-09-15 22:48:11 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-09-15 22:48:11 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-09-15 22:48:11 [scrapy] INFO: Spider opened\n2016-09-15 22:48:11 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-09-15 22:48:11 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-09-15 22:48:11 [scrapy] DEBUG: Crawled (404) <GET http://www.example.com/robots.txt> (referer: None)\n2016-09-15 22:48:12 [scrapy] DEBUG: Crawled (200) <GET http://www.example.com/> (referer: None)\n2016-09-15 22:48:12 [scrapy] DEBUG: Scraped from <200 http://www.example.com/>\n{'url': 'http://www.example.com/'}\n2016-09-15 22:48:12 [scrapy] INFO: Closing spider (finished)\n2016-09-15 22:48:12 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 436,\n 'downloader/request_count': 2,\n 'downloader/request_method_count/GET': 2,\n 'downloader/response_bytes': 1916,\n 'downloader/response_count': 2,\n 'downloader/response_status_count/200': 1,\n 'downloader/response_status_count/404': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 9, 15, 20, 48, 12, 221767),\n 'item_scraped_count': 1,\n 'log_count/DEBUG': 4,\n 'log_count/INFO': 7,\n 'response_received_count': 2,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 9, 15, 20, 48, 11, 433975)}\n2016-09-15 22:48:12 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-09-15T20:50:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "818": {"issue_url": "https://github.com/scrapy/scrapy/issues/2250", "issue_id": "#2250", "issue_summary": "Add note that \"allowed_domains\" should be a list of domains, not URLs", "issue_description": "Contributor\nredapple commented on Sep 16, 2016 \u2022\nedited\n(just logging the issue before I forget)\nIt may seem obvious by the name of the attribute that allowed_domains is about domain names, but it's not uncommon for scrapy users to make the mistake of doing allowed_domains = ['http://www.example.com']\nI believe it is worth adding a note in http://doc.scrapy.org/en/latest/topics/spiders.html?#scrapy.spiders.Spider.allowed_domains\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2016-09-15T20:27:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "819": {"issue_url": "https://github.com/scrapy/scrapy/issues/2244", "issue_id": "#2244", "issue_summary": "Add note on MailSender that \"to\" and \"cc\" must be list of recipients", "issue_description": "Contributor\nredapple commented on Sep 15, 2016\nIt can be easy to miss in the docs (see #2148)", "issue_status": "Closed", "issue_reporting_time": "2016-09-15T10:45:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "820": {"issue_url": "https://github.com/scrapy/scrapy/issues/2240", "issue_id": "#2240", "issue_summary": "Add FAQ entry on splitting items in spider middleware", "issue_description": "Contributor\nredapple commented on Sep 14, 2016\nThis is related to #1915 and the consensus that hacking on item pipeline to split items is not the way to go with current architecture,\nand that spider middleware are a good solution for this use-case.", "issue_status": "Closed", "issue_reporting_time": "2016-09-14T13:40:08Z", "fixed_by": "#3672", "pull_request_summary": "Add a FAQ entry about splitting items in item pipelines", "pull_request_description": "Member\nGallaecio commented on Mar 8, 2019\nFixes #2240\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-08T15:15:24Z", "files_changed": [["23", "docs/faq.rst"], ["2", "docs/topics/spider-middleware.rst"]]}, "821": {"issue_url": "https://github.com/scrapy/scrapy/issues/2238", "issue_id": "#2238", "issue_summary": "Scrapy csv file has uniform empty rows?", "issue_description": "Ibrahim2311 commented on Sep 14, 2016 \u2022\nedited by redapple\nNote: Originally reported on StackOverflow:https://stackoverflow.com/questions/39477662/scrapy-csv-file-has-uniform-empty-rows\nhere is the spider:\nimport scrapy\nfrom danmurphys.items import DanmurphysItem\n\nclass MySpider(scrapy.Spider):\n    name = 'danmurphys'\n    allowed_domains = ['danmurphys.com.au']\n    start_urls = ['https://www.danmurphys.com.au/dm/navigation/navigation_results_gallery.jsp?params=fh_location%3D%2F%2Fcatalog01%2Fen_AU%2Fcategories%3C%7Bcatalog01_2534374302084767_2534374302027742%7D%26fh_view_size%3D120%26fh_sort%3D-sales_value_30_days%26fh_modification%3D&resetnav=false&storeExclusivePage=false']\n\n\n    def parse(self, response):        \n        urls = response.xpath('//h2/a/@href').extract()\n        for url in urls:            \n            request = scrapy.Request(url , callback=self.parse_page)      \n            yield request\n\n    def parse_page(self , response):\n        item = DanmurphysItem()\n        item['brand'] = response.xpath('//span[@itemprop=\"brand\"]/text()').extract_first().strip()\n        item['name'] = response.xpath('//span[@itemprop=\"name\"]/text()').extract_first().strip()\n        item['url'] = response.url     \n        return item\nand here is the items :\nimport scrapy\nclass DanmurphysItem(scrapy.Item):  \n    brand = scrapy.Field()\n    name = scrapy.Field()\n    url = scrapy.Field()\nwhen I run the spider with this command :\nscrapy crawl danmurphys -o output.csv\nthe output is like this :\nHow can I avoid these uniform empty rows , (by the way when I save to json there are no empty values) .", "issue_status": "Closed", "issue_reporting_time": "2016-09-14T10:26:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "822": {"issue_url": "https://github.com/scrapy/scrapy/issues/2237", "issue_id": "#2237", "issue_summary": "What does this mean? [scrapy] WARNING: Bad proxy:", "issue_description": "vionemc commented on Sep 14, 2016\nI am just curious what does this mean\n[scrapy] WARNING: Bad proxy: xxx.xxx.xxx.xxx:xxxx\nDoes it mean the proxy is not usable? Or the proxy has a bad reputation? Or what?", "issue_status": "Closed", "issue_reporting_time": "2016-09-14T07:13:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "823": {"issue_url": "https://github.com/scrapy/scrapy/issues/2234", "issue_id": "#2234", "issue_summary": "Extension from_crawler is called for every spider", "issue_description": "Contributor\nrgtk commented on Sep 13, 2016\nExtension from_crawler class method should be called once in crawler process. Now, it is called for every spider that was added to Crawler. It is counterintuitive and make saving state between spiders using extensions really troublesome.\next.py\nclass DatabaseExtension(object):\n    @classmethod\n    def from_crawler(cls, crawler):\n        print('======================')\n        print(\"=> from_crawler CALLED\")\n        print('======================')\nscrapy_bug.py\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\n\nclass SpiderOne(scrapy.Spider):\n    name = 'one'\n\nclass SpiderTwo(scrapy.Spider):\n    name = 'two'\n\nsettings = {\n    'EXTENSIONS': {\n        'ext.DatabaseExtension': 100\n    }\n }\n\nprocess = CrawlerProcess(settings)\n\nprocess.crawl(SpiderOne)\nprocess.crawl(SpiderTwo)\n\nprocess.start()\nOutput (unuecessary content has been replacted by [...]):\n~/scrapy_playground $ python scrapy_bug.py\n2016-09-13 16:29:59 [scrapy] INFO: Scrapy 1.1.2 started (bot: scrapybot)\n2016-09-13 16:29:59 [scrapy] INFO: Overridden settings: {}\n======================\n=> from_crawler CALLED\n======================\n2016-09-13 16:29:59 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats',\n 'ext.DatabaseExtension']\n2016-09-13 16:29:59 [scrapy] INFO: Enabled downloader middlewares: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Enabled spider middlewares: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Enabled item pipelines: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Spider opened\n2016-09-13 16:29:59 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-09-13 16:29:59 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n======================\n=> from_crawler CALLED\n======================\n2016-09-13 16:29:59 [scrapy] INFO: Enabled extensions: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Enabled downloader middlewares: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Enabled spider middlewares: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Enabled item pipelines: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Spider opened\n2016-09-13 16:29:59 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-09-13 16:29:59 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n2016-09-13 16:29:59 [scrapy] INFO: Closing spider (finished)\n2016-09-13 16:29:59 [scrapy] INFO: Dumping Scrapy stats: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Spider closed (finished)\n2016-09-13 16:29:59 [scrapy] INFO: Closing spider (finished)\n2016-09-13 16:29:59 [scrapy] INFO: Dumping Scrapy stats: [...]\n2016-09-13 16:29:59 [scrapy] INFO: Spider closed (finished)\nScrapy version: 1.1.2", "issue_status": "Closed", "issue_reporting_time": "2016-09-13T14:38:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "824": {"issue_url": "https://github.com/scrapy/scrapy/issues/2233", "issue_id": "#2233", "issue_summary": "Rewrite tutorial to use *.toscrape.com websites", "issue_description": "Contributor\nredapple commented on Sep 13, 2016 \u2022\nedited\ndmoz.org changed layout a few months ago, breaking scrapy's tutorial badly.\nWe have lots of open issues or PRs from the community to fix it: #2023, #2032, #2090, #2130, #2180, not to mention the broken dirbot https://github.com/scrapy/dirbot/issues/19). Thanks to all of you for this.\nThe thing is dmoz.org may change layout again in the future and we need a more stable scraping-friendly website to serve scrapy tutorial's educational purpose.\nScrapinghub maintains a couple of fake-but-realistic websites that can be used for web scraping training, and scrapy know-how in particular: http://books.toscrape.com/ and http://quotes.toscrape.com/ (see http://sites.toscrape.com/)\n/cc @eliasdorneles and @stummjr already working on this\n\ud83d\udc4d 5", "issue_status": "Closed", "issue_reporting_time": "2016-09-13T12:40:05Z", "fixed_by": "#2236", "pull_request_summary": "[MRG+1] Update broken Scrapy tutorial to use quotes.toscrape.com", "pull_request_description": "Member\nstummjr commented on Sep 13, 2016 \u2022\nedited\nPlease, don't merge this yet because it's a WIP. For now, we have the old tutorial ported for quotes.toscrape.com, but we will now rewrite it.\nThis PR aims to fix #2233", "pull_request_status": "Merged", "issue_fixed_time": "2016-09-15T10:05:03Z", "files_changed": [["296", "docs/intro/tutorial.rst"]]}, "825": {"issue_url": "https://github.com/scrapy/scrapy/issues/2232", "issue_id": "#2232", "issue_summary": "Revise documentation about odd-numbered versions", "issue_description": "Contributor\nredapple commented on Sep 13, 2016\nVersioning and API Stability states:\nScrapy uses the odd-numbered versions for development releases.\n(...) Even Bs will be stable branches, and odd Bs will be development.\nI believe this is not true anymore starting with 1.1, 1.1 being the current recommended stable version.", "issue_status": "Closed", "issue_reporting_time": "2016-09-13T12:23:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "826": {"issue_url": "https://github.com/scrapy/scrapy/issues/2230", "issue_id": "#2230", "issue_summary": "document that process_item can return deferred", "issue_description": "Contributor\npawelmhm commented on Sep 12, 2016\npipeline process_item method can return Twisted.Deferred. It works fine in user code and we also do this in some default pipelines, e.g. media pipeline.\nWe should document that this is possible and perhaps add some examples to show users how to do it.", "issue_status": "Closed", "issue_reporting_time": "2016-09-12T11:02:45Z", "fixed_by": "#2282", "pull_request_summary": "[docs] document that process_item can return Deferred", "pull_request_description": "Contributor\npawelmhm commented on Sep 23, 2016\nfixes #2230", "pull_request_status": "Merged", "issue_fixed_time": "2016-09-23T12:05:35Z", "files_changed": [["55", "docs/topics/item-pipeline.rst"]]}, "827": {"issue_url": "https://github.com/scrapy/scrapy/issues/2223", "issue_id": "#2223", "issue_summary": "How to insert time-dependent parameter?", "issue_description": "caizixian commented on Sep 8, 2016\nFor example, if I have a spider like this\nimport time\n\nclass Spider1(scrapy.Spider):\n    name = \"spider\"\n    allowed_domains = [\"example.com\"]\n    start_urls = [\"https://www.example.com/index.html\"]\n    def parse(self, response):\n        for page in range(1,5000000):\n            request = scrapy.Request(\"https://www.example.com/{}.html\".format(page),\n                                     method='POST',\n                                     body=json.dumps({\"time\":time.time()}))\n            yield request\nAll the time in POST requests will be the time when the request is scheduled.\nWhat should I do to make the time be the actual time when the request is sent.\nTried to write middlewares to fill in the parameters, but it seems that the middleware methods are also called when the request is scheduled.", "issue_status": "Closed", "issue_reporting_time": "2016-09-08T08:48:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "828": {"issue_url": "https://github.com/scrapy/scrapy/issues/2213", "issue_id": "#2213", "issue_summary": "PyPy support", "issue_description": "Member\nlopuhin commented on Sep 1, 2016 \u2022\nedited\nIt would be nice to enable pypy tests on travis, mention pypy support in the docs and benchmark a couple of spiders.\nWhat is currently broken in tests:\n#2030 - there are failing signals tests under PyPy (old signals also didn't work under PyPy) - probably it's better to fix them after the merge, since the PR is already big.\nThere are some failures with encoding (different number of <?> inserted), filed https://bitbucket.org/pypy/pypy/issues/2389/different-behavior-of-bytesdecode-utf8\nFailure in ImagesPipelineTestCase.test_convert_image looks like a Pillow bug under PyPy?\nThere are some failures in tests that rely on implementations differences or bugs that are absent on PyPy - here just the tests must be updated.\nOther tasks:\nTest and document installation under PyPy (there can be more issues than for CPython due to missing or outdated binary wheels, e.g. pyca/cryptography#2138)\nStatus: 1 failure in https://github.com/rootAvish/scrapy/compare/signal-rewrite...lopuhin:pypy?expand=1", "issue_status": "Closed", "issue_reporting_time": "2016-09-01T09:58:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "829": {"issue_url": "https://github.com/scrapy/scrapy/issues/2209", "issue_id": "#2209", "issue_summary": "Run from another script - Flask", "issue_description": "Rimander commented on Aug 30, 2016\nHi.\nI was make a website with Flask.\nThe website has multiple options (start, stop..) for each spider.\nHow I can run each spider inside Flask?\nSo spider has access to the app", "issue_status": "Closed", "issue_reporting_time": "2016-08-30T07:46:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "830": {"issue_url": "https://github.com/scrapy/scrapy/issues/2208", "issue_id": "#2208", "issue_summary": "ImportError", "issue_description": "Pat-Lemonde commented on Aug 30, 2016\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2016-08-30T03:31:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "831": {"issue_url": "https://github.com/scrapy/scrapy/issues/2206", "issue_id": "#2206", "issue_summary": "try to extract item by iterate SelectorList, but get all items from response object issue?", "issue_description": "hexcola commented on Aug 29, 2016\nEnvironment\nOS X EI Capitan 10.11.6\nScrapy 1.1.2\nPython 2.7.10\nIssue\nWhen I iterate SelectorList try to extract item from each Selector, it will return all items from SelectorList / response, is it a bug?\nExample Code\n<ul>\n    <li>1</li>\n    <li>2</li>\n    <li>3</li>\n    <li>4</li>\n    <li>5</li>                                \n</ul>\ncore python code:\nfor li in response.xpath('//ul/li'):\n    print li.xpath('//li/text()').extract()   \nthe result will be:\n[u'1', u'2', u'3', u'4', u'5']                                                                                                         \n[u'1', u'2', u'3', u'4', u'5']                                                                                                         \n[u'1', u'2', u'3', u'4', u'5']                                                                                                         \n[u'1', u'2', u'3', u'4', u'5']                                                                                                         \n[u'1', u'2', u'3', u'4', u'5'] \nwhich I think it should be:\n[u'1']                                                                                                                                 \n[u'2']                                                                                                                                 \n[u'3']                                                                                                                                 \n[u'4']                                                                                                                                 \n[u'5'] \nNow I am using the code as follows:\nfor li in response.xpath('//ul/li'):                                                                                               \n     print Selector(text=li.extract()).xpath('//li/text()').extract()  \nSorry for my English.", "issue_status": "Closed", "issue_reporting_time": "2016-08-29T13:27:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "832": {"issue_url": "https://github.com/scrapy/scrapy/issues/2205", "issue_id": "#2205", "issue_summary": "Bad HTML parser", "issue_description": "luc-phan commented on Aug 29, 2016 \u2022\nedited\nScrapy parser is different from common browsers parser. For example :\n<li>\n    one\n    <div>\n</li>\n<li>\n    two\n</li>\nScrapy will see this :\n<li>\n    one\n    <div>\n        <li>\n            two\n        </li>\n    </div>\n</li>\nWhile common browsers would see this :\n<li>\n    one\n    <div>\n    </div>\n</li>\n<li>\n    two\n</li>\nThis issue can be avoided with BeautifulSoup, because BeautifulSoup can use different parsers : html.parser, lxml, html5lib... Is it possible to do the same with Scrapy ?", "issue_status": "Closed", "issue_reporting_time": "2016-08-29T01:29:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "833": {"issue_url": "https://github.com/scrapy/scrapy/issues/2204", "issue_id": "#2204", "issue_summary": "Fails to install on Fedora 24 server", "issue_description": "Shakyamuni177te commented on Aug 27, 2016\nInstallation fails with the following error:\ngcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\n    error: command 'gcc' failed with exit status 1\n\n    ----------------------------------------\nCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-NYwARV/cryptography/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-f00YyN-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-NYwARV/cryptography/", "issue_status": "Closed", "issue_reporting_time": "2016-08-27T11:27:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "834": {"issue_url": "https://github.com/scrapy/scrapy/issues/2203", "issue_id": "#2203", "issue_summary": "This issue is very strange centos 6.8", "issue_description": "shahidkarimi commented on Aug 27, 2016 \u2022\nedited by kmike\nWhen I check version using scapy -V or run any command, I get the following error, Please help\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 3, in <module>\n    from scrapy.cmdline import execute\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 7, in <module>\n    from scrapy.crawler import CrawlerProcess\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 5, in <module>\n    from scrapy.core.engine import ExecutionEngine\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 14, in <module>\n    from scrapy.core.downloader import Downloader\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/core/downloader/__init__.py\", line 16, in <module>\n    from .middleware import DownloaderMiddlewareManager\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py\", line 7, in <module>\n    from scrapy.http import Request, Response\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/http/__init__.py\", line 11, in <module>\n    from scrapy.http.request.form import FormRequest\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/http/request/form.py\", line 9, in <module>\n    import lxml.html\n  File \"/usr/local/lib/python2.7/site-packages/lxml-3.4.4-py2.7-linux-x86_64.egg/lxml/html/__init__.py\", line 42, in <module>\n    from lxml import etree\n  File \"lxml.etree.pyx\", line 161, in init lxml.etree (src/lxml/lxml.etree.c:198847)\nTypeError: encode() argument 1 must be string without null bytes, not unicode", "issue_status": "Closed", "issue_reporting_time": "2016-08-26T21:08:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "835": {"issue_url": "https://github.com/scrapy/scrapy/issues/2201", "issue_id": "#2201", "issue_summary": "Wrong \"Overridden settings\" log line with `custom_settings` spider attribute", "issue_description": "Contributor\nredapple commented on Aug 26, 2016\n(Just logging the issue -- I haven't investigated if it's only about custom_settings)\nTo reproduce, start a project using the basic template (which create a settings.py with ROBOTSTXT_OBEY=True\n$ scrapy version -v\nScrapy    : 1.1.2\nlxml      : 3.6.3.0\nlibxml2   : 2.9.4\nTwisted   : 16.3.2\nPython    : 3.5.1+ (default, Mar 30 2016, 22:46:26) - [GCC 5.3.1 20160330]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)\nPlatform  : Linux-4.4.0-34-generic-x86_64-with-Ubuntu-16.04-xenial\n\n$ scrapy startproject overriddensettings\nYou can start your first spider with:\n    cd overriddensettings\n    scrapy genspider example example.com\n$ cd overriddensettings/\n/overriddensettings$ scrapy genspider example example.com\nCreated spider 'example' using template 'basic' in module:\n  overriddensettings.spiders.example\n$ cat overriddensettings/settings.py |grep ROBOTS\nROBOTSTXT_OBEY = True\nThen, set ROBOTSTXT_OBEY to False in the spider's custom settings\n$ cat overriddensettings/spiders/example.py \n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = \"example\"\n    allowed_domains = [\"example.com\"]\n    start_urls = (\n        'http://www.example.com/',\n    )\n    custom_settings = {'ROBOTSTXT_OBEY': False}\n    def parse(self, response):\n        pass\nThe console logs show 'ROBOTSTXT_OBEY': True in overridden settings, while it's in fact False in this test spider\n$ scrapy crawl example\n2016-08-26 14:49:43 [scrapy] INFO: Scrapy 1.1.2 started (bot: overriddensettings)\n2016-08-26 14:49:43 [scrapy] INFO: Overridden settings: {'BOT_NAME': 'overriddensettings', 'SPIDER_MODULES': ['overriddensettings.spiders'], 'NEWSPIDER_MODULE': 'overriddensettings.spiders', 'ROBOTSTXT_OBEY': True}\n2016-08-26 14:49:43 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.logstats.LogStats']\n2016-08-26 14:49:43 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-08-26 14:49:43 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-08-26 14:49:43 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-08-26 14:49:43 [scrapy] INFO: Spider opened\n2016-08-26 14:49:43 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-08-26 14:49:43 [scrapy] DEBUG: Crawled (200) <GET http://www.example.com/> (referer: None)\n2016-08-26 14:49:44 [scrapy] INFO: Closing spider (finished)\n2016-08-26 14:49:44 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 213,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 952,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 8, 26, 12, 49, 44, 49575),\n 'log_count/DEBUG': 1,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 8, 26, 12, 49, 43, 655150)}", "issue_status": "Closed", "issue_reporting_time": "2016-08-26T13:00:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "836": {"issue_url": "https://github.com/scrapy/scrapy/issues/2200", "issue_id": "#2200", "issue_summary": "Help me solving this issue", "issue_description": "shahidkarimi commented on Aug 25, 2016\nWhen I run the crawl command, this error comes. It was working fine before I write the pipline\nTraceback (most recent call last): File \"c:\\python27\\lib\\site-packages\\scrapy\\cmdline.py\", line 150, in _run_command cmd.run(args, opts) File \"c:\\python27\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 57, in run self.crawler_process.crawl(spname, **opts.spargs) File \"c:\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 153, in crawl d = crawler.crawl(*args, **kwargs) File \"c:\\python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1274, in unwindGenerator return _inlineCallbacks(None, gen, Deferred()) --- <exception caught here> --- File \"c:\\python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1128, in _inlineCallbacks result = g.send(result) File \"c:\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 71, in crawl self.engine = self._create_engine() File \"c:\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 83, in _create_engine return ExecutionEngine(self, lambda _: self.stop()) File \"c:\\python27\\lib\\site-packages\\scrapy\\core\\engine.py\", line 69, in __init__ self.scraper = Scraper(crawler) File \"c:\\python27\\lib\\site-packages\\scrapy\\core\\scraper.py\", line 70, in __init__ self.itemproc = itemproc_cls.from_crawler(crawler) File \"c:\\python27\\lib\\site-packages\\scrapy\\middleware.py\", line 56, in from_crawler return cls.from_settings(crawler.settings, crawler) File \"c:\\python27\\lib\\site-packages\\scrapy\\middleware.py\", line 32, in from_settings mwcls = load_object(clspath) File \"c:\\python27\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 44, in load_object mod = import_module(module) File \"c:\\python27\\lib\\importlib\\__init__.py\", line 37, in import_module __import__(name) exceptions.ImportError: No module named pipelies 2016-08-25 22:53:42 [twisted] CRITICAL:", "issue_status": "Closed", "issue_reporting_time": "2016-08-25T18:04:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "837": {"issue_url": "https://github.com/scrapy/scrapy/issues/2199", "issue_id": "#2199", "issue_summary": "there is frequent logging error while i scrap web pages", "issue_description": "Canidy commented on Aug 25, 2016\nLogged from file retry.py, line 68\nTraceback (most recent call last):\nFile \"c:\\python27\\lib\\logging__init__.py\", line 884, in emit\nstream.write(fs % msg.encode(\"UTF-8\"))\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 234: invalid continuation byte\nLogged from file retry.py, line 68\nTraceback (most recent call last):\nFile \"c:\\python27\\lib\\logging__init__.py\", line 884, in emit\nstream.write(fs % msg.encode(\"UTF-8\"))\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xd3 in position 234: invalid continuation byte", "issue_status": "Closed", "issue_reporting_time": "2016-08-25T08:18:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "838": {"issue_url": "https://github.com/scrapy/scrapy/issues/2198", "issue_id": "#2198", "issue_summary": "Custom ImagesPipeline doesn't generate thumbnails", "issue_description": "briehanlombaard commented on Aug 25, 2016\nFor some reason, when using a customised ImagesPipeline thumbnails are no longer generated. This seems to be the case for 1.1.1 and 1.1.2 but not for 1.0.0 so somewhere between those releases something must have changed.\nThis example should reproduce the problem.", "issue_status": "Closed", "issue_reporting_time": "2016-08-25T08:13:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "839": {"issue_url": "https://github.com/scrapy/scrapy/issues/2196", "issue_id": "#2196", "issue_summary": "Response documentation is incorrect (does not have meta parameter)", "issue_description": "Contributor\nthomdixon commented on Aug 24, 2016\nhttp://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response\nThe Response does not accept a meta parameter. I'll try to give a pull request if I get time, but just wanted to make sure this is logged.", "issue_status": "Closed", "issue_reporting_time": "2016-08-23T22:19:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "840": {"issue_url": "https://github.com/scrapy/scrapy/issues/2192", "issue_id": "#2192", "issue_summary": "add \"https everywhere\" middleware", "issue_description": "Contributor\npawelmhm commented on Aug 23, 2016 \u2022\nedited\nI have bunch of spiders in project that consume lists of urls. It seems like currently lots of websites are moving from http to https. This means that all my outdated url are now redirecting to https. This is somewhat annoying because spider gets huge amount of redirects and it gets slower. It also leads to unexpected issues when downloading images (image pipeline doesn't follow redirects as explained here #2191 so if I have http://image.png and it gets redirected to https://image.png I get error). I know I can update my urls but it's not always easy or possible (I dont have easy access to those urls, some of them are generated by user).\nI was thinking about adding some downloader middleware that would replace protocol to https from http on all requests flowing from spider, it would be really trivial to implement (process_request method on downloader middleware object) and could be enabled by some setting key, e.g. HTTPS_EVERYWHERE = True/False.\nDoes it sound like useful addition to Scrapy or should I limit it to my project?", "issue_status": "Closed", "issue_reporting_time": "2016-08-23T14:36:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "841": {"issue_url": "https://github.com/scrapy/scrapy/issues/2191", "issue_id": "#2191", "issue_summary": "MediaPipeline ignoring redirect middleware", "issue_description": "Contributor\nGranitosaurus commented on Aug 22, 2016 \u2022\nedited\nrequest.meta['handle_httpstatus_all'] = True\ndfd = self.crawler.engine.download(request, info.spider)\nWhen Media Pipeline schedules request it attached handle_httpstatus_all meta which prevents Redirect Middleware from doing it's job. This breaks download of images that require redirect, mostly urls that need to redirect from http to https.\nMaybe implement a setting to enable redirects in this pipeline?", "issue_status": "Closed", "issue_reporting_time": "2016-08-22T14:12:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "842": {"issue_url": "https://github.com/scrapy/scrapy/issues/2187", "issue_id": "#2187", "issue_summary": "enable memusage extension by default", "issue_description": "Member\nkmike commented on Aug 19, 2016\nWhat about setting MEMUSAGE_ENABLED to True by default? It will enable memusage/max value in stats for all spiders, and the cost is just one getrusage(RUSAGE_SELF) call per minute.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-08-19T13:27:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "843": {"issue_url": "https://github.com/scrapy/scrapy/issues/2185", "issue_id": "#2185", "issue_summary": "I can't create a new scrapy project because of a lxml error", "issue_description": "josephdarkins commented on Aug 19, 2016\nI've run through the install and started the tutorial but the installation fails and I've hunted all over the internet and I can't find a solution. When installing scrapy there is an issue with lxml and I get the following error:\nld: library not found for -lrt\nI've updated xcode, brew, python and followed all the instructions. I might be doing something stupid but I get the following extract when installing:\npip install scrapy\nRequirement already satisfied (use --upgrade to upgrade): scrapy in /usr/local/lib/python2.7/site-packages/Scrapy-1.1.2-py2.7.egg\nRequirement already satisfied (use --upgrade to upgrade): Twisted>=10.0.0 in /usr/local/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): w3lib>=1.14.2 in /usr/local/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): queuelib in /usr/local/lib/python2.7/site-packages (from scrapy)\nCollecting lxml (from scrapy)\nUsing cached lxml-3.6.3.tar.gz\nRequirement already satisfied (use --upgrade to upgrade): pyOpenSSL in /usr/local/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cssselect>=0.9 in /usr/local/lib/python2.7/site-packages/cssselect-0.9.2-py2.7.egg (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.5.2 in /usr/local/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): parsel>=0.9.3 in /usr/local/lib/python2.7/site-packages/parsel-1.0.3-py2.7.egg (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): PyDispatcher>=2.0.5 in /usr/local/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): service_identity in /usr/local/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): zope.interface>=3.6.0 in /usr/local/lib/python2.7/site-packages (from Twisted>=10.0.0->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cryptography>=1.3 in /usr/local/lib/python2.7/site-packages (from pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pyasn1 in /usr/local/lib/python2.7/site-packages (from service_identity->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): attrs in /usr/local/lib/python2.7/site-packages (from service_identity->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pyasn1-modules in /usr/local/lib/python2.7/site-packages (from service_identity->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/lib/python2.7/site-packages (from zope.interface>=3.6.0->Twisted>=10.0.0->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cffi>=1.4.1 in /usr/local/lib/python2.7/site-packages (from cryptography>=1.3->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): ipaddress in /usr/local/lib/python2.7/site-packages (from cryptography>=1.3->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): enum34 in /usr/local/lib/python2.7/site-packages (from cryptography>=1.3->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): idna>=2.0 in /usr/local/lib/python2.7/site-packages (from cryptography>=1.3->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pycparser in /usr/local/lib/python2.7/site-packages (from cffi>=1.4.1->cryptography>=1.3->pyOpenSSL->scrapy)\nBuilding wheels for collected packages: lxml\nRunning setup.py bdist_wheel for lxml ... error\nComplete output from command /usr/local/opt/python/bin/python2.7 -u -c \"import setuptools, tokenize;file='/private/var/folders/sw/qts_cngj2xdgyzjykc421nd00000gn/T/pip-build-H7jLpE/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" bdist_wheel -d /var/folders/sw/qts_cngj2xdgyzjykc421nd00000gn/T/tmp1SMhZnpip-wheel- --python-tag cp27:\nBuilding lxml version 3.6.3.\nBuilding without Cython.\nUsing build configuration of libxslt 1.1.28\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib.macosx-10.10-x86_64-2.7\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/init.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/_elementpath.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/builder.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/cssselect.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/doctestcompare.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/ElementInclude.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/pyclasslookup.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/sax.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/usedoctest.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/init.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/init.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/_diffcommand.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/_html5builder.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/_setmixin.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/builder.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/clean.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/defs.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/diff.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/ElementSoup.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/formfill.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/html5parser.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/soupparser.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/usedoctest.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron\ncopying src/lxml/isoschematron/init.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron\ncopying src/lxml/lxml.etree.h -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/lxml.etree_api.h -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/includes/c14n.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/config.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/dtdvalid.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/etreepublic.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/htmlparser.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/relaxng.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/schematron.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/tree.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/uri.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xinclude.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlerror.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlparser.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlschema.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xpath.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xslt.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/etree_defs.h -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/lxml-version.h -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/rng\ncopying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/rng\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl\ncopying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl\ncopying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\nrunning build_ext\nbuilding 'lxml.etree' extension\ncreating build/temp.macosx-10.10-x86_64-2.7\ncreating build/temp.macosx-10.10-x86_64-2.7/src\ncreating build/temp.macosx-10.10-x86_64-2.7/src/lxml\nclang -fno-strict-aliasing -fno-common -dynamic -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/include/libxml2 -Isrc/lxml/includes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.macosx-10.10-x86_64-2.7/src/lxml/lxml.etree.o -w -flat_namespace\nclang -bundle -undefined dynamic_lookup build/temp.macosx-10.10-x86_64-2.7/src/lxml/lxml.etree.o -L/usr/local/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/sqlite/lib -lxslt -lexslt -lrt -lxml2 -lz -lm -o build/lib.macosx-10.10-x86_64-2.7/lxml/etree.so\nld: library not found for -lrt\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nerror: command 'clang' failed with exit status 1\nFailed building wheel for lxml\nRunning setup.py clean for lxml\nFailed to build lxml\nInstalling collected packages: lxml\nRunning setup.py install for lxml ... error\nComplete output from command /usr/local/opt/python/bin/python2.7 -u -c \"import setuptools, tokenize;file='/private/var/folders/sw/qts_cngj2xdgyzjykc421nd00000gn/T/pip-build-H7jLpE/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /var/folders/sw/qts_cngj2xdgyzjykc421nd00000gn/T/pip-jyHP6q-record/install-record.txt --single-version-externally-managed --compile:\nBuilding lxml version 3.6.3.\nBuilding without Cython.\nUsing build configuration of libxslt 1.1.28\nrunning install\nrunning build\nrunning build_py\ncreating build\ncreating build/lib.macosx-10.10-x86_64-2.7\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/init.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/_elementpath.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/builder.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/cssselect.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/doctestcompare.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/ElementInclude.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/pyclasslookup.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/sax.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/usedoctest.py -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/init.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/init.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/_diffcommand.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/_html5builder.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/_setmixin.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/builder.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/clean.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/defs.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/diff.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/ElementSoup.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/formfill.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/html5parser.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/soupparser.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncopying src/lxml/html/usedoctest.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/html\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron\ncopying src/lxml/isoschematron/init.py -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron\ncopying src/lxml/lxml.etree.h -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/lxml.etree_api.h -> build/lib.macosx-10.10-x86_64-2.7/lxml\ncopying src/lxml/includes/c14n.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/config.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/dtdvalid.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/etreepublic.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/htmlparser.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/relaxng.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/schematron.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/tree.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/uri.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xinclude.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlerror.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlparser.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlschema.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xpath.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xslt.pxd -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/etree_defs.h -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/lxml-version.h -> build/lib.macosx-10.10-x86_64-2.7/lxml/includes\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/rng\ncopying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/rng\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl\ncopying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl\ncopying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl\ncreating build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.macosx-10.10-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\nrunning build_ext\nbuilding 'lxml.etree' extension\ncreating build/temp.macosx-10.10-x86_64-2.7\ncreating build/temp.macosx-10.10-x86_64-2.7/src\ncreating build/temp.macosx-10.10-x86_64-2.7/src/lxml\nclang -fno-strict-aliasing -fno-common -dynamic -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/include/libxml2 -Isrc/lxml/includes -I/usr/local/include -I/usr/local/opt/openssl/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.macosx-10.10-x86_64-2.7/src/lxml/lxml.etree.o -w -flat_namespace\nclang -bundle -undefined dynamic_lookup build/temp.macosx-10.10-x86_64-2.7/src/lxml/lxml.etree.o -L/usr/local/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/sqlite/lib -lxslt -lexslt -lrt -lxml2 -lz -lm -o build/lib.macosx-10.10-x86_64-2.7/lxml/etree.so\nld: library not found for -lrt\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nerror: command 'clang' failed with exit status 1\n----------------------------------------\nCommand \"/usr/local/opt/python/bin/python2.7 -u -c \"import setuptools, tokenize;file='/private/var/folders/sw/qts_cngj2xdgyzjykc421nd00000gn/T/pip-build-H7jLpE/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /var/folders/sw/qts_cngj2xdgyzjykc421nd00000gn/T/pip-jyHP6q-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/sw/qts_cngj2xdgyzjykc421nd00000gn/T/pip-build-H7jLpE/lxml/", "issue_status": "Closed", "issue_reporting_time": "2016-08-19T08:13:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "844": {"issue_url": "https://github.com/scrapy/scrapy/issues/2181", "issue_id": "#2181", "issue_summary": "rename spider will load the old left .pyc instead of renamed spider", "issue_description": "samson-wang commented on Aug 15, 2016\nHi all, I'm not sure if this is an issue.\nIf got y.py and y.pyc in the spider directory.\nThen rename it with mv y.py x.py and update the new x.py.\nAfter updates, run scrapy crawl spider will use the old version of spider instead of updates in the x.py.\nHave to remove y.pyc first.\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2016-08-15T03:14:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "845": {"issue_url": "https://github.com/scrapy/scrapy/issues/2178", "issue_id": "#2178", "issue_summary": "Fail to fetch data with special symbol", "issue_description": "netcitylife commented on Aug 12, 2016 \u2022\nedited\nExecute:\nscrapy shell 'http://www.booking.com/destinationfinder/cities/it/vulcano/endorsements.en-gb.html?page=3'\nprint response.body\nYou will find there:\n<div class=\"dsf_cd_mod_endr_item_icon\">\n<i class=\"df_inner dficon-352 dsf_cd_mod_icon dsf_cd_mod_icon-lg\"></i>\n</div>\n<div class=\"dsf_cd_mod_endr_item_name\">\n<p>Pok\u00e9mon Go</p>\n<p>\n1 endorsement\n</p>\n</div>\nThen execute:\nscrapy shell 'http://www.booking.com/destinationfinder/cities/it/vulcano/endorsements.ru.html?page=3'\nprint response.body\nYou will find there:\n<div class=\"dsf_cd_mod_endr_item_icon\">\n<i class=\"df_inner dficon-352 dsf_cd_mod_icon dsf_cd_mod_icon-lg\"></i>\n</div>\n<div class=\"dsf_cd_mod_endr_item_name\">\n<p></p>\n<p>\n1 endorsement\n</p>\n</div>\nSee the difference between English and Russian versions? String \"Pok\u00e9mon Go\" is gone in Russian, despite it exists in html page source. So it is impossible to fetch this with .css() or .xpath() selectors.\nScrapy version:\nScrapy : 1.1.1\nlxml : 3.6.1.0\nlibxml2 : 2.9.4\nTwisted : 16.3.0\nPython : 2.7.12 (default, Jun 29 2016, 12:46:54) - [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2h 3 May 2016)\nPlatform : Darwin-13.4.0-x86_64-i386-64bit", "issue_status": "Closed", "issue_reporting_time": "2016-08-12T09:08:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "846": {"issue_url": "https://github.com/scrapy/scrapy/issues/2177", "issue_id": "#2177", "issue_summary": "Redirecting doesn't work in Scrapy version 1.1.1", "issue_description": "lhuaizhong commented on Aug 12, 2016 \u2022\nedited\nI run a shell:\nscrapy shell\nThen open the google:\nfetch('https://www.google.com')\n\nIn [1]: fetch('https://www.google.com')\n2016-08-12 15:55:14 [scrapy] INFO: Spider opened\n2016-08-12 15:55:14 [scrapy] DEBUG: Crawled (302) <GET https://www.google.com> (referer: None)\n\nIn [2]:\nRedirecting just doesn't work, what's the matter? appreciate if someone help.\nC:\\Users\\...>scrapy version -v\nScrapy    : 1.1.1\nlxml      : 3.6.0.0\nlibxml2   : 2.9.3\nTwisted   : 16.1.1\nPython    : 2.7.11 |Continuum Analytics, Inc.| (default, Mar  4 2016, 15:18:41) [MSC v.1500 32 bit (Intel)]\npyOpenSSL : 0.15.1 (OpenSSL 1.0.2h  3 May 2016)\nPlatform  : Windows-10-10.0.10586", "issue_status": "Closed", "issue_reporting_time": "2016-08-12T08:05:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "847": {"issue_url": "https://github.com/scrapy/scrapy/issues/2176", "issue_id": "#2176", "issue_summary": "Error Running Scrapy through script(Py/Shell)", "issue_description": "kgrvamsi commented on Aug 12, 2016\n1)Running the Scrapy command from the Python script with the subprocess\nfrom subprocess import call\nimport sys\nsys.path.append(\"SCRAPYPROJECTPATH\")\ncall ([\"scrapy\",\"crawl\",\"example\"])\n2)Running the Shell Script from the Python Script (The call value from the above script changes to sh shellscriptname.sh)\nOutput\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 109, in execute\n    settings = get_project_settings()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/project.py\", line 60, in get_project_settings\n    settings.setmodule(settings_module_path, priority='project')\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/settings/__init__.py\", line 108, in setmodule\n    module = import_module(module)\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: No module named project.settings", "issue_status": "Closed", "issue_reporting_time": "2016-08-12T05:15:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "848": {"issue_url": "https://github.com/scrapy/scrapy/issues/2170", "issue_id": "#2170", "issue_summary": "doesn't support appending log file", "issue_description": "Canidy commented on Aug 10, 2016\nlog setting doesn't support multi file.\ni.e,generate a log file according to max file size or date", "issue_status": "Closed", "issue_reporting_time": "2016-08-10T08:41:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "849": {"issue_url": "https://github.com/scrapy/scrapy/issues/2167", "issue_id": "#2167", "issue_summary": "request_to_dict fails in scrapy check command", "issue_description": "cockcrow commented on Aug 8, 2016\nProblem should be reproduced by following steps:\nscrapy startproject testing\ncd testing\nscrapy genspider http httpbin.org\nscrapy edit http\n# -*- coding: utf-8 -*-\nimport logging\nimport scrapy\nfrom scrapy.utils.reqser import request_to_dict\n\n\nclass HttpSpider(scrapy.Spider):\n    name = \"http\"\n    allowed_domains = [\"httpbin.org\"]\n    start_urls = (\n        'http://httpbin.org/get',\n    )\n\n    def parse(self, response):\n        \"\"\"\n        Default parse callback\n\n        @url http://httpbin.org/get\n        @returns request 0\n        @returns item 0\n        \"\"\"\n        request = request_to_dict(response.request, spider=self)\n        self.log(request, level=logging.INFO)\nThen the scrapy crawl http command works well, while the scrapy check blocks.\n$ scrapy crawl http\n2016-08-08 12:01:41 [scrapy] INFO: Scrapy 1.1.1 started (bot: testing)\n2016-08-08 12:01:41 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'testing.spiders', 'SPIDER_MODULES': ['testing.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'testing'}\n2016-08-08 12:01:41 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-08-08 12:01:41 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-08-08 12:01:41 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-08-08 12:01:41 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-08-08 12:01:41 [scrapy] INFO: Spider opened\n2016-08-08 12:01:41 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-08-08 12:01:41 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n2016-08-08 12:01:41 [scrapy] DEBUG: Crawled (200) <GET http://httpbin.org/robots.txt> (referer: None)\n2016-08-08 12:01:42 [scrapy] DEBUG: Crawled (200) <GET http://httpbin.org/get> (referer: None)\n2016-08-08 12:01:42 [http] INFO: {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {'download_timeout': 180.0, 'download_latency': 0.30795884132385254, 'download_slot': 'httpbin.org'}, 'headers': {'Accept-Language': ['en'], 'Accept-Encoding': ['gzip,deflate'], 'Accept': ['text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'], 'User-Agent': ['Scrapy/1.1.1 (+http://scrapy.org)']}, 'url': u'http://httpbin.org/get', 'dont_filter': True, 'priority': 0, 'callback': None, 'method': 'GET', 'errback': None}\n2016-08-08 12:01:42 [scrapy] INFO: Closing spider (finished)\n2016-08-08 12:01:42 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 431,\n 'downloader/request_count': 2,\n 'downloader/request_method_count/GET': 2,\n 'downloader/response_bytes': 713,\n 'downloader/response_count': 2,\n 'downloader/response_status_count/200': 2,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 8, 8, 4, 1, 42, 251024),\n 'log_count/DEBUG': 3,\n 'log_count/INFO': 8,\n 'response_received_count': 2,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 8, 8, 4, 1, 41, 314722)}\n2016-08-08 12:01:42 [scrapy] INFO: Spider closed (finished)\n$ scrapy check\nE\n======================================================================\nERROR: [http] parse (callback)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/contracts/__init__.py\", line 79, in cb_wrapper\n    output = cb(response)\n  File \"/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/contracts/__init__.py\", line 131, in wrapper\n    output = list(iterate_spider_output(cb(response)))\n  File \"/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/contracts/__init__.py\", line 131, in wrapper\n    output = list(iterate_spider_output(cb(response)))\n  File \"/Users/jianhaochen/testing/testing/spiders/http.py\", line 22, in parse\n    request = request_to_dict(response.request, spider=self)\n  File \"/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/utils/reqser.py\", line 18, in request_to_dict\n    cb = _find_method(spider, cb)\n  File \"/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/utils/reqser.py\", line 73, in _find_method\n    raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))\nValueError: Function <function parse at 0x10c09d938> is not a method of: <HttpSpider 'http' at 0x10be58bd0>\n\n----------------------------------------------------------------------\nRan 0 contracts in 0.983s\n\nFAILED (errors=1)\nHere the version info:\n$ python -V\nPython 2.7.10\n$ scrapy version -v\nScrapy    : 1.1.1\nlxml      : 3.6.1.0\nlibxml2   : 2.9.2\nTwisted   : 16.3.0\nPython    : 2.7.10 (default, Nov 20 2015, 11:35:08) - [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2h  3 May 2016)\nPlatform  : Darwin-15.6.0-x86_64-i386-64bit", "issue_status": "Closed", "issue_reporting_time": "2016-08-08T04:27:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "850": {"issue_url": "https://github.com/scrapy/scrapy/issues/2163", "issue_id": "#2163", "issue_summary": "missing service_identity module", "issue_description": "exotfboy commented on Aug 2, 2016\nWhen I run scrapy, I got warning:\nYou do not have a working installation of the service_identity module: 'No module named pyasn1_modules.rfc2459.......\nHowever when I tried to install the module, it tell me that it has been installed:\nWhile when scrapy job start, I got too many errors like:\nWhat's going on? How to fix that?", "issue_status": "Closed", "issue_reporting_time": "2016-08-02T13:26:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "851": {"issue_url": "https://github.com/scrapy/scrapy/issues/2162", "issue_id": "#2162", "issue_summary": "Error load chunked xls-file", "issue_description": "tonal commented on Aug 2, 2016\nI try get xls-file from http://www.wilo.ru/no_cache/glavnaja-stranica/library/dokumentacija/prais-list/?cid=108257&did=20858&sechash=27bdca8a\nBut scrapy load only first part:\n$ scrapy fetch 'http://www.wilo.ru/no_cache/glavnaja-stranica/library/dokumentacija/prais-list/?cid=108257&did=20858&sechash=27bdca8a' > ddd.xls\n2016-08-02 15:04:24 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n2016-08-02 15:04:24 [scrapy] INFO: Overridden settings: {}\n2016-08-02 15:04:24 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-08-02 15:04:24 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-08-02 15:04:24 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-08-02 15:04:24 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-08-02 15:04:24 [scrapy] INFO: Spider opened\n2016-08-02 15:04:24 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-08-02 15:04:24 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-08-02 15:04:27 [scrapy] DEBUG: Crawled (200) <GET http://www.wilo.ru/no_cache/glavnaja-stranica/library/dokumentacija/prais-list/?cid=108257&did=20858&sechash=27bdca8a> (referer: None)\n2016-08-02 15:04:27 [scrapy] INFO: Closing spider (finished)\n2016-08-02 15:04:27 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 307,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 928590,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 8, 2, 8, 4, 27, 849093),\n 'log_count/DEBUG': 2,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 8, 2, 8, 4, 24, 687076)}\n2016-08-02 15:04:27 [scrapy] INFO: Spider closed (finished)\n$ ll ddd.xls \n-rw-rw-r-- 1 tonal tonal 928218 2016-08-02 15:04 ddd.xls\nIf get from curl, all file load:\n$ curl -v 'http://www.wilo.ru/no_cache/glavnaja-stranica/library/dokumentacija/prais-list/?cid=108257&did=20858&sechash=27bdca8a' > dddd.xls\n* Hostname was NOT found in DNS cache\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 46.30.61.245...\n* Connected to www.wilo.ru (46.30.61.245) port 80 (#0)\n> GET /no_cache/glavnaja-stranica/library/dokumentacija/prais-list/?cid=108257&did=20858&sechash=27bdca8a HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: www.wilo.ru\n> Accept: */*\n> \n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0< HTTP/1.1 200 OK\n< Date: Tue, 02 Aug 2016 08:08:28 GMT\n* Server Apache is not blacklisted\n< Server: Apache\n< X-Powered-By: PHP/5.3.19\n< Pragma: private\n< Cache-control: private, must-revalidate\n< Content-disposition: attachment; filename=\"Price_04_2016.xls\"\n< Set-Cookie: fe_typo_user=4b09a3460fc97ad39a657da3e69bd734; path=/\n< Transfer-Encoding: chunked\n< Content-Type: application/octet-stream\n< \n{ [data not shown]\n100 5825k    0 5825k    0     0   204k      0 --:--:--  0:00:28 --:--:-- 66331\n* Connection #0 to host www.wilo.ru left intact\n$ ll dddd.xls \n-rw-rw-r-- 1 tonal tonal 5965312 2016-08-02 15:08 dddd.xls", "issue_status": "Closed", "issue_reporting_time": "2016-08-02T08:09:36Z", "fixed_by": "#2391", "pull_request_summary": "[MRG] Always decompress Content-Encoding: gzip at HttpCompression stage", "pull_request_description": "Contributor\nredapple commented on Nov 10, 2016\nLet SitemapSpider handle decoding of .xml.gz files if necessary.\nFixes #2389\nThe change here is to always decompress responses with Content-Encoding: gzip, whatever Content-Type says, and contrary to #193 (comment), #660, #2065\nTherefore, SitemapSpider still has to decode \"real\" .gz files/content if the HTTP response was not \"Content-Encoded\", relying on gzip magic number instead of response headers.\nI believe it follows RFC 7231 better:\nThe \"Content-Encoding\" header field indicates what content codings\nhave been applied to the representation, beyond those inherent in the\nmedia type, and thus what decoding mechanisms have to be applied in\norder to obtain data in the media type referenced by the Content-Type\nheader field. Content-Encoding is primarily used to allow a\nrepresentation's data to be compressed without losing the identity of\nits underlying media type.\n#951 (comment) also hinted at something like this.\nI think it can also fix #2162 though I need to test that.", "pull_request_status": "Merged", "issue_fixed_time": "2017-03-07T11:56:54Z", "files_changed": [["4", "scrapy/downloadermiddlewares/httpcompression.py"], ["18", "scrapy/spiders/sitemap.py"], ["4", "scrapy/utils/gz.py"], ["61", "tests/test_downloadermiddleware_httpcompression.py"], ["4", "tests/test_spider.py"]]}, "852": {"issue_url": "https://github.com/scrapy/scrapy/issues/2157", "issue_id": "#2157", "issue_summary": "Use w3lib.url.canonicalize_url()", "issue_description": "Contributor\nredapple commented on Aug 1, 2016\nw3lib v1.15 now has a canonicalize_url() (same code as scrapy's in fact).\nLet's depend on w3lib>=1.15 and use canonicalize_url from there.", "issue_status": "Closed", "issue_reporting_time": "2016-08-01T11:23:10Z", "fixed_by": "#2168", "pull_request_summary": "[MRG+1] Use w3lib.url.canonicalize_url() from w3lib 1.15.0", "pull_request_description": "Contributor\nashkulz commented on Aug 8, 2016\nfixes #2157", "pull_request_status": "Merged", "issue_fixed_time": "2016-08-16T14:59:17Z", "files_changed": [["2", "docs/topics/link-extractors.rst"], ["2", "requirements.txt"], ["3", "scrapy/linkextractors/__init__.py"], ["2", "scrapy/utils/request.py"], ["179", "scrapy/utils/url.py"], ["2", "setup.py"], ["206", "tests/test_utils_url.py"]]}, "853": {"issue_url": "https://github.com/scrapy/scrapy/issues/2156", "issue_id": "#2156", "issue_summary": "Is it any plan to support downloading \"data:image/jpeg;base64\"", "issue_description": "yssource commented on Aug 1, 2016\n[scrapy] ERROR: Error downloading <GET data:/robots.txt>: Unsupported URL scheme 'data': no handler available for that scheme\n...\nscrapy.exceptions.NotSupported: Unsupported URL scheme 'data': no handler available for that scheme\nI try to download a image form src attribute \"data:image/jpeg;base64\", But, an error happens to me.", "issue_status": "Closed", "issue_reporting_time": "2016-07-31T19:19:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "854": {"issue_url": "https://github.com/scrapy/scrapy/issues/2155", "issue_id": "#2155", "issue_summary": "Telnet: Manhole module not available from twisted.conch for Python 3+", "issue_description": "lucab0ni commented on Jul 31, 2016\nmanhole module from twisted.conch has not been ported for python versions 3+.\nActual workaround I used:\nInstall manhole separately with pip\nModify scrapy.extensions.telnet.py and try to import manhole in case the import of twisted.conch.manhole has failed. If import manhole succeed, set variable TWISTED_CONCH_AVAILABLE = True", "issue_status": "Closed", "issue_reporting_time": "2016-07-31T14:59:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "855": {"issue_url": "https://github.com/scrapy/scrapy/issues/2154", "issue_id": "#2154", "issue_summary": "response encoding is not utf-8 and impossible to change it", "issue_description": "mayouf commented on Jul 30, 2016\nHi everybody,\nThink we have a bug here.\nI am trying to scrape this page (french social and environmental data) :\nscrapy shell http://www.kelquartier.com/rhone_alpes_ain_commune_divonne_les_bains_01220-c1143/revenu_moyen.html\nI try to pick up the data in this xpath:\nIn [270]: [i.strip() for i in response.xpath('//tr[@id=\"carteNum_12\"]//td//text()').extract()]\nOut[270]: [u'', u'Densit\\xe9 de logements', u'', u'1 log./ha', u'', u'', u'']\nBUT in the page the data appear we have 3 informations in this xpath:\n\"densite de logement\"\n\"1 log./ha\"\n\"<1 log./ha\"\nThen, I noticed this specific character: <\nYET, when I check the body text from the response, this character is here....\nIn [303]: response.body[127220:127251]\nOut[303]: '<td class=\"td_B\">1 log./ha</td>'\nIn [304]: response.body[127725:127757]\nOut[304]: '<td class=\"td_B\"><1 log./ha</td>'\nWhen I check the encoding, it is not utf-8 but cp1225.\nIn    [260]  : response._body_declared_encoding()\nOut [260]  : 'cp1252\nAnd when I checked many website that I scraped in past, the response encoding was always utf-8.\nWhen I checked on stackoverflow, I saw many suggestion like:\nunicode(response.body.decode(response.encoding)).encode('utf-8')\nnewresponse = response.replace(encoding='utf-8')\nthe encoding is always cp1252:\nIn    [260]  : response._body_declared_encoding()\nOut [260]  : 'cp1252\nand my xpath still doesn't take the information I need, which is: '<1 log./ha'\nSo in summary, this is a situation where the data is present in the body but not in its xpath. And it only comes with pages with the following specific character \"<\".\nAnd trying debugging the situation, I figured out the response encoding is cp1252 and it is impossible to change it.", "issue_status": "Closed", "issue_reporting_time": "2016-07-30T14:03:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "856": {"issue_url": "https://github.com/scrapy/scrapy/issues/2151", "issue_id": "#2151", "issue_summary": "Spiders crawl cycles", "issue_description": "loveghost commented on Jul 29, 2016\nhello\nI custom spiders start_requests method.\n def start_requests(self):\n        response_encoding = \"utf-8\"\n        driver = webdriver.PhantomJS(executable_path=phantomjs_path)\n\n        driver.get(self.start_url)\n\n        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, \"hottrends-trends-list-video-container\")))\n        divs = driver.find_elements_by_class_name(\"hottrends-trends-list-video-container\")`\n        for div in divs:\n               ..........\n            request = scrapy.Request(url, callback=self.parse_detail)\n            request.meta['item'] = item\n            yield request\n       driver.close()\n\ndef parse_detail(self, response):\n            ......\nI need the crawler can perform all the time, in other words, the crawler start automatically after the stop\nhow to do\nthanks", "issue_status": "Closed", "issue_reporting_time": "2016-07-29T10:06:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "857": {"issue_url": "https://github.com/scrapy/scrapy/issues/2149", "issue_id": "#2149", "issue_summary": "logging level won't working", "issue_description": "rustjson commented on Jul 29, 2016\nUsing the code in the doc:\n logging.basicConfig(\n     filename='/home/jason/scrapyd/logs/scrapy.log',\n     format='%(levelname)s: %(message)s',\n     level=logging.ERROR\n )\nBut in the file scrapy.log, I can still see INFO DEBUG etc\nERROR: this is an error\nINFO: please\nERROR: this is an error\nDEBUG: Scraped from <200 http://www.xxxx.com>\nI did not specify any log level in my settings.py, Which part could be wrong?\nThanks\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2016-07-29T03:56:39Z", "fixed_by": "#3960", "pull_request_summary": "Update documentation for logging manually", "pull_request_description": "Contributor\nthernstig commented on Aug 15, 2019 \u2022\nedited\nUsage of basicConfig() together with crawlerRunner is not recommended.\nUpdate documentation to highlight this fact.\nCloses #2149, closes #2352, and closes #3146", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-12T11:17:50Z", "files_changed": [["14", "docs/topics/logging.rst"]]}, "858": {"issue_url": "https://github.com/scrapy/scrapy/issues/2148", "issue_id": "#2148", "issue_summary": "mail send does not work properly if argument \"to\" is string", "issue_description": "sbarratt commented on Jul 27, 2016\nScrapy Code:\nmailsender = MailSender(debug=True)\nmailsender.send(to='test@scrapy.org', subject='subject', body='body', _callback=self._catch_mail_sent)\nIn file scrapy/mail.py:\nmsg['From'] = self.mailfrom\nmsg['To'] = COMMASPACE.join(to) # Problem Line\nmsg['Date'] = formatdate(localtime=True)\nmsg['Subject'] = subject\nassumes to is a list. It sets the msg MIMEMultipart to msg['to'] = ['t', 'e', 's', 't', '@', 's', 'c', 'r', 'a', 'p', 'y','o','r','g'] instead of test@scrapy.org.", "issue_status": "Closed", "issue_reporting_time": "2016-07-27T16:31:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "859": {"issue_url": "https://github.com/scrapy/scrapy/issues/2147", "issue_id": "#2147", "issue_summary": "How to handle exception in pipeline with signal?", "issue_description": "Gwill commented on Jul 27, 2016\nJust like using Extention Class connectted spider_error signal.\nHow to handle exception in pipeline with signal?", "issue_status": "Closed", "issue_reporting_time": "2016-07-27T11:17:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "860": {"issue_url": "https://github.com/scrapy/scrapy/issues/2146", "issue_id": "#2146", "issue_summary": "Response blank after being assigned to a Selector", "issue_description": "Member\neLRuLL commented on Jul 27, 2016\n$ scrapy shell \"http://httpbin.org/ip\"\n\n...\n\nIn [1]: print response\n<200 http://httpbin.org/ip>\n\nIn [2]: sel = Selector(text='', response=response)\n\nIn [3]: print sel.response\n<200 about:blank>", "issue_status": "Closed", "issue_reporting_time": "2016-07-27T00:39:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "861": {"issue_url": "https://github.com/scrapy/scrapy/issues/2145", "issue_id": "#2145", "issue_summary": "Disabling RedirectMiddleware results in HttpCompressionMiddleware errors", "issue_description": "Contributor\nbarraponto commented on Jul 26, 2016 \u2022\nedited by redapple\nI wanted not to redirect 303 responses, but instead retry them.\nFrom the docs, I thought I could achieve it through two settings:\nREDIRECT_ENABLED = False\nRETRY_HTTP_CODES = [301, 302, 307, 308, 500, 502, 503, 504, 408]\nIt ended up giving me errors on HttpCompressionMiddleware:\nTraceback (most recent call last):\n  File \"twisted/internet/defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"scrapy/core/downloader/middleware.py\", line 53, in process_response\n    spider=spider)\n  File \"scrapy/downloadermiddlewares/httpcompression.py\", line 38, in process_response\n    response = response.replace(**kwargs)\n  File \"scrapy/http/response/text.py\", line 50, in replace\n    return Response.replace(self, *args, **kwargs)\n  File \"scrapy/http/response/__init__.py\", line 77, in replace\n    return cls(*args, **kwargs)\nTypeError: __init__() got an unexpected keyword argument 'encoding'", "issue_status": "Closed", "issue_reporting_time": "2016-07-26T15:13:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "862": {"issue_url": "https://github.com/scrapy/scrapy/issues/2144", "issue_id": "#2144", "issue_summary": "PEP8 not enforced strictly", "issue_description": "Contributor\ndarshanime commented on Jul 25, 2016\nPEP8 is recommended in the docs but is not strictly enforced.\nSome violations:\nMultiple imports in one line\nTwo blank lines not maintained between class definitions.\nUnused imports\nLines longer than 80 characters everywhere\nI believe we should strictly enforce PEP8 and also edit our CI configurations to test for these violations.\nI would love to submit a patch for this if the idea sounds good to everyone.", "issue_status": "Closed", "issue_reporting_time": "2016-07-25T12:07:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "863": {"issue_url": "https://github.com/scrapy/scrapy/issues/2142", "issue_id": "#2142", "issue_summary": "Scrapy uses `file://` URL as referer", "issue_description": "char101 commented on Jul 23, 2016 \u2022\nedited by redapple\nIf I use a local file in a request (e.g.\ndef start_requests(self):\n    yield Request(path_to_file_uri(filename))\n\ndef parse(self, response):\n    for url in json.loads(response.text):\n        yield Request(url)\nand then make a HTTP request in the callback, scrapy uses the file URL as the referer in the HTTP header.\nI think file:// URL should be avoided from being used as the referer value (or generally any protocol other than http/s) because\nit is not part of the normal hyperlink navigation\nit exposes the local file structure to the remote site\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-07-23T10:29:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "864": {"issue_url": "https://github.com/scrapy/scrapy/issues/2139", "issue_id": "#2139", "issue_summary": "https proxy dont work! again!", "issue_description": "BuGoNee commented on Jul 22, 2016\nyesterday I post a issue #2134\nit's run good just one time, and now it's happen again.\nI start a AWS EC2 instance to test version, I installed the newest version scrapy.\nScrapy : 1.1.1\nlxml : 3.6.0.0\nlibxml2 : 2.9.1\nTwisted : 16.3.0\nPython : 2.7.10 (default, Jul 20 2016, 20:53:27) - [GCC 4.8.3 20140911 (Red Hat 4.8.3-9)]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.1k-fips 8 Jan 2015)\nPlatform : Linux-4.4.10-22.54.amzn1.x86_64-x86_64-with-glibc2.2.5\nI want scraping http://www.lagou.com , this is a http website.\nsome of my proxy servers support http and https, some just support https.\nserver that support http/https can use in my spider proxy,\nserver that not support http cannot use in my spider proxy.\nI want use https because its more speedy and stable, and most of server I collected is https only.", "issue_status": "Closed", "issue_reporting_time": "2016-07-22T03:52:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "865": {"issue_url": "https://github.com/scrapy/scrapy/issues/2137", "issue_id": "#2137", "issue_summary": "DOC Deprecate official Ubuntu packages (until they get updated)", "issue_description": "Contributor\nredapple commented on Jul 21, 2016 \u2022\nedited\nCurrent official Ubuntu packages are not up-to-date with scrapy 1.1 and users are having trouble with them (see #2076, #2136)\nThe docs need to reflect that and not recommend using them.\nThis page http://doc.scrapy.org/en/latest/topics/ubuntu.html#topics-ubuntu is not relevant these days.", "issue_status": "Closed", "issue_reporting_time": "2016-07-21T15:42:13Z", "fixed_by": "#2267", "pull_request_summary": "[MRG+1] Deprecate official Ubuntu packages and update installation instructions", "pull_request_description": "Contributor\nredapple commented on Sep 20, 2016\n@ashkulz's #2166 on top of master branch.\nThis PR is to complement it with recommendation on using virtualenvs\nOriginal description:\n[Official Ubuntu packages] are not currently updated and fail to install on Ubuntu 16.04.\nAlso update the instructions to refer to the earliest supported LTS (Ubuntu 12.04).\nFixes #2137 and closes #2076", "pull_request_status": "Merged", "issue_fixed_time": "2016-09-30T15:35:29Z", "files_changed": [["170", "docs/intro/install.rst"], ["3", "docs/topics/ubuntu.rst"]]}, "866": {"issue_url": "https://github.com/scrapy/scrapy/issues/2136", "issue_id": "#2136", "issue_summary": "Installing scrapy and scrapyd from the official APT repo on Ubuntu 16.04", "issue_description": "hieu-n commented on Jul 21, 2016\nI followed the document at: http://doc.scrapy.org/en/latest/topics/ubuntu.html\nBut when I tried to run sudo apt-get update && sudo apt-get install scrapy scrapyd. I got these errors:\nReading package lists...\nW: http://archive.scrapy.org/ubuntu/dists/scrapy/InRelease: Signature by key 65D1B1CE9BCE5D04D51464F58F62CB1F627220E7 uses weak digest algorithm (SHA1)\nReading package lists...\nBuilding dependency tree...\nReading state information...\nSome packages could not be installed. This may mean that you have\nrequested an impossible situation or if you are using the unstable\ndistribution that some required packages have not yet been created\nor been moved out of Incoming.\nThe following information may help to resolve the situation:\n\nThe following packages have unmet dependencies:\n scrapy : Depends: python-support (>= 0.90.0) but it is not installable\n scrapyd : Depends: python-support (>= 0.90.0) but it is not installable\nE: Unable to correct problems, you have held broken packages.", "issue_status": "Closed", "issue_reporting_time": "2016-07-21T13:12:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "867": {"issue_url": "https://github.com/scrapy/scrapy/issues/2134", "issue_id": "#2134", "issue_summary": "https proxy dont work!", "issue_description": "BuGoNee commented on Jul 21, 2016 \u2022\nedited\nI want scraping http://www.lagou.com with proxy.\nI collected a lot of proxy server like this:\n{'https': 'https://110.72.7.236:8123'}\nsome of server can use http, some just support https.\nI have some code check server is ok:\nimport requests\n\nproxies = [\n {'https': 'https://39.66.5.213:8998'},\n {'https': 'https://111.200.106.5:8123'},\n {'https': 'https://110.72.7.236:8123'},\n {'https': 'https://123.180.122.83:8998'}\n]\ncheckUrl = 'http://www.lagou.com/\n\ndef _check_proxy(proxy, timeout = None):\n    try:\n        r = requests.get(checkUrl, proxies =  proxy ,timeout = timeout)\n        return r.status_code\n   except:\n        return 0\n\ndef main():\n    for proxy in proxies:\n        print proxy, _check_proxy(proxy, timeout=20)\nmain()\nI think proxy server states is health.\nBUT, when I proxy to scrapy , it's not ok, my proxy middleware code:\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nimport pdb\nimport random\nimport base64\nimport time\nimport datetime\nfrom codec import convert\nfrom scrapy import signals\nfrom scrapy.utils.project import get_project_settings\n\nsettings = get_project_settings()\n\nclass ProxyMiddleware(object):\n\n    def __init__(self, settings):\n        self.proxies = settings.get('PROXIES', [])\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        middleware = cls(crawler.settings)\n        crawler.signals.connect(middleware.spider_opened, signals.spider_opened)\n        crawler.signals.connect(middleware.spider_closed, signals.spider_closed)\n        return middleware\n\n    def spider_closed(self, spider):\n        pass\n\n    def spider_opened(self, spider):\n        self.PROXIES = [\n{'username': '', 'ip': '119.29.120.97', 'password': '', 'type': 'https', 'port': '80'},\n{'username': '', 'ip': '115.46.68.190', 'password': '', 'type': 'https', 'port': '8123'},\n{'username': '', 'ip': '60.163.3.9', 'password': '', 'type': 'https', 'port': '8118'},\n{'username': '', 'ip': '171.37.134.201', 'password': '', 'type': 'https', 'port': '8123'},\n]\n\n\n    def process_request(self, request, spider):\n        # Don't overwrite with a random one (server-side state for IP)\n        #  if 'proxy' in request.meta:\n            #  return\n        proxy = random.choice(self.PROXIES)\n        proxy_user_pass = proxy['username'] +':'+ proxy['password'] if proxy['username'] else ''\n        print proxy_user_pass\n        request.meta['proxy'] = \"%s://%s:%s\" % (proxy['type'], proxy['ip'], proxy['port'])\n        #  request.meta['proxy'] = \"http://%s:%s\" % (proxy['ip'], proxy['port'])\n        #  request.meta['proxy'] = 'https://180.76.163.61:10000'\n        print request.meta\n        if proxy_user_pass:\n            basic_auth = 'Basic ' + base64.encodestring(proxy_user_pass)\n            request.headers['Proxy-Authorization'] = basic_auth\n\n    def process_exception(self, request, exception, spider):\n        proxy = request.meta['proxy']\n        spider.logger.error('%s! Removing failed proxy <%s>, %d proxies left' % (\n            exception, proxy, len(self.proxies)))\n        try:\n            self.proxies.remove(proxy)\n        except ValueError:\n            pass\nsome error like this:\n2016-07-21 16:50:10 [scrapy] DEBUG: Retrying <POST http://www.lagou.com/jobs/positionAjax.json?px=new&needAddtionalResult=false> (failed 1 times): [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionLost'>>]\n2016-07-21 16:50:10 [scrapy] DEBUG: Retrying <POST http://www.lagou.com/jobs/positionAjax.json?px=new&needAddtionalResult=false> (failed 2 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <class 'twisted.internet.error.ConnectionLost'>: Connection to the other side was lost in a non-clean fashion: Connection lost.\n].\n2016-07-21 16:50:10 [scrapy] DEBUG: Gave up retrying <POST http://www.lagou.com/jobs/positionAjax.json?px=new&needAddtionalResult=false> (failed 3 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <class 'twisted.internet.error.ConnectionLost'>: Connection to the other side was lost in a non-clean fashion: Connection lost.\n].\n2016-07-21 16:50:10 [lagou_new] ERROR: An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <class 'twisted.internet.error.ConnectionLost'>: Connection to the other side was lost in a non-clean fashion: Connection lost.\n].! Removing failed proxy https://1.59.94.112:8998, 0 proxies left\n2016-07-21 16:50:10 [scrapy] DEBUG: Retrying <POST http://www.lagou.com/jobs/positionAjax.json?px=new&needAddtionalResult=false> (failed 1 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <class 'twisted.internet.error.ConnectionLost'>: Connection to the other side was lost in a non-clean fashion: Connection lost.\n].\nI did some test:\n{'https':'https://180.76.163.61:10000'} can support http, when i use it.\nrequest.meta['proxy'] = 'https://180.76.163.61:10000'\nit's worked!\nelse server cannot support http, will did not work.", "issue_status": "Closed", "issue_reporting_time": "2016-07-21T09:25:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "868": {"issue_url": "https://github.com/scrapy/scrapy/issues/2133", "issue_id": "#2133", "issue_summary": "Scrapy ignoring AWS S3 KEY settings", "issue_description": "rezhajulio commented on Jul 19, 2016\nI am running single spider with custom setting per this documentation. However it keep looking everywhere else. Here is part of the log:\n2016-07-19 20:30:21 [scrapy] INFO: Scrapy 1.1.1 started (bot: scrapybot)\n2016-07-19 20:30:21 [scrapy] INFO: Overridden settings: {}\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Skipping environment variable credential check because profile name was explicitly set.\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: env\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: assume-role\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: shared-credentials-file\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: config-file\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: ec2-credentials-file\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: boto-config\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: container-role\n2016-07-19 20:30:21 [botocore.credentials] DEBUG: Looking for credentials via: iam-role\n2016-07-19 20:30:21 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTP connection (1): 169.254.169.254\n2016-07-19 20:30:22 [botocore.utils] DEBUG: Caught exception while trying to retrieve credentials: HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /latest/meta-data/iam/security-credentials/ (Caused by ConnectTimeoutError(<botocore.awsrequest.AWSHTTPConnection object at 0x7f09baf77a90>, 'Connection to 169.254.169.254 timed out. (connect timeout=1)'))\nTraceback (most recent call last):\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/utils.py\", line 159, in _get_request\n    response = requests.get(url, timeout=timeout)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/api.py\", line 69, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/api.py\", line 50, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/sessions.py\", line 465, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/sessions.py\", line 573, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/adapters.py\", line 419, in send\n    raise ConnectTimeout(e, request=request)\nConnectTimeout: HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /latest/meta-data/iam/security-credentials/ (Caused by ConnectTimeoutError(<botocore.awsrequest.AWSHTTPConnection object at 0x7f09baf77a90>, 'Connection to 169.254.169.254 timed out. (connect timeout=1)'))\n2016-07-19 20:30:22 [botocore.utils] DEBUG: Max number of attempts exceeded (1) when attempting to retrieve data from metadata service.\n2016-07-19 20:30:22 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/endpoints.json\n2016-07-19 20:30:22 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/s3/2006-03-01/service-2.json\n2016-07-19 20:30:22 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/_retry.json\n2016-07-19 20:30:22 [botocore.client] DEBUG: Registering retry handlers for service: s3\n2016-07-19 20:30:22 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x7f09baf92320>\n2016-07-19 20:30:22 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x7f09baf8fa28>\n2016-07-19 20:30:22 [botocore.client] DEBUG: The s3 config key is not a dictionary type, ignoring its value of: None\n2016-07-19 20:30:22 [botocore.endpoint] DEBUG: Setting s3 timeout as (60, 60)\n2016-07-19 20:30:22 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-07-19 20:30:22 [py.warnings] WARNING: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead\n  ScrapyDeprecationWarning)\n\n2016-07-19 20:30:22 [py.warnings] WARNING: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/scraper-0.0.1-py2.7.egg/scraper/middlewares.py:2: ScrapyDeprecationWarning: Module `scrapy.contrib.downloadermiddleware.useragent` is deprecated, use `scrapy.downloadermiddlewares.useragent` instead\n  from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n\n2016-07-19 20:30:22 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scraper.middlewares.RotateUserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-07-19 20:30:22 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-07-19 20:30:22 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-07-19 20:30:22 [scrapy] INFO: Spider opened\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Skipping environment variable credential check because profile name was explicitly set.\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: env\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: assume-role\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: shared-credentials-file\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: config-file\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: ec2-credentials-file\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: boto-config\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: container-role\n2016-07-19 20:30:22 [botocore.credentials] DEBUG: Looking for credentials via: iam-role\n2016-07-19 20:30:22 [botocore.vendored.requests.packages.urllib3.connectionpool] INFO: Starting new HTTP connection (1): 169.254.169.254\n2016-07-19 20:30:23 [botocore.utils] DEBUG: Caught exception while trying to retrieve credentials: HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /latest/meta-data/iam/security-credentials/ (Caused by ConnectTimeoutError(<botocore.awsrequest.AWSHTTPConnection object at 0x7f09ba27f050>, 'Connection to 169.254.169.254 timed out. (connect timeout=1)'))\nTraceback (most recent call last):\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/utils.py\", line 159, in _get_request\n    response = requests.get(url, timeout=timeout)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/api.py\", line 69, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/api.py\", line 50, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/sessions.py\", line 465, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/sessions.py\", line 573, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/vendored/requests/adapters.py\", line 419, in send\n    raise ConnectTimeout(e, request=request)\nConnectTimeout: HTTPConnectionPool(host='169.254.169.254', port=80): Max retries exceeded with url: /latest/meta-data/iam/security-credentials/ (Caused by ConnectTimeoutError(<botocore.awsrequest.AWSHTTPConnection object at 0x7f09ba27f050>, 'Connection to 169.254.169.254 timed out. (connect timeout=1)'))\n2016-07-19 20:30:23 [botocore.utils] DEBUG: Max number of attempts exceeded (1) when attempting to retrieve data from metadata service.\n2016-07-19 20:30:23 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/endpoints.json\n2016-07-19 20:30:23 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/s3/2006-03-01/service-2.json\n2016-07-19 20:30:23 [botocore.loaders] DEBUG: Loading JSON file: /home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/data/_retry.json\n2016-07-19 20:30:23 [botocore.client] DEBUG: Registering retry handlers for service: s3\n2016-07-19 20:30:23 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x7f09baf92320>\n2016-07-19 20:30:23 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x7f09baf8fa28>\n2016-07-19 20:30:23 [botocore.client] DEBUG: The s3 config key is not a dictionary type, ignoring its value of: None\n2016-07-19 20:30:23 [botocore.endpoint] DEBUG: Setting s3 timeout as (60, 60)\n\n\n==== log is too long\n\n\nTraceback (most recent call last):\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/twisted/python/threadpool.py\", line 246, in inContext\n    result = inContext.theWork()\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/twisted/python/threadpool.py\", line 262, in <lambda>\n    inContext.theWork = lambda: context.call(ctx, func, *args, **kw)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/twisted/python/context.py\", line 118, in callWithContext\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/twisted/python/context.py\", line 81, in callWithContext\n    return func(*args,**kw)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 118, in _store_in_thread\n    Bucket=self.bucketname, Key=self.keyname, Body=file)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/client.py\", line 278, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/client.py\", line 561, in _make_api_call\n    operation_model, request_dict)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/endpoint.py\", line 117, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/endpoint.py\", line 142, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/endpoint.py\", line 126, in create_request\n    operation_name=operation_model.name)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/hooks.py\", line 227, in emit\n    return self._emit(event_name, kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/hooks.py\", line 210, in _emit\n    response = handler(**kwargs)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/signers.py\", line 90, in handler\n    return self.sign(operation_name, request)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/signers.py\", line 147, in sign\n    auth.add_auth(request)\n  File \"/home/rezha.pradana/Envs/traveloka/lib/python2.7/site-packages/botocore/auth.py\", line 665, in add_auth\n    raise NoCredentialsError\nNoCredentialsError: Unable to locate credentials\n2016-07-19 20:30:43 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 55517,\n 'downloader/request_count': 72,\n 'downloader/request_method_count/GET': 72,\n 'downloader/response_bytes': 2529033,\n 'downloader/response_count': 72,\n 'downloader/response_status_count/200': 72,\n 'dupefilter/filtered': 20,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 7, 19, 13, 30, 43, 867304),\n 'item_scraped_count': 1196,\n 'log_count/DEBUG': 1322,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 9,\n 'log_count/WARNING': 2,\n 'request_depth_max': 20,\n 'response_received_count': 72,\n 'scheduler/dequeued': 72,\n 'scheduler/dequeued/memory': 72,\n 'scheduler/enqueued': 72,\n 'scheduler/enqueued/memory': 72,\n 'start_time': datetime.datetime(2016, 7, 19, 13, 30, 23, 900947)}\n2016-07-19 20:30:43 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-07-19T13:45:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "869": {"issue_url": "https://github.com/scrapy/scrapy/issues/2132", "issue_id": "#2132", "issue_summary": "How to handle exception like this?", "issue_description": "gamelife1314 commented on Jul 19, 2016 \u2022\nedited\nTraceback (most recent call last):\nFile \"/usr/local/python3/lib/python3.5/site-packages/Twisted-16.3.0-py3.5.egg/twisted/internet/defer.py\", line 1126, in _inlineCallbacks\nresult = result.throwExceptionIntoGenerator(g)\nFile \"/usr/local/python3/lib/python3.5/site-packages/Twisted-16.3.0-py3.5.egg/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\nreturn g.throw(self.type, self.value, self.tb)\nFile \"/usr/local/python3/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\ndefer.returnValue((yield download_func(request=request,spider=spider)))\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.\n2016-07-19 17:01:57 [scrapy] INFO: Closing spider (finished)\n2016-07-19 17:01:57 [sogou_weixin] INFO: spider \u5b8c\u6210\u6240\u6709\u8bf7\u6c42\u5173\u95ed!\n2016-07-19 17:01:57 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 1,\n'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\n'downloader/request_bytes': 450,\n'downloader/request_count': 1,\n'downloader/request_method_count/GET': 1,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2016, 7, 19, 9, 1, 57, 30101),\n'log_count/ERROR': 1,\n'log_count/INFO': 9,\n'scheduler/dequeued': 1,\n'scheduler/dequeued/memory': 1,\n'scheduler/enqueued': 1,\n'scheduler/enqueued/memory': 1,\n'start_time': datetime.datetime(2016, 7, 19, 9, 1, 55, 597143)}\n2016-07-19 17:01:57 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-07-19T09:12:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "870": {"issue_url": "https://github.com/scrapy/scrapy/issues/2131", "issue_id": "#2131", "issue_summary": "My spider can't call the pipeline", "issue_description": "fusae commented on Jul 18, 2016\nThere are two spiders in my project, spider1 can call pipeline, but spider2 can't call pipeline, why?", "issue_status": "Closed", "issue_reporting_time": "2016-07-18T07:08:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "871": {"issue_url": "https://github.com/scrapy/scrapy/issues/2126", "issue_id": "#2126", "issue_summary": "Missing data when scrape multiple start urls", "issue_description": "hugo53 commented on Jul 15, 2016 \u2022\nedited\nWhen I try to scrape multiple start_urls, data seems to be missed. Let's say:\nstart_urls = [http://abc.com/sub1] --> return 5K records\nstart_urls = [http://abc.com/sub2] --> return 3K records\nbut\nstart_urls = [http://abc.com/sub1, http://abc.com/sub2] --> return only 3.7K records\nMy custom start_requests:\ndef start_requests(self):\n        for url in self.start_urls:\n            yield scrapy.Request(url=url, callback=self.parse, dont_filter=True)\nI think it should return at least 8K records, but only 3.7K records. Anyone experience this problem? Please help!\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2016-07-15T07:49:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "872": {"issue_url": "https://github.com/scrapy/scrapy/issues/2125", "issue_id": "#2125", "issue_summary": "scrapy.utils.log.StreamLogger doesn't seem to have flush method - breaks Python3", "issue_description": "zelenij commented on Jul 14, 2016\nMight be necessary to add the flush method to StreamLogger class, to make this work with Python3 and LOG_STDOUT = True:\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\n\n  File \"<frozen importlib._bootstrap_external>\", line 658, in exec_module\n\n  File \"<frozen importlib._bootstrap_external>\", line 764, in get_code\n\n  File \"<frozen importlib._bootstrap_external>\", line 724, in source_to_code\n\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n\nbuiltins.SyntaxError: invalid syntax (pipelines.py, line 19)\nException ignored in: <scrapy.utils.log.StreamLogger object at 0x10fddd978>\nAttributeError: 'StreamLogger' object has no attribute 'flush'", "issue_status": "Closed", "issue_reporting_time": "2016-07-14T04:51:38Z", "fixed_by": "#2239", "pull_request_summary": "[MRG+1] Add flush() method to StreamLogger", "pull_request_description": "Contributor\nredapple commented on Sep 14, 2016 \u2022\nedited\nFixes GH-2125\nLocal tests:\nwith a pretty simple spider:\n$ cat stdoutlogging/spiders/example.py \n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ExampleSpider(scrapy.Spider):\n    name = \"example\"\n    allowed_domains = [\"example.com\"]\n    start_urls = (\n        'http://www.example.com/',\n    )\n\n    def parse(self, response):\n        print(\"hello!\")\n        return {\"somekey\": \"somevalue\"}\nand Python 3.5:\n$ scrapy version -v\nScrapy    : 1.2.0dev2\nlxml      : 3.6.0.0\nlibxml2   : 2.9.3\nTwisted   : 16.4.0\nPython    : 3.5.1+ (default, Mar 30 2016, 22:46:26) - [GCC 5.3.1 20160330]\npyOpenSSL : 16.1.0 (OpenSSL 1.0.2g-fips  1 Mar 2016)\nPlatform  : Linux-4.4.0-36-generic-x86_64-with-Ubuntu-16.04-xenial\nBefore this change:\n$ scrapy crawl example -s LOG_STDOUT=1\n2016-09-14 12:07:35 [scrapy] INFO: Scrapy 1.2.0dev2 started (bot: stdoutlogging)\n2016-09-14 12:07:35 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'stdoutlogging.spiders', 'BOT_NAME': 'stdoutlogging', 'LOG_STDOUT': '1', 'SPIDER_MODULES': ['stdoutlogging.spiders'], 'ROBOTSTXT_OBEY': True}\n(...)\n2016-09-14 12:07:36 [scrapy] INFO: Spider opened\n2016-09-14 12:07:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-09-14 12:07:36 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-09-14 12:07:36 [scrapy] DEBUG: Crawled (404) <GET http://www.example.com/robots.txt> (referer: None)\n2016-09-14 12:07:36 [scrapy] DEBUG: Crawled (200) <GET http://www.example.com/> (referer: None)\n2016-09-14 12:07:36 [stdout] INFO: hello!\n2016-09-14 12:07:36 [scrapy] DEBUG: Scraped from <200 http://www.example.com/>\n{'somekey': 'somevalue'}\n2016-09-14 12:07:36 [scrapy] INFO: Closing spider (finished)\n(...stats...)\n2016-09-14 12:07:36 [scrapy] INFO: Spider closed (finished)\nException ignored in: <scrapy.utils.log.StreamLogger object at 0x7f298eb66358>\nAttributeError: 'StreamLogger' object has no attribute 'flush'\n$\nAfter the change:\n$ scrapy crawl example -s LOG_STDOUT=1\n2016-09-14 12:11:33 [scrapy] INFO: Scrapy 1.2.0dev2 started (bot: stdoutlogging)\n2016-09-14 12:11:33 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'stdoutlogging.spiders', 'SPIDER_MODULES': ['stdoutlogging.spiders'], 'BOT_NAME': 'stdoutlogging', 'ROBOTSTXT_OBEY': True, 'LOG_STDOUT': '1'}\n(...)\n2016-09-14 12:11:33 [scrapy] INFO: Spider opened\n2016-09-14 12:11:33 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-09-14 12:11:33 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-09-14 12:11:39 [scrapy] DEBUG: Crawled (404) <GET http://www.example.com/robots.txt> (referer: None)\n2016-09-14 12:11:39 [scrapy] DEBUG: Crawled (200) <GET http://www.example.com/> (referer: None)\n2016-09-14 12:11:39 [stdout] INFO: hello!\n2016-09-14 12:11:39 [scrapy] DEBUG: Scraped from <200 http://www.example.com/>\n{'somekey': 'somevalue'}\n2016-09-14 12:11:39 [scrapy] INFO: Closing spider (finished)\n(...stats...)\n2016-09-14 12:11:39 [scrapy] INFO: Spider closed (finished)\n$\nNote: I was not able to write a failing-then-passing test for this change in test_utils_log.StreamLoggerTest.\nI tried with testfixtures.OutputCapture but could not capture the stderr output", "pull_request_status": "Merged", "issue_fixed_time": "2016-09-19T08:44:45Z", "files_changed": [["4", "scrapy/utils/log.py"]]}, "873": {"issue_url": "https://github.com/scrapy/scrapy/issues/2123", "issue_id": "#2123", "issue_summary": "`django-configurations` CLI flag", "issue_description": "decentral1se commented on Jul 13, 2016\nWould anyone be interested in accepting a pull request which adds the following flag:\n$ scrapy runspider spider.py --dc=Dev\nWhere --dc is the django-configurations (inspired by pytest-django) settings environment.\nThis would then do the necessary:\nimport configurations\nconfigurations.setup()\nAnd allow django-configurations users to do something in django like:\nclass Scrapy:\n    # scrapy options ...\n\nclass Dev(Scrapy, Configuration):\n    pass", "issue_status": "Closed", "issue_reporting_time": "2016-07-13T15:44:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "874": {"issue_url": "https://github.com/scrapy/scrapy/issues/2116", "issue_id": "#2116", "issue_summary": "item['LINK'] = response.request.url returns wrong/duplicate links", "issue_description": "yssvic commented on Jul 12, 2016\nHello,\nI run scrapy v1.1.0\nI need some parts of the url to create some ids. Lately I have been noticing that response.request.url is returning a different url that the one that is used to scrape.\nIn my code I use the item['LINK'] = response.request.url but that is putting in the wrong link on some scrapers like 2/10 approximately.\nIs there a better way to fetch the url? The one that is displayed in the debug scrapy output is correct.\nThank you.\nThis one outputted in the debug window is correct:\n2016-07-12 13:25:34 [scrapy] DEBUG: Scraped from <200 XXX >\nThe one I fetch with response.request.url is different and wrong.", "issue_status": "Closed", "issue_reporting_time": "2016-07-12T13:08:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "875": {"issue_url": "https://github.com/scrapy/scrapy/issues/2115", "issue_id": "#2115", "issue_summary": "' error: command 'x86_64-linux-gnu-gcc' failed with exit status 1 '", "issue_description": "euler16 commented on Jul 12, 2016\nI wanted to install scrapy in virtualenv using pip (Python 3.5) but I get the following\nerror: command 'x86_64-linux-gnu-gcc' failed with exit status 1\nI tried with Python 2.7 but I get the same error\n\ud83d\udc4d 24", "issue_status": "Closed", "issue_reporting_time": "2016-07-11T19:07:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "876": {"issue_url": "https://github.com/scrapy/scrapy/issues/2114", "issue_id": "#2114", "issue_summary": "ImportError: No module named 'sgmllib' on ubuntu and python 3.4", "issue_description": "foebu commented on Jul 11, 2016 \u2022\nedited\nOn Ubuntu 14.04 with python 3.4 I get the following error:\nImportError: No module named 'sgmllib'\nI installed as suggested in the documentation:\nsudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev\npip install Scrapy\nAny idea?", "issue_status": "Closed", "issue_reporting_time": "2016-07-11T14:59:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "877": {"issue_url": "https://github.com/scrapy/scrapy/issues/2111", "issue_id": "#2111", "issue_summary": "Add option to Log actual TLS connection options/Cipher Suite on 1st connection to a domain", "issue_description": "Contributor\nredapple commented on Jul 11, 2016\nSee #1435 (comment) by @nyov for motivation", "issue_status": "Closed", "issue_reporting_time": "2016-07-11T07:28:12Z", "fixed_by": "#3450", "pull_request_summary": "Log cipher, certificate and temp key info on establishing an SSL connection", "pull_request_description": "Contributor\nwRAR commented on Oct 5, 2018\nFix #2111. Also related to #2726, though that ticket most likely asks for a programmatical access to all of this (I don't know if it's easily doable).\nOutput example:\n2018-10-05 20:08:36 [scrapy.core.downloader.tls] DEBUG: SSL connection to market.yandex.ru using protocol TLSv1.2, cipher ECDHE-RSA-AES128-GCM-SHA256\n2018-10-05 20:08:36 [scrapy.core.downloader.tls] DEBUG: SSL connection certificate: issuer \"/C=RU/O=Yandex LLC/OU=Yandex Certification Authority/CN=Yandex CA\", subject \"/C=RU/O=Yandex LLC/OU=ITO/L=Moscow/ST=Russian Federation/CN=market.yandex.ru\"\n2018-10-05 20:08:36 [scrapy.core.downloader.tls] DEBUG: SSL temp key: ECDH, P-256, 256 bits\nI'm not sure this should be enabled by default as a lot of websites are now HTTPS and this will be printed on all requests including redirects, but I don't know how to access settings in this class.\nNote that there is a lot of direct FFI code to access the temporary key params, I hope it works correctly and doesn't cause memleaks. It also requires OpenSSL 1.0.2, but I couldn't test the check with an older OpenSSL, as I couldn't run tests on an actual jessie system.\nThere is other info that can be added, look at the Connection methods (easy) and at the openssl s_client output (may be harder, like with the temp key info).\n\ud83d\udc4d 1\n\u2764\ufe0f 2", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-23T20:45:41Z", "files_changed": [["21", "docs/topics/settings.rst"], ["11", "scrapy/core/downloader/contextfactory.py"], ["7", "scrapy/core/downloader/handlers/http10.py"], ["11", "scrapy/core/downloader/handlers/http11.py"], ["30", "scrapy/core/downloader/tls.py"], ["1", "scrapy/settings/default_settings.py"], ["50", "scrapy/utils/ssl.py"], ["21", "tests/test_downloader_handlers.py"]]}, "878": {"issue_url": "https://github.com/scrapy/scrapy/issues/2110", "issue_id": "#2110", "issue_summary": "Files download fails with scrapy media_pipeline", "issue_description": "sergei-sh commented on Jul 10, 2016\nWhen trying to download a number of large files with one instance of MediaPipeline, it stops downloading at some moment.\nhttp://stackoverflow.com/questions/38270780/large-files-download-fails-with-scrapy-media-pipeline", "issue_status": "Closed", "issue_reporting_time": "2016-07-10T13:09:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "879": {"issue_url": "https://github.com/scrapy/scrapy/issues/2109", "issue_id": "#2109", "issue_summary": "scrapy don't detect an html element but it is visible on source page", "issue_description": "mayouf commented on Jul 9, 2016\nHello,\nCuuld someone help with a website where the informations appears in the source page and the browser but get vanish as soon as I use \"scrapy shell\" or \"scrapy crawl\".\nI am not banned for sure and the whole page appears correctly but not the div containing the data I need.\nHere is picture of the below link (french website property auction) with a regular browser like mozilla :\nhttp://www.licitor.com/ventes-judiciaires-immobilieres/tgi-fontainebleau/mercredi-15-juin-2016.html\nI encircled the data I target, BUT let us do it with the view(response) from scrapy shell:\nscrapy shell http://www.licitor.com/ventes-judiciaires-immobilieres/tgi-fontainebleau/mercredi-15-juin-2016.html\nand here is the result: a whole div disappeared !!!\nI have some error in my terminal:\nTo give you more information, I first tried on a scrapy project and I had to add DOWNLOAD_HANDLERS: {'s3': None} in my settings in order to get rid of an ERROR message...it did not work also, then by debugging I just tried with the shell and I noticed the response body was missing a part of the HTML.\nI am running on ubuntu 14 and I have anaconda installed on it with scrapy 1.03.\nWhere do I miss the point please people ?\nThanks in advance for your help, I spent the whole afternoon on it !!", "issue_status": "Closed", "issue_reporting_time": "2016-07-09T15:44:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "880": {"issue_url": "https://github.com/scrapy/scrapy/issues/2108", "issue_id": "#2108", "issue_summary": "Rules callback function isn't get called when process_request is set; If manually include calbback function process_request function, rules not working", "issue_description": "skywalker-lili commented on Jul 9, 2016 \u2022\nedited by redapple\nI have rules set like this\nrules = [\n        Rule(LinkExtractor(\n                    allow= '/topic/\\d+/organize$', \n                    restrict_xpaths = '//div[@id= \"zh-topic-organize-child-editor\"]'\n                    ),\n                process_request='request_tagPage', callback = \"parse_tagPage\", follow = True)\n    ]\nrequest_tagPage() is used to add cookie and headers to a request. I found that once I used process_request parameter, the callback function parse_tagPage() isn't get called.\nThen I manually set parse_tagPage() as callback function in the request_tagPage(). Now when response is returned, parse_tagPage() is called but the spider only crawls the links from the start_urls\nMy full spider is here:\nclass ZhihuSpider(CrawlSpider):\n    name = \"zhihu\"\n    BASE_URL = \"www.zhihu.com\"\n    get_xsrf_url = \"https://www.zhihu.com\" # url to visit first to get xsrf information\n    login_url = \"https://www.zhihu.com/login/email\" # url to visit to login and get valid cookie\n    start_urls = [\n        \"https://www.zhihu.com/topic/19776749/organize\",\n\n    ]\n    headers = {\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n        \"Accept-Encoding\": \"gzip,deflate\",\n        \"Accept-Language\": \"en-US,en;q=0.8,zh-TW;q=0.6,zh;q=0.4\",\n        \"Connection\": \"keep-alive\",\n        \"Content-Type\":\" application/x-www-form-urlencoded; charset=UTF-8\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\",\n        \"Referer\": \"http://www.zhihu.com\"\n    } \n\n    # Rules to enforce using cookie every time request a tag information page\n    #restrict_xpaths = '//div[@id= \"zh-topic-organize-page-children\"]/ul/li/ul[@class= \"zm-topic-organize-list\"]',\n    rules = [\n        Rule(LinkExtractor(\n                    allow= '/topic/\\d+/organize$', \n                    restrict_xpaths = '//div[@id= \"zh-topic-organize-child-editor\"]'\n                    ),\n                process_request='request_tagPage', callback = \"parse_tagPage\", follow = True) \n                # \u53d1\u73b0match rule\u7684\u9875\u9762\u4f1a\u7528parse_tagPage\u53bb\u5904\u7406\n                # follow = True\uff0c\u8fd9\u6837\u9875\u9762\u88abparse_tagPage\u5904\u7406\u540e\uff0c\u8fd8\u662f\u4f1a\u88ab\u6293\u5185\u90e8\u7684link\u53bb\u7ee7\u7eedcrawling\n    ] # \u4f7f\u7528list\uff0crules\u5c31\u81ea\u52a8\u6210\u4e3aiterable\n\n    # \u7528\u6765\u4fdd\u5b58tag\u7ed3\u6784\u7684\u5927dictionary\uff0c\u4f1a\u5728item pipeline\u4e2d\u5f97\u5230\u66f4\u65b0\n    d = {\"\u300c\u6839\u8bdd\u9898\u300d\":{}}\n\n    # \u7528\u6765\u4fdd\u5b58\u6bcf\u4e00\u6761tag\u7684list\uff0c\u4f1a\u5728item pipeline\u4e2d\u5f97\u5230\u66f4\u65b0\n    l = []\n\n\n    # Function to get the login response; Only called once\n    # Scrapy\u521a\u542f\u52a8\u65f6\u4f1acall\u8fd9\u4e2a\u51fd\u6570\uff0c\u51fd\u6570\u7684\u76ee\u7684\u662f\u62ff\u5230xsrf\u4fe1\u606f\n    def start_requests(self):\n        print(\"---\"*5)\n        print(\"start to request for getting the hidden info and cookie\")\n        print(\"---\"*5)\n        return [Request(self.get_xsrf_url, headers= self.headers, meta= \\\n                      {\"cookiejar\":1}, callback= self.post_login)]\n\n    # Function to post a login form, notice it gets the xsrf string first before send the form\n    # \u8fd9\u4e2a\u51fd\u6570\u4f1a\u63d0\u53d6\u53ea\u6709\u8bd5\u56felogin\u77e5\u4e4e\u65f6\u624d\u4f1a\u5f97\u5230\u7684xsrf\u4fe1\u606f\u6765\u6784\u5efa\u4e00\u4e2a\u767b\u5f55form\uff0c\u7136\u540e\u5f97\u5230\u767b\u5f55\u6210\u529f\u7684cookie\n    def post_login(self, response):\n        print(\"---\"*5)\n        print(\"preparing login...\")\n        print(\"---\"*5)\n        # Get the xsrf string\n        xsrf = Selector(response).xpath('//div[@data-za-module=\"SignInForm\"]//form//input[@name=\"_xsrf\"]/@value').extract()[0]\n        return FormRequest(self.login_url,\n                                        meta = {\"cookiejar\": response.meta[\"cookiejar\"]},\n                                        headers = self.headers,\n                                        # create form\n                                        formdata = {\n                                            \"_xsrf\": xsrf,\n                                            \"password\": \"zhihu_19891217\",\n                                            \"email\": \"skywalker.ljc@gmail.com\",\n                                            \"remeber_me\": \"true\",\n                                        },\n                                        callback = self.after_login,\n                                        )\n\n    # After login, this function request urls in the start_urls, initiate the whole process\n    # \u8fd9\u4e2a\u51fd\u6570\u4f1a\u7ed9\u767b\u5f55\u6210\u529f\u7684cookie\u7ed9start_urls, \u8fd9\u6837start_urls\u4e5f\u4f1a\u5e26\u7740cookie\u53bbrequest\n    def after_login(self, response):\n        for url in self.start_urls:\n            # No need to callback since the rules has set the process_request parameter, \n            # which specifies a function to send the actual request. \n            yield Request(url, meta = {\"cookiejar\": 1}, headers = self.headers, \\\n                                    callback = self.parse, dont_filter = True) \n                                    # self.parse is the default parser used by CrawlSpider to apply rules\n                                    # dont_filter = True so that this start_url request won't be filtered \n            print(\"A start_url has been requested:\", url)\n        print(\"---\"*5)\n        print(\"All start_urls cookies are have been requested!\")\n        print(\"---\"*5)\n\n    # Function to request tag information page\n    # \u8fd9\u4e2a\u51fd\u6570\u662f\u4e3a\u4e86\u8ba9scrapy\u722c\u540e\u7eed\u7684\u9875\u9762\u65f6\u4e5f\u4f1a\u5e26\u4e0acookie\uff1b\n    # CrawlSpider\u4f1a\u9996\u5148\u7528\u6700\u4f4e\u7ea7\u7684Request()\u53bb\u5f62\u6210\u57fa\u7840\u7684request\uff0c \u4f46\u662f\u57fa\u7840request\u65e0\u6cd5\u901a\u8fc7zhihu.com\u53cd\u722c\u866b\u673a\u5236\n    #     \u6240\u4ee5\u5728rules\u4e2d\u8981\u6c42spider\u518d\u7528\u8fd9\u4e2a\u51fd\u6570\u52a0\u5de5\u57fa\u7840request\u6210\u5e26cookie\u548cheader\u7684\u80fd\u901a\u8fc7zhihu\u53cd\u722c\u866b\u673a\u5236\u7684request\n    def request_tagPage(self, request):\n        return Request(request.url, meta = {\"cookiejar\": 1}, \\\n                    headers = self.headers, callback=self.parse_tagPage)\n                    # When use process_request, the callback in Rule object won't work, has to assign a callback here\n\n    # Finally, the function to actually parse the tag information page\n    # \u8fd9\u4e2a\u51fd\u6570\u624d\u662f\u771f\u6b63\u63a5\u53d7\u77e5\u4e4e\u7684tag\u7ed3\u6784\u9875\u9762\u5e76\u4e14parse\u9875\u9762\u7684\n    # \u8fd9\u4e2a\u51fd\u6570\u8fd8\u4f1a\u6839\u636e\u6bcf\u4e00\u4e2atag\u7684\u4fe1\u606f\uff0c\u4fee\u6539spider\u7528\u6765\u4fdd\u5b58tag structure\u7684dictionary\n    def parse_tagPage(self, response):\n        print(\"---\"*5)\n        print(\"parse_tagPage is called!\")\n        print(\"---\"*5)\n        sel = Selector(response)\n\n        # tag\u7684\u540d\u5b57\u548c\u94fe\u63a5\n        name = sel.xpath('//h1[@class= \"zm-editable-content\"]/text()').extract()[0]\n        relative_link = sel.xpath('//div[@class= \"zm-topic-topbar\"]//a/@href').extract()[0]\n\n        # tag\u7684parent\n        parents = sel.xpath('//div[@id= \"zh-topic-organize-parent-editor\"]//a[@class= \"zm-item-tag\"]/text()').extract()\n        parents = [s.replace(\"\\n\", \"\") for s in parents]\n\n        # tag\u7684children\n        children = sel.xpath('//div[@id= \"zh-topic-organize-child-editor\"]//a[@class= \"zm-item-tag\"]/text()').extract()\n        children = [s.replace(\"\\n\", \"\") for s in children]\n\n        # \u65b0\u5efa\u4e00\u4e2atag item\n        item = {}\n\n        item[\"name\"] = name\n        item[\"relative_link\"] = relative_link\n        item[\"parents\"] = parents\n        item[\"children\"] = children\n\n        self.l.append(item)", "issue_status": "Closed", "issue_reporting_time": "2016-07-09T09:16:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "881": {"issue_url": "https://github.com/scrapy/scrapy/issues/2105", "issue_id": "#2105", "issue_summary": "ImportError after Scrapy was installed", "issue_description": "euler16 commented on Jul 8, 2016\nI installed scrapy on Ubuntu 15.04 using pip install scrapy\nbut when I ran python on terminal and did import scrapy\nthe terminal reported ImportError", "issue_status": "Closed", "issue_reporting_time": "2016-07-08T14:02:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "882": {"issue_url": "https://github.com/scrapy/scrapy/issues/2104", "issue_id": "#2104", "issue_summary": "download_timeout with media_pipeline doesn't work", "issue_description": "sergei-sh commented on Jul 8, 2016 \u2022\nedited by kmike\nrequest = scrapy.Request(\n                    url=video_url,\n                    method=\"GET\",\n                    headers={\n                        \"Accept\" : \"*/*\",\n                        \"User-Agent\" : \"Mozilla\",\n                    },\n                    meta={\"download_timeout\":600, \"item\":item, \"video_url\":video_url},\n                    dont_filter=True,\n                )\n                yield request`\nThis is in MediaPipeline, subclassed, get_media_requests() method.\nThe timeout from meta has no effect. The timeout from settings.py is actually used.", "issue_status": "Closed", "issue_reporting_time": "2016-07-08T12:34:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "883": {"issue_url": "https://github.com/scrapy/scrapy/issues/2097", "issue_id": "#2097", "issue_summary": "bond login cookie, user agent and proxy in the requests process of each spider", "issue_description": "eduOS commented on Jul 8, 2016\nHi, all,\nI want to crawl a website which has a strong security protocol and want to crawl data as fast as possible. Thus I thought I need a multi-login-cookie, multi-user-agent, and multi-proxy crawler.\nI have tens of usernames and passwords and I can login using each one and get all the cookies. To hide the identity of my crawler I thought I should also replace the user-agent setting and my IP. I have found many user agents and proxies.\nI learned that the cookie is needed each time I send a request to the server, and that cookie should be of the same identity and contain the information of the previous request and the corresponding response. I've gained the knowledge of how to pass it through requests without logging in from this answer. And I know two ways to login in, one outside the scrapy(by passing the cookie to the cookiesmiddleware in the middleware.py file:\nfrom cookies import cookies # script written to login some accounts and return the cookies\nimport random\n\nclass CookiesMiddleware(object):\n    def process_request(self, request, spider):\n        cookie = random.choice(cookies)\n        request.cookies = cookie\n) and another inside it.\nWhat's more in the middleware.py file I passed the user agents randomly in the same as for cookies to the scrapy requests.\nMy question is: if I pass the cookies randomly as aforementioned, will one spider get the same cookie each time it sends a request? If not the server side will detect me as a bot and block me. What's worse, the same applies to the user-agents and proxies. How to bond each trinity(login cookie, user-agent and proxy) starting from the login, extending the aforesaid answer both in the horizontal and vertical dimension?\nThanks. Please kindly point me in the right direction, and any suggestions will be highly received and appreciated.\nSincerely,\nLerner", "issue_status": "Closed", "issue_reporting_time": "2016-07-08T07:32:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "884": {"issue_url": "https://github.com/scrapy/scrapy/issues/2092", "issue_id": "#2092", "issue_summary": "Scrapy 1.1 - exceptions.ValueError: Invalid DNS-ID.", "issue_description": "nealhnguyen commented on Jul 6, 2016 \u2022\nedited by redapple\nHello I'm crawling websites with insecure connections. When I try to crawl, I get the stack trace,\n2016-07-05 15:50:17 [twisted] CRITICAL: Error during info_callback\nTraceback (most recent call last):\n  File \"c:\\python27\\lib\\site-packages\\twisted\\protocols\\tls.py\", line 421, in dataReceived\n    self._write(bytes)\n  File \"c:\\python27\\lib\\site-packages\\twisted\\protocols\\tls.py\", line 569, in _write\n    sent = self._tlsConnection.send(toSend)\n  File \"c:\\python27\\lib\\site-packages\\OpenSSL\\SSL.py\", line 1270, in send\n    result = _lib.SSL_write(self._ssl, buf, len(buf))\n  File \"c:\\python27\\lib\\site-packages\\OpenSSL\\SSL.py\", line 933, in wrapper\n    callback(Connection._reverse_mapping[ssl], where, return_code)\n--- <exception caught here> ---\n  File \"c:\\python27\\lib\\site-packages\\twisted\\internet\\_sslverify.py\", line 1154, in infoCallback\n    return wrapped(connection, where, ret)\n  File \"c:\\python27\\lib\\site-packages\\scrapy\\core\\downloader\\tls.py\", line 45, in _identityVerifyingInfoCallback\n    verifyHostname(connection, self._hostnameASCII)\n  File \"c:\\python27\\lib\\site-packages\\service_identity\\pyopenssl.py\", line 45, in verify_hostname\n    obligatory_ids=[DNS_ID(hostname)],\n  File \"c:\\python27\\lib\\site-packages\\service_identity\\_common.py\", line 245, in __init__\n    raise ValueError(\"Invalid DNS-ID.\")\nexceptions.ValueError: Invalid DNS-ID.\nIs there any way to ignore the invalid certificate?", "issue_status": "Closed", "issue_reporting_time": "2016-07-05T22:52:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "885": {"issue_url": "https://github.com/scrapy/scrapy/issues/2090", "issue_id": "#2090", "issue_summary": "Tutorial paths out of date", "issue_description": "znorris commented on Jul 5, 2016\nThe x-paths and css paths in the tutorial no longer work for dmoz.org and need to be updated.\nI'll try to get a PR in for them once I have it sorted out.", "issue_status": "Closed", "issue_reporting_time": "2016-07-05T17:27:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "886": {"issue_url": "https://github.com/scrapy/scrapy/issues/2088", "issue_id": "#2088", "issue_summary": "DEFAULT_REQUEST_HEADERS can't set User-Agent", "issue_description": "ghost commented on Jul 4, 2016 \u2022\nedited by ghost\nIf I use DEFAULT_REQUEST_HEADERS to set User-Agent, it doesn't work.\nDEFAULT_REQUEST_HEADERS = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.84 Safari/537.36',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n    'Accept-Encoding': 'gzip, deflate, sdch',\n    'Accept-Language': 'en-US,en;q=0.8,zh-CN;q=0.6,zh;q=0.4',\n}\nI know I can set User-Agent for crawler by using USER_AGENT setting key:\n# settings.py\nUSER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.84 Safari/537.36'\nI think this behaviour is not corresponding with the doc User Agent and DefaultHeadersMiddleware (This middleware sets all default requests headers specified in the DEFAULT_REQUEST_HEADERS setting.)\nIf this behaviour is designed, maybe the doc should be modified.\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2016-07-04T15:47:08Z", "fixed_by": "#2091", "pull_request_summary": "[MRG+1] prioritize default headers over user agent middlewares", "pull_request_description": "Member\neLRuLL commented on Jul 6, 2016 \u2022\nedited\nfixes #2088", "pull_request_status": "Merged", "issue_fixed_time": "2016-07-06T15:29:39Z", "files_changed": [["6", "docs/topics/settings.rst"], ["6", "scrapy/settings/default_settings.py"]]}, "887": {"issue_url": "https://github.com/scrapy/scrapy/issues/2086", "issue_id": "#2086", "issue_summary": "Scrapy logs [partial] when proxy is used", "issue_description": "pawneetdev commented on Jul 4, 2016 \u2022\nedited\nScrapy logs [partial] when using proxy to scrape and does not return any data. I am using proxymesh.\nmiddleware.py\nimport base64\n\nclass ProxyMiddleware(object):\n    process_request(self, request, spider):\n    request.meta['proxy'] = \"http://......\"\n\n    proxy_user_pass = \"username:password\"\n\n    encoded_user_pass = base64.encodestring(proxy_user_pass)\n    request.headers['Proxy-Authorization'] = 'Basic ' + encoded_user_pass\nsettings.py\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 1,\n    'mcaScrapy.middlewares.ProxyMiddleware': 100,\n}\nOutput\n2016-05-02 11:30:06 [scrapy] DEBUG: Crawled (200) <GET http://example.com/....> (referer: http://example.com/....) ['partial']\n2016-05-02 11:30:06 [scrapy] DEBUG: Crawled (200) <POST http://example.com/....> (referer: http://example.com/....) ['partial']\n2016-05-02 11:30:06 [scrapy] DEBUG: Crawled (200) <POST http://example.com/....> (referer: http://example.com/....) ['partial']\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2016-07-04T04:10:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "888": {"issue_url": "https://github.com/scrapy/scrapy/issues/2084", "issue_id": "#2084", "issue_summary": "d", "issue_description": "nealhnguyen commented on Jul 2, 2016 \u2022\nedited\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2016-07-01T22:26:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "889": {"issue_url": "https://github.com/scrapy/scrapy/issues/2081", "issue_id": "#2081", "issue_summary": "Requirement already satisfied (use --upgrade to upgrade)", "issue_description": "chengshuyi commented on Jun 30, 2016\nI have installed scrapy,because i can import scrapy in python shell.But i cant use scrapy command in shell.Meanwhile,as i type sudo pip3 install scrapy,showing a list of requirement.\nRequirement already satisfied (use --upgrade to upgrade): scrapy in ./.local/lib/python3.5/site-packages\nRequirement already satisfied (use --upgrade to upgrade): cssselect>=0.9 in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): queuelib in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): w3lib>=1.14.2 in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): parsel>=0.9.3 in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): lxml in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): Twisted>=10.0.0 in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pyOpenSSL in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): PyDispatcher>=2.0.5 in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.5.2 in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): service-identity in ./.local/lib/python3.5/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): zope.interface>=4.0.2 in ./.local/lib/python3.5/site-packages (from Twisted>=10.0.0->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cryptography>=1.3 in ./.local/lib/python3.5/site-packages (from pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pyasn1 in ./.local/lib/python3.5/site-packages (from service-identity->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pyasn1-modules in ./.local/lib/python3.5/site-packages (from service-identity->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): attrs in ./.local/lib/python3.5/site-packages (from service-identity->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in ./.local/lib/python3.5/site-packages (from zope.interface>=4.0.2->Twisted>=10.0.0->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): idna>=2.0 in ./.local/lib/python3.5/site-packages (from cryptography>=1.3->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cffi>=1.4.1 in ./.local/lib/python3.5/site-packages (from cryptography>=1.3->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pycparser in ./.local/lib/python3.5/site-packages (from cffi>=1.4.1->cryptography>=1.3->pyOpenSSL->scrapy)\nI want a hand.", "issue_status": "Closed", "issue_reporting_time": "2016-06-30T11:10:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "890": {"issue_url": "https://github.com/scrapy/scrapy/issues/2079", "issue_id": "#2079", "issue_summary": "why I get extra info by the css selector?", "issue_description": "woshichuanqilz commented on Jun 28, 2016\nI want to get the related question info in this stackoverflow link\nHere is the relate question:\n\nAnd here is my result by css selector\nAs you see in the pic I get the res like Firebug-like debugger for Google Chrome and Black Bar at top of website when using Google Chrome which can't be found in the web page.\nBut I can't find the Firebug-like or Black Bar\nThen I search in the web source code and I can't find the Firebug, too\nSo, why I get the extra info here ?", "issue_status": "Closed", "issue_reporting_time": "2016-06-28T06:43:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "891": {"issue_url": "https://github.com/scrapy/scrapy/issues/2078", "issue_id": "#2078", "issue_summary": "what's wrong with my css selector?", "issue_description": "woshichuanqilz commented on Jun 28, 2016 \u2022\nedited\nI use selectorgadget in Chrome to get the css selector of the related question in this stackoverflow link\nAnd I get the res like this\nAnd I test in the scrapy shell but I get nothing\nHow to fix this question ?\nupdate:\nAnd so is this question\nBut some css selector worked like : .user-details a", "issue_status": "Closed", "issue_reporting_time": "2016-06-28T04:21:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "892": {"issue_url": "https://github.com/scrapy/scrapy/issues/2077", "issue_id": "#2077", "issue_summary": "Broad crawl possible memory leak", "issue_description": "rampage644 commented on Jun 24, 2016\nHi,\nI was doing broad crawl and noticed constantly increasing memory consumption for a spider. Pruning my spider to most simple form doesn't help me here (memory still increases constantly).\nI also noticed that others spiders (with much smaller crawl rates, CONCURRENT_REQUESTS = 16) don't have such problem.\nSo i was wondering if I misuse scrapy or there is a problem. Brief issue search didn't show anything, so I went ahead and created experimental spider for tests: https://github.com/rampage644/experimental\nFirst, I'd like to know if someone has experienced memory problems with high rate crawl or another memory problem.\nSecond, I'd like to figure out why this simple spider leaks and can we do anything about that?", "issue_status": "Closed", "issue_reporting_time": "2016-06-24T11:45:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "893": {"issue_url": "https://github.com/scrapy/scrapy/issues/2076", "issue_id": "#2076", "issue_summary": "Ubuntu official repository - installation failed (not able to find python-support)", "issue_description": "inkrement commented on Jun 24, 2016 \u2022\nedited\nI used the manual to install scrapy on ubuntu 16.04, but it failed because it was not able to install python-support (>= 0.90.0). Other sources report that this package is not part of the new ubuntu xenial anymore.\nQuick&Dirty-Workaround:\nwget http://launchpadlibrarian.net/109052632/python-support_1.0.15_all.deb\nsudo dpkg -i python-support_1.0.15_all.deb\nsudo apt-get update && sudo apt-get install scrapy\nhttp://askubuntu.com/questions/766169/why-no-more-python-support-in-16-04\nhttps://launchpad.net/ubuntu/xenial/amd64/python-support/1.0.15", "issue_status": "Closed", "issue_reporting_time": "2016-06-24T06:20:13Z", "fixed_by": "#2267", "pull_request_summary": "[MRG+1] Deprecate official Ubuntu packages and update installation instructions", "pull_request_description": "Contributor\nredapple commented on Sep 20, 2016\n@ashkulz's #2166 on top of master branch.\nThis PR is to complement it with recommendation on using virtualenvs\nOriginal description:\n[Official Ubuntu packages] are not currently updated and fail to install on Ubuntu 16.04.\nAlso update the instructions to refer to the earliest supported LTS (Ubuntu 12.04).\nFixes #2137 and closes #2076", "pull_request_status": "Merged", "issue_fixed_time": "2016-09-30T15:35:29Z", "files_changed": [["170", "docs/intro/install.rst"], ["3", "docs/topics/ubuntu.rst"]]}, "894": {"issue_url": "https://github.com/scrapy/scrapy/issues/2075", "issue_id": "#2075", "issue_summary": "how to discard timeouted request", "issue_description": "zymtech commented on Jun 23, 2016\nHello,\nI am new to scrapy , hope to find some help here. I was using scrapy with unstable proxies,spider always close early than expected for network problem. So, how to ignore all kinds of errors and let spider goes on. I have tried adding errback function to handle httperror, decreasing CONCURRENT_REQUESTS setting and increasing the DOWNLOAD_TIMEOUT setting,etc, it didn't work eventually. So, can anybody tell me how to handle errors like \"user timeout caused connection failure\"?\nTHANKS :) .", "issue_status": "Closed", "issue_reporting_time": "2016-06-23T16:22:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "895": {"issue_url": "https://github.com/scrapy/scrapy/issues/2073", "issue_id": "#2073", "issue_summary": "Redirect Middleware doesn't respect chain order", "issue_description": "Contributor\nrgtk commented on Jun 22, 2016 \u2022\nedited\nWhen custom middleware in DOWNLOADER_MIDDLEWARES is added below all redirect-related middlewares in chain, custom's process_response method is not executed before redirect-related ones.\nI tried to override chain directly in project's settings and extensions without any results. Setting redirect middlewares to None works as expected.\nscrapy_redirect_first.py:\nimport scrapy\n\nclass DummyDownloaderExtension(object):\n    def process_response(self, request, response, spider):\n        if 'something' in response.url:\n            request.meta['dont_redirect'] = True\n\n        print(\"==============> DUMMY YOU!\")\n        return response\n\nclass Spider(scrapy.Spider):\n    name = 'github-scrapy'\n    start_urls = ['http://blog.scrapinghub.com'] # use http, to be redirected to https\n    custom_settings = {\n        'DOWNLOADER_MIDDLEWARES': {\n            'scrapy_redirect_first.DummyDownloaderExtension': 50,\n            'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\n            'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600\n        }\n    }\n\n    def parse(self, response):\n        return {}\nOutput:\n~ $ scrapy runspider scrapy_redirect_first.py\n2016-06-22 18:01:41 [scrapy] INFO: Scrapy 1.1.0 started (bot: scrapybot)\n2016-06-22 18:01:41 [scrapy] INFO: Overridden settings: {}\n2016-06-22 18:01:41 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-06-22 18:01:41 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy_redirect_first.DummyDownloaderExtension',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-06-22 18:01:41 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-06-22 18:01:41 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-06-22 18:03:02 [scrapy] INFO: Spider opened\n2016-06-22 18:03:02 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-06-22 18:03:02 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-06-22 18:03:02 [scrapy] DEBUG: Redirecting (301) to <GET https://blog.scrapinghub.com/> from <GET http://blog.scrapinghub.c>\n==============> DUMMY YOU!\n2016-06-22 18:03:03 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/> (referer: None)\n2016-06-22 18:03:03 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com/>\n{}\n2016-06-22 18:03:03 [scrapy] INFO: Closing spider (finished)\n2016-06-22 18:03:03 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 436,\n 'downloader/request_count': 2,\n 'downloader/request_method_count/GET': 2,\n 'downloader/response_bytes': 33772,\n 'downloader/response_count': 2,\n 'downloader/response_status_count/200': 1,\n 'downloader/response_status_count/301': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 6, 22, 16, 3, 3, 868303),\n 'item_scraped_count': 1,\n 'log_count/DEBUG': 4,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'start_time': datetime.datetime(2016, 6, 22, 16, 3, 2, 684250)}\n2016-06-22 18:03:03 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-06-22T16:14:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "896": {"issue_url": "https://github.com/scrapy/scrapy/issues/2072", "issue_id": "#2072", "issue_summary": "FormRequest returns empty response.body randomly", "issue_description": "jh88 commented on Jun 22, 2016 \u2022\nedited\nFormRequests will return response with empty bodies, but these pages are definitely not empty. And the empty pages are random.\nHere is a piece of code to test. I got a lot of empty responses if autothrottle is disabled and some when enabled. All Requests are fine, only FormRequests have the issue.\nimport scrapy\n\nclass TestcSpider(scrapy.Spider):\n    name = \"testc\"\n    # allowed_domains = [\"http://shop.coles.com.au/\"]\n    start_urls = [\"http://shop.coles.com.au/online/national\"]\n    # download_delay = 3\n\n    def parse(self, response):\n        for div in response.xpath('//ul[@id=\"aisleMenu\"]/li/div')[1:]:\n            a = div.xpath('h2/a')\n            category_name = a.xpath('text()').extract_first().strip()\n            category_urlName = a.xpath('@href').re(r'/([^/]+)/*$')[0]\n\n\n            for li in div.xpath('ul/li'):\n                if (li.xpath('@class').extract_first()):\n                    subcategory_name = li.xpath('a/text()').extract_first().strip()\n                    subcategory_urlName = li.xpath('a/@href').re(r'/([^/]+)/*$')[0]\n                else:\n                    subsubcategory_name = li.xpath('a/text()').extract_first().strip()\n                    subsubcategory_urlName = li.xpath('a/@href').re(r'/([^/]+)/*$')[0]\n\n                    url = self.make_url(category_urlName, subcategory_urlName, subsubcategory_urlName)\n                    request = scrapy.Request(url, cookies={'ColesSearchPageSizeCookie': '100'}, callback = self.get_products)\n                    request.meta['category'] = [category_name, category_urlName]\n                    request.meta['subcategory'] = [subcategory_name, subcategory_urlName]\n                    request.meta['subsubcategory'] = [subsubcategory_name, subsubcategory_urlName]\n\n                    yield request\n\n\n    def get_products(self, response):\n\n        check = response.xpath('//div[@class=\"list-view viewContainer clearfix searchEspot\"]/div[@class=\"outer-prod prodtile\"]').extract_first()\n        if check is None:\n            print response.url\n            print response.meta\n            print '------------'\n\n        if response.xpath('//div[@class=\"pagination clearfix\"]/ul[@class=\"navigator\"]/li[@class=\"next\"]').extract_first() is not None:\n            data_refresh = response.xpath('//div[@id=\"searchDisplay\"]/@data-refresh').extract_first()\n            formdata = dict([\n                (x.split(':')[0].strip(), x.split(':')[1].strip(\"' \"))\n                for x in data_refresh.strip(\"{}\").split(',')\n            ])\n\n            formdata['beginIndex'] = str(int(formdata['beginIndex']) + int(formdata['pageSize']))\n\n            url = self.make_next_url(response.meta['category'][1], response.meta['subcategory'][1])\n            request = scrapy.FormRequest(url, cookies={'ColesSearchPageSizeCookie': '100'}, formdata = formdata, callback = self.get_products)\n            request.meta['category'] = response.meta['category']\n            request.meta['subcategory'] = response.meta['subcategory']\n            request.meta['subsubcategory'] = response.meta['subsubcategory']\n\n            yield request\n\n    def make_url(self, category_urlName, subcategory_urlName, subsubcategory_urlName):\n        return 'http://shop.coles.com.au/online/national/{0}/{1}/{2}'.format(category_urlName, subcategory_urlName, subsubcategory_urlName)\n\n\n    def make_next_url(self, category_urlName, subcategory_urlName):\n        return 'http://shop.coles.com.au/online/national/{0}/{1}/ColesCategoryView'.format(category_urlName, subcategory_urlName)", "issue_status": "Closed", "issue_reporting_time": "2016-06-22T12:18:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "897": {"issue_url": "https://github.com/scrapy/scrapy/issues/2070", "issue_id": "#2070", "issue_summary": "Scrapy installation error", "issue_description": "kiral26 commented on Jun 22, 2016\nHi,\nI'm installing Scrapy on Windows, and got this error when trying to do pip install Scrapy:\nbuild\\temp.win-amd64-2.7\\Release_openssl.c(429) : fatal error C1083: Cannot\nopen include file: 'openssl/opensslv.h': No such file or directory\nI've checked that I got the file mentioned in \"C:\\OpenSSL-Win64\\include\\openssl\", and I've added to path C:\\openssl-win64\\include\\ in user variable. But I'm still getting the error. Can someone please help out, thanks!", "issue_status": "Closed", "issue_reporting_time": "2016-06-22T03:16:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "898": {"issue_url": "https://github.com/scrapy/scrapy/issues/2068", "issue_id": "#2068", "issue_summary": "Depth First Order scraping works in an unexpected way", "issue_description": "flagist0 commented on Jun 21, 2016\nScrapy FAQ points out that to enable DFO you have to set DEPTH_PRIORITY to 1.\nBut it works the opposite way. If I understand it correctly, the problem is here:\nrequest.priority -= depth * self.prio.\nRequest's priority is decreased by its depth, putting it to the end of the queue.\nSo to enable DFO you have to replace '-=' with '+=' or to set DEPTH_PRIORITY to -1 (which is counter-intuitive).", "issue_status": "Closed", "issue_reporting_time": "2016-06-21T10:21:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "899": {"issue_url": "https://github.com/scrapy/scrapy/issues/2063", "issue_id": "#2063", "issue_summary": "IOError, 'Not a gzipped file'", "issue_description": "Contributor\nDharmeshPandav commented on Jun 19, 2016 \u2022\nedited\nwhile trying to access sitemap from robots.txt , Scrapy fails with IOError, 'Not a gzipped file' error\nnot sure if this issue is related to following issue(s)\n#193 -> closed issue\n#660 -> merged pull request to address issue 193\n#951 -> open issue\nline where code fails in gzip.py at line # 197\ndef _read_gzip_header(self):\n        magic = self.fileobj.read(2)\n        if magic != '\\037\\213':\n            raise IOError, 'Not a gzipped file'\nResponse Header\nContent-Encoding: gzip\nAccept-Ranges: bytes\nX-Amz-Request-Id: BFFF010DDE6268DA\nVary: Accept-Encoding\nServer: AmazonS3\nLast-Modified: Wed, 15 Jun 2016 19:02:20 GMT\nEtag: \"300bb71d6897cb2a22bba0bd07978c84\"\nCache-Control: no-transform\nDate: Sun, 19 Jun 2016 10:54:53 GMT\nContent-Type: binary/octet-stream\nError Log:\n Traceback (most recent call last):\n  File \"c:\\venv\\scrapy1.0\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"c:\\venv\\scrapy1.0\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"c:\\venv\\scrapy1.0\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"c:\\venv\\scrapy1.0\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"c:\\venv\\scrapy1.0\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"D:\\projects\\sitemap_spider\\sitemap_spider\\spiders\\mainspider.py\", line 31, in _parse_sitemap\n    body = self._get_sitemap_body(response)\n  File \"c:\\venv\\scrapy1.0\\lib\\site-packages\\scrapy\\spiders\\sitemap.py\", line 67, in _get_sitemap_body\n    return gunzip(response.body)\n  File \"c:\\venv\\scrapy1.0\\lib\\site-packages\\scrapy\\utils\\gz.py\", line 37, in gunzip\n    chunk = read1(f, 8196)\n  File \"c:\\venv\\scrapy1.0\\lib\\site-packages\\scrapy\\utils\\gz.py\", line 21, in read1\n    return gzf.read(size)\n  File \"c:\\python27\\Lib\\gzip.py\", line 268, in read\n    self._read(readsize)\n  File \"c:\\python27\\Lib\\gzip.py\", line 303, in _read\n    self._read_gzip_header()\n  File \"c:\\python27\\Lib\\gzip.py\", line 197, in _read_gzip_header\n    raise IOError, 'Not a gzipped file'\ni did download file manually and was able to extract the content so it is not like file is corrupted\nas an example sitemap url : you can follow amazon robots.txt", "issue_status": "Closed", "issue_reporting_time": "2016-06-19T11:15:35Z", "fixed_by": "#2065", "pull_request_summary": "[MRG+2] [HttpCompressionMW] Do not decompress gzip binary/octet-stream responses", "pull_request_description": "Contributor\nredapple commented on Jun 20, 2016\nThis should fix #2063 and concerns from #951 (comment)\nAs outlined in #2063 (comment) , the idea here is:\nif Content-Encoding is said to be \"gzip\",\nto process Content-Type: binary/octet-stream (or application/octet-stream) as an opaque file that higher-level layers should handle,\neven if HTTP specs would require the engine to gunzip the body for the client application.\nThis is already what happens for Content-Type: application/(x-)gzip (see #660)\nThis behavior is inline with what browsers do for sitemap files for example,\ne.g. Amazon being \"famous\" for sending back this kind of HTTP headers for https://www.amazon.fr/sitemaps.*.xml.gz files (gzipped-compressed XML sitemap files):\n{b'Content-Encoding': [b'gzip'],\n b'Etag': [b'\"212a0faf86341f24001a784bf6d8df51\"'],\n b'Accept-Ranges': [b'bytes'],\n b'Server': [b'Server'],\n b'Last-Modified': [b'Fri, 08 Apr 2016 21:47:04 GMT'],\n b'Vary': [b'Accept-Encoding'],\n b'Content-Type': [b'binary/octet-stream'],\n b'Date': [b'Mon, 20 Jun 2016 14:31:26 GMT'],\n b'X-Amz-Request-Id': [b'0E43407DAC8EE8DE']}", "pull_request_status": "Merged", "issue_fixed_time": "2016-07-11T10:15:38Z", "files_changed": [["7", "scrapy/utils/gz.py"], ["20", "tests/test_downloadermiddleware_httpcompression.py"]]}, "900": {"issue_url": "https://github.com/scrapy/scrapy/issues/2062", "issue_id": "#2062", "issue_summary": "Weird scrapy shell traceback issue.", "issue_description": "peterpalinchuk commented on Jun 18, 2016\nHi i get the following scrapy error and have no idea how and why.\nTraceback (most recent call last): File \"/Library/Frameworks/Python.framework/Versions/3.5/bin/scrapy\", line 11, in <module> sys.exit(execute()) File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/cmdline.py\", line 121, in execute cmds = _get_commands_dict(settings, inproject) File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/cmdline.py\", line 45, in _get_commands_dict cmds = _get_commands_from_module('scrapy.commands', inproject) File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/cmdline.py\", line 28, in _get_commands_from_module for cmd in _iter_command_classes(module): File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/cmdline.py\", line 19, in _iter_command_classes for module in walk_modules(module_name): File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/utils/misc.py\", line 71, in walk_modules submod = import_module(fullpath) File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked File \"<frozen importlib._bootstrap_external>\", line 662, in exec_module File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scrapy/commands/version.py\", line 6, in <module> import OpenSSL File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/OpenSSL/__init__.py\", line 8, in <module> from OpenSSL import rand, crypto, SSL File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/OpenSSL/rand.py\", line 12, in <module> from OpenSSL._util import ( File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/OpenSSL/_util.py\", line 6, in <module> from cryptography.hazmat.bindings.openssl.binding import Binding File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/cryptography/hazmat/bindings/openssl/binding.py\", line 14, in <module> from cryptography.hazmat.bindings._openssl import ffi, lib ImportError: dynamic module does not define module export function (PyInit__openssl)\nI've been having a lot of problems with ssl errors lately. I got an error like this the other day while using the urllib.request library to open a https://... domain. Perhaps you guys can help me figure out how to solve this problem. I'm on mac, by the way, using Python 3.5\nYour help is highly appreciated.", "issue_status": "Closed", "issue_reporting_time": "2016-06-18T12:38:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "901": {"issue_url": "https://github.com/scrapy/scrapy/issues/2059", "issue_id": "#2059", "issue_summary": "Unable install scrapy on Ubuntu 16.04 with python 3+", "issue_description": "pavlomorozov commented on Jun 17, 2016 \u2022\nedited\nHello.\nI tried to install Scrapy as in official manual [(http://doc.scrapy.org/en/master/topics/ubuntu.html#topics-ubuntu] on fresh installed Ubuntu 16.04 and it fails. Then I take few steps from [http://stackoverflow.com/questions/37669290/how-to-install-scrapy-on-unbuntu-16-04/37677910#37677910)] before process by manual:\napt-get install -y \\\n    python3 \\\n    python-dev \\\n    python3-dev\n\n# for cryptography\napt-get install -y \\\n    build-essential \\\n    libssl-dev \\\n    libffi-dev\n\n# for lxml\napt-get install -y \\\n    libxml2-dev \\\n    libxslt-dev\n\n# install pip\napt-get install -y python-pip \nand this fails too. Here my console log:\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\nSee \"man sudo_root\" for details.\n\np@ScrapyPython3:~$ apt-get install -y \\\n>     python3 \\\n>     python-dev \\\n>     python3-dev\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\np@ScrapyPython3:~$ sudo apt-get install -y     python3     python-dev     python3-dev\n[sudo] password for p: \nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\npython3 is already the newest version (3.5.1-3).\nThe following additional packages will be installed:\n  libexpat1-dev libpython-dev libpython2.7-dev libpython3-dev libpython3.5-dev\n  python2.7-dev python3.5-dev\nThe following NEW packages will be installed:\n  libexpat1-dev libpython-dev libpython2.7-dev libpython3-dev libpython3.5-dev\n  python-dev python2.7-dev python3-dev python3.5-dev\n0 upgraded, 9 newly installed, 0 to remove and 177 not upgraded.\nNeed to get 65.9 MB of archives.\nAfter this operation, 95.9 MB of additional disk space will be used.\nGet:1 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libexpat1-dev amd64 2.1.0-7ubuntu0.16.04.1 [115 kB]\nGet:2 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libpython2.7-dev amd64 2.7.11-7ubuntu1 [27.8 MB]\nGet:3 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libpython-dev amd64 2.7.11-1 [7,728 B]\nGet:4 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libpython3.5-dev amd64 3.5.1-10 [37.3 MB]\nGet:5 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libpython3-dev amd64 3.5.1-3 [6,926 B]\nGet:6 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 python2.7-dev amd64 2.7.11-7ubuntu1 [280 kB]\nGet:7 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 python-dev amd64 2.7.11-1 [1,160 B]\nGet:8 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 python3.5-dev amd64 3.5.1-10 [413 kB]\nGet:9 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 python3-dev amd64 3.5.1-3 [1,186 B]\nFetched 65.9 MB in 3min 38s (301 kB/s)                                         \nSelecting previously unselected package libexpat1-dev:amd64.\n(Reading database ... 205453 files and directories currently installed.)\nPreparing to unpack .../libexpat1-dev_2.1.0-7ubuntu0.16.04.1_amd64.deb ...\nUnpacking libexpat1-dev:amd64 (2.1.0-7ubuntu0.16.04.1) ...\nSelecting previously unselected package libpython2.7-dev:amd64.\nPreparing to unpack .../libpython2.7-dev_2.7.11-7ubuntu1_amd64.deb ...\nUnpacking libpython2.7-dev:amd64 (2.7.11-7ubuntu1) ...\nSelecting previously unselected package libpython-dev:amd64.\nPreparing to unpack .../libpython-dev_2.7.11-1_amd64.deb ...\nUnpacking libpython-dev:amd64 (2.7.11-1) ...\nSelecting previously unselected package libpython3.5-dev:amd64.\nPreparing to unpack .../libpython3.5-dev_3.5.1-10_amd64.deb ...\nUnpacking libpython3.5-dev:amd64 (3.5.1-10) ...\nSelecting previously unselected package libpython3-dev:amd64.\nPreparing to unpack .../libpython3-dev_3.5.1-3_amd64.deb ...\nUnpacking libpython3-dev:amd64 (3.5.1-3) ...\nSelecting previously unselected package python2.7-dev.\nPreparing to unpack .../python2.7-dev_2.7.11-7ubuntu1_amd64.deb ...\nUnpacking python2.7-dev (2.7.11-7ubuntu1) ...\nSelecting previously unselected package python-dev.\nPreparing to unpack .../python-dev_2.7.11-1_amd64.deb ...\nUnpacking python-dev (2.7.11-1) ...\nSelecting previously unselected package python3.5-dev.\nPreparing to unpack .../python3.5-dev_3.5.1-10_amd64.deb ...\nUnpacking python3.5-dev (3.5.1-10) ...\nSelecting previously unselected package python3-dev.\nPreparing to unpack .../python3-dev_3.5.1-3_amd64.deb ...\nUnpacking python3-dev (3.5.1-3) ...\nProcessing triggers for doc-base (0.10.7) ...\nProcessing 1 added doc-base file...\nProcessing triggers for man-db (2.7.5-1) ...\nSetting up libexpat1-dev:amd64 (2.1.0-7ubuntu0.16.04.1) ...\nSetting up libpython2.7-dev:amd64 (2.7.11-7ubuntu1) ...\nSetting up libpython-dev:amd64 (2.7.11-1) ...\nSetting up libpython3.5-dev:amd64 (3.5.1-10) ...\nSetting up libpython3-dev:amd64 (3.5.1-3) ...\nSetting up python2.7-dev (2.7.11-7ubuntu1) ...\nSetting up python-dev (2.7.11-1) ...\nSetting up python3.5-dev (3.5.1-10) ...\nSetting up python3-dev (3.5.1-3) ...\np@ScrapyPython3:~$ apt-get install -y \\\n>     build-essential \\\n>     libssl-dev \\\n>     libffi-dev\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\np@ScrapyPython3:~$ sudo apt-get install -y     build-essential     libssl-dev     libffi-dev\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nbuild-essential is already the newest version (12.1ubuntu2).\nbuild-essential set to manually installed.\nThe following additional packages will be installed:\n  libssl-doc zlib1g-dev\nThe following NEW packages will be installed:\n  libffi-dev libssl-dev libssl-doc zlib1g-dev\n0 upgraded, 4 newly installed, 0 to remove and 177 not upgraded.\nNeed to get 2,801 kB of archives.\nAfter this operation, 11.1 MB of additional disk space will be used.\nGet:1 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 zlib1g-dev amd64 1:1.2.8.dfsg-2ubuntu4 [168 kB]\nGet:2 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libssl-dev amd64 1.0.2g-1ubuntu4.1 [1,394 kB]\nGet:3 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libssl-doc all 1.0.2g-1ubuntu4.1 [1,078 kB]\nGet:4 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libffi-dev amd64 3.2.1-4 [161 kB]\nFetched 2,801 kB in 14s (190 kB/s)                                             \nSelecting previously unselected package zlib1g-dev:amd64.\n(Reading database ... 205764 files and directories currently installed.)\nPreparing to unpack .../zlib1g-dev_1%3a1.2.8.dfsg-2ubuntu4_amd64.deb ...\nUnpacking zlib1g-dev:amd64 (1:1.2.8.dfsg-2ubuntu4) ...\nSelecting previously unselected package libssl-dev:amd64.\nPreparing to unpack .../libssl-dev_1.0.2g-1ubuntu4.1_amd64.deb ...\nUnpacking libssl-dev:amd64 (1.0.2g-1ubuntu4.1) ...\nSelecting previously unselected package libssl-doc.\nPreparing to unpack .../libssl-doc_1.0.2g-1ubuntu4.1_all.deb ...\nUnpacking libssl-doc (1.0.2g-1ubuntu4.1) ...\nSelecting previously unselected package libffi-dev:amd64.\nPreparing to unpack .../libffi-dev_3.2.1-4_amd64.deb ...\nUnpacking libffi-dev:amd64 (3.2.1-4) ...\nProcessing triggers for man-db (2.7.5-1) ...\nProcessing triggers for doc-base (0.10.7) ...\nProcessing 1 added doc-base file...\nProcessing triggers for install-info (6.1.0.dfsg.1-5) ...\nSetting up zlib1g-dev:amd64 (1:1.2.8.dfsg-2ubuntu4) ...\nSetting up libssl-dev:amd64 (1.0.2g-1ubuntu4.1) ...\nSetting up libssl-doc (1.0.2g-1ubuntu4.1) ...\nSetting up libffi-dev:amd64 (3.2.1-4) ...\np@ScrapyPython3:~$ apt-get install -y \\\n>     libxml2-dev \\\n>     libxslt-dev\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\np@ScrapyPython3:~$ sudo apt-get install -y     libxml2-dev     libxslt-dev\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nNote, selecting 'libxslt1-dev' instead of 'libxslt-dev'\nThe following additional packages will be installed:\n  icu-devtools libicu-dev\nSuggested packages:\n  icu-doc\nThe following NEW packages will be installed:\n  icu-devtools libicu-dev libxml2-dev libxslt1-dev\n0 upgraded, 4 newly installed, 0 to remove and 177 not upgraded.\nNeed to get 9,860 kB of archives.\nAfter this operation, 47.4 MB of additional disk space will be used.\nGet:1 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 icu-devtools amd64 55.1-7 [165 kB]\nGet:2 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libicu-dev amd64 55.1-7 [8,546 kB]\nGet:3 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libxml2-dev amd64 2.9.3+dfsg1-1ubuntu0.1 [743 kB]\nGet:4 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libxslt1-dev amd64 1.1.28-2.1 [406 kB]\nFetched 9,860 kB in 36s (274 kB/s)                                             \nSelecting previously unselected package icu-devtools.\n(Reading database ... 207591 files and directories currently installed.)\nPreparing to unpack .../icu-devtools_55.1-7_amd64.deb ...\nUnpacking icu-devtools (55.1-7) ...\nSelecting previously unselected package libicu-dev:amd64.\nPreparing to unpack .../libicu-dev_55.1-7_amd64.deb ...\nUnpacking libicu-dev:amd64 (55.1-7) ...\nSelecting previously unselected package libxml2-dev:amd64.\nPreparing to unpack .../libxml2-dev_2.9.3+dfsg1-1ubuntu0.1_amd64.deb ...\nUnpacking libxml2-dev:amd64 (2.9.3+dfsg1-1ubuntu0.1) ...\nSelecting previously unselected package libxslt1-dev:amd64.\nPreparing to unpack .../libxslt1-dev_1.1.28-2.1_amd64.deb ...\nUnpacking libxslt1-dev:amd64 (1.1.28-2.1) ...\nProcessing triggers for man-db (2.7.5-1) ...\nProcessing triggers for doc-base (0.10.7) ...\nProcessing 1 added doc-base file...\nSetting up icu-devtools (55.1-7) ...\nSetting up libicu-dev:amd64 (55.1-7) ...\nSetting up libxml2-dev:amd64 (2.9.3+dfsg1-1ubuntu0.1) ...\nSetting up libxslt1-dev:amd64 (1.1.28-2.1) ...\np@ScrapyPython3:~$ apt-get install -y python-pip\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\np@ScrapyPython3:~$ sudo apt-get install -y python-pip\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  libpython-all-dev python-all python-all-dev python-pip-whl\n  python-pkg-resources python-setuptools python-wheel\nSuggested packages:\n  python-setuptools-doc\nThe following NEW packages will be installed:\n  libpython-all-dev python-all python-all-dev python-pip python-pip-whl\n  python-pkg-resources python-setuptools python-wheel\n0 upgraded, 8 newly installed, 0 to remove and 177 not upgraded.\nNeed to get 1,583 kB of archives.\nAfter this operation, 3,065 kB of additional disk space will be used.\nGet:1 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libpython-all-dev amd64 2.7.11-1 [992 B]\nGet:2 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 python-all amd64 2.7.11-1 [978 B]\nGet:3 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 python-all-dev amd64 2.7.11-1 [1,000 B]\nGet:4 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 python-pip-whl all 8.1.1-2ubuntu0.1 [1,110 kB]\nGet:5 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 python-pip all 8.1.1-2ubuntu0.1 [144 kB]\nGet:6 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 python-pkg-resources all 20.7.0-1 [108 kB]\nGet:7 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 python-setuptools all 20.7.0-1 [169 kB]\nGet:8 http://us.archive.ubuntu.com/ubuntu xenial/universe amd64 python-wheel all 0.29.0-1 [48.0 kB]\nFetched 1,583 kB in 3s (422 kB/s)  \nSelecting previously unselected package libpython-all-dev:amd64.\n(Reading database ... 208343 files and directories currently installed.)\nPreparing to unpack .../libpython-all-dev_2.7.11-1_amd64.deb ...\nUnpacking libpython-all-dev:amd64 (2.7.11-1) ...\nSelecting previously unselected package python-all.\nPreparing to unpack .../python-all_2.7.11-1_amd64.deb ...\nUnpacking python-all (2.7.11-1) ...\nSelecting previously unselected package python-all-dev.\nPreparing to unpack .../python-all-dev_2.7.11-1_amd64.deb ...\nUnpacking python-all-dev (2.7.11-1) ...\nSelecting previously unselected package python-pip-whl.\nPreparing to unpack .../python-pip-whl_8.1.1-2ubuntu0.1_all.deb ...\nUnpacking python-pip-whl (8.1.1-2ubuntu0.1) ...\nSelecting previously unselected package python-pip.\nPreparing to unpack .../python-pip_8.1.1-2ubuntu0.1_all.deb ...\nUnpacking python-pip (8.1.1-2ubuntu0.1) ...\nSelecting previously unselected package python-pkg-resources.\nPreparing to unpack .../python-pkg-resources_20.7.0-1_all.deb ...\nUnpacking python-pkg-resources (20.7.0-1) ...\nSelecting previously unselected package python-setuptools.\nPreparing to unpack .../python-setuptools_20.7.0-1_all.deb ...\nUnpacking python-setuptools (20.7.0-1) ...\nSelecting previously unselected package python-wheel.\nPreparing to unpack .../python-wheel_0.29.0-1_all.deb ...\nUnpacking python-wheel (0.29.0-1) ...\nProcessing triggers for man-db (2.7.5-1) ...\nSetting up libpython-all-dev:amd64 (2.7.11-1) ...\nSetting up python-all (2.7.11-1) ...\nSetting up python-all-dev (2.7.11-1) ...\nSetting up python-pip-whl (8.1.1-2ubuntu0.1) ...\nSetting up python-pip (8.1.1-2ubuntu0.1) ...\nSetting up python-pkg-resources (20.7.0-1) ...\nSetting up python-setuptools (20.7.0-1) ...\nSetting up python-wheel (0.29.0-1) ...\np@ScrapyPython3:~$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 627220E7\nExecuting: /tmp/tmp.ZQw2BsNciq/gpg.1.sh --keyserver\nhkp://keyserver.ubuntu.com:80\n--recv\n627220E7\ngpg: requesting key 627220E7 from hkp server keyserver.ubuntu.com\ngpg: key 627220E7: public key \"Scrapy Team (APT Signing Key) <info@scrapy.org>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1  (RSA: 1)\np@ScrapyPython3:~$ echo 'deb http://archive.scrapy.org/ubuntu scrapy main' | sudo tee /etc/apt/sources.list.d/scrapy.list\ndeb http://archive.scrapy.org/ubuntu scrapy main\np@ScrapyPython3:~$ \np@ScrapyPython3:~$ sudo apt-get update && sudo apt-get install scrapy\nGet:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [94.5 kB]\nHit:2 http://us.archive.ubuntu.com/ubuntu xenial InRelease                     \nGet:3 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease [94.5 kB]   \nGet:4 http://archive.scrapy.org/ubuntu scrapy InRelease [6,002 B]              \nHit:5 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease           \nGet:6 http://archive.scrapy.org/ubuntu scrapy/main amd64 Packages [6,085 B]\nGet:7 http://archive.scrapy.org/ubuntu scrapy/main i386 Packages [6,085 B]\nFetched 207 kB in 1s (161 kB/s)     \nReading package lists... Done\nW: http://archive.scrapy.org/ubuntu/dists/scrapy/InRelease: Signature by key 65D1B1CE9BCE5D04D51464F58F62CB1F627220E7 uses weak digest algorithm (SHA1)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nSome packages could not be installed. This may mean that you have\nrequested an impossible situation or if you are using the unstable\ndistribution that some required packages have not yet been created\nor been moved out of Incoming.\nThe following information may help to resolve the situation:\n\nThe following packages have unmet dependencies:\n scrapy : Depends: python-support (>= 0.90.0) but it is not installable\nE: Unable to correct problems, you have held broken packages.\np@ScrapyPython3:~$ \n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2016-06-17T05:54:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "902": {"issue_url": "https://github.com/scrapy/scrapy/issues/2057", "issue_id": "#2057", "issue_summary": "spidercls has no function \"update_settings\", but only \"update\"", "issue_description": "hajaja commented on Jun 15, 2016\n/home/csdong/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py in init(self, spidercls, settings)\n31 self.spidercls = spidercls\n32 self.settings = settings.copy()\n---> 33 self.spidercls.update_settings(self.settings)\nAttributeError: 'module' object has no attribute 'update_settings'", "issue_status": "Closed", "issue_reporting_time": "2016-06-15T07:25:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "903": {"issue_url": "https://github.com/scrapy/scrapy/issues/2056", "issue_id": "#2056", "issue_summary": "[DOC] Mention that Scrapy + Python 3 + Windows do not work together", "issue_description": "Contributor\nredapple commented on Jun 14, 2016\nEven if it's announced in https://blog.scrapinghub.com/2016/05/25/data-extraction-with-scrapy-and-python-3/\nit's not clear enough that Scrapy does not work on Python 3 on Windows\nImportError: cannot import name '_win32stdio'\nSee comments at http://stackoverflow.com/a/37818668/", "issue_status": "Closed", "issue_reporting_time": "2016-06-14T17:54:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "904": {"issue_url": "https://github.com/scrapy/scrapy/issues/2055", "issue_id": "#2055", "issue_summary": "Some of the errors occurred in the Chinese web crawling", "issue_description": "buzai commented on Jun 14, 2016 \u2022\nedited by redapple\nThe html source is :\n<dd>\n<a href=\"5367496.html\" =style=\"\" style=\"\">\u4e66\u53cb\u7fa4</a>\n</dd>\n<dd>\n<a href=\"5367497.html\" =style=\"\" style=\"\">\u5341\u6708\u4efd\u6253\u8d4f\u6e05\u5355\u3002</a>\n</dd>\n<dt>\u300a\u4fee\u771f\u804a\u5929\u7fa4\u300b\u4e5d\u6d32\u4e00\u53f7\u7fa4</dt>\n<dd>\n<a class=\"firefinder-match\" href=\"5367498.html\" =style=\"\" style=\"\">\u7b2c\u4e00\u7ae0 \u9ec4\u5c71\u771f\u541b\u548c\u4e5d\u6d32\u4e00\u53f7\u7fa4</a>\n</dd>\n<dd>\n<a class=\"firefinder-match\" href=\"5367499.html\" =style=\"\" style=\"\">\u7b2c\u4e8c\u7ae0 \u4e14\u5f85\u672c\u5c0a\u7b97\u4e0a\u4e00\u5366</a>\n</dd>\nmy spider is :\n    def parse(self, response):\n        # a = '\u7b2c'.encode('Unicode')\\\"h1title\\\"\n        arr = response.xpath(\"//dd//a[contains(.,\\\"\u7b2c\\\")]\").extract()\n        print arr[0]\nI just want the a tag with the word \"\u7b2c\" in the beginning.\nbut it not work , but in the fixfox firexpath i can find right.\nsome error is :\n2016-06-14 20:14:11 [scrapy] ERROR: Spider error processing <GET http://www.23us.cc/html/101/101573/> (referer: None)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/Twisted-16.2.0-py2.7-linux-x86_64.egg/twisted/internet/defer.py\", line 588, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/home/chenhao/Desktop/ssss/sinaweibo/sinaweibo/spiders/xiao.py\", line 18, in parse\n    arr = response.xpath(\"//dd//a[contains(.,\\\"\u7b2c\\\")]\").extract()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/http/response/text.py\", line 115, in xpath\n    return self.selector.xpath(query)\n  File \"/usr/local/lib/python2.7/dist-packages/parsel/selector.py\", line 179, in xpath\n    smart_strings=self._lxml_smart_strings)\n  File \"lxml.etree.pyx\", line 1507, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:52198)\n  File \"xpath.pxi\", line 295, in lxml.etree.XPathElementEvaluator.__call__ (src/lxml/lxml.etree.c:151999)\n  File \"apihelpers.pxi\", line 1393, in lxml.etree._utf8 (src/lxml/lxml.etree.c:27125)\nValueError: All strings must be XML compatible: Unicode or ASCII, no NULL bytes or control characters", "issue_status": "Closed", "issue_reporting_time": "2016-06-14T15:19:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "905": {"issue_url": "https://github.com/scrapy/scrapy/issues/2053", "issue_id": "#2053", "issue_summary": "Cannot import 'newspaper' package", "issue_description": "ChristopherLucas commented on Jun 13, 2016 \u2022\nedited\nHi,\nThanks for the great software.\nI'm trying to import the 'newspaper' module. However, I'm running into an error, which I can't sort. I'm using scrapy-1.1.0rc1.\nScrapy was only initilializing with Python2 libraries, so I had to append the directory where 'newspaper' is installed to path. However, I'm getting a downstream import error. Any help would be much appreciated. I looked in the documentation but couldn't find anything.\nThanks! Code and error below.\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nfrom newspapers.items import NewspapersItem\n\nimport sys\nprint(sys.path)\nsys.path.extend(['/home/christopher/.local/lib/python3.5/site-packages'])\nprint(sys.path)\nimport newspaper\n\nclass NewspaperSpider(CrawlSpider):\n  # Spider here\nAnd output.\n$ scrapy crawl newspapers -o newspapers.json -t json\n['/usr/bin', '/usr/lib/python27.zip', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-linux2', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/lib/python2.7/site-packages', '/usr/lib/python2.7/site-packages/gtk-2.0', '/home/christopher/Misc/scrapy_sandbox/newspapers']\n['/usr/bin', '/usr/lib/python27.zip', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-linux2', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/lib/python2.7/site-packages', '/usr/lib/python2.7/site-packages/gtk-2.0', '/home/christopher/Misc/scrapy_sandbox/newspapers', '/home/christopher/.local/lib/python3.5/site-packages']\nTraceback (most recent call last):\n  File \"/usr/bin/scrapy\", line 9, in <module>\n    load_entry_point('Scrapy==1.0.5', 'console_scripts', 'scrapy')()\n  File \"/usr/lib/python2.7/site-packages/scrapy/cmdline.py\", line 142, in execute\n    cmd.crawler_process = CrawlerProcess(settings)\n  File \"/usr/lib/python2.7/site-packages/scrapy/crawler.py\", line 209, in __init__\n    super(CrawlerProcess, self).__init__(settings)\n  File \"/usr/lib/python2.7/site-packages/scrapy/crawler.py\", line 115, in __init__\n    self.spider_loader = _get_spider_loader(settings)\n  File \"/usr/lib/python2.7/site-packages/scrapy/crawler.py\", line 296, in _get_spider_loader\n    return loader_cls.from_settings(settings.frozencopy())\n  File \"/usr/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 30, in from_settings\n    return cls(settings)\n  File \"/usr/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 21, in __init__\n    for module in walk_modules(name):\n  File \"/usr/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 71, in walk_modules\n    submod = import_module(fullpath)\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/home/christopher/Misc/scrapy_sandbox/newspapers/newspapers/spiders/newspaper_spider.py\", line 10, in <module>\n    import newspaper\n  File \"/home/christopher/.local/lib/python3.5/site-packages/newspaper/__init__.py\", line 10, in <module>\n    from .article import Article, ArticleException\n  File \"/home/christopher/.local/lib/python3.5/site-packages/newspaper/article.py\", line 12, in <module>\n    from . import images\n  File \"/home/christopher/.local/lib/python3.5/site-packages/newspaper/images.py\", line 15, in <module>\n    import urllib.request\nImportError: No module named request", "issue_status": "Closed", "issue_reporting_time": "2016-06-13T03:07:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "906": {"issue_url": "https://github.com/scrapy/scrapy/issues/2051", "issue_id": "#2051", "issue_summary": "if response.status == 403,I want to autochange ip_proxy. how i do ?", "issue_description": "jin10086 commented on Jun 12, 2016\n'class ProxyMiddleware(object):\ndef process_request(self, request, spider):\nproxy = random.choice(PROXIES)\nif proxy['user_pass'] is not None:\nrequest.meta['proxy'] = \"http://%s\" % proxy['ip_port']\nencoded_user_pass = base64.encodestring(proxy['user_pass'])\nrequest.headers['Proxy-Authorization'] = 'Basic ' + encoded_user_pass\nprint \"ProxyMiddleware have pass********_\" + proxy['ip_port']\nelse:\nprint \"_ProxyMiddleware no pass********\" + proxy['ip_port']\nrequest.meta['proxy'] = \"http://%s\" % proxy['ip_port']\n'\n\ud83d\udc4e 1", "issue_status": "Closed", "issue_reporting_time": "2016-06-12T01:39:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "907": {"issue_url": "https://github.com/scrapy/scrapy/issues/2049", "issue_id": "#2049", "issue_summary": "utils.is_gzipped returns false on application/x-gzip;charset=utf-8", "issue_description": "Contributor\nTethik commented on Jun 12, 2016 \u2022\nedited\nI had a site return the following header, which caused SitemapSpider to not parse a sitemap xml that was gzipped.\nContent-Type: application/x-gzip;charset=utf-8\nLooking into the code it seems that the function utils.is_gzipped does not take into account cases where the Content-Type header would include charset.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-06-12T00:46:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "908": {"issue_url": "https://github.com/scrapy/scrapy/issues/2047", "issue_id": "#2047", "issue_summary": "add \"flags\" kwarg to Scrapy Request", "issue_description": "Contributor\npawelmhm commented on Jun 10, 2016\nScrapy response.flags are really useful and underappreciated feature. It would be useful to add a way to pass flags argument to Request at initialization, e.g.\ndef some_callback(self, response):\n      yield Request('http://example.com', flags={\"issued from\": \"some_callback\", \"going to\": \"somewhere\")\nand then in log output you will have:\n[scrapy.core.engine] Crawled (200) <GET http://example.com>  ['issued_from\": \"some_callback', \"going to\": \"somewhere\"]\nThis is useful because it allows you to have some heplful debugging metadata directly in logs. If you want to debug some spider problem you will have some metadata directly in logs and you dont have to start debugger in some callback and investigate response meta or some other stuff.", "issue_status": "Closed", "issue_reporting_time": "2016-06-10T12:05:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "909": {"issue_url": "https://github.com/scrapy/scrapy/issues/2037", "issue_id": "#2037", "issue_summary": "Some logging should indicate spider", "issue_description": "Contributor\nrgtk commented on Jun 8, 2016\nLogs such as: INFO: Dumping Scrapy stats, INFO: Closing spider (shutdown), INFO: Crawled.... should indicate from which spider log was issued. It is must-have to debug and analyze logs for crawler that have multiple spiders.", "issue_status": "Closed", "issue_reporting_time": "2016-06-08T09:57:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "910": {"issue_url": "https://github.com/scrapy/scrapy/issues/2035", "issue_id": "#2035", "issue_summary": "LOG_UNSERIALIZABLE_REQUESTS messages should be WARNINGs, not ERRORs", "issue_description": "Member\nkmike commented on Jun 6, 2016\nCurrently if LOG_UNSERIALIZABLE_REQUESTS is True and request can't be serialized users get\nERROR: Unable to serialize request: ...\nmessages in log (see here). I think they are confusing because spider still works. What do you think about changing log level to WARNING?", "issue_status": "Closed", "issue_reporting_time": "2016-06-06T13:44:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "911": {"issue_url": "https://github.com/scrapy/scrapy/issues/2033", "issue_id": "#2033", "issue_summary": "Document 'download_latency' key in requests' meta dict", "issue_description": "Contributor\nredapple commented on Jun 6, 2016 \u2022\nedited\nrequest.meta['download_latency'] can be useful for analysing a spider or a website, but it's not documented (see #2021).", "issue_status": "Closed", "issue_reporting_time": "2016-06-06T10:27:29Z", "fixed_by": "#2328", "pull_request_summary": "[MRG+1] Document `download_latency` meta key", "pull_request_description": "Member\nstummjr commented on Oct 18, 2016\nThis PR aims to fix #2033\nThoughts on it?", "pull_request_status": "Merged", "issue_fixed_time": "2016-11-14T16:24:14Z", "files_changed": [["10", "docs/topics/request-response.rst"]]}, "912": {"issue_url": "https://github.com/scrapy/scrapy/issues/2028", "issue_id": "#2028", "issue_summary": "Will enabling ajaxcrawl miss some web contents?", "issue_description": "ibowen commented on Jun 5, 2016\nHello Scrapy, when I enabled ajaxcrawl in setting.py, some contents on non-ajax pages were lost. What could it be the problem? Is it possible to switch the ajaxcrawl in spider?\nMy code:\nhttps://github.com/ibowen/crowdfunding/blob/master/seedinvest_crawl/seedinvest_crawl/spiders/seedinvest.py\nScrapy View of enabling ajaxcrawl:\nScrapy View of unenabling ajaxcrawl:", "issue_status": "Closed", "issue_reporting_time": "2016-06-05T02:07:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "913": {"issue_url": "https://github.com/scrapy/scrapy/issues/2027", "issue_id": "#2027", "issue_summary": "Scrapy getting errors with Proxies -- twisted.python.failure.Failure OpenSSL.SSL.Error", "issue_description": "liondancer commented on Jun 4, 2016 \u2022\nedited\nI'm pretty new to scrapy and I am trying to scrape some craigslist pages using some proxies but I am getting some errors as shown below. I tried the command scrapy shell \"https://craigslist.org\" and it seemed to work fine.\nFrom my understanding, if I want to use proxies, I have to build custom Downloader Middleware. I have done so here:\nclass ProxyConnect(object):\n    def __init__(self):\n        self.proxies = None\n        with open(os.path.join(os.getcwd(), \"chisel\", \"downloaders\", \"resources\", \"config.json\")) as config:\n            proxies = json.load(config)\n            self.proxies = proxies[\"proxies\"]\n\n    def process_request(self, request, spider):\n        if \"proxy\" in request.meta:\n            return\n        proxy = random.choice(self.proxies)\n        ip, port, username, password = proxy[\"ip\"], proxy[\"port\"], proxy[\"username\"], proxy[\"password\"]\n        request.meta[\"proxy\"] = \"http://\" + ip + \":\" + port\n        user_pass = username + \":\" + password\n        if user_pass:\n            basic_auth = 'Basic ' + base64.encodestring(user_pass)\n            request.headers['Proxy-Authorization'] = basic_auth\nThis is my project structure:\n/chisel\n    __init__.py\n    pipelines.py\n    items.py\n    settings.py\n    /downloaders\n        __init__.py\n        /downloader_middlewares\n            __init__.py\n        proxy_connect.py\n        /resources\n          config.json\n    /spiders\n        __init__.py\n        craiglist_spider.py\n        /spider_middlewares\n            __init__.py\n        /resources\n          craigslist.json\nscrapy.cfg\nsettings.py:\nDOWNLOADER_MIDDLEWARES = {\n    'chisel.downloaders.downloader_middlewares.proxy_connect.ProxyConnect': 100,\n    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110\n}\nI was able to test to see if my proxy is working with this command it and it worked and got a source page back\ncurl -x 'http://{USERNAME}:{PASSWORD}@{IP}:{PORT}' -v \"http://www.google.com/\"\nScrapy version\n$ scrapy version -v\nScrapy    : 1.1.0\nlxml      : 3.6.0.0\nlibxml2   : 2.9.2\nTwisted   : 16.2.0\nPython    : 2.7.10 (default, Oct 23 2015, 19:19:21) - [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2h  3 May 2016)\nPlatform  : Darwin-15.5.0-x86_64-i386-64bit\nError:\n$ scrapy crawl craigslist\n2016-06-04 01:44:14 [scrapy] INFO: Scrapy 1.1.0 started (bot: chisel)\n2016-06-04 01:44:14 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'chisel.spiders', 'SPIDER_MODULES': ['chisel.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'chisel'}\n2016-06-04 01:44:14 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-06-04 01:44:14 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'chisel.downloaders.downloader_middlewares.proxy_connect.ProxyConnect',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-06-04 01:44:14 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-06-04 01:44:14 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-06-04 01:44:14 [scrapy] INFO: Spider opened\n2016-06-04 01:44:14 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-06-04 01:44:14 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-06-04 01:44:16 [scrapy] DEBUG: Retrying <GET https://geo.craigslist.org/robots.txt> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-06-04 01:44:17 [scrapy] DEBUG: Retrying <GET https://geo.craigslist.org/robots.txt> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-06-04 01:44:18 [scrapy] DEBUG: Gave up retrying <GET https://geo.craigslist.org/robots.txt> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-06-04 01:44:18 [scrapy] ERROR: Error downloading <GET https://geo.craigslist.org/robots.txt>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\nResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-06-04 01:44:20 [scrapy] DEBUG: Retrying <GET https://geo.craigslist.org/iso/MD> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-06-04 01:44:21 [scrapy] DEBUG: Retrying <GET https://geo.craigslist.org/iso/MD> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-06-04 01:44:24 [scrapy] DEBUG: Gave up retrying <GET https://geo.craigslist.org/iso/MD> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-06-04 01:44:24 [scrapy] ERROR: Error downloading <GET https://geo.craigslist.org/iso/MD>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2016-06-04 01:44:24 [scrapy] INFO: Closing spider (finished)\n2016-06-04 01:44:24 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 6,\n 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 6,\n 'downloader/request_bytes': 1668,\n 'downloader/request_count': 6,\n 'downloader/request_method_count/GET': 6,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 6, 4, 8, 44, 24, 329662),\n 'log_count/DEBUG': 7,\n 'log_count/ERROR': 2,\n 'log_count/INFO': 7,\n 'scheduler/dequeued': 3,\n 'scheduler/dequeued/memory': 3,\n 'scheduler/enqueued': 3,\n 'scheduler/enqueued/memory': 3,\n 'start_time': datetime.datetime(2016, 6, 4, 8, 44, 14, 963452)}\n2016-06-04 01:44:24 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-06-04T08:57:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "914": {"issue_url": "https://github.com/scrapy/scrapy/issues/2025", "issue_id": "#2025", "issue_summary": "Command line: allow multiple output pipelines", "issue_description": "Contributor\neltermann commented on Jun 4, 2016\nSo, for reasons, I have a spider that yields 3 kinds of items, let's say TypeOneItem, TypeTwoItem and TypeTreeItem.\nI'd like to do something like this:\n$ scrapy crawl myspider --criteria1=\"isinstance(TypeOneItem)\" -t1 csv -o1 items_of_type1.csv --criteria2=\"isinstance(TypeTwoItem)\" -o2 items_of_type2.json --criteria3=\"isinstance(TypeTreeItem)\" -t3 xml -o3 items_of_type3.xml\nOf course, the names of arguments and fields could be different, but that is the idea.\nAny thoughs?", "issue_status": "Closed", "issue_reporting_time": "2016-06-03T23:49:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "915": {"issue_url": "https://github.com/scrapy/scrapy/issues/2024", "issue_id": "#2024", "issue_summary": "Increase number of full spider examples in documents", "issue_description": "mobcdi commented on Jun 3, 2016\nI believe for new users it would be useful if you provided more full examples of spiders in the documentation e.g. working crawler. The documentation is detailed but with the various files that need to be edited or created to create a working spider it can be difficult for new users to get up to speed.\nThese reference spiders could also be used to guide users in solving their problem e.g use the basic crawler as per document .... and add a rule to ....\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-06-03T09:41:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "916": {"issue_url": "https://github.com/scrapy/scrapy/issues/2022", "issue_id": "#2022", "issue_summary": "How should I save data to disk while crawling?", "issue_description": "GoingMyWay commented on Jun 1, 2016 \u2022\nedited\nSorry, it is not a programming error.\nAs I know, Scrapy crawls the data into buffer, and when the program exit, it saves the data to disk.\nMy machine has 8G memory, and the data I am crawling maybe 8-10G.\nHow can I set the scrapy to save data while crawling so that it can avoid memory error\uff1f\nThere are 2200000 pages to be crawled, And how can I set that after crawled 10000 pages, it saves the data to disk?", "issue_status": "Closed", "issue_reporting_time": "2016-06-01T03:40:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "917": {"issue_url": "https://github.com/scrapy/scrapy/issues/2021", "issue_id": "#2021", "issue_summary": "The url download time", "issue_description": "15310944349 commented on May 31, 2016\nI want scrapy send http request to the time it takes to receive the content, I do not know if you have any good suggestions?", "issue_status": "Closed", "issue_reporting_time": "2016-05-31T12:45:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "918": {"issue_url": "https://github.com/scrapy/scrapy/issues/2020", "issue_id": "#2020", "issue_summary": "Scrapy runs and does not create new html files for site it is scraping.", "issue_description": "Karla-Isabel-Sandoval commented on May 31, 2016 \u2022\nedited by redapple\nScrapy runs and does not create new html files for the site it is scraping.\nIt runs successfully with a (200) message and is able to reach Telnet.\nScrapy is no longer producing new html files and or a separate resource file as suggested here .\nI am confused as to why my scrapy crawl $myprojectname is not fetching any new html data?\n2016-05-31 11:32:48 [scrapy] INFO: Spider opened\n2016-05-31 11:32:48 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-05-31 11:32:48 [scrapy] DEBUG: Telnet console listening on 127.1.5.1:****\nDEBUG: Crawled (200)\n'log_count/DEBUG': 4,\n 'log_count/INFO': 7,\n 'offsite/domains': 1,\n 'offsite/filtered': 25,\n 'request_depth_max': 1,\n 'response_received_count': 2,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 5, 31, 9, 32, 48, 773501)}\n2016-05-31 11:32:50 [scrapy] INFO: Spider closed (finished)\nResults from the crawl above are not in order and the console listening number has been changed for privacy.", "issue_status": "Closed", "issue_reporting_time": "2016-05-31T09:31:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "919": {"issue_url": "https://github.com/scrapy/scrapy/issues/2019", "issue_id": "#2019", "issue_summary": "pip to install scrapy", "issue_description": "chengxxxxwang commented on May 28, 2016\nI am new to using command line and apologize in advance for any confusion. I am trying to install scrapy but I get an Errno 13 Permission denied as response.\nException:\nTraceback (most recent call last):\nFile \"/Library/Python/2.7/site-packages/pip-8.1.0-py2.7.egg/pip/basecommand.py\", line 209, in main\nstatus = self.run(options, args)\nFile \"/Library/Python/2.7/site-packages/pip-8.1.0-py2.7.egg/pip/commands/install.py\", line 317, in run\nprefix=options.prefix_path,\nFile \"/Library/Python/2.7/site-packages/pip-8.1.0-py2.7.egg/pip/req/req_set.py\", line 732, in install\n**kwargs\nFile \"/Library/Python/2.7/site-packages/pip-8.1.0-py2.7.egg/pip/req/req_install.py\", line 835, in install\nself.move_wheel_files(self.source_dir, root=root, prefix=prefix)\nFile \"/Library/Python/2.7/site-packages/pip-8.1.0-py2.7.egg/pip/req/req_install.py\", line 1030, in move_wheel_files\nisolated=self.isolated,\nFile \"/Library/Python/2.7/site-packages/pip-8.1.0-py2.7.egg/pip/wheel.py\", line 344, in move_wheel_files\nclobber(source, lib_dir, True)\nFile \"/Library/Python/2.7/site-packages/pip-8.1.0-py2.7.egg/pip/wheel.py\", line 315, in clobber\nensure_dir(destdir)\nFile \"/Library/Python/2.7/site-packages/pip-8.1.0-py2.7.egg/pip/utils/init.py\", line 83, in ensure_dir\nos.makedirs(path)\nFile \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 157, in makedirs\nmkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/pyasn1'", "issue_status": "Closed", "issue_reporting_time": "2016-05-28T03:15:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "920": {"issue_url": "https://github.com/scrapy/scrapy/issues/2018", "issue_id": "#2018", "issue_summary": "The setting ITEM_PIPELINE can't be overridden from terminal", "issue_description": "liusangel commented on May 27, 2016\nIn my settings.py I have:\nITEM_PIPELINES = {\n    'turing.pipelines.InitFieldsNotInitializedPipeline': 299,\n    'turing.pipelines.SetNoneIfFieldEmptyPipeline': 300,\n    'turing.pipelines.CheckCategoryPipeline': 301,\n    'turing.pipelines.CheckContactPipeline': 302,\n}\nAnd it works great. But sometime I want run the spider without ANY pipelines.\nWhen I run\nscrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES=[] -a test_extract_url=http://example.com/ example_spider\nI get this error:\nreturn d.iteritems(**kw)\nexceptions.AttributeError: 'str' object has no attribute 'iteritems'\nHow can I run the spider without the pipelines?\nSo far I tried:\nscrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES=[] -a test_extract_url=http://example.com/ example_spider\nscrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES={} -a test_extract_url=http://example.com/ example_spider\nscrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s \"ITEM_PIPELINES=[]\" -a test_extract_url=http://example.com/ example_spider\nscrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s \"ITEM_PIPELINES={}\" -a test_extract_url=http://example.com/ example_spider\nscrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES=['turing.pipelines.InitFieldsNotInitializedPipeline': 299,] -a test_extract_url=http://example.com/ example_spider\nscrapy crawl -s FEED_URI=stdout: -s FEED_FORMAT=json -s ITEM_PIPELINES={'turing.pipelines.InitFieldsNotInitializedPipeline': 299,} -a test_extract_url=http://example.com/ example_spider\nOthers combinations\nLook in the docs http://doc.scrapy.org/en/latest/topics/settings.html\nHopefully you can help me. Thanks.", "issue_status": "Closed", "issue_reporting_time": "2016-05-27T15:42:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "921": {"issue_url": "https://github.com/scrapy/scrapy/issues/2017", "issue_id": "#2017", "issue_summary": "request_fingerprint not is unique", "issue_description": "nengine commented on May 27, 2016\nPlease see an example below. Should it generate same fingerprint? Thanks.\nfrom scrapy.http import Request\nfrom scrapy.utils.request import request_fingerprint\n\nr1 = Request(\"http://www.example.com/123\")\nr2 = Request(\"http://example.com/123\")\n\nprint request_fingerprint(r1)\nprint request_fingerprint(r2)\n1577e4ad857665390d44cd04a638104d0575d903\na907c28bf08125b8a87535a117c2d8a4a629415c", "issue_status": "Closed", "issue_reporting_time": "2016-05-27T11:11:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "922": {"issue_url": "https://github.com/scrapy/scrapy/issues/2014", "issue_id": "#2014", "issue_summary": "`RobotsTxtMiddleware` sometimes misbehaves", "issue_description": "Contributor\nstarrify commented on May 27, 2016\nSome requests might have been fetched before RobotsTxtMiddleware could start filtering them:\n...\n43: 2016-05-27 05:13:25 DEBUG   [scrapy.core.engine] Crawled (200) <GET http://i1.adis.ws/s/boohooamplience/szz99792_ms.json?protocol=http&deep=true> (referer: http://www.boohoo.com/new-in/yolanda-boutique-bandage-neon-cut-out-swimsuit/invt/szz99792)\n44: 2016-05-27 05:13:27 DEBUG   [scrapy.core.engine] Crawled (200) <GET http://mark.reevoo.com/robots.txt> (referer: None)\n45: 2016-05-27 05:13:27 DEBUG   [scrapy.core.engine] Crawled (200) <GET http://mark.reevoo.com/reevoomark/product_summary?locale=en-GB&sku=szz99940&trkref=BHO&callback=> (referer: http://i1.adis.ws/s/boohooamplience/szz99940_ms.json?protocol=http&deep=true)\n46: 2016-05-27 05:13:27 DEBUG   [scrapy.downloadermiddlewares.robotstxt] Forbidden by robots.txt: <GET http://mark.reevoo.com/reevoomark/product_summary?locale=en-GB&sku=szz99792&trkref=BHO&callback=>\n47: 2016-05-27 05:13:27 DEBUG   [scrapy.core.scraper] Scraped from <200 http://mark.reevoo.com/reevoomark/product_summary?locale=en-GB&sku=szz99940&trkref=BHO&callback=> \n...\nAnd once that happens, following IgnoreRequest exceptions are not handled (logs from the same job):\n...\n62: 2016-05-26 20:11:55 ERROR   [scrapy.core.scraper] Spider error processing <GET http://mark.reevoo.com/reevoomark/product_summary?locale=en-GB&sku=mzz83566&trkref=BHO&callback=> (referer: http://i1.adis.ws/s/boohooamplience/mzz83566_ms.json?protocol=http&deep=true)\nIgnoreRequest\n...\n66: 2016-05-26 20:11:57 ERROR   [scrapy.core.scraper] Spider error processing <GET http://mark.reevoo.com/reevoomark/product_summary?locale=en-GB&sku=dzz80021&trkref=BHO&callback=> (referer: http://i1.adis.ws/s/boohooamplience/dzz80021_ms.json?protocol=http&deep=true)\nIgnoreRequest\n...\n70: 2016-05-26 20:12:00 ERROR   [scrapy.core.scraper] Spider error processing <GET http://mark.reevoo.com/reevoomark/product_summary?locale=en-GB&sku=dzz78696&trkref=BHO&callback=> (referer: http://i1.adis.ws/s/boohooamplience/dzz78696_ms.json?protocol=http&deep=true)\nIgnoreRequest\n...\nFor details of the job, please check https://staging.scrapinghub.com/p/53580/job/17/5/#/log (sorry but the link is not publicly accessible)", "issue_status": "Closed", "issue_reporting_time": "2016-05-27T06:17:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "923": {"issue_url": "https://github.com/scrapy/scrapy/issues/2012", "issue_id": "#2012", "issue_summary": "Customable self-adaptive delay?", "issue_description": "wlnirvana commented on May 26, 2016\nMy crawler frequently got recaptcha page after about 15mins scraping. I am considering to implement a recaptcha page detection, and when it is detected enforce a delay on all subsequent requests to wait for my IP to be unblocked.", "issue_status": "Closed", "issue_reporting_time": "2016-05-26T05:35:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "924": {"issue_url": "https://github.com/scrapy/scrapy/issues/2011", "issue_id": "#2011", "issue_summary": "Tried to stop a LoopingCall that was not running", "issue_description": "ramwin commented on May 26, 2016 \u2022\nedited by kmike\nI'm using mysql to store my spider data, but when I set the piplines to store data into local mysql server, it raise an error.\npipelines.py\nimport json\nimport requests\nfrom mysql.connector import connection\n\nMYSQL_SERVER = '192.168.1.90'   # using this, the spider can run\nMYSQL_SERVER = 'localhost'   # using this, the spider raise an error\nMYSQL_DB = 'scrapy'\nMYSQL_USER = 'crawler'\nMYSQL_PASS = 'crawl'\nMYSQL_TABLE = 'pm25in'\n\nclass Pm25InPipeline(object):\n    def __init__(self):                                                                \n        pass\n\n    def process_item(self, item, spider):                                              \n        command = '''insert into {table} (monitortime, monitorcity, monitorpoint,      \n            AQIindex, airsituation, primarypullutant, PM25content, PM10content,        \n            CO, NO2, O3_1h, O3_8h, SO2)                                                \n            values ( \"{monitortime}\", \"{monitorcity}\",\"{monitorpoint}\", \"{AQIindex}\",  \n            \"{airsituation}\", \"{primarypollutant}\", {PM25content}, {PM10content},      \n            {CO}, {NO2}, {O3_1h}, {O3_8h}, {SO2} );                                    \n            '''.format(table=MYSQL_TABLE, **dict(item))                                \n        self.cursor.execute(command)                                                   \n        return item                                                                    \n\n    def open_spider(self, spider):                                                     \n        self.cnx = connection.MySQLConnection(                                         \n            host=MYSQL_SERVER,                                                         \n            user=MYSQL_USER,                                                           \n            password=MYSQL_PASS,                                                       \n            database=MYSQL_DB,                                                         \n            charset='utf8')                                                            \n        self.cursor = self.cnx.cursor()                                                \n\n    def close_spider(self,spider):                                                     \n        self.cnx.commit()                                                              \n        self.cnx.close()                                                               \nthe Traceback:\nwangx@wangx-PC:~/github/pm25in$ scrapy crawl pm25spider\nUnhandled error in Deferred:\n\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py\", line 57, in run\n    self.crawler_process.crawl(spname, **opts.spargs)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 163, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 167, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1274, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1126, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 87, in crawl\n    yield self.engine.close()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 100, in close\n    return self._close_all_spiders()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 340, in _close_all_spiders\n    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 298, in close_spider\n    dfd = slot.close()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 44, in close\n    self._maybe_fire_closing()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 51, in _maybe_fire_closing\n    self.heartbeat.stop()\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py\", line 202, in stop\n    assert self.running, (\"Tried to stop a LoopingCall that was \"\nexceptions.AssertionError: Tried to stop a LoopingCall that was not running.\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2016-05-26T02:13:02Z", "fixed_by": "#2382", "pull_request_summary": "Test Slot's heartbeat state before stopping it", "pull_request_description": "Contributor\nredapple commented on Nov 7, 2016\nAlso add a test on state of looping task in LogStats extension.\nFixes #2011 and #2362\nBefore (scrapy 1.2.1 Python 2.7.12):\n(with my test bogus pipeline from #2362 (comment)\nclass CryptictracebacksPipeline(object):\n\n    def open_spider(self, spider):\n        a = int(spider)/0\n\n    def process_item(self, item, spider):\n        return item\n)\n$ scrapy crawl example\n2016-11-07 16:51:52 [scrapy] INFO: Scrapy 1.2.1 started (bot: cryptictracebacks)\n(...)\n2016-11-07 16:51:52 [scrapy] INFO: Enabled item pipelines:\n['cryptictracebacks.pipelines.CryptictracebacksPipeline']\n2016-11-07 16:51:52 [scrapy] INFO: Spider opened\n2016-11-07 16:51:52 [scrapy] INFO: Closing spider (shutdown)\nUnhandled error in Deferred:\n2016-11-07 16:51:52 [twisted] CRITICAL: Unhandled error in Deferred:\n\n2016-11-07 16:51:52 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1258, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 87, in crawl\n    yield self.engine.close()\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 100, in close\n    return self._close_all_spiders()\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 340, in _close_all_spiders\n    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 298, in close_spider\n    dfd = slot.close()\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 44, in close\n    self._maybe_fire_closing()\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 51, in _maybe_fire_closing\n    self.heartbeat.stop()\n  File \"/home/paul/.virtualenvs/scrapy12/local/lib/python2.7/site-packages/twisted/internet/task.py\", line 202, in stop\n    assert self.running, (\"Tried to stop a LoopingCall that was \"\nAssertionError: Tried to stop a LoopingCall that was not running.\nAfter (still Python 2):\n$ scrapy crawl example\n2016-11-07 16:52:58 [scrapy] INFO: Scrapy 1.2.1 started (bot: cryptictracebacks)\n(...)\n2016-11-07 16:52:58 [scrapy] INFO: Enabled item pipelines:\n['cryptictracebacks.pipelines.CryptictracebacksPipeline']\n2016-11-07 16:52:58 [scrapy] INFO: Spider opened\n2016-11-07 16:52:58 [scrapy] INFO: Closing spider (shutdown)\n2016-11-07 16:52:58 [scrapy] INFO: Dumping Scrapy stats:\n{'finish_reason': 'shutdown',\n 'finish_time': datetime.datetime(2016, 11, 7, 15, 52, 58, 180288),\n 'log_count/INFO': 6}\n2016-11-07 16:52:58 [scrapy] INFO: Spider closed (shutdown)\nUnhandled error in Deferred:\n2016-11-07 16:52:58 [twisted] CRITICAL: Unhandled error in Deferred:\n\n\nTraceback (most recent call last):\n  File \"/home/paul/src/scrapy/scrapy/commands/crawl.py\", line 57, in run\n    self.crawler_process.crawl(spname, **opts.spargs)\n  File \"/home/paul/src/scrapy/scrapy/crawler.py\", line 163, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"/home/paul/src/scrapy/scrapy/crawler.py\", line 167, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/home/paul/.virtualenvs/scrapydev/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1331, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"/home/paul/.virtualenvs/scrapydev/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1185, in _inlineCallbacks\n    result = g.send(result)\n  File \"/home/paul/src/scrapy/scrapy/crawler.py\", line 90, in crawl\n    six.reraise(*exc_info)\n  File \"/home/paul/src/scrapy/scrapy/crawler.py\", line 74, in crawl\n    yield self.engine.open_spider(self.spider, start_requests)\nexceptions.TypeError: int() argument must be a string or a number, not 'ExampleSpider'\n2016-11-07 16:52:58 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"/home/paul/.virtualenvs/scrapydev/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1185, in _inlineCallbacks\n    result = g.send(result)\n  File \"/home/paul/src/scrapy/scrapy/crawler.py\", line 90, in crawl\n    six.reraise(*exc_info)\n  File \"/home/paul/src/scrapy/scrapy/crawler.py\", line 74, in crawl\n    yield self.engine.open_spider(self.spider, start_requests)\nTypeError: int() argument must be a string or a number, not 'ExampleSpider'\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2016-11-08T12:32:07Z", "files_changed": [["3", "scrapy/core/engine.py"], ["3", "scrapy/extensions/logstats.py"], ["11", "tests/pipelines.py"], ["13", "tests/test_crawl.py"]]}, "925": {"issue_url": "https://github.com/scrapy/scrapy/issues/2010", "issue_id": "#2010", "issue_summary": "Unicode Link Extractor", "issue_description": "k-m-engin commented on May 26, 2016\nWhen using the following to extract all of the links from a response:\nself.link_extractor = LinkExtractor()\n...\nlinks = self.link_extractor.extract_links(response)\nOn rare occasions, the following error is thrown:\n2016-05-25 12:13:55,432 [root] [ERROR]  Error on http://detroit.curbed.com/2016/5/5/11605132/tiny-house-designer-show, traceback: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 1203, in mainLoop\n    self.runUntilCurrent()\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 825, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 393, in callback\n    self._startRunCallbacks(result)\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 501, in _startRunCallbacks\n    self._runCallbacks()\n--- <exception caught here> ---\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 588, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/var/www/html/DomainCrawler/DomainCrawler/spiders/hybrid_spider.py\", line 223, in parse\n    items.extend(self._extract_requests(response))\n  File \"/var/www/html/DomainCrawler/DomainCrawler/spiders/hybrid_spider.py\", line 477, in _extract_requests\n    links = self.link_extractor.extract_links(response)\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/linkextractors/lxmlhtml.py\", line 111, in extract_links\n    all_links.extend(self._process_links(links))\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/linkextractors/__init__.py\", line 103, in _process_links\n    link.url = canonicalize_url(urlparse(link.url))\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/url.py\", line 85, in canonicalize_url\n    parse_url(url), encoding=encoding)\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/url.py\", line 46, in _safe_ParseResult\n    to_native_str(parts.netloc.encode('idna')),\n  File \"/usr/local/lib/python2.7/encodings/idna.py\", line 164, in encode\n    result.append(ToASCII(label))\n  File \"/usr/local/lib/python2.7/encodings/idna.py\", line 73, in ToASCII\n    raise UnicodeError(\"label empty or too long\")\nexceptions.UnicodeError: label empty or too long\nI was able to find some information concerning the error from here.\nMy question is: What is the best way to handle this? Even if there is one bad link in the response, I'd want all of the other good links to be extracted.", "issue_status": "Closed", "issue_reporting_time": "2016-05-25T21:10:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "926": {"issue_url": "https://github.com/scrapy/scrapy/issues/2004", "issue_id": "#2004", "issue_summary": "MediaPipeline (and ImagesPipeline/FilesPipeline) does not handle HTTP redirections", "issue_description": "ghost commented on May 23, 2016\nBasically, what's happened is that my spider is unable to download the files because the file_urls provided are actually redirected to the final download link. However, because of the following code, the redirect download middleware is effectively disabled, which makes the download fail.\ndef _check_media_to_download(self, result, request, info):\n        if result is not None:\n            return result\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        else:\n            request.meta['handle_httpstatus_all'] = True\n            dfd = self.crawler.engine.download(request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        return dfd\nAnd here is the check in the redirect middleware that disables it:\nif (request.meta.get('dont_redirect', False) or\n                response.status in getattr(spider, 'handle_httpstatus_list', []) or\n                response.status in request.meta.get('handle_httpstatus_list', []) or\n                request.meta.get('handle_httpstatus_all', False)):\n            return response\nMy question is: What is the point of enabling the handling of httpstatus_all, when it effectively disables all of the checks?\n\ud83d\udc4d 4\n\ud83d\ude15 8", "issue_status": "Closed", "issue_reporting_time": "2016-05-22T18:34:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "927": {"issue_url": "https://github.com/scrapy/scrapy/issues/2003", "issue_id": "#2003", "issue_summary": "Could not find suitable distribution for Requirement.parse('Scrapy==1.1.0')", "issue_description": "robsonpeixoto commented on May 21, 2016\nWhen I execute the python setup.py develop I got the error:\nSearching for Scrapy==1.1.0\nReading https://pypi.python.org/simple/Scrapy/\nNo local packages or download links found for Scrapy==1.1.0\nerror: Could not find suitable distribution for Requirement.parse('Scrapy==1.1.0')", "issue_status": "Closed", "issue_reporting_time": "2016-05-21T01:41:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "928": {"issue_url": "https://github.com/scrapy/scrapy/issues/2002", "issue_id": "#2002", "issue_summary": "Are there plans to add Web ARChive support?", "issue_description": "robsonpeixoto commented on May 20, 2016 \u2022\nedited\nFrom Wikipedia, the free encyclopedia\nThe Web ARChive (WARC) archive format specifies a method for combining multiple digital resources into an aggregate archive file together with related information. The WARC format is a revision of the Internet Archive's ARC File Format[5] that has traditionally been used to store \"web crawls\" as sequences of content blocks harvested from the World Wide Web. The WARC format generalizes the older format to better support the harvesting, access, and exchange needs of archiving organizations. Besides the primary content currently recorded, the revision accommodates related secondary content, such as assigned metadata, abbreviated duplicate detection events, and later-date transformations.[6]\nWARC is now recognised by most national library systems as the standard to follow for web archival.[7]\nhttps://en.wikipedia.org/wiki/Web_ARChive", "issue_status": "Closed", "issue_reporting_time": "2016-05-19T21:13:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "929": {"issue_url": "https://github.com/scrapy/scrapy/issues/2000", "issue_id": "#2000", "issue_summary": "TextResponse doesn't initialize when it needed", "issue_description": "Contributor\nmatveinazaruk commented on May 20, 2016\nWhen I perform rest request ('POST' with body and headers), I got a response with text body. But type of response in callback is Response, but expected - TextResponse.", "issue_status": "Closed", "issue_reporting_time": "2016-05-19T19:22:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "930": {"issue_url": "https://github.com/scrapy/scrapy/issues/1999", "issue_id": "#1999", "issue_summary": "TextResponse doesn't initialize when it needed.", "issue_description": "Contributor\nmatveinazaruk commented on May 20, 2016\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2016-05-19T19:20:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "931": {"issue_url": "https://github.com/scrapy/scrapy/issues/1998", "issue_id": "#1998", "issue_summary": "Cannot import '_win32stdio' (but pywin32 is already installed)", "issue_description": "RussBaz commented on May 19, 2016\nI am using Python 3.5.1 (64 bit), Windows 10, VS 2015 Update 2. lxml (3.6.0) and pywin32 (220.1) are installed. Scrapy (1.1.0) was installed successfully. Then, when I run an example from 'http://doc.scrapy.org/en/latest/intro/overview.html' in my virtual environment, I get the following exception:\n(env) D:\\Projects\\tscrapy> scrapy runspider stackoverflow_spider.py -o top-stackoverflow-questions.json\n2016-05-19 17:36:00 [scrapy] INFO: Scrapy 1.1.0 started (bot: scrapybot)\n2016-05-19 17:36:00 [scrapy] INFO: Overridden settings: {'FEED_URI': 'top-stackoverflow-questions.json', 'FEED_FORMAT': 'json'}\n2016-05-19 17:36:00 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.feedexport.FeedExporter']\nUnhandled error in Deferred:\n2016-05-19 17:36:00 [twisted] CRITICAL: Unhandled error in Deferred:\n\n\nTraceback (most recent call last):\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\commands\\runspider.py\", line 87, in run\n    self.crawler_process.crawl(spidercls, **opts.spargs)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 163, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 167, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1274, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 72, in crawl\n    self.engine = self._create_engine()\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 97, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\core\\engine.py\", line 68, in __init__\n    self.downloader = downloader_cls(crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\core\\downloader\\__init__.py\", line 88, in __init__\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\middleware.py\", line 34, in from_settings\n    mwcls = load_object(clspath)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 44, in load_object\n    mod = import_module(module)\n  File \"C:\\Program Files\\Python 3.5\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\n\n  File \"<frozen importlib._bootstrap_external>\", line 662, in exec_module\n\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\downloadermiddlewares\\retry.py\", line 23, in <module>\n    from scrapy.xlib.tx import ResponseFailed\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\xlib\\tx\\__init__.py\", line 3, in <module>\n    from twisted.web import client\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\web\\client.py\", line 41, in <module>\n    from twisted.internet.endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\endpoints.py\", line 34, in <module>\n    from twisted.internet.stdio import StandardIO, PipeAddress\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\stdio.py\", line 30, in <module>\n    from twisted.internet import _win32stdio\nbuiltins.ImportError: cannot import name '_win32stdio'\n2016-05-19 17:36:00 [twisted] CRITICAL:\nTraceback (most recent call last):\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 72, in crawl\n    self.engine = self._create_engine()\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\crawler.py\", line 97, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\core\\engine.py\", line 68, in __init__\n    self.downloader = downloader_cls(crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\core\\downloader\\__init__.py\", line 88, in __init__\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\middleware.py\", line 34, in from_settings\n    mwcls = load_object(clspath)\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 44, in load_object\n    mod = import_module(module)\n  File \"C:\\Program Files\\Python 3.5\\lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 662, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\downloadermiddlewares\\retry.py\", line 23, in <module>\n    from scrapy.xlib.tx import ResponseFailed\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\scrapy\\xlib\\tx\\__init__.py\", line 3, in <module>\n    from twisted.web import client\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\web\\client.py\", line 41, in <module>\n    from twisted.internet.endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\endpoints.py\", line 34, in <module>\n    from twisted.internet.stdio import StandardIO, PipeAddress\n  File \"d:\\projects\\tscrapy\\env\\lib\\site-packages\\twisted\\internet\\stdio.py\", line 30, in <module>\n    from twisted.internet import _win32stdio\nImportError: cannot import name '_win32stdio'", "issue_status": "Closed", "issue_reporting_time": "2016-05-19T11:52:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "932": {"issue_url": "https://github.com/scrapy/scrapy/issues/1997", "issue_id": "#1997", "issue_summary": "IGNORED_EXTENSIONS not working because of url_has_any_extension implementation", "issue_description": "knaveofdiamonds commented on May 19, 2016\nThe scrapy.linkextractors.IGNORED_EXTENSIONS were not being ignored for me in my scraper run. Digging into it, this seems to be because posixpath returns the extension with the '.', but the extensions are defined without the '.'\nFor example, this test fails:\ndef test_url_has_any_extension(self):\n    self.assertTrue(url_has_any_extension('http://www.example.com/image.jpg', ['jpg']))\nWhat is the correct solution here - to add stripping the '.' in url_has_any_extension or to add the '.' before all the extensions in IGNORED_EXTENSIONS? (Happy to do the work for a patch, but not sure which way to go).", "issue_status": "Closed", "issue_reporting_time": "2016-05-19T11:30:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "933": {"issue_url": "https://github.com/scrapy/scrapy/issues/1996", "issue_id": "#1996", "issue_summary": "Restarting a scrapy instance or call parse method manually", "issue_description": "pawneetdev commented on May 19, 2016 \u2022\nedited\nI want to call the parse method manually or restart scrapy after the scraping is complete. Is it possible? I found nothing related to that in scrapy documentation. Also, can I run call it infinitely until a condition is satisfied?", "issue_status": "Closed", "issue_reporting_time": "2016-05-19T04:41:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "934": {"issue_url": "https://github.com/scrapy/scrapy/issues/1993", "issue_id": "#1993", "issue_summary": "while crawling website like https://www.netflix.com,getting Forbidden by robots.txt: <GET https://www.netflix.com/>", "issue_description": "dgrene commented on May 17, 2016\nwhile crawling website like https://www.netflix.com, getting Forbidden by robots.txt: <GET https://www.netflix.com/>\nERROR: No response downloaded for: https://www.netflix.com/", "issue_status": "Closed", "issue_reporting_time": "2016-05-17T11:26:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "935": {"issue_url": "https://github.com/scrapy/scrapy/issues/1990", "issue_id": "#1990", "issue_summary": "request_to_dict error when executing enqueue_request (Scheduler)", "issue_description": "rafaelcapucho commented on May 14, 2016\nHello,\nI was trying to implement the scrapy-redis, the problem was discussed over there but after some debugging I see that the problem could be in scrapy.\nThe redis_scheduler is simple: https://github.com/rolando/scrapy-redis/blob/master/scrapy_redis/scheduler.py\nThey use a simple queue.push(request):\n    def enqueue_request(self, request):\n        if not request.dont_filter and self.df.request_seen(request):\n            return\n        if self.stats:\n            self.stats.inc_value('scheduler/enqueued/redis', spider=self.spider)\n        self.queue.push(request)\nAfter that, the request pass through 2 method:\n    def push(self, request):\n        \"\"\"Push a request\"\"\"\n        self.server.lpush(self.key, self._encode_request(request))\n\n    def _encode_request(self, request):\n        \"\"\"Encode a request object\"\"\"\n        return pickle.dumps(request_to_dict(request, self.spider), protocol=-1)\nI have added 3 prints into scrapy request_to_dict to understand the inputs:\ndef request_to_dict(request, spider=None):\n    \"\"\"Convert Request object to a dict.\n\n    If a spider is given, it will try to find out the name of the spider method\n    used in the callback and store that as the callback.\n    \"\"\"\n    cb = request.callback\n    print('callback: ', cb)\n    print('request: ', request)\n    print('spider: ', spider)\n\n    if callable(cb):\n        cb = _find_method(spider, cb)\n    eb = request.errback\n    if callable(eb):\n        eb = _find_method(spider, eb)\n    d = {\n        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)\n        'callback': cb,\n        'errback': eb,\n        'method': request.method,\n        'headers': dict(request.headers),\n        'body': request.body,\n        'cookies': request.cookies,\n        'meta': request.meta,\n        '_encoding': request._encoding,\n        'priority': request.priority,\n        'dont_filter': request.dont_filter,\n    }\n    return d\nRunning the shell command It return:\ncallback: <bound method Deferred.callback of <Deferred at 0x7f3e13b69358>> \nrequest: <GET http://www.epocacosmeticos.com.br/any-url-goes-here>\nspider: <EpocaCosmeticosSpider 'epocacosmeticos.com.br' at 0x7f3e13032d30>\nThis is the error that I found when running the shell command with the scrapy-redis scheduler:\nValueError: Function <bound method Deferred.callback of <Deferred at 0x7f3e13b69358>> is not a method of: <EpocaCosmeticosSpider 'epocacosmeticos.com.br' at 0x7f3e13032d30>\nThe error is generated by _find_method\ndef _find_method(obj, func):\n    if obj:\n        try:\n            func_self = six.get_method_self(func)\n        except AttributeError:  # func has no __self__\n            pass\n        else:\n            if func_self is obj:\n                return six.get_method_function(func).__name__\n    raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))\nThe request is created by the shell command, why it can't be used into request_to_dict?\nUsing python 3.5.1 - scrapy 1.1.0,\nThank you", "issue_status": "Closed", "issue_reporting_time": "2016-05-14T06:36:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "936": {"issue_url": "https://github.com/scrapy/scrapy/issues/1987", "issue_id": "#1987", "issue_summary": "New Instance of scrapy.Request", "issue_description": "gdomod commented on May 12, 2016\nHi there,\ni want to scraps a board with different users.\ni create a subclass from Request but\nclass Test(scrapy.Request): def __init__(self, *args, **kwargs): super(Test, self).__init__(*args, **kwargs)\nBut i didnt get new instance from Request Object, so there i always got same cookies etc.\nCan anybody help me please", "issue_status": "Closed", "issue_reporting_time": "2016-05-12T14:13:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "937": {"issue_url": "https://github.com/scrapy/scrapy/issues/1985", "issue_id": "#1985", "issue_summary": "Scrapy 1.1.0 ImagesPipeline backward incompatibility", "issue_description": "Contributor\nredapple commented on May 11, 2016 \u2022\nedited\n#1891 is not backward compatible.\nUsers of Scrapy<1.1 ImagesPipeline could access upper-case attributes,\ne.g.\nclass CustomImagesPipeline(ImagesPipeline):\n    (...)\n    def item_completed(self, results, item, info):\n        item = super(CustomImagesPipeline, self).item_completed(\n            results, item, info)\n        # Note: not all items do have an images field.\n        if self.IMAGES_RESULT_FIELD not in item.fields:\n            return item\nLeading to the following exception in 1.1.0(rc4):\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 588, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/tmp/unpacked-eggs/__main__.egg/*****/pipelines/screenshots.py\", line 539, in item_completed\n    results, item, info)\n  File \"/tmp/unpacked-eggs/__main__.egg/*****/pipelines/screenshots.py\", line 242, in item_completed\n    images = item.pop(self.IMAGES_RESULT_FIELD, [])\nAttributeError: 'CustomImagesPipeline' object has no attribute 'IMAGES_RESULT_FIELD'", "issue_status": "Closed", "issue_reporting_time": "2016-05-11T18:24:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "938": {"issue_url": "https://github.com/scrapy/scrapy/issues/1982", "issue_id": "#1982", "issue_summary": "'=' added at end of urls extracted from LinkExtractor", "issue_description": "gmargari commented on May 11, 2016\n$ scrapy shell \"https://uat.payleap.com/transactservices.svc\"\n> from scrapy.linkextractors import LinkExtractor\n> [ l.url for l in LinkExtractor(allow=()).extract_links(response) ]\n['https://uat.payleap.com/TransactServices.svc?wsdl=']\nScrapy shell info:\n2016-05-11 16:37:21 [scrapy] INFO: Scrapy 1.0.5 started (bot: scrapybot)\n2016-05-11 16:37:21 [scrapy] INFO: Optional features available: ssl, http11\n2016-05-11 16:37:21 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2016-05-11 16:37:22 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, CoreStats, SpiderState\n2016-05-11 16:37:22 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2016-05-11 16:37:22 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2016-05-11 16:37:22 [scrapy] INFO: Enabled item pipelines: \n2016-05-11 16:37:22 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n2016-05-11 16:37:22 [scrapy] INFO: Spider opened\n2016-05-11 16:37:23 [scrapy] DEBUG: Crawled (200) <GET https://uat.payleap.com/transactservices.svc> (referer: None)", "issue_status": "Closed", "issue_reporting_time": "2016-05-11T13:40:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "939": {"issue_url": "https://github.com/scrapy/scrapy/issues/1978", "issue_id": "#1978", "issue_summary": "Delay after every 5 minutes", "issue_description": "pawneetdev commented on May 11, 2016\nDOWNLOAD_DELAY gives delay between each request. I want to give a delay of 10 seconds after every 5 minutes.", "issue_status": "Closed", "issue_reporting_time": "2016-05-11T04:30:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "940": {"issue_url": "https://github.com/scrapy/scrapy/issues/1976", "issue_id": "#1976", "issue_summary": "empty WARNING message in scrapy.core.downloader.tls (1.1.0rc4/master)", "issue_description": "Contributor\nnyov commented on May 11, 2016\nSometimes I'm getting empty warnings now, on 1.1.0rc4 and master branch.\n(at least on rc3 as well)\n2016-05-07 00:33:46 [scrapy.core.downloader.tls] WARNING: \n2016-05-07 00:33:47 [scrapy.core.downloader.tls] WARNING: \n2016-05-07 00:33:48 [scrapy.core.downloader.tls] WARNING: \nIt happens in a broad linkcheck crawl; so I couldn't pinpoint what URLs might be responsible for that, at this time. The only other observation so far is, that it doesn't happen on a cache-replayed run (which might be obvious, as there is no TLS there).", "issue_status": "Closed", "issue_reporting_time": "2016-05-11T03:29:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "941": {"issue_url": "https://github.com/scrapy/scrapy/issues/1971", "issue_id": "#1971", "issue_summary": "Minor issue in comments of custom deprecation handler", "issue_description": "umrashrf commented on May 8, 2016\nI made some tests and I found out this comment is not completely true.\nAlso, if some code uses issubclass(sub, OldName) or isinstance(sub(), OldName) checks they'll still return True if sub is a subclass of NewName instead of OldName.\nI think this is always the case even without using create_deprecated_class. Can someone else check and confirm if that's right?", "issue_status": "Closed", "issue_reporting_time": "2016-05-08T04:54:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "942": {"issue_url": "https://github.com/scrapy/scrapy/issues/1970", "issue_id": "#1970", "issue_summary": "problem of scrapy runspider spider -a", "issue_description": "wangjian2014 commented on May 7, 2016\nThe content of my scrapy script is like\ncoding:utf-8\nimport sys\nimport scrapy\nimport json\nimport logging\nfrom scrapy.mail import MailSender\nimport time\nimport datetime\nclass StackOverflowSpider(scrapy.Spider):\nname = 'stackoverflow'\nstart_urls = ['http://www.baidu.com']\ndef __init__(self,systemName,date1):\n    print 'date1 is '+date1\n    dateNow = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    self.file = open('errorlog/'+systemName+'_'+dateNow+'_error.log', 'a')\n    self.systemName=systemName\n    self.date1=date1\n    date2=''\n    self.date2=date2\n\ndef parse(self, response):\nI want to use shell script to run this spider.The content of my shell script(date-who-script) is like\n!/bin/bash\ndatebegin=\"2016-05-04 11:00:00\"\nscrapy runspider sn_yunji_email_with_inputparam.py -a systemName='EPPSOAAS' -a date1=$datebegin\nWhen I run this shell script like ./date-who-script,it can't run.But if I change $datebegin to datebegin,it can run.But the value of date1 is 'datebegin',not \"2016-05-04 11:00:00\".\nI think the symbol of $ is not recognized by scrapy.\nhow can I post the value of variable datebegin to scrapy? thank you very much!", "issue_status": "Closed", "issue_reporting_time": "2016-05-07T13:40:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "943": {"issue_url": "https://github.com/scrapy/scrapy/issues/1969", "issue_id": "#1969", "issue_summary": "User timeout caused connection failure", "issue_description": "night1008 commented on May 7, 2016 \u2022\nedited\nHello, when I send requests at the same time(with dont_filter=True, because some will be error, and the 'DOWNLOAD_TIMEOUT': 60),\nsome requests appear the info as following:\nRetrying <GET http://www.xxx> (failed 1 times): User timeout caused connection failure: Getting http://www.xxx took longer than 1800 seconds..\nIt doesn't appear other error, and some log that\nhave crawled XX pages, and XX items.\nI know the log from https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/webclient.py\nbut I don't know why.\nCan someone help me?", "issue_status": "Closed", "issue_reporting_time": "2016-05-07T03:00:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "944": {"issue_url": "https://github.com/scrapy/scrapy/issues/1968", "issue_id": "#1968", "issue_summary": "response.css can't find elements that is there", "issue_description": "rafaelcapucho commented on May 6, 2016\nHello,\nWhen running\nscrapy shell https://www.belezanaweb.com.br/homens/\nAnd executing a simple:\nIn [2]: response.css('div.item').extract()\nOut[2]: []\nThe item is over there:\nIt isn't load using JS because it is visible in curl:\ncurl https://www.belezanaweb.com.br/homens/ | less:\nCould it be a parser bug? Running on Scrapy1.1.0rc4\nThank you\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-05-06T04:56:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "945": {"issue_url": "https://github.com/scrapy/scrapy/issues/1967", "issue_id": "#1967", "issue_summary": "Issue with using bindaddress", "issue_description": "mikkogozalo commented on May 5, 2016\nIt seems that bindaddress isn't working. Snippet of the code where I test it is here:\nhttps://gist.github.com/mikkogozalo/e44975444307a37078d43f8bc88e2489\nDebug output is here:\nhttps://gist.github.com/mikkogozalo/739d17964e2c133c80191bae22ef502e", "issue_status": "Closed", "issue_reporting_time": "2016-05-05T03:05:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "946": {"issue_url": "https://github.com/scrapy/scrapy/issues/1965", "issue_id": "#1965", "issue_summary": "Add option to set JSON output encoding", "issue_description": "Contributor\nredapple commented on May 2, 2016\nSome users may prefer UTF-8 encoding of characters when using JSON output, instead of the the default \\UXXXX espace sequences. See #1963 (comment).\nThe option to turn off ensure_ascii and/or output encoding for JsonItemExporter could be interesting.", "issue_status": "Closed", "issue_reporting_time": "2016-05-02T12:44:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "947": {"issue_url": "https://github.com/scrapy/scrapy/issues/1963", "issue_id": "#1963", "issue_summary": "The encoding problem in outputing data to file", "issue_description": "wi-cuckoo commented on Apr 29, 2016\nHere I got an upset problem. When I use -o file option, I find the text in file just display as below:\n{\"title\": \"Docker \\u5b9e\\u6218\\u7cfb\\u5217 \\u2014 \\u8fd0\\u884c\\u7b2c\\u4e00\\u4e2a\\u5bb9\\u5668\", \"short_desc\": \"\\u5f53\\u4f60\\u5bf9\\u4e00\\u4e9b\\u6982\\u5ff5\\u7406\\u89e3\\u4e0d\\u6df1\\u7684\\u65f6\\u5019\\uff0c\\u5b9e\\u8df5\\u662f\\u6700\\u597d\\u7684\\u89e3\\u51b3\\u529e\\u6cd5\\n\\n\", \"time\": \"07 Apr 2015\"},\nwhile I run file outputfile command in terminal, it shows blog.json: ASCII text, with very long lines. I can't understand, really confused. I just want the file display with my local language. And I know write utf-8 string to file will work correctly. So I added encode('utf-8') before assigning to Item['key'], but still no any change. Please help me, thx", "issue_status": "Closed", "issue_reporting_time": "2016-04-29T15:43:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "948": {"issue_url": "https://github.com/scrapy/scrapy/issues/1961", "issue_id": "#1961", "issue_summary": "update Requirements section in README", "issue_description": "Member\nkmike commented on Apr 29, 2016\nIt says Python 2.7 is required, but Scrapy 1.1+ supports Python 3.3+ as well. I think we should update it as a part of 1.1 release, but not earlier - it may confuse users.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-04-28T19:21:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "949": {"issue_url": "https://github.com/scrapy/scrapy/issues/1959", "issue_id": "#1959", "issue_summary": "ImportError: No module named spiders", "issue_description": "ghost commented on Apr 28, 2016 \u2022\nedited by redapple\nI'm sure my script is ok because it can run on my old computer \uff0cbut on my new computer it get this error\nE:\\python\\python27\\python.exe E:/pycharm/PycharmProjects/spider/test/novelspider/main.py\nTraceback (most recent call last):\n  File \"E:/pycharm/PycharmProjects/spider/test/novelspider/main.py\", line 3, in <module>\n    cmdline.execute(\"scrapy crawl novspider\".split())\n  File \"E:\\python\\python27\\lib\\site-packages\\scrapy\\cmdline.py\", line 142, in execute\n    cmd.crawler_process = CrawlerProcess(settings)\n  File \"E:\\python\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 209, in __init__\n    super(CrawlerProcess, self).__init__(settings)\n  File \"E:\\python\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 115, in __init__\n    self.spider_loader = _get_spider_loader(settings)\n  File \"E:\\python\\python27\\lib\\site-packages\\scrapy\\crawler.py\", line 296, in _get_spider_loader\n    return loader_cls.from_settings(settings.frozencopy())\n  File \"E:\\python\\python27\\lib\\site-packages\\scrapy\\spiderloader.py\", line 30, in from_settings\n    return cls(settings)\n  File \"E:\\python\\python27\\lib\\site-packages\\scrapy\\spiderloader.py\", line 21, in __init__\n    for module in walk_modules(name):\n  File \"E:\\python\\python27\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 63, in walk_modules\n    mod = import_module(path)\n  File \"E:\\python\\python27\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: No module named spiders\nI do not know where the problem is , please help me !", "issue_status": "Closed", "issue_reporting_time": "2016-04-28T02:23:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "950": {"issue_url": "https://github.com/scrapy/scrapy/issues/1957", "issue_id": "#1957", "issue_summary": "how send post request", "issue_description": "aslijiasheng commented on Apr 27, 2016 \u2022\nedited by redapple\nI am trying to crawl latest reviews from http://www.ci123.com/category.php/8029/848/ store but return 404 http codes.\nwith the the postman post request works and I get desired response\n\nbut use scrapy post request return 404\nimport scrapy\nfrom scrapy.http import FormRequest\n\nclass DmozSpider(scrapy.Spider):\n    name = \"ci123\"\n    allowed_domains = [\"ci123.com\"]\n\n    def start_requests(self):\n        return [ FormRequest(\"http://www.ci123.com/category.php/8029/848/\",\n            formdata={'someparam': 'foo', 'otherparam': 'bar'},\n            callback=self.parse, method = \"POST\") ]\n\n    def parse(self, response):\n        return scrapy.FormRequest.from_response(\n                response,\n                formdata={'username': 'john', 'password': 'secret'},\n                callback=self.after_login\n                )\n    def after_login(self, response):\n            # check login succeed before going on\n        if \"authentication failed\" in response.body:\n            self.logger.error(\"Login failed\")\n            return\n\n        # continue scraping with authenticated session...\ncodes problem??", "issue_status": "Closed", "issue_reporting_time": "2016-04-27T03:15:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "951": {"issue_url": "https://github.com/scrapy/scrapy/issues/1956", "issue_id": "#1956", "issue_summary": "disable 'stable' version on readthedocs.org", "issue_description": "Member\nkmike commented on Apr 26, 2016\nCurrently 'latest' points to 1.0, 'stable' points to 1.1 and 'master' points to 1.2. I.e. latest is the oldest, and stable is not released yet.\nWhat do you think about removing 'stable' from the list of doc versions? I can do it.", "issue_status": "Closed", "issue_reporting_time": "2016-04-26T17:50:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "952": {"issue_url": "https://github.com/scrapy/scrapy/issues/1955", "issue_id": "#1955", "issue_summary": "ImportError: cannot import name walk_modules", "issue_description": "fanpei91 commented on Apr 26, 2016\nWhen I execute scrapy command, here is the error message:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/__init__.py\", line 48, in <module>\n    from scrapy.spiders import Spider\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/spiders/__init__.py\", line 115, in <module>\n    from scrapy.spiders.crawl import CrawlSpider, Rule\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/spiders/crawl.py\", line 11, in <module>\n    from scrapy.utils.spider import iterate_spider_output\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/utils/spider.py\", line 7, in <module>\n    from scrapy.utils.misc import  arg_to_iter\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 4, in <module>\n    from importlib import import_module\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 5, in <module>\n    import scrapy.spiderloader\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/spiderloader.py\", line 7, in <module>\n    from scrapy.utils.misc import walk_modules\nImportError: cannot import name walk_modules", "issue_status": "Closed", "issue_reporting_time": "2016-04-26T17:03:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "953": {"issue_url": "https://github.com/scrapy/scrapy/issues/1953", "issue_id": "#1953", "issue_summary": "Scrapy 1.1.0rc3 error on fetch command", "issue_description": "rafaelcapucho commented on Apr 26, 2016\nHello,\nWhen I run the fetch command I get errors:\nhttps://paste.ee/r/35fj7\nThe spider is fine considering that I can run the crawl command normally.\nUsing Py 3.5.1", "issue_status": "Closed", "issue_reporting_time": "2016-04-25T20:49:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "954": {"issue_url": "https://github.com/scrapy/scrapy/issues/1946", "issue_id": "#1946", "issue_summary": "Doc Issue - Installing on Fedora 23 (possibly other rhel based as well)", "issue_description": "XakV commented on Apr 21, 2016\nHello - this is a paste of how I got scrapy to successfully install on Fedora 23. It may be of use to other RHEL based distros as well. It's a bit tricky now that Python3 is the default.\n(sorry about the cut and paste - it's late)\nPre-Reqs;\npython-devel, python-rpm-macros, libffi-devel, redhat-rpm-config, openssl-devel, libxml2-devel, libxml-devel, python-lxml, python-libxml2, python-cffi, glib2-devel gnet2-devel libxslt-devel\nand then finally\npython2 -m pip install --user --upgrade pip\n(in a virtualenv)\nI might've missed something, but hope this helps.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-04-21T04:27:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "955": {"issue_url": "https://github.com/scrapy/scrapy/issues/1941", "issue_id": "#1941", "issue_summary": "canonicalize_url in linkextractor: not what browsers do", "issue_description": "Member\nDigenis commented on Apr 20, 2016 \u2022\nedited\nBy default, the link extractor calls canonicalize_url on the collected links.\nThe following is not what browsers do:\ncanonicalize_url('http://example.com/index.php?/a/=/o/')\n'http://example.com/index.php?%2Fa%2F=%2Fo%2F'  # encoding forward slashes\ncanonicalize_url('http://example.com/index.php?a')\n'http://example.com/index.php?a='  # appending = on empty arguments\nI doubt this is a problem in canonicalize_url\nbecause it's not meant to mimic browsers in the first place, is it?\nHowever this is a problem for the link extractor\nbecause it can potentially end up extracting urls\nthat are wrong from the server's perspective.\nIn this example, the server doesn't recognise the extractor's url, only the browser's:\n# http://forum.laptop.bg/index.php?/discover/\nLinkExtractor(restrict_xpaths=('//a[contains(@href, \"/topic\")]',)).extract_links(response)[0].url\n# Extractor: http://forum.laptop.bg/index.php?%2Ftopic%2F57339-%D0%BB%D0%B0%D0%BF%D1%82%D0%BE%D0%BF-asus-w90vp%2F=&comment=221153&do=findComment\n# Browser:   http://forum.laptop.bg/index.php?/topic/57339-%D0%BB%D0%B0%D0%BF%D1%82%D0%BE%D0%BF-asus-w90vp/&do=findComment&comment=221153\nWas this a design decision or a bug?", "issue_status": "Closed", "issue_reporting_time": "2016-04-20T10:57:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "956": {"issue_url": "https://github.com/scrapy/scrapy/issues/1940", "issue_id": "#1940", "issue_summary": "Better API for creating requests from responses", "issue_description": "Member\nkmike commented on Apr 20, 2016 \u2022\nedited\nSometime Request needs information about the response to be sent correctly. There are at least 2 use cases:\nURL encoding depends on response encoding (see #1923);\nrelative URLs should be resolved based on response.url or its base url (see #548).\nI think the current API is not good enough. The most obvious code is not correct:\nfor url in response.xpath(\"//a/@href\").extract():\n    yield scrapy.Request(url, self.parse)\nTo do that correctly user has to write the following:\nfor url in response.xpath(\"//a/@href\").extract():\n    yield scrapy.Request(response.urljoin(url), self.parse, encoding=response.encoding)\nOr this:\nfor link in LinkExtractor().extract_links(response):\n    yield scrapy.Request(link.url, self.parse, encoding=response.encoding)\nLinkeExtractor solution has gotchas, e.g. canonicalize_url is called by default and fragments are removed. It means that e.g. Ajax crawlable URLs are not handled (no escaped_fragment even if a website supports it); it also makes it harder to use Scrapy with scrapy-splash which can handle fragments.\nThis all is too easy to get wrong; I think just documenting these gotchas is not good enough for a framework - it should make the easiest way to write something the correct way. IMHO in the API shouldn't require user to instantiate weird objects or pass response encoding:\nfor url in response.xpath(\"//a/@href\").extract():\n    something.send_request(url, self.parse)\nThis can be implemented if we provide a method on Response to send new requests.\nA related use case is async def functions or methods (#1144 (comment)): it is not possible to yield Requests in async def functions, so adding a request should be either a method of self or a method of response if we want to support async def callbacks.\nFormRequest.from_response(response, ...) can also be written as something like response.submit(...).\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2016-04-20T01:05:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "957": {"issue_url": "https://github.com/scrapy/scrapy/issues/1937", "issue_id": "#1937", "issue_summary": "It's can not be real time updated, when I use request.meta pass a dict argument to a callback", "issue_description": "authen29 commented on Apr 18, 2016 \u2022\nedited by kmike\nThis is the part of codes:\ndef detail_page_request(self, response):\n      ct_item = response.meta.get('ct_item')\n      ct_item['title'] = title\n      req = Request(url, headers=self.headers, meta={'ct_item':ct_item, })\n      return req\n\ndef parse(self, response):\n      ct_item = response.meta.get(\"ct_item\")\n     print ct_item\nI get the ct_item not updated by the code:ct_item['title'] = title", "issue_status": "Closed", "issue_reporting_time": "2016-04-18T09:12:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "958": {"issue_url": "https://github.com/scrapy/scrapy/issues/1936", "issue_id": "#1936", "issue_summary": "SSL errors crawling https sites using proxies", "issue_description": "lhuaizhong commented on Apr 18, 2016 \u2022\nedited\nI'm unable to scrape below https sites through https supported proxies, have tried version 1.0.5 and 1.1.0rc3. I'm using proxymesh, but https://www.python.org and others work well.\n(env-scrapy-1.1) D:\\work>scrapy version -v\nScrapy : 1.1.0rc3\nlxml : 3.6.0.0\nlibxml2 : 2.9.0\nTwisted : 16.1.1\nPython : 2.7.11 (v2.7.11:6d1b6a68f775, Dec 5 2015, 20:32:19) [MSC v.1500 32 bit (Intel)]\npyOpenSSL : 16.0.0 (OpenSSL 1.0.2g 1 Mar 2016)\nPlatform : Windows-10-10.0.10586\n2016-04-18 12:56:25 [scrapy] DEBUG: Gave up retrying <GET https://xxx.xxx.xxx/> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>] 2016-04-18 12:56:25 [scrapy] ERROR: Error downloading <GET https://xxx.xxx.xxx/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]\nD:\\work>scrapy version -v\nScrapy : 1.0.5\nlxml : 3.5.0.0\nlibxml2 : 2.9.3\nTwisted : 15.5.0\nPython : 2.7.11 (v2.7.11:6d1b6a68f775, Dec 5 2015, 20:32:19) [MSC v.1500 32 bit (Intel)]\npyOpenSSL : 0.15.1 (OpenSSL 1.0.2f 28 Jan 2016)\nPlatform : Windows-10-10.0.10586\n2016-04-18 12:58:29 [scrapy] ERROR: Error downloading <GET https://xxx.xxx.xxx/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]>]\nThe #1429 (comment) doesn't work as well.\nMay anyone help? thanks a lot in advance.", "issue_status": "Closed", "issue_reporting_time": "2016-04-18T05:05:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "959": {"issue_url": "https://github.com/scrapy/scrapy/issues/1935", "issue_id": "#1935", "issue_summary": "scrapper is getting erros", "issue_description": "flik commented on Apr 18, 2016 \u2022\nedited by redapple\n scrapy runspider a.py -o top-stackoverflow-questions.json\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/__init__.py\", line 48, in <module>\n    from scrapy.spiders import Spider\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spiders/__init__.py\", line 10, in <module>\n    from scrapy.http import Request\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/http/__init__.py\", line 8, in <module>\n    from scrapy.http.headers import Headers\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/http/headers.py\", line 2, in <module>\n    from w3lib.http import headers_dict_to_raw\nImportError: No module named w3lib.http", "issue_status": "Closed", "issue_reporting_time": "2016-04-18T04:30:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "960": {"issue_url": "https://github.com/scrapy/scrapy/issues/1934", "issue_id": "#1934", "issue_summary": "FormRequest.from_response() doesn't always use method POST", "issue_description": "cd65d commented on Apr 15, 2016\nFormRequest(response) defaults to method of POST in form.py so I cannot figure out why in some cases form_response() actually sends a GET:\nimport scrapy\n\nclass LoginSpider(scrapy.Spider):\n    name = 'foo'\n    start_urls = ['http://strongswiftdurable.com/daily-training-sessions/']\n\n    def parse(self, response):\n        return scrapy.FormRequest.from_response(\n            response,\n            formdata={'username':'<username>', #has been redacted\n                    'password':'<password>', #has been redacted\n                    '_wpnonce':'96700335a2', #this changes every 24hrs via wp_nonce see https://codex.wordpress.org/WordPress_Nonces  \n                    '_wp_http_referer':'/daily-training-sessions/',\n                    'login':'Login',\n                    'redirect':'/daily-training-sessions/',\n                    },\n            callback=self.after_login\n        )\n\n    def after_login(self, response):\n        # check login succeed before going on\n        if \"Daily Training Sessions and Included Plans\" in response.body:\n            print \"worked!\"\n            return\n        else:\n            print \"Error\"\nhere's the packet capture:\nGET /?s=&username=<redacted>%40<redacted>&redirect=%2Fdaily-training-sessions%2F&_wp_http_referer=%2Fdaily-training-sessions%2F&login=Login&password=<redacted>&_wpnonce=96700335a2 HTTP/1.1\nAccept-Language: en\nAccept-Encoding: gzip,deflate\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nUser-Agent: Scrapy/1.0.5 (+http://scrapy.org)\nHost: strongswiftdurable.com\nReferer: http://strongswiftdurable.com/daily-training-sessions/\nSo for some reason the request is in the form of a query string vs a POST. however if i recall query string don't usually have the \"s=\" in the front.", "issue_status": "Closed", "issue_reporting_time": "2016-04-14T21:04:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "961": {"issue_url": "https://github.com/scrapy/scrapy/issues/1932", "issue_id": "#1932", "issue_summary": "Request issue about parameter headers", "issue_description": "hsh075623201 commented on Apr 14, 2016\nHello,\nurl == \"http://113.108.216.86/api/roll/get?pageid=155&lid=1686&num=10&page=1\nheaders = {\n                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n                    \"Accept-Language\": \"zh-CN,zh;q=0.8\",\n                    \"Connection\": \"keep-alive\",\n                    \"Host\": \"feed.mix.sina.com.cn\",\n                    \"Upgrade-Insecure-Requests\": \"1\",\n                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\"\n                }\n                Request(url, dont_filter=True,headers=headers)\nThe code get errors as bellow:\n2016-04-14 17:51:39 [scrapy] DEBUG: Crawled (404) <GET http://113.108.216.86/api/roll/get?pageid=155&lid=1686&num=10&page=1> (referer: None)\n2016-04-14 17:51:40 [scrapy] DEBUG: Ignoring response <404 http://113.108.216.86/api/roll/get?pageid=155&lid=1686&num=10&page=1>: HTTP status code is not handled or not allowed\nbut , similar code when i use requests module ,it works. pls see bellow:\nheaders = {\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n        \"Accept-Language\": \"zh-CN,zh;q=0.8\",\n        \"Connection\": \"keep-alive\",\n        \"Host\": \"feed.mix.sina.com.cn\",\n        \"Upgrade-Insecure-Requests\": \"1\",\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\"\n    }\n    res = requests.get(\"http://113.108.216.86/api/roll/get?pageid=155&lid=1686&num=10&page=1\", headers=headers)\nSo, how can I use parameter headers in scrapy?\nThanks very much in advance!!", "issue_status": "Closed", "issue_reporting_time": "2016-04-14T10:19:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "962": {"issue_url": "https://github.com/scrapy/scrapy/issues/1931", "issue_id": "#1931", "issue_summary": "start_requests() not taking the Dynamic Variables", "issue_description": "kgrvamsi commented on Apr 14, 2016\nI'm being trying to iterate a loop of urls in which i have three layers of scraping done\n1)Scrapy the first URL and get the respective data\n2)Based upon the data use that in one of the for loop range\n3)Run the Scraping and fetch new urls and append it to a variable and then again parse them\nHere is the example code that i'm looking out for\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\n\nclass TestSpider(BaseSpider):\n    name = \"test_spider\"\n    allowed_domains=[\"amazon.com\"]\n    some_value=0\n    data=[]\n    def start_requests(self):\n        yield Request(first-url,callback=self.parse)\n       ''' this is the place the some_value is not replaced with the new value '''\n        for i in range(2, some_value):\n            url = 'http://www.amazon.com/dp/%i/' % i\n            yield Request(url,callback=self.somefunction)\n\n         for site in data:\n           yield Request(site,callback=self.somefunction)\n\n    def parse(self, response):\n        '''does nothing'''\n        if self.some_value == 0:\n              self.some_value = int(response.xpath(somexpath).extract())\n        url =  response.xpath(somexpath).extract()[0]\n        self.data.append(str(url))\n        print 'parse'", "issue_status": "Closed", "issue_reporting_time": "2016-04-13T21:03:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "963": {"issue_url": "https://github.com/scrapy/scrapy/issues/1930", "issue_id": "#1930", "issue_summary": "Scrapy 1.1.0 RC3 - exception thrown with invalid ssl certificate", "issue_description": "natoinet commented on Apr 14, 2016\nHello,\nI am crawling sometimes websites with an invalid ssl certificate. For example, Scrapy 1.1.0 RC3 fails to open when I do:\nscrapy shell https://www.directoriosanitario.com/directorio\nor\nscrapy shell https://saobinv.5go.cc/top/\nand throws the following exception:\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure service_identity.exceptions.VerificationError: VerificationError(errors=[DNSMismatch(mismatched_id=DNS_ID(hostname=b'www.directoriosanitario.com'))])>]\nI tried it with Scrapy 1.0.5 on python 2.7 and the spider opens but warns with:\nAttributeError: 'NoneType' object has no attribute 'failVerification'\nIs there a way to force the spider to open with Scrapy 1.1.0 RC3?", "issue_status": "Closed", "issue_reporting_time": "2016-04-13T18:56:45Z", "fixed_by": "#1933", "pull_request_summary": "Ignore HTTPS certificate verification failures", "pull_request_description": "Contributor\nredapple commented on Apr 14, 2016\nFixes #1930\nEven with verify=False in CertificateOptions, Twisted's default ClientTLSOptions raises exception for invalid certificates and connections are closed, retried and eventually fail (see #1930)\nThis change is rather radical, ignoring all verification failure (that is, not doing any).\nWe could do with a warning about invalid certificates.\nThis change verifies hostname against certificate but does not fail, only showing a warning.", "pull_request_status": "Merged", "issue_fixed_time": "2016-04-20T12:57:33Z", "files_changed": [["11", "scrapy/core/downloader/contextfactory.py"], ["36", "scrapy/core/downloader/tls.py"], ["26", "tests/keys/example-com.cert.pem"], ["84", "tests/keys/example-com.conf"], ["24", "tests/keys/example-com.gen.README"], ["28", "tests/keys/example-com.key.pem"], ["6", "tests/mockserver.py"], ["19", "tests/test_downloader_handlers.py"]]}, "964": {"issue_url": "https://github.com/scrapy/scrapy/issues/1929", "issue_id": "#1929", "issue_summary": "Process died, reason None", "issue_description": "berkantaydin commented on Apr 13, 2016\nMy spider working 12 hours after stop - endless crawling - then scrapyd said\nProcess died: exitstatus=None project='default' ...\nHow can solve this ? Please, i need more verbose. Is this a memory issue ?", "issue_status": "Closed", "issue_reporting_time": "2016-04-13T10:14:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "965": {"issue_url": "https://github.com/scrapy/scrapy/issues/1928", "issue_id": "#1928", "issue_summary": "Allow to dump `response.body` automatically when callback failed", "issue_description": "Makman2 commented on Apr 13, 2016\nHello :)\nI've recently working with scrapy, and I got the idea to automatically dump the body of a gotten response to a file when an exception was raised inside a callback. A log would be printed saying where to find the file and the user can inspect what maybe has gone wrong :)\nAs I implemented an own little \"crash dump system\" (which works completely manually, meaning you need to catch and dump everything yourself), I thought of an integrated version.\nLet me know what you think about this idea :)\nThanks in advance \ud83d\udc4d", "issue_status": "Closed", "issue_reporting_time": "2016-04-13T00:34:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "966": {"issue_url": "https://github.com/scrapy/scrapy/issues/1927", "issue_id": "#1927", "issue_summary": "Update changelog with changes since 1.1.0RC3", "issue_description": "Contributor\nredapple commented on Apr 12, 2016 \u2022\nedited\n(major) Merged changes since v1.1.0rc3:\n#1875 -- FS cache storage misuses mtime for expiration (bugfix)\n#1822 -- Allow core Scheduler priority queue customization\n#1835 -- Add pps extension to IGNORED_EXTENSIONS (fixing #1834)\n#1857 -- response_status_message should not fail on non-standard HTTP codes (bugfix)\n#1851 -- Rename isbinarytext function to binary_is_text for clarity\n#1883 -- Make FilesPipeline work with S3FilesStore using botocore (bugfix)\n#1879 -- Improved Architecture overview (docs)\n#1847 -- Added FEED_TEMPDIR to settings (fixing #1779)\n#1902 -- Making it case-insensitive when extracting sitemap URLs from a robots.txt\n#1913 -- Fix link extractor tests for non-ASCII characters from latin1 document (tests)\n#1881 -- Remove duplicate code now handled by newer w3lib (refactoring)\n#1891 -- Change Files/ImagesPipelines class attributes to instance attributes (fixing #1850)\n#1938 -- Set SNI properly when using CONNECT (fixing #1936)\n#1912 -- Fix HTTP Pool key for HTTPS proxy tunneled connections (CONNECT method) (fixing #1807)\n#1933 -- Ignore HTTPS certificate verification failures (fixing #1930)\n#1923 -- Use newer w3lib.url.safe_url_string() and re-enable HTTP request tests\n#1947 -- Fix canonicalize_url() on Python 3 and re-enable tests", "issue_status": "Closed", "issue_reporting_time": "2016-04-12T14:33:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "967": {"issue_url": "https://github.com/scrapy/scrapy/issues/1925", "issue_id": "#1925", "issue_summary": "Can't access the web page need cookie.", "issue_description": "woshichuanqilz commented on Apr 12, 2016\nI write a code about the access web need cookie use the meta = {cookiejar}, at first I fill in the user form and get the right response. ( the url is github.com)\nAnd then I want to access the github.com/stars which need login. But the request to the github.com/stars was redirected to the github.com/login seems like that there is something wrong with the cookie?\nI paste the debug cookies info and my code bellow.\n2016-04-12 17:56:12 [scrapy] DEBUG: Received cookies from: <200 https://github.com/login>\n\nSet-Cookie: logged_in=no; domain=.github.com; path=/; expires=Sat, 12 Apr 2036 09:56:57 -0000; secure; HttpOnly\n\nSet-Cookie: _gh_sess=eyJzZXNzaW9uX2lkIjoiNzZmZGQzYzUwMDEzZWI2YmQ0ZTM0YTg2MTU3ODQxNTMiLCJfY3NyZl90b2tlbiI6ImU3M0VrSDdZSjdNelFWU3hmc3B5Z2xGL2t6TE14VEVJMHpjcDczeVUrUjg9In0%3D--c5afe25f0ad610b934025b078f5a1b59f80d1bdf; path=/; secure; HttpOnly\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport scrapy\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request, FormRequest\n\n\nclass GitHubLogin(scrapy.Spider) :\n    name = \"GH\"\n    allowed_domains = [\"github.com\"]\n    start_urls = [\n        \"http://github.com\"\n    ]\n    def start_requests(self):\n        return [Request(\"https://github.com/login\", meta = {'cookiejar' : 1}, callback = self.post_login)]\n\n    def post_login(self, response):\n        print 'Preparing login'\n        authenticity_token = Selector(response).xpath(\"/html/body/div[4]/div[1]/div/form/div[1]/input[2]/@value\").extract()[0]\n\n        formdata_utf = u\"\\u2713\".encode('utf-8')\n        return [FormRequest.from_response(response,   \n                            meta = {'cookiejar' : response.meta['cookiejar']},\n                            formdata = {\n                            'authenticity_token': authenticity_token,\n                            'commit': 'Sign in',\n                            'login': 'username',\n                            'password': 'password',\n                            'utf': formdata_utf\n                            },\n                            callback = self.after_login,\n                            dont_filter = True\n                            )]\n\n    def after_login(self, response) :\n        return [Request(\"https://github.com/stars\", meta = {'cookiejar' : response.meta['cookiejar']}, callback = self.post_login)]\n\n    def post_login(self, response):\n        print \"the response url is \" + response.url", "issue_status": "Closed", "issue_reporting_time": "2016-04-12T10:18:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "968": {"issue_url": "https://github.com/scrapy/scrapy/issues/1922", "issue_id": "#1922", "issue_summary": "Failed installing scrapy", "issue_description": "AFAgarap commented on Apr 11, 2016\nsudo pip3 install scrapy\nWhen I'm installing scrapy in my computer, this is the error I'm getting:\nCommand \"/usr/bin/python3 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-xfj07fck/cffi/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-h3d3uz53-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-xfj07fck/cffi/\nScreenshots\n\nMy computer runs on Deepin Linux OS (15.1).\nI already tried upgrading setuptools: sudo pip3 install --upgrade setuptools. Still getting the same error. Thank you in advance for helping!", "issue_status": "Closed", "issue_reporting_time": "2016-04-11T15:54:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "969": {"issue_url": "https://github.com/scrapy/scrapy/issues/1917", "issue_id": "#1917", "issue_summary": "Scrapy==0.24.1 not compatible with w3lib==1.14.1", "issue_description": "tom-aa commented on Apr 11, 2016\nWhen I install Scrapy 0.24.1, it will automatically install w3lib 1.14.1, and this will made scrapy failed:\n(w3lib 1.13 works fine)\nerror log:\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/http/request/init.py\", line 26, in init\nself._set_url(url)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/http/request/init.py\", line 52, in _set_url\nself._url = escape_ajax(safe_url_string(url))\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/url.py\", line 102, in escape_ajax\ndefrag, frag = urlparse.urldefrag(url)\nAttributeError: 'function' object has no attribute 'urldefrag'", "issue_status": "Closed", "issue_reporting_time": "2016-04-11T12:42:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "970": {"issue_url": "https://github.com/scrapy/scrapy/issues/1915", "issue_id": "#1915", "issue_summary": "Allow multiple items through pipelines?", "issue_description": "dxue2012 commented on Apr 10, 2016\nThe documentation for pipeline specifies that process_item must either return a dict with data, Item object or raise a DropItem exception. Is there a reason why we aren't allowed to return an iterable of dictionaries with data (or Item objects)? It seems impossible to write a pipeline that modifies the input item and returns multiple items under the current framework.\nThank you!\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2016-04-10T06:52:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "971": {"issue_url": "https://github.com/scrapy/scrapy/issues/1914", "issue_id": "#1914", "issue_summary": "LinkExtractor: can't extract link with more than one '&' charactor", "issue_description": "liuyunclouder commented on Apr 10, 2016\nI wanted to extract links like '/questions/tagged/postgresql?page=3&sort=frequent&pagesize=50' from this page:http://stackoverflow.com/questions/tagged/postgresql?sort=frequent&pageSize=50\nso I wrote the following LinkExtractor:\nrules = [\nRule(LinkExtractor(allow=r'questions/tagged/postgresql?page=[0-9]&sort=frequent&pagesize=50'),\ncallback='parse_item', follow=True)\n]\nit didn't work.\nafter I changed it to:\nrules = [\nRule(LinkExtractor(allow=r'questions/tagged/postgresql?page=[0-9]&sort=frequent.*?'),\ncallback='parse_item', follow=True)\n]\nmiracle happened! it worked\nso I thought there might be a bug in the LinkExtractor", "issue_status": "Closed", "issue_reporting_time": "2016-04-10T03:36:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "972": {"issue_url": "https://github.com/scrapy/scrapy/issues/1911", "issue_id": "#1911", "issue_summary": "[boto] ERROR: Unable to read instance data, giving up, and add the `DOWNLOAD_HANDLERS = {'S3': None,}` doesn't work'", "issue_description": "woshichuanqilz commented on Apr 8, 2016\nupdate\nI find another link,\nI get a optional_features.remove('boto') KeyError: 'boto' when I add the two line in the code :\nfrom scrapy import optional_features\noptional_features.remove('boto')\nseparate line\nI try to Using FormRequest.from_response() to simulate a user login and get the example code like\nthis :\nimport scrapy\n\nclass LoginSpider(scrapy.Spider):\n    name = 'example.com'\n    start_urls = ['http://www.example.com/users/login.php']\n    print 'Hello World 3'\n\n    def parse(self, response):\n        print 'Hello World 2' # this print I haven't gotten.\n        return scrapy.FormRequest.from_response(\n            response,\n            formdata={'username': 'john', 'password': 'secret'},\n            callback=self.after_login\n        )\n\n    def after_login(self, response):\n        # check login succeed before going on\n        print 'hello world' + response.url \n        if \"authentication failed\" in response.body:\n            self.logger.error(\"Login failed\")\n            return\n\n        # continue scraping with authenticated session...\nI make a print in the parse function and not get it.\nThen I check the debuginfo and get this :\nI find the following link:\nanswer link\nand add the following statement , but it doesn't work. How do I solve this question ?\nDOWNLOAD_HANDLERS = {'S3': None,}\nthis is the whole debug info:\n(virtualenvscrapy) h:\\GitCode\\Scrapy\\lzscrapy\n$ scrapy crawl loginspider\nHello World 3\n2016-04-08 10:23:08 [scrapy] INFO: Scrapy 1.0.5 started (bot: lzscrapy)\n2016-04-08 10:23:08 [scrapy] INFO: Optional features available: ssl, http11, boto\n2016-04-08 10:23:08 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'lzscrapy.spiders', 'SPIDER_MODULES': ['lzscrapy.spiders'], 'BOT_NAME': 'lzscrapy'}\n2016-04-08 10:23:10 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n2016-04-08 10:23:10 [boto] DEBUG: Retrieving credentials from metadata server.\n2016-04-08 10:23:10 [boto] ERROR: Caught exception reading instance data\nTraceback (most recent call last):\nFile \"c:\\anaconda2\\lib\\site-packages\\boto\\utils.py\", line 210, in retry_url\nr = opener.open(req, timeout=timeout)\nFile \"c:\\anaconda2\\lib\\urllib2.py\", line 431, in open\nresponse = self._open(req, data)\nFile \"c:\\anaconda2\\lib\\urllib2.py\", line 449, in _open\n'_open', req)\nFile \"c:\\anaconda2\\lib\\urllib2.py\", line 409, in _call_chain\nresult = func(*args)\nFile \"c:\\anaconda2\\lib\\urllib2.py\", line 1227, in http_open\nreturn self.do_open(httplib.HTTPConnection, req)\nFile \"c:\\anaconda2\\lib\\urllib2.py\", line 1197, in do_open\nraise URLError(err)\nURLError: <urlopen error [Errno 10051] >\n2016-04-08 10:23:10 [boto] ERROR: Unable to read instance data, giving up\n2016-04-08 10:23:10 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2016-04-08 10:23:10 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2016-04-08 10:23:10 [scrapy] INFO: Enabled item pipelines:\n2016-04-08 10:23:10 [scrapy] INFO: Spider opened\n2016-04-08 10:23:10 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-04-08 10:23:10 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025\n2016-04-08 10:23:11 [scrapy] DEBUG: Crawled (404) <GET http://www.example.com/users/login.php> (referer: None)\n2016-04-08 10:23:11 [scrapy] DEBUG: Ignoring response <404 http://www.example.com/users/login.php>: HTTP status code is not handled or not allowed\n2016-04-08 10:23:11 [scrapy] INFO: Closing spider (finished)\n2016-04-08 10:23:11 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 228,\n'downloader/request_count': 1,\n'downloader/request_method_count/GET': 1,\n'downloader/response_bytes': 968,\n'downloader/response_count': 1,\n'downloader/response_status_count/404': 1,\n'finish_reason': 'finished',\n'finish_time': datetime.datetime(2016, 4, 8, 2, 23, 11, 219000),\n'log_count/DEBUG': 4,\n'log_count/ERROR': 2,\n'log_count/INFO': 7,\n'response_received_count': 1,\n'scheduler/dequeued': 1,\n'scheduler/dequeued/memory': 1,\n'scheduler/enqueued': 1,\n'scheduler/enqueued/memory': 1,\n'start_time': datetime.datetime(2016, 4, 8, 2, 23, 10, 487000)}\n2016-04-08 10:23:11 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-04-08T02:34:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "973": {"issue_url": "https://github.com/scrapy/scrapy/issues/1910", "issue_id": "#1910", "issue_summary": "why not support python3.x ?", "issue_description": "biij5698 commented on Apr 7, 2016\nwe love python3.x,but scrapy not support python3.x,it's very pity", "issue_status": "Closed", "issue_reporting_time": "2016-04-07T13:39:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "974": {"issue_url": "https://github.com/scrapy/scrapy/issues/1908", "issue_id": "#1908", "issue_summary": "How to work with a very large \u201callowed_domains\u201d attribute in scrapy?", "issue_description": "15310944349 commented on Apr 7, 2016\nBecause the allowed_domains is very big, it throws this exception:\nregex = r'^(.*.)?(%s)$' % '|'.join(re.escape(d) for d in allowed_domains if d is not None)\nHow do I solve this problem?", "issue_status": "Closed", "issue_reporting_time": "2016-04-07T10:32:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "975": {"issue_url": "https://github.com/scrapy/scrapy/issues/1907", "issue_id": "#1907", "issue_summary": "'Selector' object has no attribute 're_first'", "issue_description": "VinGarcia commented on Apr 7, 2016\nI was trying to use the re_first() described on the Selector documentation page:\nhttp://doc.scrapy.org/en/latest/topics/selectors.html\nI am testing it on scrapy shell, and I got the error:\nfrom scrapy.selector import Selector\n>>> body = '<html><body><span>good</span></body></html>'\n>>> Selector(text=body).re_first\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\nAttributeError: 'Selector' object has no attribute 're_first'\nWhat am I missing?", "issue_status": "Closed", "issue_reporting_time": "2016-04-07T05:52:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "976": {"issue_url": "https://github.com/scrapy/scrapy/issues/1904", "issue_id": "#1904", "issue_summary": "CrawlerProcess doesn't load Item Pipeline component", "issue_description": "zouge commented on Apr 4, 2016\nIf I using scrapy crawl spider_name , everything is fun. BUT When I using CrawlerProcess to wrote my spider, I found CrawlerProcess doesn't load Item Pipeline component !", "issue_status": "Closed", "issue_reporting_time": "2016-04-04T12:50:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "977": {"issue_url": "https://github.com/scrapy/scrapy/issues/1901", "issue_id": "#1901", "issue_summary": "URLError: <urlopen error [Errno 24] Too many open files>", "issue_description": "ricoxor commented on Apr 1, 2016 \u2022\nedited by redapple\nI'm using scrapyd on a dedicated server and I have this error lot of times :\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 588, in _runCallbacks\n  File \"dirbot/spiders/expired.py\", line 54, in errback_httpbin\n  File \"dirbot/spiders/expired.py\", line 65, in checkDomain\n  File \"/usr/lib/python2.7/urllib2.py\", line 154, in urlopen\n  File \"/usr/lib/python2.7/urllib2.py\", line 431, in open\n  File \"/usr/lib/python2.7/urllib2.py\", line 449, in _open\n  File \"/usr/lib/python2.7/urllib2.py\", line 409, in _call_chain\n  File \"/usr/lib/python2.7/urllib2.py\", line 1227, in http_open\n  File \"/usr/lib/python2.7/urllib2.py\", line 1197, in do_open\nURLError: <urlopen error [Errno 24] Too many open files>\nI already add this configuration but I still have this error.\nat /etc/sysctl.conf\nadd:\nnet.core.somaxconn=131072\nfs.file-max=131072\nand then:\nsudo sysctl -p\nat /usr/include/linux/limits.h\nchange:\nNR_OPEN = 65536\nat /etc/security/limits.conf\nadd:\n*                soft    nofile          65535\n*                hard    nofile          65535\nCan someone help me to fix that ?\nUpdate,\nMy configuration Scrapy :\ncustom_settings = {\n    'RETRY_ENABLED': False,\n    'DEPTH_LIMIT' : 0,\n    'DEPTH_PRIORITY' : 1,\n    'LOG_ENABLED' : False,\n    'CONCURRENT_REQUESTS_PER_DOMAIN' : 64,\n    'CONCURRENT_REQUESTS' : 128,\n    'REACTOR_THREADPOOL_MAXSIZE' : 30,\n    'COOKIES_ENABLED' : False,\n    'DOWNLOAD_TIMEOUT' : 10,\n    'DOWNLOAD_WARNSIZE' : 66554432\n}", "issue_status": "Closed", "issue_reporting_time": "2016-04-01T13:16:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "978": {"issue_url": "https://github.com/scrapy/scrapy/issues/1899", "issue_id": "#1899", "issue_summary": "HttpCompressionMiddleware tries to decode HEAD responses", "issue_description": "Contributor\nArturGaspar commented on Mar 31, 2016\nThe middleware tries to decode the body of HEAD responses, which are not expected to have a message body.", "issue_status": "Closed", "issue_reporting_time": "2016-03-31T15:06:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "979": {"issue_url": "https://github.com/scrapy/scrapy/issues/1894", "issue_id": "#1894", "issue_summary": "failed with error code 1 in /tmp/pip-build-j3yFWp/cffi", "issue_description": "donhuvy commented on Mar 30, 2016 \u2022\nedited by redapple\n    3 root      20   0       0      0      0 S  0.0  0.0   0:00.63 ksoftirqd/0                                                                                                                 \n    5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H                                                                                                                \n    7 root      rt   0       0      0      0 S  0.0  0.0   0:00.00 migration/0                                                                                                                 \n    8 root      20   0       0      0      0 S  0.0  0.0   0:00.00 rcu_bh                                                                                                                      \n    9 root      20   0       0      0      0 S  0.0  0.0   0:00.00 rcuob/0                                                                                                                     \n   10 root      20   0       0      0      0 S  0.0  0.0   1:22.54 rcu_sched                                                                                                                   \n   11 root      20   0       0      0      0 R  0.0  0.0   1:44.46 rcuos/0                                                                                                                     \n   12 root      rt   0       0      0      0 S  0.0  0.0   0:34.28 watchdog/0                                                                                                                  \n   13 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 khelper                                                                                                                     \n   14 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kdevtmpfs                                                                                                                   \n   15 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 netns                                                                                                                       \n   16 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 perf                                                                                                                        \n   17 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 writeback                                                                                                                   \n   18 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kintegrityd                                                                                                                 \n   19 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 bioset                                                                                                                      \n   20 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kblockd                                                                                                                     \n   21 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 md                                                                                                                          \n   26 root      20   0       0      0      0 S  0.0  0.0   0:06.50 khungtaskd                                                                                                                  \n   27 root      20   0       0      0      0 S  0.0  0.0   0:11.23 kswapd0                                                                                                                     \n   28 root      25   5       0      0      0 S  0.0  0.0   0:00.00 ksmd                                                                                                                        \n   29 root      39  19       0      0      0 S  0.0  0.0   0:30.80 khugepaged                                                                                                                  \n   30 root      20   0       0      0      0 S  0.0  0.0   0:00.00 fsnotify_mark                                                                                                               \n   31 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 crypto                                                                                                                      \n   39 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kthrotld                                                                                                                    \n   41 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kmpath_rdacd                                                                                                                \n   42 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kpsmoused                                                                                                                   \n   43 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 ipv6_addrconf                                                                                                               \n   63 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 deferwq                                                                                                                     \n   93 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kauditd                                                                                                                     \n[root@vultr ~]# \n[root@vultr ~]# clear\n[root@vultr ~]# uptime\n 15:20:36 up 61 days,  6:03,  1 user,  load average: 0.00, 0.01, 0.05\n[root@vultr ~]# python --version\nPython 2.7.5\n[root@vultr ~]# \n\n\n\n\n\n\n\n\n\n\n[root@vultr ~]# yum install python-pip -y\nLoaded plugins: fastestmirror\nRepodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast\nbase                                                                      | 3.6 kB  00:00:00     \nepel/x86_64/metalink                                                      |  26 kB  00:00:00     \nepel                                                                      | 4.3 kB  00:00:00     \nextras                                                                    | 3.4 kB  00:00:00     \nupdates                                                                   | 3.4 kB  00:00:00     \n(1/4): extras/7/x86_64/primary_db                                         | 101 kB  00:00:01     \n(2/4): epel/x86_64/updateinfo                                             | 523 kB  00:00:02     \n(3/4): updates/7/x86_64/primary_db                                        | 3.2 MB  00:00:02     \n(4/4): epel/x86_64/primary_db                                             | 3.9 MB  00:00:02     \nDetermining fastest mirrors\n * base: mirrors.noction.com\n * epel: anorien.csc.warwick.ac.uk\n * extras: mirror.denit.net\n * updates: centos.ams.host-engine.com\nResolving Dependencies\n--> Running transaction check\n---> Package python-pip.noarch 0:7.1.0-1.el7 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n=================================================================================================\n Package                 Arch                Version                     Repository         Size\n=================================================================================================\nInstalling:\n python-pip              noarch              7.1.0-1.el7                 epel              1.5 M\n\nTransaction Summary\n=================================================================================================\nInstall  1 Package\n\nTotal download size: 1.5 M\nInstalled size: 6.6 M\nDownloading packages:\npython-pip-7.1.0-1.el7.noarch.rpm                                         | 1.5 MB  00:00:02     \nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Installing : python-pip-7.1.0-1.el7.noarch                                                 1/1 \n  Verifying  : python-pip-7.1.0-1.el7.noarch                                                 1/1 \n\nInstalled:\n  python-pip.noarch 0:7.1.0-1.el7                                                                \n\nComplete!\n[root@vultr ~]# yum install python-devel -y\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.noction.com\n * epel: anorien.csc.warwick.ac.uk\n * extras: mirror.denit.net\n * updates: centos.ams.host-engine.com\nResolving Dependencies\n--> Running transaction check\n---> Package python-devel.x86_64 0:2.7.5-34.el7 will be installed\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n=================================================================================================\n Package                   Arch                Version                   Repository         Size\n=================================================================================================\nInstalling:\n python-devel              x86_64              2.7.5-34.el7              base              391 k\n\nTransaction Summary\n=================================================================================================\nInstall  1 Package\n\nTotal download size: 391 k\nInstalled size: 1.0 M\nDownloading packages:\npython-devel-2.7.5-34.el7.x86_64.rpm                                      | 391 kB  00:00:01     \nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Installing : python-devel-2.7.5-34.el7.x86_64                                              1/1 \n  Verifying  : python-devel-2.7.5-34.el7.x86_64                                              1/1 \n\nInstalled:\n  python-devel.x86_64 0:2.7.5-34.el7                                                             \n\nComplete!\n[root@vultr ~]# yum install gcc gcc-devel -y\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.noction.com\n * epel: anorien.csc.warwick.ac.uk\n * extras: mirror.denit.net\n * updates: centos.ams.host-engine.com\nPackage gcc-4.8.5-4.el7.x86_64 already installed and latest version\nNo package gcc-devel available.\nNothing to do\n[root@vultr ~]# yum install libxm12 libxml2-devel -y\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.noction.com\n * epel: anorien.csc.warwick.ac.uk\n * extras: mirror.denit.net\n * updates: centos.ams.host-engine.com\nNo package libxm12 available.\nPackage libxml2-devel-2.9.1-6.el7_2.2.x86_64 already installed and latest version\nNothing to do\n[root@vultr ~]# yum install libxslt libxslt-devel -y\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.noction.com\n * epel: anorien.csc.warwick.ac.uk\n * extras: mirror.denit.net\n * updates: centos.ams.host-engine.com\nPackage libxslt-1.1.28-5.el7.x86_64 already installed and latest version\nPackage libxslt-devel-1.1.28-5.el7.x86_64 already installed and latest version\nNothing to do\n[root@vultr ~]# yum install openssl openssl-devel -y\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirrors.noction.com\n * epel: anorien.csc.warwick.ac.uk\n * extras: mirror.denit.net\n * updates: centos.ams.host-engine.com\nResolving Dependencies\n--> Running transaction check\n---> Package openssl.x86_64 1:1.0.1e-51.el7_2.2 will be updated\n---> Package openssl.x86_64 1:1.0.1e-51.el7_2.4 will be an update\n--> Processing Dependency: openssl-libs(x86-64) = 1:1.0.1e-51.el7_2.4 for package: 1:openssl-1.0.1e-51.el7_2.4.x86_64\n---> Package openssl-devel.x86_64 1:1.0.1e-51.el7_2.2 will be updated\n---> Package openssl-devel.x86_64 1:1.0.1e-51.el7_2.4 will be an update\n--> Running transaction check\n---> Package openssl-libs.x86_64 1:1.0.1e-51.el7_2.2 will be updated\n---> Package openssl-libs.x86_64 1:1.0.1e-51.el7_2.4 will be an update\n--> Finished Dependency Resolution\n\nDependencies Resolved\n\n=================================================================================================\n Package                 Arch             Version                        Repository         Size\n=================================================================================================\nUpdating:\n openssl                 x86_64           1:1.0.1e-51.el7_2.4            updates           711 k\n openssl-devel           x86_64           1:1.0.1e-51.el7_2.4            updates           1.2 M\nUpdating for dependencies:\n openssl-libs            x86_64           1:1.0.1e-51.el7_2.4            updates           951 k\n\nTransaction Summary\n=================================================================================================\nUpgrade  2 Packages (+1 Dependent package)\n\nTotal download size: 2.8 M\nDownloading packages:\nupdates/7/x86_64/prestodelta                                              | 290 kB  00:00:01     \n(1/3): openssl-1.0.1e-51.el7_2.4.x86_64.rpm                               | 711 kB  00:00:02     \n(2/3): openssl-devel-1.0.1e-51.el7_2.4.x86_64.rpm                         | 1.2 MB  00:00:02     \n(3/3): openssl-libs-1.0.1e-51.el7_2.4.x86_64.rpm                          | 951 kB  00:00:02     \n-------------------------------------------------------------------------------------------------\nTotal                                                            1.1 MB/s | 2.8 MB  00:00:02     \nRunning transaction check\nRunning transaction test\nTransaction test succeeded\nRunning transaction\n  Updating   : 1:openssl-libs-1.0.1e-51.el7_2.4.x86_64                                       1/6 \n  Updating   : 1:openssl-devel-1.0.1e-51.el7_2.4.x86_64                                      2/6 \n    100% |\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2| 3.7MB 98kB/s \nInstalling collected packages: lxml\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n  Running setup.py install for lxml\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n    Complete output from command /usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip-build-HoJPgz/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-TVEeh0-record/install-record.txt --single-version-externally-managed --compile:\n    Building lxml version 3.6.0.\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n    Building without Cython.\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n    Using build configuration of libxslt 1.1.28\n    Building against libxml2/libxslt in the following directory: /usr/lib64\n    running install\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n    running build\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n    running build_py\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n    creating build\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n    creating build/lib.linux-x86_64-2.7\n    creating build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/pyclasslookup.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/__init__.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/cssselect.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/doctestcompare.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/ElementInclude.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/builder.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/_elementpath.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/sax.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml\n    creating build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/__init__.py -> build/lib.linux-x86_64-2.7/lxml/includes\n    creating build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/_html5builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/__init__.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/clean.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/_diffcommand.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/html5parser.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/diff.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/defs.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/ElementSoup.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/soupparser.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/formfill.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/_setmixin.py -> build/lib.linux-x86_64-2.7/lxml/html\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron\n    copying src/lxml/isoschematron/__init__.py -> build/lib.linux-x86_64-2.7/lxml/isoschematron\n    copying src/lxml/lxml.etree.h -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/lxml.etree_api.h -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/includes/c14n.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/etreepublic.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/htmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/relaxng.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/dtdvalid.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xinclude.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/uri.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/config.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xpath.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xmlschema.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/schematron.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xmlerror.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/tree.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xslt.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/lxml-version.h -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/etree_defs.h -> build/lib.linux-x86_64-2.7/lxml/includes\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n    copying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n    copying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n    copying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    running build_ext\n    building 'lxml.etree' extension\n    creating build/temp.linux-x86_64-2.7\n    creating build/temp.linux-x86_64-2.7/src\n    creating build/temp.linux-x86_64-2.7/src/lxml\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -O0 -fPIC -I/usr/include/libxml2 -Isrc/lxml/includes -I/usr/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.linux-x86_64-2.7/src/lxml/lxml.etree.o -w\n    {standard input}: Assembler messages:\n    {standard input}: Error: open CFI at the end of file; missing .cfi_endproc directive\n    gcc: internal compiler error: Killed (program cc1)\n    Please submit a full bug report,\n    with preprocessed source if appropriate.\n    See <http://bugzilla.redhat.com/bugzilla> for instructions.\n    Compile failed: command 'gcc' failed with exit status 4\n    creating tmp\n    cc -I/usr/include/libxml2 -I/usr/include/libxml2 -c /tmp/xmlXPathInitnhByXS.c -o tmp/xmlXPathInitnhByXS.o\n    cc tmp/xmlXPathInitnhByXS.o -L/usr/lib64 -lxml2 -o a.out\n    error: command 'gcc' failed with exit status 4\n\n    ----------------------------------------\nCommand \"/usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip-build-HoJPgz/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-TVEeh0-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-HoJPgz/lxml\n[root@vultr ~]# CFLAGS=\"-O0\" pip install lxml\nYou are using pip version 7.1.0, however version 8.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nCollecting lxml\n  Using cached lxml-3.6.0.tar.gz\nInstalling collected packages: lxml\n  Running setup.py install for lxml\n    Complete output from command /usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip-build-XnOG9k/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-_As3rP-record/install-record.txt --single-version-externally-managed --compile:\n    Building lxml version 3.6.0.\n    Building without Cython.\n    Using build configuration of libxslt 1.1.28\n    Building against libxml2/libxslt in the following directory: /usr/lib64\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-x86_64-2.7\n    creating build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/pyclasslookup.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/__init__.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/cssselect.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/doctestcompare.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/ElementInclude.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/builder.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/_elementpath.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/sax.py -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml\n    creating build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/__init__.py -> build/lib.linux-x86_64-2.7/lxml/includes\n    creating build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/_html5builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/__init__.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/clean.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/_diffcommand.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/html5parser.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/diff.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/defs.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/builder.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/ElementSoup.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/soupparser.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/formfill.py -> build/lib.linux-x86_64-2.7/lxml/html\n    copying src/lxml/html/_setmixin.py -> build/lib.linux-x86_64-2.7/lxml/html\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron\n    copying src/lxml/isoschematron/__init__.py -> build/lib.linux-x86_64-2.7/lxml/isoschematron\n    copying src/lxml/lxml.etree.h -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/lxml.etree_api.h -> build/lib.linux-x86_64-2.7/lxml\n    copying src/lxml/includes/c14n.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/etreepublic.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/htmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/relaxng.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/dtdvalid.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xinclude.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/uri.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/config.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xpath.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xmlschema.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/schematron.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xmlerror.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/tree.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/xslt.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/lxml-version.h -> build/lib.linux-x86_64-2.7/lxml/includes\n    copying src/lxml/includes/etree_defs.h -> build/lib.linux-x86_64-2.7/lxml/includes\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n    copying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n    copying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n    copying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\n    creating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n    running build_ext\n    building 'lxml.etree' extension\n    creating build/temp.linux-x86_64-2.7\n    creating build/temp.linux-x86_64-2.7/src\n    creating build/temp.linux-x86_64-2.7/src/lxml\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -O0 -fPIC -I/usr/include/libxml2 -Isrc/lxml/includes -I/usr/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.linux-x86_64-2.7/src/lxml/lxml.etree.o -w\n    100% |\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2| 225kB 89kB/s \nInstalling collected packages: cssselect, queuelib, idna, setuptools, enum34, ipaddress, pycparser, cffi, cryptography, pyOpenSSL, w3lib, lxml, zope.interface, Twisted, pyasn1-modules, attrs, service-identity, scrapy\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\u00e2\n  Running setup.py install for cssselect\n  Found existing installation: setuptools 0.9.8\n    Uninstalling setuptools-0.9.8:\n      Successfully uninstalled setuptools-0.9.8\n  Running setup.py install for enum34\n  Running setup.py install for pycparser\n  Running setup.py install for cffi\n    Complete output from command /usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip-build-j3yFWp/cffi/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-JweBuG-record/install-record.txt --single-version-externally-managed --compile:\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    Package libffi was not found in the pkg-config search path.\n    Perhaps you should add the directory containing `libffi.pc'\n    to the PKG_CONFIG_PATH environment variable\n    No package 'libffi' found\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-x86_64-2.7\n    creating build/lib.linux-x86_64-2.7/cffi\n    copying cffi/lock.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/__init__.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/model.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/ffiplatform.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/commontypes.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/cffi_opcode.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/backend_ctypes.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/verifier.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/setuptools_ext.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/api.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/cparser.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/vengine_cpy.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/vengine_gen.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/recompiler.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/gc_weakref.py -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/_cffi_include.h -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/parse_c_type.h -> build/lib.linux-x86_64-2.7/cffi\n    copying cffi/_embedding.h -> build/lib.linux-x86_64-2.7/cffi\n    running build_ext\n    building '_cffi_backend' extension\n    creating build/temp.linux-x86_64-2.7\n    creating build/temp.linux-x86_64-2.7/c\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -DUSE__THREAD -I/usr/include/ffi -I/usr/include/libffi -I/usr/include/python2.7 -c c/_cffi_backend.c -o build/temp.linux-x86_64-2.7/c/_cffi_backend.o\n    c/_cffi_backend.c:15:17: fatal error: ffi.h: No such file or directory\n     #include <ffi.h>\n                     ^\n    compilation terminated.\n    error: command 'gcc' failed with exit status 1\n\n    ----------------------------------------\nCommand \"/usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip-build-j3yFWp/cffi/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-JweBuG-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-j3yFWp/cffi\n[root@vultr ~]# ", "issue_status": "Closed", "issue_reporting_time": "2016-03-30T15:27:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "980": {"issue_url": "https://github.com/scrapy/scrapy/issues/1890", "issue_id": "#1890", "issue_summary": "Disk queues don't preserve Request class", "issue_description": "Member\nkmike commented on Mar 30, 2016\nWhen a Request subclass (e.g. FormRequest) is sent to a disk queue a bare Request is what you get back.\nThis is inconvenient for scrapy-splash: Splash requests all have Splash URL as request.url, but for logging it is nice to display the requested URL, not only Splash URL. In scrapy-splash this is implemented by changing __repr__ in a Request subclass, but it works only when request is kept in memory.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2016-03-29T19:32:20Z", "fixed_by": "#2510", "pull_request_summary": "[MRG+1] Preserve request class when converting to/from dicts (utils.reqser)", "pull_request_description": "Member\nelacuesta commented on Jan 24, 2017 \u2022\nedited\nAttempting to fix #1890", "pull_request_status": "Merged", "issue_fixed_time": "2017-02-08T17:30:08Z", "files_changed": [["6", "scrapy/utils/reqser.py"], ["13", "tests/test_utils_reqser.py"]]}, "981": {"issue_url": "https://github.com/scrapy/scrapy/issues/1888", "issue_id": "#1888", "issue_summary": "Why engine fetch requests from scheduler first other than the start_urls generated ones?", "issue_description": "cockcrow commented on Mar 29, 2016\nFrom HERE I found that Scrapy engine fetch requests from scheduler before the start_urls generated ones.\nIn my usage, I enqueued thousands of start urls (which from various domains) to the queue and the crawling goes not so fast (maybe networking issues). The problems comes up with me was that the spider itself extracts links and follows them, then Scrapy will fetch the requests from scheduler. It makes the concurrency lower.\nI would like to learn about the design purpose of this mechanism.\nBRs.", "issue_status": "Closed", "issue_reporting_time": "2016-03-29T08:37:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "982": {"issue_url": "https://github.com/scrapy/scrapy/issues/1886", "issue_id": "#1886", "issue_summary": "Installation on shared hosting, is it possible", "issue_description": "Spereicle commented on Mar 29, 2016\nRegrets - Novice to python.\nIs it possible to install Scrapy in a shared hosting environment with shell access.\n'pip install scrapy' does run -- and results in various warnings and errors, including:\nerror: could not create '/usr/local/lib/python2.7/dist-packages/scrapy': Permission denied\nCan the package be placed in my user account area?\nThe complete log is attached.\nThanks, and sorry for the rather basic questions.\npip_scrapy.txt", "issue_status": "Closed", "issue_reporting_time": "2016-03-28T21:27:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "983": {"issue_url": "https://github.com/scrapy/scrapy/issues/1885", "issue_id": "#1885", "issue_summary": "Scrapy installation in Ubuntu: pkg_resources.DistributionNotFound: attrs", "issue_description": "nikhilgeo commented on Mar 28, 2016\nI installed scrapy by following the tutorial here, the installation was success but once I try to setup a project by it shows\nnikhil@nikhil:~$ scrapy startproject tutorial\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 5, in <module>\n    from pkg_resources import load_entry_point\n  File \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 2749, in <module>\n    working_set = WorkingSet._build_master()\n  File \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 444, in _build_master\n    ws.require(__requires__)\n  File \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 725, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 628, in resolve\n    raise DistributionNotFound(req)\npkg_resources.DistributionNotFound: attrs\nnikhil@nikhil:~$ pkg_resources.DistributionNotFound: attrs\npkg_resources.DistributionNotFound:: command not found\nAny idea about how to solve this error ..? I couldn't find any useful pointers via Google.", "issue_status": "Closed", "issue_reporting_time": "2016-03-28T12:24:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "984": {"issue_url": "https://github.com/scrapy/scrapy/issues/1884", "issue_id": "#1884", "issue_summary": "ImageException \"uncaught\"", "issue_description": "Contributor\ndjunzu commented on Mar 28, 2016\nMy log have some exceptions like this one:\n2016-03-25 20:33:35 [XYZ_SpiderName] WARNING: File (error): Error processing file from <GET http://url_path/xyz_img.jpg> referred in <None>: Image too small (279x560 < 300x300)\nTraceback (most recent call last):\n  File \"/home/me/.local/lib64/python2.7/site-packages/Scrapy-1.0.5-py2.7.egg/scrapy/pipelines/files.py\", line 268, in media_downloaded\n    checksum = self.file_downloaded(response, request, info)\n  File \"/home/me/.local/lib64/python2.7/site-packages/Scrapy-1.0.5-py2.7.egg/scrapy/pipelines/images.py\", line 60, in file_downloaded\n    return self.image_downloaded(response, request, info)\n  File \"/home/me/.local/lib64/python2.7/site-packages/Scrapy-1.0.5-py2.7.egg/scrapy/pipelines/images.py\", line 64, in image_downloaded\n    for path, image, buf in self.get_images(response, request, info):\n  File \"/home/me/.local/lib64/python2.7/site-packages/Scrapy-1.0.5-py2.7.egg/scrapy/pipelines/images.py\", line 82, in get_images\n    (width, height, self.MIN_WIDTH, self.MIN_HEIGHT))\nImageException: Image too small (279x560 < 300x300)\nThe exception is raised because the downloaded image is too small. OK!\nThe exception is caught and the event is logged. OK!\nThe exception is then reraised. -> Is this right?\nI do not expect to see this kind of exception when running a spider. I think the expected behavior is to have the event logged, but not propagated as a uncaught exception.", "issue_status": "Closed", "issue_reporting_time": "2016-03-28T02:34:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "985": {"issue_url": "https://github.com/scrapy/scrapy/issues/1865", "issue_id": "#1865", "issue_summary": "Error on scrapy startproject tutorial", "issue_description": "maelfosso commented on Mar 15, 2016\nHi\nI'm on Ubuntu 14.04 and it's my first time to use Scrapy.\nI have this error when I try to create a Scrapy project\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 5, in <module>\n    from pkg_resources import load_entry_point\n  File \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 2749, in <module>\n    working_set = WorkingSet._build_master()\n  File \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 444, in _build_master\n    ws.require(__requires__)\n  File \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 725, in require\n    needed = self.resolve(parse_requirements(requirements))\n  File \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 628, in resolve\n    raise DistributionNotFound(req)\npkg_resources.DistributionNotFound: service-identity\nHow to solve it please", "issue_status": "Closed", "issue_reporting_time": "2016-03-15T18:21:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "986": {"issue_url": "https://github.com/scrapy/scrapy/issues/1864", "issue_id": "#1864", "issue_summary": "can't export to json file when use JsonWriterPipeline", "issue_description": "danbao commented on Mar 15, 2016\nScrapy 1.0.5\nPython 2.7.11\npipelines.py\n# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n\nimport codecs\nimport json\n\n\nclass JsonWriterPipeline(object):\n    def __init__(self):\n        self.file = codecs.open('items.json', mode = 'wb', encoding = 'utf-8')\n\n    def process_item(self, item, spider):\n        line = json.dumps(dict(item)) + \"\\n\"\n        self.file.write(line.decode('unicode_escape'))\n        return item\nError information\n2016-03-16 00:03:39 [scrapy] ERROR: Error processing {'records': [{'en_kicker': u'Sinosphere'}]}\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 588, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/Users/linzhen/OneDrive/Source/newsCrawler/newsCrawler/pipelines.py\", line 17, in process_item\n    line = json.dumps(dict(item)) + \"\\n\"\n  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py\", line 244, in dumps\n    return _default_encoder.encode(obj)\n  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 207, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File \"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 270, in iterencode\n    return _iterencode(o, 0)\n  File \n![qq20160316-0 2x](https://cloud.githubusercontent.com/assets/4090783/13784812/a6394e30-eb0b-11e5-9753-4fb1bfd3530b.png)\n\"/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 184, in default\n    raise TypeError(repr(o) + \" is not JSON serializable\")\nTypeError: {'en_kicker': u'Sinosphere'} is not JSON serializable", "issue_status": "Closed", "issue_reporting_time": "2016-03-15T16:15:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "987": {"issue_url": "https://github.com/scrapy/scrapy/issues/1863", "issue_id": "#1863", "issue_summary": "ImportError: No module named '${project_name}'", "issue_description": "djkuddel commented on Mar 15, 2016\nHello,\nI try the tutorial and I got the following error. Can anyone help?\nBest Regards,\nAndy\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 9, in\nload_entry_point('Scrapy==1.0.5', 'console_scripts', 'scrapy')()\nFile \"/usr/local/lib/python3.4/dist-packages/scrapy/cmdline.py\", line 109, in execute\nsettings = get_project_settings()\nFile \"/usr/local/lib/python3.4/dist-packages/scrapy/utils/project.py\", line 60, in get_project_settings\nsettings.setmodule(settings_module_path, priority='project')\nFile \"/usr/local/lib/python3.4/dist-packages/scrapy/settings/init.py\", line 108, in setmodule\nmodule = import_module(module)\nFile \"/usr/lib/python3.4/importlib/init.py\", line 109, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 2254, in _gcd_import\nFile \"\", line 2237, in _find_and_load\nFile \"\", line 2212, in _find_and_load_unlocked\nFile \"\", line 321, in _call_with_frames_removed\nFile \"\", line 2254, in _gcd_import\nFile \"\", line 2237, in _find_and_load\nFile \"\", line 2224, in _find_and_load_unlocked\nImportError: No module named '${project_name}'", "issue_status": "Closed", "issue_reporting_time": "2016-03-15T11:25:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "988": {"issue_url": "https://github.com/scrapy/scrapy/issues/1862", "issue_id": "#1862", "issue_summary": "Trouble executing code on spider close", "issue_description": "johtso commented on Mar 15, 2016\nI have a CrawlSpider that needs to batch up it's items and send them off in groups once the crawl has completely finished.\nI initially saw the closed method that can be defined on a scraper which is supposed to be called with the reason. I found that this didn't work. The base scraper close is called, but when that calls my class's closed method it goes into some Twisted deferred code and never seems to be executed.\nI tried the hacky solution of just overriding close instead, but yielding Items from there doesn't seem to work.\nI then tried solving the problem at the pipeline level.\nI added this to my pipeline, but again, the method doesn't seem to be called:\n    @classmethod\n    def from_crawler(cls, crawler):\n        pipeline = cls()\n        crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)\n\n    def spider_closed(self, *args, **kwargs):\n        print args, kwargs\n        1/0\nAm I misunderstanding something / doing something wrong?", "issue_status": "Closed", "issue_reporting_time": "2016-03-14T18:44:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "989": {"issue_url": "https://github.com/scrapy/scrapy/issues/1859", "issue_id": "#1859", "issue_summary": "ImportError: No module named spiders", "issue_description": "kallesoderlund commented on Mar 14, 2016\nI've been working on a scrapy project, and although I'm fairly new to scrapy I've been able to get it to run properly. However, after I tried to get Scrapyd to work, I can't get Scrapy to work either.\nI get the ImportError, and can't figure out what to do.\nI did a pip uninstall scrapy and reinstalled it, but still the same problem. I can't even start a new scrapy project, I get the same ImportError message. I run Scrapy 1.04. Please help", "issue_status": "Closed", "issue_reporting_time": "2016-03-14T08:49:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "990": {"issue_url": "https://github.com/scrapy/scrapy/issues/1858", "issue_id": "#1858", "issue_summary": "Can't get attribute value which attribute name start with #?", "issue_description": "cage1618 commented on Mar 13, 2016\nfor example, the page source is bellow:\n<img src=\"http://www1.pcbaby.com.cn/images/blank.gif\" #src=\"http://img0.pcbaby.com.cn/pcbaby/1603/10/2799244_yunzaoqi0310-11.jpg\" />\nBut I get next in scrapy:\n<img src=\"http://www1.pcbaby.com.cn/images/blank.gif\" />\nI can't get the attribute \"#src\" in scrapy.\nHow can I get that? please help me! Hope your reply! thanks!", "issue_status": "Closed", "issue_reporting_time": "2016-03-13T07:14:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "991": {"issue_url": "https://github.com/scrapy/scrapy/issues/1856", "issue_id": "#1856", "issue_summary": "different machine deploy spider in scrapyd shows \"socket.error: [Errno 104] Connection reset by peer\"", "issue_description": "xyniceboy commented on Mar 10, 2016\noperation\nimport commands\ncommands.getstatusoutput('scrapyd-deploy crawlers -p crawlers_hoscs')\nresult\nDeploying to project \"crawlers_hoscs\" in http://192.162.25.1:6800/addversion.json\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapyd-deploy\", line 273, in\nmain()\nFile \"/usr/local/bin/scrapyd-deploy\", line 96, in main\nif not _upload_egg(target, egg, project, version):\nFile \"/usr/local/bin/scrapyd-deploy\", line 195, in _upload_egg\nreturn _http_post(req)\nFile \"/usr/local/bin/scrapyd-deploy\", line 211, in _http_post\nf = urllib2.urlopen(request)\nFile \"/usr/local/lib/python2.7/urllib2.py\", line 154, in urlopen\nreturn opener.open(url, data, timeout)\nFile \"/usr/local/lib/python2.7/urllib2.py\", line 431, in open\nresponse = self._open(req, data)\nFile \"/usr/local/lib/python2.7/urllib2.py\", line 449, in _open\n'_open', req)\nFile \"/usr/local/lib/python2.7/urllib2.py\", line 409, in _call_chain\nresult = func(*args)\nFile \"/usr/local/lib/python2.7/urllib2.py\", line 1227, in http_open\nreturn self.do_open(httplib.HTTPConnection, req)\nFile \"/usr/local/lib/python2.7/urllib2.py\", line 1200, in do_open\nr = h.getresponse(buffering=True)\nFile \"/usr/local/lib/python2.7/httplib.py\", line 1073, in getresponse\nresponse.begin()\nFile \"/usr/local/lib/python2.7/httplib.py\", line 415, in begin\nversion, status, reason = self._read_status()\nFile \"/usr/local/lib/python2.7/httplib.py\", line 371, in _read_status\nline = self.fp.readline(_MAXLINE + 1)\nFile \"/usr/local/lib/python2.7/socket.py\", line 476, in readline\ndata = self._sock.recv(self._rbufsize)\nsocket.error: [Errno 104] Connection reset by peer", "issue_status": "Closed", "issue_reporting_time": "2016-03-10T08:23:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "992": {"issue_url": "https://github.com/scrapy/scrapy/issues/1855", "issue_id": "#1855", "issue_summary": "SSL errors crawling https sites using proxies", "issue_description": "Cesped commented on Mar 9, 2016\nI'm unable to scrape https sites through https supported proxies. I've tried with proxymesh as well as other proxy services. I can scrape most of this sites without proxies or using Tor.\nCurl seems to work fine too:\ncurl -x https://xx.xx.xx.xx:xx --proxy-user user:pass -L https://www.base.net:443\nRetrieves the site's html.\nSetup:\nOS:\nOS X El Capitan v10.11.3\nScrapy:\nscrapy version -v\nScrapy    : 1.0.5\nlxml      : 3.5.0.0\nlibxml2   : 2.9.2\nTwisted   : 15.5.0\nPython    : 2.7.11 (default, Dec  7 2015, 23:36:10) - [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)]\npyOpenSSL : 0.15.1 (OpenSSL 1.0.2g  1 Mar 2016)\nPlatform  : Darwin-15.3.0-x86_64-i386-64bit\nSolutions tried:\n1 - Installing Scrapy-1.1.0rc3\n2016-03-09 12:44:59 [scrapy] ERROR: Error downloading <GET https://www.base.net/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'unknown protocol')]>]\nOther website:\n2016-03-09 12:56:45 [scrapy] DEBUG: Retrying <GET https://es.alojadogatopreto.com/es-es/> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]\n2 - #1764 (comment)\nUsing SSLv23_METHOD\n2016-03-09 12:22:40 [scrapy] ERROR: Error downloading <GET https://www.base.net/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'unknown protocol')]>]\nUsing other SSL methods\n2016-03-09 12:24:11 [scrapy] ERROR: Error downloading <GET https://www.base.net/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_GET_RECORD', 'wrong version number')]>]\n3 - #1227 (comment) | Get same errors as in 1 & 2.\n4 - #1429 (comment) | Get same errors as in 1 & 2.", "issue_status": "Closed", "issue_reporting_time": "2016-03-09T12:15:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "993": {"issue_url": "https://github.com/scrapy/scrapy/issues/1850", "issue_id": "#1850", "issue_summary": "EXPIRES in Files/ImagesPipeline should be a instance attribute and not a class attribute", "issue_description": "Contributor\ndjunzu commented on Mar 6, 2016\nCurrently EXPIRES in FilesPipeline and ImagesPipeline is defined as a class attribute. When running more than one spider with CrawlerProcess it is impossible to have different expires values for different spiders. Changing the EXPIRES attribute to an instance attribute solves the problem.\n(I would do a pull request, but I have no idea how to properly update the tests.)", "issue_status": "Closed", "issue_reporting_time": "2016-03-06T02:45:13Z", "fixed_by": "#1891", "pull_request_summary": "[MRG+1] Change Files/ImagesPipelines class attributes to instance attributes", "pull_request_description": "Contributor\ndjunzu commented on Mar 30, 2016\nfix #1850\nAddressed issues:\nMove default settings from FilesPipelines.py and ImagesPipelines.py to settings/default_settings.py\nChange class attributes from FilesPipelines.py and ImagesPipelines.py to instance attributes. It allows that distinct spiders running together have distinct custom settings for the pipelines.\nNote: I have no way to test AWS related stuff. So I haven't touched any code related to AWS.\n@kmike as you have looked at the issue before, I think you should review the PR.", "pull_request_status": "Merged", "issue_fixed_time": "2016-04-08T10:55:09Z", "files_changed": [["83", "docs/topics/media-pipeline.rst"], ["27", "scrapy/pipelines/files.py"], ["39", "scrapy/pipelines/images.py"], ["10", "scrapy/settings/default_settings.py"], ["31", "tests/test_pipeline_files.py"], ["48", "tests/test_pipeline_images.py"]]}, "994": {"issue_url": "https://github.com/scrapy/scrapy/issues/1849", "issue_id": "#1849", "issue_summary": "Spider has no attribute 'update_settings'", "issue_description": "DeckerCHAN commented on Mar 6, 2016\n\"D:\\Program Files\\Python\\Python35-32\\python.exe\" G:/Python/ParkingSearch/entry_point.py\n2016-03-06 12:43:40 [scrapy] INFO: Scrapy 1.0.5 started (bot: scrapybot)\n2016-03-06 12:43:40 [scrapy] INFO: Optional features available: http11, ssl\n2016-03-06 12:43:40 [scrapy] INFO: Overridden settings: {}\nTraceback (most recent call last):\nFile \"G:/Python/ParkingSearch/entry_point.py\", line 16, in\nmain()\nFile \"G:/Python/ParkingSearch/entry_point.py\", line 11, in main\nprocess.crawl(go_and_see_aus_spider)\nFile \"D:\\Program Files\\Python\\Python35-32\\lib\\site-packages\\scrapy\\crawler.py\", line 150, in crawl\ncrawler = self._create_crawler(crawler_or_spidercls)\nFile \"D:\\Program Files\\Python\\Python35-32\\lib\\site-packages\\scrapy\\crawler.py\", line 166, in _create_crawler\nreturn Crawler(spidercls, self.settings)\nFile \"D:\\Program Files\\Python\\Python35-32\\lib\\site-packages\\scrapy\\crawler.py\", line 32, in init\nself.spidercls.update_settings(self.settings)\nAttributeError: module 'crawler.spiders.test_spider' has no attribute 'update_settings'", "issue_status": "Closed", "issue_reporting_time": "2016-03-06T01:47:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "995": {"issue_url": "https://github.com/scrapy/scrapy/issues/1846", "issue_id": "#1846", "issue_summary": "jsonlines exporting without escaping", "issue_description": "szhem commented on Mar 5, 2016\nHi there,\nI'd like to export json items (I have not implemented any scrapy.Item because the response is in json itself) without any escaping with the following custom exporter\nclass UnicodeJsonLinesItemExporter(JsonLinesItemExporter):\n    def __init__(self, file, **kwargs):\n        super(UnicodeJsonLinesItemExporter, self).__init__(file, **kwargs)\n        self.file = file\n        self.encoder = ScrapyJSONEncoder(ensure_ascii=False, encoding=\"utf-8\")\nHowever not all of the types of items can be exported as expected, for example, the following snippets behaves in the scrapy shell differently (please notice there is umlaut within item - u'v\u00e4lue2')\nItem is exported as expected\n>>> exporter=UnicodeJsonLinesItemExporter(sys.stdout)\n>>> exporter.export_item({u'field1':u'value1',u'field2':{u'field22':u'v\u00e4lue2'}})\n{\"field2\": {\"field22\": \"v\u00e4lue2\"}, \"field1\": \"value1\"}\nItem cannot be exported\n>>> exporter=UnicodeJsonLinesItemExporter(sys.stdout)\n>>> exporter.export_item({u'field1':u'value1',u'field2':u'v\u00e4lue2'})\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"/temp/build_venv/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 89, in export_item\nself.file.write(self.encoder.encode(itemdict) + '\\n')\n  File \"/usr/lib/python2.7/json/encoder.py\", line 204, in encode\nreturn ''.join(chunks)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 2: ordinal not in range(128)\nAs I understand correctly the issue is within BaseItemExporter\nclass BaseItemExporter(object):\n    ...\n    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._to_str_if_unicode)\n        return serializer(value)\n    ...\n    def _to_str_if_unicode(self, value):\n        return value.encode(self.encoding) if isinstance(value, unicode) else value\n    ...\n    def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n    ...\n        for field_name in field_iter:\n            if field_name in item:\n                field = {} if isinstance(item, dict) else item.fields[field_name]\n                value = self.serialize_field(field, field_name, item[field_name])\n            else:\n                value = default_value\n\n            yield field_name, value\nIn the first case {u'field1':u'value1',u'field2':{u'field22':u'v\u00e4lue2'}} value of field2 isn't a json, and value is not encoded and exported as expected.\nIn the second case {u'field1':u'value1',u'field2':u'v\u00e4lue2'} value of field2 is unicode and it is encoded so that exporting fails.\nI can override serialize_field method like the following\nclass UnicodeJsonLinesItemExporter(JsonLinesItemExporter):\n    def __init__(self, file, **kwargs):\n        super(UnicodeJsonLinesItemExporter, self).__init__(file, **kwargs)\n        self.file = file\n        self.encoder = ScrapyJSONEncoder(ensure_ascii=False, encoding=\"utf-8\")\n\n    def _noop(self, value):\n        return value\n\n    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._noop)\n        return serializer(value)\nand then everything works as expected\n>>> exporter = UnicodeJsonLinesItemExporter(sys.stdout)\n>>> exporter.export_item({u'field1':u'value1',u'field2':u'v\u00e4lue2'})\n{\"field2\": \"v\u00e4lue2\", \"field1\": \"value1\"}\n>>>\n>>> exporter.export_item({u'field1':u'value1',u'field2':{u'field22':u'v\u00e4lue2'}})\n{\"field2\": {\"field22\": \"v\u00e4lue2\"}, \"field1\": \"value1\"}\nbut I'm wondering whether it is an expected behavior of BaseItemExporter to use _to_str_if_unicode to reencode the item values or not?", "issue_status": "Closed", "issue_reporting_time": "2016-03-05T10:36:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "996": {"issue_url": "https://github.com/scrapy/scrapy/issues/1844", "issue_id": "#1844", "issue_summary": "How could I pass parameter to ImagesPipeline from my spider?", "issue_description": "seaguest commented on Mar 4, 2016\nHello,\nI want to download all images on one page and save them in some folder whose name depends on the original request URL.\nFor example, I want to scrape example.com/123.html\nI want to download all images on that page and put them under ./123 folder.\nIs it possible ?\nHow could I pass parameter to ImagesPipeline from my spider?", "issue_status": "Closed", "issue_reporting_time": "2016-03-04T12:09:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "997": {"issue_url": "https://github.com/scrapy/scrapy/issues/1843", "issue_id": "#1843", "issue_summary": "how to use headers in crawlspider?CRAWLERA use in crawlspider?", "issue_description": "aohan237 commented on Mar 4, 2016\ni buy your CRAWLERA PLANS,but the offcial doc said dont to override the parse function,but how to transfer the CRAWLERA headers to the crawlspider\nthanks a lot", "issue_status": "Closed", "issue_reporting_time": "2016-03-04T06:43:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "998": {"issue_url": "https://github.com/scrapy/scrapy/issues/1841", "issue_id": "#1841", "issue_summary": "Is it possible to modify the response content through Scrapy Selector?", "issue_description": "seaguest commented on Mar 3, 2016\nI am using Scrapy to deep copy some content on one page, to crawl the content and download the images in that content and update the image original value accordingly.\nFor example I have:\n<div class=\"A\">\n    <img original=\"example1.com/1/1.png\"></img>\n</div>\nI need to download the image and update the new image original value\uff08for example to mysite.com/1/1.png\uff09, then save the content.\nwhat I will have finally is:\n<div class=\"A\">\n    <img original=\"mysite.com/1/1.png\"></img>\n</div>\nand image on my disk.\nIs it possible to modify the value through Selector?\nOr must I download the image first and update the \"original\" value separately? any better solution?", "issue_status": "Closed", "issue_reporting_time": "2016-03-03T01:59:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "999": {"issue_url": "https://github.com/scrapy/scrapy/issues/1839", "issue_id": "#1839", "issue_summary": "mutiple spiders affect each other", "issue_description": "aohan237 commented on Mar 2, 2016\ni have two spiders in one project,and only run one spider at a time but the varis shows in another spider when one spider runs .\nIE.\nclass spider1(scrapy.spider):\nname='spiderone'\nprint 'hello'\ndef parse:\nclass spider2(scrapy.spider):\nname='spidertwo'\ndef parse:\nwhen i run spidertwo,it will print 'hello'. but why? I only run one spider at a time\nI'm really confused about it. how to deal with mutiple spiders ...the official dont show.\nthanks a lot!!! from a scrapy beginner", "issue_status": "Closed", "issue_reporting_time": "2016-03-02T13:19:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1000": {"issue_url": "https://github.com/scrapy/scrapy/issues/1837", "issue_id": "#1837", "issue_summary": "Add more extensions to IGNORED_EXTENSIONS", "issue_description": "Member\nkmike commented on Mar 2, 2016\nA follow-up to #1835. For the record, in my last project I had to also add the following extensions to ignored:\n{\n    '7z', '7zip', 'xz', 'gz', 'tar', 'bz2',   # archives\n    'cdr',  # Corel Draw files\n    'apk', # Android packages\n}\ncdr and apk look safe; I wonder if we can add .gz: Scrapy decompresses gz responses automatically, so if we add .gz to IGNORED_EXTENSIONS spider may stop following e.g. sitemap.xml.gz links.\nOn the other hand, for broad crawls most .gz files are archives, and we're filtering out .zip files already.", "issue_status": "Closed", "issue_reporting_time": "2016-03-02T11:23:04Z", "fixed_by": "#4066", "pull_request_summary": "Add .dmg, .iso & .apk to ignored other extensions", "pull_request_description": "Contributor\nakhterwahab commented on Oct 8, 2019 \u2022\nedited by Gallaecio\nI run into downloading large files while scraping\nFixes #1837, fixes #2067", "pull_request_status": "Merged", "issue_fixed_time": "2019-11-14T10:26:04Z", "files_changed": [["9", "scrapy/linkextractors/__init__.py"]]}, "1001": {"issue_url": "https://github.com/scrapy/scrapy/issues/1834", "issue_id": "#1834", "issue_summary": "IGNORED_EXTENSIONS does not include pps files", "issue_description": "Contributor\ndjunzu commented on Mar 2, 2016\nI just got the error AttributeError: 'Response' object has no attribute 'body_as_unicode' because of a pps file (Power Point).\nMay I suggest to extend IGNORED_EXTENSIONS in linkextractors with pps?", "issue_status": "Closed", "issue_reporting_time": "2016-03-01T23:52:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1002": {"issue_url": "https://github.com/scrapy/scrapy/issues/1833", "issue_id": "#1833", "issue_summary": "Scraper not registering last page", "issue_description": "eadebruijn commented on Mar 1, 2016\nSo my code (pasted) below almost does what I want. Instead, it covers 29/30 pages, and then leaves out the last. Furthermore, I would preferably have it go beyond, but the website has no button for it (the pages actually do work when you manually fill in page=31 in the link). When Depth_Limit is 29 it's all fine, but on 30 I get the following error in the command prompt:\nFile \"C:\\Users\\Ewald\\Scrapy\\OB\\OB\\spiders\\spider_OB.py\", line 23, in parse\nnext_link = 'https://zoek.officielebekendmakingen.nl/' + s.xpath('//a[@Class=\"volgende\"]/@href').extract()[0]\nIndexError: list index out of range\nI've tried variious approaches, but they all seem to fail me... Also, I can't for the life of me format this post properly :(.\nclass OB_Crawler(CrawlSpider):\nname = 'OB5'\nallowed_domains = [\"https://www.officielebekendmakingen.nl/\"]\nstart_urls = [\"https://zoek.officielebekendmakingen.nl/zoeken/resultaat/?zkt=Uitgebreid&pst=Tractatenblad|Staatsblad|Staatscourant|BladGemeenschappelijkeRegeling|ParlementaireDocumenten&vrt=Cybersecurity&zkd=InDeGeheleText&dpr=Alle&sdt=DatumPublicatie&ap=&pnr=18&rpp=10&_page=1&sorttype=1&sortorder=4\"]\ncustom_settings = {\n'BOT_NAME': 'OB-crawler',\n'DEPTH_LIMIT': 30,\n'DOWNLOAD_DELAY': 0.1\n}\ndef parse(self, response):\n    s = Selector(response)\n    next_link = 'https://zoek.officielebekendmakingen.nl/' + s.xpath('//a[@class=\"volgende\"]/@href').extract()[0]\n    if len(next_link):\n        yield self.make_requests_from_url(next_link)\n    posts = response.selector.xpath('//div[@class = \"lijst\"]/ul/li')\n    for post in posts:\n        i = TextPostItem()\n        i['title'] = ' '.join(post.xpath('a/@href').extract()).replace(';', '').replace('  ', '').replace('\\r\\n', '')\n        i['link'] = ' '.join(post.xpath('a/text()').extract()).replace(';', '').replace('  ', '').replace('\\r\\n', '')\n        i['info'] = ' '.join(post.xpath('a/em/text()').extract()).replace(';', '').replace('  ', '').replace('\\r\\n', '').replace(',', '-')\n        yield i       ", "issue_status": "Closed", "issue_reporting_time": "2016-03-01T14:58:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1003": {"issue_url": "https://github.com/scrapy/scrapy/issues/1830", "issue_id": "#1830", "issue_summary": "why RFPDupeFilter.request_seen gets called 3 times for one page crawling?", "issue_description": "seaguest commented on Mar 1, 2016\nHello,\nTo avoid crawling duplicated URLs, I store all crawled URLs in database, and then check if one URL requested is already in DB or not.\nHere is my filter class:\nclass CustomFilter(RFPDupeFilter):\n    \"\"\"A dupe filter that considers specific ids in the url\"\"\"\n    db = DBManager(settings[ 'MONGODB_VISITED_URLS' ])\n\n    def request_seen(self, request):\n        if self.db.exist(\"url\", request.url):\n            return True\n        else:\n            visitedUrl = VisitedURL()\n            visitedUrl['url'] = request.url\n            visitedUrl['orgDupURL'] = None\n            self.db.insert(visitedUrl)\n            return False\nHowever I found request_seen method gets called 3 times for one page crawling, here are stack trace for the 3 calls in order:\n------------------------------ 1\nrequest_seen [filter.py:18]\nenqueue_request [scheduler.py:51]\nschedule [engine.py:189]\ncrawl [engine.py:183]\n_process_spidermw_output [scraper.py:183]\n[defer.py:63]\n_oneWorkUnit [task.py:491]\n_tick [task.py:645]\nrunUntilCurrent [base.py:825]\nmainLoop [base.py:1203]\nrun [base.py:1194]\nstart [crawler.py:251]\n[start.py:15]\nrun [pydevd.py:931]\n[pydevd.py:1524]\n------------------------------ 2\nrequest_seen [filter.py:18]\nenqueue_request [scheduler.py:51]\nschedule [engine.py:189]\ncrawl [engine.py:183]\n_handle_downloader_output [engine.py:155]\n_runCallbacks [defer.py:588]\n_startRunCallbacks [defer.py:501]\ncallback [defer.py:393]\n_runCallbacks [defer.py:588]\n_startRunCallbacks [defer.py:501]\ncallback [defer.py:393]\nconnectionLost [http11.py:320]\n_bodyDataFinished_CONNECTED [_newclient.py:1161]\ndispatcher [_newclient.py:916]\nconnectionLost [_newclient.py:537]\n_disconnectParser [_newclient.py:1513]\n_finishResponse_WAITING [_newclient.py:1487]\ndispatcher [_newclient.py:916]\n_finished [_newclient.py:440]\ndataReceived [http.py:1478]\nrawDataReceived [_newclient.py:299]\ndataReceived [basic.py:578]\ndataReceived [_newclient.py:385]\ndataReceived [_newclient.py:1533]\ndataReceived [endpoints.py:102]\n_dataReceived [tcp.py:215]\ndoRead [tcp.py:209]\n_doReadOrWrite [posixbase.py:597]\ncallWithContext [context.py:81]\ncallWithContext [context.py:118]\ncallWithContext [log.py:84]\ncallWithLogger [log.py:101]\ndoPoll [epollreactor.py:396]\nmainLoop [base.py:1206]\nrun [base.py:1194]\nstart [crawler.py:251]\n[start.py:15]\nrun [pydevd.py:931]\n[pydevd.py:1524]\n------------------------------ 3\nrequest_seen [filter.py:18]\nenqueue_request [scheduler.py:51]\nschedule [engine.py:189]\ncrawl [engine.py:183]\n_handle_downloader_output [engine.py:155]\n_runCallbacks [defer.py:588]\n_startRunCallbacks [defer.py:501]\ncallback [defer.py:393]\n_runCallbacks [defer.py:588]\n_startRunCallbacks [defer.py:501]\ncallback [defer.py:393]\nconnectionLost [http11.py:320]\n_bodyDataFinished_CONNECTED [_newclient.py:1161]\ndispatcher [_newclient.py:916]\nconnectionLost [_newclient.py:537]\n_disconnectParser [_newclient.py:1513]\n_finishResponse_WAITING [_newclient.py:1487]\ndispatcher [_newclient.py:916]\n_finished [_newclient.py:440]\ndataReceived [http.py:1478]\nrawDataReceived [_newclient.py:299]\ndataReceived [basic.py:578]\ndataReceived [_newclient.py:385]\ndataReceived [_newclient.py:1533]\ndataReceived [endpoints.py:102]\ndataReceived [policies.py:120]\n_flushReceiveBIO [tls.py:392]\ndataReceived [tls.py:422]\n_dataReceived [tcp.py:215]\ndoRead [tcp.py:209]\n_doReadOrWrite [posixbase.py:597]\ncallWithContext [context.py:81]\ncallWithContext [context.py:118]\ncallWithContext [log.py:84]\ncallWithLogger [log.py:101]\ndoPoll [epollreactor.py:396]\nmainLoop [base.py:1206]\nrun [base.py:1194]\nstart [crawler.py:251]\n[start.py:15]\nrun [pydevd.py:931]\n[pydevd.py:1524]\nI thought that this method should be called only once for one page, why it gets called 3 times?\nthis brought me a problem, seems the DB access gets a pb, the result are not reflected, in the end I have 3 record in DB.\nI will look into the pb, but I would still need to understand why this method gets 3 calls, did I make something wrong?\nthanks and best regards", "issue_status": "Closed", "issue_reporting_time": "2016-03-01T13:45:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1004": {"issue_url": "https://github.com/scrapy/scrapy/issues/1824", "issue_id": "#1824", "issue_summary": "pip install scrapy error", "issue_description": "codelegant commented on Feb 28, 2016\nI use pip to install scrapy, then a error occured\nCommand \"\"c:\\program files\\python35\\python.exe\" -u -c \"import setuptools, tokeniz e;__file__='C:\\\\Users\\\\PROGRA~2\\\\AppData\\\\Local\\\\Temp\\\\pip-build-i9vpky2z\\\\lxml\\\\ setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace(' \\r\\n', '\\n'), __file__, 'exec'))\" install --record C:\\Users\\PROGRA~2\\AppData\\Loca l\\Temp\\pip-1l5qv44w-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\PROGRA~2\\AppData\\Local\\Temp\\pip- build-i9vpky2z\\lxml", "issue_status": "Closed", "issue_reporting_time": "2016-02-28T08:39:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1005": {"issue_url": "https://github.com/scrapy/scrapy/issues/1823", "issue_id": "#1823", "issue_summary": "Needed a possibility to pass start_urls parameter in constructor", "issue_description": "Felix-neko commented on Feb 27, 2016\nI have a Spider that should get its start_urls from an external source: file system, database, etc.\nIt will be extremely useful to pass it directly in constructor. Especially if I want to parse some similar sites on different URLs.", "issue_status": "Closed", "issue_reporting_time": "2016-02-27T15:45:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1006": {"issue_url": "https://github.com/scrapy/scrapy/issues/1819", "issue_id": "#1819", "issue_summary": "install lxml trouble", "issue_description": "aimer12 commented on Feb 26, 2016 \u2022\nedited by redapple\nthere are many troubles installing lxml on window, because of its needs of visual studio .why i should install a software to install a lib for python. maybe it's better to provise an alternitive for other parse enginee. maybe beautifulsoup.", "issue_status": "Closed", "issue_reporting_time": "2016-02-26T08:08:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1007": {"issue_url": "https://github.com/scrapy/scrapy/issues/1814", "issue_id": "#1814", "issue_summary": "Update 1.1 release notes about new TLS/SSL context factory", "issue_description": "Contributor\nredapple commented on Feb 25, 2016\n#1794 should make TLS/SSL connections work better in most cases, but there are 2 main points to be aware of:\ndefault method is now OpenSSL.SSL.SSLv23_METHOD instead of OpenSSL.SSL.TLSv1_METHOD: it allows protocol negotiation (from TLS 1.2 -- when supported by platform -- to SSLv3) ;\ncustom DOWNLOADER_CLIENTCONTEXTFACTORY setting classes should accept an OpenSSL.SSL method param at init ; HTTP11 downloader handler instantiates the factory in backward compatible way though", "issue_status": "Closed", "issue_reporting_time": "2016-02-24T21:38:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1008": {"issue_url": "https://github.com/scrapy/scrapy/issues/1807", "issue_id": "#1807", "issue_summary": "Scrapy keep wrong proxy setting", "issue_description": "DrJackilD commented on Feb 23, 2016\nHi! I have one issue, when trying to test proxies with Scrapy. I want to check proxies with httpbin.org, and make crawler:\nclass CheckerSpider(scrapy.Spider):\n    name = \"checker\"\n    start_urls = (\n        'https://www.httpbin.org/ip'\n    )\n    connection = get_connection()\n\n    def start_requests(self):\n\n        with self.connection.cursor() as cursor:\n            limit = int((datetime.now() - datetime(1970, 1, 1)).total_seconds()) - 3600\n            q = \"\"\" SELECT *\n                    FROM {}\n                    WHERE active = 1 AND last_checked <= {} OR last_checked IS NULL;\"\"\".format(DB_TABLE, limit)\n            cursor.execute(q)\n            proxy_list = cursor.fetchall()\n\n        for proxy in proxy_list[:15]:\n            word = get_random_word()\n            req = scrapy.Request(self.start_urls, self.check_proxy, dont_filter=True)\n            req.meta['proxy'] = 'https://{}:8080'.format(proxy['ip'])\n            req.meta['item'] = proxy\n            user_pass = base64.encodestring('{}:{}'.format(PROXY_USER, PROXY_PASSWORD))\n            req.headers['Proxy-Authorization'] = 'Basic {}'.format(user_pass)\n            req.headers['User-Agent'] = get_user_agent()\n            yield req\n\n    def check_proxy(self, response):\n        print response.request.meta['proxy']\n        print response.meta['item']['ip']\n        print response.body\nBut when I'm testing it, I've see that Scrapy connect to url only with 5 proxies and then didn't change it. Example output (just messed IP):\n2016-02-23 14:54:36 [scrapy] DEBUG: Crawled (200) <GET https://www.httpbin.org/ip> (referer: None)\nhttps://192.168.100.130:8080\n192.168.100.130\n{\n  \"origin\": \"192.168.100.130\"\n}\n\n2016-02-23 14:54:36 [scrapy] DEBUG: Crawled (200) <GET https://www.httpbin.org/ip> (referer: None)\nhttps://192.168.100.131:8080\n192.168.100.131\n{\n  \"origin\": \"192.168.100.131\"\n}\n2016-02-23 14:54:37 [scrapy] DEBUG: Crawled (200) <GET https://www.httpbin.org/ip> (referer: None)\nhttps://192.168.100.132:8080\n192.168.100.132\n{\n  \"origin\": \"192.168.100.132\"\n}\n\n# Here Scrapy used wrong proxy to connect to site.\n2016-02-23 14:54:37 [scrapy] DEBUG: Crawled (200) <GET https://www.httpbin.org/ip> (referer: None)\nhttps://192.168.100.134:8080\n192.168.100.134\n{\n  \"origin\": \"192.168.100.130\"\n}\nMay be I've make something wrong? Any idea? Thank you.\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2016-02-23T13:04:14Z", "fixed_by": "#1912", "pull_request_summary": "[MRG+1] Fix HTTP Pool key for HTTPS proxy tunneled connections (CONNECT method)", "pull_request_description": "Contributor\nredapple commented on Apr 9, 2016\nShould fix #1807\nIt adds the proxy host and port to the key used to cache HTTP connections, instead of just the remote host and port.\nI have no concrete proposal for unittesting this though.\nI only was able to debug-print HTTP pool keys and testing with https://httpbin.org/ip with a few open HTTPS proxies.\nWith current code, before this patch, connections do get mixed with current default Twisted Agent key = (parsedURI.scheme, parsedURI.host, parsedURI.port)", "pull_request_status": "Merged", "issue_fixed_time": "2016-04-20T11:32:04Z", "files_changed": [["11", "scrapy/core/downloader/handlers/http11.py"]]}, "1009": {"issue_url": "https://github.com/scrapy/scrapy/issues/1805", "issue_id": "#1805", "issue_summary": "Refactor SpiderLoader class for more flexible subclassing", "issue_description": "Contributor\nlagenar commented on Feb 22, 2016\nHi there,\nThis is not really an issue but just a suggestion to consider to make it easier to subclass the SpiderLoader class. I have a very big list of spiders and what I wanted to do is to optimize how spiders are searched on the file system by caching the location, and also because I want to avoid all spider modules to get imported.\nWhat I found on the init method of that class is:\n    def __init__(self, settings):\n            self.spider_modules = settings.getlist('SPIDER_MODULES')\n            self._spiders = {}\n            for name in self.spider_modules:\n                for module in walk_modules(name):\n                    self._load_spiders(module)`\nThe problem here is that the constructor is doing too much work and it's a bit difficult to override the default behaviour. Anyway, I think I will just implement my own class without sub classing but looks like this could be refactored a bit.", "issue_status": "Closed", "issue_reporting_time": "2016-02-22T18:27:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1010": {"issue_url": "https://github.com/scrapy/scrapy/issues/1802", "issue_id": "#1802", "issue_summary": "Modify scrapy scheduler to allow for using queue class other than PriorityQueue", "issue_description": "KrisB88 commented on Feb 22, 2016\nI'm currently working on a project for a client that is a rather broad crawl, something around 200-300 urls. I've been tweaking this project to maximize the scrapy performance because my item pipelines really don't add much overhead if any to the performance. However, I ran into a problem with scrapy's built-in queue strategy.\nThe problem has been that the url queue becomes loaded with really long sequences of urls from a single domain. Because I'm trying to maintain some kind of politeness to these sites with a reasonable download rate, the performance suffers greatly because every url has some kind of download delay which essentially stalls the scraper. To fix this I implemented a simple queue that uses a heap which holds a PriorityQueue from queuelib to further filter urls by their domain. Domains are managed separately in this class with heapq which allows for choosing the next url based on the domain which has been called least recently. By doing so, I was able to increase the crawler's performance by over 500 percent using the exact same settings and only running a single instance of the spider. This is just a first implementation that can be improved with some custom extensions, but as a first run has shown fantastic improvements in both download rates and cpu utilization.\nThere was one issue I ran into when making this change to scrapy which is that the scheduler initializes its memory queue to a PriorityQueue. Therefore, I had to create a custom scheduler which only deviated from the built-in scheduler in its from_crawler. init, and open methods. Seems like a decent candidate for changes to the scheduler class and something I would be happy to do. Anyways, just my thoughts and I'd love to hear what everyone else thinks. I can share my code as well for anyone that wants to see the queue I'm using for this broad crawl.", "issue_status": "Closed", "issue_reporting_time": "2016-02-21T21:15:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1011": {"issue_url": "https://github.com/scrapy/scrapy/issues/1789", "issue_id": "#1789", "issue_summary": "Document from_crawler for Scrapy middlewares", "issue_description": "Contributor\nredapple commented on Feb 18, 2016\nSee motivation #1788\nFrom @kmike\nis not clear that Scrapy middlewares have the same features as extensions. Accessing settings is a common use case, I think we should provide examples in middleware docs, even if they are redundant.", "issue_status": "Closed", "issue_reporting_time": "2016-02-18T10:39:06Z", "fixed_by": "#3626", "pull_request_summary": "[MRG+1] Document that the main entry point of downloader and spider middlewar\u2026", "pull_request_description": "Member\nGallaecio commented on Feb 13, 2019\n\u2026es is from_crawler()\nFixes #1789", "pull_request_status": "Merged", "issue_fixed_time": "2019-03-15T07:58:35Z", "files_changed": [["8", "docs/topics/downloader-middleware.rst"], ["8", "docs/topics/spider-middleware.rst"]]}, "1012": {"issue_url": "https://github.com/scrapy/scrapy/issues/1788", "issue_id": "#1788", "issue_summary": "Can't pass custom settings to middleware in scrapy `crawl`", "issue_description": "GilJ commented on Feb 18, 2016\nWe have written some custom downloader middleware to which we want to pass a custom setting; the layout is basically the following:\nfrom scrapy.utils.project import get_project_settings\n\nclass CustomMiddleware(object):\n    def __init__(self):\n        settings = get_project_settings()\n        custom_setting = settings.get('CUSTOM_SETTING')\n    [...]\nAnd we launch our crawler with\nscrapy crawl CrawlerName -s CUSTOM_SETTING=1\nHowever, the custom setting is not set in our downloader middleware. It is available in our item pipeline.\nAre we accessing the settings wrong, or should we pass another flag? I did not find anything in the documentation.", "issue_status": "Closed", "issue_reporting_time": "2016-02-18T10:27:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1013": {"issue_url": "https://github.com/scrapy/scrapy/issues/1785", "issue_id": "#1785", "issue_summary": "Running multiple spiders in the same process with 'CrawlerRunner'", "issue_description": "xyniceboy commented on Feb 17, 2016\nhow to execute \"Running multiple spiders in the same process with 'CrawlerRunner'\" linked by http://doc.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process. thanks.", "issue_status": "Closed", "issue_reporting_time": "2016-02-17T15:44:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1014": {"issue_url": "https://github.com/scrapy/scrapy/issues/1784", "issue_id": "#1784", "issue_summary": "Alternatives to Lxml as XML processing engine", "issue_description": "gerosalesc commented on Feb 17, 2016\nThis has been troubling me for some time now but I would like this project to support a more powerful XML/HTML processing engine as an alternative to Lxml. The only contender for lxml in Python: Zorba. But why?\nZorba supports XQuery technology as well as JSONiq.\nZorba has Python bindings. I know they are not precisely the best bindings ever but at least they exist.\nI think XPath 1.0 is very limited for more complex structures.\nLxml extensions are ok but not that much when compared to XQuery capabilities by default.\nZorba can be hosted as a service.", "issue_status": "Closed", "issue_reporting_time": "2016-02-16T21:49:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1015": {"issue_url": "https://github.com/scrapy/scrapy/issues/1783", "issue_id": "#1783", "issue_summary": "get_base_url fails for non-ascii URLs in Python 3", "issue_description": "Member\nkmike commented on Feb 17, 2016\nE.g. for this page\nhttp://naftir.com/index.php/%D9%85%D8%AE%D8%A7%D8%B2%D9%86-%D8%B0%D8%AE%DB%8C%D8%B1%D9%87\nscrapy.utils.response.get_base_url raises the following exception in Pyhton 3:\nFile \"/Users/kmike/svn/scrapy/scrapy/utils/response.py\", line 30, in get_base_url\n    response.encoding)\n  File \"/Users/kmike/envs/dl/lib/python3.5/site-packages/w3lib-1.13.0-py3.5.egg/w3lib/html.py\", line 287, in get_base_url\n    baseurl = moves.urllib.parse.urljoin(baseurl, m.group(1).encode(encoding))\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/parse.py\", line 415, in urljoin\n    base, url, _coerce_result = _coerce_args(base, url)\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/parse.py\", line 114, in _coerce_args\n    return _decode_args(args) + (_encode_result,)\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/parse.py\", line 98, in _decode_args\n    return tuple(x.decode(encoding, errors) if x else '' for x in args)\n  File \"/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/parse.py\", line 98, in <genexpr>\n    return tuple(x.decode(encoding, errors) if x else '' for x in args)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd9 in position 28: ordinal not in range(128)", "issue_status": "Closed", "issue_reporting_time": "2016-02-16T20:56:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1016": {"issue_url": "https://github.com/scrapy/scrapy/issues/1782", "issue_id": "#1782", "issue_summary": "PY3: error decoding Content-Disposition header", "issue_description": "Member\nkmike commented on Feb 17, 2016\nThis request\nscrapy shell 'http://npe.com.cn/plus/save_to_doc.php?id=1666'\nraises this error:\nTraceback (most recent call last):\n  File \"/Users/kmike/envs/dl/bin/scrapy\", line 9, in <module>\n    load_entry_point('Scrapy', 'console_scripts', 'scrapy')()\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 142, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 88, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/kmike/svn/scrapy/scrapy/cmdline.py\", line 149, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/kmike/svn/scrapy/scrapy/commands/shell.py\", line 71, in run\n    shell.start(url=url)\n  File \"/Users/kmike/svn/scrapy/scrapy/shell.py\", line 47, in start\n    self.fetch(url, spider)\n  File \"/Users/kmike/svn/scrapy/scrapy/shell.py\", line 112, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/Users/kmike/envs/dl/lib/python3.5/site-packages/Twisted-15.5.0-py3.5.egg/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/Users/kmike/envs/dl/lib/python3.5/site-packages/Twisted-15.5.0-py3.5.egg/twisted/python/failure.py\", line 368, in raiseException\n    raise self.value.with_traceback(self.tb)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xb8 in position 25: invalid start byte\nThe error points to a wrong location (similar to #1760); the real traceback is\nTraceback (most recent call last):\n  File \"/Users/kmike/envs/dl/lib/python3.5/site-packages/Twisted-15.5.0-py3.5.egg/twisted/internet/defer.py\", line 1126, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/Users/kmike/envs/dl/lib/python3.5/site-packages/Twisted-15.5.0-py3.5.egg/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/Users/kmike/svn/scrapy/scrapy/core/downloader/middleware.py\", line 43, in process_request\n    defer.returnValue((yield download_func(request=request,spider=spider)))\n  File \"/Users/kmike/envs/dl/lib/python3.5/site-packages/Twisted-15.5.0-py3.5.egg/twisted/internet/defer.py\", line 588, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/Users/kmike/svn/scrapy/scrapy/core/downloader/handlers/http11.py\", line 272, in _cb_bodydone\n    respcls = responsetypes.from_args(headers=headers, url=url)\n  File \"/Users/kmike/svn/scrapy/scrapy/responsetypes.py\", line 110, in from_args\n    cls = self.from_headers(headers)\n  File \"/Users/kmike/svn/scrapy/scrapy/responsetypes.py\", line 78, in from_headers\n    cls = self.from_content_disposition(headers[b'Content-Disposition'])\n  File \"/Users/kmike/svn/scrapy/scrapy/responsetypes.py\", line 62, in from_content_disposition\n    filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n  File \"/Users/kmike/svn/scrapy/scrapy/utils/python.py\", line 129, in to_native_str\n    return to_unicode(text, encoding, errors)\n  File \"/Users/kmike/svn/scrapy/scrapy/utils/python.py\", line 107, in to_unicode\n    return text.decode(encoding, errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xb8 in position 25: invalid start byte\nIt looks like Content-Disposition is decoded using utf-8, but the encoding was not UTF-8.", "issue_status": "Closed", "issue_reporting_time": "2016-02-16T19:09:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1017": {"issue_url": "https://github.com/scrapy/scrapy/issues/1781", "issue_id": "#1781", "issue_summary": "Encoding issues", "issue_description": "psychok7 commented on Feb 16, 2016\nI am fairly new to scrapy and i am trying to crawl a page but the final item is not showing correctly all characters.\nThe site header:\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\nI try:\nscrapy shell \"https://olx.pt/anuncio/t4-excelente-preo-centro-almada-IDyKPJd.html#c49d3d94cf;promoted\"\nMy response:\nresponse.encoding: 'utf-8'\nresponse._headers_encoding: 'utf-8'\nWhat am i doing wrong?", "issue_status": "Closed", "issue_reporting_time": "2016-02-16T18:13:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1018": {"issue_url": "https://github.com/scrapy/scrapy/issues/1780", "issue_id": "#1780", "issue_summary": "Scrapy/scrapy startproject failed", "issue_description": "vincent6987 commented on Feb 16, 2016\nfailed to startproject, here is the error info:\nFile \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/string.py\", line 125, in substitute\nreturn self.pattern.sub(convert, self.template)\nTypeError: cannot use a string pattern on a bytes-like object", "issue_status": "Closed", "issue_reporting_time": "2016-02-16T14:06:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1019": {"issue_url": "https://github.com/scrapy/scrapy/issues/1779", "issue_id": "#1779", "issue_summary": "Allow setting temp file location for S3 and FTP feed storage", "issue_description": "Contributor\nredapple commented on Feb 16, 2016\nTriggered by this question on StackOverflow\nCurrent BlockingFeedStorage uses TemporaryFile(prefix='feed-'). There could be a setting to set a different folder than the default one.", "issue_status": "Closed", "issue_reporting_time": "2016-02-16T12:34:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1020": {"issue_url": "https://github.com/scrapy/scrapy/issues/1778", "issue_id": "#1778", "issue_summary": "Public default ACL policy for storing on S3 with ImagesPipeline and FilesPipeline", "issue_description": "Member\nlopuhin commented on Feb 15, 2016\nThe policy here\nscrapy/scrapy/pipelines/files.py\nLine 83 in 4158839\n POLICY = 'public-read' \nis public-read - this means that even though the bucket is private, uploaded item will be public and accessible if you know the URL (at least that is what I get from direct tests, and I don't see where it could be overridden).\nProbably this should be changed to private, and noted as a backwards incompatible security fix.", "issue_status": "Closed", "issue_reporting_time": "2016-02-15T15:57:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1021": {"issue_url": "https://github.com/scrapy/scrapy/issues/1773", "issue_id": "#1773", "issue_summary": "Unable to run scrapy on Python3.5.1", "issue_description": "adeelehsan commented on Feb 9, 2016\nI have python3.5.1 on my system and i have also creates the virtual enviorment with python 3.5.1. I have installed scrapy 1.0.5 and when i tried to run the command \"Scrapy version\" or scrapy startproject , I am getting following error:\nTraceback (most recent call last):\nFile \"/home/adeel/Intermix_py3.5/venv/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/home/adeel/Intermix_py3.5/venv/lib/python3.5/site-packages/scrapy/cmdline.py\", line 122, in execute\ncmds = _get_commands_dict(settings, inproject)\nFile \"/home/adeel/Intermix_py3.5/venv/lib/python3.5/site-packages/scrapy/cmdline.py\", line 46, in _get_commands_dict\ncmds = _get_commands_from_module('scrapy.commands', inproject)\nFile \"/home/adeel/Intermix_py3.5/venv/lib/python3.5/site-packages/scrapy/cmdline.py\", line 29, in _get_commands_from_module\nfor cmd in _iter_command_classes(module):\nFile \"/home/adeel/Intermix_py3.5/venv/lib/python3.5/site-packages/scrapy/cmdline.py\", line 21, in _iter_command_classes\nfor obj in vars(module).itervalues():\nAttributeError: 'dict' object has no attribute 'itervalues'", "issue_status": "Closed", "issue_reporting_time": "2016-02-09T14:47:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1022": {"issue_url": "https://github.com/scrapy/scrapy/issues/1770", "issue_id": "#1770", "issue_summary": "PY3: Fail to download the second or later requests to hosts using secure cookies", "issue_description": "Contributor\norangain commented on Feb 7, 2016\nEnvironment\nMac OS X 10.10.5\nPython 3.4.2\nScrapy 1.1.0rc1\nTwisted 15.5.0\nSteps to Reproduce\nSave the following spider as secure_cookie_spider.py.\nimport scrapy\n\n\nclass SecureCookieSpider(scrapy.Spider):\n   name = 'secure_cookie_spider'\n   start_urls = [\n       'https://github.com/',\n   ]\n\n   def parse(self, response):\n       # Request the same url again\n       yield scrapy.Request(url=response.url, callback=self.parse_second_request)\n\n   def parse_second_request(self, response):\n       pass\nRun the following command.\n$ scrapy runspider secure_cookie_spider.py\nExpected Results\nNo error is reported.\nActual Results\nFail to download the second request with AttributeError: 'WrappedRequest' object has no attribute 'type'.\n$ scrapy runspider secure_cookie_spider.py\n2016-02-07 11:57:11 [scrapy] INFO: Scrapy 1.1.0rc1 started (bot: scrapybot)\n2016-02-07 11:57:11 [scrapy] INFO: Overridden settings: {}\n2016-02-07 11:57:11 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.logstats.LogStats']\n2016-02-07 11:57:11 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-02-07 11:57:11 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-02-07 11:57:11 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-02-07 11:57:11 [scrapy] INFO: Spider opened\n2016-02-07 11:57:11 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-02-07 11:57:12 [scrapy] DEBUG: Crawled (200) <GET https://github.com/> (referer: None)\n2016-02-07 11:57:12 [scrapy] ERROR: Error downloading <GET https://github.com/>\nTraceback (most recent call last):\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/core/downloader/middleware.py\", line 37, in process_request\n    response = yield method(request=request, spider=spider)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/downloadermiddlewares/cookies.py\", line 39, in process_request\n    jar.add_cookie_header(request)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/http/cookies.py\", line 42, in add_cookie_header\n    cookies += self.jar._cookies_for_domain(host, wreq)\n  File \"/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/cookiejar.py\", line 1242, in _cookies_for_domain\n    if not self._policy.return_ok(cookie, request):\n  File \"/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/cookiejar.py\", line 1077, in return_ok\n    if not fn(cookie, request):\n  File \"/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/http/cookiejar.py\", line 1103, in return_ok_secure\n    if cookie.secure and request.type != \"https\":\nAttributeError: 'WrappedRequest' object has no attribute 'type'\n2016-02-07 11:57:12 [scrapy] INFO: Closing spider (finished)\n2016-02-07 11:57:12 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 1,\n 'downloader/exception_type_count/builtins.AttributeError': 1,\n 'downloader/request_bytes': 211,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 9735,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 2, 7, 2, 57, 12, 757829),\n 'log_count/DEBUG': 1,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'request_depth_max': 1,\n 'response_received_count': 1,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'start_time': datetime.datetime(2016, 2, 7, 2, 57, 11, 384330)}\n2016-02-07 11:57:12 [scrapy] INFO: Spider closed (finished)\nNote that no error is reported in Python 2.", "issue_status": "Closed", "issue_reporting_time": "2016-02-07T03:18:58Z", "fixed_by": "#1771", "pull_request_summary": "[MRG+1] PY3: Implement some attributes of WrappedRequest required in Python 3", "pull_request_description": "Contributor\norangain commented on Feb 7, 2016\nPurpose\nFix #1770, the failure to download the second or later requests to hosts using secure cookies. The problem is caused by missing type attribute of scrapy.http.WrappedRequest.\nThough the official document seems to be outdated, request argument of CookieJar.add_cookie_header() requires some attributes instead of methods in Python 3. This is because urllib.request.Request's methods get_hosts(), get_type(), unverifiable() and get_origin_req_host() were deprecated in Python 3.3 and removed in Python 3.4.\nSee also:\nIssue 15409: Deprecation Warning fix on cookiejar module - Python tracker\nhttp://bugs.python.org/issue15409\ncpython: ea8078365d3b\nhttps://hg.python.org/cpython/rev/ea8078365d3b\nChanges\nImplement some attributes of scrapy.http.cookies.WrappedRequest required in Python 3.\nAdd unit tests.", "pull_request_status": "Merged", "issue_fixed_time": "2016-02-08T10:52:00Z", "files_changed": [["22", "scrapy/http/cookies.py"], ["4", "tests/test_http_cookies.py"]]}, "1023": {"issue_url": "https://github.com/scrapy/scrapy/issues/1768", "issue_id": "#1768", "issue_summary": "PY3: Output to stdout cause TypeError in Scrapy 1.1.0rc1", "issue_description": "Contributor\norangain commented on Feb 7, 2016\nEnvironment\nMac OS X 10.10.5\nPython 3.4.2\nScrapy 1.1.0rc1\nSteps to Reproduce\nSave the following spider as myspider.py.\nimport scrapy\n\n\nclass BlogSpider(scrapy.Spider):\n    name = 'blogspider'\n    start_urls = ['https://blog.scrapinghub.com']\n\n    def parse(self, response):\n        yield {'title': response.css('title::text').extract_first()}\nRun the following command.\n$ scrapy runspider myspider.py -o - -t jl\nExpected Results\nExtracted item is written to the stdout without error.\n$ scrapy runspider myspider.py -o - -t jl 2> /dev/null\n{\"title\": \"The Scrapinghub Blog\"}\nActual Results\nTypeError is raised and no item is written to the stdout.\n$ scrapy runspider myspider.py -o - -t jl 2> /dev/null\n$ scrapy runspider myspider.py -o - -t jl\n2016-02-07 07:48:09 [scrapy] INFO: Scrapy 1.1.0rc1 started (bot: scrapybot)\n2016-02-07 07:48:09 [scrapy] INFO: Overridden settings: {'FEED_FORMAT': 'jl', 'FEED_URI': 'stdout:'}\n2016-02-07 07:48:09 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.feedexport.FeedExporter']\n2016-02-07 07:48:09 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-02-07 07:48:09 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-02-07 07:48:09 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-02-07 07:48:09 [scrapy] INFO: Spider opened\n2016-02-07 07:48:09 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-02-07 07:48:09 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com> (referer: None)\n2016-02-07 07:48:09 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': 'The Scrapinghub Blog'}\n2016-02-07 07:48:09 [scrapy] ERROR: Error caught on signal handler: <bound method FeedExporter.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x1108c05c0>>\nTraceback (most recent call last):\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\n    return receiver(*arguments, **named)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/extensions/feedexport.py\", line 194, in item_scraped\n    slot.exporter.export_item(item)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/exporters.py\", line 91, in export_item\n    self.file.write(to_bytes(self.encoder.encode(itemdict) + '\\n'))\nTypeError: must be str, not bytes\n2016-02-07 07:48:09 [scrapy] INFO: Closing spider (finished)\n2016-02-07 07:48:09 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 221,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 37941,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 2, 6, 22, 48, 9, 972031),\n 'item_scraped_count': 1,\n 'log_count/DEBUG': 2,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 2, 6, 22, 48, 9, 425909)}\n2016-02-07 07:48:09 [scrapy] INFO: Spider closed (finished)\nNote:\nUsing the other output types, jsonlines, json, csv, pickle and marshal also cause the same error, but output type xml works fine.\nDumping to a file, i.e. -o somefile, does not cause the error.\nUsing Python 2.7 does not cause the error.", "issue_status": "Closed", "issue_reporting_time": "2016-02-06T23:15:43Z", "fixed_by": "#1769", "pull_request_summary": "[MRG+1] PY3: Fix TypeError when outputting to stdout", "pull_request_description": "Contributor\norangain commented on Feb 7, 2016\nPurpose\nFix #1768, the failure to output to stdout in Python 3.\nChanges\nUse sys.stdout.buffer instead of sys.stdout in Python 3.\nNote\nBuffering behavior seems to be different between Python 2 and 3, though I don't think it's a matter.\nIn Python 2\nThe exporeted item is shown at the line immediately after the scraped log, i.e. it seems to be unbuffered.\n$ scrapy runspider myspider.py -o - -t jl\n2016-02-07 08:06:55 [scrapy] INFO: Scrapy 1.1.0rc1 started (bot: scrapybot)\n2016-02-07 08:06:55 [scrapy] INFO: Overridden settings: {'FEED_FORMAT': 'jl', 'FEED_URI': 'stdout:'}\n2016-02-07 08:06:55 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-02-07 08:06:55 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-02-07 08:06:55 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-02-07 08:06:55 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-02-07 08:06:55 [scrapy] INFO: Spider opened\n2016-02-07 08:06:55 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-02-07 08:06:55 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-02-07 08:06:56 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com> (referer: None)\n2016-02-07 08:06:56 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': u'The Scrapinghub Blog'}\n{\"title\": \"The Scrapinghub Blog\"}\n2016-02-07 08:06:56 [scrapy] INFO: Closing spider (finished)\n2016-02-07 08:06:56 [scrapy] INFO: Stored jl feed (1 items) in: stdout:\n2016-02-07 08:06:56 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 221,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 37889,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 2, 6, 23, 6, 56, 340778),\n 'item_scraped_count': 1,\n 'log_count/DEBUG': 3,\n 'log_count/INFO': 8,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 2, 6, 23, 6, 55, 491594)}\n2016-02-07 08:06:56 [scrapy] INFO: Spider closed (finished)\nIn Python 3\nThe exporeted item is shown at the last, i.e. it seems to be buffered.\n$ scrapy runspider myspider.py -o - -t jl\n2016-02-07 10:05:52 [scrapy] INFO: Scrapy 1.2.0dev2 started (bot: scrapybot)\n2016-02-07 10:05:52 [scrapy] INFO: Overridden settings: {'FEED_FORMAT': 'jl', 'FEED_URI': 'stdout:'}\n2016-02-07 10:05:52 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats']\n2016-02-07 10:05:53 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-02-07 10:05:53 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-02-07 10:05:53 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-02-07 10:05:53 [scrapy] INFO: Spider opened\n2016-02-07 10:05:53 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-02-07 10:05:53 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com> (referer: None)\n2016-02-07 10:05:53 [scrapy] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n{'title': 'The Scrapinghub Blog'}\n2016-02-07 10:05:53 [scrapy] INFO: Closing spider (finished)\n2016-02-07 10:05:53 [scrapy] INFO: Stored jl feed (1 items) in: stdout:\n2016-02-07 10:05:53 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 222,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 37890,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 2, 7, 1, 5, 53, 804361),\n 'item_scraped_count': 1,\n 'log_count/DEBUG': 2,\n 'log_count/INFO': 8,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2016, 2, 7, 1, 5, 53, 40263)}\n2016-02-07 10:05:53 [scrapy] INFO: Spider closed (finished)\n{\"title\": \"The Scrapinghub Blog\"}", "pull_request_status": "Merged", "issue_fixed_time": "2016-02-10T16:30:11Z", "files_changed": [["5", "scrapy/extensions/feedexport.py"]]}, "1024": {"issue_url": "https://github.com/scrapy/scrapy/issues/1766", "issue_id": "#1766", "issue_summary": "PY3: SitemapSpider fail to extract sitemap URLs from robots.txt in Scrapy 1.1.0rc1", "issue_description": "Contributor\norangain commented on Feb 6, 2016\nEnvironment\nMac OS X 10.10.5\nPython 3.4.2\nScrapy 1.1.0rc1\nSteps to Reproduce\nSave the following spider as sitemap_spider.py.\nfrom scrapy.spiders import SitemapSpider\n\n\nclass BlogSitemapSpider(SitemapSpider):\n   name = \"blog_sitemap\"\n   allowed_domains = [\"blog.scrapinghub.com\"]\n\n   sitemap_urls = [\n       'https://blog.scrapinghub.com/robots.txt',\n   ]\n   sitemap_rules = [\n       (r'/2016/', 'parse'),\n   ]\n\n   def parse(self, response):\n       pass\nRun the following command.\n$ scrapy runspider sitemap_spider.py\nExpected Results\nThe spider crawl several pages according to the sitemaps without error.\nActual Results\nThe spider fail to extract sitemap URLs from robots.txt. No pages are crawled.\n$ scrapy runspider sitemap_spider.py 2016-02-06 20:55:51 [scrapy] INFO: Scrapy 1.1.0rc1 started (bot: scrapybot)\n2016-02-06 20:55:51 [scrapy] INFO: Overridden settings: {}\n2016-02-06 20:55:52 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.logstats.LogStats']\n2016-02-06 20:55:52 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-02-06 20:55:52 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-02-06 20:55:52 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-02-06 20:55:52 [scrapy] INFO: Spider opened\n2016-02-06 20:55:52 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-02-06 20:55:52 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/robots.txt> (referer: None)\n2016-02-06 20:55:52 [scrapy] ERROR: Spider error processing <GET https://blog.scrapinghub.com/robots.txt> (referer: None)\nTraceback (most recent call last):\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n    for x in result:\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/spiders/sitemap.py\", line 35, in _parse_sitemap\n    for url in sitemap_urls_from_robots(response.body):\n  File \"/private/tmp/scrapy1.1/venv/lib/python3.4/site-packages/scrapy/utils/sitemap.py\", line 42, in sitemap_urls_from_robots\n    if line.lstrip().startswith('Sitemap:'):\nTypeError: startswith first arg must be bytes or a tuple of bytes, not str\n2016-02-06 20:55:52 [scrapy] INFO: Closing spider (finished)\n2016-02-06 20:55:52 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 231,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 1009,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 2, 6, 11, 55, 52, 570098),\n 'log_count/DEBUG': 1,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'spider_exceptions/TypeError': 1,\n 'start_time': datetime.datetime(2016, 2, 6, 11, 55, 52, 97618)}\n2016-02-06 20:55:52 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2016-02-06T14:53:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1025": {"issue_url": "https://github.com/scrapy/scrapy/issues/1764", "issue_id": "#1764", "issue_summary": "sslv3 alert handshake failure when making a request", "issue_description": "Contributor\nlagenar commented on Feb 5, 2016\nHi there, I recently upgraded to the latest scrapy and on some sites SSL enabled sites I get an exception when trying to make requests to it, while on previous scrapy versions I didn't have this issue.\nThe issue can be seen by making a request with scrapy shell:\nscrapy shell \"https://www.gohastings.com/\"\nThe error I get is:\nRetrying <GET https://www.gohastings.com/> (failed 1 times): <twisted.python.failure.Failure OpenSSL.SSL.Error: ('SSL routines', 'SSL3_READ_BYTES', 'sslv3 alert handshake failure'), ('SSL routines', 'SSL3_WRITE_BYTES', 'ssl handshake failure')>", "issue_status": "Closed", "issue_reporting_time": "2016-02-05T14:34:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1026": {"issue_url": "https://github.com/scrapy/scrapy/issues/1763", "issue_id": "#1763", "issue_summary": "BlogSpider on scrapy.org does not crawl archive pages now", "issue_description": "Contributor\norangain commented on Feb 5, 2016\nThe BlogSpider seen at http://scrapy.org/ now crawls only the home page of https://blog.scrapinghub.com/. This is because links to archives on the sidebar of the blog have been replaced with a dropdown list.\nSteps to Reproduce\nAs seen at http://scrapy.org/:\n$ pip install scrapy\n$ cat > myspider.py <<EOF\nimport scrapy\n\nclass BlogSpider(scrapy.Spider):\n    name = 'blogspider'\n    start_urls = ['http://blog.scrapinghub.com']\n\n    def parse(self, response):\n        for url in response.css('ul li a::attr(\"href\")').re(r'.*/\\d\\d\\d\\d/\\d\\d/$'):\n            yield scrapy.Request(response.urljoin(url), self.parse_titles)\n\n    def parse_titles(self, response):\n        for post_title in response.css('div.entries > ul > li a::text').extract():\n            yield {'title': post_title}\nEOF\n$ scrapy runspider myspider.py\nExpected Result\nThe spider crawls several tens of archive pages.\nActual Result\n$ scrapy runspider myspider.py\n2016-02-05 22:12:15 [scrapy] INFO: Scrapy 1.0.5 started (bot: scrapybot)\n2016-02-05 22:12:15 [scrapy] INFO: Optional features available: ssl, http11\n2016-02-05 22:12:15 [scrapy] INFO: Overridden settings: {}\n2016-02-05 22:12:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n2016-02-05 22:12:15 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2016-02-05 22:12:15 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2016-02-05 22:12:15 [scrapy] INFO: Enabled item pipelines:\n2016-02-05 22:12:15 [scrapy] INFO: Spider opened\n2016-02-05 22:12:15 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-02-05 22:12:15 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-02-05 22:12:15 [scrapy] DEBUG: Redirecting (301) to <GET https://blog.scrapinghub.com/> from <GET http://blog.scrapinghub.com>\n2016-02-05 22:12:15 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/> (referer: None)\n2016-02-05 22:12:15 [scrapy] INFO: Closing spider (finished)\n2016-02-05 22:12:15 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 436,\n 'downloader/request_count': 2,\n 'downloader/request_method_count/GET': 2,\n 'downloader/response_bytes': 38260,\n 'downloader/response_count': 2,\n 'downloader/response_status_count/200': 1,\n 'downloader/response_status_count/301': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 2, 5, 13, 12, 15, 983486),\n 'log_count/DEBUG': 3,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'start_time': datetime.datetime(2016, 2, 5, 13, 12, 15, 383464)}\n2016-02-05 22:12:15 [scrapy] INFO: Spider closed (finished)\nWorkaround\nReplacing the CSS selector in the parse() method ul li a::attr(\"href\") with option::attr(\"value\") should fix the problem. However, extracting URLs from <option> elements seems a little bit funny as a tutorial.", "issue_status": "Closed", "issue_reporting_time": "2016-02-05T14:06:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1027": {"issue_url": "https://github.com/scrapy/scrapy/issues/1762", "issue_id": "#1762", "issue_summary": "scrapy/xlib/pydispatch no longer in master", "issue_description": "Contributor\nlagenar commented on Feb 5, 2016\nHi there, I'm not sure if this is an issue or a design change. I just updated to the latest scrapy and my project has failed because it doesn't find the module scrapy.xlib.pydispatch. It was working fine some days ago and it looks like the code was removed from the release.", "issue_status": "Closed", "issue_reporting_time": "2016-02-05T14:04:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1028": {"issue_url": "https://github.com/scrapy/scrapy/issues/1760", "issue_id": "#1760", "issue_summary": "Exceptions are not helpful", "issue_description": "Member\nkmike commented on Feb 5, 2016\nSee #1759 (comment).", "issue_status": "Closed", "issue_reporting_time": "2016-02-04T19:11:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1029": {"issue_url": "https://github.com/scrapy/scrapy/issues/1759", "issue_id": "#1759", "issue_summary": "Python 3.4+ still not supported in 1.1.0rc1", "issue_description": "jschilling1 commented on Feb 4, 2016\nLooks like you guys might have only tested 3.3 for now, i can't even run that one because of a Django 1.9 dependency, running 3.4.4 and 3.5.1 results in the following:\nTraceback (most recent call last):\n  File \"/home/user/.pyenv/versions/3.4.4/lib/python3.4/site-packages/scrapy/commands/crawl.py\", line 57, in run\n    self.crawler_process.crawl(spname, **opts.spargs)\n  File \"/home/user/.pyenv/versions/3.4.4/lib/python3.4/site-packages/scrapy/crawler.py\", line 152, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"/home/user/.pyenv/versions/3.4.4/lib/python3.4/site-packages/scrapy/crawler.py\", line 156, in _crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/home/user/.pyenv/versions/3.4.4/lib/python3.4/site-packages/twisted/internet/defer.py\", line 1274, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"/home/user/.pyenv/versions/3.4.4/lib/python3.4/site-packages/twisted/internet/defer.py\", line 1126, in _inlineCallbacks\n    result = result.throwExceptionIntoGenerator(g)\n  File \"/home/user/.pyenv/versions/3.4.4/lib/python3.4/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n    return g.throw(self.type, self.value, self.tb)\n  File \"/home/user/.pyenv/versions/3.4.4/lib/python3.4/site-packages/scrapy/crawler.py\", line 80, in crawl\n    yield exc\nbuiltins.SyntaxError: invalid syntax (context_managers.py, line 535)\n2016-02-04 11:45:07 [twisted] CRITICAL: ", "issue_status": "Closed", "issue_reporting_time": "2016-02-04T16:52:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1030": {"issue_url": "https://github.com/scrapy/scrapy/issues/1745", "issue_id": "#1745", "issue_summary": "Reactor conflict", "issue_description": "ghost commented on Jan 29, 2016\nI'm launching scrapy manually from separate thread using twisted reactor. So the situation is: when one reactor is already running and scrapy tries to launch another one in crawler.start - that all crashes.\nWould you mind to add some checks for existing and running reactor? Thanx.", "issue_status": "Closed", "issue_reporting_time": "2016-01-29T15:23:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1031": {"issue_url": "https://github.com/scrapy/scrapy/issues/1744", "issue_id": "#1744", "issue_summary": "Redirects do not obey deny or allow patterns", "issue_description": "trifle commented on Jan 29, 2016\nCurrently, redirects go wherever they want, ignoring the deny_res and deny_domain patterns set on a LinkExtractor. This means that no matter how one configures the LinkExtractor, denied URLs may end up being downloaded, parsed and stored.\nThe best way to stop this from happening would be to write a new downloader middleware. It should subclass BaseRedirectMiddleware, load the allow/deny patterns the same way OffsiteMiddleware does and raise IgnoreRequest when redirect_urls match these patterns.\nThere is already one related, but less generic issue #1042 .\nI think I remember some discussion about a desirable refactor of the URL pattern matching logic (which is all over the place), but I can't find it currently.\nI'd love to provide a pull request but it's unlikely I find the time to do so and I also don't have any experience writing tests (sorry).", "issue_status": "Closed", "issue_reporting_time": "2016-01-29T13:57:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1032": {"issue_url": "https://github.com/scrapy/scrapy/issues/1743", "issue_id": "#1743", "issue_summary": "Can't install scrapy", "issue_description": "onmyway133 commented on Jan 29, 2016\nI'm using pip install scrapy on Mac OS X El Capitan, Python 2.7.10, git version 2.5.4 (Apple Git-61)\nand get error\nCleaning up...\nException:\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/commands/install.py\", line 283, in run\n    requirement_set.install(install_options, global_options, root=options.root_path)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/req.py\", line 1431, in install\n    requirement.uninstall(auto_confirm=True)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/req.py\", line 598, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/req.py\", line 1836, in remove\n    renames(path, new_path)\n  File \"/Library/Python/2.7/site-packages/pip-1.5.6-py2.7.egg/pip/util.py\", line 295, in renames\n    shutil.move(old, new)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 302, in move\n    copy2(src, real_dst)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 131, in copy2\n    copystat(src, dst)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 103, in copystat\n    os.chflags(dst, st.st_flags)\nOSError: [Errno 1] Operation not permitted: '/tmp/pip-Ma6isi-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six-1.4.1-py2.7.egg-info'\n\nStoring debug log for failure in /Users/khoa/Library/Logs/pip.log", "issue_status": "Closed", "issue_reporting_time": "2016-01-29T09:12:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1033": {"issue_url": "https://github.com/scrapy/scrapy/issues/1739", "issue_id": "#1739", "issue_summary": "DFO vs BFO in Scrapy FAQ", "issue_description": "Member\nkmike commented on Jan 27, 2016 \u2022\nedited\n@vincent-ferotin sent a message to scrapy-users a while ago (https://groups.google.com/forum/#!searchin/scrapy-users/Vincent$20F%C3%A9rotin/scrapy-users/n56O2sCAbp0/IbN8XGusAgAJ) and created a repo (https://github.com/vincent-ferotin/scraping-github) to demonstrate an issue: http://doc.scrapy.org/en/latest/faq.html#does-scrapy-crawl-in-breadth-first-or-depth-first-order says scrapy crawls DFO by default, but in practice this is not observed.\nIt looks like the tricky part is Downloader. By default Scrapy processes 16 requests in parallel. It means that Downloader asks Scheduler for 16 requests; after Downloader get them they are no longer handled by Scheduler. Downloader processes these requests in no particular order if there is enough concurrency per domain (8 by defualt), or uses a FIFO queue if concurrency is not enough. FIFO means BFO crawl, so for short request queues order is BFO regardless of Scheduler settings.\nI think we should at least clarify that in docs.\nSee also: #1727, #1440, #1371.", "issue_status": "Closed", "issue_reporting_time": "2016-01-27T18:09:39Z", "fixed_by": "#3621", "pull_request_summary": "Document that the crawl order is BFO for small numbers of start requests", "pull_request_description": "Member\nGallaecio commented on Feb 12, 2019\nFixes #1739", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-08T16:26:05Z", "files_changed": [["12", "docs/faq.rst"]]}, "1034": {"issue_url": "https://github.com/scrapy/scrapy/issues/1738", "issue_id": "#1738", "issue_summary": "AttributeError when exporting non-string types through XMLFeedExporter", "issue_description": "Member\nstummjr commented on Jan 27, 2016\nScrapy 1.0.4 fails to export items to XML when those items have non-string types. I tested it with a spider that generates an item like this: {'int': 2, 'boolean1': True, 'boolean2': False, 'time': datetime.datetime(2015, 1, 1, 1, 1, 1)}.\nHere is what I got when running the Spider to export XML items:\n$ scrapy runspider example.py -o items.xml\n...\n2016-01-27 15:55:42 [scrapy] DEBUG: Scraped from <200 http://www.example.com/>\n{'int': 2, 'boolean': True, 'boolean2': False, 'time': datetime.datetime(2015, 1, 1, 1, 1, 1)}\n2016-01-27 15:55:42 [scrapy] ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7f5f70075650>>\nTraceback (most recent call last):\n  File \"/home/stummjr/.virtualenvs/scrapy/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/home/stummjr/.virtualenvs/scrapy/local/lib/python2.7/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 57, in robustApply\n    return receiver(*arguments, **named)\n  File \"/home/stummjr/.virtualenvs/scrapy/local/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 193, in item_scraped\n    slot.exporter.export_item(item)\n  File \"/home/stummjr/.virtualenvs/scrapy/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 130, in export_item\n    self._export_xml_field(name, value)\n  File \"/home/stummjr/.virtualenvs/scrapy/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 146, in _export_xml_field\n    self._xg_characters(serialized_value)\n  File \"/home/stummjr/.virtualenvs/scrapy/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 157, in _xg_characters\n    serialized_value = serialized_value.decode(self.encoding)\nAttributeError: 'int' object has no attribute 'decode'\n...", "issue_status": "Closed", "issue_reporting_time": "2016-01-27T18:03:42Z", "fixed_by": "#1747", "pull_request_summary": "[MRG+1] Fix bug on XMLItemExporter with non-string fields in items", "pull_request_description": "Member\nstummjr commented on Jan 30, 2016\nThis PR fixes #1738", "pull_request_status": "Merged", "issue_fixed_time": "2016-02-02T04:35:39Z", "files_changed": [["4", "scrapy/exporters.py"], ["14", "tests/test_exporters.py"]]}, "1035": {"issue_url": "https://github.com/scrapy/scrapy/issues/1733", "issue_id": "#1733", "issue_summary": "KeyError in robotstxt middleware", "issue_description": "Member\nkmike commented on Jan 27, 2016\nI'm getting these errors in robots.txt middleware:\n2016-01-27 16:18:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://yellowpages.co.th>\nTraceback (most recent call last):\n  File \"/Users/kmike/envs/scraping/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/Users/kmike/svn/scrapy/scrapy/downloadermiddlewares/robotstxt.py\", line 65, in robot_parser\n    if isinstance(self._parsers[netloc], Deferred):\nKeyError: 'yellowpages.co.th'\nIt looks like #1473 caused it (I can't get this issue in Scrapy 1.0.4, but it present in Scrapy master). It happens when page failed to download and HTTP cache is enabled. I haven't debugged it further.", "issue_status": "Closed", "issue_reporting_time": "2016-01-27T11:25:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1036": {"issue_url": "https://github.com/scrapy/scrapy/issues/1732", "issue_id": "#1732", "issue_summary": "BaseSettings.__repr__ is too verbose for scrapy shell", "issue_description": "Member\nkmike commented on Jan 27, 2016\nSettings object got __repr__ in #1149, but it looks weird in scrapy shell:\n[s] Available Scrapy objects:\n[s] crawler <scrapy.crawler.Crawler object at 0x10c042fd0>\n[s] item {}\n[s] request <GET http://yellowpages.co.th>\n[s] response <302 http://yellowpages.co.th>\n[s] settings {'CLOSESPIDER_ITEMCOUNT': <SettingsAttribute value=0 priority=0>, 'RETRY_HTTP_CODES': <SettingsAttribute value=[500, 502, 503, 504, 408] priority=0>, 'HTTPCACHE_DIR': <SettingsAttribute value='httpcache-2' priority=20>, 'MEMUSAGE_REPORT': <SettingsAttribute value=False priority=0>, 'DOWNLOAD_HANDLERS_BASE': <SettingsAttribute value=<BaseSettings {'s3': <SettingsAttribute value='scrapy.core.downloader.handlers.s3.S3DownloadHandler' priority=0>, 'ftp': <SettingsAttribute value='scrapy.core.downloader.handlers.ftp.FTPDownloadHandler' priority=0>, 'http': <SettingsAttribute value='scrapy.core.downloader.handlers.http.HTTPDownloadHandler' priority=0>, 'https': <SettingsAttribute value='scrapy.core.downloader.handlers.http.HTTPDownloadHandler' priority=0>, 'file': <SettingsAttribute value='scrapy.core.downloader.handlers.file.FileDownloadHandler' priority=0>}> priority=0>, 'RETRY_PRIORITY_ADJUST': <SettingsAttribute value=-1 priority=0>, 'MAIL_FROM': <SettingsAttribute value='scrapy@localhost' priority=0>, 'HTTPCACHE_EXPIRATION_SECS': <SettingsAttribute value=86400 priority=20>, 'SPIDER_LOADER_CLASS': <SettingsAttribute value='scrapy.spiderloader.SpiderLoader' priority=0>, 'COMPRESSION_ENABLED': <SettingsAttribute value=True priority=0>, 'DOWNLOAD_TIMEOUT': <SettingsAttribute value=180 priority=0>, 'MAIL_PASS': <SettingsAttribute value=None priority=0>, 'MEMUSAGE_LIMIT_MB': <SettingsAttribute value=0 priority=0>, 'EXTENSIONS': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'DEPTH_PRIORITY': <SettingsAttribute value=0 priority=0>, 'TELNETCONSOLE_HOST': <SettingsAttribute value='127.0.0.1' priority=0>, 'MEMDEBUG_NOTIFY': <SettingsAttribute value=[] priority=0>, 'HTTPPROXY_AUTH_ENCODING': <SettingsAttribute value='latin-1' priority=0>, 'DOWNLOAD_WARNSIZE': <SettingsAttribute value=33554432 priority=0>, 'SPIDER_MODULES': <SettingsAttribute value=['acrawler.spiders'] priority=20>, 'RETRY_TIMES': <SettingsAttribute value=2 priority=0>, 'TELNETCONSOLE_PORT': <SettingsAttribute value=[6023, 6073] priority=0>, 'TELNETCONSOLE_ENABLED': <SettingsAttribute value=False priority=20>, 'DOWNLOADER_MIDDLEWARES': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'HTTPCACHE_DBM_MODULE': <SettingsAttribute value='anydbm' priority=0>, 'ROBOTSTXT_OBEY': <SettingsAttribute value=True priority=20>, 'DEPTH_LIMIT': <SettingsAttribute value=0 priority=0>, 'REACTOR_THREADPOOL_MAXSIZE': <SettingsAttribute value=10 priority=0>, 'FEED_EXPORT_FIELDS': <SettingsAttribute value=None priority=0>, 'CLOSESPIDER_PAGECOUNT': <SettingsAttribute value=0 priority=0>, 'LOG_SHORT_NAMES': <SettingsAttribute value=False priority=0>, 'AUTOTHROTTLE_MAX_DELAY': <SettingsAttribute value=60 priority=20>, 'URLLENGTH_LIMIT': <SettingsAttribute value=2083 priority=0>, 'FEED_EXPORTERS': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'LOG_ENCODING': <SettingsAttribute value='utf-8' priority=0>, 'FEED_EXPORTERS_BASE': <SettingsAttribute value=<BaseSettings {'xml': <SettingsAttribute value='scrapy.exporters.XmlItemExporter' priority=0>, 'jsonlines': <SettingsAttribute value='scrapy.exporters.JsonLinesItemExporter' priority=0>, 'jl': <SettingsAttribute value='scrapy.exporters.JsonLinesItemExporter' priority=0>, 'json': <SettingsAttribute value='scrapy.exporters.JsonItemExporter' priority=0>, 'csv': <SettingsAttribute value='scrapy.exporters.CsvItemExporter' priority=0>, 'pickle': <SettingsAttribute value='scrapy.exporters.PickleItemExporter' priority=0>, 'marshal': <SettingsAttribute value='scrapy.exporters.MarshalItemExporter' priority=0>}> priority=0>, 'FEED_FORMAT': <SettingsAttribute value='jsonlines' priority=0>, 'DOWNLOAD_DELAY': <SettingsAttribute value=0 priority=0>, 'HTTPCACHE_GZIP': <SettingsAttribute value=False priority=0>, 'DOWNLOADER_MIDDLEWARES_BASE': <SettingsAttribute value=<BaseSettings {'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': <SettingsAttribute value=400 priority=0>, 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': <SettingsAttribute value=550 priority=0>, 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': <SettingsAttribute value=590 priority=0>, 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware': <SettingsAttribute value=830 priority=0>, 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': <SettingsAttribute value=100 priority=0>, 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': <SettingsAttribute value=350 priority=0>, 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': <SettingsAttribute value=600 priority=0>, 'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': <SettingsAttribute value=560 priority=0>, 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': <SettingsAttribute value=750 priority=0>, 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': <SettingsAttribute value=900 priority=0>, 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': <SettingsAttribute value=300 priority=0>, 'scrapy.downloadermiddlewares.retry.RetryMiddleware': <SettingsAttribute value=500 priority=0>, 'scrapy.downloadermiddlewares.stats.DownloaderStats': <SettingsAttribute value=850 priority=0>, 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': <SettingsAttribute value=700 priority=0>, 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': <SettingsAttribute value=580 priority=0>}> priority=0>, 'DNSCACHE_ENABLED': <SettingsAttribute value=True priority=0>, 'CONCURRENT_REQUESTS_PER_IP': <SettingsAttribute value=0 priority=0>, 'EDITOR': <SettingsAttribute value='nano' priority=0>, 'MAIL_HOST': <SettingsAttribute value='localhost' priority=0>, 'CONCURRENT_REQUESTS': <SettingsAttribute value=100 priority=20>, 'AUTOTHROTTLE_START_DELAY': <SettingsAttribute value=1 priority=20>, 'CLOSESPIDER_ERRORCOUNT': <SettingsAttribute value=0 priority=0>, 'STATS_CLASS': <SettingsAttribute value='scrapy.statscollectors.MemoryStatsCollector' priority=0>, 'FEED_STORAGES_BASE': <SettingsAttribute value=<BaseSettings {'': <SettingsAttribute value='scrapy.extensions.feedexport.FileFeedStorage' priority=0>, 's3': <SettingsAttribute value='scrapy.extensions.feedexport.S3FeedStorage' priority=0>, 'ftp': <SettingsAttribute value='scrapy.extensions.feedexport.FTPFeedStorage' priority=0>, 'file': <SettingsAttribute value='scrapy.extensions.feedexport.FileFeedStorage' priority=0>, 'stdout': <SettingsAttribute value='scrapy.extensions.feedexport.StdoutFeedStorage' priority=0>}> priority=0>, 'REDIRECT_ENABLED': <SettingsAttribute value=True priority=0>, 'AUTOTHROTTLE_ENABLED': <SettingsAttribute value=True priority=20>, 'COMMANDS_MODULE': <SettingsAttribute value='' priority=0>, 'AUTOTHROTTLE_DEBUG': <SettingsAttribute value=False priority=0>, 'NEWSPIDER_MODULE': <SettingsAttribute value='acrawler.spiders' priority=20>, 'LOG_UNSERIALIZABLE_REQUESTS': <SettingsAttribute value=False priority=0>, 'DOWNLOAD_MAXSIZE': <SettingsAttribute value=1073741824 priority=0>, 'MAIL_PORT': <SettingsAttribute value=25 priority=0>, 'REFERER_ENABLED': <SettingsAttribute value=True priority=0>, 'HTTPCACHE_POLICY': <SettingsAttribute value='scrapy.extensions.httpcache.DummyPolicy' priority=0>, 'STATS_DUMP': <SettingsAttribute value=True priority=0>, 'MEMUSAGE_NOTIFY_MAIL': <SettingsAttribute value=[] priority=0>, 'DOWNLOAD_HANDLERS': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'LOG_DATEFORMAT': <SettingsAttribute value='%Y-%m-%d %H:%M:%S' priority=0>, 'LOG_LEVEL': <SettingsAttribute value='DEBUG' priority=0>, 'DOWNLOADER_HTTPCLIENTFACTORY': <SettingsAttribute value='scrapy.core.downloader.webclient.ScrapyHTTPClientFactory' priority=0>, 'REDIRECT_MAX_TIMES': <SettingsAttribute value=20 priority=0>, 'REDIRECT_PRIORITY_ADJUST': <SettingsAttribute value=2 priority=0>, 'DUPEFILTER_CLASS': <SettingsAttribute value='scrapy.dupefilters.BaseDupeFilter' priority=10>, 'RETRY_ENABLED': <SettingsAttribute value=True priority=0>, 'SPIDER_CONTRACTS': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'HTTPCACHE_ENABLED': <SettingsAttribute value=True priority=20>, 'LOG_ENABLED': <SettingsAttribute value=True priority=0>, 'MAIL_USER': <SettingsAttribute value=None priority=0>, 'HTTPCACHE_ALWAYS_STORE': <SettingsAttribute value=False priority=0>, 'LOGSTATS_INTERVAL': <SettingsAttribute value=0 priority=10>, 'DEFAULT_ITEM_CLASS': <SettingsAttribute value='scrapy.item.Item' priority=0>, 'DNS_TIMEOUT': <SettingsAttribute value=60 priority=0>, 'DEPTH_STATS': <SettingsAttribute value=True priority=0>, 'DOWNLOADER_CLIENTCONTEXTFACTORY': <SettingsAttribute value='scrapy.core.downloader.contextfactory.ScrapyClientContextFactory' priority=0>, 'MEMUSAGE_CHECK_INTERVAL_SECONDS': <SettingsAttribute value=60.0 priority=0>, 'EXTENSIONS_BASE': <SettingsAttribute value=<BaseSettings {'scrapy.extensions.corestats.CoreStats': <SettingsAttribute value=0 priority=0>, 'scrapy.extensions.feedexport.FeedExporter': <SettingsAttribute value=0 priority=0>, 'scrapy.extensions.memdebug.MemoryDebugger': <SettingsAttribute value=0 priority=0>, 'scrapy.extensions.memusage.MemoryUsage': <SettingsAttribute value=0 priority=0>, 'scrapy.extensions.logstats.LogStats': <SettingsAttribute value=0 priority=0>, 'scrapy.extensions.telnet.TelnetConsole': <SettingsAttribute value=0 priority=0>, 'scrapy.extensions.closespider.CloseSpider': <SettingsAttribute value=0 priority=0>, 'scrapy.extensions.spiderstate.SpiderState': <SettingsAttribute value=0 priority=0>, 'scrapy.extensions.throttle.AutoThrottle': <SettingsAttribute value=0 priority=0>}> priority=0>, 'FEED_STORAGES': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'BOT_NAME': <SettingsAttribute value='acrawler' priority=20>, 'SPIDER_CONTRACTS_BASE': <SettingsAttribute value=<BaseSettings {'scrapy.contracts.default.ScrapesContract': <SettingsAttribute value=3 priority=0>, 'scrapy.contracts.default.UrlContract': <SettingsAttribute value=1 priority=0>, 'scrapy.contracts.default.ReturnsContract': <SettingsAttribute value=2 priority=0>}> priority=0>, 'METAREFRESH_MAXDELAY': <SettingsAttribute value=100 priority=0>, 'CONCURRENT_REQUESTS_PER_DOMAIN': <SettingsAttribute value=8 priority=0>, 'HTTPCACHE_IGNORE_HTTP_CODES': <SettingsAttribute value=[] priority=0>, 'KEEP_ALIVE': <SettingsAttribute value=True priority=10>, 'ITEM_PROCESSOR': <SettingsAttribute value='scrapy.pipelines.ItemPipelineManager' priority=0>, 'MEMUSAGE_WARNING_MB': <SettingsAttribute value=0 priority=0>, 'FEED_STORE_EMPTY': <SettingsAttribute value=False priority=0>, 'COOKIES_DEBUG': <SettingsAttribute value=False priority=0>, 'FEED_URI': <SettingsAttribute value=None priority=0>, 'SPIDER_MIDDLEWARES': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'DOWNLOADER': <SettingsAttribute value='scrapy.core.downloader.Downloader' priority=0>, 'AUTOTHROTTLE_TARGET_CONCURRENCY': <SettingsAttribute value=1.0 priority=20>, 'USER_AGENT': <SettingsAttribute value='acrawler' priority=20>, 'AJAXCRAWL_ENABLED': <SettingsAttribute value=False priority=0>, 'COOKIES_ENABLED': <SettingsAttribute value=False priority=20>, 'DNSCACHE_SIZE': <SettingsAttribute value=10000 priority=0>, 'LOG_FORMAT': <SettingsAttribute value='%(asctime)s [%(name)s] %(levelname)s: %(message)s' priority=0>, 'ITEM_PIPELINES': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'LOG_FORMATTER': <SettingsAttribute value='scrapy.logformatter.LogFormatter' priority=0>, 'HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS': <SettingsAttribute value=[] priority=0>, 'METAREFRESH_ENABLED': <SettingsAttribute value=True priority=0>, 'HTTPCACHE_IGNORE_MISSING': <SettingsAttribute value=False priority=0>, 'HTTPCACHE_IGNORE_SCHEMES': <SettingsAttribute value=['file'] priority=0>, 'SCHEDULER_MEMORY_QUEUE': <SettingsAttribute value='scrapy.squeues.LifoMemoryQueue' priority=0>, 'SCHEDULER_DISK_QUEUE': <SettingsAttribute value='scrapy.squeues.PickleLifoDiskQueue' priority=0>, 'RANDOMIZE_DOWNLOAD_DELAY': <SettingsAttribute value=True priority=0>, 'SETTINGS_MODULE': <SettingsAttribute value='acrawler.settings' priority=20>, 'TEMPLATES_DIR': <SettingsAttribute value='/Users/kmike/svn/scrapy/scrapy/templates' priority=0>, 'LOG_STDOUT': <SettingsAttribute value=False priority=0>, 'CONCURRENT_ITEMS': <SettingsAttribute value=100 priority=0>, 'DOWNLOADER_STATS': <SettingsAttribute value=True priority=0>, 'LOG_FILE': <SettingsAttribute value=None priority=0>, 'HTTPCACHE_STORAGE': <SettingsAttribute value='scrapy.extensions.httpcache.FilesystemCacheStorage' priority=20>, 'MEMDEBUG_ENABLED': <SettingsAttribute value=False priority=0>, 'FEED_URI_PARAMS': <SettingsAttribute value=None priority=0>, 'DEFAULT_REQUEST_HEADERS': <SettingsAttribute value=<BaseSettings {'Accept-Language': <SettingsAttribute value='en' priority=0>, 'Accept': <SettingsAttribute value='text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8' priority=0>}> priority=0>, 'CLOSESPIDER_TIMEOUT': <SettingsAttribute value=0 priority=0>, 'SCHEDULER': <SettingsAttribute value='scrapy.core.scheduler.Scheduler' priority=0>, 'SPIDER_MIDDLEWARES_BASE': <SettingsAttribute value=<BaseSettings {'scrapy.spidermiddlewares.referer.RefererMiddleware': <SettingsAttribute value=700 priority=0>, 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': <SettingsAttribute value=50 priority=0>, 'scrapy.spidermiddlewares.depth.DepthMiddleware': <SettingsAttribute value=900 priority=0>, 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware': <SettingsAttribute value=800 priority=0>, 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': <SettingsAttribute value=500 priority=0>}> priority=0>, 'ITEM_PIPELINES_BASE': <SettingsAttribute value=<BaseSettings {}> priority=0>, 'STATSMAILER_RCPTS': <SettingsAttribute value=[] priority=0>, 'MEMUSAGE_ENABLED': <SettingsAttribute value=False priority=0>}\n[s] spider <DefaultSpider 'default' at 0x10ebdcf90>\n[s] Useful shortcuts:\n[s] shelp() Shell help (print this help)\n[s] fetch(req_or_url) Fetch request (or URL) and update local objects\n[s] view(response) View response in a browser", "issue_status": "Closed", "issue_reporting_time": "2016-01-27T10:56:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1037": {"issue_url": "https://github.com/scrapy/scrapy/issues/1729", "issue_id": "#1729", "issue_summary": "Provide TextResponse.text attribute", "issue_description": "Member\nkmike commented on Jan 27, 2016\nWhat about adding response.text property as a shortcut for response.body_as_unicode(), like requests library does it? TextResponse and its subclasses will have it.", "issue_status": "Closed", "issue_reporting_time": "2016-01-26T19:39:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1038": {"issue_url": "https://github.com/scrapy/scrapy/issues/1718", "issue_id": "#1718", "issue_summary": "Support S3 downloader handler on Python3", "issue_description": "Contributor\nredapple commented on Jan 25, 2016\nContinuation of #1716 (comment)", "issue_status": "Closed", "issue_reporting_time": "2016-01-25T12:32:23Z", "fixed_by": "#1761", "pull_request_summary": "[MRG+1] Py3 S3 botocore", "pull_request_description": "Member\nlopuhin commented on Feb 5, 2016\nThe main idea is that we try to use botocore by default, falling back to boto on Py2. botocore is more recent and has better Py3 support. Should fix #1718 (see comments there about some changes in test). boto code path is still tested in precise environment.", "pull_request_status": "Merged", "issue_fixed_time": "2016-02-18T14:11:03Z", "files_changed": [["11", "docs/topics/feed-exports.rst"], ["65", "scrapy/core/downloader/handlers/s3.py"], ["32", "scrapy/extensions/feedexport.py"], ["10", "scrapy/http/headers.py"], ["99", "scrapy/pipelines/files.py"], ["21", "scrapy/utils/boto.py"]]}, "1039": {"issue_url": "https://github.com/scrapy/scrapy/issues/1714", "issue_id": "#1714", "issue_summary": "can`t install ubuntu 14.04/python 2.7.6", "issue_description": "caojianfeng commented on Jan 24, 2016\nroot@4e1d23b42fbc:/# pip install scrapy\nCollecting scrapy\nUsing cached Scrapy-1.0.4-py2-none-any.whl\nCollecting cssselect>=0.9 (from scrapy)\nUsing cached cssselect-0.9.1.tar.gz\nCollecting queuelib (from scrapy)\nUsing cached queuelib-1.4.2-py2.py3-none-any.whl\nCollecting pyOpenSSL (from scrapy)\nUsing cached pyOpenSSL-0.15.1-py2.py3-none-any.whl\nCollecting w3lib>=1.8.0 (from scrapy)\nUsing cached w3lib-1.13.0-py2.py3-none-any.whl\nCollecting lxml (from scrapy)\nUsing cached lxml-3.5.0.tar.gz\nCollecting Twisted>=10.0.0 (from scrapy)\nUsing cached Twisted-15.5.0.tar.bz2\nCollecting six>=1.5.2 (from scrapy)\nUsing cached six-1.10.0-py2.py3-none-any.whl\nCollecting service-identity (from scrapy)\nUsing cached service_identity-14.0.0-py2.py3-none-any.whl\nCollecting cryptography>=0.7 (from pyOpenSSL->scrapy)\nUsing cached cryptography-1.2.1.tar.gz\nCollecting zope.interface>=3.6.0 (from Twisted>=10.0.0->scrapy)\nUsing cached zope.interface-4.1.3.tar.gz\nCollecting characteristic>=14.0.0 (from service-identity->scrapy)\nUsing cached characteristic-14.3.0-py2.py3-none-any.whl\nCollecting pyasn1-modules (from service-identity->scrapy)\nUsing cached pyasn1_modules-0.0.8-py2.py3-none-any.whl\nCollecting pyasn1 (from service-identity->scrapy)\n/usr/local/lib/python2.7/dist-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:315: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning.\nSNIMissingWarning\n/usr/local/lib/python2.7/dist-packages/pip/vendor/requests/packages/urllib3/util/ssl.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\nInsecurePlatformWarning\nUsing cached pyasn1-0.1.9-py2.py3-none-any.whl\nCollecting idna>=2.0 (from cryptography>=0.7->pyOpenSSL->scrapy)\nUsing cached idna-2.0-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): setuptools>=1.0 in /usr/local/lib/python2.7/dist-packages (from cryptography>=0.7->pyOpenSSL->scrapy)\nCollecting enum34 (from cryptography>=0.7->pyOpenSSL->scrapy)\nUsing cached enum34-1.1.2.tar.gz\nCollecting ipaddress (from cryptography>=0.7->pyOpenSSL->scrapy)\nUsing cached ipaddress-1.0.16-py27-none-any.whl\nCollecting cffi>=1.4.1 (from cryptography>=0.7->pyOpenSSL->scrapy)\nUsing cached cffi-1.5.0.tar.gz\nComplete output from command python setup.py egg_info:\nunable to execute 'x86_64-linux-gnu-gcc': No such file or directory\nunable to execute 'x86_64-linux-gnu-gcc': No such file or directory\n    No working compiler found, or bogus compiler options\n    passed to the compiler from Python's distutils module.\n    See the error messages above.\n    (If they are about -mno-fused-madd and you are on OS/X 10.8,\n    see http://stackoverflow.com/questions/22313407/ .)\n\n----------------------------------------\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-jyxj2M/cffi\nroot@4e1d23b42fbc:/#", "issue_status": "Closed", "issue_reporting_time": "2016-01-24T03:37:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1040": {"issue_url": "https://github.com/scrapy/scrapy/issues/1713", "issue_id": "#1713", "issue_summary": "Pickle and marshal output doesn't work for multiple items", "issue_description": "Member\neliasdorneles commented on Jan 24, 2016\nSo, I was trying to add a test for Pickle output and found out that it doesn't really work for more than one item.\nTry running the following spider with scrapy runspider samplespider.py -o output.pickle:\nimport scrapy\n\nclass SampleSpider(scrapy.Spider):\n    name = 'sample'\n    start_urls = ['http://example.com']\n\n    def parse(self, response):\n        yield {'key1': '10', 'key2': '20'}\n        yield {'key1': '11', 'key2': '21'}\n        yield {'key1': '12', 'key2': '22'}\nIt writes an output.pickle file alright, but if you try to read the output, you get only the first item:\n>>> import pickle\n>>> pickle.load(open('output.pickle'))\n{'key2': '20', 'key1': '10'}\nThe pickle itself seems to have the data:\n$ cat output.pickle \n\ufffd\ufffd}q\ufffd(U\ufffdkey2q\ufffdU\ufffd20q\ufffdU\ufffdkey1q\ufffdU\ufffd10qu.\ufffd\ufffd}q\ufffd(U\ufffdkey2q\ufffdU\ufffd21q\ufffdU\ufffdkey1q\ufffdU\ufffd11qu.\ufffd\ufffd}q\ufffd(U\ufffdkey2q\ufffdU\ufffd22q\ufffdU\ufffdkey1q\ufffdU\ufffd12qu.\nIt's just not usable.\nAnd it happens the same with the marchal output, try doing scrapy runspider samplespider.py -o output.marshal and then:\n>>> import marshal\n>>> marshal.load(open('output.marshal'))\n{'key2': '20', 'key1': '10'}\nAm I missing something?\nIs anyone actually using these features? :)", "issue_status": "Closed", "issue_reporting_time": "2016-01-23T19:07:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1041": {"issue_url": "https://github.com/scrapy/scrapy/issues/1707", "issue_id": "#1707", "issue_summary": "Python 3.3 tests are unstable", "issue_description": "Member\nkmike commented on Jan 21, 2016\nThey often fail on first try - bench command takes too long to execute.", "issue_status": "Closed", "issue_reporting_time": "2016-01-21T11:55:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1042": {"issue_url": "https://github.com/scrapy/scrapy/issues/1704", "issue_id": "#1704", "issue_summary": "[Docs] Mention that .re() does HTML entities decoding", "issue_description": "Contributor\nredapple commented on Jan 20, 2016\nDocumentation on Selectors says:\nextract()\nSerialize and return the matched nodes as a list of unicode strings. Percent encoded content is unquoted.\nre(regex)\nApply the given regex and return a list of unicode strings with the matches.\nregex can be either a compiled regular expression or a string which will be compiled to a regular expression using re.compile(regex)\nbut it doesn't say that .re() (and .re_first()) also perform HTML-entities decoding (except &lt; and &amp;)\nSee #1700\nand https://stackoverflow.com/questions/34887730/how-to-extract-raw-html-from-a-scrapy-selector#comment57542664_34897754\nparsel documentation is even less verbose.", "issue_status": "Closed", "issue_reporting_time": "2016-01-20T16:30:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1043": {"issue_url": "https://github.com/scrapy/scrapy/issues/1702", "issue_id": "#1702", "issue_summary": "Add support for Google AppEngine", "issue_description": "bhagyas commented on Jan 20, 2016\nScrapy should be runnable on AppEngine.", "issue_status": "Closed", "issue_reporting_time": "2016-01-20T13:43:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1044": {"issue_url": "https://github.com/scrapy/scrapy/issues/1700", "issue_id": "#1700", "issue_summary": "No way to .extract selector retaining html entities/quoting?", "issue_description": "jschilling1 commented on Jan 20, 2016\nI'm extracting js data using .re_first() and later converting it to python native data. The problem is extract/re method don't seem to provide a way to not unquote html, i.e.\noriginal html:\n{my_fields:['O&#\\39;Connor Park'], } (ignore the slashes..)\nre_first() output\n{my_fields:['O'Connor Park'], }\nturning this output into json won't work.\nIs there an easy way around it?\nEDIT:\nre.search(r'exp', extract_first()) actually works, keeping the raw quoted html\nextract_first() outputs {my_fields:['O&#\\39;Connor Park'],\nis it intentional that extract_first() and re_first() deal with quoting differently?", "issue_status": "Closed", "issue_reporting_time": "2016-01-19T21:40:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1045": {"issue_url": "https://github.com/scrapy/scrapy/issues/1697", "issue_id": "#1697", "issue_summary": "Elegent way to access constants in settings.py from spider class definition", "issue_description": "wlnirvana commented on Jan 19, 2016\nI would like to access settings.py from spider to do some customization like the following:\nclass BarSpider(scrapy.Spider):\n    name = \"bar\"\n    allowed_domains = [\"bar.com\"]\n    start_urls = (\n        'http://www.bar.com/',\n    )\n\n    def __init__(self, *args, **kwargs):\n        super(BarSpider, self).__init__(*args, **kwargs)\n        print self.settings\n        self.foo = settings.get('foo')\n\n    def parse(self, response):\n        pass\nIs this possible?", "issue_status": "Closed", "issue_reporting_time": "2016-01-19T15:17:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1046": {"issue_url": "https://github.com/scrapy/scrapy/issues/1696", "issue_id": "#1696", "issue_summary": "Https11TestCase.test_timeout_download_from_spider fails to cleanup", "issue_description": "Member\nlopuhin commented on Jan 19, 2016\nFrom #1678 (comment)\nCurrently skipped https test test_timeout_download_from_spider fails to cleanup (both on py2 and py3), not sure if is some test artifact or real issue, and how to dig it out:\n_______________________________________________________________ Https11TestCase.test_timeout_download_from_spider ________________________________________________________________\nNOTE: Incompatible Exception Representation, displaying natively:\n\nDirtyReactorAggregateError: Reactor was unclean.\nSelectables:\n<<class 'twisted.internet.tcp.Client'> to ('localhost', 58185) at 10d488750>", "issue_status": "Closed", "issue_reporting_time": "2016-01-19T14:56:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1047": {"issue_url": "https://github.com/scrapy/scrapy/issues/1695", "issue_id": "#1695", "issue_summary": "Codecov review checks", "issue_description": "Contributor\nnramirezuy commented on Jan 19, 2016\nSome of the checks are not showing. Ex: #1692 #1654\nDoes anyone knows what is this about?", "issue_status": "Closed", "issue_reporting_time": "2016-01-19T14:35:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1048": {"issue_url": "https://github.com/scrapy/scrapy/issues/1685", "issue_id": "#1685", "issue_summary": "django.setup() used for django_item causes a massive memory leak in Scrapy", "issue_description": "jschilling1 commented on Jan 17, 2016\nI need to call django.setup() to use django_item, when i do, Scrapy's memory usage keeps growing with the number of returned items.\nMy actual spider returns about 100 pages per html, but 1000 makes it grow to a gigabyte in minutes for the purposes of this.\ndef parse(self, response):\nreturn ({'url': 'http://localhost/=%s' % randint(1, 9999999)} for i in xrange(1000))\nDisabling 'debug_toolbar' in INSTALLED_APPS fixes this problem.\nI'd really like to know how its affecting Scrapy. It took me a while to track it down django, guppy and trackrefs didn't point to a specific problem except a unicode() and dict() object count build up.", "issue_status": "Closed", "issue_reporting_time": "2016-01-17T15:15:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1049": {"issue_url": "https://github.com/scrapy/scrapy/issues/1684", "issue_id": "#1684", "issue_summary": "Test Windows builds with Appveyor integration", "issue_description": "Contributor\nredapple commented on Jan 16, 2016\nJust like scrapy has Travis CI integration, scrapy can also be tested on Windows with http://www.appveyor.com/", "issue_status": "Closed", "issue_reporting_time": "2016-01-15T19:11:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1050": {"issue_url": "https://github.com/scrapy/scrapy/issues/1679", "issue_id": "#1679", "issue_summary": "python script call scrapy missing data", "issue_description": "kedizhou commented on Jan 15, 2016\ni have a spider youtube script,example, use command 'scrapy crawl youtube' by manual in linux command terminal, return 1000line json data file,but use crontab call following script return 950 line json data file,missing 50 line data,why does this?\nfrom multiprocessing import Pool\nimport os, sys\n\ndef _crawl(spider_name=None):\n    if spider_name:\n        path = os.path.dirname(os.path.abspath(sys.argv[0]))\n        os.chdir(path + '/' + spider_name)\n        os.environ[\"PATH\"] = os.environ[\"PATH\"] + ':/usr/local/bin/'\n        os.system('scrapy crawl %s' % spider_name)\n    return None\n\n\ndef run_crawler(list):\n    pool = Pool(processes=len(list))\n    pool.map(_crawl, list)\n\n\nlist = ['youtube']\nrun_crawler(list)", "issue_status": "Closed", "issue_reporting_time": "2016-01-15T06:30:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1051": {"issue_url": "https://github.com/scrapy/scrapy/issues/1673", "issue_id": "#1673", "issue_summary": "Spider status Collection Issue", "issue_description": "agramesh commented on Jan 13, 2016\nHi\nBefore I was use scrapy version \"Scrapy v 0.24\" and use the below method for collect the spider status\n    def insertStats(self,spider,spider_stats):\n        d = self.dbpool.runInteraction(self.stats_spider_closed, spider, spider_stats)\n        return d\n\n    def stats_spider_closed(self,conn, spider, spider_stats):\nBut now i updated to Scrapy v 1.0 , and now i receive the error like this, how can resolve it.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/xlib/pydispatch/robustapply.py\", line 57, in robustApply\n    return receiver(*arguments, **named)\nTypeError: insertStats() takes exactly 3 arguments (2 given)", "issue_status": "Closed", "issue_reporting_time": "2016-01-13T07:36:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1052": {"issue_url": "https://github.com/scrapy/scrapy/issues/1672", "issue_id": "#1672", "issue_summary": "extract_nth()", "issue_description": "joshuakeough commented on Jan 13, 2016\nIt would be extremely useful an extract_nth() method to add flexibility to the extract_first() method when extracting text with selectors.", "issue_status": "Closed", "issue_reporting_time": "2016-01-13T03:13:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1053": {"issue_url": "https://github.com/scrapy/scrapy/issues/1670", "issue_id": "#1670", "issue_summary": "I have tried to execute the program but i got an unhanded Exception Can any one please help me", "issue_description": "PILLUTLAAVINASH commented on Jan 12, 2016", "issue_status": "Closed", "issue_reporting_time": "2016-01-12T05:22:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1054": {"issue_url": "https://github.com/scrapy/scrapy/issues/1668", "issue_id": "#1668", "issue_summary": "Set support for robots.txt to be on by default", "issue_description": "mvl22 commented on Jan 11, 2016\nWe are seeing sites receiving crawls with this bot's signature which disobey robots.txt, falling into our honeytrap.\nPlease set support for robots.txt to be on by default. Currently the default is do not obey:\nhttp://doc.scrapy.org/en/latest/topics/settings.html#robotstxt-obey\nI would argue that crawlers have a responsibility to ensure that they act as a well-behaved web citizen by default. This applies even more to open-source code crawlers, which are likely to be used by people the author has no control over.\nSo please kindly ensure this behaviour is on by default. If someone is installing this crawler, and they are using it on their own website, they will have full knowledge of why robots.txt rules are in place, and so can decide to disable the feature. If they are using it on another site, then the robots.txt should be respected, because that is what the site owner has specifically defined as being reasonable usage.", "issue_status": "Closed", "issue_reporting_time": "2016-01-11T10:59:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1055": {"issue_url": "https://github.com/scrapy/scrapy/issues/1667", "issue_id": "#1667", "issue_summary": "\"Server not found\" doc.scrapy.org", "issue_description": "jschilling1 commented on Jan 11, 2016\nI've had this issue consistently for months using Firefox.\nSeveral refreshes of the page finally allow it to load.\nI realize this could be a dns issue with my isp that is completely unrelated to scrapy but the fact that i've never had the same problem with any other site merits a bug report in case anyone else notices this.", "issue_status": "Closed", "issue_reporting_time": "2016-01-10T21:02:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1056": {"issue_url": "https://github.com/scrapy/scrapy/issues/1666", "issue_id": "#1666", "issue_summary": "[help] selectors.rst line 168 error", "issue_description": "smileboywtu commented on Jan 9, 2016\nerror\nThe pseudo-class :attr() is unknown.\nI just follow the document, and comes with the error, I'm totally a newer to scrapy. can some body tell me if I need to import something myself?", "issue_status": "Closed", "issue_reporting_time": "2016-01-09T03:07:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1057": {"issue_url": "https://github.com/scrapy/scrapy/issues/1665", "issue_id": "#1665", "issue_summary": "XMLFeedSpider encoding issue", "issue_description": "busla commented on Jan 8, 2016\nScrapy version: 1.04\nMy spider (from the XMLFeedSpider example in the docs) doesn\u00b4t seem to read the defined itertag since it contains the iso-8859-1 letter \"\u00de\"\nhttp://www.w3schools.com/charsets/ref_html_8859.asp\nI\u00b4ve tried my code on a different url with english xml tags and it works fine.\n# -*- coding: utf-8 -*-\n\nimport scrapy\n\nfrom scrapy.spiders import XMLFeedSpider\nfrom althingi_scraper.items import PartyItem\n\nclass PartySpider(XMLFeedSpider):\n    name = 'party'\n    allowed_domains = ['http://www.althingi.is']\n    #session = '145'  \n\n    start_urls = [\n        #'http://www.althingi.is/altext/xml/thingflokkar/?lthing=%s' % session,        \n        'http://www.althingi.is/altext/xml/thingflokkar/'\n    ]   \n\n    itertag = '\u00feingflokkar'\n\n    def parse_node(self, response, node):        \n        item = PartyItem()\n        item['party_id'] = node.xpath('@id').extract()\n        item['name'] = node.xpath('heiti').extract()\n        #item['short_abbr'] = node.xpath('stuttskammst\u00f6fun').extract()\n        #item['long_abbr'] = node.xpath('l\u00f6ngskammst\u00f6fun').extract()\n\n        return item\nAny thoughts?", "issue_status": "Closed", "issue_reporting_time": "2016-01-07T22:36:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1058": {"issue_url": "https://github.com/scrapy/scrapy/issues/1658", "issue_id": "#1658", "issue_summary": "i have something wrong when i use Scrapy", "issue_description": "guistory commented on Jan 5, 2016\n-- coding: utf-8 --\nimport scrapy\nfrom scrapy.http import Request\nfrom hbzc.items import HbzcItem\nclass HbzcsSpider(scrapy.Spider):\nname = \"hbzcs\"\ncity = '?citycode=130000000-130700000-130728000&cityname=\u6cb3\u5317\u7701\u5f20\u5bb6\u53e3\u5e02\u6000\u5b89\u53bf'\nallowed_domains = [\"www.ccgp-hebei.gov.cn\"]\nstart_urls = (\n'http://www.ccgp-hebei.gov.cn/zfcg/web/getPreWinAnncList_1.html'+city,#\u9884\u4e2d\u6807\u516c\u544a\u5217\u8868\n)\n\ndef parse(self, response):\n    sel = response.xpath\n    htm = '.html'\n    prehtm = 'http://www.ccgp-hebei.gov.cn/zfcg/preBidingAnncDetail_'        \n    prehtmlist =[prehtm+h+htm for h in [t[1] for t in [i.split(\"'\") for i in sel('//*[@id=\"moreprewinannctable\"]/tr/@onclick').extract()]]]#\u9884\u4e2d\u6807\u516c\u544a\u8be6\u7ec6\n    for preurl in prehtmlist:\n        yield Request(preurl,callback=self.parse_item)\n\ndef parse_item0(self,response):\n    sel = response.xpath\n    htm = '.html'\n    dinghtm = 'http://www.ccgp-hebei.gov.cn/zfcg/1/bidingAnncDetail_'\n    dinghtmlist = [dinghtm+h+htm for h in [t[1] for t in [i.split(\"'\") for i in sel('//*[@id=\"moredingannctable\"]/tr/@onclick').extract()]]]#\u62db\u6807\u516c\u544a\u8be6\u7ec6\n    for dingurl in dinghtmlist:\n        item = response.meta['item']\n        #print dingurl\n        yield Request(url=dingurl,callback=self.parse_item1,meta={'item':item})\n\ndef parse_item(self,response):\n    sel = response.xpath\n    item = HbzcItem()\n    item['fs'] = sel('/html/body/table/tr/td/table/tr[4]/td/table/tr[1]/td/table/tr[1]/td[4]/text()').extract()\n    item['dl'] = sel('/html/body/table/tr/td/table/tr[4]/td/table/tr[1]/td/table/tr[3]/td[6]/text()').extract()\n    item['zb'] = sel('/html/body/table/tr/td/table/tr[4]/td/table/tr[7]/td/span/span[1]/text()').extract()\n    item['mc'] = sel('/html/body/table/tr/td/table/tr[4]/td/table/tr[1]/td/table/tr[2]/td[2]/text()').extract()        \n    return Request('http://www.ccgp-hebei.gov.cn/zfcg/web/getBidingList_1.html?citycode=130000000-130700000-130728000',callback=self.parse_item0,meta={'item':item})\n\n\ndef parse_item1(self,response): \n    sel = response.xpath       \n    item = response.meta['item']\n    item['ys'] = sel('/html/body/table/tr/td/table/tr[4]/td/table/tr[7]/td/span/span[8]/text()').extract()\n    item['kb'] = sel('//*[@id=\"bidopentime2\"]/text()').extract()\n    print item\n    return item", "issue_status": "Closed", "issue_reporting_time": "2016-01-05T10:15:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1059": {"issue_url": "https://github.com/scrapy/scrapy/issues/1653", "issue_id": "#1653", "issue_summary": "Segfault on a fresh scrapy installation", "issue_description": "DenJohX commented on Dec 30, 2015\nI'm on a Debian Testing machine, using a fresh virtualenv and a fresh install of scrapy it seems to install without errors, but it cannot be started just ends in a segmentation fault.\n$ pip install scrapy\n\nDownloading/unpacking scrapy\n  Downloading Scrapy-1.0.3-py2-none-any.whl (290kB): 290kB downloaded\nDownloading/unpacking cssselect>=0.9 (from scrapy)\n  Downloading cssselect-0.9.1.tar.gz\n  Running setup.py (path:/tmp/pip-build-wx54VN/cssselect/setup.py) egg_info for package cssselect\n\n    no previously-included directories found matching 'docs/_build'\nDownloading/unpacking queuelib (from scrapy)\n  Downloading queuelib-1.4.2-py2.py3-none-any.whl\nDownloading/unpacking pyOpenSSL (from scrapy)\n  Downloading pyOpenSSL-0.15.1-py2.py3-none-any.whl (102kB): 102kB downloaded\nDownloading/unpacking w3lib>=1.8.0 (from scrapy)\n  Downloading w3lib-1.13.0-py2.py3-none-any.whl\nDownloading/unpacking lxml (from scrapy)\n  Downloading lxml-3.5.0.tar.gz (3.8MB): 3.8MB downloaded\n  Running setup.py (path:/tmp/pip-build-wx54VN/lxml/setup.py) egg_info for package lxml\n    Building lxml version 3.5.0.\n    Building without Cython.\n    Using build configuration of libxslt 1.1.28\n\n    warning: no previously-included files found matching '*.py'\nDownloading/unpacking Twisted>=10.0.0 (from scrapy)\n  Downloading Twisted-15.5.0.tar.bz2 (3.1MB): 3.1MB downloaded\n  Running setup.py (path:/tmp/pip-build-wx54VN/Twisted/setup.py) egg_info for package Twisted\n\nDownloading/unpacking six>=1.5.2 (from scrapy)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nDownloading/unpacking service-identity (from scrapy)\n  Downloading service_identity-14.0.0-py2.py3-none-any.whl\nDownloading/unpacking cryptography>=0.7 (from pyOpenSSL->scrapy)\n  Downloading cryptography-1.1.2.tar.gz (349kB): 349kB downloaded\n  Running setup.py (path:/tmp/pip-build-wx54VN/cryptography/setup.py) egg_info for package cryptography\n\n    no previously-included directories found matching 'docs/_build'\n    warning: no previously-included files matching '*' found under directory 'vectors'\nDownloading/unpacking zope.interface>=3.6.0 (from Twisted>=10.0.0->scrapy)\n  Downloading zope.interface-4.1.3.tar.gz (141kB): 141kB downloaded\n  Running setup.py (path:/tmp/pip-build-wx54VN/zope.interface/setup.py) egg_info for package zope.interface\n\n    warning: no previously-included files matching '*.dll' found anywhere in distribution\n    warning: no previously-included files matching '*.pyc' found anywhere in distribution\n    warning: no previously-included files matching '*.pyo' found anywhere in distribution\n    warning: no previously-included files matching '*.so' found anywhere in distribution\n    warning: no previously-included files matching 'coverage.xml' found anywhere in distribution\n    no previously-included directories found matching 'docs/_build'\nDownloading/unpacking characteristic>=14.0.0 (from service-identity->scrapy)\n  Downloading characteristic-14.3.0-py2.py3-none-any.whl\nDownloading/unpacking pyasn1-modules (from service-identity->scrapy)\n  Downloading pyasn1_modules-0.0.8-py2.py3-none-any.whl\nDownloading/unpacking pyasn1 (from service-identity->scrapy)\n  Downloading pyasn1-0.1.9-py2.py3-none-any.whl\nDownloading/unpacking idna>=2.0 (from cryptography>=0.7->pyOpenSSL->scrapy)\n  Downloading idna-2.0-py2.py3-none-any.whl (61kB): 61kB downloaded\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /home/dennis/.virtualenvs/scraper/lib/python2.7/site-packages (from cryptography>=0.7->pyOpenSSL->scrapy)\nDownloading/unpacking enum34 (from cryptography>=0.7->pyOpenSSL->scrapy)\n  Downloading enum34-1.1.2.tar.gz (46kB): 46kB downloaded\n  Running setup.py (path:/tmp/pip-build-wx54VN/enum34/setup.py) egg_info for package enum34\n\nDownloading/unpacking ipaddress (from cryptography>=0.7->pyOpenSSL->scrapy)\n  Downloading ipaddress-1.0.16-py27-none-any.whl\nDownloading/unpacking cffi>=1.1.0 (from cryptography>=0.7->pyOpenSSL->scrapy)\n  Downloading cffi-1.4.2.tar.gz (365kB): 365kB downloaded\n  Running setup.py (path:/tmp/pip-build-wx54VN/cffi/setup.py) egg_info for package cffi\n\nDownloading/unpacking pycparser (from cffi>=1.1.0->cryptography>=0.7->pyOpenSSL->scrapy)\n  Downloading pycparser-2.14.tar.gz (223kB): 223kB downloaded\n  Running setup.py (path:/tmp/pip-build-wx54VN/pycparser/setup.py) egg_info for package pycparser\n\n    warning: no previously-included files matching 'yacctab.*' found under directory 'tests'\n    warning: no previously-included files matching 'lextab.*' found under directory 'tests'\n    warning: no previously-included files matching 'yacctab.*' found under directory 'examples'\n    warning: no previously-included files matching 'lextab.*' found under directory 'examples'\nInstalling collected packages: scrapy, cssselect, queuelib, pyOpenSSL, w3lib, lxml, Twisted, six, service-identity, cryptography, zope.interface, characteristic, pyasn1-modules, pyasn1, idna, enum34, ipaddress, cffi, pycparser\n  Running setup.py install for cssselect\n\n    no previously-included directories found matching 'docs/_build'\n  Running setup.py install for lxml\n    Building lxml version 3.5.0.\n    Building without Cython.\n    Using build configuration of libxslt 1.1.28\n    building 'lxml.etree' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/libxml2 -Isrc/lxml/includes -I/usr/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.linux-x86_64-2.7/src/lxml/lxml.etree.o -w\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/src/lxml/lxml.etree.o -lxslt -lexslt -lxml2 -lz -lm -o build/lib.linux-x86_64-2.7/lxml/etree.so\n    building 'lxml.objectify' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/libxml2 -Isrc/lxml/includes -I/usr/include/python2.7 -c src/lxml/lxml.objectify.c -o build/temp.linux-x86_64-2.7/src/lxml/lxml.objectify.o -w\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/src/lxml/lxml.objectify.o -lxslt -lexslt -lxml2 -lz -lm -o build/lib.linux-x86_64-2.7/lxml/objectify.so\n\n  Running setup.py install for Twisted\n\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c conftest.c -o conftest.o\n    building 'twisted.test.raiser' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c twisted/test/raiser.c -o build/temp.linux-x86_64-2.7/twisted/test/raiser.o\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/twisted/test/raiser.o -o build/lib.linux-x86_64-2.7/twisted/test/raiser.so\n    building 'twisted.python._sendmsg' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c twisted/python/_sendmsg.c -o build/temp.linux-x86_64-2.7/twisted/python/_sendmsg.o\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/twisted/python/_sendmsg.o -o build/lib.linux-x86_64-2.7/twisted/python/_sendmsg.so\n    building 'twisted.runner.portmap' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c twisted/runner/portmap.c -o build/temp.linux-x86_64-2.7/twisted/runner/portmap.o\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/twisted/runner/portmap.o -o build/lib.linux-x86_64-2.7/twisted/runner/portmap.so\n    changing mode of build/scripts-2.7/mailmail from 644 to 755\n    changing mode of build/scripts-2.7/twistd from 644 to 755\n    changing mode of build/scripts-2.7/manhole from 644 to 755\n    changing mode of build/scripts-2.7/tap2rpm from 644 to 755\n    changing mode of build/scripts-2.7/ckeygen from 644 to 755\n    changing mode of build/scripts-2.7/trial from 644 to 755\n    changing mode of build/scripts-2.7/tap2deb from 644 to 755\n    changing mode of build/scripts-2.7/pyhtmlizer from 644 to 755\n    changing mode of build/scripts-2.7/tkconch from 644 to 755\n    changing mode of build/scripts-2.7/conch from 644 to 755\n    changing mode of build/scripts-2.7/cftp from 644 to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/mailmail to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/twistd to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/manhole to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/tap2rpm to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/ckeygen to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/trial to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/tap2deb to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/pyhtmlizer to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/tkconch to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/conch to 755\n    changing mode of /home/dennis/.virtualenvs/scraper/bin/cftp to 755\n  Running setup.py install for cryptography\n    c/_cffi_backend.c: In function \u2018allocate_with_allocator\u2019:\n    c/_cffi_backend.c:3025:26: warning: initialization from incompatible pointer type [-Wincompatible-pointer-types]\n         PyObject *my_alloc = allocator[0];\n                              ^\n    c/_cffi_backend.c:3026:25: warning: initialization from incompatible pointer type [-Wincompatible-pointer-types]\n         PyObject *my_free = allocator[1];\n                             ^\n    c/_cffi_backend.c:3027:40: warning: initialization from incompatible pointer type [-Wincompatible-pointer-types]\n         PyObject *dont_clear_after_alloc = allocator[2];\n                                            ^\n    c/_cffi_backend.c: In function \u2018b_newp\u2019:\n    c/_cffi_backend.c:3175:34: warning: passing argument 3 of \u2018direct_newp\u2019 from incompatible pointer type [-Wincompatible-pointer-types]\n         return direct_newp(ct, init, default_allocator);\n                                      ^\n    c/_cffi_backend.c:3069:18: note: expected \u2018PyObject * const (*)[3] {aka struct _object * const (*)[3]}\u2019 but argument is of type \u2018PyObject * const* {aka struct _object * const*}\u2019\n     static PyObject *direct_newp(CTypeDescrObject *ct, PyObject *init,\n                      ^\n    In file included from c/cffi1_module.c:14:0,\n                     from c/_cffi_backend.c:6413:\n    c/ffi_obj.c: In function \u2018ffi_new\u2019:\n    c/ffi_obj.c:356:39: warning: passing argument 4 of \u2018_ffi_new\u2019 from incompatible pointer type [-Wincompatible-pointer-types]\n         return _ffi_new(self, args, kwds, default_allocator);\n                                           ^\n    c/ffi_obj.c:337:18: note: expected \u2018PyObject * const (*)[3] {aka struct _object * const (*)[3]}\u2019 but argument is of type \u2018PyObject * const* {aka struct _object * const*}\u2019\n     static PyObject *_ffi_new(FFIObject *self, PyObject *args, PyObject *kwds,\n                      ^\n    c/ffi_obj.c: In function \u2018_ffi_new_with_allocator\u2019:\n    c/ffi_obj.c:364:21: warning: passing argument 4 of \u2018_ffi_new\u2019 from incompatible pointer type [-Wincompatible-pointer-types]\n                         &PyTuple_GET_ITEM(allocator, 1));\n                         ^\n    c/ffi_obj.c:337:18: note: expected \u2018PyObject * const (*)[3] {aka struct _object * const (*)[3]}\u2019 but argument is of type \u2018PyObject ** {aka struct _object **}\u2019\n     static PyObject *_ffi_new(FFIObject *self, PyObject *args, PyObject *kwds,\n                      ^\n\n    Installed /tmp/pip-build-wx54VN/cryptography/.eggs/cffi-1.4.2-py2.7-linux-x86_64.egg\n    Searching for pycparser\n    Reading https://pypi.python.org/simple/pycparser/\n    Best match: pycparser 2.14\n    Downloading https://pypi.python.org/packages/source/p/pycparser/pycparser-2.14.tar.gz#md5=a2bc8d28c923b4fe2b2c3b4b51a4f935\n    Processing pycparser-2.14.tar.gz\n    Writing /tmp/easy_install-oVdWII/pycparser-2.14/setup.cfg\n    Running pycparser-2.14/setup.py -q bdist_egg --dist-dir /tmp/easy_install-oVdWII/pycparser-2.14/egg-dist-tmp-AhItwA\n    warning: no previously-included files matching 'yacctab.*' found under directory 'tests'\n    warning: no previously-included files matching 'lextab.*' found under directory 'tests'\n    warning: no previously-included files matching 'yacctab.*' found under directory 'examples'\n    warning: no previously-included files matching 'lextab.*' found under directory 'examples'\n    zip_safe flag not set; analyzing archive contents...\n    Moving pycparser-2.14-py2.7.egg to /tmp/pip-build-wx54VN/cryptography/.eggs\n\n    Installed /tmp/pip-build-wx54VN/cryptography/.eggs/pycparser-2.14-py2.7.egg\n\n    no previously-included directories found matching 'docs/_build'\n    warning: no previously-included files matching '*' found under directory 'vectors'\n    generating cffi module 'build/temp.linux-x86_64-2.7/_padding.c'\n    generating cffi module 'build/temp.linux-x86_64-2.7/_constant_time.c'\n    generating cffi module 'build/temp.linux-x86_64-2.7/_openssl.c'\n    building '_openssl' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_openssl.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_openssl.o\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_openssl.o -lssl -lcrypto -o build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/_openssl.so\n    building '_constant_time' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_constant_time.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_constant_time.o\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_constant_time.o -o build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/_constant_time.so\n    building '_padding' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_padding.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_padding.o\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_padding.o -o build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/_padding.so\n  Running setup.py install for zope.interface\n\n    warning: no previously-included files matching '*.dll' found anywhere in distribution\n    warning: no previously-included files matching '*.pyc' found anywhere in distribution\n    warning: no previously-included files matching '*.pyo' found anywhere in distribution\n    warning: no previously-included files matching '*.so' found anywhere in distribution\n    warning: no previously-included files matching 'coverage.xml' found anywhere in distribution\n    no previously-included directories found matching 'docs/_build'\n    building 'zope.interface._zope_interface_coptimizations' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c src/zope/interface/_zope_interface_coptimizations.c -o build/temp.linux-x86_64-2.7/src/zope/interface/_zope_interface_coptimizations.o\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/src/zope/interface/_zope_interface_coptimizations.o -o build/lib.linux-x86_64-2.7/zope/interface/_zope_interface_coptimizations.so\n    Skipping installation of /home/dennis/.virtualenvs/scraper/lib/python2.7/site-packages/zope/__init__.py (namespace package)\n    Installing /home/dennis/.virtualenvs/scraper/lib/python2.7/site-packages/zope.interface-4.1.3-py2.7-nspkg.pth\n  Running setup.py install for enum34\n\n  Running setup.py install for cffi\n    building '_cffi_backend' extension\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -DUSE__THREAD -I/usr/include/python2.7 -c c/_cffi_backend.c -o build/temp.linux-x86_64-2.7/c/_cffi_backend.o\n    c/_cffi_backend.c: In function \u2018allocate_with_allocator\u2019:\n    c/_cffi_backend.c:3025:26: warning: initialization from incompatible pointer type [-Wincompatible-pointer-types]\n         PyObject *my_alloc = allocator[0];\n                              ^\n    c/_cffi_backend.c:3026:25: warning: initialization from incompatible pointer type [-Wincompatible-pointer-types]\n         PyObject *my_free = allocator[1];\n                             ^\n    c/_cffi_backend.c:3027:40: warning: initialization from incompatible pointer type [-Wincompatible-pointer-types]\n         PyObject *dont_clear_after_alloc = allocator[2];\n                                            ^\n    c/_cffi_backend.c: In function \u2018b_newp\u2019:\n    c/_cffi_backend.c:3175:34: warning: passing argument 3 of \u2018direct_newp\u2019 from incompatible pointer type [-Wincompatible-pointer-types]\n         return direct_newp(ct, init, default_allocator);\n                                      ^\n    c/_cffi_backend.c:3069:18: note: expected \u2018PyObject * const (*)[3] {aka struct _object * const (*)[3]}\u2019 but argument is of type \u2018PyObject * const* {aka struct _object * const*}\u2019\n     static PyObject *direct_newp(CTypeDescrObject *ct, PyObject *init,\n                      ^\n    In file included from c/cffi1_module.c:14:0,\n                     from c/_cffi_backend.c:6413:\n    c/ffi_obj.c: In function \u2018ffi_new\u2019:\n    c/ffi_obj.c:356:39: warning: passing argument 4 of \u2018_ffi_new\u2019 from incompatible pointer type [-Wincompatible-pointer-types]\n         return _ffi_new(self, args, kwds, default_allocator);\n                                           ^\n    c/ffi_obj.c:337:18: note: expected \u2018PyObject * const (*)[3] {aka struct _object * const (*)[3]}\u2019 but argument is of type \u2018PyObject * const* {aka struct _object * const*}\u2019\n     static PyObject *_ffi_new(FFIObject *self, PyObject *args, PyObject *kwds,\n                      ^\n    c/ffi_obj.c: In function \u2018_ffi_new_with_allocator\u2019:\n    c/ffi_obj.c:364:21: warning: passing argument 4 of \u2018_ffi_new\u2019 from incompatible pointer type [-Wincompatible-pointer-types]\n                         &PyTuple_GET_ITEM(allocator, 1));\n                         ^\n    c/ffi_obj.c:337:18: note: expected \u2018PyObject * const (*)[3] {aka struct _object * const (*)[3]}\u2019 but argument is of type \u2018PyObject ** {aka struct _object **}\u2019\n     static PyObject *_ffi_new(FFIObject *self, PyObject *args, PyObject *kwds,\n                      ^\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protetor-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/c/_cffi_backend.o -lffi -o build/lib.linux-x86_64-2.7/_cffi_backend.so\n\n  Running setup.py install for pycparser\n\n    warning: no previously-included files matching 'yacctab.*' found under directory 'tests'\n    warning: no previously-included files matching 'lextab.*' found under directory 'tests'\n    warning: no previously-included files matching 'yacctab.*' found under directory 'examples'\n    warning: no previously-included files matching 'lextab.*' found under directory 'examples'\n    Build the lexing/parsing tables\nSuccessfully installed scrapy cssselect queuelib pyOpenSSL w3lib lxml Twisted six service-identity cryptography zope.interface characteristic pyasn1-modules pyasn1 idna enum34 ipaddress cffi pycparser\nCleaning up...\nAside from the lots of warnings, it seems to compile ok with no errors, but it cannot be started.\n$ scrapy\n[1]    13574 segmentation fault  scrapy\nin a python console:\nPython 2.7.11 (default, Dec  9 2015, 00:29:25) \n[GCC 5.3.1 20151205] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import scrapy\n[1]    14681 segmentation fault  python\nBy the warnings the problem seems to be with ffi and cryptography packages but I'm not sure as all of them installs just fine.\nAny ideas on how to solve this?\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2015-12-29T22:50:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1060": {"issue_url": "https://github.com/scrapy/scrapy/issues/1650", "issue_id": "#1650", "issue_summary": "response.xpath may not work in scrapy shell", "issue_description": "yuiopt commented on Dec 29, 2015\nscrapy shell http://huaban.com/favorite/photography/\ntested xpath :\nresponse.xpath('//div[@id=\"page\"]/div[@Class=\"wrapper\"]')\nbut got an empty list", "issue_status": "Closed", "issue_reporting_time": "2015-12-29T03:36:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1061": {"issue_url": "https://github.com/scrapy/scrapy/issues/1648", "issue_id": "#1648", "issue_summary": "parcing fail", "issue_description": "Anlight commented on Dec 28, 2015\nIt does not collect data from the title\ntoster.py:\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors import LinkExtractor\nfrom toster.items import DjangoItem\nclass DjangoSpider(CrawlSpider):\nname = \"django\"\nallowed_domains = [\"www.toster.ru\"]\nstart_urls = [\n'http://www.toster.ru/tag/django/questions',\n]\nrules = [\n    Rule(LinkExtractor(\n        allow=['/tag/django/questions\\?page=\\d']),\n        callback='parse_item',\n        follow=True)\n]\n\n\ndef parse_item(self, response):\n\n    selector_list = response.css('div.thing')\n\n    for selector in selector_list:\n        item = DjangoItem()\n        item['title'] = selector.xpath('div/h2/a/text()').extract()\n        # item['url'] = selector.xpath('a/@href').extract()\n\n        yield item\nitem:\nfrom scrapy import Field, Item\nclass DjangoItem(Item):\n# define the fields for your item here like:\n# name = scrapy.Field()\ntitle = Field()\n# url = Field()", "issue_status": "Closed", "issue_reporting_time": "2015-12-28T05:19:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1062": {"issue_url": "https://github.com/scrapy/scrapy/issues/1647", "issue_id": "#1647", "issue_summary": "Scrapy 1.0: How to run crawler in Celery?", "issue_description": "imWildCat commented on Dec 21, 2015\nI've posted a question on StackOverFlow but there is no answer:\nhttp://stackoverflow.com/questions/34065315/scrapy-1-0-how-to-run-crawler-in-celery\nI tried the sample from document http://doc.scrapy.org/en/stable/topics/practices.html, but there would be an error ReactorNotRestartable while running it a second time.\nsettings = get_project_settings()\n\nrunner = CrawlerRunner(settings=settings)\n\n@defer.inlineCallbacks\ndef crawl():\n    yield runner.crawl(LatestNewsSpider)\n    reactor.stop()\n\ndef run_spider():\n    crawl()\n    reactor.run()", "issue_status": "Closed", "issue_reporting_time": "2015-12-21T12:17:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1063": {"issue_url": "https://github.com/scrapy/scrapy/issues/1645", "issue_id": "#1645", "issue_summary": "COC?", "issue_description": "alison985 commented on Dec 20, 2015\nHi,\nI'm just wondering what your code of conduct is for this project?\nThanks,\nAlison", "issue_status": "Closed", "issue_reporting_time": "2015-12-20T10:11:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1064": {"issue_url": "https://github.com/scrapy/scrapy/issues/1636", "issue_id": "#1636", "issue_summary": "Twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion:", "issue_description": "billyfung commented on Dec 11, 2015\nRunning scrapy shell https://modelmayhem in OS X with scrapy 1.0.3 and python 2.7\n2015-12-10 17:47:57 [scrapy] INFO: Spider opened\n2015-12-10 17:47:58 [scrapy] DEBUG: Retrying <GET https://www.modelmayhem.com/> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2015-12-10 17:47:58 [scrapy] DEBUG: Retrying <GET https://www.modelmayhem.com/> (failed 2 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\n2015-12-10 17:47:59 [scrapy] DEBUG: Gave up retrying <GET https://www.modelmayhem.com/> (failed 3 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nTraceback (most recent call last):\n  File \"/Users/billyfung/anaconda/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/Users/billyfung/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/billyfung/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/billyfung/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/billyfung/anaconda/lib/python2.7/site-packages/scrapy/commands/shell.py\", line 63, in run\n    shell.start(url=url)\n  File \"/Users/billyfung/anaconda/lib/python2.7/site-packages/scrapy/shell.py\", line 44, in start\n    self.fetch(url, spider)\n  File \"/Users/billyfung/anaconda/lib/python2.7/site-packages/scrapy/shell.py\", line 87, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/Users/billyfung/anaconda/lib/python2.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<string>\", line 2, in raiseException\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\nNot sure what this is except that when I load the site in my browser it's pretty slow to load up as well.", "issue_status": "Closed", "issue_reporting_time": "2015-12-11T01:50:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1065": {"issue_url": "https://github.com/scrapy/scrapy/issues/1635", "issue_id": "#1635", "issue_summary": "Test for startproject command fails in OS X", "issue_description": "Contributor\nrmax commented on Dec 11, 2015\nApparently, os.mknod is a privileged call in OS X:\n$ python -c 'import os; os.mknod(\"/tmp/test\")'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nOSError: [Errno 1] Operation not permitted\nOutput of the test failure:\n$ nosetests tests/test_commands.py\n...................E..\n======================================================================\nERROR: test_startproject_template_override\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\n    result = f(*args, **kw)\n  File \"lib/python2.7/site-packages/twisted/internet/utils.py\", line 201, in runWithWarningsSuppressed\n    reraise(exc_info[1], exc_info[2])\n  File \"python2.7/site-packages/twisted/internet/utils.py\", line 197, in runWithWarningsSuppressed\n    result = f(*a, **kw)\n  File \"scrapy/tests/test_commands.py\", line 86, in test_startproject_template_override\n    os.mknod(join(self.tmpl_proj, 'root_template'))\nOSError: [Errno 1] Operation not permitted", "issue_status": "Closed", "issue_reporting_time": "2015-12-10T22:56:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1066": {"issue_url": "https://github.com/scrapy/scrapy/issues/1634", "issue_id": "#1634", "issue_summary": "_monkeypatches.py: 'NoneType' object has no attribute 'startswith'", "issue_description": "Contributor\nyarikoptic commented on Dec 11, 2015\nDid not try yet to come up with minimal example to demonstrate this issue but it is reproducible for me:\n$> datalad --dbg crawl\nTraceback (most recent call last):\n  File \"/home/yoh/proj/datalad/datalad/venv-tests/bin/datalad\", line 9, in <module>\n    load_entry_point('datalad==0.1.dev0', 'console_scripts', 'datalad')()\n  File \"/home/yoh/proj/datalad/datalad/datalad/cmdline/main.py\", line 199, in main\n    cmdlineargs.func(cmdlineargs)\n  File \"/home/yoh/proj/datalad/datalad/datalad/interface/base.py\", line 151, in call_from_parser\n    return self(**kwargs)\n  File \"/home/yoh/proj/datalad/datalad/datalad/interface/crawl.py\", line 44, in __call__\n    from datalad.crawler.pipeline import load_pipeline_from_config, get_pipeline_config_path\n  File \"/home/yoh/proj/datalad/datalad/datalad/crawler/pipeline.py\", line 21, in <module>\n    from .newmain import lgr\n  File \"/home/yoh/proj/datalad/datalad/datalad/crawler/newmain.py\", line 21, in <module>\n    from .nodes.matches import *\n  File \"/home/yoh/proj/datalad/datalad/datalad/crawler/nodes/matches.py\", line 18, in <module>\n    from scrapy.selector import Selector\n  File \"/home/yoh/proj/datalad/datalad/venv-tests/local/lib/python2.7/site-packages/scrapy/__init__.py\", line 27, in <module>\n    from . import _monkeypatches\n  File \"/home/yoh/proj/datalad/datalad/venv-tests/local/lib/python2.7/site-packages/scrapy/_monkeypatches.py\", line 24, in <module>\n    and getattr(v, '__module__', '').startswith('twisted'):\nAttributeError: 'NoneType' object has no attribute 'startswith'\n()\n> /home/yoh/proj/datalad/datalad/venv-tests/local/lib/python2.7/site-packages/scrapy/_monkeypatches.py(24)<module>()\n-> and getattr(v, '__module__', '').startswith('twisted'):\n(Pdb) l\n 19     # to prevent bugs like Twisted#7989 while serializing requests\n 20     import twisted.persisted.styles  # NOQA\n 21     # Remove only entries with twisted serializers for non-twisted types.\n 22     for k, v in frozenset(copyreg.dispatch_table.items()):\n 23         if not getattr(k, '__module__', '').startswith('twisted') \\\n 24  ->             and getattr(v, '__module__', '').startswith('twisted'):\n 25             copyreg.dispatch_table.pop(k)\n[EOF]\n(Pdb) p k\nNone\n(Pdb) p v\nNone\n(Pdb) p copyreg\nNone\nnot sure it came to it but the issue is (if I pdb before this madness happens):\n(Pdb) p getattr(k, '__module__', '')\n'__builtin__'\n(Pdb) p getattr(v, '__module__', '')\nNone\n(Pdb) p v\n<function mpq_reducer at 0x7f474bb4ab90>\n(Pdb) p v.__module__\nNone\n(Pdb) p k, v\n(<type 'mpq'>, <function mpq_reducer at 0x7f474bb4ab90>)\nso assigned __module__ is None. As a quick resolution wrapped into str() call to assure str there\nand str(getattr(v, '__module__', '')).startswith('twisted'):", "issue_status": "Closed", "issue_reporting_time": "2015-12-10T21:11:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1067": {"issue_url": "https://github.com/scrapy/scrapy/issues/1633", "issue_id": "#1633", "issue_summary": "Scrapy allows to override spider attributes and methods from command line", "issue_description": "Contributor\npawelmhm commented on Dec 10, 2015\nthis is partly bug, partly feature but you can do something like this:\nscrapy crawl dmoz -a start_requests=\"this\"\nand it will effectively overwrite spider start_requests with string \"this\" so that you will get\n2015-12-10 14:19:05 [twisted] CRITICAL: Unhandled error in Deferred:\n\n\nTraceback (most recent call last):\n  File \"/home/pawel//src/scrapy/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/home/pawel/scrapy/scrapy/commands/crawl.py\", line 57, in run\n    self.crawler_process.crawl(spname, **opts.spargs)\n  File \"/home/pawelsrc/scrapy/scrapy/crawler.py\", line 153, in crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1274, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks\n    result = g.send(result)\n  File \"/home/pawel/src/scrapy/scrapy/crawler.py\", line 72, in crawl\n    start_requests = iter(self.spider.start_requests())\nexceptions.TypeError: 'str' object is not callable\n2015-12-10 14:19:05 [twisted] CRITICAL: \nI discovered this when client asked to add option to pass download_delay when scheduling. Turns out you can do this without any updates to spider, but the type will be invalid.\nI think it could be dangerous though, it allows lots of control over spider settings and can cause mysterious bugs, imagine you have some spider attribute that is string and someone passes this attribute from command line too. It would be unexpected for most users that command line argument can overwrite object attribute. The line responsible for this is here: https://github.com/scrapy/scrapy/blob/master/scrapy/spiders/__init__.py#L30", "issue_status": "Closed", "issue_reporting_time": "2015-12-10T13:21:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1068": {"issue_url": "https://github.com/scrapy/scrapy/issues/1632", "issue_id": "#1632", "issue_summary": "User Warning: cookie lib bug!", "issue_description": "gmeans commented on Dec 10, 2015\nI've had a spider running pretty consistently for over a year now and just recently had the following error pop up:\nUserWarning: cookielib bug!\n    Traceback (most recent call last):\n      File \"/usr/lib/python2.7/cookielib.py\", line 1582, in make_cookies\n        parse_ns_headers(ns_hdrs), request)\n      File \"/usr/lib/python2.7/cookielib.py\", line 485, in parse_ns_headers\n        v = http2time(_strip_quotes(v))  # None if invalid\n      File \"/usr/lib/python2.7/cookielib.py\", line 266, in http2time\n        return _str2time(day, mon, yr, hr, min, sec, tz)\n      File \"/usr/lib/python2.7/cookielib.py\", line 176, in _str2time\n        t = _timegm((yr, mon, day, hr, min, sec, tz))\n      File \"/usr/lib/python2.7/cookielib.py\", line 76, in _timegm\n        return timegm(tt)\n      File \"/usr/lib/python2.7/calendar.py\", line 613, in timegm\n        days = datetime.date(year, month, 1).toordinal() - _EPOCH_ORD + day - 1\n    ValueError: year is out of range\n     [cookielib.py:1584]\nThe problem is that this error crashes the spider process completely. The spider is running within scrapyd.\nHas anyone seen this before? I'm running Scrapy 0.24.4 on this instance due to an SSL issue that prevented me from upgrading.", "issue_status": "Closed", "issue_reporting_time": "2015-12-09T19:47:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1069": {"issue_url": "https://github.com/scrapy/scrapy/issues/1630", "issue_id": "#1630", "issue_summary": "Cannot download files from website", "issue_description": "raspatan commented on Dec 9, 2015\nI'm starting with Scrapy in order to automatise file downloading from websites. As a test, I want to download the jpg files from this website. Perhaps I'm too noob for this, or perhaps the Wiki regarding downloading files is not very clear on the issue. In any case, here is the problem:\nMy code is this:\nIn settings.py, I have added these lines:\nITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}\n\nIMAGES_STORE = '/home/lucho/Scrapy/jpg/'\nMy items.py file is:\nimport scrapy\n\nclass JpgItem(scrapy.Item):\n    image_urls = scrapy.Field()\n    images = scrapy.Field()\n    pass\nMy pipeline file is:\nimport scrapy\nfrom scrapy.pipelines.images import ImagesPipeline\nfrom scrapy.exceptions import DropItem\n\nclass JpgPipeline(object):\n#    def process_item(self, item, spider):\n#        return item\n    def get_media_requests(self, item, info):\n        for image_url in item['image_urls']:\n            yield scrapy.Request(image_url)\n\n    def item_completed(self, results, item, info):\n        image_paths = [x['path'] for ok, x in results if ok]\n        if not image_paths:\n            raise DropItem(\"Item contains no images\")\n        item['image_paths'] = image_paths\n        return item\nFinally, my spider file is:\nimport scrapy\n\nclass JpgSpider(scrapy.Spider):\n    name = \"jpg\"\n    allowed_domains = [\"http://www.kevinsmedia.com\"]\n    start_urls = [\n        \"http://www.kevinsmedia.com/km/mp3z/Fluke/Risotto/\"\n    ]\n\n    def parse(self, response):\n        yield JpgItem(\n            file_urls=[\n                'http://www.kevinsmedia.com/km/mp3z/Fluke/Risotto/AlbumArtSmall.jpg',\n            ]\n        )\n(I added a def parse() to my spider, but this is restrictive, since I don't want to specify every file I need but to detect a class of files (.jpg) and download them all)\nThe output of \"scrapy crawl jpg\" is:\n2015-12-07 10:29:39 [scrapy] INFO: Scrapy 1.0.3.post6+g2d688cd started (bot: jpg)\n2015-12-07 10:29:39 [scrapy] INFO: Optional features available: ssl, http11\n2015-12-07 10:29:39 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'jpg.spiders', 'SPIDER_MODULES': ['jpg.spiders'], 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'BOT_NAME': 'jpg'}\n2015-12-07 10:29:39 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n2015-12-07 10:29:40 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-12-07 10:29:40 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-12-07 10:29:40 [scrapy] INFO: Enabled item pipelines: ImagesPipeline\n2015-12-07 10:29:40 [scrapy] INFO: Spider opened\n2015-12-07 10:29:40 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-12-07 10:29:40 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-12-07 10:29:40 [scrapy] DEBUG: Crawled (200) <GET http://www.kevinsmedia.com/km/mp3z/Fluke/Risotto/> (referer: None)\n2015-12-07 10:29:40 [scrapy] ERROR: Spider error processing <GET http://www.kevinsmedia.com/km/mp3z/Fluke/Risotto/> (referer: None)\nTraceback (most recent call last):\n  File \"/usr/lib/pymodules/python2.7/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/usr/lib/pymodules/python2.7/scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"/usr/lib/pymodules/python2.7/scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/usr/lib/pymodules/python2.7/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/usr/lib/pymodules/python2.7/scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/home/lucho/Documents/Academia/Research/Database Chile/Code/Scrapy/jpg/jpg/spiders/jpg.py\", line 12, in parse\n    yield JpgItem(\nNameError: global name 'JpgItem' is not defined\n2015-12-07 10:29:41 [scrapy] INFO: Closing spider (finished)\n2015-12-07 10:29:41 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 254,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 2975,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2015, 12, 7, 13, 29, 41, 1049),\n 'log_count/DEBUG': 2,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'spider_exceptions/NameError': 1,\n 'start_time': datetime.datetime(2015, 12, 7, 13, 29, 40, 106049)}\n2015-12-07 10:29:41 [scrapy] INFO: Spider closed (finished)\nSo, there is an error, as indicated there, but google is not helping me on this. In case it matters, I'm using Ubuntu.\nPS: I have also posted this on SE, here.", "issue_status": "Closed", "issue_reporting_time": "2015-12-08T19:15:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1070": {"issue_url": "https://github.com/scrapy/scrapy/issues/1625", "issue_id": "#1625", "issue_summary": "Test failure: IStreamClientEndpointStringParser removed from twisted 15.5", "issue_description": "Contributor\npalego commented on Dec 4, 2015\nAs indicated at https://twistedmatrix.com/trac/browser/trunk/NEWS, a Twisted interface that had been deprecated was removed from the latest version:\nv14.0.0\ntwisted.internet.interfaces.IStreamClientEndpointStringParser is now deprecated in favor of twisted.internet.interfaces.IStreamClientEndpointStringParserWithReactor. (#5069)\nv15.5.0\ntwisted.internet.interfaces.IStreamClientEndpointStringParser has been removed and Twisted will no longer use parsers implementing this interface. (#8094)\nThis causes the py27 series of Tox tests to fail in the following fashion:\n========================================================================== ERRORS ===========================================================================\n_________________________________________________________ ERROR collecting scrapy/xlib/tx/client.py _________________________________________________________\nscrapy/xlib/tx/client.py:37: in <module>\n    from .endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint\nscrapy/xlib/tx/endpoints.py:27: in <module>\n    from twisted.internet.interfaces import IStreamClientEndpointStringParser\nE   ImportError: cannot import name IStreamClientEndpointStringParser\n_______________________________________________________ ERROR collecting scrapy/xlib/tx/endpoints.py ________________________________________________________\nscrapy/xlib/tx/endpoints.py:27: in <module>\n    from twisted.internet.interfaces import IStreamClientEndpointStringParser\nE   ImportError: cannot import name IStreamClientEndpointStringParser\n=============================================== 1279 passed, 2 skipped, 13 xfailed, 2 error in 212.51 seconds ===============================================\nERROR: InvocationError: '/home/polo/scrapy/.tox/py27/bin/py.test --cov=scrapy --cov-report= scrapy tests'\n__________________________________________________________________________ summary __________________________________________________________________________\nERROR:   py27: commands failed```\n\nWith the newer interface IStreamClientEndpointStringParserWithReactor, the method  parseStreamClient now takes the reactor as an additional parameter (http://twistedmatrix.com/documents/current)./api/twisted.internet.interfaces.IStreamClientEndpointStringParserWithReactor.html).", "issue_status": "Closed", "issue_reporting_time": "2015-12-04T01:34:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1071": {"issue_url": "https://github.com/scrapy/scrapy/issues/1621", "issue_id": "#1621", "issue_summary": "Issue running 'tox'", "issue_description": "Duji commented on Dec 2, 2015\nI'm trying to run scrapy's test suite but to no luck.\nError for log:\nactionid: py27\nmsg: getenv\ncmdargs: [local('/home/duji/473/scrapy-1.0/.tox/py27/bin/pip'), 'install', '-rrequirements.txt', 'boto', 'Pillow', 'leveldb', '-rtests/requirements.txt']\nenv: {'QT_QPA_PLATFORMTHEME': 'appmenu-qt5', 'XDG_GREETER_DATA_DIR': '/var/lib/lightdm-data/duji', 'GNOME_DESKTOP_SESSION_ID': 'this-is-deprecated', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'QT_IM_MODULE': 'ibus', 'LOGNAME': 'duji', 'USER': 'duji', 'PATH': '/home/duji/473/scrapy-1.0/.tox/py27/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games', 'XDG_VTNR': '7', 'GNOME_KEYRING_CONTROL': '/run/user/1000/keyring-XQE6Fx', 'DISPLAY': ':0', 'LANG': 'en_US.UTF-8', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'XDG_SESSION_PATH': '/org/freedesktop/DisplayManager/Session0', 'XAUTHORITY': '/home/duji/.Xauthority', 'LANGUAGE': 'en_US', 'SESSION_MANAGER': 'local/duji-Q330:@/tmp/.ICE-unix/1975,unix/duji-Q330:/tmp/.ICE-unix/1975', 'SHLVL': '1', 'MANDATORY_PATH': '/usr/share/gconf/ubuntu.mandatory.path', 'CLUTTER_IM_MODULE': 'xim', 'VIRTUAL_ENV': '/home/duji/473/scrapy-1.0/.tox/py27', 'TEXTDOMAIN': 'im-config', 'JOB': 'dbus', 'WINDOWID': '69206027', 'SESSIONTYPE': 'gnome-session', 'XMODIFIERS': '@im=ibus', 'PYTHONHASHSEED': '3846923193', 'GPG_AGENT_INFO': '/run/user/1000/keyring-XQE6Fx/gpg:0:1', 'HOME': '/home/duji', 'QT4_IM_MODULE': 'xim', 'SELINUX_INIT': 'YES', 'SSH_AUTH_SOCK': '/run/user/1000/keyring-XQE6Fx/ssh', 'XDG_RUNTIME_DIR': '/run/user/1000', 'GTK_IM_MODULE': 'ibus', 'COMPIZ_CONFIG_PROFILE': 'ubuntu', 'COMPIZ_BIN_PATH': '/usr/bin/', 'VTE_VERSION': '3409', 'GDMSESSION': 'ubuntu', 'IM_CONFIG_PHASE': '1', 'TEXTDOMAINDIR': '/usr/share/locale/', 'XDG_DATA_DIRS': '/usr/share/ubuntu:/usr/share/gnome:/usr/local/share/:/usr/share/', 'XDG_SEAT_PATH': '/org/freedesktop/DisplayManager/Seat0', 'XDG_CONFIG_DIRS': '/etc/xdg/xdg-ubuntu:/usr/share/upstart/xdg:/etc/xdg', 'XDG_CURRENT_DESKTOP': 'Unity', 'XDG_SESSION_ID': 'c2', 'DBUS_SESSION_BUS_ADDRESS': 'unix:abstract=/tmp/dbus-MHQEOgPzYy', '': '/usr/local/bin/tox', 'DEFAULTS_PATH': '/usr/share/gconf/ubuntu.default.path', 'SESSION': 'ubuntu', 'DESKTOP_SESSION': 'ubuntu', 'UPSTART_SESSION': 'unix:abstract=/com/ubuntu/upstart-session/1000/1800', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'GNOME_KEYRING_PID': '1797', 'OLDPWD': '/home/duji', 'GDM_LANG': 'en_US', 'GTK_MODULES': 'overlay-scrollbar:unity-gtk-module', 'INSTANCE': '', 'PWD': '/home/duji/473/scrapy-1.0', 'COLORTERM': 'gnome-terminal', 'XDG_MENU_PREFIX': 'gnome-', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arj=01;31:.taz=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.zip=01;31:.z=01;31:.Z=01;31:.dz=01;31:.gz=01;31:.lz=01;31:.xz=01;31:.bz2=01;31:.bz=01;31:.tbz=01;31:.tbz2=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.war=01;31:.ear=01;31:.sar=01;31:.rar=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.jpg=01;35:.jpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.webm=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.axv=01;35:.anx=01;35:.ogv=01;35:.ogx=01;35:.aac=00;36:.au=00;36:.flac=00;36:.mid=00;36:.midi=00;36:.mka=00;36:.mp3=00;36:.mpc=00;36:.ogg=00;36:.ra=00;36:.wav=00;36:.axa=00;36:.oga=00;36:.spx=00;36:_.xspf=00;36:', 'XDG_SEAT': 'seat0'}\nCollecting boto\nUsing cached boto-2.38.0-py2.py3-none-any.whl\nCollecting Pillow\nUsing cached Pillow-3.0.0.tar.gz\nCollecting leveldb\nCollecting Twisted>=10.0.0 (from -r requirements.txt (line 1))\nCollecting lxml (from -r requirements.txt (line 2))\nCollecting pyOpenSSL (from -r requirements.txt (line 3))\nUsing cached pyOpenSSL-0.15.1-py2.py3-none-any.whl\nCollecting cssselect>=0.9 (from -r requirements.txt (line 4))\nCollecting w3lib>=1.8.0 (from -r requirements.txt (line 5))\nUsing cached w3lib-1.13.0-py2.py3-none-any.whl\nCollecting queuelib (from -r requirements.txt (line 6))\nUsing cached queuelib-1.4.2-py2.py3-none-any.whl\nCollecting six>=1.5.2 (from -r requirements.txt (line 7))\nUsing cached six-1.10.0-py2.py3-none-any.whl\nCollecting service-identity (from -r requirements.txt (line 8))\nUsing cached service_identity-14.0.0-py2.py3-none-any.whl\nCollecting mock (from -r tests/requirements.txt (line 1))\nUsing cached mock-1.3.0-py2.py3-none-any.whl\nCollecting mitmproxy==0.10.1 (from -r tests/requirements.txt (line 2))\nCollecting netlib==0.10.1 (from -r tests/requirements.txt (line 3))\nCollecting pytest==2.7.3 (from -r tests/requirements.txt (line 4))\nUsing cached pytest-2.7.3-py2.py3-none-any.whl\nCollecting pytest-twisted (from -r tests/requirements.txt (line 5))\nUsing cached pytest_twisted-1.5-py27-none-any.whl\nCollecting jmespath (from -r tests/requirements.txt (line 6))\nUsing cached jmespath-0.9.0-py2.py3-none-any.whl\nCollecting testfixtures (from -r tests/requirements.txt (line 7))\nUsing cached testfixtures-4.5.1-py2.py3-none-any.whl\nCollecting zope.interface>=3.6.0 (from Twisted>=10.0.0->-r requirements.txt (line 1))\nCollecting cryptography>=0.7 (from pyOpenSSL->-r requirements.txt (line 3))\nUsing cached cryptography-1.1.1.tar.gz\nCollecting pyasn1-modules (from service-identity->-r requirements.txt (line 8))\nUsing cached pyasn1_modules-0.0.8-py2.py3-none-any.whl\nCollecting pyasn1 (from service-identity->-r requirements.txt (line 8))\nUsing cached pyasn1-0.1.9-py2.py3-none-any.whl\nCollecting characteristic>=14.0.0 (from service-identity->-r requirements.txt (line 8))\nUsing cached characteristic-14.3.0-py2.py3-none-any.whl\nCollecting pbr>=0.11 (from mock->-r tests/requirements.txt (line 1))\nUsing cached pbr-1.8.1-py2.py3-none-any.whl\nCollecting funcsigs (from mock->-r tests/requirements.txt (line 1))\nUsing cached funcsigs-0.4-py2.py3-none-any.whl\nCollecting urwid>=1.1 (from mitmproxy==0.10.1->-r tests/requirements.txt (line 2))\nCollecting flask (from mitmproxy==0.10.1->-r tests/requirements.txt (line 2))\nCollecting py>=1.4.29 (from pytest==2.7.3->-r tests/requirements.txt (line 4))\nUsing cached py-1.4.31-py2.py3-none-any.whl\nCollecting greenlet (from pytest-twisted->-r tests/requirements.txt (line 5))\nCollecting decorator (from pytest-twisted->-r tests/requirements.txt (line 5))\nUsing cached decorator-4.0.4-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): setuptools in ./.tox/py27/lib/python2.7/site-packages (from zope.interface>=3.6.0->Twisted>=10.0.0->-r requirements.txt (line 1))\nCollecting idna>=2.0 (from cryptography>=0.7->pyOpenSSL->-r requirements.txt (line 3))\nUsing cached idna-2.0-py2.py3-none-any.whl\nCollecting enum34 (from cryptography>=0.7->pyOpenSSL->-r requirements.txt (line 3))\nCollecting ipaddress (from cryptography>=0.7->pyOpenSSL->-r requirements.txt (line 3))\nUsing cached ipaddress-1.0.15-py27-none-any.whl\nCollecting cffi>=1.1.0 (from cryptography>=0.7->pyOpenSSL->-r requirements.txt (line 3))\nCollecting itsdangerous>=0.21 (from flask->mitmproxy==0.10.1->-r tests/requirements.txt (line 2))\nCollecting Jinja2>=2.4 (from flask->mitmproxy==0.10.1->-r tests/requirements.txt (line 2))\nUsing cached Jinja2-2.8-py2.py3-none-any.whl\nCollecting Werkzeug>=0.7 (from flask->mitmproxy==0.10.1->-r tests/requirements.txt (line 2))\nUsing cached Werkzeug-0.11.2-py2.py3-none-any.whl\nCollecting pycparser (from cffi>=1.1.0->cryptography>=0.7->pyOpenSSL->-r requirements.txt (line 3))\nCollecting MarkupSafe (from Jinja2>=2.4->flask->mitmproxy==0.10.1->-r tests/requirements.txt (line 2))\nBuilding wheels for collected packages: Pillow, cryptography\nRunning setup.py bdist_wheel for Pillow\nComplete output from command /home/duji/473/scrapy-1.0/.tox/py27/bin/python2.7 -c \"import setuptools;file='/tmp/pip-build-DDxyDe/Pillow/setup.py';exec(compile(open(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" bdist_wheel -d /tmp/tmp8Rwfb4pip-wheel-:\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib.linux-x86_64-2.7\ncreating build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PcdImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageColor.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/EpsImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/IcoImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/SunImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/FpxImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageFont.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/XpmImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageDraw2.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/BufrStubImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/MspImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/BmpImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/BdfFontFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/GdImageFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageSequence.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageDraw.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/FontFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageWin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/Image.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/IptcImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageMode.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/XbmImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/init.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/TarIO.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/SgiImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/JpegImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/CurImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/_binary.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImagePalette.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PaletteFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageGrab.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/TiffTags.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/WalImageFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/TgaImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/FitsStubImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageFilter.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageTransform.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/MicImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/GifImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/OleFileIO.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/WmfImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageStat.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ExifTags.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/MpegImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PdfImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/FliImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/SpiderImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/MpoImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/GimpPaletteFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageTk.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PpmImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/Hdf5StubImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageChops.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PcfFontFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageEnhance.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageOps.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ContainerIO.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/IcnsImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/McIdasImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/GimpGradientFile.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/WebPImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/JpegPresets.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageMath.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageCms.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/_util.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImtImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PsdImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PyAccess.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageShow.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PSDraw.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageMorph.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/TiffImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PalmImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/Jpeg2KImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PngImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/DcxImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PcxImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImageQt.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/features.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/GribStubImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/XVThumbImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/ImagePath.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/GbrImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\ncopying PIL/PixarImagePlugin.py -> build/lib.linux-x86_64-2.7/PIL\nrunning egg_info\nwriting top-level names to Pillow.egg-info/top_level.txt\nwriting dependency_links to Pillow.egg-info/dependency_links.txt\nwriting Pillow.egg-info/PKG-INFO\nwarning: manifest_maker: standard file '-c' not found\nreading manifest file 'Pillow.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwriting manifest file 'Pillow.egg-info/SOURCES.txt'\ncopying PIL/OleFileIO-README.md -> build/lib.linux-x86_64-2.7/PIL\nrunning build_ext\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/tmp/pip-build-DDxyDe/Pillow/setup.py\", line 767, in\nzip_safe=not debug_build(),\nFile \"/usr/lib/python2.7/distutils/core.py\", line 151, in setup\ndist.run_commands()\nFile \"/usr/lib/python2.7/distutils/dist.py\", line 953, in run_commands\nself.run_command(cmd)\nFile \"/usr/lib/python2.7/distutils/dist.py\", line 972, in run_command\ncmd_obj.run()\nFile \"/home/duji/473/scrapy-1.0/.tox/py27/local/lib/python2.7/site-packages/wheel/bdist_wheel.py\", line 175, in run\nself.run_command('build')\nFile \"/usr/lib/python2.7/distutils/cmd.py\", line 326, in run_command\nself.distribution.run_command(command)\nFile \"/usr/lib/python2.7/distutils/dist.py\", line 972, in run_command\ncmd_obj.run()\nFile \"/usr/lib/python2.7/distutils/command/build.py\", line 128, in run\nself.run_command(cmd_name)\nFile \"/usr/lib/python2.7/distutils/cmd.py\", line 326, in run_command\nself.distribution.run_command(command)\nFile \"/usr/lib/python2.7/distutils/dist.py\", line 972, in run_command\ncmd_obj.run()\nFile \"/usr/lib/python2.7/distutils/command/build_ext.py\", line 337, in run\nself.build_extensions()\nFile \"/tmp/pip-build-DDxyDe/Pillow/setup.py\", line 515, in build_extensions\n% (f, f))\nValueError: --enable-jpeg requested but jpeg not found, aborting.\nFailed building wheel for Pillow\nRunning setup.py bdist_wheel for cryptography\nComplete output from command /home/duji/473/scrapy-1.0/.tox/py27/bin/python2.7 -c \"import setuptools;file='/tmp/pip-build-DDxyDe/cryptography/setup.py';exec(compile(open(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" bdist_wheel -d /tmp/tmpY5FGLmpip-wheel-:\nInstalled /tmp/pip-build-DDxyDe/cryptography/.eggs/cffi-1.3.1-py2.7-linux-x86_64.egg\nSearching for pycparser\nReading https://pypi.python.org/simple/pycparser/\nBest match: pycparser 2.14\nDownloading https://pypi.python.org/packages/source/p/pycparser/pycparser-2.14.tar.gz#md5=a2bc8d28c923b4fe2b2c3b4b51a4f935\nProcessing pycparser-2.14.tar.gz\nWriting /tmp/easy_install-cQ35PR/pycparser-2.14/setup.cfg\nRunning pycparser-2.14/setup.py -q bdist_egg --dist-dir /tmp/easy_install-cQ35PR/pycparser-2.14/egg-dist-tmp-cKkrRo\nwarning: no previously-included files matching 'yacctab.' found under directory 'tests'\nwarning: no previously-included files matching 'lextab.' found under directory 'tests'\nwarning: no previously-included files matching 'yacctab.' found under directory 'examples'\nwarning: no previously-included files matching 'lextab.' found under directory 'examples'\nzip_safe flag not set; analyzing archive contents...\nMoving pycparser-2.14-py2.7.egg to /tmp/pip-build-DDxyDe/cryptography/.eggs\nInstalled /tmp/pip-build-DDxyDe/cryptography/.eggs/pycparser-2.14-py2.7.egg\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib.linux-x86_64-2.7\ncreating build/lib.linux-x86_64-2.7/cryptography\ncopying src/cryptography/init.py -> build/lib.linux-x86_64-2.7/cryptography\ncopying src/cryptography/about.py -> build/lib.linux-x86_64-2.7/cryptography\ncopying src/cryptography/utils.py -> build/lib.linux-x86_64-2.7/cryptography\ncopying src/cryptography/exceptions.py -> build/lib.linux-x86_64-2.7/cryptography\ncopying src/cryptography/fernet.py -> build/lib.linux-x86_64-2.7/cryptography\ncreating build/lib.linux-x86_64-2.7/cryptography/x509\ncopying src/cryptography/x509/base.py -> build/lib.linux-x86_64-2.7/cryptography/x509\ncopying src/cryptography/x509/general_name.py -> build/lib.linux-x86_64-2.7/cryptography/x509\ncopying src/cryptography/x509/init.py -> build/lib.linux-x86_64-2.7/cryptography/x509\ncopying src/cryptography/x509/oid.py -> build/lib.linux-x86_64-2.7/cryptography/x509\ncopying src/cryptography/x509/extensions.py -> build/lib.linux-x86_64-2.7/cryptography/x509\ncopying src/cryptography/x509/name.py -> build/lib.linux-x86_64-2.7/cryptography/x509\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat\ncopying src/cryptography/hazmat/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings\ncopying src/cryptography/hazmat/bindings/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncopying src/cryptography/hazmat/primitives/constant_time.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncopying src/cryptography/hazmat/primitives/serialization.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncopying src/cryptography/hazmat/primitives/padding.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncopying src/cryptography/hazmat/primitives/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncopying src/cryptography/hazmat/primitives/hmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncopying src/cryptography/hazmat/primitives/hashes.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncopying src/cryptography/hazmat/primitives/cmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncopying src/cryptography/hazmat/primitives/keywrap.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/backends\ncopying src/cryptography/hazmat/backends/multibackend.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends\ncopying src/cryptography/hazmat/backends/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends\ncopying src/cryptography/hazmat/backends/interfaces.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/openssl\ncopying src/cryptography/hazmat/bindings/openssl/binding.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/openssl\ncopying src/cryptography/hazmat/bindings/openssl/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/openssl\ncopying src/cryptography/hazmat/bindings/openssl/_conditional.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/openssl\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/commoncrypto\ncopying src/cryptography/hazmat/bindings/commoncrypto/binding.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/commoncrypto\ncopying src/cryptography/hazmat/bindings/commoncrypto/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/commoncrypto\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\ncopying src/cryptography/hazmat/primitives/kdf/x963kdf.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\ncopying src/cryptography/hazmat/primitives/kdf/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\ncopying src/cryptography/hazmat/primitives/kdf/hkdf.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\ncopying src/cryptography/hazmat/primitives/kdf/pbkdf2.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\ncopying src/cryptography/hazmat/primitives/kdf/concatkdf.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\ncopying src/cryptography/hazmat/primitives/asymmetric/rsa.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\ncopying src/cryptography/hazmat/primitives/asymmetric/padding.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\ncopying src/cryptography/hazmat/primitives/asymmetric/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\ncopying src/cryptography/hazmat/primitives/asymmetric/utils.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\ncopying src/cryptography/hazmat/primitives/asymmetric/dsa.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\ncopying src/cryptography/hazmat/primitives/asymmetric/ec.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\ncopying src/cryptography/hazmat/primitives/asymmetric/dh.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\ncopying src/cryptography/hazmat/primitives/twofactor/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\ncopying src/cryptography/hazmat/primitives/twofactor/totp.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\ncopying src/cryptography/hazmat/primitives/twofactor/utils.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\ncopying src/cryptography/hazmat/primitives/twofactor/hotp.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\ncopying src/cryptography/hazmat/primitives/ciphers/modes.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\ncopying src/cryptography/hazmat/primitives/ciphers/base.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\ncopying src/cryptography/hazmat/primitives/ciphers/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\ncopying src/cryptography/hazmat/primitives/ciphers/algorithms.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/interfaces\ncopying src/cryptography/hazmat/primitives/interfaces/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/interfaces\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/rsa.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/hmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/hashes.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/cmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/ciphers.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/backend.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/x509.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/utils.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/dsa.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncopying src/cryptography/hazmat/backends/openssl/ec.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\ncreating build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\ncopying src/cryptography/hazmat/backends/commoncrypto/init.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\ncopying src/cryptography/hazmat/backends/commoncrypto/hmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\ncopying src/cryptography/hazmat/backends/commoncrypto/hashes.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\ncopying src/cryptography/hazmat/backends/commoncrypto/ciphers.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\ncopying src/cryptography/hazmat/backends/commoncrypto/backend.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\nrunning egg_info\nwriting entry points to src/cryptography.egg-info/entry_points.txt\nwriting top-level names to src/cryptography.egg-info/top_level.txt\nwriting requirements to src/cryptography.egg-info/requires.txt\nwriting dependency_links to src/cryptography.egg-info/dependency_links.txt\nwriting src/cryptography.egg-info/PKG-INFO\nwarning: manifest_maker: standard file '-c' not found\nreading manifest file 'src/cryptography.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nno previously-included directories found matching 'docs/_build'\nwarning: no previously-included files matching '*' found under directory 'vectors'\nwriting manifest file 'src/cryptography.egg-info/SOURCES.txt'\nrunning build_ext\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_padding.c'\ncreating build/temp.linux-x86_64-2.7\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_constant_time.c'\ngenerating cffi module 'build/temp.linux-x86_64-2.7/_openssl.c'\nbuilding '_openssl' extension\ncreating build/temp.linux-x86_64-2.7/build\ncreating build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7\nx86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_openssl.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_openssl.o\nbuild/temp.linux-x86_64-2.7/_openssl.c:410:25: fatal error: openssl/aes.h: No such file or directory\n#include <openssl/aes.h>\n^\ncompilation terminated.\nerror: command 'x86_64-linux-gnu-gcc' failed with exit status 1\nFailed building wheel for cryptography\nFailed to build Pillow cryptography\nInstalling collected packages: boto, Pillow, leveldb, zope.interface, Twisted, lxml, six, idna, pyasn1, enum34, ipaddress, pycparser, cffi, cryptography, pyOpenSSL, cssselect, w3lib, queuelib, pyasn1-modules, characteristic, service-identity, pbr, funcsigs, mock, urwid, itsdangerous, MarkupSafe, Jinja2, Werkzeug, flask, netlib, mitmproxy, py, pytest, greenlet, decorator, pytest-twisted, jmespath, testfixtures\nRunning setup.py install for Pillow\nComplete output from command /home/duji/473/scrapy-1.0/.tox/py27/bin/python2.7 -c \"import setuptools, tokenize;file='/tmp/pip-build-DDxyDe/Pillow/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-n2GO9b-record/install-record.txt --single-version-externally-managed --compile --install-headers /home/duji/473/scrapy-1.0/.tox/py27/include/site/python2.7/Pillow:\nrunning install\nrunning build\nrunning build_py\nrunning egg_info\nwriting top-level names to Pillow.egg-info/top_level.txt\nwriting dependency_links to Pillow.egg-info/dependency_links.txt\nwriting Pillow.egg-info/PKG-INFO\nwarning: manifest_maker: standard file '-c' not found\nreading manifest file 'Pillow.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwriting manifest file 'Pillow.egg-info/SOURCES.txt'\nrunning build_ext\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/tmp/pip-build-DDxyDe/Pillow/setup.py\", line 767, in <module>\n    zip_safe=not debug_build(),\n  File \"/usr/lib/python2.7/distutils/core.py\", line 151, in setup\n    dist.run_commands()\n  File \"/usr/lib/python2.7/distutils/dist.py\", line 953, in run_commands\n    self.run_command(cmd)\n  File \"/usr/lib/python2.7/distutils/dist.py\", line 972, in run_command\n    cmd_obj.run()\n  File \"/home/duji/473/scrapy-1.0/.tox/py27/local/lib/python2.7/site-packages/setuptools/command/install.py\", line 61, in run\n    return orig.install.run(self)\n  File \"/usr/lib/python2.7/distutils/command/install.py\", line 601, in run\n    self.run_command('build')\n  File \"/usr/lib/python2.7/distutils/cmd.py\", line 326, in run_command\n    self.distribution.run_command(command)\n  File \"/usr/lib/python2.7/distutils/dist.py\", line 972, in run_command\n    cmd_obj.run()\n  File \"/usr/lib/python2.7/distutils/command/build.py\", line 128, in run\n    self.run_command(cmd_name)\n  File \"/usr/lib/python2.7/distutils/cmd.py\", line 326, in run_command\n    self.distribution.run_command(command)\n  File \"/usr/lib/python2.7/distutils/dist.py\", line 972, in run_command\n    cmd_obj.run()\n  File \"/usr/lib/python2.7/distutils/command/build_ext.py\", line 337, in run\n    self.build_extensions()\n  File \"/tmp/pip-build-DDxyDe/Pillow/setup.py\", line 515, in build_extensions\n    % (f, f))\nValueError: --enable-jpeg requested but jpeg not found, aborting.\n\n----------------------------------------\nCommand \"/home/duji/473/scrapy-1.0/.tox/py27/bin/python2.7 -c \"import setuptools, tokenize;file='/tmp/pip-build-DDxyDe/Pillow/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-n2GO9b-record/install-record.txt --single-version-externally-managed --compile --install-headers /home/duji/473/scrapy-1.0/.tox/py27/include/site/python2.7/Pillow\" failed with error code 1 in /tmp/pip-build-DDxyDe/Pillow", "issue_status": "Closed", "issue_reporting_time": "2015-12-02T06:47:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1072": {"issue_url": "https://github.com/scrapy/scrapy/issues/1620", "issue_id": "#1620", "issue_summary": "Might be a typo", "issue_description": "kburman commented on Nov 28, 2015\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/http/response/text.py#L44\nit should be\nself._body = body.encode(self.encoding)\nnot\nself._body = body.encode(self._encoding)", "issue_status": "Closed", "issue_reporting_time": "2015-11-28T13:35:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1073": {"issue_url": "https://github.com/scrapy/scrapy/issues/1617", "issue_id": "#1617", "issue_summary": "logging in scrapy may cause memory leak", "issue_description": "Zephor5 commented on Nov 26, 2015\nI'm trying to write a web view to track my crawler tasks and the logs as well\nSo, I write a simple Queue-based threaded log handler to save the log to a capped mongo collection.\nThen I found the Spider and Request leaking.\nAs I debuged, I found that Spider and Request objects were passed into the logging Record many times.\nSo I sugguest you guys may add some docs or fix this in the future.", "issue_status": "Closed", "issue_reporting_time": "2015-11-26T08:32:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1074": {"issue_url": "https://github.com/scrapy/scrapy/issues/1616", "issue_id": "#1616", "issue_summary": "Big download is not being cancelled properly", "issue_description": "katcipis commented on Nov 25, 2015\nI am running scrapy 1.0.1 in my pyhon web crawler and setting the max download size like this:\nDOWNLOAD_MAXSIZE = 41943040 #40MB.\nWhen I run into a bigger file, scrappy keeps logging the error: ERROR: Received (51316256) bytes larger than download max size (41943040) but never stops. I have looked into the code and saw a call to cancel the processe here, but in my case is not working, the download keeps going on.\nAny thoughts on why this would happen ? It seems to be something wrong on twisted not cancelling the download.\nDetailed version info:\nscrapy version -v\n2015-11-25 10:44:54 [scrapy] INFO: Scrapy 1.0.1 started (bot:)\n2015-11-25 10:44:54 [scrapy] INFO: Optional features available: ssl, http11\n2015-11-25 10:44:54 [scrapy] INFO: Overridden settings: {'COOKIES_DEBUG': True, 'DOWNLOAD_TIMEOUT': 600, 'SPIDER_MODULES': ['spiders'], 'CONCURRENT_REQUESTS': 10, 'RANDOMIZE_DOWNLOAD_DELAY': False, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'RETRY_TIMES': 10, 'BOT_NAME': 'bot', 'DOWNLOAD_MAXSIZE': 41943040, 'USER_AGENT': 'Mozilla/5.0 (X11; Linux i686; rv:23.0) Gecko/20100101 Firefox/23.0', 'NEWSPIDER_MODULE': 'spiders'}\nScrapy  : 1.0.1\nlxml    : 3.5.0.0\nlibxml2 : 2.9.2\nTwisted : 15.4.0\nPython  : 2.7.10 (default, Aug 13 2015, 12:27:27) - [GCC 4.9.2]\nPlatform: Linux-3.16.0-38-generic-x86_64-with", "issue_status": "Closed", "issue_reporting_time": "2015-11-25T12:46:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1075": {"issue_url": "https://github.com/scrapy/scrapy/issues/1613", "issue_id": "#1613", "issue_summary": "Deprecating `from_settings()` constructors", "issue_description": "Contributor\njdemaeyer commented on Nov 23, 2015\nOutsourcing a discussion from #1605 here: Should from_settings() constructors in pipelines/extensions/middlewares be deprecated?\nWith the exception of spider loaders, Scrapy only instantiates components when the crawler object is already available. from_settings() constructors are not really documented, besides for the spider loader API and the MailSender extension, and their support is inconsistent (e.g. spiders cannot provide from_settings() constructors).\nAre there use cases where middlewares etc. need to be instantiated with settings, but without a crawler?", "issue_status": "Closed", "issue_reporting_time": "2015-11-23T14:56:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1076": {"issue_url": "https://github.com/scrapy/scrapy/issues/1612", "issue_id": "#1612", "issue_summary": "scrapy LOG_LEVEL setting in Spider.custom_settings does not work", "issue_description": "YAmikep commented on Nov 23, 2015\nI set the LOG_LEVEL setting to INFO in the Spider class via the custom_settings attribute but I still see the DEBUG messages in the console.\nWhen I set it on the settings.py file or via the command line option --loglevel, it works.\nI thought any settings could be set via the custom_settings attribute. Is that a bug? (Scrapy 1.0.3 and python 2.7.10)\nclass TestSpider(scrapy.Spider):\n    name = \"Test\"\n    ...\n    custom_settings = {\n        'LOG_LEVEL': 'INFO',\n    }\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2015-11-23T01:33:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1077": {"issue_url": "https://github.com/scrapy/scrapy/issues/1608", "issue_id": "#1608", "issue_summary": "TypeError: 'ItemLoader' object does not support item assignment", "issue_description": "haoflynet commented on Nov 19, 2015\nThere is my code:\ndef parse(self, response):\nitem = ItemLoader(item=MyItem(), response = response)\nitem.default_output_processor = TakeFirst()\n    item.add_value('name', 'value')\n\n    yield scrapy.Request(Myurl, meta={'myitem': item}, callback=self.parse_next)\n\ndef parse_next(self, response):\n    item = response.meta['myitem']\n\n    newitem = ItemLoader(item=item, response=response)\n\n    newitem.add_value('name1', 'value1')\n    newitem.add_xpath('name2', '//div')\n\n    # type(response.meta['myitem']), it output \"<class 'scrapy.loader.ItemLoader'>\"\n    # type(newitem), it also output \"<class 'scrapy.loader.ItemLoader'>\"\n\n    yield newitem.load_item()\nI want to transfer one item to another parse function, but I encounter one exception at last.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/media/sf_workspace/yiche/yiche/spiders/CarstyleSpider.py\", line 117, in parse_car\n    yield carstyle.load_item()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/loader/__init__.py\", line 86, in load_item\n    item[field_name] = value\nTypeError: 'ItemLoader' object does not support item assignment\n2015-11-19 20:03:07 [scrapy] INFO: Closing spider (finished)", "issue_status": "Closed", "issue_reporting_time": "2015-11-19T12:17:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1078": {"issue_url": "https://github.com/scrapy/scrapy/issues/1607", "issue_id": "#1607", "issue_summary": "Is there any reason why there is no loader.remove_value() method?", "issue_description": "Contributor\npawelmhm commented on Nov 18, 2015\nSometimes I'd like to remove something from loader but with current scrapy.loader code this seems impossible, I have to load_item(), pop field from item and create new loader. Is this function missing for a reason or just missing because we think it's not needed?\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2015-11-18T11:26:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1079": {"issue_url": "https://github.com/scrapy/scrapy/issues/1606", "issue_id": "#1606", "issue_summary": "response.body is duplicate", "issue_description": "fanpei91 commented on Nov 17, 2015\nAccess the text page(not mine) by browsers or wget and you will find the response content is not duplicate, but scrapy's response.body is duplicate. I had tried set the scrapy's headers same as a real brower's, but it is still duplicate.\nJust use the follow sample code, and you will find the issue.\nscrapy shell \"http://files.qidian.com/Author4/3615059/88542882.txt\"\nSorry for my bad english.", "issue_status": "Closed", "issue_reporting_time": "2015-11-16T20:27:06Z", "fixed_by": "#1905", "pull_request_summary": "[MRG+1] Modified read failure recovery in utils/gz.py to read only the last f.extrasize bytes of f.extrabuf[ ]", "pull_request_description": "Contributor\nrootAvish commented on Apr 6, 2016\nThis fixes #1606. Building off the remarks of @jdmaeyer in that bug report, the major difference between downloading the file from this server and his pastebin link, is that this server is a Microsoft IIS instance. A look and the http request that was being sent\nIn [4]: print(request.headers)\n{'Accept-Language': ['en'], 'Accept-Encoding': ['gzip,deflate'], 'Accept': ['text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'], 'User-Agent': ['Scrapy/1.2.0dev2 (+http://scrapy.org)']}\nled me to httpcompression since we are accepted gzip encoded data.\nI manually saved the response to a file and tried decompressing it using gunzip, which is also used by scrapy.utils.gz for the same purpose.\nrootavish@rootavish:~/scrapy$ gunzip --decompress dbg.gz \n\ngzip: dbg.gz: unexpected end of file\nHowever, the same file was handled by KDE's Ark without any issues, so I looked into the code in scrapy.utils.gz for problems. Changing the chunk size of the read to something smaller than the file size seemed to fix it.\n chunk = read1(f, 8196) # changing chunk size to 4096 fixed the problem\nThis behaviour is unexpected since the read should ideally only take place till the end of the file, but due to the archive not being in pristine condition(possibly due to the compression taking place on a Windows based IIS instance and decoding on *nix), there was a wrap around read happening. The ideal solution would be to patch the gzip code itself, but since that's not an option we can make do with monkeypatching the same. The idea was to replace _read_eof with a no-op for partial archives, something that won't work if GzipFile does it's checksum comparison. The problem should be fixed now.\nrootavish@rootavish:~/scrapy$ scrapy shell \"http://files.qidian.com/Author4/3615059/88542882.txt\"\n2016-04-06 08:34:51 [scrapy] INFO: Scrapy 1.2.0dev2 started (bot: scrapybot)\n2016-04-06 08:34:51 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0, 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n2016-04-06 08:34:51 [scrapy] INFO: Enabled extensions:\n['scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-04-06 08:34:51 [scrapy] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-04-06 08:34:51 [scrapy] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-04-06 08:34:51 [scrapy] INFO: Enabled item pipelines:\n[]\n2016-04-06 08:34:51 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-04-06 08:34:51 [scrapy] INFO: Spider opened\n2016-04-06 08:34:54 [scrapy] DEBUG: Crawled (200) <GET http://files.qidian.com/Author4/3615059/88542882.txt> (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7f7b70ce6b50>\n[s]   item       {}\n[s]   request    <GET http://files.qidian.com/Author4/3615059/88542882.txt>\n[s]   response   <200 http://files.qidian.com/Author4/3615059/88542882.txt>\n[s]   settings   <scrapy.settings.Settings object at 0x7f7b70ce69d0>\n[s]   spider     <DefaultSpider 'default' at 0x7f7b6febb090>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\nIn [1]: import requests\n\nIn [2]: resp = requests.get(\"http://files.qidian.com/Author4/3615059/88542882.txt\")\n2016-04-06 08:35:20 [urllib3.connectionpool] INFO: Starting new HTTP connection (1): files.qidian.com\n2016-04-06 08:35:20 [urllib3.connectionpool] DEBUG: Setting read timeout to None\n2016-04-06 08:35:20 [urllib3.connectionpool] DEBUG: \"GET /Author4/3615059/88542882.txt HTTP/1.1\" 200 None\n\nIn [3]: response.body_as_unicode() == resp.text\nOut[3]: True\n\nIn [4]: response.body.count('document')\nOut[4]: 1\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2016-08-17T12:51:30Z", "files_changed": [["2", "scrapy/utils/gz.py"], ["1", "tests/sample_data/compressed/unexpected-eof-output.txt"], ["BIN", "+5.01", "KB", "tests/sample_data/compressed/unexpected-eof.gz"], ["10", "tests/test_utils_gz.py"]]}, "1080": {"issue_url": "https://github.com/scrapy/scrapy/issues/1604", "issue_id": "#1604", "issue_summary": "ImportError: Error loading object 'scrapy.contrib.memusage.MemoryUsage': No module named uu", "issue_description": "praveshjain commented on Nov 16, 2015\nI've been using scrapy on Mac for over 6 months now when today, out of nowhere I start getting this issue.\nI ran it like I normally do using \"scrapy crawl \". I also tried checking my twisted installation, which is working fine.\nHere is the complete stacktrace of the issue:\n2015-11-16 17:07:00+0530 [scrapy] INFO: Scrapy 0.24.6 started (bot: scrapybot)\n2015-11-16 17:07:00+0530 [scrapy] INFO: Optional features available: ssl, http11\n2015-11-16 17:07:00+0530 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'Crawler.spiders', 'SPIDER_MODULES': ['Crawler.spiders', 'Crawler.availability_spiders'], 'COOKIES_ENABLED': False, 'USER_AGENT': 'Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0'}\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/Library/Python/2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/Library/Python/2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\nfunc(_a, *_kw)\nFile \"/Library/Python/2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\ncmd.run(args, opts)\nFile \"/Library/Python/2.7/site-packages/scrapy/commands/crawl.py\", line 60, in run\nself.crawler_process.start()\nFile \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 92, in start\nif self.start_crawling():\nFile \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 124, in start_crawling\nreturn self._start_crawler() is not None\nFile \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 139, in _start_crawler\ncrawler.configure()\nFile \"/Library/Python/2.7/site-packages/scrapy/crawler.py\", line 46, in configure\nself.extensions = ExtensionManager.from_crawler(self)\nFile \"/Library/Python/2.7/site-packages/scrapy/middleware.py\", line 50, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/Library/Python/2.7/site-packages/scrapy/middleware.py\", line 29, in from_settings\nmwcls = load_object(clspath)\nFile \"/Library/Python/2.7/site-packages/scrapy/utils/misc.py\", line 42, in load_object\nraise ImportError(\"Error loading object '%s': %s\" % (path, e))\nImportError: Error loading object 'scrapy.contrib.memusage.MemoryUsage': No module named uu\nPlease provide guidelines on why this is happening.", "issue_status": "Closed", "issue_reporting_time": "2015-11-16T11:44:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1081": {"issue_url": "https://github.com/scrapy/scrapy/issues/1602", "issue_id": "#1602", "issue_summary": "scrapy not printing out stacktrace on exception", "issue_description": "somghosh commented on Nov 16, 2015\nIs there a special mechanism to force scrapy to print out all python exception/stacktrace. (i put the question here as well: http://stackoverflow.com/questions/33725800/scrapy-not-printing-out-stacktrace-on-exception)\nI made a simple mistake of getting a list attribute wrong resulting in AttributeError which did not show up in full in the logs\nWhat showed up was :\n2015-11-15 22:13:50 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 264,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 40342,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2015, 11, 15, 22, 13, 50, 860480),\n 'log_count/CRITICAL': 1,\n 'log_count/DEBUG': 1,\n 'log_count/INFO': 1,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'spider_exceptions/AttributeError': 1,\n 'start_time': datetime.datetime(2015, 11, 15, 22, 13, 49, 222371)}\nSo it showed the AttributeError count of 1, but didnt tell me where and how, I had to manually place ipdb.set_trace() in code to find out where it got an error. Scrapy by itself continued to carry out other threads without printing anything\nipdb>\nAttributeError: \"'list' object has no attribute 'match'\"\n> /Users/username/Programming/regent/regentscraper/spiders/regent_spider.py(139)request_listing_detail_pages_from_listing_id_list()\n    138             volatile_props = ListingScanVolatilePropertiesItem()\n--> 139             volatile_props['position_in_search'] = list_of_listing_ids.match(listing_id) + rank_of_first_item_in_page\n    140\nscrapy settings\n# -*- coding: utf-8 -*-\n\n# Scrapy settings for regentscraper project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     http://doc.scrapy.org/en/latest/topics/settings.html\n#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\n#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n\nimport sys\nimport os\nimport django\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__name__), os.pardir)))\n\nprint sys.path\n\nos.environ['DJANGO_SETTINGS_MODULE'] = 'regent.settings'\ndjango.setup()  #new for Django 1.8\n\n\n\nBOT_NAME = 'regentscraper'\n\nSPIDER_MODULES = ['regentscraper.spiders']\nNEWSPIDER_MODULE = 'regentscraper.spiders'\n\n\nITEM_PIPELINES = {\n   'regentscraper.pipelines.ListingScanPipeline': 300,\n}", "issue_status": "Closed", "issue_reporting_time": "2015-11-16T01:06:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1082": {"issue_url": "https://github.com/scrapy/scrapy/issues/1600", "issue_id": "#1600", "issue_summary": "gevent.hub.LoopExit in scrapy", "issue_description": "Monk-Liu commented on Nov 14, 2015\nI dont't know why, my spider just stop after\n2015-11-14 10:26:39 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\nand when I forced to stop it , it raise :gevent.hub.LoopExit: This operation would block forever\nall debug info just like that:\n2015-11-14 10:24:14 [scrapy] INFO: Scrapy 1.0.3 started (bot: TCspider)\n2015-11-14 10:24:14 [scrapy] INFO: Optional features available: ssl, http11\n2015-11-14 10:24:14 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'TCspider.spiders', 'SPIDER_MODULES': ['TCspider.spiders'], 'LOGSTATS_INTERVAL': 0, 'BOT_NAME': 'TCspider'}\n2015-11-14 10:24:14 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, CoreStats, SpiderState\n2015-11-14 10:24:14 [py.warnings] WARNING: /home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/scrapy/utils/deprecate.py:155: ScrapyDeprecationWarning: scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware class is deprecated, use scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware instead\nScrapyDeprecationWarning)\n2015-11-14 10:24:23 [scrapy] INFO: Enabled downloader middlewares: HostMiddleware, UserAgentMiddleware, ProxyMiddleware, UserAgentMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-11-14 10:24:23 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-11-14 10:24:23 [scrapy] INFO: Enabled item pipelines: TcspiderPipeline\n2015-11-14 10:24:23 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n^C2015-11-14 10:24:53 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force\nTraceback (most recent call last):\nFile \"/home/liuqifan/Virtualenv/PY2/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\nfunc(_a, *_kw)\nFile \"/home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\ncmd.run(args, opts)\nFile \"/home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/scrapy/commands/shell.py\", line 63, in run\nshell.start(url=url)\nFile \"/home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/scrapy/shell.py\", line 44, in start\nself.fetch(url, spider)\nFile \"/home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/scrapy/shell.py\", line 87, in fetch\nreactor, self._schedule, request, spider)\nFile \"/home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/twisted/internet/threads.py\", line 120, in blockingCallFromThread\nresult = queue.get()\nFile \"/usr/lib64/python2.7/Queue.py\", line 168, in get\nself.not_empty.wait()\nFile \"/usr/lib64/python2.7/threading.py\", line 340, in wait\nwaiter.acquire()\nFile \"gevent/_semaphore.pyx\", line 112, in gevent._semaphore.Semaphore.acquire (gevent/gevent._semaphore.c:3386)\nFile \"/home/liuqifan/Virtualenv/PY2/lib/python2.7/site-packages/gevent/hub.py\", line 338, in switch\nreturn greenlet.switch(self)\ngevent.hub.LoopExit: This operation would block forever\nAppreciated if anysome tell me what's wrong with the program", "issue_status": "Closed", "issue_reporting_time": "2015-11-14T02:35:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1083": {"issue_url": "https://github.com/scrapy/scrapy/issues/1599", "issue_id": "#1599", "issue_summary": "Scrapy taking 30+ minutes to execute any spiders or scrapy bench", "issue_description": "bnussey commented on Nov 13, 2015\nI have a project using Scrapy 1.0.3. Everything was running fine, and after no considerable changes, spiders are taking at least 30 minutes to execute. Here are some logs from the prod environment:\n0:  2015-11-13 12:00:50 INFO    Log opened.\n1:  2015-11-13 12:00:50 INFO    [scrapy.log] Scrapy 1.0.3.post6+g2d688cd started\n2:  2015-11-13 12:39:26 INFO    [scrapy.utils.log] Scrapy 1.0.3.post6+g2d688cd started (bot: fancy)\n3:  2015-11-13 12:39:26 INFO    [scrapy.utils.log] Optional features available: ssl, http11, boto\nAs you can see from the logs, it took ~40 minutes to even start.\nFrom my console if I run scrapy bench, scrapy list or scrapy check I get the same problem.\nDoes anyone have any ideas?\nI've checked this on our dev and prod environment and having the same issue.\nI thought it could be code related, but if it's effecting just basic scrapy commands, I am a bit confused as to what this could be.\nNormal python scripts execute without issue.\nThanks", "issue_status": "Closed", "issue_reporting_time": "2015-11-13T18:19:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1084": {"issue_url": "https://github.com/scrapy/scrapy/issues/1596", "issue_id": "#1596", "issue_summary": "FormRequest doesn't handle input elements without type attribute", "issue_description": "Member\nkmike commented on Nov 13, 2015\nWhen <input> element has no type attribute browsers use default input type ('text'). See https://html.spec.whatwg.org/multipage/forms.html#attr-input-type:\nThe missing value default is the Text state.\nFormRequest doesn't submit such fields at all. Likely adding or not(@type) somewhere to xpaths can fix it.", "issue_status": "Closed", "issue_reporting_time": "2015-11-12T21:50:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1085": {"issue_url": "https://github.com/scrapy/scrapy/issues/1595", "issue_id": "#1595", "issue_summary": "FormRequest should consider input type values case-insensitive", "issue_description": "Member\nkmike commented on Nov 13, 2015\n<input> type attribute is an enumerated attribute; values of such attributes are case-insensitive. All XPaths in FormRequest source code treat them as case-sensitive.", "issue_status": "Closed", "issue_reporting_time": "2015-11-12T21:41:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1086": {"issue_url": "https://github.com/scrapy/scrapy/issues/1593", "issue_id": "#1593", "issue_summary": "Fix REDIRECT_PRIORITY_ADJUST docs", "issue_description": "Member\ncurita commented on Nov 12, 2015\nNot sure about it, but what's explained here: http://scrapy.readthedocs.org/en/latest/topics/settings.html#redirect-priority-adjust about priorities seems wrong, priorities in requests work the other way around (higher priority executes first), so a negative adjust would meant to decrease the original request priority to execute the redirect later.\nBy all means if this isn't the case, some note about why it seems to contradict what it is said about priorities in http://scrapy.readthedocs.org/en/latest/topics/request-response.html#request-objects would be helpful, got really confused there :(", "issue_status": "Closed", "issue_reporting_time": "2015-11-12T01:45:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1087": {"issue_url": "https://github.com/scrapy/scrapy/issues/1592", "issue_id": "#1592", "issue_summary": "Document LOG_UNSERIALIZABLE_REQUESTS setting", "issue_description": "Member\ncurita commented on Nov 12, 2015\nIt's implemented in scrapy/core/scheduler.py#L31 and not mentioned anywhere in docs. I think it's really useful, could be mentioned at least in http://scrapy.readthedocs.org/en/latest/topics/jobs.html#request-serialization.\nSince it isn't documented we could take the chance and rename it as well, this kind of settings are usually named <something>_DEBUG.", "issue_status": "Closed", "issue_reporting_time": "2015-11-11T20:18:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1088": {"issue_url": "https://github.com/scrapy/scrapy/issues/1590", "issue_id": "#1590", "issue_summary": "Working with forms issue", "issue_description": "Contributor\nLazar-T commented on Nov 12, 2015\n/", "issue_status": "Closed", "issue_reporting_time": "2015-11-11T18:51:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1089": {"issue_url": "https://github.com/scrapy/scrapy/issues/1589", "issue_id": "#1589", "issue_summary": "How to package scrapy program", "issue_description": "LinkZhang commented on Nov 10, 2015\nI used scrapy to write a spider,now I want to run in server,so I use pyinstaller to package it.I add a setup.py\nfrom scrapy import cmdline\ncmdline.execute(\"scrapy crawl Spiderl\".split())\nuse\npyinstaller -F setup.py\nsucceed get a exe,but run it get a error \"Unknown command: crawl\". the document said \"crawl must run in the root of project directory\". So how can I package a scrapy-program with out copy the source code of project to server", "issue_status": "Closed", "issue_reporting_time": "2015-11-10T09:49:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1090": {"issue_url": "https://github.com/scrapy/scrapy/issues/1588", "issue_id": "#1588", "issue_summary": "UnicodeDecodeError from scraper.py", "issue_description": "pilgrim2go commented on Nov 9, 2015\nHi,\nI'm using scrapy 1.0.3 in OSX and have following error when fetching.\nTraceback (most recent call last):\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/__init__.py\", line 851, in emit\n    msg = self.format(record)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/__init__.py\", line 724, in format\n    return fmt.format(record)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/__init__.py\", line 464, in format\n    record.message = record.getMessage()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/logging/__init__.py\", line 328, in getMessage\n    msg = msg % self.args\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 26: ordinal not in range(128)\nLogged from file scraper.py, line 237\nComment this line in scrapyer.py removed the error.\n            #logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\nWhat the issue is and how to solve it?\nThanks,\nVan", "issue_status": "Closed", "issue_reporting_time": "2015-11-09T04:11:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1091": {"issue_url": "https://github.com/scrapy/scrapy/issues/1585", "issue_id": "#1585", "issue_summary": "rules in CrawlSpider don't work", "issue_description": "ghost commented on Nov 4, 2015\nI recently tried to use scrapy to crawl my blog. Everything worked well until I came to rules. It didn't work and I did not know what was wrong. I followed the doc in scrapy crawlspider rules doc\nHere is the code:\n# -*- coding: utf8 -*-\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\n\nclass BlogSpider(CrawlSpider):\n    name = 'leonard-peng.blog'\n    allowed_domains = ['leonard-peng.github.io']\n    start_urls = ['http://leonard-peng.github.io/']\n    rules = (\n        # try to match '/2015/09/04/regex/'\n        Rule(LinkExtractor(allow=('/2015/09/04/regex/', )), callback=\"parse_item\"),\n        Rule(LinkExtractor(allow=('2015/09/04/regex/', )), callback=\"parse_item\"),\n        Rule(LinkExtractor(allow=(r'/\\d{4}/\\d{2}/\\d{2}/.*', )), callback=\"parse_item\"),\n    )\n\n    def parse(self, response):\n        print 'get root url data'\n\n    def parse_item(self, response):\n        print 'get article url data'\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2015-11-04T07:34:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1092": {"issue_url": "https://github.com/scrapy/scrapy/issues/1584", "issue_id": "#1584", "issue_summary": "Failing to execute scrapy command - Cryptography/OpenSSL issue", "issue_description": "uzairjawed commented on Nov 4, 2015\nHi this is my output of 'scrapy createproject tutorial' command\n$ scrapy createproject tutorial\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 7, in\nfrom scrapy.cmdline import execute\nFile \"/Library/Python/2.7/site-packages/scrapy/init.py\", line 48, in\nfrom scrapy.spiders import Spider\nFile \"/Library/Python/2.7/site-packages/scrapy/spiders/init.py\", line 10, in\nfrom scrapy.http import Request\nFile \"/Library/Python/2.7/site-packages/scrapy/http/init.py\", line 15, in\nfrom scrapy.http.response.html import HtmlResponse\nFile \"/Library/Python/2.7/site-packages/scrapy/http/response/html.py\", line 8, in\nfrom scrapy.http.response.text import TextResponse\nFile \"/Library/Python/2.7/site-packages/scrapy/http/response/text.py\", line 13, in\nfrom scrapy.utils.response import get_base_url\nFile \"/Library/Python/2.7/site-packages/scrapy/utils/response.py\", line 12, in\nfrom twisted.web import http\nFile \"/Library/Python/2.7/site-packages/twisted/web/http.py\", line 92, in\nfrom twisted.internet import interfaces, reactor, protocol, address\nFile \"/Library/Python/2.7/site-packages/twisted/internet/reactor.py\", line 38, in\nfrom twisted.internet import default\nFile \"/Library/Python/2.7/site-packages/twisted/internet/default.py\", line 56, in\ninstall = _getInstallFunction(platform)\nFile \"/Library/Python/2.7/site-packages/twisted/internet/default.py\", line 50, in _getInstallFunction\nfrom twisted.internet.selectreactor import install\nFile \"/Library/Python/2.7/site-packages/twisted/internet/selectreactor.py\", line 18, in\nfrom twisted.internet import posixbase\nFile \"/Library/Python/2.7/site-packages/twisted/internet/posixbase.py\", line 18, in\nfrom twisted.internet import error, udp, tcp\nFile \"/Library/Python/2.7/site-packages/twisted/internet/tcp.py\", line 29, in\nfrom twisted.internet._newtls import (\nFile \"/Library/Python/2.7/site-packages/twisted/internet/_newtls.py\", line 21, in\nfrom twisted.protocols.tls import TLSMemoryBIOFactory, TLSMemoryBIOProtocol\nFile \"/Library/Python/2.7/site-packages/twisted/protocols/tls.py\", line 41, in\nfrom OpenSSL.SSL import Error, ZeroReturnError, WantReadError\nFile \"/Library/Python/2.7/site-packages/OpenSSL/init.py\", line 8, in\nfrom OpenSSL import rand, crypto, SSL\nFile \"/Library/Python/2.7/site-packages/OpenSSL/rand.py\", line 11, in\nfrom OpenSSL._util import (\nFile \"/Library/Python/2.7/site-packages/OpenSSL/_util.py\", line 6, in\nfrom cryptography.hazmat.bindings.openssl.binding import Binding\nFile \"/Library/Python/2.7/site-packages/cryptography/hazmat/bindings/openssl/binding.py\", line 182, in\nBinding.init_static_locks()\nFile \"/Library/Python/2.7/site-packages/cryptography/hazmat/bindings/openssl/binding.py\", line 139, in init_static_locks\ncls._ensure_ffi_initialized()\nFile \"/Library/Python/2.7/site-packages/cryptography/hazmat/bindings/openssl/binding.py\", line 134, in _ensure_ffi_initialized\ncls._register_osrandom_engine()\nFile \"/Library/Python/2.7/site-packages/cryptography/hazmat/bindings/openssl/binding.py\", line 99, in _register_osrandom_engine\n_openssl_assert(cls.lib, cls.lib.ERR_peek_error() == 0)\nFile \"/Library/Python/2.7/site-packages/cryptography/hazmat/bindings/openssl/binding.py\", line 43, in _openssl_assert\nerrors\ncryptography.exceptions.InternalError: Unknown OpenSSL error. Please file an issue at https://github.com/pyca/cryptography/issues with information on how to reproduce this.", "issue_status": "Closed", "issue_reporting_time": "2015-11-04T06:31:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1093": {"issue_url": "https://github.com/scrapy/scrapy/issues/1578", "issue_id": "#1578", "issue_summary": "Implement a deprecation cycle for HTTP 400 responses handling", "issue_description": "Member\nkmike commented on Nov 2, 2015\n#1289 (and #1530) are backwards-incompatible. Should we revert this change and implement it in a backwards-compatible way, with a deprecation cycle?\nrevert default RETRY_HTTP_CODES to old value;\nadd RETRY_HTTP_CODES with new values to generated settings.py;\nraise a warning if users don\u2019t have RETRY_HTTP_CODES in their settings.py files, saying the default value will change in future releases.\nSee also: #1529", "issue_status": "Closed", "issue_reporting_time": "2015-11-02T10:40:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1094": {"issue_url": "https://github.com/scrapy/scrapy/issues/1577", "issue_id": "#1577", "issue_summary": "Streaming request with subprocess calls in pipeline", "issue_description": "ssbb commented on Nov 2, 2015\nHi!\nI want to download first 100-200kb of file for each scraped item (normal size is much bigger) and run subprocess call. Now I have this logic (inside celery): downloading X bytes of file (with requests/urllib), call ffprobe via subprocess, stop if file can be parsed. If not - get more X bytes and check again... Loop here.\nSo what you think if I will move this logic to Scrapy pipeline? I will have blocks?", "issue_status": "Closed", "issue_reporting_time": "2015-11-02T07:01:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1095": {"issue_url": "https://github.com/scrapy/scrapy/issues/1573", "issue_id": "#1573", "issue_summary": "'module' object has no attribute 'Filed'", "issue_description": "rylanchiu commented on Oct 30, 2015\nI am following the tutorial and when I ran the scrapy crawl lyric_spider, there is an error:\nAttributeError: 'module' object has no attribute 'Filed'\nThis is my items.py code:\nimport scrapy\nclass LyricItem(scrapy.Item):\nsinger = scrapy.Field()\ntitle = scrapy.Filed()\npublish_date = scrapy.Filed()\nword = scrapy.Filed()\nAnd this is my lyric_spider:\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractor import LinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\nfrom lyric.items import LyricItem\nclass LyricSpider(scrapy.Spider):\nname = \"lyric_spider\"\nallowed_domains = [\"littleoslo.com\"]\nstart_urls = [\n\"http://www.littleoslo.com/lyc/home/category/style/rap/\"\n]\nrules = (\nRule(LinkExtractor(allow = (r'page/\\d+')), follow = True),\n)\ndef parse(self, response):\n    for href in response.css(\"div > div > a::attr('href')\"):\n        url = response.urljoin(href.extract())\n        yield scrapy.Request(url, callback = self.parse_dir_contents)\n\ndef parse_dir_contents(self, response):\n    for sel in response.xpath('//div/h3/a'):\n        item = LyricItem()\n        link = sel.xpath('@href').extract()\n        yield item\nAnd my directory is exactly as the tutorial and the version of scrapy is 1.0.3\nAnyone can help me? Thanks a lot!!!", "issue_status": "Closed", "issue_reporting_time": "2015-10-30T16:11:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1096": {"issue_url": "https://github.com/scrapy/scrapy/issues/1571", "issue_id": "#1571", "issue_summary": "How to deal with the Chinese character in url", "issue_description": "rylanchiu commented on Oct 30, 2015\nI am following the link in the page to scrape the content in this page:\nhttp://www.littleoslo.com/lyc/home/category/style/rap/\nHowever, the url address of the songs contains url characters. I try to scrape the data using following code but failed:\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractor import LinkExtractor\nfrom scrapy.selector import HtmlXPathSelector\n\nclass LyricSpider(scrapy.Spider):\n     name = \"lyric_spider\"\n     allowed_domains = [\"littleoslo.com\"]\n     start_urls = [\n        \"http://www.littleoslo.com/lyc/home/category/style/rap/\"\n     ]\n     rules = (\n         Rule(LinkExtractor(allow = (r'page/\\d+')), follow = True),\n     )\n\n\n\n    def parse(self, response):\n        for href in response.css(\"div > div > a::attr('href')\"):\n            url = response.urljoin(href.extract())\n            yield scrapy.Request(url, callback = self.parse_dir_contents)\n\n    def parse_dir_contents(self, response):\n        for sel in response.xpath('//div/h3/a'):\n            item = LyricItem()\n            link = sel.xpath('@href').extract()\n            yield item\nThanks in advance!!!", "issue_status": "Closed", "issue_reporting_time": "2015-10-30T13:41:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1097": {"issue_url": "https://github.com/scrapy/scrapy/issues/1567", "issue_id": "#1567", "issue_summary": "Access settings from feed storages", "issue_description": "jersub commented on Oct 29, 2015\nHi,\nFeed storages should be able to access settings the new way.\nFor instance, S3FeedStorage uses both AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the settings. Currently they're gotten from scrapy.conf but this is now deprecated: this cause the build of PR #1559 to fail.\nWhat about adding settings to the constructor of storages?", "issue_status": "Closed", "issue_reporting_time": "2015-10-29T10:21:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1098": {"issue_url": "https://github.com/scrapy/scrapy/issues/1564", "issue_id": "#1564", "issue_summary": "[Bug] Incorrectly picked URL in `scrapy.linkextractors.regex.RegexLinkExtractor` when there is a `<base>` tag.", "issue_description": "Contributor\nstarrify commented on Oct 29, 2015\nIssue Description\nIncorrectly picked URL in scrapy.linkextractors.regex.RegexLinkExtractor when there is a <base> tag.\nHow to Reproduce the Issue & Version Used\n[pengyu@GLaDOS tmp]$ python2\nPython 2.7.10 (default, Sep  7 2015, 13:51:49) \n[GCC 5.2.0] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import scrapy\n>>> scrapy.__version__\nu'1.0.3'\n>>> html_body = '''\n... <html>\n...     <head>\n...         <base href=\"http://b.com/\">\n...     </head>\n...     <body>\n...         <a href=\"test.html\"></a>\n...     </body>\n... </html>\n... '''\n>>> response = scrapy.http.TextResponse(url='http://a.com/', body=html_body)\n>>> import scrapy.linkextractors.regex\n>>> scrapy.linkextractors.regex.RegexLinkExtractor().extract_links(response)\n__main__:1: ScrapyDeprecationWarning: SgmlLinkExtractor is deprecated and will be removed in future releases. Please use scrapy.linkextractors.LinkExtractor\n[Link(url='http://a.com/test.html', text=u'', fragment='', nofollow=False)]\nExpected Result\nURL of the extracted link shall start with 'http://b.com/'\nSuggested Fix\nThe issue can be fixed by editing a few lines in scrapy/linkextractors/regex.py", "issue_status": "Closed", "issue_reporting_time": "2015-10-29T06:35:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1099": {"issue_url": "https://github.com/scrapy/scrapy/issues/1562", "issue_id": "#1562", "issue_summary": "[Bug] Incorrectly picked URL in `scrapy.http.FormRequest.from_response` when there is a `<base>` tag", "issue_description": "Contributor\nstarrify commented on Oct 29, 2015\nIssue Description\nIncorrectly picked URL in scrapy.http.FormRequest.from_response when there is a <base> tag.\nHow to Reproduce the Issue & Version Used\n[pengyu@GLaDOS tmp]$ python2\nPython 2.7.10 (default, Sep  7 2015, 13:51:49) \n[GCC 5.2.0] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import scrapy\n>>> scrapy.__version__\nu'1.0.3'\n>>> html_body = '''\n... <html>\n...     <head>\n...         <base href=\"http://b.com/\">\n...     </head>\n...     <body>\n...         <form action=\"test_form\">\n...         </form>\n...     </body>\n... </html>\n... '''\n>>> response = scrapy.http.TextResponse(url='http://a.com/', body=html_body)\n>>> request = scrapy.http.FormRequest.from_response(response)\n>>> request.url\n'http://a.com/test_form'\nExpected Result\nrequest.url shall be 'http://b.com/test_form'\nSuggested Fix\nThe issue can be fixed by fixing a few lines in scrapy/http/request/form.py", "issue_status": "Closed", "issue_reporting_time": "2015-10-29T06:15:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1100": {"issue_url": "https://github.com/scrapy/scrapy/issues/1561", "issue_id": "#1561", "issue_summary": "Overriding `CLOSESPIDER_PAGECOUNT` settings within Spider does not get honored", "issue_description": "deanq commented on Oct 29, 2015\nIs this one of those settings that cannot be overridden per Spider instance? Is it finalized before instantiation?\nclass SiteSpider(CrawlSpider):\n\n    def __init__(self, **kw):\n        settings.set('CLOSESPIDER_PAGECOUNT', kw.get('maxpages', 100), priority='cmdline')\n        super(SiteSpider, self).__init__(**kw)\n...\nWhat am I missing or is this even possible?", "issue_status": "Closed", "issue_reporting_time": "2015-10-29T06:03:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1101": {"issue_url": "https://github.com/scrapy/scrapy/issues/1558", "issue_id": "#1558", "issue_summary": "AttributeError: 'Settings' object has no attribute 'update_settings'", "issue_description": "mbudge commented on Oct 28, 2015\nHi,\nI'm trying to run a custom scrapy spider from a python script, however I can't figure out how to do this from the scrapy docs.\nThis page appears to be out of date otherwise it's in need of a few examples http://doc.scrapy.org/en/latest/topics/api.html?highlight=api\nfrom spiders.scrapehomepagelinksspider import scrapehomepagelinksspider\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy.crawler import Crawler\nfrom scrapy.xlib.pydispatch import dispatcher\nfrom scrapy import signals\nfrom twisted.internet import reactor\ndef stop_reactor():\nreactor.stop()\ndispatcher.connect(stop_reactor, signal=signals.spider_closed)\nsettings = get_project_settings()\nspider = scrapehomepagelinksspider('www.yahoo.com')\ncrawler = Crawler(settings)\ncrawler.crawl(spider)\nlinks = reactor.run()\nprint links\nError\npython scrapy1.py\nTraceback (most recent call last):\nFile \"scrapy1.py\", line 26, in\ncrawler = Crawler(settings)\nFile \"/home/matt/.local/lib/python2.7/site-packages/scrapy/crawler.py\", line 32, in init\nself.spidercls.update_settings(self.settings)\nAttributeError: 'Settings' object has no attribute 'update_settings'\nThanks", "issue_status": "Closed", "issue_reporting_time": "2015-10-28T09:16:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1102": {"issue_url": "https://github.com/scrapy/scrapy/issues/1557", "issue_id": "#1557", "issue_summary": "ImportError: No module named crawler", "issue_description": "mbudge commented on Oct 26, 2015\nHi,\nI've written a python script were I'm trying to invoke scrapy but I keep getting an import error.\nI'm following this guide http://doc.scrapy.org/en/latest/topics/practices.html\nThe script is stored in the projects folder.\nScript:\n\"from scrapy.crawler import Crawler\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\nprocess = CrawlerProcess(get_project_settings())\nprocess.crawl('linksspider', domain='scrapinghub.com')\nprocess.start()\"\nTraceback:\nTraceback (most recent call last):\nFile \"scrapy.py\", line 3, in\nfrom scrapy.crawler import Crawler\nFile \"......\", line 3, in\nfrom scrapy.crawler import Crawler\nImportError: No module named crawler\nI can't find the answer on google/SA/Github.\nCan anyone help?\nThanks", "issue_status": "Closed", "issue_reporting_time": "2015-10-26T17:55:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1103": {"issue_url": "https://github.com/scrapy/scrapy/issues/1556", "issue_id": "#1556", "issue_summary": "`tox` tests fail", "issue_description": "Contributor\ndarshanime commented on Oct 26, 2015\nI tried running the tests for Scrapy using $ tox but I get a compilation error. Here is the log.", "issue_status": "Closed", "issue_reporting_time": "2015-10-26T10:23:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1104": {"issue_url": "https://github.com/scrapy/scrapy/issues/1553", "issue_id": "#1553", "issue_summary": "Crawl each host only N times", "issue_description": "herkyl commented on Oct 25, 2015\nI'm using CrawlSpider to crawl the entire web looking for sites that use a certain script tag. I would like to limit the amount of requests Scrapy makes per host. Right now when I get stuck on something like FourSquare, Scrapy easily makes thousands of requests on the same host.", "issue_status": "Closed", "issue_reporting_time": "2015-10-25T17:11:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1105": {"issue_url": "https://github.com/scrapy/scrapy/issues/1552", "issue_id": "#1552", "issue_summary": "How to create custom Scrapy Item Exporter?", "issue_description": "bnussey commented on Oct 23, 2015\nI'm trying to create a custom Scrapy Item Exporter based off JsonLinesItemExporter so I can slightly alter the structure it produces.\nI have read the documentation here http://doc.scrapy.org/en/latest/topics/exporters.html but it doesn't state how to create a custom exporter, where to store it or how to link it to your Pipeline.\nI have identified how to go custom with the Feed Exporters but this is not going to suit my requirements, as I want to call this exporter from my Pipeline.\nHere is the code I've come up with which has been stored in a file in the root of the project called exporters.py\nfrom scrapy.contrib.exporter import JsonLinesItemExporter\n\nclass FanItemExporter(JsonLinesItemExporter):\n\ndef __init__(self, file, **kwargs):\n    self._configure(kwargs, dont_fail=True)\n    self.file = file\n    self.encoder = ScrapyJSONEncoder(**kwargs)\n    self.first_item = True\n\ndef start_exporting(self):\n    self.file.write(\"\"\"{\n'product': [\"\"\")\n\ndef finish_exporting(self):\n    self.file.write(\"]}\")\n\ndef export_item(self, item):\n    if self.first_item:\n        self.first_item = False\n    else:\n        self.file.write(',\\n')\n    itemdict = dict(self._get_serialized_fields(item))\n    self.file.write(self.encoder.encode(itemdict))\nI have simply tried calling this from my pipeline by using FanItemExporter and trying variations of the import but it's not resulting in anything.", "issue_status": "Closed", "issue_reporting_time": "2015-10-22T21:25:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1106": {"issue_url": "https://github.com/scrapy/scrapy/issues/1550", "issue_id": "#1550", "issue_summary": "Running scrapy shell against a local file", "issue_description": "alecxe commented on Oct 20, 2015\nBefore Scrapy 1.0, I could execute:\n scrapy shell index.html\nIn >=1.0, it started to throw ValueError: Missing scheme in request url: index.html:\n$ scrapy shell index.html\n2015-10-12 15:32:59 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n2015-10-12 15:32:59 [scrapy] INFO: Optional features available: ssl, http11, boto\n2015-10-12 15:32:59 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0}\nTraceback (most recent call last):\n  File \"/Users/user/.virtualenvs/so/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/Users/user/.virtualenvs/so/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/user/.virtualenvs/so/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/user/.virtualenvs/so/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/user/.virtualenvs/so/lib/python2.7/site-packages/scrapy/commands/shell.py\", line 50, in run\n    spidercls = spidercls_for_request(spider_loader, Request(url),\n  File \"/Users/user/.virtualenvs/so/lib/python2.7/site-packages/scrapy/http/request/__init__.py\", line 24, in __init__\n    self._set_url(url)\n  File \"/Users/user/.virtualenvs/so/lib/python2.7/site-packages/scrapy/http/request/__init__.py\", line 59, in _set_url\n    raise ValueError('Missing scheme in request url: %s' % self._url)\nValueError: Missing scheme in request url: index.html \nAs a workaround, I've used the \"file\" protocol providing the full path to a file:\n$ scrapy shell file:////absolute/path/to/index.html\nFrom a comment to the relevant SO topic http://stackoverflow.com/questions/33088877/scrapy-shell-against-a-local-file, we can see that the relevant change was introduced here.\nWould it be possible and would it make sense to bring the previous behavior back so that we can execute the shell against a local file as easy as scrapy shell filename?\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2015-10-19T20:35:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1107": {"issue_url": "https://github.com/scrapy/scrapy/issues/1549", "issue_id": "#1549", "issue_summary": "use scrapy with celery producing incorrect log", "issue_description": "night1008 commented on Oct 19, 2015\nhello, I use scrapy with celery, and I have already config the scrapy and celery log level, for example, --loglevel=error, but it always produce the debug log to stdout or file, I look the source code, it come from the scrapy/scrapy/logformatter.py, it's not my want.Can you tell me what it happend?\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2015-10-19T09:27:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1108": {"issue_url": "https://github.com/scrapy/scrapy/issues/1547", "issue_id": "#1547", "issue_summary": "Failed to get a link the text of which matches my rules", "issue_description": "hellojammy commented on Oct 17, 2015\nMy code :\nclass SpiderLtuSpider(CrawlSpider):\n    name = \"spider_ltu\"\n    start_urls = []\n\n    def __init__(self):\n        self.start_urls.append(\"http://chuansong.me/account/lengtoo?start=0\")\n        self.rules = [ Rule(sle(allow=(\"http://chuansong.me/n/\\d*.*\", ), restrict_xpaths=('//a[@class=\"question_link\"]'), ), callback='parse_article', ),]\nI want to get the link, the text of which matches the regular expression(e.g.'Text*'). The first and the third one blow are what I want, and the second one should be excluded.\n<a class=\"question_link\" href=\"/n/1818306\" target=\"_blank\">Test_Ok hello</a> \n<a class=\"question_link\" href=\"/n/1818306\" target=\"_blank\">AAA</a>\n<a class=\"question_link\" href=\"/n/1818306\" target=\"_blank\">Test ok</a>\nAnd I want to match the rules by the XPath restrict_xpaths. Would you please correct the code ? Thank you!", "issue_status": "Closed", "issue_reporting_time": "2015-10-17T16:40:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1109": {"issue_url": "https://github.com/scrapy/scrapy/issues/1546", "issue_id": "#1546", "issue_summary": "scrapy startproject tutorial fails in Jupyter with \"SyntaxError: invalid syntax", "issue_description": "datakid commented on Oct 17, 2015\nFor some reason I could only get start project to work in cli, not in jupyter?", "issue_status": "Closed", "issue_reporting_time": "2015-10-17T05:24:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1110": {"issue_url": "https://github.com/scrapy/scrapy/issues/1542", "issue_id": "#1542", "issue_summary": "pip install Scrapy, no scrapy in Scripts", "issue_description": "stcalica commented on Oct 15, 2015\nHaving a problem doing this command:\npython scrapy startproject foo\nget this error:\npython: can't open file 'scrapy': [Errno 2] No such file or directory\nI looked in C:\\Python27\\Scripts and could not find scrapy file. Pip is not installing it right I believe.\nI uninstalled and installed and upgraded. Nothing works.\nI am on Windows 7 64-bit using python 2.7\n`C:\\Users\\kacalica>pip install scrapy\nRequirement already satisfied (use --upgrade to upgrade): scrapy in c:\\python27\nlib\\site-packages\nRequirement already satisfied (use --upgrade to upgrade): cssselect>=0.9 in c:\\p\nython27\\lib\\site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): queuelib in c:\\python2\n7\\lib\\site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pyOpenSSL in c:\\python\n27\\lib\\site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): lxml in c:\\python27\\li\nb\\site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.5.2 in c:\\pytho\nn27\\lib\\site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): w3lib>=1.2 in c:\\pytho\nn27\\lib\\site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): Twisted>=10.0.0 in c:\npython27\\lib\\site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cryptography>=0.7 in c\n:\\python27\\lib\\site-packages (from pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): zope.interface>=3.6.0\nin c:\\python27\\lib\\site-packages (from Twisted>=10.0.0->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in c:\\pytho\nn27\\lib\\site-packages (from cryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): enum34 in c:\\python27\nlib\\site-packages (from cryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): ipaddress in c:\\python\n27\\lib\\site-packages (from cryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pyasn1>=0.1.8 in c:\\py\nthon27\\lib\\site-packages (from cryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): idna>=2.0 in c:\\python\n27\\lib\\site-packages (from cryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cffi>=1.1.0 in c:\\pyth\non27\\lib\\site-packages (from cryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pycparser in c:\\python\n27\\lib\\site-packages (from cffi>=1.1.0->cryptography>=0.7->pyOpenSSL->scrapy)\nC:\\Users\\kacalica>pip install -U scrapy\nCollecting scrapy\nUsing cached Scrapy-1.0.3-py2-none-any.whl\nc:\\python27\\lib\\site-packages\\pip_vendor\\requests\\packages\\urllib3\\util\\ssl_.py\n:90: InsecurePlatformWarning: A true SSLContext object is not available. This pr\nevents urllib3 from configuring SSL appropriately and may cause certain SSL conn\nections to fail. For more information, see https://urllib3.readthedocs.org/en/la\ntest/security.html#insecureplatformwarning.\nInsecurePlatformWarning\nRequirement already up-to-date: cssselect>=0.9 in c:\\python27\\lib\\site-packages\n(from scrapy)\nRequirement already up-to-date: queuelib in c:\\python27\\lib\\site-packages (from\nscrapy)\nRequirement already up-to-date: pyOpenSSL in c:\\python27\\lib\\site-packages (from\nscrapy)\nRequirement already up-to-date: w3lib>=1.8.0 in c:\\python27\\lib\\site-packages (f\nrom scrapy)\nRequirement already up-to-date: lxml in c:\\python27\\lib\\site-packages (from scra\npy)\nRequirement already up-to-date: Twisted>=10.0.0 in c:\\python27\\lib\\site-packages\n(from scrapy)\nRequirement already up-to-date: six>=1.5.2 in c:\\python27\\lib\\site-packages (fro\nm scrapy)\nRequirement already up-to-date: service-identity in c:\\python27\\lib\\site-package\ns (from scrapy)\nRequirement already up-to-date: cryptography>=0.7 in c:\\python27\\lib\\site-packag\nes (from pyOpenSSL->scrapy)\nRequirement already up-to-date: zope.interface>=3.6.0 in c:\\python27\\lib\\site-pa\nckages (from Twisted>=10.0.0->scrapy)\nRequirement already up-to-date: characteristic>=14.0.0 in c:\\python27\\lib\\site-p\nackages (from service-identity->scrapy)\nRequirement already up-to-date: pyasn1-modules in c:\\python27\\lib\\site-packages\n(from service-identity->scrapy)\nRequirement already up-to-date: pyasn1 in c:\\python27\\lib\\site-packages (from se\nrvice-identity->scrapy)\nCollecting setuptools (from cryptography>=0.7->pyOpenSSL->scrapy)\nDownloading setuptools-18.4-py2.py3-none-any.whl (462kB)\n100% |################################| 462kB 999kB/s\nRequirement already up-to-date: enum34 in c:\\python27\\lib\\site-packages (from cr\nyptography>=0.7->pyOpenSSL->scrapy)\nRequirement already up-to-date: ipaddress in c:\\python27\\lib\\site-packages (from\ncryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already up-to-date: idna>=2.0 in c:\\python27\\lib\\site-packages (from\ncryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already up-to-date: cffi>=1.1.0 in c:\\python27\\lib\\site-packages (fr\nom cryptography>=0.7->pyOpenSSL->scrapy)\nRequirement already up-to-date: pycparser in c:\\python27\\lib\\site-packages (from\ncffi>=1.1.0->cryptography>=0.7->pyOpenSSL->scrapy)\nInstalling collected packages: scrapy, setuptools\nFound existing installation: Scrapy 0.25.1\nUninstalling Scrapy-0.25.1:\nSuccessfully uninstalled Scrapy-0.25.1\nFound existing installation: setuptools 18.3.2\nUninstalling setuptools-18.3.2:\nSuccessfully uninstalled setuptools-18.3.2\nSuccessfully installed scrapy-1.0.3 setuptools-18.4\nC:\\Users\\kacalica>python scrapy startproject foo\npython: can't open file 'scrapy': [Errno 2] No such file or directory`", "issue_status": "Closed", "issue_reporting_time": "2015-10-14T21:58:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1111": {"issue_url": "https://github.com/scrapy/scrapy/issues/1540", "issue_id": "#1540", "issue_summary": "ImportError: No module named scrapy", "issue_description": "rylanchiu commented on Oct 13, 2015\nI installed the newest version of scrapy. But when I open the ipython and import the scrapy, an error occurs: ImportError: No module named scrapy. It seems that the scrapy can not be found. How can I solve this problem? Thank you very much!", "issue_status": "Closed", "issue_reporting_time": "2015-10-13T09:38:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1112": {"issue_url": "https://github.com/scrapy/scrapy/issues/1539", "issue_id": "#1539", "issue_summary": "Incompatible library version", "issue_description": "rylanchiu commented on Oct 13, 2015\nI installed the scrapy successfully. But when I try to start a project: scrapy startproject helloworld, an error occur:\nReason: Incompatible library version: etree.so requires version 12.0.0 or later, but libxml2.2.dylib provides version 10.0.0\nI have download the newest version of lxml already. But the error still exits.", "issue_status": "Closed", "issue_reporting_time": "2015-10-13T08:43:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1113": {"issue_url": "https://github.com/scrapy/scrapy/issues/1538", "issue_id": "#1538", "issue_summary": "Get \"[scrapy] DEBUG: Redirecting (301)\" ERROR, HOW TO FIX?", "issue_description": "hellojammy commented on Oct 12, 2015\n2015-10-13 00:29:12 [scrapy] DEBUG: Redirecting (301) to < GET http://www.guokr.com/search/article/?&page=1&wd=china > from < GET http://guokr.com/search/article/?&page=1&wd=china >\nYou see, it was redirecting from one url to another url ,and they are the same!! WHY??\nI use a CustomUserAgentMiddleware to change the user-agent", "issue_status": "Closed", "issue_reporting_time": "2015-10-12T16:37:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1114": {"issue_url": "https://github.com/scrapy/scrapy/issues/1536", "issue_id": "#1536", "issue_summary": "PY3 enable test_commands.ParseCommandTest", "issue_description": "Member\nkmike commented on Oct 12, 2015\nTests for 'parse' command are disabled in Python 3 because they require HTTP download handler in Python 3. See also: #1535, #1455.", "issue_status": "Closed", "issue_reporting_time": "2015-10-12T13:11:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1115": {"issue_url": "https://github.com/scrapy/scrapy/issues/1534", "issue_id": "#1534", "issue_summary": "Unable to stop logs from getting printed on console", "issue_description": "rrrazdan commented on Oct 7, 2015\nHi Guys,\nI am unable to stop logs from getting printed on console for ubuntu,I haven't defined any logger myself, and for logging i am using python default logging. But I am unable to suppress the logs from getting printed on the console. I have tried every setting and every command line parameter.\nScrapy==1.0.3\nTwisted==15.3.0\nOS: ubuntu 12.04, OSX 10.10.2\nPython: 2.7\ncommand: scrapy crawl test -L ERROR <--- This prints everything\nThe logs come from scrapy.core.engine and scrapy.core.scrapper", "issue_status": "Closed", "issue_reporting_time": "2015-10-07T16:28:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1116": {"issue_url": "https://github.com/scrapy/scrapy/issues/1531", "issue_id": "#1531", "issue_summary": "remove --lsprof option and bundled lsprofcalltree.py library", "issue_description": "Member\nkmike commented on Oct 7, 2015\nWhat about removing --lsprof option and the bundled lsprofcalltree.py library? They are untested (see #1417), undocumented, don't work in Python 3, and there are easier to setup profiler UIs than KCacheGrind (e.g. https://github.com/jiffyclub/snakeviz) which work with .prof files directly.\nI think we don't even have to deprecate it because removing this feature can't break user code.", "issue_status": "Closed", "issue_reporting_time": "2015-10-06T20:30:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1117": {"issue_url": "https://github.com/scrapy/scrapy/issues/1529", "issue_id": "#1529", "issue_summary": "make Scrapy 1.1 release", "issue_description": "Member\nkmike commented on Oct 6, 2015\nWhat about making a \"boring\" Scrapy 1.1 release, without much new features? User-facing improvements are small (deferreds in downloader middlewares, small scrapy shell improvements, AUTOTHROTTLE_TARGET_CONCURRENCY, docs improvements, loader.nested_xpath/nested_css, this kind of things), but internal changes are big: parsel, Python 3 porting progress. This makes backporting fixes to 1.0.x harder. By making 1.1 release we\u2019ll be able to forget about 1.0.x codebase and let all the internal changes settle down.", "issue_status": "Closed", "issue_reporting_time": "2015-10-06T12:52:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1118": {"issue_url": "https://github.com/scrapy/scrapy/issues/1526", "issue_id": "#1526", "issue_summary": "json file output: sometimes closing bracket is not written?", "issue_description": "julienaubert commented on Oct 6, 2015\nscrapy 1.0.3\ninvoked: scrapy runspider forumscrapers/spiders/myspider.py --pdb -o test.json\n2015-10-06 12:16:16 [scrapy] INFO: Closing spider (finished)\n2015-10-06 12:16:16 [scrapy] INFO: Stored json feed (14829 items) in: test.json\nI briefly checked the scrapy code for 1.0.3, I don't see any obvious issue. The close spider is signaled and the success callback of store is called (since I get the log output, line 183 in feedexport.py).\nMy spider is not using any custom pipelines, it has default settings (autothrottle is enabled though).\nThis has occurred a few times - sometimes it works sometimes it doesn't. (no other process has touched test.json).\nAny ideas?", "issue_status": "Closed", "issue_reporting_time": "2015-10-06T10:50:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1119": {"issue_url": "https://github.com/scrapy/scrapy/issues/1525", "issue_id": "#1525", "issue_summary": "Is there a way to yield all of my requests and save the responses so I can iterate through all of the responses at once.", "issue_description": "sirskitzo commented on Oct 6, 2015\nThis script finds all of the links on a website that match a certain keyword. If a URL is a match, I would like to yield a request and save the responses for each url. Once all of them have been crawled, I would then like to iterate through a dict, or list, of the responses and find the email and phone number that is used the most time throughout a website. I think I have it setup as I like, but the output is yielding an empty response each time.\nHere is my attempt (Four methods; the remaining ones are just helpers): http://pastebin.com/wLb9FguG\nThis code works, but the response item is always from the previous website for some reason. I can't figure it out.", "issue_status": "Closed", "issue_reporting_time": "2015-10-05T20:14:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1120": {"issue_url": "https://github.com/scrapy/scrapy/issues/1521", "issue_id": "#1521", "issue_summary": "signals docs are confusing", "issue_description": "Member\nkmike commented on Oct 3, 2015\nIt seems it is not explained how to connect a callback to a singnal anywhere in Scrapy docs.\nhttp://doc.scrapy.org/en/latest/topics/signals.html tells:\nYou can connect to signals (or send your own) through the Signals API.\nBut if you follow this link you get docs for scrapy.signalmanager.SignalManager - that's fine, but it is not explained where to get a SignalManager instance from.\nThere is an example in Extension docs (http://doc.scrapy.org/en/latest/topics/extensions.html#sample-extension), but\na) this is just an example;\nb) it is not explained that crawler.signals is a SignalManager instance;\nc) this example is neither in Signals docs nor in SignalManager docs.\nThere is also a bit of information here: http://doc.scrapy.org/en/latest/topics/api.html#scrapy.crawler.Crawler.signals, but\na) it is not linked to neither from Signal docs nor from SignalManager, so you can't find it if you don't know about it already;\nb) it is not explained that crawler.signals is the only way to access signals.\nSo in the end users may get some luck connecting signals if they start from Crawler docs, but almost no luck if they start from Signals docs.", "issue_status": "Closed", "issue_reporting_time": "2015-10-03T00:45:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1121": {"issue_url": "https://github.com/scrapy/scrapy/issues/1520", "issue_id": "#1520", "issue_summary": "Move scrapy.telnet to scrapy.extensions.telnet", "issue_description": "Member\nkmike commented on Oct 3, 2015\nTelnetConsole is an extension, why isn't it in scrapy/extensions?", "issue_status": "Closed", "issue_reporting_time": "2015-10-02T22:41:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1122": {"issue_url": "https://github.com/scrapy/scrapy/issues/1517", "issue_id": "#1517", "issue_summary": "In toys select any category infinte loading time. showing 404 not found.", "issue_description": "harshmaur commented on Oct 1, 2015\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2015-10-01T13:33:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1123": {"issue_url": "https://github.com/scrapy/scrapy/issues/1515", "issue_id": "#1515", "issue_summary": "Document scrapy.utils.url.canonicalize_url", "issue_description": "Contributor\nmgedmin commented on Sep 29, 2015\nhttp://doc.scrapy.org/en/1.0/topics/link-extractors.html#module-scrapy.linkextractors.lxmlhtml says\nIt is unclear to me what \"canonicalize\" means. This ought to be documented somewhere.\nI initially tried to submit a pull request (#1514) to fix this, but it turns out to be more complicated than I expected.", "issue_status": "Closed", "issue_reporting_time": "2015-09-29T08:05:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1124": {"issue_url": "https://github.com/scrapy/scrapy/issues/1507", "issue_id": "#1507", "issue_summary": "custom \"CsvItemExporter\" doesn't support 'from_settings' ?", "issue_description": "jschilling1 commented on Sep 23, 2015\nI used to access settings in my custom exporter using \"from scrapy.conf import settings\" which is now depreciated. 'from_settings' and 'from_crawler' don't work there what should i do?\nclass CustomCsvItemExporter(CsvItemExporter):\n    def __init__(self, *args, **kwargs):\n        super(CustomCsvItemExporter, self).__init__(*args, **kwargs)\n    @classmethod\n    def from_settings(cls, settings):\n        print ('from_settings ran')\n        return cls(settings)", "issue_status": "Closed", "issue_reporting_time": "2015-09-23T15:38:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1125": {"issue_url": "https://github.com/scrapy/scrapy/issues/1506", "issue_id": "#1506", "issue_summary": "debian shell completions aren't installed via pip", "issue_description": "jschilling1 commented on Sep 22, 2015\nit'd be nice if zsh completion worked for pip installation in ubuntu", "issue_status": "Closed", "issue_reporting_time": "2015-09-22T02:03:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1126": {"issue_url": "https://github.com/scrapy/scrapy/issues/1502", "issue_id": "#1502", "issue_summary": "Question: lxml or scrapy selector", "issue_description": "mirzadelic commented on Sep 17, 2015\nShould i use lxml library or scrapy.selector.Selector ?\nlxml is fast, but how it is compared to scrapy.selector.Selector?", "issue_status": "Closed", "issue_reporting_time": "2015-09-17T09:03:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1127": {"issue_url": "https://github.com/scrapy/scrapy/issues/1500", "issue_id": "#1500", "issue_summary": "how can know the number of pending requests ?", "issue_description": "hansenDise commented on Sep 17, 2015\nhow can know the number of pending requests ? or can we rerange the requests in queue?", "issue_status": "Closed", "issue_reporting_time": "2015-09-17T02:32:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1128": {"issue_url": "https://github.com/scrapy/scrapy/issues/1495", "issue_id": "#1495", "issue_summary": "deprecate Spider.make_requests_from_url", "issue_description": "Member\nkmike commented on Sep 13, 2015\nHey,\nSpider.make_requests_from_url is a shortcut for a single use case: sending initial requests from a spider with start_urls attribute. It saves a single line of code (for url in start_urls) in one specific use case, but yet we have to document it, users have to read about it and understand what it is for.\nThe name suggests it is more general - but we can't yield Link objects and get requests created by make_requests_from_url, it does nothing for CrawlSpider (I'd expect URLs to be passed through this method), and it is ignored for SitemapSpider. This is inconsistent.\nWhat about deprecating make_requests_from_url and removing it from docs? IMHO it will make API simpler, this hook does nothing useful now, and if we ever want a general 'process URL and get a Request' method we'd have to use another name for backwards compatibility anyways.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2015-09-13T15:12:15Z", "fixed_by": "#1728", "pull_request_summary": "deprecate Spider.make_requests_from_url.", "pull_request_description": "Member\nkmike commented on Jan 27, 2016 \u2022\nedited\nFixes #1495.\nTodo:\nimplementation is incorrect; fix it\ntests", "pull_request_status": "Merged", "issue_fixed_time": "2017-02-20T14:23:50Z", "files_changed": [["26", "docs/topics/spiders.rst"], ["18", "scrapy/spiders/__init__.py"], ["32", "scrapy/utils/deprecate.py"], ["5", "tests/spiders.py"], ["4", "tests/test_engine.py"], ["29", "tests/test_spider.py"]]}, "1129": {"issue_url": "https://github.com/scrapy/scrapy/issues/1493", "issue_id": "#1493", "issue_summary": "Copying items not work correctly with list values", "issue_description": "ils78 commented on Sep 12, 2015\nitem = Item()\nitem['some_field'] = []\nc_item = Item(item)\nc_item['some_field'].append(1)\nprint item['some_field'], c_item['some_field']\n[1] [1]", "issue_status": "Closed", "issue_reporting_time": "2015-09-12T08:24:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1130": {"issue_url": "https://github.com/scrapy/scrapy/issues/1490", "issue_id": "#1490", "issue_summary": "ImportError: No module named spiders", "issue_description": "vazeer commented on Sep 11, 2015\nfrom scrapy.spiders import CrawlSpider, Rule\ni have Scrapy 0.24.5\nPython 2.7.6", "issue_status": "Closed", "issue_reporting_time": "2015-09-11T13:48:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1131": {"issue_url": "https://github.com/scrapy/scrapy/issues/1489", "issue_id": "#1489", "issue_summary": "Scrapy - Retrieve spider object in dupefilter", "issue_description": "stejesh commented on Sep 11, 2015\nThis is scrapy's default Dupefilter class method request_seen\nclass RFPDupeFilter(BaseDupeFilter):\n\n    def request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)\nWhile implementing a custom dupefilter. i cannot retrieve the spider object from this class unlike other scrapy middleware\nIs there any way i can know which spider object this is? so i can customize it via a spider on spider basis?\nAlso i cannot just implement a middleware which reads urls and puts it into a list & checks duplicates instead of a custom dupefilter. This is because i need to pause/resume crawls and need scrapy to store the request fingerprint by default using the JOBDIR setting", "issue_status": "Closed", "issue_reporting_time": "2015-09-11T10:12:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1132": {"issue_url": "https://github.com/scrapy/scrapy/issues/1487", "issue_id": "#1487", "issue_summary": "Set `scrapy shell name.tld` default scheme to http", "issue_description": "umrashrf commented on Sep 10, 2015\nI propose default scheme for sites be set to http:// when using scrapy shell. Like how browsers work.\nscrapy shell yahoo.com fails but should work.\nissue label = trivial", "issue_status": "Closed", "issue_reporting_time": "2015-09-10T16:26:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1133": {"issue_url": "https://github.com/scrapy/scrapy/issues/1486", "issue_id": "#1486", "issue_summary": "twisted.web._newclient.ResponseNeverReceived:", "issue_description": "VMRuiz commented on Sep 9, 2015\nHello, When I try to load this website https://www.notjustalabel.com/products/womens I get this error:\n2015-09-09 16:51:16 [scrapy] INFO: Spider opened\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/home/victor/.local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/victor/.local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/home/victor/.local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/home/victor/.local/lib/python2.7/site-packages/scrapy/commands/shell.py\", line 63, in run\n    shell.start(url=url)\n  File \"/home/victor/.local/lib/python2.7/site-packages/scrapy/shell.py\", line 44, in start\n    self.fetch(url, spider)\n  File \"/home/victor/.local/lib/python2.7/site-packages/scrapy/shell.py\", line 87, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<string>\", line 2, in raiseException\ntwisted.web._newclient.ResponseNeverReceived: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_READ_BYTES', 'ssl handshake failure')]>]\nI have also checked that TLSv1 can negociate this cert with curl and everything is fine\ncurl --tlsv1 -k https://www.notjustalabel.com/products/womens\nThis is my scrapy deployment:\n\u279c  mencanta-spiders git:(master) \u2717 scrapy version -v\n2015-09-09 16:55:31 [scrapy] INFO: Scrapy 1.0.3 started (bot: mencanta)\n2015-09-09 16:55:31 [scrapy] INFO: Optional features available: ssl, http11, boto\n2015-09-09 16:55:31 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'mencanta.spiders', 'LOG_LEVEL': 'INFO', 'HTTPCACHE_EXPIRATION_SECS': 604800, 'HTTPCACHE_IGNORE_HTTP_CODES': [301, 302, 307, 403, 404, 401, 400, 402, 407, 500], 'SPIDER_MODULES': ['mencanta.spiders'], 'HTTPCACHE_ENABLED': True, 'BOT_NAME': 'mencanta', 'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.85 Safari/537.36'}\nScrapy  : 1.0.3\nlxml    : 3.4.4.0\nlibxml2 : 2.9.2\nTwisted : 15.4.0\nPython  : 2.7.9 (default, Apr  2 2015, 15:33:21) - [GCC 4.9.2]\nPlatform: Linux-3.19.0-26-generic-x86_64-with-Ubuntu-15.04-vivid", "issue_status": "Closed", "issue_reporting_time": "2015-09-09T14:57:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1134": {"issue_url": "https://github.com/scrapy/scrapy/issues/1485", "issue_id": "#1485", "issue_summary": "Error with pip install scrapy", "issue_description": "d3prof3t commented on Sep 8, 2015\nI ran the following command on my py2.7.10-devel ; pip install scrapy.\nIt didn't run completely and throwed errors as follows:\nFailed building wheel for wheel\nFailed building wheel for cryptography\nFailed building wheel for lxml\nFailed building wheel for cffi\nI'm posting a paste of my error log, check it here: http://pastie.org/10404586\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2015-09-08T06:50:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1135": {"issue_url": "https://github.com/scrapy/scrapy/issues/1483", "issue_id": "#1483", "issue_summary": "meta not passed to ftpclient when using make_request_from_url", "issue_description": "eyurtsev commented on Sep 8, 2015\nI apologize in advance if I misunderstood the API, but I suspect that there's a bug with passing meta information to the FTPClient.\nI was successful in following this post for crawling a public ftp server: http://stackoverflow.com/questions/27770610/using-scrapy-to-crawl-a-public-ftp-server\nIn this post, the spider uses start_requests. So this kind of construction works:\n    def start_requests(self):\n        urls = self.start_urls\n        for url in urls:\n            yield Request(url, meta={'ftp_user': 'anonymous', 'ftp_password': ''})\n(That works successfully.)\nHowever, if instead of using start_requests, I use make_request_from_url:\n    def make_request_from_url(self, url):\n        yield Request(url, meta={'ftp_user': 'anonymous', 'ftp_password': ''})\nThen, I get the following error message:\n'''\npython2.7/site-packages/scrapy/core/downloader/handlers/ftp.py\", line 72, in download_request\ncreator = ClientCreator(reactor, FTPClient, request.meta[\"ftp_user\"],\nKeyError: 'ftp_user'\n'''", "issue_status": "Closed", "issue_reporting_time": "2015-09-07T19:45:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1136": {"issue_url": "https://github.com/scrapy/scrapy/issues/1482", "issue_id": "#1482", "issue_summary": "Dependency issues on OSX 10.10.5", "issue_description": "sebiwi commented on Sep 7, 2015\nHi, I installed scrapy using pip and virtualenv.\nEverything seems to go fine, but as I try to do anything I get this error:\nTraceback (most recent call last):\n  File \"/Users/seb/stuff/scrapy/scrapy/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/Users/seb/stuff/scrapy/scrapy/lib/python2.7/site-packages/scrapy/__init__.py\", line 48, in <module>\n    from scrapy.spiders import Spider\n  File \"/Users/seb/stuff/scrapy/scrapy/lib/python2.7/site-packages/scrapy/spiders/__init__.py\", line 10, in <module>\n    from scrapy.http import Request\n  File \"/Users/seb/stuff/scrapy/scrapy/lib/python2.7/site-packages/scrapy/http/__init__.py\", line 11, in <module>\n    from scrapy.http.request.form import FormRequest\n  File \"/Users/seb/stuff/scrapy/scrapy/lib/python2.7/site-packages/scrapy/http/request/form.py\", line 9, in <module>\n    import lxml.html\n  File \"/Users/seb/stuff/scrapy/scrapy/lib/python2.7/site-packages/lxml/html/__init__.py\", line 42, in <module>\n    from lxml import etree\nImportError: dlopen(/Users/seb/stuff/scrapy/scrapy/lib/python2.7/site-packages/lxml/etree.so, 2): Library not loaded: libxml2.2.dylib\n  Referenced from: /Users/seb/stuff/scrapy/scrapy/lib/python2.7/site-packages/lxml/etree.so\n  Reason: Incompatible library version: etree.so requires version 12.0.0 or later, but libxml2.2.dylib provides version 10.0.0\nI tried updating lxml using pip, but I already have the latest version. Why is this happening?", "issue_status": "Closed", "issue_reporting_time": "2015-09-07T15:31:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1137": {"issue_url": "https://github.com/scrapy/scrapy/issues/1474", "issue_id": "#1474", "issue_summary": "*** AttributeError: 'Request' object has no attribute 'method'", "issue_description": "thalesfc commented on Sep 2, 2015\nHello everyone, I am getting a bizarre AttributeError: 'Request' object has no attribute 'method'\nHere is the original code:\nclass ModularSpider(CrawlSpider):\n    name = 'modular_spider'\n    allowed_domains = ['fatsecret.com']\n    login_url = 'https://www.fatsecret.com/Auth.aspx?pa=s'\n    start_urls = ['http://www.fatsecret.com/member/dorindam59']\n\n    rules = [\n        # main rule - process members page\n        Rule(\n            LinkExtractor(\n                allow='member\\/[^\\/\\?]+$',\n                deny='inweb'  # deny my own user\n            ),\n            follow=True,\n            callback='schedule'\n        ),\n    ]\n\n    def schedule(self, response):\n        print 10 * \"$\", response.url\n        import pdb\n        pdb.set_trace()\n        r = Request(url=response.url\n                    # callback=users.parse_user,\n                    # dont_filter=True,\n                    # priority=1\n                    )\n        print 10 * \"END\"\n        print r\n        return\n\n... (code continues)\nand here are some investigation I have made\n> /Users/thalesfc/Envs/inweb/lib/python2.7/site-packages/scrapy/http/response/__init__.py(34)_get_url()\n-> def _get_url(self):\n(Pdb) s\n> /Users/thalesfc/Envs/inweb/lib/python2.7/site-packages/scrapy/http/response/__init__.py(35)_get_url()\n-> return self._url\n(Pdb) s\n--Return--\n> /Users/thalesfc/Envs/inweb/lib/python2.7/site-packages/scrapy/http/response/__init__.py(35)_get_url()->'http://www.f...onna+Mashburn'\n-> return self._url\n(Pdb) s\n--Call--\n> /Users/thalesfc/Envs/inweb/lib/python2.7/site-packages/scrapy/utils/trackref.py(28)__new__()\n-> def __new__(cls, *args, **kwargs):\n(Pdb) *args\n*** SyntaxError: invalid syntax (<stdin>, line 1)\n(Pdb) args\ncls = <class 'scrapy.http.request.Request'>\nargs = ()\nkwargs = {'url': 'http://www.fatsecret.com/member/Donna+Mashburn'}\n(Pdb) kwarfs\n*** NameError: name 'kwarfs' is not defined\n(Pdb) kwargs\n{'url': 'http://www.fatsecret.com/member/Donna+Mashburn'}\n(Pdb) object.__new__(cls)\n*** AttributeError: 'Request' object has no attribute 'method'\n(Pdb) cls\n<class 'scrapy.http.request.Request'>\nI know maybe I just made some basic mistake, but could not find something related here or on StackOverflow.", "issue_status": "Closed", "issue_reporting_time": "2015-09-01T20:20:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1138": {"issue_url": "https://github.com/scrapy/scrapy/issues/1471", "issue_id": "#1471", "issue_summary": "Support for returning deferreds in middlewares", "issue_description": "Contributor\nArturGaspar commented on Sep 1, 2015\nIs there a reason why middlewares cannot return deferreds?\nIt could be used to fix the limitation mentioned in the documentation of RobotsTxtMiddleware.\nThe change is simple enough I could make a pull request for it myself, but I thought I would ask first if there is any reason for it to be the way it is.", "issue_status": "Closed", "issue_reporting_time": "2015-09-01T09:24:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1139": {"issue_url": "https://github.com/scrapy/scrapy/issues/1465", "issue_id": "#1465", "issue_summary": "Versioning of dependencies in recent releases", "issue_description": "idella commented on Aug 29, 2015\nI do packaging @ gentoo and I am quite confused / unclear on setting the dependencies for this package given the content of setup.py and tox.ini\nFirst; python implementations supported.\nFrom setup.py\n'Programming Language :: Python',\n'Programming Language :: Python :: 2',\n'Programming Language :: Python :: 2.7',\nfrom tox.ini\n[tox]\nenvlist = py27\n[testenv:py33]\nbasepython = python3.3\nSecond; Runtime / test phase deps\nfrom setup.py\ninstall_requires=[\n'Twisted>=10.0.0',\nfrom tox.ini\npyOpenSSL==0.13\nlxml==2.3.2\nTwisted==11.1.0\nwhich comes under [testenv:precise]\nprecise (ubunutu) has little to do with packaging for gentoo. However if it actually requires Twisted==11.1.0 for the tests, gentoo has a real problem. Twisted==11.1.0 is so old it was dropped from portage some time ago. I still don't have leveldb installed and the testsuite runs and passes all except the tests listed in /issues/1464.\nIdeally a setup.py has a listing of tests_require like install_requires. I am guessing this was not done due to the mass conflict of versioning of dependencies between the various python versions. Currently the ebuilds of scrapy have only python 2.7 support and given all the above, to add python3 support would require a new ebuild of the same package due only to the disparate versioning of dependencies.\nQuite simply, if runtime deps have conflicting versions against deps of the testsuite, it breaks portage in the form of a qa violation. The deps ought match those listed in setup.py and any other source file listing them.", "issue_status": "Closed", "issue_reporting_time": "2015-08-29T14:51:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1140": {"issue_url": "https://github.com/scrapy/scrapy/issues/1464", "issue_id": "#1464", "issue_summary": "Failures of test_proxy_connect.py in recent releases", "issue_description": "idella commented on Aug 29, 2015\nUnder python2.7 (only) in versions 1.0.2 & 1.0.3, test file test_proxy_connect.py yields 4 failures persistently, with the stipulated deps including mitmproxy-0.10.1 and netlib-0.10.1. I installed the plugin pytest-twisted specially for this test since it is not in portage.\nWhichever way I ran them I get, under py2.7,\nscrapy/downloadermiddlewares/ajaxcrawl.py .\nscrapy/extensions/httpcache.py .\nscrapy/http/cookies.py .\nscrapy/utils/datatypes.py .\nscrapy/utils/misc.py .\nscrapy/utils/python.py ..\nscrapy/utils/response.py .\nscrapy/utils/template.py .\nscrapy/utils/url.py .\ntests/test_proxy_connect.py FF..FF\n\n=================== FAILURES ===================\n____________ ProxyConnectTestCase.test_https_connect_tunnel ____________\n\nresult = None, g = <generator object test_https_connect_tunnel at 0x7f37208f1a00>\ndeferred = <Deferred at 0x7f37208ec5a8 current result: None>\n\n    def _inlineCallbacks(result, g, deferred):\n        \"\"\"\n        See L{inlineCallbacks}.\n        \"\"\"\n        # This function is complicated by the need to prevent unbounded recursion\n        # arising from repeatedly yielding immediately ready deferreds.  This while\n        # loop and the waiting variable solve that by manually unfolding the\n        # recursion.\n\n        waiting = [True, # waiting for result?\n                   None] # result\n\n        while 1:\n            try:\n                # Send the last result back as the result of the yield expression.\n                isFailure = isinstance(result, failure.Failure)\n                if isFailure:\n                    result = result.throwExceptionIntoGenerator(g)\n                else:\n>                   result = g.send(result)\n\n/usr/lib64/python2.7/site-packages/twisted/internet/defer.py:1128: \n _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../work/scrapy-1.0.3/tests/test_proxy_connect.py:56: in test_https_connect_tunnel\n    self._assert_got_response_code(200, l)\n../../../work/scrapy-1.0.3/tests/test_proxy_connect.py:104: in _assert_got_response_code\n    self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n/usr/lib64/python2.7/site-packages/twisted/trial/_synctest.py:437: in assertEqual\n    super(_Assertions, self).assertEqual(first, second, msg)\nE   FailTest: 0 != 1\n------------------- Captured stderr call ---------------\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/mnt/gen2/TmpDir/portage/dev-python/scrapy-1.0.3/work/scrapy-1.0.3/tests/mockserver.py\", line 205, in <module>\n    httpPort = reactor.listenTCP(8998, factory)\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/posixbase.py\", line 492, in listenTCP\n    p.startListening()\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/tcp.py\", line 984, in startListening\n    raise CannotListenError(self.interface, self.port, le)\ntwisted.internet.error.CannotListenError: Couldn't listen on any:8998: [Errno 98] Address already in use.\n__________ ProxyConnectTestCase.test_https_connect_tunnel_error __________\nresult = None, g = <generator object test_https_connect_tunnel_error at 0x7f371ef53f00>\ndeferred = <Deferred at 0x7f371ef825a8 current result: None>\n\n    def _inlineCallbacks(result, g, deferred):\n        \"\"\"\n        See L{inlineCallbacks}.\n        \"\"\"\n        # This function is complicated by the need to prevent unbounded recursion\n        # arising from repeatedly yielding immediately ready deferreds.  This while\n        # loop and the waiting variable solve that by manually unfolding the\n        # recursion.\n\n        waiting = [True, # waiting for result?\n                   None] # result\n\n        while 1:\n            try:\n                # Send the last result back as the result of the yield expression.\n                isFailure = isinstance(result, failure.Failure)\n                if isFailure:\n                    result = result.throwExceptionIntoGenerator(g)\n                else:\n>                   result = g.send(result)\n\n/usr/lib64/python2.7/site-packages/twisted/internet/defer.py:1128: \n _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../work/scrapy-1.0.3/tests/test_proxy_connect.py:72: in test_https_connect_tunnel_error\n    self._assert_got_tunnel_error(l)\n../../../work/scrapy-1.0.3/tests/test_proxy_connect.py:107: in _assert_got_tunnel_error\n    self.assertEqual(str(log).count('TunnelError'), 1)\n/usr/lib64/python2.7/site-packages/twisted/trial/_synctest.py:437: in assertEqual\n    super(_Assertions, self).assertEqual(first, second, msg)\nE   FailTest: 0 != 1\n----------------------------------------------------- Captured stderr call -----------------------------------------------------\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/mnt/gen2/TmpDir/portage/dev-python/scrapy-1.0.3/work/scrapy-1.0.3/tests/mockserver.py\", line 205, in <module>\n    httpPort = reactor.listenTCP(8998, factory)\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/posixbase.py\", line 492, in listenTCP\n    p.startListening()\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/tcp.py\", line 984, in startListening\n    raise CannotListenError(self.interface, self.port, le)\ntwisted.internet.error.CannotListenError: Couldn't listen on any:8998: [Errno 98] Address already in use.\n_____ ProxyConnectTestCase.test_https_tunnel_auth_error _____\nresult = None, g = <generator object test_https_tunnel_auth_error at 0x7f3720c32280>\ndeferred = <Deferred at 0x7f37259842d8 current result: None>\n\n    def _inlineCallbacks(result, g, deferred):\n        \"\"\"\n        See L{inlineCallbacks}.\n        \"\"\"\n        # This function is complicated by the need to prevent unbounded recursion\n        # arising from repeatedly yielding immediately ready deferreds.  This while\n        # loop and the waiting variable solve that by manually unfolding the\n        # recursion.\n\n        waiting = [True, # waiting for result?\n                   None] # result\n\n        while 1:\n            try:\n                # Send the last result back as the result of the yield expression.\n                isFailure = isinstance(result, failure.Failure)\n                if isFailure:\n                    result = result.throwExceptionIntoGenerator(g)\n                else:\n>                   result = g.send(result)\n\n/usr/lib64/python2.7/site-packages/twisted/internet/defer.py:1128: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../work/scrapy-1.0.3/tests/test_proxy_connect.py:82: in test_https_tunnel_auth_error\n    self._assert_got_tunnel_error(l)\n../../../work/scrapy-1.0.3/tests/test_proxy_connect.py:107: in _assert_got_tunnel_error\n    self.assertEqual(str(log).count('TunnelError'), 1)\n/usr/lib64/python2.7/site-packages/twisted/trial/_synctest.py:437: in assertEqual\n    super(_Assertions, self).assertEqual(first, second, msg)\nE   FailTest: 0 != 1\n--------- Captured stderr call ------------\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/mnt/gen2/TmpDir/portage/dev-python/scrapy-1.0.3/work/scrapy-1.0.3/tests/mockserver.py\", line 205, in <module>\n    httpPort = reactor.listenTCP(8998, factory)\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/posixbase.py\", line 492, in listenTCP\n    p.startListening()\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/tcp.py\", line 984, in startListening\n    raise CannotListenError(self.interface, self.port, le)\ntwisted.internet.error.CannotListenError: Couldn't listen on any:8998: [Errno 98] Address already in use.\n________________________ ProxyConnectTestCase.test_https_tunnel_without_leak_proxy_authorization_header ________________________\n\nresult = None, g = <generator object test_https_tunnel_without_leak_proxy_authorization_header at 0x7f371eea9410>\ndeferred = <Deferred at 0x7f372570b758 current result: None>\n\n    def _inlineCallbacks(result, g, deferred):\n        \"\"\"\n        See L{inlineCallbacks}.\n        \"\"\"\n        # This function is complicated by the need to prevent unbounded recursion\n        # arising from repeatedly yielding immediately ready deferreds.  This while\n        # loop and the waiting variable solve that by manually unfolding the\n        # recursion.\n\n        waiting = [True, # waiting for result?\n                   None] # result\n\n        while 1:\n            try:\n                # Send the last result back as the result of the yield expression.\n                isFailure = isinstance(result, failure.Failure)\n                if isFailure:\n                    result = result.throwExceptionIntoGenerator(g)\n                else:\n>                   result = g.send(result)\n\n/usr/lib64/python2.7/site-packages/twisted/internet/defer.py:1128: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n../../../work/scrapy-1.0.3/tests/test_proxy_connect.py:91: in test_https_tunnel_without_leak_proxy_authorization_header\n    self._assert_got_response_code(200, l)\n../../../work/scrapy-1.0.3/tests/test_proxy_connect.py:104: in _assert_got_response_code\n    self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n/usr/lib64/python2.7/site-packages/twisted/trial/_synctest.py:437: in assertEqual\n    super(_Assertions, self).assertEqual(first, second, msg)\nE   FailTest: 0 != 1\n----------------------------------------------------- Captured stderr call -----------------------------------------------------\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/mnt/gen2/TmpDir/portage/dev-python/scrapy-1.0.3/work/scrapy-1.0.3/tests/mockserver.py\", line 205, in <module>\n    httpPort = reactor.listenTCP(8998, factory)\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/posixbase.py\", line 492, in listenTCP\n    p.startListening()\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/tcp.py\", line 984, in startListening\n    raise CannotListenError(self.interface, self.port, le)\ntwisted.internet.error.CannotListenError: Couldn't listen on any:8998: [Errno 98] Address already in use.\n====== 4 failed, most passed in 21.65 seconds ======\nI was expecting the installing of the plugin of pytest-twisted to make it pass but no impact.\nIt appears that once the first fails it cascades onto the tests that follow making them fail. Note the\n[Errno 98] Address already. I suspect this is still occupied by a prior test and need be liberated for the following test(s). Or alternatively are they expected failures that are registered as failures and cause the suite to exit with ====== 4 failed, ???\nDo you require any further information?\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2015-08-29T10:08:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1141": {"issue_url": "https://github.com/scrapy/scrapy/issues/1463", "issue_id": "#1463", "issue_summary": "Unsupported URL scheme : no handler available for that scheme", "issue_description": "aldarund commented on Aug 28, 2015\nI have a lot of errors like this in the error log ( im using sentry ) and they are just pollute the log.\nUnsupported URL scheme 'mailto': no handler available for that scheme\nUnsupported URL scheme 'about': no handler available for that scheme\netc.\nSo the log just become very polluted with this errors because there really a lot of them. Is\nMy spider:\n class ExternalLinkSpider(CrawlSpider):\n   name = 'external_link_spider'\n   allowed_domains = ['']\n   start_urls = ['']\n\n   rules = (Rule(LxmlLinkExtractor(allow=()), callback='parse_obj', follow=True),)\n\n   def parse_obj(self, response):\n     if not isinstance(response, HtmlResponse):\n        return\n     for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n        if not link.nofollow:\n            yield LinkCrawlItem(domain=link.url)\nIf there somethin wrong with my spider that could be fixed to stop producing this error, please advice.", "issue_status": "Closed", "issue_reporting_time": "2015-08-27T22:49:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1142": {"issue_url": "https://github.com/scrapy/scrapy/issues/1460", "issue_id": "#1460", "issue_summary": "Error when running 'scrapy' in command line", "issue_description": "rosspalmer commented on Aug 26, 2015\nHello!\nI have trying installing scrapy using both the 'pip' method and installing directly from the source code and keep running into the same error.\nWhen I attempt to run any 'scrapy' based command in the terminal I receive the following output:\nbash: /usr/bin/scrapy: No such file or directory\nThis error is very much correct as all python packages on my machine would be stored in the directory below and I have confirmed that the scrapy modules are indeed present.\n/usr/local/bin/\nAny ideas on how to resolve this issue? I am running Ubuntu 14.04 LTS", "issue_status": "Closed", "issue_reporting_time": "2015-08-26T18:07:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1143": {"issue_url": "https://github.com/scrapy/scrapy/issues/1458", "issue_id": "#1458", "issue_summary": "Impossible to subclass scrapy.core.downloader.Downloader: 'module' object has no attribute 'downloader'", "issue_description": "Djayb6 commented on Aug 26, 2015\nOn scrapy 0.24.6 I successfully subclassed the default's downloader like so:\nimport scrapy\nfrom scrapy.utils.httpobj import urlparse_cached\n\n\nclass WebsiteDownloader(scrapy.core.downloader.Downloader):\n\n    def _get_slot_key(self, request, spider):\n        if 'download_slot' in request.meta:\n            return request.meta['download_slot']\n\n        return request.meta['website_id'] if not request.meta.get('social_request') \\\n            else urlparse_cached(request).hostname or ''\nHowever when I updated scrapy to 1.0.3, I got the following error:\nFile \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 71, in crawl\n    self.engine = self._create_engine()\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 83, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/core/engine.py\", line 65, in __init__\n    downloader_cls = load_object(self.settings['DOWNLOADER'])\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 44, in load_object\n    mod = import_module(module)\n  File \"/usr/local/Cellar/python/2.7.8_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/Users/me/dev/spider/downloader.py\", line 7, in <module>\n    class WebsiteDownloader(scrapy.core.downloader.Downloader):\nexceptions.AttributeError: 'module' object has no attribute 'downloader'\nWoudl I have an idea of what's going on ? Thank you", "issue_status": "Closed", "issue_reporting_time": "2015-08-26T04:28:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1144": {"issue_url": "https://github.com/scrapy/scrapy/issues/1457", "issue_id": "#1457", "issue_summary": "disable codecov/project check", "issue_description": "Member\nkmike commented on Aug 26, 2015\nIt checks that coverage is not decreased. But if coverage is not 100% yet and some covered lines are deleted then coverage decreases; we shouldn't mark a PR failed in this case. Deleting code is a good thing :)", "issue_status": "Closed", "issue_reporting_time": "2015-08-25T21:07:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1145": {"issue_url": "https://github.com/scrapy/scrapy/issues/1454", "issue_id": "#1454", "issue_summary": "Simplify Python 3 badge", "issue_description": "Member\npablohoffman commented on Aug 25, 2015\nWe've recently added a badge to highlight progress on Python 3 port. I suggest to simplify the Python 3 support badge to just show a percentage. If you look at the other badges we have in the scrapy project you'll see that the value of them is usually simple (a number, percentage, or short text).\nShowing just the percentage 54% would not only be more consistent with the rest but also easier to understand (what's \"633 / 1153\" anyway? which even comes before the percentage!)", "issue_status": "Closed", "issue_reporting_time": "2015-08-24T19:35:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1146": {"issue_url": "https://github.com/scrapy/scrapy/issues/1452", "issue_id": "#1452", "issue_summary": "pkg_resources.ContextualVersionConflict: (pyasn1 0.1.7", "issue_description": "c8forfor commented on Aug 24, 2015\nHello,\nI run a ubuntu 14 LTS machine.\nI get a conflict error with every scrapy command.\nFor instance:\nWhen I try :\nscrapy startproject store_name\nI get the following error:\n\nTraceback (most recent call last):\nFile \"/usr/bin/scrapy\", line 5, in\nfrom pkg_resources import load_entry_point\nFile \"build/bdist.linux-x86_64/egg/pkg_resources/init.py\", line 3084, in\nFile \"build/bdist.linux-x86_64/egg/pkg_resources/init.py\", line 3070, in _call_aside\nFile \"build/bdist.linux-x86_64/egg/pkg_resources/init.py\", line 3097, in _initialize_master_working_set\nFile \"build/bdist.linux-x86_64/egg/pkg_resources/init.py\", line 653, in _build_master\nFile \"build/bdist.linux-x86_64/egg/pkg_resources/init.py\", line 666, in _build_from_requirements\nFile \"build/bdist.linux-x86_64/egg/pkg_resources/init.py\", line 844, in resolve\npkg_resources.ContextualVersionConflict: (pyasn1 0.1.7 (/usr/lib/python2.7/dist-packages), Requirement.parse('pyasn1>=0.1.8'), set(['pyasn1-modules']))\n\nCan someone help me ?", "issue_status": "Closed", "issue_reporting_time": "2015-08-24T15:16:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1147": {"issue_url": "https://github.com/scrapy/scrapy/issues/1445", "issue_id": "#1445", "issue_summary": "gcc-4.2 error when installing in Yosemite", "issue_description": "stevenlordiam commented on Aug 21, 2015\nI tried almost any solution I Googled but none can solve the error. At first it's the lxml problem but after I successfully install lxml by macport, the error still happens as: error: command 'gcc-4.2' failed with exit status 1. FYI, I do install the xcode command line tools. So I think this may be a Scrapy problem?\nSome attempts as follow:\nI tried sudo pip install Scrapy, the error message is as below:\nRequirement already satisfied (use --upgrade to upgrade): Scrapy in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/Scrapy-1.0.3-py2.7.egg\nCollecting Twisted>=10.0.0 (from Scrapy)\n  Using cached Twisted-15.3.0.tar.bz2\nCollecting w3lib>=1.8.0 (from Scrapy)\n  Using cached w3lib-1.12.0-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): queuelib in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from Scrapy)\nCollecting lxml (from Scrapy)\n  Using cached lxml-3.4.4.tar.gz\nRequirement already satisfied (use --upgrade to upgrade): pyOpenSSL in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyOpenSSL-0.15.1-py2.7.egg (from Scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cssselect>=0.9 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from Scrapy)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.5.2 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from Scrapy)\nRequirement already satisfied (use --upgrade to upgrade): service-identity in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/service_identity-14.0.0-py2.7.egg (from Scrapy)\nRequirement already satisfied (use --upgrade to upgrade): zope.interface>=3.6.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from Twisted>=10.0.0->Scrapy)\nCollecting cryptography>=0.7 (from pyOpenSSL->Scrapy)\n  Using cached cryptography-1.0.tar.gz\nCollecting characteristic>=14.0.0 (from service-identity->Scrapy)\n  Using cached characteristic-14.3.0-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): pyasn1 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from service-identity->Scrapy)\nCollecting pyasn1-modules (from service-identity->Scrapy)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/setuptools-18.2-py2.7.egg (from zope.interface>=3.6.0->Twisted>=10.0.0->Scrapy)\nRequirement already satisfied (use --upgrade to upgrade): idna>=2.0 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from cryptography>=0.7->pyOpenSSL->Scrapy)\nRequirement already satisfied (use --upgrade to upgrade): enum34 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from cryptography>=0.7->pyOpenSSL->Scrapy)\nRequirement already satisfied (use --upgrade to upgrade): ipaddress in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from cryptography>=0.7->pyOpenSSL->Scrapy)\nCollecting cffi>=1.1.0 (from cryptography>=0.7->pyOpenSSL->Scrapy)\n  Using cached cffi-1.2.1.tar.gz\nRequirement already satisfied (use --upgrade to upgrade): pycparser in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from cffi>=1.1.0->cryptography>=0.7->pyOpenSSL->Scrapy)\nBuilding wheels for collected packages: Twisted, lxml, cryptography, cffi\n  Running setup.py bdist_wheel for Twisted\n  Complete output from command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c \"import setuptools;__file__='/private/tmp/pip-build-qaftVw/Twisted/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" bdist_wheel -d /tmp/tmpRNGVADpip-wheel-:\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build/lib.macosx-10.6-x86_64-2.7\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted\n  copying twisted/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted\n  copying twisted/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted\n  copying twisted/copyright.py -> build/lib.macosx-10.6-x86_64-2.7/twisted\n  copying twisted/plugin.py -> build/lib.macosx-10.6-x86_64-2.7/twisted\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/application\n  copying twisted/application/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application\n  copying twisted/application/app.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application\n  copying twisted/application/internet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application\n  copying twisted/application/reactors.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application\n  copying twisted/application/service.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application\n  copying twisted/application/strports.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/application/test\n  copying twisted/application/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application/test\n  copying twisted/application/test/test_internet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application/test\n  copying twisted/application/test/test_service.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/application/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/avatar.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/checkers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/endpoints.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/error.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/interfaces.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/ls.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/manhole.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/manhole_ssh.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/manhole_tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/mixin.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/recvline.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/stdio.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/telnet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/ttymodes.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  copying twisted/conch/unix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/conch/client\n  copying twisted/conch/client/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/client\n  copying twisted/conch/client/agent.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/client\n  copying twisted/conch/client/connect.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/client\n  copying twisted/conch/client/default.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/client\n  copying twisted/conch/client/direct.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/client\n  copying twisted/conch/client/knownhosts.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/client\n  copying twisted/conch/client/options.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/client\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/conch/insults\n  copying twisted/conch/insults/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/insults\n  copying twisted/conch/insults/client.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/insults\n  copying twisted/conch/insults/colors.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/insults\n  copying twisted/conch/insults/helper.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/insults\n  copying twisted/conch/insults/insults.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/insults\n  copying twisted/conch/insults/text.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/insults\n  copying twisted/conch/insults/window.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/insults\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/conch/openssh_compat\n  copying twisted/conch/openssh_compat/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/openssh_compat\n  copying twisted/conch/openssh_compat/factory.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/openssh_compat\n  copying twisted/conch/openssh_compat/primes.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/openssh_compat\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/conch/scripts\n  copying twisted/conch/scripts/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/scripts\n  copying twisted/conch/scripts/cftp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/scripts\n  copying twisted/conch/scripts/ckeygen.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/scripts\n  copying twisted/conch/scripts/conch.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/scripts\n  copying twisted/conch/scripts/tkconch.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/scripts\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/address.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/agent.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/channel.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/common.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/connection.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/factory.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/filetransfer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/forwarding.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/keys.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/service.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/session.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/sexpy.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/transport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  copying twisted/conch/ssh/userauth.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ssh\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/keydata.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_address.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_agent.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_cftp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_channel.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_checkers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_ckeygen.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_conch.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_connection.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_default.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_endpoints.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_filetransfer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_forwarding.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_helper.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_insults.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_keys.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_knownhosts.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_manhole.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_mixin.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_openssh_compat.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_recvline.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_scripts.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_session.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_ssh.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_telnet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_text.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_transport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_unix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_userauth.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  copying twisted/conch/test/test_window.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ui\n  copying twisted/conch/ui/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ui\n  copying twisted/conch/ui/ansi.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ui\n  copying twisted/conch/ui/tkvt100.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/conch/ui\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/cred\n  copying twisted/cred/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred\n  copying twisted/cred/_digest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred\n  copying twisted/cred/checkers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred\n  copying twisted/cred/credentials.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred\n  copying twisted/cred/error.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred\n  copying twisted/cred/portal.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred\n  copying twisted/cred/strcred.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/cred/test\n  copying twisted/cred/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred/test\n  copying twisted/cred/test/test_cramauth.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred/test\n  copying twisted/cred/test/test_cred.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred/test\n  copying twisted/cred/test/test_digestauth.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred/test\n  copying twisted/cred/test/test_simpleauth.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred/test\n  copying twisted/cred/test/test_strcred.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/cred/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/enterprise\n  copying twisted/enterprise/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/enterprise\n  copying twisted/enterprise/adbapi.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/enterprise\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_baseprocess.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_dumbwin32proc.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_glibbase.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_newtls.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_pollingfile.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_posixserialport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_posixstdio.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_signals.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_ssl.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_sslverify.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_threadedselect.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_win32serialport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/_win32stdio.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/abstract.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/address.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/base.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/cfreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/default.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/defer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/endpoints.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/epollreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/error.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/fdesc.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/gireactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/glib2reactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/gtk2reactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/gtk3reactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/gtkreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/inotify.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/interfaces.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/kqreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/main.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/pollreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/posixbase.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/process.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/protocol.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/pyuisupport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/qtreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/reactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/selectreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/serialport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/ssl.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/stdio.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/task.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/tcp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/threads.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/tksupport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/udp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/unix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/utils.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/win32eventreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/wxreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  copying twisted/internet/wxsupport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  copying twisted/internet/iocpreactor/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  copying twisted/internet/iocpreactor/abstract.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  copying twisted/internet/iocpreactor/const.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  copying twisted/internet/iocpreactor/interfaces.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  copying twisted/internet/iocpreactor/reactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  copying twisted/internet/iocpreactor/setup.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  copying twisted/internet/iocpreactor/tcp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  copying twisted/internet/iocpreactor/udp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/_posixifaces.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/_win32ifaces.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/connectionmixins.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/fakeendpoint.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/modulehelpers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/process_cli.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/process_connectionlost.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/process_gireactornocompat.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/process_helper.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/reactormixins.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_abstract.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_address.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_base.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_baseprocess.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_core.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_default.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_endpoints.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_epollreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_fdset.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_filedescriptor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_gireactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_glibbase.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_gtkreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_inlinecb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_inotify.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_iocp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_main.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_newtls.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_pollingfile.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_posixbase.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_posixprocess.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_process.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_protocol.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_qtreactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_serialport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_sigchld.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_socket.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_stdio.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_tcp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_threads.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_time.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_tls.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_udp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_udp_internals.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_unix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  copying twisted/internet/test/test_win32events.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_buffer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_file.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_filter.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_flatten.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_format.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_global.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_io.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_json.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_legacy.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_levels.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_logger.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_observer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_stdlib.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  copying twisted/logger/_util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_buffer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_file.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_filter.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_flatten.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_format.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_global.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_io.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_json.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_legacy.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_levels.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_logger.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_observer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_stdlib.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  copying twisted/logger/test/test_util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/logger/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/alias.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/bounce.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/imap4.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/mail.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/maildir.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/pb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/pop3.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/pop3client.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/protocols.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/relay.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/relaymanager.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/smtp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  copying twisted/mail/tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/mail/scripts\n  copying twisted/mail/scripts/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/scripts\n  copying twisted/mail/scripts/mailmail.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/scripts\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/pop3testserver.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_bounce.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_imap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_mail.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_mailmail.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_options.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_pop3.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_pop3client.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_scripts.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  copying twisted/mail/test/test_smtp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/mail/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/manhole\n  copying twisted/manhole/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole\n  copying twisted/manhole/_inspectro.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole\n  copying twisted/manhole/explorer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole\n  copying twisted/manhole/gladereactor.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole\n  copying twisted/manhole/service.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole\n  copying twisted/manhole/telnet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/test\n  copying twisted/manhole/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/test\n  copying twisted/manhole/test/test_explorer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/ui\n  copying twisted/manhole/ui/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/ui\n  copying twisted/manhole/ui/gtk2manhole.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/ui\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/ui/test\n  copying twisted/manhole/ui/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/ui/test\n  copying twisted/manhole/ui/test/test_gtk2manhole.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/manhole/ui/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/_rfc1982.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/authority.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/cache.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/client.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/common.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/dns.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/error.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/hosts.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/resolve.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/root.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/secondary.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/server.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/srvconnect.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  copying twisted/names/tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_cache.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_client.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_common.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_dns.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_examples.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_hosts.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_names.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_resolve.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_rfc1982.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_rootresolve.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_server.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_srvconnect.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  copying twisted/names/test/test_util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/names/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/news\n  copying twisted/news/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news\n  copying twisted/news/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news\n  copying twisted/news/database.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news\n  copying twisted/news/news.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news\n  copying twisted/news/nntp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news\n  copying twisted/news/tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/news/test\n  copying twisted/news/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news/test\n  copying twisted/news/test/test_database.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news/test\n  copying twisted/news/test/test_news.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news/test\n  copying twisted/news/test/test_nntp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/news/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  copying twisted/pair/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  copying twisted/pair/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  copying twisted/pair/ethernet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  copying twisted/pair/ip.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  copying twisted/pair/raw.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  copying twisted/pair/rawudp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  copying twisted/pair/testing.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  copying twisted/pair/tuntap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/pair/test\n  copying twisted/pair/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair/test\n  copying twisted/pair/test/test_ethernet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair/test\n  copying twisted/pair/test/test_ip.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair/test\n  copying twisted/pair/test/test_rawudp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair/test\n  copying twisted/pair/test/test_tuntap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/pair/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/persisted\n  copying twisted/persisted/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/persisted\n  copying twisted/persisted/aot.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/persisted\n  copying twisted/persisted/crefutil.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/persisted\n  copying twisted/persisted/dirdbm.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/persisted\n  copying twisted/persisted/sob.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/persisted\n  copying twisted/persisted/styles.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/persisted\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/persisted/test\n  copying twisted/persisted/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/persisted/test\n  copying twisted/persisted/test/test_styles.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/persisted/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/cred_anonymous.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/cred_file.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/cred_memory.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/cred_sshkeys.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/cred_unix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_conch.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_core.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_ftp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_inet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_mail.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_manhole.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_names.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_news.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_portforward.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_qtstub.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_reactors.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_runner.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_socks.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_telnet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_trial.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_web.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  copying twisted/plugins/twisted_words.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/plugins\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/positioning\n  copying twisted/positioning/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning\n  copying twisted/positioning/_sentence.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning\n  copying twisted/positioning/base.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning\n  copying twisted/positioning/ipositioning.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning\n  copying twisted/positioning/nmea.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/positioning/test\n  copying twisted/positioning/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning/test\n  copying twisted/positioning/test/receiver.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning/test\n  copying twisted/positioning/test/test_base.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning/test\n  copying twisted/positioning/test/test_nmea.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning/test\n  copying twisted/positioning/test/test_sentence.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/positioning/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/amp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/basic.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/dict.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/finger.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/ftp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/htb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/ident.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/loopback.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/memcache.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/pcp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/policies.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/portforward.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/postfix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/shoutcast.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/sip.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/socks.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/stateful.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/telnet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/tls.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  copying twisted/protocols/wire.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/gps\n  copying twisted/protocols/gps/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/gps\n  copying twisted/protocols/gps/nmea.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/gps\n  copying twisted/protocols/gps/rockwell.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/gps\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/mice\n  copying twisted/protocols/mice/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/mice\n  copying twisted/protocols/mice/mouseman.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/mice\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/test\n  copying twisted/protocols/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/test\n  copying twisted/protocols/test/test_basic.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/test\n  copying twisted/protocols/test/test_tls.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/protocols/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/_inotify.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/_release.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/_shellcomp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/_textattributes.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/_tzhelper.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/compat.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/components.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/constants.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/context.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/deprecate.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/dist.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/dist3.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/failure.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/fakepwd.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/filepath.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/finalize.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/formmethod.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/hook.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/htmlizer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/lockfile.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/log.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/logfile.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/modules.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/monkey.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/procutils.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/randbytes.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/rebuild.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/reflect.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/release.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/roots.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/runtime.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/sendmsg.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/shortcut.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/syslog.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/systemd.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/text.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/threadable.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/threadpool.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/urlpath.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/usage.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/versions.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/win32.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/zippath.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/python/zipstream.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/cmodulepullpipe.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/deprecatedattributes.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/modules_helpers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/pullpipe.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_components.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_constants.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_deprecate.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_dist.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_dist3.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_fakepwd.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_htmlizer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_inotify.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_release.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_runtime.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_sendmsg.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_shellcomp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_syslog.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_systemd.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_textattributes.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_tzhelper.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_urlpath.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_versions.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_win32.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_zippath.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  copying twisted/python/test/test_zipstream.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/python/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  copying twisted/runner/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  copying twisted/runner/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  copying twisted/runner/inetd.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  copying twisted/runner/inetdconf.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  copying twisted/runner/inetdtap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  copying twisted/runner/procmon.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  copying twisted/runner/procmontap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/runner/test\n  copying twisted/runner/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner/test\n  copying twisted/runner/test/test_procmon.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner/test\n  copying twisted/runner/test/test_procmontap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/_twistd_unix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/_twistw.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/htmlizer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/manhole.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/tap2deb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/tap2rpm.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/trial.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  copying twisted/scripts/twistd.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/scripts/test\n  copying twisted/scripts/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts/test\n  copying twisted/scripts/test/test_scripts.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts/test\n  copying twisted/scripts/test/test_tap2deb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts/test\n  copying twisted/scripts/test/test_tap2rpm.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/scripts/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  copying twisted/spread/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  copying twisted/spread/banana.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  copying twisted/spread/flavors.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  copying twisted/spread/interfaces.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  copying twisted/spread/jelly.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  copying twisted/spread/pb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  copying twisted/spread/publish.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  copying twisted/spread/util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/spread/ui\n  copying twisted/spread/ui/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread/ui\n  copying twisted/spread/ui/gtk2util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread/ui\n  copying twisted/spread/ui/tktree.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread/ui\n  copying twisted/spread/ui/tkutil.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/spread/ui\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/tap\n  copying twisted/tap/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/tap\n  copying twisted/tap/ftp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/tap\n  copying twisted/tap/manhole.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/tap\n  copying twisted/tap/portforward.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/tap\n  copying twisted/tap/socks.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/tap\n  copying twisted/tap/telnet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/tap\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/_preamble.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/crash_test_dummy.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/iosim.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/mock_win32process.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/myrebuilder1.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/myrebuilder2.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/plugin_basic.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/plugin_extra1.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/plugin_extra2.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_cmdline.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_echoer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_fds.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_linger.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_reader.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_signal.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_stdinreader.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_tester.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_tty.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/process_twisted.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/proto_helpers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/reflect_helper_IE.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/reflect_helper_VE.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/reflect_helper_ZDE.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/ssl_helpers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/stdio_test_consumer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/stdio_test_halfclose.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/stdio_test_hostpeer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/stdio_test_lastwrite.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/stdio_test_loseconn.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/stdio_test_producer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/stdio_test_write.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/stdio_test_writeseq.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_abstract.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_adbapi.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_amp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_application.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_banana.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_compat.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_context.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_cooperator.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_defer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_defgen.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_dict.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_dirdbm.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_doc.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_error.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_explorer.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_factories.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_failure.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_fdesc.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_finger.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_formmethod.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_ftp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_ftp_options.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_hook.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_htb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_ident.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_internet.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_iosim.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_iutils.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_jelly.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_lockfile.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_log.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_logfile.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_loopback.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_manhole.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_memcache.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_modules.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_monkey.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_nmea.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_paths.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_pb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_pbfailure.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_pcp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_persisted.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_plugin.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_policies.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_postfix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_process.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_protocols.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_randbytes.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_rebuild.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_reflect.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_roots.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_setup.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_shortcut.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_sip.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_sob.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_socks.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_ssl.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_sslverify.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_stateful.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_stdio.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_strerror.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_stringtransport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_strports.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_task.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_tcp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_tcp_internals.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_text.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_threadable.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_threadpool.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_threads.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_tpfile.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_twistd.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_twisted.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_udp.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_unix.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/test_usage.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  copying twisted/test/testutils.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/_asyncrunner.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/_asynctest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/_synctest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/itrial.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/reporter.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/runner.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/unittest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  copying twisted/trial/util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/distreporter.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/disttrial.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/managercommands.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/options.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/worker.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/workercommands.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/workerreporter.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  copying twisted/trial/_dist/workertrial.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist/test\n  copying twisted/trial/_dist/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist/test\n  copying twisted/trial/_dist/test/test_distreporter.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist/test\n  copying twisted/trial/_dist/test/test_disttrial.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist/test\n  copying twisted/trial/_dist/test/test_options.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist/test\n  copying twisted/trial/_dist/test/test_worker.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist/test\n  copying twisted/trial/_dist/test/test_workerreporter.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist/test\n  copying twisted/trial/_dist/test/test_workertrial.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/_dist/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/detests.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/erroneous.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/mockcustomsuite.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/mockcustomsuite2.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/mockcustomsuite3.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/mockdoctest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/moduleself.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/moduletest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/novars.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/ordertests.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/packages.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/sample.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/scripttest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/skipping.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/suppression.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_assertions.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_asyncassertions.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_deferred.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_doctest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_keyboard.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_loader.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_log.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_output.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_plugins.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_pyunitcompat.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_reporter.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_runner.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_script.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_suppression.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_testcase.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_tests.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/test_warning.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  copying twisted/trial/test/weird.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/trial/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/_element.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/_flatten.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/_newclient.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/_responses.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/_stan.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/client.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/demo.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/distrib.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/domhelpers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/error.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/guard.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/html.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/http.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/http_headers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/iweb.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/microdom.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/proxy.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/resource.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/rewrite.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/script.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/server.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/soap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/static.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/sux.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/template.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/twcgi.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/vhost.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/wsgi.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  copying twisted/web/xmlrpc.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/web/_auth\n  copying twisted/web/_auth/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/_auth\n  copying twisted/web/_auth/basic.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/_auth\n  copying twisted/web/_auth/digest.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/_auth\n  copying twisted/web/_auth/wrapper.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/_auth\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/_util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/requesthelper.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_agent.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_cgi.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_distrib.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_domhelpers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_error.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_flatten.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_html.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_http.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_http_headers.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_httpauth.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_newclient.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_proxy.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_resource.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_script.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_soap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_stan.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_static.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_template.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_util.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_vhost.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_web.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_web__responses.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_webclient.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_wsgi.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_xml.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  copying twisted/web/test/test_xmlrpc.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/web/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/words\n  copying twisted/words/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words\n  copying twisted/words/_version.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words\n  copying twisted/words/ewords.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words\n  copying twisted/words/iwords.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words\n  copying twisted/words/service.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words\n  copying twisted/words/tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words\n  copying twisted/words/xmpproutertap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  copying twisted/words/im/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  copying twisted/words/im/baseaccount.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  copying twisted/words/im/basechat.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  copying twisted/words/im/basesupport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  copying twisted/words/im/interfaces.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  copying twisted/words/im/ircsupport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  copying twisted/words/im/locals.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  copying twisted/words/im/pbsupport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/im\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols\n  copying twisted/words/protocols/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols\n  copying twisted/words/protocols/irc.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols\n  copying twisted/words/protocols/msn.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols\n  copying twisted/words/protocols/oscar.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/client.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/component.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/error.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/ijabber.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/jid.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/jstrports.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/sasl.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/sasl_mechanisms.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/xmlstream.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  copying twisted/words/protocols/jabber/xmpp_stringprep.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/protocols/jabber\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_basechat.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_basesupport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_domish.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_irc.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_irc_service.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_ircsupport.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabberclient.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabbercomponent.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabbererror.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabberjid.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabberjstrports.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabbersasl.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabbersaslmechanisms.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabberxmlstream.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_jabberxmppstringprep.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_msn.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_oscar.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_service.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_tap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_xishutil.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_xmlstream.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_xmpproutertap.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  copying twisted/words/test/test_xpath.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/test\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/words/xish\n  copying twisted/words/xish/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/xish\n  copying twisted/words/xish/domish.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/xish\n  copying twisted/words/xish/utility.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/xish\n  copying twisted/words/xish/xmlstream.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/xish\n  copying twisted/words/xish/xpath.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/xish\n  copying twisted/words/xish/xpathparser.py -> build/lib.macosx-10.6-x86_64-2.7/twisted/words/xish\n  running egg_info\n  creating Twisted.egg-info\n  writing requirements to Twisted.egg-info/requires.txt\n  writing Twisted.egg-info/PKG-INFO\n  writing top-level names to Twisted.egg-info/top_level.txt\n  writing dependency_links to Twisted.egg-info/dependency_links.txt\n  writing manifest file 'Twisted.egg-info/SOURCES.txt'\n  warning: manifest_maker: standard file '-c' not found\n\n  reading manifest file 'Twisted.egg-info/SOURCES.txt'\n  writing manifest file 'Twisted.egg-info/SOURCES.txt'\n  creating build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor/iocpsupport\n  copying twisted/internet/iocpreactor/iocpsupport/iocpsupport.c -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor/iocpsupport\n  copying twisted/internet/iocpreactor/iocpsupport/winsock_pointers.c -> build/lib.macosx-10.6-x86_64-2.7/twisted/internet/iocpreactor/iocpsupport\n  copying twisted/python/_sendmsg.c -> build/lib.macosx-10.6-x86_64-2.7/twisted/python\n  copying twisted/runner/portmap.c -> build/lib.macosx-10.6-x86_64-2.7/twisted/runner\n  copying twisted/test/raiser.c -> build/lib.macosx-10.6-x86_64-2.7/twisted/test\n  running build_ext\n  gcc-4.2 -fno-strict-aliasing -fno-common -dynamic -isysroot /Developer/SDKs/MacOSX10.6.sdk -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c conftest.c -o conftest.o\n  building 'twisted.test.raiser' extension\n  creating build/temp.macosx-10.6-x86_64-2.7\n  creating build/temp.macosx-10.6-x86_64-2.7/twisted\n  creating build/temp.macosx-10.6-x86_64-2.7/twisted/test\n  gcc-4.2 -fno-strict-aliasing -fno-common -dynamic -isysroot /Developer/SDKs/MacOSX10.6.sdk -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c twisted/test/raiser.c -o build/temp.macosx-10.6-x86_64-2.7/twisted/test/raiser.o\n  In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                   from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                   from twisted/test/raiser.c:4:\n  /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n  In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                   from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                   from twisted/test/raiser.c:4:\n  /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n  fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't figure out the architecture type of: /var/tmp//ccIkEZkD.out\n  error: command 'gcc-4.2' failed with exit status 1\n\n  ----------------------------------------\n  Running setup.py bdist_wheel for lxml\n  Complete output from command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c \"import setuptools;__file__='/private/tmp/pip-build-qaftVw/lxml/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" bdist_wheel -d /tmp/tmpUhY7WBpip-wheel-:\n  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'bugtrack_url'\n    warnings.warn(msg)\n  Building lxml version 3.4.4.\n  Building without Cython.\n  Using build configuration of libxslt 1.1.28\n  Building against libxml2/libxslt in one of the following directories:\n    /usr/local/Cellar/libxslt/1.1.28_1/lib\n    /usr/local/Cellar/libxml2/2.9.2/lib\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build/lib.macosx-10.6-x86_64-2.7\n  creating build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/_elementpath.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/builder.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/cssselect.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/doctestcompare.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/ElementInclude.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/pyclasslookup.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/sax.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/usedoctest.py -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  creating build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  creating build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/_diffcommand.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/_html5builder.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/_setmixin.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/builder.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/clean.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/defs.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/diff.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/ElementSoup.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/formfill.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/html5parser.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/soupparser.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  copying src/lxml/html/usedoctest.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/html\n  creating build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron\n  copying src/lxml/isoschematron/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron\n  copying src/lxml/lxml.etree.h -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/lxml.etree_api.h -> build/lib.macosx-10.6-x86_64-2.7/lxml\n  copying src/lxml/includes/c14n.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/config.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/dtdvalid.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/etreepublic.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/htmlparser.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/relaxng.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/schematron.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/tree.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/uri.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/xinclude.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/xmlerror.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/xmlparser.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/xmlschema.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/xpath.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/xslt.pxd -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/etree_defs.h -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  copying src/lxml/includes/lxml-version.h -> build/lib.macosx-10.6-x86_64-2.7/lxml/includes\n  creating build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources\n  creating build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/rng\n  copying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/rng\n  creating build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl\n  copying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl\n  copying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl\n  creating build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n  copying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.macosx-10.6-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\n  running build_ext\n  building 'lxml.etree' extension\n  creating build/temp.macosx-10.6-x86_64-2.7\n  creating build/temp.macosx-10.6-x86_64-2.7/src\n  creating build/temp.macosx-10.6-x86_64-2.7/src/lxml\n  gcc-4.2 -fno-strict-aliasing -fno-common -dynamic -isysroot /Developer/SDKs/MacOSX10.6.sdk -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/Cellar/libxslt/1.1.28_1/include -I/usr/local/Cellar/libxml2/2.9.2/include/libxml2 -I/private/tmp/pip-build-qaftVw/lxml/src/lxml/includes -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.macosx-10.6-x86_64-2.7/src/lxml/lxml.etree.o -w -flat_namespace\n  In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                   from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                   from src/lxml/lxml.etree.c:16:\n  /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n  src/lxml/lxml.etree.c: In function \u2018__pyx_f_4lxml_5etree__receiveXSLTError\u2019:\n  src/lxml/lxml.etree.c:38955: error: expected expression before \u2018char\u2019\n  src/lxml/lxml.etree.c:39097: error: expected expression before \u2018int\u2019\n  In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                   from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                   from src/lxml/lxml.etree.c:16:\n  /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n  src/lxml/lxml.etree.c: In function \u2018__pyx_f_4lxml_5etree__receiveXSLTError\u2019:\n  src/lxml/lxml.etree.c:38955: error: expected expression before \u2018char\u2019\n  src/lxml/lxml.etree.c:39097: error: expected expression before \u2018int\u2019\n  fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't figure out the architecture type of: /var/tmp//ccl6XLkX.out\n  error: command 'gcc-4.2' failed with exit status 1\n\n  ----------------------------------------\n  Running setup.py bdist_wheel for cryptography\n  Complete output from command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c \"import setuptools;__file__='/private/tmp/pip-build-qaftVw/cryptography/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" bdist_wheel -d /tmp/tmpV6XKMCpip-wheel-:\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  _configtest.c:1: error: thread-local storage not supported for this target\n  _configtest.c:1: error: thread-local storage not supported for this target\n  fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't figure out the architecture type of: /var/tmp//ccpTuWyw.out\n  Note: will not use '__thread' in the C code\n  The above error message can be safely ignored\n  In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                   from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                   from c/_cffi_backend.c:2:\n  /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n  c/_cffi_backend.c: In function \u2018_testfunc9\u2019:\n  c/_cffi_backend.c:5778: warning: implicit declaration of function \u2018va_start\u2019\n  c/_cffi_backend.c:5780: warning: implicit declaration of function \u2018va_arg\u2019\n  c/_cffi_backend.c:5780: error: expected expression before \u2018int\u2019\n  c/_cffi_backend.c:5785: warning: implicit declaration of function \u2018va_end\u2019\n  In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                   from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                   from c/_cffi_backend.c:2:\n  /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n  c/_cffi_backend.c: In function \u2018_testfunc9\u2019:\n  c/_cffi_backend.c:5778: warning: implicit declaration of function \u2018va_start\u2019\n  c/_cffi_backend.c:5780: warning: implicit declaration of function \u2018va_arg\u2019\n  c/_cffi_backend.c:5780: error: expected expression before \u2018int\u2019\n  c/_cffi_backend.c:5785: warning: implicit declaration of function \u2018va_end\u2019\n  fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't figure out the architecture type of: /var/tmp//ccBqIRZc.out\n  Traceback (most recent call last):\n    File \"<string>\", line 1, in <module>\n    File \"/private/tmp/pip-build-qaftVw/cryptography/setup.py\", line 307, in <module>\n      **keywords_with_side_effects(sys.argv)\n    File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/core.py\", line 112, in setup\n      _setup_distribution = dist = klass(attrs)\n    File \"build/bdist.macosx-10.6-x86_64/egg/setuptools/dist.py\", line 268, in __init__\n    File \"build/bdist.macosx-10.6-x86_64/egg/setuptools/dist.py\", line 313, in fetch_build_eggs\n    File \"build/bdist.macosx-10.6-x86_64/egg/pkg_resources/__init__.py\", line 836, in resolve\n    File \"build/bdist.macosx-10.6-x86_64/egg/pkg_resources/__init__.py\", line 1081, in best_match\n    File \"build/bdist.macosx-10.6-x86_64/egg/pkg_resources/__init__.py\", line 1093, in obtain\n    File \"build/bdist.macosx-10.6-x86_64/egg/setuptools/dist.py\", line 380, in fetch_build_egg\n    File \"build/bdist.macosx-10.6-x86_64/egg/setuptools/command/easy_install.py\", line 629, in easy_install\n\n    File \"build/bdist.macosx-10.6-x86_64/egg/setuptools/command/easy_install.py\", line 659, in install_item\n\n    File \"build/bdist.macosx-10.6-x86_64/egg/setuptools/command/easy_install.py\", line 842, in install_eggs\n\n    File \"build/bdist.macosx-10.6-x86_64/egg/setuptools/command/easy_install.py\", line 1070, in build_and_install\n\n    File \"build/bdist.macosx-10.6-x86_64/egg/setuptools/command/easy_install.py\", line 1058, in run_setup\n\n  distutils.errors.DistutilsError: Setup script exited with error: command 'gcc-4.2' failed with exit status 1\n\n  ----------------------------------------\n  Running setup.py bdist_wheel for cffi\n  Complete output from command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c \"import setuptools;__file__='/private/tmp/pip-build-qaftVw/cffi/setup.py';exec(compile(open(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" bdist_wheel -d /tmp/tmpKN90MNpip-wheel-:\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  Package libffi was not found in the pkg-config search path.\n  Perhaps you should add the directory containing `libffi.pc'\n  to the PKG_CONFIG_PATH environment variable\n  No package 'libffi' found\n  _configtest.c:1: error: thread-local storage not supported for this target\n  _configtest.c:1: error: thread-local storage not supported for this target\n  fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't figure out the architecture type of: /var/tmp//ccBORy2u.out\n  Note: will not use '__thread' in the C code\n  The above error message can be safely ignored\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build/lib.macosx-10.6-x86_64-2.7\n  creating build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/__init__.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/api.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/backend_ctypes.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/cffi_opcode.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/commontypes.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/cparser.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/ffiplatform.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/gc_weakref.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/lock.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/model.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/recompiler.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/setuptools_ext.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/vengine_cpy.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/vengine_gen.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/verifier.py -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/_cffi_include.h -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  copying cffi/parse_c_type.h -> build/lib.macosx-10.6-x86_64-2.7/cffi\n  running build_ext\n  building '_cffi_backend' extension\n  creating build/temp.macosx-10.6-x86_64-2.7\n  creating build/temp.macosx-10.6-x86_64-2.7/c\n  gcc-4.2 -fno-strict-aliasing -fno-common -dynamic -isysroot /Developer/SDKs/MacOSX10.6.sdk -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/include/ffi -I/usr/include/libffi -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c c/_cffi_backend.c -o build/temp.macosx-10.6-x86_64-2.7/c/_cffi_backend.o\n  In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                   from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                   from c/_cffi_backend.c:2:\n  /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n  c/_cffi_backend.c: In function \u2018_testfunc9\u2019:\n  c/_cffi_backend.c:5778: warning: implicit declaration of function \u2018va_start\u2019\n  c/_cffi_backend.c:5780: warning: implicit declaration of function \u2018va_arg\u2019\n  c/_cffi_backend.c:5780: error: expected expression before \u2018int\u2019\n  c/_cffi_backend.c:5785: warning: implicit declaration of function \u2018va_end\u2019\n  In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                   from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                   from c/_cffi_backend.c:2:\n  /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n  c/_cffi_backend.c: In function \u2018_testfunc9\u2019:\n  c/_cffi_backend.c:5778: warning: implicit declaration of function \u2018va_start\u2019\n  c/_cffi_backend.c:5780: warning: implicit declaration of function \u2018va_arg\u2019\n  c/_cffi_backend.c:5780: error: expected expression before \u2018int\u2019\n  c/_cffi_backend.c:5785: warning: implicit declaration of function \u2018va_end\u2019\n  fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't figure out the architecture type of: /var/tmp//ccy5VtdJ.out\n  error: command 'gcc-4.2' failed with exit status 1\n\n  ----------------------------------------\nFailed to build Twisted lxml cryptography cffi\nInstalling collected packages: Twisted, w3lib, lxml, cffi, cryptography, characteristic, pyasn1-modules\n  Running setup.py install for Twisted\n    Complete output from command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -c \"import setuptools, tokenize;__file__='/private/tmp/pip-build-qaftVw/Twisted/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-v3Jkvx-record/install-record.txt --single-version-externally-managed --compile:\n    running install\n    running build\n    running build_py\n    running egg_info\n    writing requirements to Twisted.egg-info/requires.txt\n    writing Twisted.egg-info/PKG-INFO\n    writing top-level names to Twisted.egg-info/top_level.txt\n    writing dependency_links to Twisted.egg-info/dependency_links.txt\n    warning: manifest_maker: standard file '-c' not found\n\n    reading manifest file 'Twisted.egg-info/SOURCES.txt'\n    writing manifest file 'Twisted.egg-info/SOURCES.txt'\n    running build_ext\n    gcc-4.2 -fno-strict-aliasing -fno-common -dynamic -isysroot /Developer/SDKs/MacOSX10.6.sdk -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c conftest.c -o conftest.o\n    building 'twisted.test.raiser' extension\n    gcc-4.2 -fno-strict-aliasing -fno-common -dynamic -isysroot /Developer/SDKs/MacOSX10.6.sdk -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c twisted/test/raiser.c -o build/temp.macosx-10.6-x86_64-2.7/twisted/test/raiser.o\n    In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                     from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                     from twisted/test/raiser.c:4:\n    /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n    In file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                     from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                     from twisted/test/raiser.c:4:\n    /Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\n    fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't figure out the architecture type of: /var/tmp//ccydLgtg.out\n    error: command 'gcc-4.2' failed with exit status 1\n\n    ----------------------------------------\nAnd then I tried sudo easy_install Scrapy, also an error.\nSearching for Scrapy\nBest match: Scrapy 1.0.3\nProcessing Scrapy-1.0.3-py2.7.egg\nScrapy 1.0.3 is already the active version in easy-install.pth\nInstalling scrapy script to /Library/Frameworks/Python.framework/Versions/2.7/bin\n\nUsing /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/Scrapy-1.0.3-py2.7.egg\nProcessing dependencies for Scrapy\nSearching for lxml\nReading https://pypi.python.org/simple/lxml/\nBest match: lxml 3.4.4\nDownloading https://pypi.python.org/packages/source/l/lxml/lxml-3.4.4.tar.gz#md5=a9a65972afc173ec7a39c585f4eea69c\nProcessing lxml-3.4.4.tar.gz\nWriting /tmp/easy_install-V2qeH3/lxml-3.4.4/setup.cfg\nRunning lxml-3.4.4/setup.py -q bdist_egg --dist-dir /tmp/easy_install-V2qeH3/lxml-3.4.4/egg-dist-tmp-8ZZ_9v\nBuilding lxml version 3.4.4.\nBuilding without Cython.\nUsing build configuration of libxslt 1.1.28\nBuilding against libxml2/libxslt in one of the following directories:\n  /usr/local/Cellar/libxslt/1.1.28_1/lib\n  /usr/local/Cellar/libxml2/2.9.2/lib\n/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'bugtrack_url'\n  warnings.warn(msg)\nIn file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                 from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                 from src/lxml/lxml.etree.c:16:\n/Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\nsrc/lxml/lxml.etree.c: In function \u2018__pyx_f_4lxml_5etree__receiveXSLTError\u2019:\nsrc/lxml/lxml.etree.c:38955: error: expected expression before \u2018char\u2019\nsrc/lxml/lxml.etree.c:39097: error: expected expression before \u2018int\u2019\nIn file included from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/unicodeobject.h:4,\n                 from /Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h:85,\n                 from src/lxml/lxml.etree.c:16:\n/Developer/SDKs/MacOSX10.6.sdk/usr/include/stdarg.h:4:25: error: stdarg.h: No such file or directory\nsrc/lxml/lxml.etree.c: In function \u2018__pyx_f_4lxml_5etree__receiveXSLTError\u2019:\nsrc/lxml/lxml.etree.c:38955: error: expected expression before \u2018char\u2019\nsrc/lxml/lxml.etree.c:39097: error: expected expression before \u2018int\u2019\nfatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: can't figure out the architecture type of: /var/tmp//ccpnx2e4.out\nerror: Setup script exited with error: command 'gcc-4.2' failed with exit status 1\nAny advice how to fix it?", "issue_status": "Closed", "issue_reporting_time": "2015-08-21T00:47:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1148": {"issue_url": "https://github.com/scrapy/scrapy/issues/1438", "issue_id": "#1438", "issue_summary": "Update ubuntu packages", "issue_description": "gmeans commented on Aug 18, 2015\nCan we get the ubuntu packages updated w/ the 1.0.3 release?\nThe latest one I see is 1.0.1.post1+g5b8c9e5-1435726464.", "issue_status": "Closed", "issue_reporting_time": "2015-08-18T11:38:45Z", "fixed_by": "#3684", "pull_request_summary": "Remove docs/topics/ubuntu.rst", "pull_request_description": "Member\nGallaecio commented on Mar 15, 2019 \u2022\nedited\nI think it is the logical step after #2137.\nThis change should probably be accompanied by the closing of pull request #1439, which is about changing the page removed here.\nCloses #1438, closes #1439\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2019-07-12T12:03:11Z", "files_changed": [["41", "docs/topics/ubuntu.rst"]]}, "1149": {"issue_url": "https://github.com/scrapy/scrapy/issues/1437", "issue_id": "#1437", "issue_summary": "Login Issue with scrapy", "issue_description": "hueyzhao commented on Aug 15, 2015\nProblem Description:\nI want to crawl some info from the bbs of my college. Here is the address:http://bbs.byr.cn\nBelow is the code of my spider:\nfrom lxml import etree\nimport scrapy\ntry:\n    from scrapy.spiders import Spider\nexcept:\n    from scrapy.spiders import BaseSpider as Spider\nfrom scrapy.http import Request\n\nclass ITJobInfoSpider(scrapy.Spider):\n    name = \"ITJobInfoSpider\"\n    start_urls = [\"http://bbs.byr.cn/#!login\"]\n\n\n\n    def parse(self,response):\n        return scrapy.FormRequest.from_response(\n            response,\n            formdata={'method':'post','id': 'username', 'passwd': 'password'},\n            formxpath='//form[@action=\"/login\"]',\n            callback=self.after_login\n        )\n\n    def after_login(self,response):\n        print \"######response body: \" + response.body +\"\\n\"\n        if \"authentication failed\" in response.body:\n            print \"#######Login failed#########\\n\"\n        return\nHowever, with this code, I often get an Error: raise ValueError(\"No <form> element found in %s\" % response)\nMy Investigation:\nI find that this Error happens when scrapy try to parse the HTML code of the url: http://bbs.byr.cn, scrapy parses the page with lxml. Below is the code\nroot = LxmlDocument(response, lxml.html.HTMLParser)\nforms = root.xpath('//form')\nif not forms:\n    raise ValueError(\"No <form> element found in %s\" % response)\nSo I look into the code with the code print etree.tostring(root). And find that HTML element: </form> is parsed into &lt;/form&gt;\nno wonder the code forms = root.xpath('//form') will return an empty forms list.\nBut I don't know why this is happening, maybe the HTML code encoding?\nThanks advance for anyone who can help me out? BTW, if anyone want to write code against the website(http://bbs.byr.cn/), I can give you an test account, pls leave me an email address in the comment.\nThanks a lot, guys!!", "issue_status": "Closed", "issue_reporting_time": "2015-08-15T07:48:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1150": {"issue_url": "https://github.com/scrapy/scrapy/issues/1435", "issue_id": "#1435", "issue_summary": "Retry until finding working options for https sites", "issue_description": "Member\ndangra commented on Aug 15, 2015\nWe have seen multiple cases for crawlers failing to connect to https urls due to differences in SSL_options+openssl+pyopenssl, i.e. #82 #194 #1429 #1434 #1227 #981 #26 #1101, our users usually found workarounds hacking ContextFactory but solutions remain hidden and lost in private project's code.\nI think we have a chance to fix this problem for everyone by automating the process of retrying until finding a working set of options.\nI think we can design a solution around making ContextFactory aware of request, configuring it trough values in request.meta (similar to #186), but also adding a downloadermiddleware that retries requests using different SSL options, of course it learns and set working options per hostname on following requests.", "issue_status": "Closed", "issue_reporting_time": "2015-08-14T22:19:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1151": {"issue_url": "https://github.com/scrapy/scrapy/issues/1434", "issue_id": "#1434", "issue_summary": "Issue scraping through https", "issue_description": "teddb commented on Aug 13, 2015\nI can successfully scrape an http page through a proxy. I can also scrape an https page without using a proxy. I cannot scrape an https page through a proxy.\nMy spider works successfully if the 's' in 'https' of the target page is removed or I disable the proxy. I can access the https page through the proxy through my browser.\nHere is what I added to the settings file:\nDOWNLOADER_MIDDLEWARES = {'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110, 'test_website.middlewares.ProxyMiddleware': 100}\nPROXIES = [{'ip_port': 'us-il.proxymesh.com:31280', 'user_pass': 'username:password'}]\nHere is the code for my spider:\nimport scrapy\n\nclass TestSpider(scrapy.Spider):\n    name = \"test_spider\"\n    allowed_domains = \"ipify.org\"\n     start_urls = [\"https://api.ipify.org\"]\n\n    def parse(self, response):\n        with open('test.html', 'wb') as f:\n            f.write(response.body)\nHere is the middlewares file:\nimport base64\nimport random\nfrom settings import PROXIES\n\nclass ProxyMiddleware(object):\n    def process_request(self, request, spider):\n        proxy = random.choice(PROXIES)\n        if proxy['user_pass'] is not None:\n            request.meta['proxy'] = \"http://%s\" % proxy['ip_port']\n            encoded_user_pass = base64.encodestring(proxy['user_pass'])\n            request.headers['Proxy-Authorization'] = 'Basic ' + encoded_user_pass            \n        else:\n            request.meta['proxy'] = \"http://%s\" % proxy['ip_port']\nHere is the log file:\n    2015-08-12 20:15:50 [scrapy] INFO: Scrapy 1.0.3 started (bot: test_website)\n2015-08-12 20:15:50 [scrapy] INFO: Optional features available: ssl, http11\n2015-08-12 20:15:50 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'test_website.spiders', 'SPIDER_MODULES': ['test_website.spiders'], 'LOG_STDOUT': True, 'LOG_FILE': 'log.txt', 'BOT_NAME': 'test_website'}\n2015-08-12 20:15:51 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n2015-08-12 20:15:53 [scrapy] INFO: Enabled downloader middlewares: ProxyMiddleware, HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-08-12 20:15:53 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-08-12 20:15:53 [scrapy] INFO: Enabled item pipelines: \n2015-08-12 20:15:53 [scrapy] INFO: Spider opened\n2015-08-12 20:15:53 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-08-12 20:15:53 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-08-12 20:15:53 [scrapy] DEBUG: Retrying <GET https://api.ipify.org> (failed 1 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2015-08-12 20:15:53 [scrapy] DEBUG: Retrying <GET https://api.ipify.org> (failed 2 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2015-08-12 20:15:53 [scrapy] DEBUG: Gave up retrying <GET https://api.ipify.org> (failed 3 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2015-08-12 20:15:53 [scrapy] ERROR: Error downloading <GET https://api.ipify.org>: [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2015-08-12 20:15:53 [scrapy] INFO: Closing spider (finished)\n2015-08-12 20:15:53 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/exception_count': 3,\n 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,\n 'downloader/request_bytes': 819,\n 'downloader/request_count': 3,\n 'downloader/request_method_count/GET': 3,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2015, 8, 13, 2, 15, 53, 943000),\n 'log_count/DEBUG': 4,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'scheduler/dequeued': 3,\n 'scheduler/dequeued/memory': 3,\n 'scheduler/enqueued': 3,\n 'scheduler/enqueued/memory': 3,\n 'start_time': datetime.datetime(2015, 8, 13, 2, 15, 53, 38000)}\n2015-08-12 20:15:53 [scrapy] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2015-08-13T02:17:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1152": {"issue_url": "https://github.com/scrapy/scrapy/issues/1432", "issue_id": "#1432", "issue_summary": "CrawlSpider - change the link extractor rules while spider is running?", "issue_description": "stejesh commented on Aug 13, 2015\nI understand this is a support question, but i've got no proper solutions after a lot of searching on sites like stackoverflow.com\nMy query is for the CrawlSpider\nI understand rules is a static variable,\nCan i change the rules in runtime say, like\n@classmethod\ndef set_rules(cls,rules):\n cls.rules = rules\nby\nself.set_rules(rules)\n\nIs this the acceptable practice for the `CrawlSpider` ? if not please suggest the appropriate method\nMy use case,\nI'm using scrapy to crawl certain categories A,B,C....Z of a particular website. each category has 1000 links spread over 10 pages\nand when scrapy hits a link in a some category which is \"too old\". I'd like the crawler to stop following the remainder of the 10 pages ONLY for that category alone and thus my requirement of dynamic rule changes.\nPlease point me out on the right direction.\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2015-08-12T21:36:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1153": {"issue_url": "https://github.com/scrapy/scrapy/issues/1429", "issue_id": "#1429", "issue_summary": "SSL issue when scraping website", "issue_description": "gmeans commented on Aug 12, 2015\nI have a spider that's throwing the following error when trying to crawl this URL.\n>>> fetch('https://vconnections.org/resources')\n2015-08-12 10:07:28 [scrapy] INFO: Spider opened\n2015-08-12 10:07:28 [scrapy] DEBUG: Retrying <GET https://vconnections.org/resources> (failed 1 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2015-08-12 10:07:33 [scrapy] DEBUG: Gave up retrying <GET https://vconnections.org/resources> (failed 2 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"/Users/gmeans/.virtualenvs/backlink/lib/python2.7/site-packages/scrapy/shell.py\", line 87, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/Users/gmeans/.virtualenvs/backlink/lib/python2.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<string>\", line 2, in raiseException\nResponseNeverReceived: [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\nOther SSL urls work fine, and I tried implementing the solution from this previous issue:\n#981\nclass CustomContextFactory(ScrapyClientContextFactory):\n    def getContext(self, hostname=None, port=None):\n        ctx = ClientContextFactory.getContext(self)\n        # Enable all workarounds to SSL bugs as documented by\n        # http://www.openssl.org/docs/ssl/SSL_CTX_set_options.html\n        ctx.set_options(SSL.OP_ALL)\n        if hostname:\n            ClientTLSOptions(hostname, ctx)\n        return ctx\nScrapy==1.0.3\nTwisted==15.3.0\npyOpenSSL==0.15.1\nOpenSSL 1.0.1k 8 Jan 2015\nAny ideas on what else I could try? Thanks!", "issue_status": "Closed", "issue_reporting_time": "2015-08-12T14:15:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1154": {"issue_url": "https://github.com/scrapy/scrapy/issues/1426", "issue_id": "#1426", "issue_summary": "scrapy startproject became too verbose in Scrapy 1.0", "issue_description": "Member\nkmike commented on Aug 12, 2015\n> scrapy startproject foo\n2015-08-12 03:46:07 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n2015-08-12 03:46:07 [scrapy] INFO: Optional features available: ssl, http11, boto\n2015-08-12 03:46:07 [scrapy] INFO: Overridden settings: {}\nNew Scrapy project 'foo' created in:\n    /Users/kmike/scrap/foo\n\nYou can start your first spider with:\n    cd foo\n    scrapy genspider example example.com\nI think [scrapy] INFO log messages are not necessary.", "issue_status": "Closed", "issue_reporting_time": "2015-08-11T22:47:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1155": {"issue_url": "https://github.com/scrapy/scrapy/issues/1422", "issue_id": "#1422", "issue_summary": "Html http-equiv Tag with status code 200", "issue_description": "AlanChien commented on Aug 11, 2015\nDear Scrapy developer,\nI crawl following url :\n\"http://www.baidu.com/link?url=trNkDflaMpWNZbbB5OphH_bi_8jMpse4zyjTFrexyYw-XAg9M9VmS6YfvsEdUaJEbHKMKfdoPu1LUVMynFNXv1DVVRE6vnmYvCZ1s4jkJae&wd=&eqid=ac8e85b700005ad90000000455c83eee\"\nand response with status code 200 and content as follow:\n\"<script>window.location.replace(\"http://testrepository.readthedocs.org/en/latest/MANUAL.html\")</script>\\n<noscript><META http-equiv=\"refresh\" content=\"0;URL=\\'http://testrepository.readthedocs.org/en/latest/MANUAL.html\\'\"></noscript>\\n\"\nIt's won't auto redirect to correct url, why? Maybe it's not scrapy bug, but is there any solution to solve this?\nVery thanksful.\nBest wishes.", "issue_status": "Closed", "issue_reporting_time": "2015-08-11T02:27:27Z", "fixed_by": "#3768", "pull_request_summary": "Implement the METAREFRESH_IGNORE_TAGS setting", "pull_request_description": "Member\nGallaecio commented on May 8, 2019\nImplements a new setting that can be used to fix #1422.\nI actually believe that the default value should be an empty list, so that we follow an approach more similar to that of web browsers, and #1422 gets fixed automatically without requiring changing a setting. But these changes aim for backward compatibility.", "pull_request_status": "Merged", "issue_fixed_time": "2019-06-25T20:25:58Z", "files_changed": [["10", "docs/topics/downloader-middleware.rst"], ["4", "scrapy/downloadermiddlewares/redirect.py"], ["1", "scrapy/settings/default_settings.py"], ["4", "scrapy/utils/response.py"], ["19", "tests/test_downloadermiddleware_redirect.py"]]}, "1156": {"issue_url": "https://github.com/scrapy/scrapy/issues/1418", "issue_id": "#1418", "issue_summary": "unable to pull some urls", "issue_description": "rbaral commented on Aug 10, 2015\nHi,\nI tried the example spider with the url https://scholar.google.com and it didn't work.\nI just had copied the spider file and removed unwanted things (for me). Mine version of spider file is as shown below:\nimport scrapy\nclass ScholarlySpider(scrapy.Spider):\nname = \"scholar\"\nallowed_domains = [\"scholar.google.com\"]\nbaseURL=\"https://scholar.google.com\"\nstart_urls = [baseURL]\ndef parse(self, response):\n        #nothing specific in this method right now\n        print response\nMine items.py looks like this:\nimport scrapy\nclass ScholarlycrawlItem(scrapy.Item):\n# define the fields for your item here like:\n# name = scrapy.Field()\ntitle = scrapy.Field()\nlink = scrapy.Field()\ndesc = scrapy.Field()\nI have other files - pipelines.py and settings.py as the default one when creating the new scrapy project.\nSorry, I couldn't figure out how to format the code section in this bug tracking system, but I hope it is still clear as I dont have that much code section. The screenshot of the program execution is included in three images - error1 - error3.jpg because I couldnot grab the whole screen in a single image.\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2015-08-10T18:05:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1157": {"issue_url": "https://github.com/scrapy/scrapy/issues/1417", "issue_id": "#1417", "issue_summary": "Scrapy --lsprof has no tests", "issue_description": "Member\nkmike commented on Aug 10, 2015\nWe should add some tests for --lsprof command-line option.", "issue_status": "Closed", "issue_reporting_time": "2015-08-10T07:41:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1158": {"issue_url": "https://github.com/scrapy/scrapy/issues/1416", "issue_id": "#1416", "issue_summary": "It seems like the last callback is executed for only one url but as many times as the total number of URLs", "issue_description": "teddb commented on Aug 9, 2015\nThe following post had been brought up previously, but can someone post an explanation?\nVery tricky problem that I don't know how to debug. I have a spider which is a 3-level deep:\ndef parse(self, response):\n    # some treatment\n    # a loop\n         request = scrapy.Request(url=<calculated_url>, callback=parseChapter)\n         request.meta['item'] = # a dictionary containing some data of the just parsed page\n         yield request\n\ndef parseChapter(self, response):\n    # some treatment\n    # a loop\n         request = scrapy.Request(url=<calculated_url>, callback=parseCategory)\n         request.meta['item'] = # a dictionary containing some data of the just parsed page\n         # print request.meta['item'] is good and different in every iteration\n         yield request\n\ndef parseCategory(self, response):\n    # print response.meta['item'] is not good because it displays the same value many times\n    # for every new call of parseChapter, meta['item'] received is always the same\n    # some treatment\nThe weird thing is that the passed dictionary in the meta of the request is the same in parseCategory, and response.url into parseCateogry is fine.\nThis problem I'm talking about happens only in the callback parseCategory, it does work well in parseChapter (passed meta is good and different for every new call)\nCan you help me out with this?", "issue_status": "Closed", "issue_reporting_time": "2015-08-08T23:14:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1159": {"issue_url": "https://github.com/scrapy/scrapy/issues/1410", "issue_id": "#1410", "issue_summary": "allow specifying max_active_size for scraper.slot", "issue_description": "vincentlaucy commented on Aug 5, 2015\nIt's seems to me that such behaviour is undocumented - scraper will be in back out state once there is over 5M requests, as in\nscrapy/scrapy/core/scraper.py\nLine 29 in 342cb62\n self.max_active_size = max_active_size \nwhere the default value is always used.\nscrapy/scrapy/core/scraper.py\nLine 79 in 342cb62\n self.slot = Slot() \nI am reaching a deadlock scenario that I have over 5M requests in the slot, then since enqueue request will check for backout, new request can never be completed.\nI can submit a PR if it make sense to extract to settings, say SCRAPER_SLOT_MAX_ACTIVE_SIZE", "issue_status": "Closed", "issue_reporting_time": "2015-08-05T06:52:50Z", "fixed_by": "#3551", "pull_request_summary": "[MRG+1] Add ability to change max_active_size by setting", "pull_request_description": "Contributor\njpbalarini commented on Dec 26, 2018\nThis PR fixes #1410\nIn our setup we had urls that were bigger than 5Mb (the default value) and we were reaching a deadlock scenario.\nThese changes allow for configuration of the max_active_size by setting the SCRAPER_SLOT_MAX_ACTIVE_SIZE configuration.\nThanks!\n\ud83d\udc4d 2", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-23T18:40:06Z", "files_changed": [["11", "docs/topics/settings.rst"], ["2", "scrapy/core/scraper.py"], ["2", "scrapy/settings/default_settings.py"]]}, "1160": {"issue_url": "https://github.com/scrapy/scrapy/issues/1407", "issue_id": "#1407", "issue_summary": "scrapy Request failing silently in some conditions", "issue_description": "Contributor\npawelmhm commented on Aug 3, 2015\nLooks like Scrapy is silently failing in case user attempts to make requests to url which meets two conditions:\nrequest url is incorrect, e.g. no netloc\nrequest url is outside allowed_domains\nconsider following simple spider\nfrom scrapy import Spider, Request\n\n\nclass SimpleSpider(Spider):\n    name = \"simple\"\n    start_urls = [\n        \"http://httpbin.org/get\"\n    ]\n    allowed_domains = [\"httpbin.org\"]\n\n    def parse(self, response):\n        url_not_allowed = \"http:/github.com\"\n        yield Request(url_not_allowed)\nrequested url is not in allowed_domains and is additionally incorrect. When you run this spider, you get following output (silent failure)\n2015-08-03 16:20:20 [scrapy] INFO: Spider opened\n2015-08-03 16:20:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-08-03 16:20:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-08-03 16:20:21 [scrapy] DEBUG: Crawled (200) <GET http://httpbin.org/get> (referer: None)\n2015-08-03 16:20:21 [scrapy] INFO: Closing spider (finished)\nno exception is printed despite the fact that url is incorrect and not allowed.\nExpected output: there should be either exception about incorrect url, or warning saying Filtered offsite request to 'github.com'.", "issue_status": "Closed", "issue_reporting_time": "2015-08-03T14:24:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1161": {"issue_url": "https://github.com/scrapy/scrapy/issues/1406", "issue_id": "#1406", "issue_summary": "Scrapy and RotatingFileHandler", "issue_description": "zelenij commented on Aug 3, 2015\nWouldn't it be nice if one could configure scrapy to use RotatingFileHandler instead of just FileHandler for logging? With something like regularly running process it makes sense, doesn't it? :)", "issue_status": "Closed", "issue_reporting_time": "2015-08-02T21:06:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1162": {"issue_url": "https://github.com/scrapy/scrapy/issues/1404", "issue_id": "#1404", "issue_summary": "Exception in LxmLinkExtractor.extract_links 'NoneType' object has no attribute 'iter'", "issue_description": "aldarund commented on Aug 2, 2015\nStacktrace (most recent call last):\n\n  File \"scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"scrapy/spiders/crawl.py\", line 69, in _parse_response\n    for requests_or_item in iterate_spider_output(cb_res):\n  File \"ex_link_crawl/spiders/external_link_spider.py\", line 45, in parse_obj\n    for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 108, in extract_links\n    links = self._extract_links(doc, response.url, response.encoding, base_url)\n  File \"scrapy/linkextractors/__init__.py\", line 103, in _extract_links\n    return self.link_extractor._extract_links(*args, **kwargs)\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 50, in _extract_links\n    for el, attr, attr_val in self._iter_links(selector._root):\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 38, in _iter_links\n    for el in document.iter(etree.Element):\nMy use of extractor is following:\ndef parse_obj(self, response):\n        if not isinstance(response, HtmlResponse):\n            return\n        for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n            if not link.nofollow:\n                yield LinkCrawlItem(domain=link.url)", "issue_status": "Closed", "issue_reporting_time": "2015-08-02T12:26:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1163": {"issue_url": "https://github.com/scrapy/scrapy/issues/1402", "issue_id": "#1402", "issue_summary": "Exception in LxmLinkExtractor.extract_links ValueError(\"Invalid IPv6 URL\")", "issue_description": "aldarund commented on Aug 2, 2015\nI have a lot of this records in my error log.\nStacktrace (most recent call last):\n\n  File \"scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"scrapy/spiders/crawl.py\", line 69, in _parse_response\n    for requests_or_item in iterate_spider_output(cb_res):\n  File \"ex_link_crawl/spiders/external_link_spider.py\", line 45, in parse_obj\n    for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 108, in extract_links\n    links = self._extract_links(doc, response.url, response.encoding, base_url)\n  File \"scrapy/linkextractors/__init__.py\", line 103, in _extract_links\n    return self.link_extractor._extract_links(*args, **kwargs)\n  File \"scrapy/linkextractors/lxmlhtml.py\", line 52, in _extract_links\n    attr_val = urljoin(base_url, attr_val)\n  File \"python2.7/urlparse.py\", line 261, in urljoin\n    urlparse(url, bscheme, allow_fragments)\n  File \"python2.7/urlparse.py\", line 143, in urlparse\n    tuple = urlsplit(url, scheme, allow_fragments)\n  File \"python2.7/urlparse.py\", line 191, in urlsplit\n    raise ValueError(\"Invalid IPv6 URL\")\nI pass the response to the link extractor and get this error, so dont think its a error in my code.\n    def parse_obj(self, response):\n        if not isinstance(response, HtmlResponse):\n            return\n        for link in LxmlLinkExtractor(allow=(), deny=self.allowed_domains).extract_links(response):\n            if not link.nofollow:\n                yield LinkCrawlItem(domain=link.url)", "issue_status": "Closed", "issue_reporting_time": "2015-08-02T12:21:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1164": {"issue_url": "https://github.com/scrapy/scrapy/issues/1392", "issue_id": "#1392", "issue_summary": "rename \"anon\" Slot classes in the code", "issue_description": "Contributor\nnyov commented on Jul 29, 2015\nI wonder what people would say to this change?\ndiff --git a/scrapy/core/downloader/__init__.py b/scrapy/core/downloader/__init__.py\nindex 9654718..743a891 100644\n--- a/scrapy/core/downloader/__init__.py\n+++ b/scrapy/core/downloader/__init__.py\n@@ -15,7 +15,7 @@ from .middleware import DownloaderMiddlewareManager\n from .handlers import DownloadHandlers\n\n\n-class Slot(object):\n+class DownloadSlot(object):\n     \"\"\"Downloader slot\"\"\"\n\n     def __init__(self, concurrency, delay, randomize_delay):\n@@ -48,7 +48,7 @@ class Slot(object):\n\n     def __str__(self):\n         return (\n-            \"<downloader.Slot concurrency=%r delay=%0.2f randomize_delay=%r \"\n+            \"<downloader.DownloadSlot concurrency=%r delay=%0.2f randomize_delay=%r \"\n             \"len(active)=%d len(queue)=%d len(transferring)=%d lastseen=%s>\" % (\n                 self.concurrency, self.delay, self.randomize_delay,\n                 len(self.active), len(self.queue), len(self.transferring),\n@@ -105,7 +105,7 @@ class Downloader(object):\n         if key not in self.slots:\n             conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n             conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n-            self.slots[key] = Slot(conc, delay, self.randomize_delay)\n+            self.slots[key] = DownloadSlot(conc, delay, self.randomize_delay)\n\n         return key, self.slots[key]\n\ndiff --git a/scrapy/core/engine.py b/scrapy/core/engine.py\nindex 992327b..6e1a47b 100644\n--- a/scrapy/core/engine.py\n+++ b/scrapy/core/engine.py\n@@ -21,7 +21,7 @@ from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n logger = logging.getLogger(__name__)\n\n\n-class Slot(object):\n+class EngineSlot(object):\n\n     def __init__(self, start_requests, close_if_idle, nextcall, scheduler):\n         self.closing = False\n@@ -231,7 +231,7 @@ class ExecutionEngine(object):\n         nextcall = CallLaterOnce(self._next_request, spider)\n         scheduler = self.scheduler_cls.from_crawler(self.crawler)\n         start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n-        slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n+        slot = EngineSlot(start_requests, close_if_idle, nextcall, scheduler)\n         self.slot = slot\n         self.spider = spider\n         yield scheduler.open(spider)\ndiff --git a/scrapy/core/scraper.py b/scrapy/core/scraper.py\nindex 244499b..3082342 100644\n--- a/scrapy/core/scraper.py\n+++ b/scrapy/core/scraper.py\n@@ -20,7 +20,7 @@ from scrapy.core.spidermw import SpiderMiddlewareManager\n logger = logging.getLogger(__name__)\n\n\n-class Slot(object):\n+class ScraperSlot(object):\n     \"\"\"Scraper slot (one per running spider)\"\"\"\n\n     MIN_RESPONSE_SIZE = 1024\n@@ -76,7 +76,7 @@ class Scraper(object):\n     @defer.inlineCallbacks\n     def open_spider(self, spider):\n         \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n-        self.slot = Slot()\n+        self.slot = ScraperSlot()\n         yield self.itemproc.open_spider(spider)\n\n     def close_spider(self, spider):", "issue_status": "Closed", "issue_reporting_time": "2015-07-29T05:51:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1165": {"issue_url": "https://github.com/scrapy/scrapy/issues/1389", "issue_id": "#1389", "issue_summary": "inaccurate doc string in utils.python.isbinarytext", "issue_description": "Contributor\nGregoryVigoTorres commented on Jul 28, 2015\nIt says:\n\"\"\"Return True if the given text is considered binary, or False\notherwise, by looking for binary bytes at their chars\n\"\"\"\nHowever, instead of returning False, a TypeError is raised if text is not bytes.\ncontext:\nPorting responsetypes to Python 3\non branch tmp-py3, just pulled a few minutes ago\nrunning\n<tests.test_responsetypes.ResponseTypesTest testMethod=test_from_body>", "issue_status": "Closed", "issue_reporting_time": "2015-07-28T12:53:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1166": {"issue_url": "https://github.com/scrapy/scrapy/issues/1387", "issue_id": "#1387", "issue_summary": "Recursive POST requests from RedirectMiddleware", "issue_description": "rverbitsky commented on Jul 27, 2015\nHello, dear Scrapy maintainers!\nToday I've got crazy recursive POST redirects from FormRequest's with server responded 301.\nChanging\nredirected = request.replace(url=redirected_url)\nto\nredirected = request.replace(url=redirected_url, method='GET')\nin RedirectMiddleware solve's this problem.\nIs this a bug?", "issue_status": "Closed", "issue_reporting_time": "2015-07-27T05:10:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1167": {"issue_url": "https://github.com/scrapy/scrapy/issues/1378", "issue_id": "#1378", "issue_summary": "Using OrderedDict instead of Dict in items.py", "issue_description": "chiragmatkar commented on Jul 23, 2015\nCan we get order list of items in output in same order as we populate the Dict ?", "issue_status": "Closed", "issue_reporting_time": "2015-07-23T12:31:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1168": {"issue_url": "https://github.com/scrapy/scrapy/issues/1377", "issue_id": "#1377", "issue_summary": "response.body contains no HTML", "issue_description": "Contributor\ndarshanime commented on Jul 23, 2015\nMy target site issues a redirect which when I stop forcefully causes the response object to come back blank without any HTML. Also, on another site, it contains the flash screen (used to denote Loading of results).\nIs there a way to simulate the page loading and then pull the HTML into the response object?", "issue_status": "Closed", "issue_reporting_time": "2015-07-22T21:39:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1169": {"issue_url": "https://github.com/scrapy/scrapy/issues/1376", "issue_id": "#1376", "issue_summary": "How to tell Scrapy to stop following pages in the middle of the program?", "issue_description": "wolfgangersmith commented on Jul 23, 2015\nOkay, Basically\nScrapy has to follow 10 pages & each page has ---> 100 links\nWhile scraping data from each link (Request Object created per link) I compute the TIMESTAMP and if it's \"too old\" (according to a program-pre-defined variable) It must STOP following links(pages). However it must NOT stop the program, but rather STOP following links and finish up scraping the \"Request\" objects it had previously created untill that point, and in doing so it will eventually come to a halt (it runs out links to scrape)\nIs there anyway i can do this in scrapy?\nI tried raising a CloseSpider Exception, but scrapy would not complete scraping all the \"Request\" objects it had already created.", "issue_status": "Closed", "issue_reporting_time": "2015-07-22T20:33:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1170": {"issue_url": "https://github.com/scrapy/scrapy/issues/1375", "issue_id": "#1375", "issue_summary": "How to delete existing model object from DB for saving new one through piplines?", "issue_description": "rajeshbk042 commented on Jul 22, 2015\nI have wrote new delete function to delete existing model item if commit=False, but its not working, not showing any error. How can I achieve this? because I want to implement dynamic custom crawler as all scraping sites are dynamic, its updating contents at each specific time.\nHelp me please....\nclass SaveItem(DjangoItem):\ndjango_model = ThinData\ndef save(self, commit=True):\n    if not commit:\n        self.delete(self.instance)       \n\ndef delete(self, item):\n    \"\"\"\n\n    :param item:\n    :return:\n    \"\"\"\n\n    item.delete()", "issue_status": "Closed", "issue_reporting_time": "2015-07-22T07:24:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1171": {"issue_url": "https://github.com/scrapy/scrapy/issues/1374", "issue_id": "#1374", "issue_summary": "[Feature Request] Supporting argument `formcss` in `scrapy.http.request.FormRequest.from_response`", "issue_description": "Contributor\nstarrify commented on Jul 22, 2015\nIt has been supporting formxpath, and I would like to say that it is not a bad idea for supporting formcss, too.\nRelated issue(s): #1136", "issue_status": "Closed", "issue_reporting_time": "2015-07-22T05:39:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1172": {"issue_url": "https://github.com/scrapy/scrapy/issues/1370", "issue_id": "#1370", "issue_summary": "Signals spider_closed", "issue_description": "Yegorov commented on Jul 18, 2015\nWhy at the end of the spider does not work callback?\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.crawler import Crawler\nfrom scrapy.utils.project import get_project_settings\nfrom scrapy import signals\n\ndef t():\n  print \"\\n\\n\\nTEST\\n\\n\\n\"\n\ndef p(text):\n  print text\n\ndef test(my):\n    def tt():\n        print my\n    return tt\n\nspider = MySpider()\ncrawler = Crawler(spider, settings)\n\ncrawler.signals.connect(test(\"\\n\\n\\nTEST\\n\\n\\n\"), signal=signals.spider_closed) # not working\ncrawler.signals.connect(lambda : p(\"\\n\\n\\nTEST\\n\\n\\n\"), signal=signals.spider_closed) # not working\ncrawler.signals.connect(t, signal=signals.spider_closed) # only way", "issue_status": "Closed", "issue_reporting_time": "2015-07-17T22:32:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1173": {"issue_url": "https://github.com/scrapy/scrapy/issues/1369", "issue_id": "#1369", "issue_summary": "HttpProxyMiddleware", "issue_description": "StagnantIce commented on Jul 17, 2015\nHello all, i use scrapy to parse some prices, but search result is empty.\nscrapy fetch https://market.yandex.ru/product/11168521/offers?how=aprice\nand\nwget https://market.yandex.ru/product/11168521/offers?how=aprice\nhave different results. Also. if I use browser with proxy, i see search results. If I use scrapy i see message \"No result for this filter, try change it\"\nIf HttpProxyMiddleware disable, itsl worked.\nScrapy 0.24.5\nThank you.", "issue_status": "Closed", "issue_reporting_time": "2015-07-17T12:43:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1174": {"issue_url": "https://github.com/scrapy/scrapy/issues/1368", "issue_id": "#1368", "issue_summary": "ImagesPipeline not updating the 'images' attribute in item", "issue_description": "ritesh-gopal commented on Jul 16, 2015\nI am using Scrapy 0.25 with Portia to scrape sites.\nI enabled the ImagesPipeline in my project\nAfter deploying to scrapyd, I have been able to save the images to my specified directory.\nAccording to the documentation, the ImagesPipeline will update the 'item' with an attribute 'images' indicating the file name of the saved image. The issue is that when I inspect my items scraped from the scrapyd UI, there is no 'images' attribute.So there is no way to link the downloaded images to my items.", "issue_status": "Closed", "issue_reporting_time": "2015-07-16T17:10:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1175": {"issue_url": "https://github.com/scrapy/scrapy/issues/1367", "issue_id": "#1367", "issue_summary": "ImagesPipeline broke?", "issue_description": "Venereo commented on Jul 16, 2015\nwhen adding scrapy.pipelines.images.ImagesPipeline in pipelines, scrapy shows nothing:\n2015-07-16 15:55:49 [scrapy] INFO: Enabled item pipelines: \n2015-07-16 15:55:49 [scrapy] INFO: Spider opened\nchanging the very same line to scrapy.pipelines.files.FilesPipeline works perfectly:\n2015-07-16 16:10:54 [scrapy] INFO: Enabled item pipelines: FilesPipeline\n2015-07-16 16:10:54 [scrapy] INFO: Spider opened\nno errors, nothing, just ignores the ImagesPipeline\nroot@kali:~# scrapy --version\nScrapy 1.1.0dev1 - no active project\nAt least also in 1.0.1", "issue_status": "Closed", "issue_reporting_time": "2015-07-16T15:14:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1176": {"issue_url": "https://github.com/scrapy/scrapy/issues/1365", "issue_id": "#1365", "issue_summary": "A way to crawl a single page without ScrapyRT?", "issue_description": "Contributor\nGranitosaurus commented on Jul 16, 2015\nScrapyRT lets you provide url and a callback to crawl a single page. I think this feature should exist in scrapy itself in a form of new Spider class/modification\nPeople usually comment out a test start_request() method with test Request to crawl a single for testing purposes. Though maybe a new Spider class or addition to scrapy.Spider class would be appropriate solution ?\nSomething like:\nclass SingleSpider(scrapy.Spider):\n    single_kwargs = {'url': '',\n                     'callback': '',\n                     'meta': {}}\n\n    def __init__(self, **kwargs):\n        super(SingleSpider, self).__init__(**kwargs)\n        single_kwargs = kwargs.get('single', '{}')\n        self.single_kwargs.update(eval(single_kwargs))\n\n    def start_requests(self):\n        callback = getattr(self, self.single_kwargs.pop('callback'), None)\n        if self.single_kwargs['url']:\n            return [scrapy.Request(callback=callback, **self.single_kwargs)]\n        else:\n            return self.generate_start_requests()\n\n    def generate_start_requests(self):\n        \"\"\"normal start_requests from scrapy.Spider\"\"\"\n        return super(SingleSpider, self).start_requests()\n\nclass ShBlogSpider(SingleSpider):\n    name = \"blog.scrapinghub\"\n    single_kwargs = {'callback': 'parse_blog', 'meta': {'index': '0'}}\n    ...\nwhich allows to scrape single page with something like:\n scrapy crawl blog.scrapinghub -a single=\"{'url': 'http://blog.scrapinghub.com/2015/06/19/link-analysis-algorithms-explained/'}\"\nThere are several issues with the spider above (like no formrequest support and potentially breaking on some spider kwargs) but those could be resolved.", "issue_status": "Closed", "issue_reporting_time": "2015-07-16T12:00:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1177": {"issue_url": "https://github.com/scrapy/scrapy/issues/1361", "issue_id": "#1361", "issue_summary": "Feature Request: Allow setting `host` for S3 FilePipeline", "issue_description": "zachguo commented on Jul 15, 2015\nPer boto/boto#621, a AWS_S3_HOST in settings would be helpful.", "issue_status": "Closed", "issue_reporting_time": "2015-07-15T06:40:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1178": {"issue_url": "https://github.com/scrapy/scrapy/issues/1360", "issue_id": "#1360", "issue_summary": "UnicodeEncodeError on LinkExtractor's extract_links", "issue_description": "nautilus28 commented on Jul 13, 2015\nI recently upgraded scrapy to version 1.0.1. When I tried:\nscrapy shell http://baywindows.com/my-very-gay-straight-cruise-123436\nIn [1]: from scrapy.linkextractors import LinkExtractor\nIn [2]: link_extractor = LinkExtractor()\nIn [3]: link_extractor.extract_links(response)\nI got this exception: UnicodeEncodeError: 'charmap' codec can't encode characters in position 9-18: character maps to\nIs it a bug or Scrapy version 1.0.1 has a few changes that I was not aware of?\nthanks in advance,\nCanh", "issue_status": "Closed", "issue_reporting_time": "2015-07-13T08:50:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1179": {"issue_url": "https://github.com/scrapy/scrapy/issues/1356", "issue_id": "#1356", "issue_summary": "How soon would Scrapy support python3.3+?", "issue_description": "wolfgangersmith commented on Jul 12, 2015\nScrapy is my favourite web crawler,\nI wanted to know an approximate timeframe if possible for when scrapy would be able to support python3.3+\nThere are an increasing amount of opensource projects which do not support python2.7 so this made me wonder.\nThanks", "issue_status": "Closed", "issue_reporting_time": "2015-07-12T07:07:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1180": {"issue_url": "https://github.com/scrapy/scrapy/issues/1347", "issue_id": "#1347", "issue_summary": "logging level issues in 1.0.1", "issue_description": "ileodo commented on Jul 10, 2015\nUnder the following setting up:\nin setting.py:\nLOG_ENABLED = False\nin script to start the crawler:\nconfigure_logging()\nprocess = CrawlerProcess(get_project_settings())\nprocess.crawl('retriever')\nprocess.start()\nin crawler:\nlogging.debug(\"PC get page [%s]:- %s\" % (item['id'], item['url']))\nThere are still a lot of logging info.\neven:\n2015-07-09 23:29:20 [scrapy] DEBUG: Scraped from <200 http://www.example.com>\nIs is a bug?", "issue_status": "Closed", "issue_reporting_time": "2015-07-09T22:30:36Z", "fixed_by": "#3762", "pull_request_summary": "[MRG+1] doc: update configure_logging docs to discourage use with CrawlerProcess", "pull_request_description": "Contributor\nVandenn commented on May 2, 2019\nUpdate to the configure_logging docs in order to prevent users from using this function with CrawlerProcess as this produces unexpected behavior.\nFixes #1347", "pull_request_status": "Merged", "issue_fixed_time": "2019-05-09T15:59:21Z", "files_changed": [["7", "docs/topics/logging.rst"]]}, "1181": {"issue_url": "https://github.com/scrapy/scrapy/issues/1346", "issue_id": "#1346", "issue_summary": "PickleFifoDiskQueue and FifoMemoryQueue throw EOF when used to persist a crawl", "issue_description": "Varriount commented on Jul 8, 2015\nWhen using scrapy's PickleFifoDiskQueue and FifoMemoryQueue objects for persistence and request scheduling, an EOF exception is thrown:\nTraceback (most recent call last):\n  File \"C:\\x64\\python27\\lib\\site-packages\\scrapy-1.1.0dev1-py2.7.egg\\scrapy\\commands\\crawl.py\", line 58, in run\n    self.crawler_process.start()\n  File \"C:\\x64\\python27\\lib\\site-packages\\scrapy-1.1.0dev1-py2.7.egg\\scrapy\\crawler.py\", line 253, in start\n    reactor.run(installSignalHandlers=False)  # blocking call\n  File \"C:\\x64\\python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 1194, in run\n    self.mainLoop()\n  File \"C:\\x64\\python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 1203, in mainLoop\n    self.runUntilCurrent()\n--- <exception caught here> ---\n  File \"C:\\x64\\python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 825, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"C:\\x64\\python27\\lib\\site-packages\\scrapy-1.1.0dev1-py2.7.egg\\scrapy\\utils\\reactor.py\", line 41, in __call__\n    return self._func(*self._a, **self._kw)\n  File \"C:\\x64\\python27\\lib\\site-packages\\scrapy-1.1.0dev1-py2.7.egg\\scrapy\\core\\engine.py\", line 105, in _next_request\n    if not self._next_request_from_scheduler(spider):\n  File \"C:\\x64\\python27\\lib\\site-packages\\scrapy-1.1.0dev1-py2.7.egg\\scrapy\\core\\engine.py\", line 132, in _next_request_from_scheduler\n    request = slot.scheduler.next_request()\n  File \"C:\\x64\\python27\\lib\\site-packages\\scrapy-1.1.0dev1-py2.7.egg\\scrapy\\core\\scheduler.py\", line 68, in next_request\n    request = self._dqpop()\n  File \"C:\\x64\\python27\\lib\\site-packages\\scrapy-1.1.0dev1-py2.7.egg\\scrapy\\core\\scheduler.py\", line 98, in _dqpop\n    d = self.dqs.pop()\n  File \"C:\\x64\\python27\\lib\\site-packages\\queuelib\\pqueue.py\", line 43, in pop\n    m = q.pop()\n  File \"C:\\x64\\python27\\lib\\site-packages\\scrapy-1.1.0dev1-py2.7.egg\\scrapy\\squeues.py\", line 21, in pop\n    return deserialize(s)\nexceptions.EOFError:\nThere isn't any problem when not persisting the crawl (omitting '-s JOBDIR=crawl-1'), so my best guess is that the problem lies mainly with PickleFifoDiskQueue.\nI'm running Python 2.7 x64 on Windows 8 x64, using the latest Scrapy from the master branch. This bug affects the latest stable Scrapy build as well.\nEdit: After some investigation, it seems that a race condition is occurring when the FifoDiskQueue object's headf and tailf file descriptors point to the same file. Adding a 'sleep' to the pop() method greatly decreases the occurrence of the EOF exception over multiple runs.", "issue_status": "Closed", "issue_reporting_time": "2015-07-08T03:58:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1182": {"issue_url": "https://github.com/scrapy/scrapy/issues/1344", "issue_id": "#1344", "issue_summary": "boto caught exception on reading instance data", "issue_description": "nautilus28 commented on Jul 7, 2015\nWhen I ran a simple spider on my virtualenv with boto installed, although I don't use any S3 related middlewares, Scrapy raised this exception:\nTraceback (most recent call last):\nFile \"/.../lib/python2.7/site-packages/boto/utils.py\", line 210, in retry_url\nr = opener.open(req, timeout=timeout)\nFile \"/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 431, in open\nresponse = self._open(req, data)\nFile \"/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 449, in _open\n'_open', req)\nFile \"/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 409, in _call_chain\nresult = func(*args)\nFile \"/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 1227, in http_open\nreturn self.do_open(httplib.HTTPConnection, req)\nFile \"/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py\", line 1197, in do_open\nraise URLError(err)\nURLError:\n2015-07-07 09:20:26 [boto] ERROR: Unable to read instance data, giving up\nenvironment:\nOS X Yosemite 10.10.4\npython 2.7.10\nscrapy 1.0.1\nboto 2.38.0\nAny helps?\nThanks in advance,\nCanh", "issue_status": "Closed", "issue_reporting_time": "2015-07-07T02:28:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1183": {"issue_url": "https://github.com/scrapy/scrapy/issues/1343", "issue_id": "#1343", "issue_summary": "custom_settings isn't reflected in the \"Overridden settings:\" log message", "issue_description": "nautilus28 commented on Jul 7, 2015\nHi,\nI would like to notice that custom_settings per spider does not work properly. Global settings updated nothing when I added custom_settings in spiders, and unittests for that are missing too.\nRegards,\nCanh", "issue_status": "Closed", "issue_reporting_time": "2015-07-07T01:38:43Z", "fixed_by": "#2894", "pull_request_summary": "Move logging of overriden settings to Crawler init", "pull_request_description": "Contributor\nredapple commented on Aug 23, 2017 \u2022\nedited\nFixes #1343\nProposal for #1343.\nThe idea here is to keep log_scrapy_info() only with Scrapy-framework related stuff,\nand move settings and custom settings logging closer to where the framework is instantiated to perform a crawl (Crawler, Engine etc)", "pull_request_status": "Merged", "issue_fixed_time": "2017-08-29T14:20:47Z", "files_changed": [["5", "scrapy/crawler.py"], ["4", "scrapy/utils/log.py"]]}, "1184": {"issue_url": "https://github.com/scrapy/scrapy/issues/1342", "issue_id": "#1342", "issue_summary": "Can't install on OSX 10.10.4", "issue_description": "jooddang commented on Jul 7, 2015\n[jooddang@a ~]$ scrapy startproject tutorial\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 7, in\nfrom scrapy.cmdline import execute\nFile \"/Library/Python/2.7/site-packages/scrapy/init.py\", line 48, in\nfrom scrapy.spiders import Spider\nFile \"/Library/Python/2.7/site-packages/scrapy/spiders/init.py\", line 10, in\nfrom scrapy.http import Request\nFile \"/Library/Python/2.7/site-packages/scrapy/http/init.py\", line 12, in\nfrom scrapy.http.request.rpc import XmlRpcRequest\nFile \"/Library/Python/2.7/site-packages/scrapy/http/request/rpc.py\", line 7, in\nfrom six.moves import xmlrpc_client as xmlrpclib\nImportError: cannot import name xmlrpc_client\nOn Python console\nimport scrapy\nmakes error - can't import _monkeypatches module", "issue_status": "Closed", "issue_reporting_time": "2015-07-06T23:27:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1185": {"issue_url": "https://github.com/scrapy/scrapy/issues/1341", "issue_id": "#1341", "issue_summary": "It seems that something is going wrong.", "issue_description": "jonaprieto commented on Jul 6, 2015\nAfter install scrapy from pip on isolated environment, I could not run the example with success from the website so this is the log at the end:\nFirst:\n$ scrapy runspider myscrapy.py                                                                                                                      [9:51:40]\n2015-07-06 09:52:28 [scrapy] INFO: Scrapy 1.0.1 started (bot: scrapybot)\n2015-07-06 09:52:28 [scrapy] INFO: Optional features available: ssl, http11\n2015-07-06 09:52:28 [scrapy] INFO: Overridden settings: {}\n2015-07-06 09:52:28 [py.warnings] WARNING: :0: UserWarning: You do not have a working installation of the service_identity module: 'No module named service_identity'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n\n2015-07-06 09:52:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n2015-07-06 09:52:28 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-07-06 09:52:28 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-07-06 09:52:28 [scrapy] INFO: Enabled item pipelines:\n2015-07-06 09:52:28 [scrapy] INFO: Spider opened\n2015-07-06 09:52:28 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-07-06 09:52:28 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-07-06 09:52:29 [scrapy] DEBUG: Crawled (200) <GET http://blog.scrapinghub.com> (referer: None)\n2015-07-06 09:52:29 [scrapy] ERROR: Spider error processing <GET http://blog.scrapinghub.com> (referer: None)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n    yield next(it)\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 28, in process_spider_output\n    for x in result:\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py\", line 22, in <genexpr>\n    return (_set_referer(r) for r in result or ())\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py\", line 54, in <genexpr>\n    return (r for r in result or () if _filter(r))\n  File \"/Users/usuario/Desktop/web/myscrapy.py\", line 9, in parse\n    yield scrapy.Request(response.urljoin(url), self.parse_titles)\nNameError: global name 'scrapy' is not defined\n2015-07-06 09:52:29 [scrapy] INFO: Closing spider (finished)\n2015-07-06 09:52:29 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 218,\n 'downloader/request_count': 1,\n 'downloader/request_method_count/GET': 1,\n 'downloader/response_bytes': 26371,\n 'downloader/response_count': 1,\n 'downloader/response_status_count/200': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2015, 7, 6, 14, 52, 29, 221816),\n 'log_count/DEBUG': 2,\n 'log_count/ERROR': 1,\n 'log_count/INFO': 7,\n 'log_count/WARNING': 1,\n 'response_received_count': 1,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'spider_exceptions/NameError': 1,\n 'start_time': datetime.datetime(2015, 7, 6, 14, 52, 28, 827605)}\n2015-07-06 09:52:29 [scrapy] INFO: Spider closed (finished)\nWhat about NameError: global name 'scrapy' is not defined?\npip has installed the package successful.", "issue_status": "Closed", "issue_reporting_time": "2015-07-06T14:55:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1186": {"issue_url": "https://github.com/scrapy/scrapy/issues/1339", "issue_id": "#1339", "issue_summary": "arg 'settings' is not used in func 'update_settings'", "issue_description": "xinghun854 commented on Jul 5, 2015\nversion: 1.1.0dev1\nscrapy/spiders/init.py\n79 def update_settings(cls, settings):\n80 settings.setdict(cls.custom_settings or {}, priority='spider')\narg 'settings' is not used in func 'update_settings', is that right or wrong?", "issue_status": "Closed", "issue_reporting_time": "2015-07-05T11:25:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1187": {"issue_url": "https://github.com/scrapy/scrapy/issues/1338", "issue_id": "#1338", "issue_summary": "Impossible to create exe file when using py2exe", "issue_description": "LEDfan commented on Jul 5, 2015\nHi, thanks for this project! Works good, except one thing \ud83d\ude03\nI have a scrapy project which I want to run as exe and create an installer of it by using the inno setup tool. I'm using this setup.py:\nimport sys\nfrom scrapy import *\nfrom zope.interface import *\nfrom distutils.core import setup\nimport py2exe\n\nbase = None\nif sys.platform == \"win32\":\n    base = \"Win32GUI\"\n\nsetup(\n    name = \"Random123\",\n    version = \"0.1\",\n    description = \"My GUI application!\",\n    options = {\n        \"py2exe\": {\n            \"packages\": [\"pkg_resources\", \"os\", \"twisted\", \"scrapy\", \"test\", 'zope', 'zope.interface', 'pkgutil', 'lxml',\n                         'lxml.etree', 'lxml._elementpath', 'multipart', 'email.mime.multipart', 'email.mime.text',\n                         ],\n            \"includes\": [\"pkg_resources\", \"os\", \"twisted\", \"scrapy\", \"test\", 'zope', 'zope.interface', 'pkgutil', 'lxml',\n                         'lxml._elementpath', 'multipart', 'email.mime.multipart', 'email.mime.text',\n                         ],\n            \"skip_archive\": True\n        },\n    },\n    console=['run.py'],\n)\nIf you would try to run this via python setup.py py2exe it will work. However when you try to run the dist/run.exe file it will fail. Python will tell that some module is trying to compile some C++ files by using cffi. This modules is https://github.com/pyca/cryptography . This is used by scrapy (twisted) for creating SSL connections, right?\nI was able to solve this by removing SSL support from scrapy/twisted, but this is of course not a permanent solution. For now I don't need to scrape HTTPS site, but maybe in the future.\nThe relevant issue is pyca/cryptography#2039\nDo you maybe have a solution for this?", "issue_status": "Closed", "issue_reporting_time": "2015-07-04T19:47:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1188": {"issue_url": "https://github.com/scrapy/scrapy/issues/1337", "issue_id": "#1337", "issue_summary": "tox fails with \"Could not open requirements file\"", "issue_description": "Contributor\nolafdietsche commented on Jul 4, 2015\nFollowing instructions at Running tests, I just ran\ntox\nin the top level scrapy directory. Unfortunately, it failed with\nCould not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\nERROR: could not install deps [-rrequirements.txt, boto, Pillow, leveldb, service_identity, -rtests/requirements.txt]\neven though both requirements.txt and tests/requirements.txt are accessible.\nI worked around this by manually installing the dependencies with pip install ... and removing the deps entry from the tox.ini file. After that, tox ran successfully with only 3 tests failing\n============== 3 failed, 1134 passed, 3 skipped in 204.43 seconds ==============\nThis is the current master branch at commit 2a7dc31.", "issue_status": "Closed", "issue_reporting_time": "2015-07-04T11:03:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1189": {"issue_url": "https://github.com/scrapy/scrapy/issues/1334", "issue_id": "#1334", "issue_summary": "Attribute `handle_httpstatus_list` not working for codes 301 and 302", "issue_description": "iwxfer commented on Jul 3, 2015\nWhen set handle_httpstatus_list = [301, 302], the spider doesn't execute parse. Howeber, it executes for other codes like 404.", "issue_status": "Closed", "issue_reporting_time": "2015-07-03T08:40:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1190": {"issue_url": "https://github.com/scrapy/scrapy/issues/1332", "issue_id": "#1332", "issue_summary": "Need urgent help! Scrapy: command not found", "issue_description": "ikansal2 commented on Jul 2, 2015\nI am trying to use scrapy to build a web crawler. I am using it on Mac OS yosemite and after installing scrapy I tried to run the program given on the scrapy website. But when I try to run it, terminal says scarpy: command not found. I really need help and quick help! Please. I think my scrapy is installed in a different path but I don't know how to change it so that it works.", "issue_status": "Closed", "issue_reporting_time": "2015-07-01T19:51:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1191": {"issue_url": "https://github.com/scrapy/scrapy/issues/1328", "issue_id": "#1328", "issue_summary": "Scrapy Benchmarking", "issue_description": "yiakwy commented on Jun 30, 2015\nI want to make a benchmark tools. But before I do this, I want to use existing scrapy tools or service to do it. I found \"scrapy bench\", and it is interesting. Can anybody kindly tell me how it works and where is the source? If I want to make such a benchmarking script where I should start and what are best practices?\nThank you very much!!!", "issue_status": "Closed", "issue_reporting_time": "2015-06-30T11:41:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1192": {"issue_url": "https://github.com/scrapy/scrapy/issues/1327", "issue_id": "#1327", "issue_summary": "XMLGenerated xml looks ugly", "issue_description": "Andrej730 commented on Jun 30, 2015\nWhenever i trying to get output items in xml i get something like:\n<?xml version=\"1.0\" encoding=\"utf8\"?>\n<news><item><name>0</name><content>00000</content></item><item><name>1</name><content>11111</content></item><item><name>2</name><content>22222</content></item></news>\nMaybe scrapy have some already provided ways to \"beautify\" it to something like (like xmls i found in all docs examples):\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<news>\n   <item>\n      <name>0</name>\n      <content>00000</content>\n   </item>\n   <item>\n      <name>1</name>\n      <content>11111</content>\n   </item>\n   <item>\n      <name>2</name>\n      <content>22222</content>\n   </item>\n</news>", "issue_status": "Closed", "issue_reporting_time": "2015-06-30T10:47:11Z", "fixed_by": "#2456", "pull_request_summary": "[MRG+1] Feed exports: beautify JSON and XML", "pull_request_description": "Member\nelacuesta commented on Dec 19, 2016 \u2022\nedited\nHello folks, these are my two cents to fix #1327.\nI look forward to reading your comments \ud83d\ude04\n\ud83d\udc4d 1", "pull_request_status": "Merged", "issue_fixed_time": "2017-05-12T16:12:49Z", "files_changed": [["13", "docs/topics/exporters.rst"], ["17", "docs/topics/feed-exports.rst"], ["46", "scrapy/exporters.py"], ["5", "scrapy/extensions/feedexport.py"], ["1", "scrapy/settings/default_settings.py"], ["172", "tests/test_feedexport.py"]]}, "1193": {"issue_url": "https://github.com/scrapy/scrapy/issues/1325", "issue_id": "#1325", "issue_summary": "Error on starting a spider", "issue_description": "pigletfly commented on Jun 29, 2015\nHi,I am using scrapy on CentOS,python 2.7.3 under virtualenwrapper,when I start a spider an error occurs,the following is traceback;\nFile \"/root/.virtualenvs/trademark/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/root/.virtualenvs/trademark/lib/python2.7/site-packages/scrapy/cmdline.py\", line 142, in execute\n    cmd.crawler_process = CrawlerProcess(settings)\n  File \"/root/.virtualenvs/trademark/lib/python2.7/site-packages/scrapy/crawler.py\", line 209, in __init__\n    super(CrawlerProcess, self).__init__(settings)\n  File \"/root/.virtualenvs/trademark/lib/python2.7/site-packages/scrapy/crawler.py\", line 115, in __init__\n    self.spider_loader = _get_spider_loader(settings)\n  File \"/root/.virtualenvs/trademark/lib/python2.7/site-packages/scrapy/crawler.py\", line 296, in _get_spider_loader\n    return loader_cls.from_settings(settings.frozencopy())\n  File \"/root/.virtualenvs/trademark/lib/python2.7/site-packages/scrapy/settings/__init__.py\", line 124, in frozencopy\n    copy = self.copy()\n  File \"/root/.virtualenvs/trademark/lib/python2.7/site-packages/scrapy/settings/__init__.py\", line 118, in copy\n    return copy.deepcopy(self)\n  File \"/usr/local/lib/python2.7/copy.py\", line 190, in deepcopy\n    y = _reconstruct(x, rv, 1, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 334, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 163, in deepcopy\n    y = copier(x, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 163, in deepcopy\n    y = copier(x, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 190, in deepcopy\n    y = _reconstruct(x, rv, 1, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 334, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 163, in deepcopy\n    y = copier(x, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 190, in deepcopy\n    y = _reconstruct(x, rv, 1, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 334, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 163, in deepcopy\n    y = copier(x, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 190, in deepcopy\n    y = _reconstruct(x, rv, 1, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 334, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 163, in deepcopy\n    y = copier(x, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 257, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 190, in deepcopy\n    y = _reconstruct(x, rv, 1, memo)\n  File \"/usr/local/lib/python2.7/copy.py\", line 329, in _reconstruct\n    y = callable(*args)\n  File \"/root/.virtualenvs/trademark/lib/python2.7/copy_reg.py\", line 93, in __newobj__\n    return cls.__new__(cls, *args)\nTypeError: object.__new__(thread.lock) is not safe, use thread.lock.__new__()\nThanks", "issue_status": "Closed", "issue_reporting_time": "2015-06-29T02:37:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1194": {"issue_url": "https://github.com/scrapy/scrapy/issues/1323", "issue_id": "#1323", "issue_summary": "PIL instead of Pillow", "issue_description": "timschwab commented on Jun 26, 2015\nI don't know if this qualifies as an issue, but I've noticed that Scrapy uses PIL rather than Pillow for its image manipulations. For example, for the ImagesPipeline. Development in PIL stopped in 2009, but Pillow, a fork of PIL, is now the new standard. This seems to be what I've gathered from the Intertubes and Wikipedia.\nThis isn't a huge deal, but I have been having some issues with PIL. They are fixable, but will get worse as time progresses.", "issue_status": "Closed", "issue_reporting_time": "2015-06-26T14:54:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1195": {"issue_url": "https://github.com/scrapy/scrapy/issues/1322", "issue_id": "#1322", "issue_summary": "Request url transcoding causes error results.", "issue_description": "lhuaizhong commented on Jun 26, 2015\nRequest fetching URL:\nhttp://www.lightingproducts.philips.com/search-tool.html?brand=Chloride#!f=%40Brand%3aChloride&brandId=ba1bd486-2f99-4f80-9546-f05501e66013&isCatSearch=1\nWithin version 0.16.x:\n2015-06-26 16:14:06+0800 [philips3Spider] DEBUG: Crawled (200) GET http://www.lightingproducts.philips.com/search-tool.html?brand=Chloride?_escaped_fragment_=f=%40Brand%3aChloride&brandId=ba1bd486-2f99-4f80-9546-f05501e66013&isCatSearch=1 (referer: http://www.lightingproducts.philips.com/)\nWithin version 0.24.x and 1.0:\n2015-06-26 16:12:35+0800 [philips3Spider] DEBUG: Crawled (200) GET http://www.lightingproducts.philips.com/search-tool.html?brand=Chloride&_escaped_fragment_=f%3D%2540Brand%253aChloride%26brandId%3Dba1bd486-2f99-4f80-9546-f05501e66013%26isCatSearch%3D1 (referer: http://www.lightingproducts.philips.com/)\nIt works on version 0.16 without url encode while fetching, not works on 0.24 and 1.0 due to url encode automatically.\nHow can I avoid url encode in request fetching?\nThanks a lot for help.", "issue_status": "Closed", "issue_reporting_time": "2015-06-26T08:41:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1196": {"issue_url": "https://github.com/scrapy/scrapy/issues/1317", "issue_id": "#1317", "issue_summary": "Versioning and API Stability docs are outdated", "issue_description": "Member\nkmike commented on Jun 25, 2015\nQuotes from http://doc.scrapy.org/en/1.0/versioning.html:\nScrapy uses the odd-numbered versions for development releases.\nSo far, only zero is available for A as Scrapy hasn\u2019t yet reached 1.0.\nAPI stability is one of Scrapy major goals for the 1.0 release, which doesn\u2019t have a due date scheduled yet.", "issue_status": "Closed", "issue_reporting_time": "2015-06-24T19:38:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1197": {"issue_url": "https://github.com/scrapy/scrapy/issues/1314", "issue_id": "#1314", "issue_summary": "ImportError:cryptography.hazmat.bindings.openssl.binding", "issue_description": "adammfrank commented on Jun 24, 2015\nOS: Ubuntu 14.04.2\nPython: 2.7.6, 2.7.8, 2.7.9, 2.7.10 (same result on each)\nAfter updating Scrapy from 0.24 to 1.0 today, I can no longer get anything to run. Everything I run, even for the tutorial, throws the error:\nImportError:cryptography.hazmat.bindings.openssl.binding", "issue_status": "Closed", "issue_reporting_time": "2015-06-23T20:28:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1198": {"issue_url": "https://github.com/scrapy/scrapy/issues/1309", "issue_id": "#1309", "issue_summary": "provide an option to disable scrapy.utils.log.TopLevelFormatter", "issue_description": "Member\nkmike commented on Jun 16, 2015\nLong names in logs (e.g. [scrapy.core.scraper] instead of generic [scrapy]) are useful because\nthey show where the log message came from - this makes debugging easier;\nit has an educational value - users become aware of various Scrapy components;\nto filter out a specific kind of messages (or make them more verbose) user can set a custom log level just for one logger, but to do that user needs to know logger full name.\nMy preference is to even use full names by default, but an option to enable them is also fine.\nThoughts?", "issue_status": "Closed", "issue_reporting_time": "2015-06-15T21:27:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1199": {"issue_url": "https://github.com/scrapy/scrapy/issues/1303", "issue_id": "#1303", "issue_summary": "DOWNLOAD_WARNSIZE is too verbose", "issue_description": "Member\nkmike commented on Jun 15, 2015\nWhen DOWNLOAD_WARNSIZE is set, there is no Content-Length header and warn limit is reached, Scrapy starts to issue WARNING log messages, a message per a couple of KBs downloaded:\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1049869) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1051239) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1052609) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1053979) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1055349) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1056719) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1058089) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1059459) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1060829) bytes larger than download warn size (1048576).\n2015-06-15 16:22:23 [scrapy] WARNING: Received (1062199) bytes larger than download warn size (1048576).\n...\nI think the message should be issued only once.", "issue_status": "Closed", "issue_reporting_time": "2015-06-15T11:31:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1200": {"issue_url": "https://github.com/scrapy/scrapy/issues/1300", "issue_id": "#1300", "issue_summary": "Settings documentation useless", "issue_description": "surfer190 commented on Jun 14, 2015\nPlease look at this documentation from the layman's point of view.\ndocs/topics/settings.rst\nDoes it answer the following:\nHow do I set settings values?\nHow do I retrieve settings values?\nThe documentation assumes that the user is a regular python user.", "issue_status": "Closed", "issue_reporting_time": "2015-06-13T20:58:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1201": {"issue_url": "https://github.com/scrapy/scrapy/issues/1299", "issue_id": "#1299", "issue_summary": "maximum recursion depth exceeded", "issue_description": "seozed commented on Jun 12, 2015\nMy spider returns the error.\nI tried to add sys.setrecursionlimit(10000). but not work.\n2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [scrapy] NOLEVEL: Current UA:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\n2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [scrapy] NOLEVEL: Current UA:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\n2015-06-12 11:49:52+0800 [-] ERROR: Unable to format event {'log_namespace': 'stderr', 'log_level': <LogLevel=error>, 'format': '%(log_legacy)s', 'log_logger': <Logger 'stderr'>, 'log_source': None, 'system': '-', 'log_io': u'2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [scrapy] NOLEVEL: Current UA:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3', 'time': 1434080992.928, 'log_format': u'{log_io}', 'message': (), 'log_time': 1434080992.928}: maximum recursion depth exceeded while calling a Python object\n2015-06-12 11:49:52+0800 [-] ERROR: Temporarily disabling observer LegacyLogObserverWrapper(<bound method ScrapyFileLogObserver._emit_with_crawler of <scrapy.log.ScrapyFileLogObserver instance at 0x05626210>>) due to exception: [Failure instance: Traceback: <type 'exceptions.RuntimeError'>: maximum recursion depth exceeded in __instancecheck__\n    D:\\Python27\\lib\\site-packages\\twisted\\python\\log.py:557:emit\n    D:\\Python27\\lib\\site-packages\\twisted\\python\\util.py:830:untilConcludes\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_io.py:170:write\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_logger.py:132:emit\n    --- <exception caught here> ---\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_observer.py:131:__call__\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_legacy.py:93:__call__\n    D:\\Python27\\lib\\site-packages\\scrapy\\log.py:52:_emit_with_crawler\n    D:\\Python27\\lib\\site-packages\\scrapy\\log.py:48:_emit\n    D:\\Python27\\lib\\site-packages\\twisted\\python\\log.py:557:emit\n    D:\\Python27\\lib\\site-packages\\twisted\\python\\util.py:830:untilConcludes\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_io.py:170:write\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_logger.py:132:emit\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_observer.py:140:__call__\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_logger.py:178:failure\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_logger.py:132:emit\n    D:\\Python27\\lib\\site-packages\\twisted\\logger\\_observer.py:133:__call__\n    D:\\Python27\\lib\\site-packages\\twisted\\python\\failure.py:203:__init__\n    ]\n    Traceback (most recent call last):\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\python\\log.py\", line 557, in emit\n        util.untilConcludes(self.write, timeStr + \" \" + msgStr)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\python\\util.py\", line 830, in untilConcludes\n        return f(*a, **kw)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_io.py\", line 170, in write\n        self.log.emit(self.level, format=u\"{log_io}\", log_io=line)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_logger.py\", line 132, in emit\n        self.observer(event)\n    --- <exception caught here> ---\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_observer.py\", line 131, in __call__\n        observer(event)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_legacy.py\", line 93, in __call__\n        self.legacyObserver(event)\n      File \"D:\\Python27\\lib\\site-packages\\scrapy\\log.py\", line 52, in _emit_with_crawler\n        ev = self._emit(eventDict)\n      File \"D:\\Python27\\lib\\site-packages\\scrapy\\log.py\", line 48, in _emit\n        log.FileLogObserver.emit(self, ev)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\python\\log.py\", line 557, in emit\n        util.untilConcludes(self.write, timeStr + \" \" + msgStr)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\python\\util.py\", line 830, in untilConcludes\n        return f(*a, **kw)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_io.py\", line 170, in write\n        self.log.emit(self.level, format=u\"{log_io}\", log_io=line)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_logger.py\", line 132, in emit\n        self.observer(event)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_observer.py\", line 140, in __call__\n        observer=brokenObserver,\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_logger.py\", line 178, in failure\n        self.emit(level, format, log_failure=failure, **kwargs)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_logger.py\", line 132, in emit\n        self.observer(event)\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\logger\\_observer.py\", line 133, in __call__\n        brokenObservers.append((observer, Failure()))\n      File \"D:\\Python27\\lib\\site-packages\\twisted\\python\\failure.py\", line 203, in __init__\n        if isinstance(exc_value, str) and exc_type is None:\n    exceptions.RuntimeError: maximum recursion depth exceeded in __instancecheck__\nmy spider code:\nimport json\nimport sys\nfrom scrapy.spider import BaseSpider\nfrom scrapy.http import Request\nfrom soufang.items import SoufangItem\nfrom scrapy import log\nreload(sys)\nsys.setdefaultencoding('utf8')\n\nclass SoufangSpider(BaseSpider):\n\n    name = 'soufang'\n    allowed_domains = ['fang.com']\n    start_urls = ['http://fang.com/SoufunFamily.htm']\n\n    log.start('crawl.log', loglevel=log.DEBUG)\n\n    def parse(self, response):\n        \"\"\"\u5206\u6790\u57ce\u5e02\u5217\u8868\u9875\uff0c\u63d0\u53d6\u57ce\u5e02\u5173\u952e\u8bcd\uff0c\u5e76\u6784\u9020json\u9875\u9762\u8bf7\u6c42\"\"\"\n\n        for link in response.css('table#senfe1 tr td a').xpath('@href').extract():\n            self.rnum = True\n            pagenum = 0\n\n            for i in xrange(300):\n                if not link:\n                    continue\n                city = link.split('//')[1].split('.')[0]\n                pagenum += 1\n                cityRootUrl = r'http://newhouse.%s.fang.com/house/s/?x1=111.168964&x2=115.446336&y1=21.921421&y2=24.307946&strDistrict=&strRoundStation=&railway=&strPurpose=&strPrice=&strHuxing=&saling=&strStartDate=&isyouhui=&strOrderBy=&strKeyword=&railway_station=&strComarea=&housetag=&strSort=mobileyh&a=ajaxXfMapSearch&city=%s&PageNo=%d' % (city, city, pagenum)\n                yield Request(cityRootUrl, callback=self.parse_json)\n\n                if not self.rnum:\n                    break\n\n\n    def parse_json(self, response):\n        \"\"\"\u89e3\u6790JSON\u6570\u636e\"\"\"\n\n        if not response.body:\n            return\n        response_data = json.loads(response.body)\n\n        print response_data['rnum']\n        if response_data['rnum'] <= 1:\n            self.rnum = False\n            return\n        for houses in response_data['list']:\n            item = SoufangItem()\n            item.setdefault('title','Null')\n            item.setdefault('x','Null')\n            item.setdefault('y','Null')\n            item.setdefault('district','Null')\n            item.setdefault('address','Null')\n            item.setdefault('newCode','Null')\n            item.setdefault('houseurl','Null')\n            item.setdefault('purpose','Null')\n            item.setdefault('price_num','Null')\n            item.setdefault('price_unit','Null')\n            item.setdefault('tel400','Null')\n            item.setdefault('tagcount','Null')\n            item.setdefault('tags','Null')\n            item.setdefault('alias','Null')\n            item.setdefault('city','Null')\n            item.setdefault('propertyCategory','Null')\n            item.setdefault('projectFeatures','Null')\n            item.setdefault('decorateCondition','Null')\n            item.setdefault('loopLineLocal','Null')\n            item.setdefault('CBDCategory','Null')\n            item.setdefault('plotRatio','Null')\n            item.setdefault('landscapingRatio','Null')\n            item.setdefault('openingDate','Null')\n            item.setdefault('checkInDate','Null')\n            item.setdefault('propertyCompany','Null')\n            item.setdefault('developers','Null')\n            item.setdefault('saleAddress','Null')\n            item.setdefault('propertyAddress','Null')\n            item.setdefault('floorSpace','Null')\n            item.setdefault('areaOfStructure','Null')\n            item.setdefault('equityYears','Null')\n            item.setdefault('agent','Null')\n            item.setdefault('investor','Null')\n            item.setdefault('projectState','Null')\n            item.setdefault('completionTime','Null')\n            item.setdefault('projectConfig','Null')\n            item.setdefault('trafficInfo','Null')\n            item.setdefault('storeInfo','Null')\n            item.setdefault('carportInfo','Null')\n            item.setdefault('projectIntro','Null')\n            item.setdefault('storeNumber','Null')\n            item.setdefault('projectPics','Null')\n            item.setdefault('developersAbout','Null')\n            item.setdefault('buildingCategory','Null')\n            item.setdefault('licence','Null')\n            item.setdefault('decoreteMaterial','Null')\n            item.setdefault('commercialSpace','Null')\n            item.setdefault('saleState','Null')\n            item.setdefault('usedRatio','Null')\n            item.setdefault('ROI','Null')\n            item.setdefault('storeyModelSpace','Null')\n            item.setdefault('StoreySpace','Null')\n            item.setdefault('officeSpace','Null')\n            item.setdefault('houseSpace','Null')\n            item.setdefault('houseFacility','Null')\n            item.setdefault('targetState','Null')\n            item.setdefault('stateArrange','Null')\n            item.setdefault('developersPageUrl','Null')\n            item['title'] = houses.setdefault('title', 'Null')\n            item['x'] = houses.setdefault('x', 'Null')\n            item['y'] = houses.setdefault('y', 'Null')\n            item['district'] = houses.setdefault('district', 'Null')\n            item['address'] = houses.setdefault('address', 'Null')\n            item['newCode'] = houses.setdefault('newCode', 'Null')\n            item['houseurl'] = houses.setdefault('houseurl', 'Null')\n            item['purpose'] = houses.setdefault('purpose', 'Null')\n            item['price_num'] = houses.setdefault('price_num', 'Null')\n            item['price_unit'] = houses.setdefault('price_unit', 'Null')\n            item['tel400'] = houses.setdefault('tel400', 'Null')\n            item['tagcount'] = houses.setdefault('tagcount', 'Null')\n            if not houses['houseurl']:\n                from scrapy.shell import inspect_response\n                inspect_response(response, self)\n            detailurl = '{0[houseurl]}house/{0[newCode]}/housedetail.htm'.format(houses)\n            yield Request(detailurl, meta={'item': item}, callback=self.parse_detail)\n\n\n    def parse_detail(self, response):\n        \"\"\"\u89e3\u6790\u697c\u76d8\u8be6\u60c5\u9875\uff0c\u6700\u591a\u5b57\u6bb5\u7684\u9875\u9762\"\"\"\n\n        item = response.meta['item']\n        fields = {\n            u'\u6807\u7b7e\u7ec4': 'tags',\n            u'\u522b\u540d': 'alias',\n            u'\u7269\u4e1a\u7c7b\u522b': 'propertyCategory',\n            u'\u5199\u5b57\u697c\u7c7b\u578b': 'propertyCategory',\n            u'\u5546\u94fa\u7c7b\u578b': 'propertyCategory',\n            u'\u9879\u76ee\u7279\u8272': 'projectFeatures',\n            u'\u88c5\u4fee\u72b6\u51b5': 'decorateCondition',\n            u'\u73af\u7ebf\u4f4d\u7f6e': 'loopLineLocal',\n            u'\u6240\u5c5e\u5546\u5708': 'CBDCategory',\n            u'\u5bb9\u79ef\u7387': 'plotRatio',\n            u'\u7eff \u5316 \u7387': 'landscapingRatio',\n            u'\u5f00\u76d8\u65f6\u95f4': 'openingDate',\n            u'\u4ea4\u623f\u65f6\u95f4': 'checkInDate',\n            u'\u5165\u4f4f\u65f6\u95f4': 'checkInDate',\n            u'\u4f4f\u623f\u65f6\u95f4': 'checkInDate',\n            u'\u7269\u4e1a\u7ba1\u7406\u516c\u53f8': 'propertyCompany',\n            u'\u7269\u4e1a\u516c\u53f8': 'propertyCompany',\n            u'\u5f00 \u53d1 \u5546': 'developers',\n            u'\u5f00\u53d1\u5546': 'developers',\n            u'\u552e\u697c\u5730\u5740': 'saleAddress',\n            u'\u7269\u4e1a\u5730\u5740': 'propertyAddress',\n            u'\u5360\u5730\u9762\u79ef': 'floorSpace',\n            u'\u5efa\u7b51\u9762\u79ef': 'areaOfStructure',\n            u'\u4ea7\u6743\u5e74\u9650': 'equityYears',\n            u'\u4ea7\u6743\u63cf\u8ff0': 'equityYears',\n            u'\u4ee3\u7406\u5546': 'agent',\n            u'\u6295\u8d44\u5546': 'investor',\n            u'\u5de5\u7a0b\u8fdb\u5ea6': 'projectState',\n            u'\u7ae3\u5de5\u65f6\u95f4': 'completionTime',\n            u'\u9879\u76ee\u914d\u5957': 'projectConfig',\n            u'\u4ea4\u901a\u72b6\u51b5': 'trafficInfo',\n            u'\u697c\u5c42\u72b6\u51b5': 'storeInfo',\n            u'\u8f66\u4f4d\u4fe1\u606f': 'carportInfo',\n            u'\u9879\u76ee\u7b80\u4ecb': 'projectIntro',\n            u'\u5546\u94fa\u603b\u5957\u6570': 'storeNumber',\n            u'\u697c\u76d8\u56fe\u7247': 'projectPics',\n            '#\u4f4f\u5b85\u7c7b': None,\n            u'\u5efa\u7b51\u7c7b\u522b': 'buildingCategory',\n            u'\u9884\u552e\u8bb8\u53ef\u8bc1': 'licence',\n            u'\u5efa\u6750\u88c5\u4fee': 'decoreteMaterial',\n            '#\u5546\u94fa\u7c7b': None,\n            u'\u5546\u4e1a\u9762\u79ef': 'commercialSpace',\n            u'\u79df\u552e\u72b6\u6001': 'saleState',\n            u'\u4f7f\u7528\u7387': 'usedRatio',\n            u'\u6295\u8d44\u56de\u62a5\u7387': 'ROI',\n            u'\u6807\u51c6\u5c42\u9762\u79ef': 'storeyModelSpace',\n            u'\u5355\u5c42\u5546\u94fa\u9762\u79ef': 'StoreySpace',\n            u'\u529e\u516c\u9762\u79ef': 'officeSpace',\n            u'\u5355\u5957\u9762\u79ef': 'houseSpace',\n            u'\u5185\u90e8\u8bbe\u65bd': 'houseFacility',\n            u'\u76ee\u6807\u4e1a\u6001': 'targetState',\n            u'\u4e1a\u6001\u89c4\u5212': 'stateArrange',\n        }\n        #\u63d0\u53d6\u5f00\u53d1\u5546\u7684\u94fe\u63a5\n        tmep_companys = {\n                    u'\u5f00\u53d1\u5546': None,\n                    u'\u6295\u8d44\u5546': None,\n                    u'\u4ee3\u7406\u5546': None,\n                    u'\u7269\u4e1a\u7ba1\u7406\u516c\u53f8': None,\n                    u'\u7269\u4e1a\u516c\u53f8': None,\n                    }\n        #\u6807\u7b7e\u7ec4\n        item['tags'] = self.filterBlank(response.css('.lpicon.tf a').xpath('text()').extract())\n        #\u522b\u540d\n        item['alias'] = self.filterBlank(response.css('.h1_label').xpath('text()').extract()).replace(u'\u522b\u540d\uff1a', '')\n        #\u5f53\u524d\u57ce\u5e02\n        item['city'] = self.filterBlank(response.css('div.s2 .s4Box a').xpath('text()').extract())\n        #\u4e3b\u8981\u4fe1\u606f\u6240\u5728\u6a21\u5757\n        content = response.css('.besic_inform')\n\n\n        # \u57fa\u672c\u4fe1\u606f\u5757\n        for info_field in content.css('table').xpath('tr/td'):\n            field_title = self.filterBlank(info_field.xpath('strong/text()').extract())\n            field_text = self.filterBlank(info_field.xpath('text()').extract())\n            #\u5982\u679c\u5b57\u6bb5\u540d\u79f0\u5728\u5b57\u5178\u5185,\u5c31\u5b58\u50a8\u4ed6\u7684\u6587\u672c\n            if fields.has_key(field_title):\n                item[fields[field_title]] = field_text\n            if field_title == u'\u6240\u5c5e\u5546\u5708':\n                item['CBDCategory']= self.filterBlank(info_field.xpath('span/a/text()').extract())\n                print item['CBDCategory']\n\n        #\u9879\u76ee\u914d\u5957\n        item['projectConfig'] = self.filterBlank(content.css('#xq_xmpt_anchor+div').xpath('text()').extract(), separator='\\n')\n        #\u4ea4\u901a\u72b6\u51b5\n        item['trafficInfo'] = self.filterBlank(content.css('#xq_jtzk_anchor+div').xpath('text()').extract(), separator='\\n')\n        #\u8f66\u4f4d\u4fe1\u606f\n        item['carportInfo'] = self.filterBlank(content.css('#xq_cwxx_anchor+div').xpath('text()').extract(), separator='\\n')\n        #\u9879\u76ee\u7b80\u4ecb\n        item['projectIntro'] = self.filterBlank(content.css('#xq_xmjs_anchor+div').xpath('text()').extract(), separator='\\n')\n        #\u5185\u90e8\u8bbe\u65bd/\u5efa\u6750\u88c5\u4fee\n        houseFacility = []\n\n        #\u904d\u5386\u3001\u62bd\u53d6\u5b57\u7b26\u4e32\uff0c\u6700\u540e\u518d\u7528\u6362\u884c\u7b26\u62fc\u63a5\u5b57\u7b26\u4e32\n        for tag in content.css('#xq_jczx_anchor+div strong'):\n            houseFacility.append('{0}{1}'.format(\n                self.filterBlank(tag.xpath('text()[1]').extract()),\n                self.filterBlank(tag.xpath('following-sibling::text()[1]').extract())\n            ))\n\n        item['houseFacility'] = '\\n'.join(houseFacility)\n        #\u76f8\u5173\u4fe1\u606f\n        for tag in content.css('#xq_xgxx_anchor+div strong'):\n            s = self.filterBlank(tag.xpath('text()').extract()).replace(':', '')\n            if fields.has_key(s):\n                item[fields[s]] = self.filterBlank(tag.xpath('following-sibling::text()[1]').extract())\n                if s in tmep_companys:\n                    item[fields[s]] = self.filterBlank(tag.xpath('following-sibling::a[1]/text()').extract())\n                    tmep_companys[s] = self.filterBlank(tag.xpath('following-sibling::a[1]/@href').extract())\n        item['developersPageUrl'] = tmep_companys[u'\u5f00\u53d1\u5546']\n        yield Request(tmep_companys[u'\u5f00\u53d1\u5546'], meta={'item': item}, callback=self.parse_companypage)\n\n    def parse_companypage(self, response):\n        \"\"\"\n        \u89e3\u6790\u516c\u53f8detail\u9875\u9762\u6a21\u5757,\u76ee\u524d\uff0c\u53ea\u89e3\u6790\u5f00\u53d1\u5546\u9875\u9762\n        \"\"\"\n\n        item = response.meta['item']\n        temp_str_company = u'\u516c\u53f8\u7b80\u4ecb'\n        item['developersAbout'] = self.filterBlank(response.xpath(\"//p/span[contains(text(),'%s')]/following-sibling::text()[1]\" % temp_str_company).extract())\n        yield Request(item['houseurl'], meta={'item': item}, callback=self.parsepic)\n\n    def parsepic(self, response):\n        \"\"\"\u6536\u96c6\u697c\u76d8\u56fe\u7247\uff0c\u8df3\u5230\u697c\u76d8\u4e3b\u9875\u6536\u96c6\u9875\u9762\u8f6e\u64ad\u7684\u56fe\u7247\u3002\"\"\"\n\n        item = response.meta['item']\n        item['projectPics'] = self.filterBlank(response.css('ul#imageShowSmall li img').xpath('@src').extract())\n        print 'done'\n        yield item\n\n    def filterBlank(self, l, separator=','):\n        \"\"\"\u8fc7\u6ee4\u7a7a\u767d\u6807\u7b7e\u4e14\u5408\u5e76\u6240\u6709\u5217\u8868\u9879\"\"\"\n        return separator.join(l).strip()", "issue_status": "Closed", "issue_reporting_time": "2015-06-12T05:44:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1202": {"issue_url": "https://github.com/scrapy/scrapy/issues/1296", "issue_id": "#1296", "issue_summary": "Use scrapy.mail.MailSender is not able to quit after send mail in a stand alone script.", "issue_description": "lyjbupt commented on Jun 11, 2015\nExample code as below, please help me out :)\nfrom twisted.internet import reactor\nfrom scrapy.mail import MailSender\ndef f():\npass\nmailer = MailSender(smtphost='xxx.yyy.com', mailfrom=noreply@yyy.com')\nmailer.send(to=[\"foobar@yyy.com\"], subject=\"Some subject\", body=\"Some body\")\nreactor.callLater(1,f)\nreactor.run()", "issue_status": "Closed", "issue_reporting_time": "2015-06-11T07:55:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1203": {"issue_url": "https://github.com/scrapy/scrapy/issues/1288", "issue_id": "#1288", "issue_summary": "exceptions.IOError: Not a gzipped file", "issue_description": "tonal commented on Jun 8, 2015\n$ scrapy fetch 'http://www.airs.ru/index.php?q=catalog/heating/burner/producer-cib-unigas/page-0/order-title-asc'\n2015-06-08 10:50:15+0600 [scrapy] INFO: Scrapy 0.25.0-454-gfa1039f started (bot: scrapybot)\n2015-06-08 10:50:15+0600 [scrapy] INFO: Optional features available: ssl, http11, boto\n2015-06-08 10:50:15+0600 [scrapy] INFO: Overridden settings: {}\n2015-06-08 10:50:15+0600 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, CoreStats, SpiderState\n2015-06-08 10:50:16+0600 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-06-08 10:50:16+0600 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-06-08 10:50:16+0600 [scrapy] INFO: Enabled item pipelines: \n2015-06-08 10:50:16+0600 [default] INFO: Spider opened\n2015-06-08 10:50:16+0600 [default] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-06-08 10:50:16+0600 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-06-08 10:50:16+0600 [default] ERROR: Error downloading <GET http://www.airs.ru/index.php?q=catalog/heating/burner/producer-cib-unigas/page-0/order-title-asc>\n        Traceback (most recent call last):\n          File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 490, in _startRunCallbacks\n            self._runCallbacks()\n          File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 577, in _runCallbacks\n            current.result = callback(current.result, *args, **kw)\n          File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 382, in callback\n            self._startRunCallbacks(result)\n          File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 490, in _startRunCallbacks\n            self._runCallbacks()\n        --- <exception caught here> ---\n          File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 577, in _runCallbacks\n            current.result = callback(current.result, *args, **kw)\n          File \"/usr/lib/pymodules/python2.7/scrapy/core/downloader/middleware.py\", line 46, in process_response\n            response = method(request=request, response=response, spider=spider)\n          File \"/usr/lib/pymodules/python2.7/scrapy/contrib/downloadermiddleware/httpcompression.py\", line 27, in process_response\n            decoded_body = self._decode(response.body, encoding.lower())\n          File \"/usr/lib/pymodules/python2.7/scrapy/contrib/downloadermiddleware/httpcompression.py\", line 43, in _decode\n            body = gunzip(body)\n          File \"/usr/lib/pymodules/python2.7/scrapy/utils/gz.py\", line 20, in gunzip\n            chunk = f.read(8196)\n          File \"/usr/lib/python2.7/gzip.py\", line 261, in read\n            self._read(readsize)\n          File \"/usr/lib/python2.7/gzip.py\", line 296, in _read\n            self._read_gzip_header()\n          File \"/usr/lib/python2.7/gzip.py\", line 190, in _read_gzip_header\n            raise IOError, 'Not a gzipped file'\n        exceptions.IOError: Not a gzipped file\nIn scrapy scrapy fetch --headers ... also IOError\nHeaders for curl:\n$ curl -v -H 'Accept-Encoding: gzip, deflate, compress' http://www.airs.ru/index.php?q=catalog/heating/burner/producer-cib-unigas/page-0/order-title-asc > /dev/null\n* Hostname was NOT found in DNS cache\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 62.76.190.156...\n* Connected to www.airs.ru (62.76.190.156) port 80 (#0)\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0> GET /index.php?q=catalog/heating/burner/producer-cib-unigas/page-0/order-title-asc HTTP/1.1\n> User-Agent: curl/7.35.0\n> Host: www.airs.ru\n> Accept: */*\n> Accept-Encoding: gzip, deflate, compress\n> \n< HTTP/1.1 404 Not Found\n* Server nginx is not blacklisted\n< Server: nginx\n< Date: Mon, 08 Jun 2015 05:08:59 GMT\n< Content-Type: text/html; charset=utf-8\n< Transfer-Encoding: chunked\n< Connection: keep-alive\n< X-Powered-By: PHP/5.2.17\n< Set-Cookie: SESS8a5a439f1c15790e74709e9d7d429ba3=v477l94mq6fnfu55ceh18a2rg1; expires=Wed, 01-Jul-2015 08:42:19 GMT; path=/; domain=.airs.ru\n< Last-Modified: Thu, 04 Jun 2015 20:02:00 GMT\n< ETag: \"223c8deda8bd618407cdf708254bd689\"\n< Expires: Sun, 19 Nov 1978 05:00:00 GMT\n< Cache-Control: max-age=172800, private, must-revalidate\n< Content-Encoding: gzip\n< Transfer-Encoding: chunked", "issue_status": "Closed", "issue_reporting_time": "2015-06-08T05:10:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1204": {"issue_url": "https://github.com/scrapy/scrapy/issues/1287", "issue_id": "#1287", "issue_summary": "Files Pipeline fails on URLs with \"?\"/get attributes", "issue_description": "mrwonko commented on Jun 6, 2015\nThe Files Pipeline does not strip get parameters from the extension and then fails to write the file, at least on Windows/NTFS where \"?\" is disallowed in filenames.\ne.g. files_urls = [ \"http://foo.bar/baz.txt?fizz\" ] tries to create <hash>.txt?fizz, raising an IOError.", "issue_status": "Closed", "issue_reporting_time": "2015-06-06T11:29:51Z", "fixed_by": "#3954", "pull_request_summary": "Disallow media extensions unregistered with IANA", "pull_request_description": "Contributor\nOmarFarrag commented on Aug 13, 2019 \u2022\nedited by Gallaecio\nIf a media extension is empty string or is unregistered with IANA, then try to guess the MIME type from the url then the extension from MIME type using built-in mimetypes lib..\nFixes #3953, fixes #1287", "pull_request_status": "Merged", "issue_fixed_time": "2019-09-16T12:04:06Z", "files_changed": [["9", "scrapy/pipelines/files.py"], ["6", "tests/test_pipeline_files.py"]]}, "1205": {"issue_url": "https://github.com/scrapy/scrapy/issues/1283", "issue_id": "#1283", "issue_summary": "1.0.0rc2 is not on pypi", "issue_description": "Member\nkmike commented on Jun 6, 2015\n//cc @curita @dangra", "issue_status": "Closed", "issue_reporting_time": "2015-06-05T20:34:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1206": {"issue_url": "https://github.com/scrapy/scrapy/issues/1281", "issue_id": "#1281", "issue_summary": "What, where is CrawlSpider?", "issue_description": "rrshaban commented on Jun 4, 2015\nThe docs define class scrapy.contrib.spiders.CrawlSpider\nBut in Scrapy, updated to the latest version, I'm getting nothing. What's the deal?\n>>> import scrapy\n>>> scrapy.contrib.spiders\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'module' object has no attribute 'contrib'", "issue_status": "Closed", "issue_reporting_time": "2015-06-03T23:57:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1207": {"issue_url": "https://github.com/scrapy/scrapy/issues/1279", "issue_id": "#1279", "issue_summary": "to stop CrawlerProcess started with stop_after_crawl=False two Ctrl-Cs are required", "issue_description": "Member\nkmike commented on Jun 4, 2015\nWhen CrawlerProcess is started with stop_after_crawl=True (default) and user presses Ctrl-C the following happens:\nCtrl-C signal handler is changed to \"unclean shutown\" so that second Ctrl-C will definitely stop the reactor;\nself.stop method is called which asks all crawlers to stop;\nwhen all crawlers are stopped a Deferred returned from self.join() executes its callback. This callback is set to \"stop reactor\", so the reactor stops.\nBut with crawler_process.start(stop_after_crawl=False) there is no step (3) because self.join is not used. It means that to stop CrawlerProcess user have to send signal (press Ctrl-C) twice. The first Ctrl-C only asks all current crawlers to stop.\nI think it is better to change it - a signal should mean \"graceful shutdown\" regardless of stop_after_crawl option.", "issue_status": "Closed", "issue_reporting_time": "2015-06-03T20:15:32Z", "fixed_by": "#1284", "pull_request_summary": "CrawlerProcess cleanup", "pull_request_description": "Member\nkmike commented on Jun 6, 2015\nremoved unnecessary lambda (@curita - is it unnecessary? why did you add it?)\nfixed #1279;\nDNS setup is extracted to _get_dns_resolver method.", "pull_request_status": "Merged", "issue_fixed_time": "2015-06-09T14:21:25Z", "files_changed": [["24", "scrapy/crawler.py"]]}, "1208": {"issue_url": "https://github.com/scrapy/scrapy/issues/1277", "issue_id": "#1277", "issue_summary": "Scrapy and twisted Unhandled Error", "issue_description": "Free16t commented on Jun 3, 2015\nHi,\nI'm trying to use Hyphe. Hyphe is using Scrapy but I failed to connect scrapy.\nHere is my log file from scrapyd.log\n2015-06-02 14:29:22+0200 [-] \"127.0.0.1\" - - [02/Jun/2015:12:29:22 +0000] \"POST /schedule.json HTTP/1.0\" 200 50 \"-\" \"Twisted PageGetter\"\n2015-06-02 14:29:23+0200 [-] \"127.0.0.1\" - - [02/Jun/2015:12:29:22 +0000] \"GET /jobs HTTP/1.0\" 200 434 \"-\" \"Twisted PageGetter\"\n2015-06-02 14:29:23+0200 [HTTPChannel,43147,127.0.0.1] Unhandled Error\nTraceback (most recent call last):\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/http.py\", line 1755, in allContentReceived\nreq.requestReceived(command, path, version)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/http.py\", line 823, in requestReceived\nself.process()\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/server.py\", line 189, in process\nself.render(resrc)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/server.py\", line 238, in render\nbody = resrc.render(self)\nFile \"/usr/lib/pymodules/python2.7/scrapyd/webservice.py\", line 18, in render\nreturn JsonResource.render(self, txrequest)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/txweb.py\", line 10, in render\nr = resource.Resource.render(self, txrequest)\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/web/resource.py\", line 250, in render\nreturn m(request)\nFile \"/usr/lib/pymodules/python2.7/scrapyd/webservice.py\", line 37, in render_POST\nself.root.scheduler.schedule(project, spider, **args)\nFile \"/usr/lib/pymodules/python2.7/scrapyd/scheduler.py\", line 15, in schedule\nq = self.queues[project]\nexceptions.KeyError: 'hyphe.xxx'\nSincerely", "issue_status": "Closed", "issue_reporting_time": "2015-06-03T10:18:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1209": {"issue_url": "https://github.com/scrapy/scrapy/issues/1274", "issue_id": "#1274", "issue_summary": "HTTP_PROXY variable with username and empty password not supported", "issue_description": "Contributor\nrdowinton commented on Jun 2, 2015\nScrapy doesn't support proxy authentication when the password is empty when using the HTTP_PROXY environment variable to supply the proxy argument.", "issue_status": "Closed", "issue_reporting_time": "2015-06-02T11:53:47Z", "fixed_by": "#1313", "pull_request_summary": "[MRG+1] Support empty password for http_proxy config", "pull_request_description": "Member\neliasdorneles commented on Jun 23, 2015\nThis fixes #1274, adding support to empty password authentication for proxies -- useful for API key based auth like:\nexport http_proxy=http://<API key>:@paygo.crawlera.com:8010\nDoes this look good?", "pull_request_status": "Merged", "issue_fixed_time": "2015-06-30T22:58:16Z", "files_changed": [["2", "scrapy/downloadermiddlewares/httpproxy.py"], ["8", "tests/test_downloadermiddleware_httpproxy.py"]]}, "1210": {"issue_url": "https://github.com/scrapy/scrapy/issues/1270", "issue_id": "#1270", "issue_summary": "scrapy logging show UnicodeDecodeError", "issue_description": "binbibi commented on Jun 1, 2015\nhello,all, I use scrapy 1.00rc1 from pip ,my pc is win7x64,python is 2.7.9(x64),\nwhen I start my project\nscrapy crawl imzx\nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\logging\\__init__.py\", line 859, in emit\n    msg = self.format(record)\n  File \"C:\\Python27\\lib\\logging\\__init__.py\", line 732, in format\n    return fmt.format(record)\n  File \"C:\\Python27\\lib\\logging\\__init__.py\", line 474, in format\n    s = self._fmt % record.__dict__\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd6 in position 19: ordinal not in range(128)\nLogged from file log.py, line 108\nThen I find utils/log.py ,line 108 ,I commented-out the code as you can see\n#logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                #{'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\nAnd I restart my project,it's OK ,but something about logging error still:\nTraceback (most recent call last):\n  File \"C:\\Python27\\lib\\logging\\__init__.py\", line 859, in emit\n    msg = self.format(record)\n  File \"C:\\Python27\\lib\\logging\\__init__.py\", line 732, in format\n    return fmt.format(record)\n  File \"C:\\Python27\\lib\\logging\\__init__.py\", line 474, in format\n    s = self._fmt % record.__dict__\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd6 in position 19: ordinal not in range(128)\nLogged from file engine.py, line 212\nIt's to much in my screen,though too many logging Traceback , my code work fine ,I get all data I need, But I confused,my code work fine in scrapy 0.24 without any above logging Traceback,so I think it's maybe scrapy bug", "issue_status": "Closed", "issue_reporting_time": "2015-06-01T13:17:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1211": {"issue_url": "https://github.com/scrapy/scrapy/issues/1269", "issue_id": "#1269", "issue_summary": "Scrapy is not following links as per pattern given in \"follow_patterns\"", "issue_description": "drprabhakar commented on Jun 1, 2015\nI am deploying my Portia spider in scrapyd. I have given a pattern to be followed in Crawling section in Portia.\nWhile deploying the spider, scrapyd is not following the link pattern which I have given.\nHow to fix this issue?\nAlready asked in scrapy/scrapyd#95", "issue_status": "Closed", "issue_reporting_time": "2015-06-01T09:08:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1212": {"issue_url": "https://github.com/scrapy/scrapy/issues/1265", "issue_id": "#1265", "issue_summary": "Backward incompatibility for relocated paths in settings", "issue_description": "Member\ncurita commented on May 29, 2015\nReported by @dangra\nThis issue manifests when mixing old paths and new ones for extensions and middlewares (this can happen for example while using a newer version of Scrapy in a project that hasn't updated to the new paths yet). Since paths aren't normalized, the same component can be loaded twice.\nTake these settings for example:\n# scrapy/settings/default_settings.py\nEXTENSIONS_BASE = {\n    'scrapy.extensions.debug.StackTraceDump': 100,  # new path\n} \n# myproject/settings.py\nEXTENSIONS = {\n    'scrapy.contrib.debug.StackTraceDump': 200,  # old path\n}\nWhile merging both dictionaries to build the list of components, the same StackTraceDump class is going to be loaded twice since it appears in two different keys.", "issue_status": "Closed", "issue_reporting_time": "2015-05-29T14:14:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1213": {"issue_url": "https://github.com/scrapy/scrapy/issues/1262", "issue_id": "#1262", "issue_summary": "Scrapy HTTP Request should join response header values if there are multiple values for one header", "issue_description": "Contributor\npawelmhm commented on May 28, 2015\nCurrently if HTTP response contains multiple headers with same name Scrapy spider will get only first value in response.\nFor example response containing following headers\nSet-Cookie:visid_incap_41120=SOdwDQFZSKyIVH4gYFaRyw8IZ1UAAAAAQUIPAAAAAABTKkcWDVDo9bCbj+nVIygb; expires=Fri, 26 May 2017 19:18:33 GMT; path=/; Domain=.nextworth.com\nSet-Cookie:incap_ses_133_41120=GojSSxpMZFtuOlBBgYPYAQ8IZ1UAAAAAeDlo/Gc3JjyBa4ayRvSQjw==; path=/; Domain=.nextworth.com\nwill be represented like this in scrapy spider\nIn [11]: response.headers\nOut[11]: \n{\n 'Set-Cookie': 'visid_incap_41120=B+qHstQzQx2iYZcXTqWEKmkEZ1UAAAAAQUIPAAAAAABRy0c541mfOL+Q4tAYtmaQ; expires=Fri, 26 May 2017 18:14:02 GMT; path=/; Domain=.nextworth.com',\n}\nas you see only first cookie is visible in headers, others are ignored.\nI know there is response.headers.getlist which will return all values of given header, I also know well I can get all cookies from spider by just accessing cookie middleware that keeps track of cookies. But most users probably don't know that and they expect to have all cookies in response and it is extremely common to get multiple \"set-cookie\" headers.\nAlso it seems to me that this is an actual bug. I checked the standard and it openly allows sending multiple Set-Cookie headers:\nAn origin server may include multiple Set-Cookie headers in a\nresponse. Note that an intervening gateway could fold multiple such\nheaders into a single header.\n-- http://www.w3.org/Protocols/rfc2109/rfc2109\nCan we change response object to get proper value of multiple headers with same keys?", "issue_status": "Closed", "issue_reporting_time": "2015-05-28T12:41:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1214": {"issue_url": "https://github.com/scrapy/scrapy/issues/1257", "issue_id": "#1257", "issue_summary": "It seems like the last callback is executed for only one url but as many times as the total number of URLs", "issue_description": "bilougit commented on May 26, 2015\nHi there,\nVery tricky problem that I don't know how to debug. I have a spider which is a 3-level deep:\ndef parse(self, response):\n    # some treatment\n    # a loop\n         request = scrapy.Request(url=<calculated_url>, callback=parseChapter)\n         request.meta['item'] = # a dictionary containing some data of the just parsed page\n         yield request\n\ndef parseChapter(self, response):\n    # some treatment\n    # a loop\n         request = scrapy.Request(url=<calculated_url>, callback=parseCategory)\n         request.meta['item'] = # a dictionary containing some data of the just parsed page\n         # print request.meta['item'] is good and different in every iteration\n         yield request\n\ndef parseCategory(self, response):\n    # print response.meta['item'] is not good because it displays the same value many times\n    # for every new call of parseChapter, meta['item'] received is always the same\n    # some treatment\nThe weird thing is that the passed dictionary in the meta of the request is the same in parseCategory, and response.url into parseCateogry is fine.\nThis problem I'm talking about happens only in the callback parseCategory, it does work well in parseChapter (passed meta is good and different for every new call)\nCan you help me out with this?\nhttp://stackoverflow.com/questions/30425215/scrapy-passes-the-same-value-into-meta", "issue_status": "Closed", "issue_reporting_time": "2015-05-25T21:50:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1215": {"issue_url": "https://github.com/scrapy/scrapy/issues/1252", "issue_id": "#1252", "issue_summary": "newlines in attributes", "issue_description": "VascoVisser commented on May 22, 2015\nI noticed that when a website spits out HTML with newlines in tag attributes scrapy's xpath selector doesn't match attributes anymore. I.e., something like\n<div class=\"the_class\n\n\"> .. </div>\nwon't match xpath query \"//div[@class='the_class']\".\nWhat I do now to work around this is:\nselector = scrapy.Selector(text=response.body.replace('\\n', ''))\nThis seems to work just fine, but maybe there is a better way to go about handling this?", "issue_status": "Closed", "issue_reporting_time": "2015-05-21T23:28:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1216": {"issue_url": "https://github.com/scrapy/scrapy/issues/1251", "issue_id": "#1251", "issue_summary": "High CPU", "issue_description": "ErikvdVen commented on May 21, 2015\nScrapy uses a lot of CPU on our server, 40% is the minimum (when running 1 spider) and most of the time it goes up to 99% (running multiple spiders at the same time). We're using a DigitalOcean droplet with 1 CPU.\nIs this normal? I tried already several things:\nadd sleep(0.1) to Pipelines\ndisabling Pipelines\nUsing Scrapyd with job persistance.\nset the amount of jobs per cpu with settings like below.\nscrapy.cfg:\n[scrapyd]    \nmax_proc = 0    \nmax_proc_per_cpu = 2\nWhich didn't help either. These are my stats when running a spider:\nNothing helps. These are my stats when I run one scraper:\n  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND\n 3221 scrapy    20   0  836724  75568   8432 R 40.5  7.4   1:04.31 python\n 3231 scrapy    20   0  836724  75568   8432 S  1.0  7.4   0:01.40 python\n 3234 scrapy    20   0  836724  75568   8432 S  1.0  7.4   0:01.42 python\n 3235 scrapy    20   0  836724  75568   8432 S  1.0  7.4   0:01.44 python\n 3228 scrapy    20   0  836724  75568   8432 S  0.7  7.4   0:01.43 python\nI found out that if I set the variable CONCURRENT_REQUESTS = 1 the CPU stays below the 40% when running 1 spider, but it takes 3 times as long to finish.\nI also tried to turn off every extension, middleware and pipeline, but that didn't reduce the CPU load either. Any ideas?", "issue_status": "Closed", "issue_reporting_time": "2015-05-21T14:39:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1217": {"issue_url": "https://github.com/scrapy/scrapy/issues/1249", "issue_id": "#1249", "issue_summary": "Error fetching a webpage", "issue_description": "faheem-cliqz commented on May 21, 2015\nHi I am having some trouble fetching this particular domain. It results in a ConnectionDone (ResponseNeverReceived) twisted error. I am able to perfectly open this in a browser.\nscrapy fetch http://www.azlyrics.com/lyrics/sila/reverans.html\n2015-05-20 23:31:36+0200 [scrapy] INFO: Scrapy 0.24.6 started (bot: scrapybot)\n2015-05-20 23:31:36+0200 [scrapy] INFO: Optional features available: ssl, http11, boto\n2015-05-20 23:31:36+0200 [scrapy] INFO: Overridden settings: {}\n2015-05-20 23:31:36+0200 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2015-05-20 23:31:37+0200 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-05-20 23:31:37+0200 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-05-20 23:31:37+0200 [scrapy] INFO: Enabled item pipelines: \n2015-05-20 23:31:37+0200 [default] INFO: Spider opened\n2015-05-20 23:31:37+0200 [default] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-05-20 23:31:37+0200 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-05-20 23:31:37+0200 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n2015-05-20 23:31:37+0200 [default] DEBUG: Retrying <GET http://www.azlyrics.com/lyrics/sila/reverans.html> (failed 1 times): [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>]\n2015-05-20 23:31:37+0200 [default] DEBUG: Retrying <GET http://www.azlyrics.com/lyrics/sila/reverans.html> (failed 2 times): [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>]\n2015-05-20 23:31:37+0200 [default] DEBUG: Gave up retrying <GET http://www.azlyrics.com/lyrics/sila/reverans.html> (failed 3 times): [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>]\n2015-05-20 23:31:37+0200 [default] ERROR: Error downloading <GET http://www.azlyrics.com/lyrics/sila/reverans.html>: [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>]\n2015-05-20 23:31:37+0200 [default] INFO: Closing spider (finished)\n2015-05-20 23:31:37+0200 [default] INFO: Dumping Scrapy stats:\n    {'downloader/exception_count': 3,\n     'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,\n     'downloader/request_bytes': 720,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2015, 5, 20, 21, 31, 37, 506997),\n     'log_count/DEBUG': 5,\n     'log_count/ERROR': 1,\n     'log_count/INFO': 7,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2015, 5, 20, 21, 31, 37, 304392)}\n2015-05-20 23:31:37+0200 [default] INFO: Spider closed (finished)\nAnother example:\nscrapy fetch \"http://members.driverguide.com/matches.php?h=6320a591c56d6766a6b62f1390c163ff&is%5B0%5D=956675&ids%5B1%5D=956674&ids%5B2%5D=956676&ids%5B3%5D=956670&ids%5B4%5D=956702&ids%5B5%5D=956673&ids%5B6%5D=956708&ids%5B7%5D=956698&ids%5B8%5D=956699&ids%5B9%5D=956599&ids%5B10%5D=956589&ids%5B11%5D=956605&ids%5B12%5D=956593&ids%5B13%5D=956594&ids%5B14%5D=956601&ids%5B15%5D=956597&cid=631&model=Pro+660X\"\n2015-05-20 23:58:09+0200 [scrapy] INFO: Scrapy 0.24.6 started (bot: scrapybot)\n2015-05-20 23:58:09+0200 [scrapy] INFO: Optional features available: ssl, http11, boto\n2015-05-20 23:58:09+0200 [scrapy] INFO: Overridden settings: {}\n2015-05-20 23:58:09+0200 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2015-05-20 23:58:10+0200 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-05-20 23:58:10+0200 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-05-20 23:58:10+0200 [scrapy] INFO: Enabled item pipelines: \n2015-05-20 23:58:10+0200 [default] INFO: Spider opened\n2015-05-20 23:58:10+0200 [default] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-05-20 23:58:10+0200 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-05-20 23:58:10+0200 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n2015-05-20 23:58:11+0200 [default] DEBUG: Retrying <GET http://members.driverguide.com/matches.php?h=6320a591c56d6766a6b62f1390c163ff&ids%5B0%5D=956675&ids%5B1%5D=956674&ids%5B2%5D=956676&ids%5B3%5D=956670&ids%5B4%5D=956702&ids%5B5%5D=956673&ids%5B6%5D=956708&ids%5B7%5D=956698&ids%5B8%5D=956699&ids%5B9%5D=956599&ids%5B10%5D=956589&ids%5B11%5D=956605&ids%5B12%5D=956593&ids%5B13%5D=956594&ids%5B14%5D=956601&ids%5B15%5D=956597&cid=631&model=Pro+660X> (failed 1 times): [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>, <twisted.python.failure.Failure <class 'twisted.web.http._DataLoss'>>]\n2015-05-20 23:58:12+0200 [default] DEBUG: Retrying <GET http://members.driverguide.com/matches.php?h=6320a591c56d6766a6b62f1390c163ff&ids%5B0%5D=956675&ids%5B1%5D=956674&ids%5B2%5D=956676&ids%5B3%5D=956670&ids%5B4%5D=956702&ids%5B5%5D=956673&ids%5B6%5D=956708&ids%5B7%5D=956698&ids%5B8%5D=956699&ids%5B9%5D=956599&ids%5B10%5D=956589&ids%5B11%5D=956605&ids%5B12%5D=956593&ids%5B13%5D=956594&ids%5B14%5D=956601&ids%5B15%5D=956597&cid=631&model=Pro+660X> (failed 2 times): [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>, <twisted.python.failure.Failure <class 'twisted.web.http._DataLoss'>>]\n2015-05-20 23:58:13+0200 [default] DEBUG: Gave up retrying <GET http://members.driverguide.com/matches.php?h=6320a591c56d6766a6b62f1390c163ff&ids%5B0%5D=956675&ids%5B1%5D=956674&ids%5B2%5D=956676&ids%5B3%5D=956670&ids%5B4%5D=956702&ids%5B5%5D=956673&ids%5B6%5D=956708&ids%5B7%5D=956698&ids%5B8%5D=956699&ids%5B9%5D=956599&ids%5B10%5D=956589&ids%5B11%5D=956605&ids%5B12%5D=956593&ids%5B13%5D=956594&ids%5B14%5D=956601&ids%5B15%5D=956597&cid=631&model=Pro+660X> (failed 3 times): [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>, <twisted.python.failure.Failure <class 'twisted.web.http._DataLoss'>>]\n2015-05-20 23:58:13+0200 [default] ERROR: Error downloading <GET http://members.driverguide.com/matches.php?h=6320a591c56d6766a6b62f1390c163ff&ids%5B0%5D=956675&ids%5B1%5D=956674&ids%5B2%5D=956676&ids%5B3%5D=956670&ids%5B4%5D=956702&ids%5B5%5D=956673&ids%5B6%5D=956708&ids%5B7%5D=956698&ids%5B8%5D=956699&ids%5B9%5D=956599&ids%5B10%5D=956589&ids%5B11%5D=956605&ids%5B12%5D=956593&ids%5B13%5D=956594&ids%5B14%5D=956601&ids%5B15%5D=956597&cid=631&model=Pro+660X>: [<twisted.python.failure.Failure <class 'twisted.internet.error.ConnectionDone'>>, <twisted.python.failure.Failure <class 'twisted.web.http._DataLoss'>>]\n2015-05-20 23:58:13+0200 [default] INFO: Closing spider (finished)\n2015-05-20 23:58:13+0200 [default] INFO: Dumping Scrapy stats:\n    {'downloader/exception_count': 3,\n     'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,\n     'downloader/request_bytes': 1755,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2015, 5, 20, 21, 58, 13, 102177),\n     'log_count/DEBUG': 5,\n     'log_count/ERROR': 1,\n     'log_count/INFO': 7,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2015, 5, 20, 21, 58, 10, 915847)}", "issue_status": "Closed", "issue_reporting_time": "2015-05-20T21:36:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1218": {"issue_url": "https://github.com/scrapy/scrapy/issues/1248", "issue_id": "#1248", "issue_summary": "error downloading, Couldn't bind: 24: Too many open files.", "issue_description": "1a1a11a commented on May 20, 2015\nI am writing the following spider, but has the error \"error downloading, Couldn't bind: 24: Too many open files. \", can you help me?\nclass Spider(scrapy.Spider):\n    name = \u201c***\u201d\n    def __init__(self, url='http://example.com/', **kw):\n        super(Spider,self).__init__(**kw)\n        self.url = url \n        self.allowed_domains = [re.sub(r'^www\\.', '', urlparse(url).hostname)]\n\n    def start_requests(self):\n        return [Request(self.url, callback=self.find_all_url, dont_filter=False)]\n\n    def find_all_url(self,response):\n        if ***:\n             self.parse(response)\n        links = LinkExtractor().extract_links(response)\n        for link in links:\n             if len(link.url) < 50:\n                   yield Request(link.url, callback = self.find_all_url, dont_filter=False)\n\n    def parse(self, response):\n        dept = deptItem()\n        dept['deptName'] = response.xpath('//title/text()').extract()[0].strip()\n        dept['url'] = response.url\n        log.msg('find an item: '+ str(response.url) +'\\n going to return item' , level = log.INFO)\n        return dept        ", "issue_status": "Closed", "issue_reporting_time": "2015-05-19T21:41:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1219": {"issue_url": "https://github.com/scrapy/scrapy/issues/1247", "issue_id": "#1247", "issue_summary": "Add denied_domains for original Spider", "issue_description": "BrieflyX commented on May 19, 2015\nThis is allow & deny domains for LinkExtractor, but if we just use Spider class. It just provides an variable 'allowed_domains', no 'denied_domains' to forbid crawling some sites.....", "issue_status": "Closed", "issue_reporting_time": "2015-05-19T08:04:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1220": {"issue_url": "https://github.com/scrapy/scrapy/issues/1246", "issue_id": "#1246", "issue_summary": "How to get img src ?", "issue_description": "oxzilla commented on May 19, 2015\nHello,\nI have some problem about get img src from this page .\nhttp://shoponline.tescolotus.com/en-GB/Product/BrowseProducts?taxonomyId=Cat00001808\nPlease kindly advice.\nTim", "issue_status": "Closed", "issue_reporting_time": "2015-05-19T04:37:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1221": {"issue_url": "https://github.com/scrapy/scrapy/issues/1245", "issue_id": "#1245", "issue_summary": "Telnet problem", "issue_description": "1a1a11a commented on May 19, 2015\nI am trying to use telnet for scrapy, and I type in:\ntelnet localhost 6023\nand I have the following results coming from terminal:\nTrying ::1...\ntelnet: connect to address ::1: Connection refused\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\nThen it stucks here, can anyone help me?\nOS: OSX Yosemite\nScrapy: 0.24\npython: 2.7.9", "issue_status": "Closed", "issue_reporting_time": "2015-05-19T03:06:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1222": {"issue_url": "https://github.com/scrapy/scrapy/issues/1239", "issue_id": "#1239", "issue_summary": "Are the .crawler and .settings Spider attributes suposed to be public API?", "issue_description": "Contributor\nramiro commented on May 16, 2015\nIf the answer is yes, shouldn't them be documented?\nOr, put in other words. If I'm using them, am I safe using from future API changes?\nBoth are very useful because they allow one to access the bound crawler and setting values from the Spider without having to create an extension just for that.", "issue_status": "Closed", "issue_reporting_time": "2015-05-15T22:47:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1223": {"issue_url": "https://github.com/scrapy/scrapy/issues/1238", "issue_id": "#1238", "issue_summary": "Unstructured Information in \"Scrapy at Glance\" Page", "issue_description": "preetis19 commented on May 15, 2015\nIn the Documentation page \"Scrapy at Glance\" (link : http://doc.scrapy.org/en/0.24/intro/overview.html)\nincomplete information is given for running the spider.\nI being a new user started executing it from there itself and later I came to know that to start a project we should follow this link : http://doc.scrapy.org/en/0.24/intro/tutorial.html", "issue_status": "Closed", "issue_reporting_time": "2015-05-15T15:08:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1224": {"issue_url": "https://github.com/scrapy/scrapy/issues/1232", "issue_id": "#1232", "issue_summary": "Request with space in url generated 550 error", "issue_description": "twayneprice commented on May 15, 2015\nWhen processing a request with a space in the url, a 550 error is generated.\nimport scrapy\n\nclass TestSpider(scrapy.Spider):\n    name = \"test\"\n    handle_httpstatus_list = [404]\n\n    def start_requests(self):\n        #This ones does not work\n        link = 'ftp://sdrftp03.dor.state.fl.us/Data Books/1976_2009_Data_Book_PDF/00FLpropdata.pdf'\n\n        #This one works\n#        link = 'ftp://sdrftp03.dor.state.fl.us/MaximumMillageData/comp07.pdf'\n\n        yield scrapy.Request(link, meta={'ftp_user':'anonymous','ftp_password': 'anonymous'})\n\n    def parse(self, response):\n        print response.body", "issue_status": "Closed", "issue_reporting_time": "2015-05-14T20:04:25Z", "fixed_by": "#1254", "pull_request_summary": "[MRG +1] Unquote request path before passing to FTPClient, it already escape paths", "pull_request_description": "Member\ndangra commented on May 24, 2015\nCloses #1232\nFor more details see debugging session in #1232 comments.", "pull_request_status": "Merged", "issue_fixed_time": "2015-06-24T13:04:45Z", "files_changed": [["4", "scrapy/core/downloader/handlers/ftp.py"], ["17", "tests/test_downloader_handlers.py"]]}, "1225": {"issue_url": "https://github.com/scrapy/scrapy/issues/1231", "issue_id": "#1231", "issue_summary": "Scrapy does not use a non-zero exit code when a scrape fails", "issue_description": "iandees commented on May 14, 2015\nWhen invoking a Scrapy spider with e.g. scrapy crawl spidername -o output.csv and the spider fails for some reason (in our case, timeout to the HTTP server), the exit code is zero, giving subsequent steps in a shell script no way to check if the scrape completed successfully.\nSee my example here:\nhttps://gist.github.com/iandees/74f51fefeb758b57f5e7", "issue_status": "Closed", "issue_reporting_time": "2015-05-14T13:37:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1226": {"issue_url": "https://github.com/scrapy/scrapy/issues/1230", "issue_id": "#1230", "issue_summary": "Traceback is missing when exception is raised in pipeline", "issue_description": "Contributor\nchekunkov commented on May 14, 2015\nCode to reproduce issue https://github.com/chekunkov/dirbot/tree/exception-in-pipeline\nWhat happens if you use Scrapy 0.24.6 (also works with rev 5eb098a)\n2015-05-14 07:13:51+0000 [dmoz] ERROR: Error processing {'description': [u'- Scripts, examples and news about Python programming for the Windows platform.\\r'],\n     'name': [u'Social Bug'],\n     'url': [u'http://win32com.goermezer.de/']}\n    Traceback (most recent call last):\n      File \"/vagrant/workspace/repositories/scrapy/scrapy/middleware.py\", line 62, in _process_chain\n        return process_chain(self.methods[methodname], obj, *args)\n      File \"/vagrant/workspace/repositories/scrapy/scrapy/utils/defer.py\", line 65, in process_chain\n        d.callback(input)\n      File \"/home/vagrant/.env/dirbot/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 383, in callback\n        self._startRunCallbacks(result)\n      File \"/home/vagrant/.env/dirbot/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 491, in _startRunCallbacks\n        self._runCallbacks()\n    --- <exception caught here> ---\n      File \"/home/vagrant/.env/dirbot/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 578, in _runCallbacks\n        current.result = callback(current.result, *args, **kw)\n      File \"/vagrant/workspace/scrapinghub/repo/dirbot-chekunkov/dirbot/pipelines.py\", line 12, in process_item\n        raise Exception\n    exceptions.Exception:\n\n2015-05-14 07:13:51+0000 [dmoz] INFO: Closing spider (finished)\nAnd this what happens if you use code from master branch:\n2015-05-14 07:14:08+0000 [scrapy] ERROR: Error processing {'description': [u'- Features Python books, resources, news and articles.\\r'],\n 'name': [u\"O'Reilly Python Center\"],\n 'url': [u'http://oreilly.com/python/']}\n2015-05-14 07:14:08+0000 [scrapy] ERROR: Error processing {'description': [u'- Resources for reporting bugs, accessing the Python source tree with CVS and taking part in the development of Python.\\r'],\n 'name': [u\"Python Developer's Guide\"],\n 'url': [u'https://www.python.org/dev/']}\n2015-05-14 07:14:08+0000 [scrapy] ERROR: Error processing {'description': [u'- Scripts, examples and news about Python programming for the Windows platform.\\r'],\n 'name': [u'Social Bug'],\n 'url': [u'http://win32com.goermezer.de/']}\n2015-05-14 07:14:08+0000 [scrapy] INFO: Closing spider (finished)\nProbably this is caused by #1060 and might be related to #1229", "issue_status": "Closed", "issue_reporting_time": "2015-05-14T07:19:09Z", "fixed_by": "#1236", "pull_request_summary": "Replace FailureFormatter with direct failure to exc_info conversions in log calls", "pull_request_description": "Member\ncurita commented on May 15, 2015\nlogging.Filters can't override log messages in logger children, they can only do so in the logger they are attached. That's why FailureFormatter (the filter that takes care of extracting the exc_info from failures to print tracebacks) doesn't propagate to the descendants of the scrapy logger.\n(TopLevelFormatter works because it doesn't override the \"entire\" log message, just the logger name, which is formatted at the end with string like '%(name)s %(msg)'. It's only the msg variable that is compute on the first logger and then propagated to the parents as it is).\nI fixed it by converting each failure to exc_info before making the log call.\nThis pr fixes #1229 and fixes #1230.", "pull_request_status": "Merged", "issue_fixed_time": "2015-05-15T18:00:36Z", "files_changed": [["20", "scrapy/core/engine.py"], ["14", "scrapy/core/scraper.py"], ["4", "scrapy/downloadermiddlewares/robotstxt.py"], ["4", "scrapy/extensions/feedexport.py"], ["3", "scrapy/log.py"], ["4", "scrapy/pipelines/files.py"], ["6", "scrapy/pipelines/media.py"], ["27", "scrapy/utils/log.py"], ["4", "scrapy/utils/signal.py"], ["3", "tests/test_pipeline_media.py"], ["45", "tests/test_utils_log.py"]]}, "1227": {"issue_url": "https://github.com/scrapy/scrapy/issues/1229", "issue_id": "#1229", "issue_summary": "Where the tracebacks at?", "issue_description": "Contributor\nnramirezuy commented on May 13, 2015\nrun the spider with scrapy runspider, it also happens on parse and crawl.\nhttps://gist.github.com/nramirezuy/65faa56c7eab1e117d77", "issue_status": "Closed", "issue_reporting_time": "2015-05-13T18:06:16Z", "fixed_by": "#1236", "pull_request_summary": "Replace FailureFormatter with direct failure to exc_info conversions in log calls", "pull_request_description": "Member\ncurita commented on May 15, 2015\nlogging.Filters can't override log messages in logger children, they can only do so in the logger they are attached. That's why FailureFormatter (the filter that takes care of extracting the exc_info from failures to print tracebacks) doesn't propagate to the descendants of the scrapy logger.\n(TopLevelFormatter works because it doesn't override the \"entire\" log message, just the logger name, which is formatted at the end with string like '%(name)s %(msg)'. It's only the msg variable that is compute on the first logger and then propagated to the parents as it is).\nI fixed it by converting each failure to exc_info before making the log call.\nThis pr fixes #1229 and fixes #1230.", "pull_request_status": "Merged", "issue_fixed_time": "2015-05-15T18:00:36Z", "files_changed": [["20", "scrapy/core/engine.py"], ["14", "scrapy/core/scraper.py"], ["4", "scrapy/downloadermiddlewares/robotstxt.py"], ["4", "scrapy/extensions/feedexport.py"], ["3", "scrapy/log.py"], ["4", "scrapy/pipelines/files.py"], ["6", "scrapy/pipelines/media.py"], ["27", "scrapy/utils/log.py"], ["4", "scrapy/utils/signal.py"], ["3", "tests/test_pipeline_media.py"], ["45", "tests/test_utils_log.py"]]}, "1228": {"issue_url": "https://github.com/scrapy/scrapy/issues/1227", "issue_id": "#1227", "issue_summary": "'NoneType' object has no attribute '_app_data' because of misuse ClientTLSOptions", "issue_description": "aldarund commented on May 13, 2015\nSee this answer for reference:\nhttp://stackoverflow.com/a/30203408/239354\nfull exception stacktrace:\n Traceback (most recent call last):\n      File \"/opt/webapps/link_crawler/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 415, in dataReceived\n        self._write(bytes)\n      File \"/opt/webapps/link_crawler/lib/python2.7/site-packages/twisted/protocols/tls.py\", line 554, in _write\n        sent = self._tlsConnection.send(toSend)\n      File \"/opt/webapps/link_crawler/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1270, in send\n        result = _lib.SSL_write(self._ssl, buf, len(buf))\n      File \"/opt/webapps/link_crawler/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 926, in wrapper\n        callback(Connection._reverse_mapping[ssl], where, return_code)\n    --- <exception caught here> ---\n      File \"/opt/webapps/link_crawler/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1055, in infoCallback\n        return wrapped(connection, where, ret)\n      File \"/opt/webapps/link_crawler/lib/python2.7/site-packages/twisted/internet/_sslverify.py\", line 1157, in _identityVerifyingInfoCallback\n        transport = connection.get_app_data()\n      File \"/opt/webapps/link_crawler/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1589, in get_app_data\n        return self._app_data\n      File \"/opt/webapps/link_crawler/lib/python2.7/site-packages/OpenSSL/SSL.py\", line 1148, in __getattr__\n        return getattr(self._socket, name)\n    exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'", "issue_status": "Closed", "issue_reporting_time": "2015-05-13T00:06:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1229": {"issue_url": "https://github.com/scrapy/scrapy/issues/1223", "issue_id": "#1223", "issue_summary": "FeedExporters export empty dicts when FEED_EXPORT_FIELDS setting is not set", "issue_description": "Contributor\nredapple commented on May 12, 2015\nReported by several users using S3 exporter and JsonItemExporter.\nIt looks like the docs for FEED_EXPORT_FIELDS do not match current behaviour:\nWhen omitted, Scrapy uses fields defined in Item subclasses a spider is yielding. If raw dicts are used as items Scrapy tries to infer field names from the exported data - currently it uses field names from the first item.\n        if self.fields_to_export is None:\n            if include_empty and not isinstance(item, dict):\n                field_iter = six.iterkeys(item.fields)\n            else:\n                field_iter = six.iterkeys(item)\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/exporters/__init__.py#L59\nThis following line fetching settings for FEED_EXPORT_FIELDS returns an empty list [] when setting is absent, and not None as one would expect (a bug in settings.getlist() IMO)\nself.export_fields = settings.getlist('FEED_EXPORT_FIELDS')\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/extensions/feedexport.py#L154", "issue_status": "Closed", "issue_reporting_time": "2015-05-12T11:59:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1230": {"issue_url": "https://github.com/scrapy/scrapy/issues/1222", "issue_id": "#1222", "issue_summary": "Problem with tutorial and DEFAULT_ITEM_CLASS", "issue_description": "nekopa commented on May 10, 2015\nThis is my first time reporting an issue on Github so I apologise if I am breaking any etiquette. I just installed scrapy (on a Vagrant Ubuntu64 box ) and was running through the tutorial but kept getting errors at the section for starting up the scrapy shell. After a lot of searching I found someone who had a similar problem (but on one of their own spiders) and it boiled down to the fact that in the first section of the tutorial you have us create in the items.py a DmozItem, but in the settings.py there is this:\nDEFAULT_ITEM_CLASS = 'tutorial.items.TutorialItem'\nAfter I changed TutorialItem to DmozItem the shell worked perfectly.\nI don't know if I missed a step somewhere in the tutorial or if this is unusual behaviour, but I just wanted to give you a heads up.", "issue_status": "Closed", "issue_reporting_time": "2015-05-09T22:32:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1231": {"issue_url": "https://github.com/scrapy/scrapy/issues/1217", "issue_id": "#1217", "issue_summary": "ItemMeta overrides fields, instead of merging it", "issue_description": "MojoJolo commented on May 8, 2015\nLooks like this update (#353) breaks the Item for the project I'm working on right now.\nIt now overrides fields, instead of merging it. @michalmo pointed it out to me after the item fields are having a key error.\nInheriting DictItem rather than Item fixed the problem though.", "issue_status": "Closed", "issue_reporting_time": "2015-05-08T07:52:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1232": {"issue_url": "https://github.com/scrapy/scrapy/issues/1213", "issue_id": "#1213", "issue_summary": "Allow FEED_EXPORT_FIELDS to be specified per Item rather than for the whole project", "issue_description": "demelziraptor commented on May 7, 2015\nThis setting was added in this commit: 1534e85\nBut it would be more useful to define per Item. Maybe change it to a dict and have the Item name as the key? Or a new attribute for the Item class?", "issue_status": "Closed", "issue_reporting_time": "2015-05-07T11:31:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1233": {"issue_url": "https://github.com/scrapy/scrapy/issues/1209", "issue_id": "#1209", "issue_summary": "Multiple Items into one pipeline -- NEO4J database Use case", "issue_description": "mayouf commented on May 5, 2015\nHi every one,\nI use scrapy in order to scrape a social network and then get the data in a NEO4J database.\nMy challenge here is to relate 2 items each other:\nclass person(scrapy.Item):\nname=Field()\nclass AnotherPerson(scrapy.Item):\nname=Field()\nI want to save those two items in my graph database by saying:\nPerson has relationship with AnotherPerson()\nWhat I need here is to send two items in ONE pipeline !! How can we do this ?\nI tried to send it through a list, but scrapy doesn't accept the list as soon as a collection is in there.\nHere is my pseudo code:\n1- I get a list of person (each person has profile and a list of firends like facebook)\n2- For each person in this list:\n---- I open his profile (through a request and send the response to a callback)\n---- I take the response and create a item: Person() and fill it\n---- I send the item with a \"yield\"\n---- Then I open his list of friend (through a request and send the response to a another callback)\n---- I have the friend list page\n---- Then For each friend in this list (the page display a name and a city):\n--------create an item: AnotherPerson()\n--------I fill this item with the name and the city\n--------I send the item with a \"yield\"\nI have two pipelines. they work well to save the data in database, but I don't have any clue to how I can relate them because for that I need to do that in the same process (ie. pipeline).\nIm not sure if I've been clear, so don't hesitate to ask for clarifications.\nThanks for your help guys\nRegards,\nMayouf", "issue_status": "Closed", "issue_reporting_time": "2015-05-05T16:00:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1234": {"issue_url": "https://github.com/scrapy/scrapy/issues/1204", "issue_id": "#1204", "issue_summary": "Scrapy doesnt check if response is html and fail with exception 'Response' object has no attribute 'body_as_unicode'", "issue_description": "aldarund commented on May 4, 2015\nIf scrapy found a link that outputs not an html but pdf or something else it fails with exception.\nHere is stracktrace:\n2015-05-03 18:16:28+0000 [external_link_spider] ERROR: Spider error processing <GET http://www.chanel.com/en_AS/Watches/print?popup=1&ref=H0451>\n        Traceback (most recent call last):\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 824, in runUntilCurrent\n            call.func(*call.args, **call.kw)\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/twisted/internet/task.py\", line 638, in _tick\n            taskObj._oneWorkUnit()\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/twisted/internet/task.py\", line 484, in _oneWorkUnit\n            result = next(self._iterator)\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 57, in <genexpr>\n            work = (callable(elem, *args, **named) for elem in iterable)\n        --- <exception caught here> ---\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 96, in iter_errback\n            yield next(it)\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/contrib/spidermiddleware/offsite.py\", line 26, in process_spider_output\n            for x in result:\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/contrib/spidermiddleware/referer.py\", line 22, in <genexpr>\n            return (_set_referer(r) for r in result or ())\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/contrib/spidermiddleware/offsite.py\", line 26, in process_spider_output\n            for x in result:\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/contrib/spidermiddleware/urllength.py\", line 33, in <genexpr>\n            return (r for r in result or () if _filter(r))\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/contrib/spidermiddleware/depth.py\", line 50, in <genexpr>\n            return (r for r in result or () if _filter(r))\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/contrib/spiders/crawl.py\", line 69, in _parse_response\n            for requests_or_item in iterate_spider_output(cb_res):\n          File \"build/bdist.linux-x86_64/egg/ex_link_crawl/spiders/external_link_spider.py\", line 28, in parse_obj\n\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/contrib/linkextractors/lxmlhtml.py\", line 96, in extract_links\n            html = Selector(response)\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/selector/unified.py\", line 80, in __init__\n            _root = LxmlDocument(response, self._parser)\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py\", line 27, in __new__\n            cache[parser] = _factory(response, parser)\n          File \"/opt/webapps/link_crawler/local/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py\", line 13, in _factory\n            body = response.body_as_unicode().strip().encode('utf8') or '<html/>'\n        exceptions.AttributeError: 'Response' object has no attribute 'body_as_unicode'", "issue_status": "Closed", "issue_reporting_time": "2015-05-03T18:39:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1235": {"issue_url": "https://github.com/scrapy/scrapy/issues/1203", "issue_id": "#1203", "issue_summary": "scrapyd traceback", "issue_description": "xpacket commented on May 3, 2015\nissue is running a schedule spider in scrapyd using curl schedule.json\nscrapy-0.24.6 and scrapyd 1.0\nFile \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n\"main\", fname, loader, pkg_name)\nFile \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\nexec code in run_globals\nFile \"/usr/lib/pymodules/python2.7/scrapyd/runner.py\", line 39, in\nmain()\nFile \"/usr/lib/pymodules/python2.7/scrapyd/runner.py\", line 36, in main\nexecute()\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 143, in execute\n2015-05-02 14:24:49-0700 [Launcher,11085/stderr] _run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 89, in _run_print_help\nfunc(_a, *_kw)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 150, in _run_command\ncmd.run(args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py\", line 58, in run\nspider = crawler.spiders.create(spname, *_opts.spargs)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermanager.py\", line 46, in create\nreturn spcls.from_crawler(self.crawler, *_spider_kwargs)\nTypeError: from_crawler() got an unexpected keyword argument '_job'\nRunning in\nscrapy 0.25.1 and scrapyd 1.0.2\nproblem just moved to another module\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/Scrapy-0.25.1-py2.7.egg/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/usr/local/lib/python2.7/dist-packages/Scrapy-0.25.1-py2.7.egg/scrapy/commands/crawl.py\", line 57, in run\n    self.crawler_process.crawl(spname, **opts.spargs)\n  File \"/usr/local/lib/python2.7/dist-packages/Scrapy-0.25.1-py2.7.egg/scrapy/crawler.py\", line 152, in crawl\n    d = crawler.crawl(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1253, in unwindGenerator\n    return _inlineCallbacks(None, gen, Deferred())\n--- <exception caught here> ---\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1107, in _inlineCallbacks\n    result = g.send(result)\n  File \"/usr/local/lib/python2.7/dist-packages/Scrapy-0.25.1-py2.7.egg/scrapy/crawler.py\", line 69, in crawl\n    self.spider = self._create_spider(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/Scrapy-0.25.1-py2.7.egg/scrapy/crawler.py\", line 79, in _create_spider\n    return self.spidercls.from_crawler(self, *args, **kwargs)\nexceptions.TypeError: from_crawler() got an unexpected keyword argument '_job'\nappears to be passing 2 arguments although method only takes in crawler", "issue_status": "Closed", "issue_reporting_time": "2015-05-03T01:50:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1236": {"issue_url": "https://github.com/scrapy/scrapy/issues/1201", "issue_id": "#1201", "issue_summary": "nofollow doesnt work correcly when there multiple values in rel attribute", "issue_description": "aldarund commented on May 2, 2015\nAccording to specs rel can have multiple values: http://www.w3.org/TR/html401/struct/links.html#adef-rel\nBut scrapy ( LxmlParserLinkExtractor and SgmlLinkExtractor(but this one doesnt matter i guess since its deprecated)) just check if it strictly only follow.\nlink = Link(url, _collect_string_content(el) or u'',\n                nofollow=True if el.get('rel') == 'nofollow' else False)\nSo the cases when links looks like this will not work correctly:\n <a href='http://blablabla.com/' rel='external nofollow'>bla bla</a>\nAnd its not from a vacuum, its from real world sites where i encountered that scrapy follows nofollow link. For example at this site: www.bruceclay.com/blog/secondary-keywords/", "issue_status": "Closed", "issue_reporting_time": "2015-05-01T21:22:36Z", "fixed_by": "#1214", "pull_request_summary": "[MRG+1] Support link rel attribute with multiple values", "pull_request_description": "Contributor\nrgtk commented on May 7, 2015\nrel attribute for a can have multiple values: http://www.w3.org/TR/html401/struct/links.html#adef-rel\nfixes #1201", "pull_request_status": "Merged", "issue_fixed_time": "2015-08-27T22:57:54Z", "files_changed": [["4", "scrapy/linkextractors/lxmlhtml.py"], ["4", "scrapy/linkextractors/sgml.py"], ["5", "scrapy/utils/misc.py"], ["15", "tests/test_linkextractors.py"]]}, "1237": {"issue_url": "https://github.com/scrapy/scrapy/issues/1200", "issue_id": "#1200", "issue_summary": "Flatten documentation files", "issue_description": "Member\npablohoffman commented on May 1, 2015\nHot on the heels of contrib-removal (and modules relocation) comes another thought: do we really need the topics/ and intro/ folders separation in scrapy docs?. Wouldn't it be better to flatten the doc files?", "issue_status": "Closed", "issue_reporting_time": "2015-05-01T04:57:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1238": {"issue_url": "https://github.com/scrapy/scrapy/issues/1195", "issue_id": "#1195", "issue_summary": "Scrapy 1.0 release notes", "issue_description": "Member\npablohoffman commented on Apr 30, 2015\nLet's put together a document with Scrapy 1.0 release notes, it should provide a:\nhigh-level summary of all changes getting into Scrapy 1.0\ntips and hints for migration from Scrapy 0.x (even though we'll provide backwards compatibility for most things)\nWe'll circulate and points to this document (in mailing list announcements, etc) as the go-to place for \"what's new about 1.0?\"", "issue_status": "Closed", "issue_reporting_time": "2015-04-29T19:59:51Z", "fixed_by": "#1244", "pull_request_summary": "1.0 release notes", "pull_request_description": "Member\ncurita commented on May 19, 2015\nRelease Notes for next stable release. Closes #1195.", "pull_request_status": "Merged", "issue_fixed_time": "2015-05-22T16:21:17Z", "files_changed": [["358", "docs/news.rst"]]}, "1239": {"issue_url": "https://github.com/scrapy/scrapy/issues/1194", "issue_id": "#1194", "issue_summary": "Deprecate htmlparser link extractor", "issue_description": "Member\npablohoffman commented on Apr 30, 2015\nLet's add a deprecation warning like we did for SGML Link extractor", "issue_status": "Closed", "issue_reporting_time": "2015-04-29T19:52:04Z", "fixed_by": "#1205", "pull_request_summary": "[MRG+1] Deprecate htmlparser link extractor", "pull_request_description": "Member\ncurita commented on May 5, 2015\nCloses #1194", "pull_request_status": "Merged", "issue_fixed_time": "2015-05-06T05:40:39Z", "files_changed": [["9", "scrapy/linkextractors/htmlparser.py"], ["4", "scrapy/linkextractors/sgml.py"]]}, "1240": {"issue_url": "https://github.com/scrapy/scrapy/issues/1193", "issue_id": "#1193", "issue_summary": "position() mod 3 = 0", "issue_description": "aaronluoq commented on Apr 29, 2015\nin Python 2.7.8 (default, Aug 1 2014, 10:01:11)\nScrapy==0.24.5\nlen(resp.xpath(\"//div[@Class='pic_text'][position() mod 3 = 0]\"))\n0\nlen(resp.xpath(\"//div[@Class='pic_text'][position() mod 1 = 0]\"))\n10\nlen(resp.xpath(\"//div[@Class='pic_text']\"))\n10", "issue_status": "Closed", "issue_reporting_time": "2015-04-29T07:08:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1241": {"issue_url": "https://github.com/scrapy/scrapy/issues/1191", "issue_id": "#1191", "issue_summary": "Spaces and similar characters in link attributes not removed by Link Extractors", "issue_description": "Djayb6 commented on Apr 29, 2015\nI am using LxmlLinkExtractor to extract links, and if some spaces or lines break are present in the hrefattribute like so:\n<a href=\"\n                         http://current-domain.com\">A link</href>\nand id this link is in a page with the following base URL: http://current-domain.com/contact/, the extracted URL of the link above is\nhttp://www.current-domain.com/contact/%0A%09%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20http://www.current-domain.com\nI tried to use process_value but there the URL is already joined so it is useless. It would be nice if a function similar to process_value could be available to sanitize the content in the link attribute", "issue_status": "Closed", "issue_reporting_time": "2015-04-28T20:16:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1242": {"issue_url": "https://github.com/scrapy/scrapy/issues/1189", "issue_id": "#1189", "issue_summary": "After a redirect, requests are sent with the wrong 'Host' header.", "issue_description": "esegal commented on Apr 27, 2015\nThis is a bug in scrapy.contrib.downloadermiddleware.redirect\nI asked scrapy to crawl http://www.pcaobus.org:\nscrapy fetch http://www.pcaobus.org\nI get a redirect loop.\nHere's the headers summary:\nRequest:\nGET / HTTP/1.0 http://www.pcaobus.org/\nHost: www.pcaobus.org\nResponse:\nHTTP/1.0 301 OK\nLocation: http://pcaobus.org/\nRequest:\nGET / HTTP/1.0 http://pcaobus.org/\nHost: www.pcaobus.org\nResponse:\nHTTP/1.0 301 OK\nLocation: http://pcaobus.org/\nNotice how the host header didn't change, eventhough the URL did.\nI've was able to fix it by adding line 25 to scrapy/contrib/downloadermiddleware/redirect.py\n    24     def _redirect(self, redirected, request, spider, reason):\n    25             redirected.headers.pop('Host', None)\n    .\n    .\n    .", "issue_status": "Closed", "issue_reporting_time": "2015-04-27T00:08:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1243": {"issue_url": "https://github.com/scrapy/scrapy/issues/1186", "issue_id": "#1186", "issue_summary": "Documentation is inconsistent regarding versions: 0.24 is \"latest\", 0.25 is \"stable\".", "issue_description": "ghost commented on Apr 22, 2015\nhttp://scrapy.org/ lists the \"latest version\" as 0.24.\nhttp://scrapy.org/doc/ lists the \"latest version\" as 0.24.\nhttp://doc.scrapy.org/en/latest/ contains documentation for version 0.24.\nhttp://doc.scrapy.org/en/stable/ contains documentation for version 0.25.\nI am confused by the inconsistency. What is the difference between \"latest version\" and \"stable version\" and why do they refer to different versions? This seems like a documentation bug.", "issue_status": "Closed", "issue_reporting_time": "2015-04-22T16:52:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1244": {"issue_url": "https://github.com/scrapy/scrapy/issues/1183", "issue_id": "#1183", "issue_summary": "Invalid XML parsing", "issue_description": "nashuiliang commented on Apr 21, 2015\nWhen the XML document has invalid characters, it causes a problem that the result is not completely.\nExample: response.body has \\0x01 , \\0x0a ....\nHave a solution?", "issue_status": "Closed", "issue_reporting_time": "2015-04-21T15:59:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1245": {"issue_url": "https://github.com/scrapy/scrapy/issues/1178", "issue_id": "#1178", "issue_summary": "Error when resuming a crawler", "issue_description": "jbinfo commented on Apr 19, 2015\nCan someone show me how to correct this issue when resuming the crawler, below the log file\n2015-04-19 10:54:38-0700 [my_spider_name] INFO: Spider opened\n2015-04-19 10:54:50-0700 [my_spider_name] INFO: Resuming crawl (5403830 requests scheduled)\n2015-04-19 10:54:50-0700 [my_spider_name] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-04-19 10:54:51-0700 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-04-19 10:54:51-0700 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n2015-04-19 10:54:51-0700 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 93, in start\n        self.start_reactor()\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 130, in start_reactor\n        reactor.run(installSignalHandlers=False)  # blocking call\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1192, in run\n        self.mainLoop()\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1201, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 824, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/reactor.py\", line 41, in __call__\n        return self._func(*self._a, **self._kw)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 107, in _next_request\n        if not self._next_request_from_scheduler(spider):\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 134, in _next_request_from_scheduler\n        request = slot.scheduler.next_request()\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/scheduler.py\", line 64, in next_request\n        request = self._dqpop()\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/scheduler.py\", line 94, in _dqpop\n        d = self.dqs.pop()\n      File \"/usr/local/lib/python2.7/dist-packages/queuelib/pqueue.py\", line 43, in pop\n        m = q.pop()\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/squeue.py\", line 18, in pop\n        s = super(SerializableQueue, self).pop()\n      File \"/usr/local/lib/python2.7/dist-packages/queuelib/queue.py\", line 157, in pop\n        self.f.seek(-size-self.SIZE_SIZE, os.SEEK_END)\n    exceptions.IOError: [Errno 22] Invalid argument", "issue_status": "Closed", "issue_reporting_time": "2015-04-19T18:01:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1246": {"issue_url": "https://github.com/scrapy/scrapy/issues/1176", "issue_id": "#1176", "issue_summary": "fetch fails to update scrapy objects in scrapy shell.", "issue_description": "Contributor\nbagratte commented on Apr 19, 2015\nMicrosoft Windows [Version 6.3.9600]\n(c) 2013 Microsoft Corporation. All rights reserved.\n\nC:\\Users\\bagratte>scrapy shell\n2015-04-19 16:15:13+0400 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)\n2015-04-19 16:15:13+0400 [scrapy] INFO: Optional features available: ssl, http11\n, django\n2015-04-19 16:15:13+0400 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL\n': 0}\n2015-04-19 16:15:13+0400 [scrapy] INFO: Enabled extensions: TelnetConsole, Close\nSpider, WebService, CoreStats, SpiderState\n2015-04-19 16:15:13+0400 [scrapy] INFO: Enabled downloader middlewares: HttpAuth\nMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, Def\naultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, Redirec\ntMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-04-19 16:15:13+0400 [scrapy] INFO: Enabled spider middlewares: HttpErrorMid\ndleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddlew\nare\n2015-04-19 16:15:13+0400 [scrapy] INFO: Enabled item pipelines:\n2015-04-19 16:15:13+0400 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6\n026\n2015-04-19 16:15:13+0400 [scrapy] DEBUG: Web service listening on 127.0.0.1:6083\n\n[s] Available Scrapy objects:\n[s]   crawler    \n[s]   item       {}\n[s]   settings   \n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\nIn [1]: fetch('http://scrapy.org/')\n2015-04-19 16:15:40+0400 [default] INFO: Spider opened\n2015-04-19 16:15:41+0400 [default] DEBUG: Redirecting (302) to  from \n2015-04-19 16:15:41+0400 [default] DEBUG: Crawled (200) \n (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    \n[s]   item       {}\n[s]   request    \n[s]   response   <200 http://scrapy.org/>\n[s]   settings   \n[s]   spider     \n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\nIn [2]: response\n\nIn [3]: response is None\nOut[3]: True\n\nIn [4]:\nDo you really want to exit ([y]/n)?\n\n\nC:\\Users\\bagratte>scrapy shell http://scrapy.org/\n2015-04-19 16:16:05+0400 [scrapy] INFO: Scrapy 0.24.5 started (bot: scrapybot)\n2015-04-19 16:16:05+0400 [scrapy] INFO: Optional features available: ssl, http11\n, django\n2015-04-19 16:16:05+0400 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL\n': 0}\n2015-04-19 16:16:05+0400 [scrapy] INFO: Enabled extensions: TelnetConsole, Close\nSpider, WebService, CoreStats, SpiderState\n2015-04-19 16:16:05+0400 [scrapy] INFO: Enabled downloader middlewares: HttpAuth\nMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, Def\naultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, Redirec\ntMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-04-19 16:16:05+0400 [scrapy] INFO: Enabled spider middlewares: HttpErrorMid\ndleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddlew\nare\n2015-04-19 16:16:05+0400 [scrapy] INFO: Enabled item pipelines:\n2015-04-19 16:16:05+0400 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6\n026\n2015-04-19 16:16:05+0400 [scrapy] DEBUG: Web service listening on 127.0.0.1:6083\n\n2015-04-19 16:16:05+0400 [default] INFO: Spider opened\n2015-04-19 16:16:06+0400 [default] DEBUG: Crawled (200) \n (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    \n[s]   item       {}\n[s]   request    \n[s]   response   <200 http://scrapy.org/>\n[s]   settings   \n[s]   spider     \n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\nIn [1]: response\nOut[1]: <200 http://scrapy.org/>\n\nIn [2]: fetch('http://www.python.org/')\n2015-04-19 16:16:41+0400 [default] DEBUG: Redirecting (301) to  from \n2015-04-19 16:16:41+0400 [default] DEBUG: Crawled (200)  (referer: None)\n[s] Available Scrapy objects:\n[s]   crawler    \n[s]   item       {}\n[s]   request    \n[s]   response   <200 https://www.python.org/>\n[s]   settings   \n[s]   spider     \n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\nIn [3]: response\nOut[3]: <200 http://scrapy.org/>\n\nIn [4]:\nDo you really want to exit ([y]/n)?\n\n\nC:\\Users\\bagratte>scrapy version -v\nScrapy  : 0.24.5\nlxml    : 3.4.2.0\nlibxml2 : 2.9.0\nTwisted : 15.1.0\nPython  : 2.7.9 (default, Dec 10 2014, 12:28:03) [MSC v.1500 64 bit (AMD64)]\nPlatform: Windows-8-6.2.9200\n\nC:\\Users\\bagratte>ipython --version\n3.1.0\n\nC:\\Users\\bagratte>\nis this an expected behavior?", "issue_status": "Closed", "issue_reporting_time": "2015-04-19T12:27:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1247": {"issue_url": "https://github.com/scrapy/scrapy/issues/1175", "issue_id": "#1175", "issue_summary": "\u6293\u53d6", "issue_description": "bigsharp commented on Apr 19, 2015\n\u6293\u53d6\u5e93", "issue_status": "Closed", "issue_reporting_time": "2015-04-19T10:30:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1248": {"issue_url": "https://github.com/scrapy/scrapy/issues/1174", "issue_id": "#1174", "issue_summary": "ImportError: cannot import name LinkExtractor", "issue_description": "pkpp1233 commented on Apr 19, 2015\nI'm getting the following error in scrapy v 0.24.5\nImportError: cannot import name LinkExtractor\nThis happens when I do:\nfrom scrapy.contrib.linkextractors import LinkExtractor\nAny ideas?", "issue_status": "Closed", "issue_reporting_time": "2015-04-19T06:33:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1249": {"issue_url": "https://github.com/scrapy/scrapy/issues/1173", "issue_id": "#1173", "issue_summary": "make.bat for building docs on windows", "issue_description": "Contributor\nbagratte commented on Apr 18, 2015\nshouldn't there be a make.bat batchfile for building the scrapy documentation on windows?", "issue_status": "Closed", "issue_reporting_time": "2015-04-18T16:02:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1250": {"issue_url": "https://github.com/scrapy/scrapy/issues/1143", "issue_id": "#1143", "issue_summary": "Bug in deploy command", "issue_description": "Contributor\njesuslosada commented on Apr 9, 2015\nError when running scrapy deploy:\nTraceback (most recent call last):\n  File \"/home/jesus/venv/test/bin/scrapy\", line 9, in <module>\n    load_entry_point('Scrapy==0.25.1', 'console_scripts', 'scrapy')()\n  File \"/home/jesus/venv/test/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/jesus/venv/test/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/home/jesus/venv/test/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/home/jesus/venv/test/local/lib/python2.7/site-packages/scrapy/commands/deploy.py\", line 106, in run\n    if not _upload_egg(target, egg, project, version):\n  File \"/home/jesus/venv/test/local/lib/python2.7/site-packages/scrapy/commands/deploy.py\", line 195, in _upload_egg\n    return _http_post(req)\n  File \"/home/jesus/venv/test/local/lib/python2.7/site-packages/scrapy/commands/deploy.py\", line 211, in _http_post\n    f = request.urlopen(request)\n  File \"/usr/lib/python2.7/urllib2.py\", line 225, in __getattr__\n    raise AttributeError, attr\nAttributeError: urlopen\nThe code that was merged with PR #1121 seems to be broken. There are several name collisions in deploy.py between method parameters named \"request\" and this import:\nfrom six.moves.urllib import request", "issue_status": "Closed", "issue_reporting_time": "2015-04-09T16:18:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1251": {"issue_url": "https://github.com/scrapy/scrapy/issues/1142", "issue_id": "#1142", "issue_summary": "Bug with Scrapy 0.25.0-298-g5846d61 and Slybot 0.9-r1039+201503231448~2f25", "issue_description": "OlgaCh commented on Apr 8, 2015\nAfter upgrade on dash both deploy and spiders scheduling stopped to work and failed with the message:\nTraceback (most recent call last):\nFile \"/usr/lib/pymodules/python2.7/scrapy/cmdline.py\", line 142, in execute\ncmd.crawler_process = CrawlerProcess(settings)\nFile \"/usr/lib/pymodules/python2.7/scrapy/crawler.py\", line 123, in init\nsuper(CrawlerProcess, self).init(settings)\nFile \"/usr/lib/pymodules/python2.7/scrapy/crawler.py\", line 77, in init\nself.spiders = smcls.from_settings(settings.frozencopy())\nFile \"/tmp/eggs-RzTd6X/main.egg/enclarity/spidermanager.py\", line 21, in from_settings\no = super(EnclaritySpiderManager, cls).from_settings(settings)\nFile \"/usr/lib/pymodules/python2.7/slybot/spidermanager.py\", line 34, in from_settings\nreturn cls(datadir, spider_cls, settings=settings)\nFile \"/usr/lib/pymodules/python2.7/slybot/spidermanager.py\", line 22, in init\nsettings.set('PLUGINS', load_plugins(settings))\nFile \"/usr/lib/pymodules/python2.7/scrapy/settings/init.py\", line 92, in set\nself._assert_mutability()\nFile \"/usr/lib/pymodules/python2.7/scrapy/settings/init.py\", line 115, in _assert_mutability\nraise TypeError(\"Trying to modify an immutable Settings object\")\nTypeError: Trying to modify an immutable Settings object", "issue_status": "Closed", "issue_reporting_time": "2015-04-08T10:13:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1252": {"issue_url": "https://github.com/scrapy/scrapy/issues/1141", "issue_id": "#1141", "issue_summary": "relaxing scrapy.Item dependence on Field?", "issue_description": "Contributor\nnyov commented on Apr 8, 2015\nSince supporting simple dicts as items, I was wondering if it wouldn't be okay to also relax Item's dependence on Field, which is just a dict?\nThat would make subclassing Item slightly easier when it's not necessary to pull in scrapy.item.Field.\ndiff --git a/scrapy/item.py b/scrapy/item.py\nindex 9998010..9cb73f1 100644\n--- a/scrapy/item.py\n+++ b/scrapy/item.py\n@@ -27,7 +27,7 @@ class ItemMeta(ABCMeta):\n         fields = {}\n         new_attrs = {}\n         for n, v in six.iteritems(attrs):\n-            if isinstance(v, Field):\n+            if isinstance(v, (Field, dict)):\n                 fields[n] = v\n             else:\n                 new_attrs[n] = v", "issue_status": "Closed", "issue_reporting_time": "2015-04-08T01:42:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1253": {"issue_url": "https://github.com/scrapy/scrapy/issues/1139", "issue_id": "#1139", "issue_summary": "scrapy shell error: Deferred.callback is not a method of DefaultSpider", "issue_description": "tycho01 commented on Apr 6, 2015\nI'm trying to just run a basic scrapy shell command, yet am somehow met with a weird error, see output below:\n$ scrapy shell 'http://spu.taobao.com/spu/spulist.htm?cat=1801'\n2015-04-06 10:35:41+0000 [scrapy] INFO: Scrapy 0.25.1 started (bot: taobao)\n2015-04-06 10:35:41+0000 [scrapy] INFO: Optional features available: ssl, http11\n2015-04-06 10:35:41+0000 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'taobao.spiders', 'SPIDER_MODULES': ['taobao.spiders'], 'LOGSTATS_INTERVAL': 0, 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler', 'BOT_NAME': 'taobao'}\n2015-04-06 10:35:41+0000 [scrapy] INFO: Enabled extensions: TelnetConsole, CloseSpider, CoreStats, SpiderState\n2015-04-06 10:35:41+0000 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-04-06 10:35:41+0000 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-04-06 10:35:41+0000 [scrapy] INFO: Enabled item pipelines: TaobaoPipeline, RedisPipeline\n2015-04-06 10:35:41+0000 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-04-06 10:35:41+0000 [default] INFO: Spider opened\nTraceback (most recent call last):\n  File \"/usr/local/bin/scrapy\", line 9, in <module>\n    load_entry_point('Scrapy==0.25.1', 'console_scripts', 'scrapy')()\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/shell.py\", line 65, in run\n    shell.start(url=url)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 44, in start\n    self.fetch(url, spider)\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/shell.py\", line 87, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"<string>\", line 2, in raiseException\nValueError: Function <bound method Deferred.callback of <Deferred at 0x3765f38>> is not a method of: <DefaultSpider 'default' at 0x378aa90>\nvagrant@precise64:/vagrant/taobao/scrapy-redis/example-project/taobao$ cat /usr/local/lib/python2.7/dist-packages/twisted/internet/threads.py\nSo I understand this is about Twisted's deferred callbacks as used in Scrapy. As I understand it though, in Scrapy it basically hooks into _schedule(), which essentially just returns the spider itself. I'm having a lot of trouble finding any hits on google for similar errors though... might anyone have some insight here? My subclassed spider does have parse() defined.", "issue_status": "Closed", "issue_reporting_time": "2015-04-06T14:13:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1254": {"issue_url": "https://github.com/scrapy/scrapy/issues/1138", "issue_id": "#1138", "issue_summary": "Alternative way to pass arguments to callback", "issue_description": "Member\nkmike commented on Apr 6, 2015\nCurrently to pass data between callbacks users need to use request.meta. Example from docs (simplified):\ndef parse_page1(self, response):\n    item = MyItem(main_url=response.url)\n    request = scrapy.Request(\"http://www.example.com/some_page.html\",\n                             callback=self.parse_page2)\n    request.meta['item'] = item\n    return request\n\ndef parse_page2(self, response):\n    item = response.meta['item']\n    item['other_url'] = response.url\n    return item\nWhile it works fine, there are some issues:\nit seems understanding request.meta is a common struggle for beginners;\nwe're mixing parameters for Scrapy components with user data.\nWhat about providing an alternative way?\ndef parse_page1(self, response):\n    item = MyItem(main_url=response.url)\n    return scrapy.Request(\"http://www.example.com/some_page.html\",\n                             callback=self.parse_page2,\n                             kwargs={'item': item})\n\ndef parse_page2(self, response, item):\n    item['other_url'] = response.url\n    return item\nAdvantages:\nIf you're writing some extraction code without Scrapy (e.g. requests+lxml), then likely parsing functions have arguments. So this change makes code more natural/straightforward.\nOptional arguments or arguments with default values are easier to handle - just provide a default value using Python syntax.\nUser state is separated from Scrapy internals better.\nLess code.\nOne can see which data callback needs just by looking at callback definition.\nThis way it is easier to add extra data to meta without a risk of breaking Scrapy extensions. There should be fewer bugs with missing meta.copy().\nIn case of missing argument callback will fail earlier.\nThe implementation could add __kwargs field to request.meta and pass **meta.get('__kwargs', {}) to the callback. Alternatively, we could put keyword arguments in another dict similar to meta. It will allow to separate them better. Also, rules for passing kwargs may be different from rules for passing meta (e.g. maybe meta should be preserved/copied in some cases, but not kwargs, I'm not sure).\n\ud83d\udc4d 15", "issue_status": "Closed", "issue_reporting_time": "2015-04-06T09:44:38Z", "fixed_by": "#3563", "pull_request_summary": "[MRG+1] Callback kwargs", "pull_request_description": "Member\nelacuesta commented on Jan 4, 2019 \u2022\nedited\nFixes #1138\nThis is just a first approach. It's currently lacking docs and tests, I'll add those if the implementation is good. Update: added tests and docs\nI see (at least) two points for discussion:\nShould we also pass the same arguments to the errbacks? Or maybe add a different parameter? errback_kwargs or something like that. On the other hand, the request object is available in the failure received by the errback, failure.request.cb_kwargs gives access to the arguments, so I think it shouldn't be necessary.\nI'm not a fan of the kwargs name, I think it could be easily confused with Python's own \"kwargs\" naming convention, i.e., people could understand that any remaining keyword argument passed to the Request constructor will be passed to the callbacks. Is \"callback_kwargs\" too verbose? Maybe it's not compatible with the previous point. Update Renamed to cb_kwargs\nSample spider:\nimport scrapy\n\nclass TestCallbackKwargsSpider(scrapy.Spider):\n    name = 'callback_kwargs'\n\n    def start_requests(self):\n        data = {'a': 123, 'b': 456}\n        yield scrapy.Request('https://example.org', cb_kwargs=data)\n\n    def parse(self, response, a, b):\n        yield {'url': response.url, 'a': a, 'b': b}\n        yield response.follow(\n            response.css('a::attr(href)').get(),\n            self.parse_other,\n            cb_kwargs={'source': response.url})\n\n    def parse_other(self, response, source):\n        yield {'url': response.url, 'source': source}\n        yield response.follow(\n            response.css('a::attr(href)').get(),\n            self.parse_regular)\n\n    def parse_regular(self, response):\n        yield {'url': response.url}\nOutput:\n(...)\n2019-01-03 17:40:38 [scrapy.core.engine] INFO: Spider opened\n2019-01-03 17:40:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2019-01-03 17:40:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2019-01-03 17:40:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://example.org> (referer: None)\n2019-01-03 17:40:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://example.org>\n{'url': 'https://example.org', 'a': 123, 'b': 456}\n2019-01-03 17:40:39 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.iana.org/domains/reserved> from <GET http://www.iana.org/domains/example>\n2019-01-03 17:40:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.iana.org/domains/reserved> (referer: None)\n2019-01-03 17:40:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.iana.org/domains/reserved>\n{'url': 'https://www.iana.org/domains/reserved', 'source': 'https://example.org'}\n2019-01-03 17:40:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.iana.org/> (referer: https://www.iana.org/domains/reserved)\n2019-01-03 17:40:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.iana.org/>\n{'url': 'https://www.iana.org/'}\n2019-01-03 17:40:40 [scrapy.core.engine] INFO: Closing spider (finished)\n(...)\nTasks\nRequest changes\nResponse.follow changes\nScraper changes\nRequest serialization\nTests\nDocs\n/cc @kmike @dangra\n\ud83c\udf89 5", "pull_request_status": "Merged", "issue_fixed_time": "2019-06-26T17:01:30Z", "files_changed": [["3", "docs/topics/commands.rst"], ["13", "docs/topics/debug.rst"], ["9", "docs/topics/jobs.rst"], ["14", "docs/topics/leaks.rst"], ["58", "docs/topics/request-response.rst"], ["37", "scrapy/commands/parse.py"], ["4", "scrapy/core/scraper.py"], ["11", "scrapy/http/request/__init__.py"], ["5", "scrapy/http/response/__init__.py"], ["5", "scrapy/http/response/text.py"], ["7", "scrapy/utils/reqser.py"], ["14", "tests/test_command_parse.py"], ["5", "tests/test_http_request.py"], ["169", "tests/test_request_cb_kwargs.py"], ["2", "tests/test_utils_reqser.py"]]}, "1255": {"issue_url": "https://github.com/scrapy/scrapy/issues/1136", "issue_id": "#1136", "issue_summary": "Imporvment : why should we not add the formid field as well in formRequest.from_response", "issue_description": "Contributor\nDharmeshPandav commented on Apr 6, 2015\nCurrent signature of function call to formRequest.from_response() from the package from scrapy import FormRequest allows following parameters :\n def from_response(cls, response, formname=None, formnumber=0, formdata=None,\n                      clickdata=None, dont_click=False, formxpath=None, **kwargs):\n        kwargs.setdefault('encoding', response.encoding)\n        form = _get_form(response, formname, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n        url = _get_form_url(form, kwargs.pop('url', None))\n        method = kwargs.pop('method', form.method)\n        return cls(url=url, method=method, formdata=formdata, **kwargs)\nThe one option that we are missing IMO in formdata is the capability to select form using a ID parameter form = _get_form(response, formname, formnumber, formxpath)\nwe should add something like\nform = _get_form(response, formname, formnumber, formxpath)\nand modified defination of form = _get_form(response, formname,formid, formnumber, formxpath)\n+++\nif formid is not None:\n        f = root.xpath('//form[@id=\"%s\"]' % formid)\n        if f:\n            return f[0]", "issue_status": "Closed", "issue_reporting_time": "2015-04-06T09:10:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1256": {"issue_url": "https://github.com/scrapy/scrapy/issues/1135", "issue_id": "#1135", "issue_summary": "Per-key priorities for dictionary-like settings", "issue_description": "Contributor\njdemaeyer commented on Apr 4, 2015\nI'm new, so here's a quick intro: Hi! I'm Jakob :) I came upon this while working on my GSoC proposal.\nDuring Julia's GSoC, settings priorities were introduced to allow updating settings from different places without paying attention to order. They work great for 'simple' (non-compound) settings, but there are two issues with the dictionary-like settings (e.g. DOWNLOADER_MIDDLEWARES or EXTENSIONS) mainly used to manage extensions:\nThey are fiddly to update (and not overwrite) when honoring priorities and mutability (#1110)\nThey only assign a single priority to the complete dictionary. As this contains no information on which keys exactly were changed with what priority, the dictionary priority is not very meaninful. It forbids updating a key still at default setting (lowest priority) with a medium priority as soon as any key has been updated with a higher priority.\nThis should prove especially problematic for the proposed add-on structure (SEP-021, discussed in #591). To resolve this, every key could have its own priority associated with it. @curita suggested making the compound setting variables an instance of the Settings class (and no longer an instance of dict). This could further clean up the Settings API by deprecating the _BASE settings, as the 'real' settings dictionaries (without appendix) could be used for default settings without fearing that they get lost when reading from settings.py. I've gathered some thoughts on this in my proposal (rather long, sorry, tl;dr is that it should be possible in a fully transparent, as in \"backwards-compatible with no changes to the API\", fashion):\nAs the Settings class already provides a __getitem__() method, this will introduce no API change to reading these settings.\nThere are currently three places where the dict-like settings are written to:\nWhen defined in scrapy/settings/default_settings.py\nWhen reading from settings.py in the Settings.setmodule() method\nWhen combining the dictionaries with their _BASE in scrapy.utils.conf.build_component_list()\nScrapy's code could be updated in the following fashion with full backwards compatibility, even for non-intended uses (such as users working directly on a _BASE dictionary):\nComplete Settings dictionary-like interface by implementing:\n__setitem__(self, k, v) method that will use some default priority (maybe 'project')\n__iter__(self) method which returns an iterator over Settings.attributes\nupdate(self, custom, priority = 'project') method that behaves like dict.update() while respecting mutability and priorities. If custom is a dict object, the given priority will be used for all keys. If it is a Settings object, the already existing per-key priority values will be used. The setdict() method should become a proxy to this (more general) method\nDeprecate _BASE dictionaries by replacing them with empty ones (for backwards-compatibility) and moving default settings into 'real' dictionary, i.e.\nSPIDER_MIDDLEWARES = {}\nSPIDER_MIDDLEWARES_BASE = {\n    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n    # ...\n}\nbecomes\n  SPIDER_MIDDLEWARES_BASE = {}\n  SPIDER_MIDDLEWARES = Settings( values = {\n      'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n      # ...\n      }, priority = 'default' )\nConfiguration in settings.py should be no different for the user. The Settings.setmodule() method should therefore recognise which of its attributes are themselves Settings instances, and call their respective update() methods instead of replacing them. Alternatively, this check could be done in the SettingsAttribute.set() method.\nIntroduce a small change to the build_component_list() helper function such that it works on Settings instances instead of on dict:\ndef build_component_list(base, custom):\n    # ...\n    # OLD:  compdict = base.copy()\n    compdict = Settings(base, priority = 'default')\n    # As before:\n    compdict.update(custom)\n    # ...\nFor the 1.0 release, the settings, middleware managers and build_component_list() helper function could be tidied up by removing support for the deprecated _BASE settings", "issue_status": "Closed", "issue_reporting_time": "2015-04-04T14:00:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1257": {"issue_url": "https://github.com/scrapy/scrapy/issues/1130", "issue_id": "#1130", "issue_summary": "Unhandled error in Deferred (RobotsTxtMiddleware)", "issue_description": "Contributor\ntorymur commented on Apr 2, 2015\nDev story:\nLet's say spider downloads all .zip files from http://habrahabr.ru/post/212029/ page\nUrl with .zip files looks like this: http://layer6.jenkins.tox.im/job/qt_gui_win32/lastSuccessfulBuild/artifact/qt/build/release/TOX-Qt-GUI.zip\nIt's a polite spider, so settings file contains:\nROBOTSTXT_OBEY = True\nMiddleware parses habrahabr.ru robots.txt file as well as 'external' robots.txt file from layer6.jenkins.tox.im. It's expected behaviour.\nBut if request will be returned with error then the output would be:\n2015-04-02 17:06:16+0300 [habrahabr] DEBUG: Gave up retrying <GET http://layer6.jenkins.tox.im/robots.txt> (failed 1 times): DNS lookup failed: address 'layer6.jenkins.tox.im' not found: [Errno 8] nodename nor servname provided, or not known.\n\n2015-04-02 17:06:16+0300 [-] ERROR: Unhandled error in Deferred:\n2015-04-02 17:06:16+0300 [-] Unhandled Error\n    Traceback (most recent call last):\n    Failure: twisted.internet.error.DNSLookupError: DNS lookup failed: address 'layer6.jenkins.tox.im' not found: [Errno 8] nodename nor servname provided, or not known.", "issue_status": "Closed", "issue_reporting_time": "2015-04-02T14:36:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1258": {"issue_url": "https://github.com/scrapy/scrapy/issues/1129", "issue_id": "#1129", "issue_summary": "cannot yield item in forsubfunction of parse process", "issue_description": "suemi994 commented on Apr 2, 2015\nHere I will give a example:\nclass HiapkSpider(scrapy.spider.Spider):\n    name=\"hiapk\"\n    allowed_domains=['apk.hiapk.com']\n\n    def __init__(self,start=1,end=2):\n        self.prefix_url=\"http://apk.hiapk.com/apps?sort=5&pi=\"\n        self.base_url=\"http://apk.hiapk.com\"\n        self.prefix_path=\"../temp/\"\n        self.page_start=int(start)\n        self.page_index=self.page_start\n        self.page_end=int(end)\n        HiapkSpider.start_urls=[self.prefix_url+str(self.page_start)]\n\n    def parse(self,response):\n        self.apkList=[]\n        if self.page_index > self.page_end:\n            print 'Spider Completed'\n            return\n        for sel in response.xpath('//li[contains(@class,\"list_item\")]'):\n            item=ApkItem()\n            item['name']=sel.xpath('div/dl/dt/span/a/@href').extract()[0].split('.')[-1]\n            item['version']=sel.xpath('div/dl/dt/*[2]/text()').extract()[0][1:-1]\n            item['url']=self.base_url+sel.xpath('div/*[3]/a/@href').extract()[0]\n            item['path']=self.prefix_path+item['name']+'.apk'\n            self.apkList.append(item)\n        self.download()\n        self.page_index+=1\n        for item in self.apkList:\n            yield item\n        yield Request(self.prefix_url+str(self.page_index),\n                      callback=self.parse)\n\n\n    def download(self):\n        print 'DownLoad Start'\n        p=Pool()\n        result=[]\n        for item in self.apkList:\n           # tmp=p.apply_async(urllib.urlretrieve,[item['url'],item['path']])\n           tmp=p.apply_async(kkk,[2])\n           result.append(tmp)\n        p.close()\n        p.join()\n        for i in range(0,len(self.apkList)):\n            try:\n                print result[i].get()\n            except:\n                continue\n            else:\n                yield self.apkList[i]\nif I use yield in the for stucture of download function , this function will not be excuted. But if I yield directly in download function, all right. Very strange!", "issue_status": "Closed", "issue_reporting_time": "2015-04-02T08:02:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1259": {"issue_url": "https://github.com/scrapy/scrapy/issues/1126", "issue_id": "#1126", "issue_summary": "install scrapy failed on Mac OS X Yosemite", "issue_description": "linus-young commented on Apr 1, 2015\nI run pip install scrapy, and it only gives me the error:\nthen I realized there is something wrong with cryptography, then I found this would be helpful: Install Cryptography using your own openssl on os x\nAccroding to the link above, I run the following command:\n$ brew install openssl\n$ env ARCHFLAGS=\"-arch x86_64\" LDFLAGS=\"-L/usr/local/opt/openssl/lib\" CFLAGS=\"-I/usr/local/opt/openssl/include\" pip install cryptography\n$ pip install scrapy\nThen I successfully installed scrapy.\nI don't know whether it's common for OS X users, if so, I think maybe the installation guide could be more specific for OS X.\nSystem: OS X Yosemite 10.10.2\nPython : 2.7.9\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2015-04-01T13:17:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1260": {"issue_url": "https://github.com/scrapy/scrapy/issues/1125", "issue_id": "#1125", "issue_summary": "Support for spiders in other languages (discussion)", "issue_description": "Contributor\nnyov commented on Mar 31, 2015\nA ticket to discuss and reference Spider development in other languages (GSoC idea).\nThe goal is to allow creating spiders in any language, that can be executed by the scrapy framework like a builtin spider. (And I would like a solution that is generic enough to decouple other scrapy components in the future the same way, like item pipelines.)\nAs I see it, this is about defining an a) interface and b) protocol which should be suitable for use across different programming languages. Then there are actual c) implementation details to consider (how might scrapy's codebase need to change, to support this).\na) The interface, should be at the smallest common denominator (across languages): OS (POSIX/SYSV) IPC (Inter Process Communication), giving us these options, I believe:\nshared memory, mmapped files, message queues (not really on OSX), sockets (incl. unix sockets), pipes. (For an overview, see Beej's Guide to Unix IPC)\n(I would prefer sticking to local IPC, meaning no inet sockets, and let people who need networked IPC write their own transport middleware, for example using zeromq or json-rpc.)\nb) The protocol needs consideration on what kind of data needs to be exchanged and how we're doing it. Pipes are unidirectional (unless you're on Solaris) while sockets work full-duplex.\n(The GSoC ideas page also refers to Hadoop Streaming, which is using line-based communication through pipes. Q: How does this handle binary data?)\nWe might use some custom line-based interactions like this hadoop streaming style, or there are other standardized protocols: Protocol buffers/protobuf, BERT comes to mind.\nc) To this I would put questions on how such a foreign language Spider fits into the current scrapy framework. For example inside a project the SpiderManager currently detects available spiders, with the help of the SPIDER_MODULES settings, how would it adapt?\nThen there is statefulness to consider, do we need to know which response for the spider returned which new requests and/or items? Should it be workload-based (wait until the spider processed a response and returned everything, signaling a finish) or queue/stream-based (independent input and output, serial bus) or async callbacks. (The current Spider being asynchronous, it might be nice to have a similar, callback-based, protocol.)\nHow to best handle setup and teardown (open/close_spider), for example defining signals for spiders to trap and exit codes to return.\nNot to forget error handling - if a spider dies, is it restartable or fatal (SIGSEGV, incompatible protocol version) and stopping the crawler?\nHopefully this is a good collection of things to take into account before starting hacking. I tried to keep it brief, if anything is missing or wrong please let me know.\nLet's get this party started :)\n// cc @shaneaevans, @pablohoffman", "issue_status": "Closed", "issue_reporting_time": "2015-03-31T13:10:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1261": {"issue_url": "https://github.com/scrapy/scrapy/issues/1122", "issue_id": "#1122", "issue_summary": "Multiple Crawls (no scrapyd) signal handler Error (WebService, Address)", "issue_description": "thiagof commented on Mar 31, 2015\nI'm running a long concurrent crawl from a shell script. There are many scrapy processes running in parallel.\nTime to time one throw this errors\n015-03-31 01:11:12-0300 [scrapy] ERROR: Error caught on signal handler: <bound method ?.stop_listening of <scrapy.webservice.WebService instance at 0x7f48362a4710>>\n    Traceback (most recent call last):\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1107, in _inlineCallbacks\n        result = g.send(result)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 300, in _finish_stopping_engine\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/signalmanager.py\", line 23, in send_catch_log_deferred\n        return signal.send_catch_log_deferred(*a, **kw)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/signal.py\", line 53, in send_catch_log_deferred\n        *arguments, **named)\n    --- <exception caught here> ---\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 140, in maybeDeferred\n        result = f(*args, **kw)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/xlib/pydispatch/robustapply.py\", line 54, in robustApply\n        return receiver(*arguments, **named)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/webservice.py\", line 96, in stop_listening\n        self.port.stopListening()\n    exceptions.AttributeError: WebService instance has no attribute 'port'\n\n2015-03-31 01:12:16-0300 [scrapy] ERROR: Error caught on signal handler: <bound method ?.start_listening of <scrapy.webservice.WebService instance at 0x7fa8a733e710>>\n    Traceback (most recent call last):\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1107, in _inlineCallbacks\n        result = g.send(result)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 77, in start\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/signalmanager.py\", line 23, in send_catch_log_deferred\n        return signal.send_catch_log_deferred(*a, **kw)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/signal.py\", line 53, in send_catch_log_deferred\n        *arguments, **named)\n    --- <exception caught here> ---\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 140, in maybeDeferred\n        result = f(*args, **kw)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/xlib/pydispatch/robustapply.py\", line 54, in robustApply\n        return receiver(*arguments, **named)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/webservice.py\", line 90, in start_listening\n        self.port = listen_tcp(self.portrange, self.host, self)\n      File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/reactor.py\", line 14, in listen_tcp\n        return reactor.listenTCP(x, factory, interface=host)\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/posixbase.py\", line 495, in listenTCP\n        p.startListening()\n      File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/tcp.py\", line 991, in startListening\n        skt.listen(self.backlog)\n      File \"/usr/lib/python2.7/socket.py\", line 224, in meth\n        return getattr(self._sock,name)(*args)\n    socket.error: [Errno 98] Address already in use\nHad similar problem with telnet, but we disabled it.", "issue_status": "Closed", "issue_reporting_time": "2015-03-31T04:20:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1262": {"issue_url": "https://github.com/scrapy/scrapy/issues/1119", "issue_id": "#1119", "issue_summary": "Error in installation", "issue_description": "dongwangdw commented on Mar 30, 2015\nI installed the Scrapy-0.25.1 in crunch bang. The first time I start it. There is an error message said:\nin crawler.py 22 line , global name: SignalMadnager (which should be SignalManager) undefined. After I changed the source code then everything goes fine. But I can not see the same error in the github code. So it is a typo ?", "issue_status": "Closed", "issue_reporting_time": "2015-03-30T09:15:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1263": {"issue_url": "https://github.com/scrapy/scrapy/issues/1116", "issue_id": "#1116", "issue_summary": "fails to handle IPv6 addresses", "issue_description": "Contributor\nnyov commented on Mar 28, 2015\n$ scrapy shell 'http://[::1]/'\n2015-03-28 14:03:46+0000 [scrapy] INFO: Scrapy 0.25.0-270-gc9d7386 started (bot: scrapybot)\n2015-03-28 14:03:46+0000 [scrapy] INFO: Optional features available: ssl, http11, boto, django\n2015-03-28 14:03:46+0000 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0}\n2015-03-28 14:03:46+0000 [scrapy] INFO: Enabled extensions: TelnetConsole, CloseSpider, CoreStats, SpiderState\n2015-03-28 14:03:47+0000 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, HttpProxyMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-03-28 14:03:47+0000 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-03-28 14:03:47+0000 [scrapy] INFO: Enabled item pipelines: \n2015-03-28 14:03:47+0000 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-03-28 14:03:47+0000 [default] INFO: Spider opened\nTraceback (most recent call last):\n  File \"/usr/bin/scrapy\", line 9, in <module>\n    load_entry_point('Scrapy==0.25.0-270-gc9d7386', 'console_scripts', 'scrapy')()\n  File \"/usr/lib/pymodules/python2.7/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/usr/lib/pymodules/python2.7/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/usr/lib/pymodules/python2.7/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/usr/lib/pymodules/python2.7/scrapy/commands/shell.py\", line 65, in run\n    shell.start(url=url)\n  File \"/usr/lib/pymodules/python2.7/scrapy/shell.py\", line 44, in start\n    self.fetch(url, spider)\n  File \"/usr/lib/pymodules/python2.7/scrapy/shell.py\", line 87, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/usr/lib/pymodules/python2.7/scrapy/utils/defer.py\", line 39, in mustbe_deferred\n    result = f(*args, **kw)\n  File \"/usr/lib/pymodules/python2.7/scrapy/core/downloader/handlers/__init__.py\", line 41, in download_request\n    return handler(request, spider)\n  File \"/usr/lib/pymodules/python2.7/scrapy/core/downloader/handlers/http11.py\", line 42, in download_request\n    return agent.download_request(request)\n  File \"/usr/lib/pymodules/python2.7/scrapy/core/downloader/handlers/http11.py\", line 209, in download_request\n    d = agent.request(method, url, headers, bodyproducer)\n  File \"/usr/lib/python2.7/dist-packages/twisted/web/client.py\", line 1510, in request\n    _URI.fromBytes(uri), headers,\n  File \"/usr/lib/python2.7/dist-packages/twisted/web/client.py\", line 617, in fromBytes\n    host, port = host.split(b':')\nValueError: too many values to unpack", "issue_status": "Closed", "issue_reporting_time": "2015-03-28T14:07:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1264": {"issue_url": "https://github.com/scrapy/scrapy/issues/1115", "issue_id": "#1115", "issue_summary": "dependency on requests library / urllib python3 compatibility", "issue_description": "Contributor\nnyov commented on Mar 28, 2015\nWould it be possible to have a dependency against requests (Apache 2 licensed) in Scrapy?\nI noticed that it has superior proxy env handling, where urllib fails to parse IPs or CIDR in a $no_proxy variable. This has led to failing testcases for me and I'd like to fix it.\nAlso, requests provides a nice compat module, wrapping urllib/2 for PY2 and PY3 differences.\nIf not a dependency, could scrapy do this stuff similarly?\nPulling in urllib/urllib2/urlparse from a local wrapper seems cleaner than throwing six.moves. and try...except statements everywhere we use them. And we have plenty of those imports.", "issue_status": "Closed", "issue_reporting_time": "2015-03-28T12:58:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1265": {"issue_url": "https://github.com/scrapy/scrapy/issues/1114", "issue_id": "#1114", "issue_summary": "how to save scraped item in csv", "issue_description": "zzeenn commented on Mar 28, 2015\ni just scraped someitem from a website its all workedd very well , but wen i saved a CSV file and got a csv file in my project , so i opened it and i got nothing in that csv file just a blank page has opned , am i missing any comand or packeg ?", "issue_status": "Closed", "issue_reporting_time": "2015-03-28T08:30:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1266": {"issue_url": "https://github.com/scrapy/scrapy/issues/1111", "issue_id": "#1111", "issue_summary": "Item memory leak", "issue_description": "Contributor\nnramirezuy commented on Mar 27, 2015\nHere is a gist with outputs and how to reproduce, it is using memory_profiler.\nI think it is related to http://bugs.python.org/issue9417.\nAlso would be nice to remove or add a note in this doc.", "issue_status": "Closed", "issue_reporting_time": "2015-03-27T13:25:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1267": {"issue_url": "https://github.com/scrapy/scrapy/issues/1109", "issue_id": "#1109", "issue_summary": "SCRAPY_PROJECT environment variable is not documented", "issue_description": "Member\nkmike commented on Mar 27, 2015\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2015-03-26T21:12:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1268": {"issue_url": "https://github.com/scrapy/scrapy/issues/1107", "issue_id": "#1107", "issue_summary": "Tutorial uses deprecated `sel` shortcut", "issue_description": "Contributor\npbronez commented on Mar 27, 2015\nI'm working through the Scrapy Tutorial for the first time. The Extracting Data section uses sel extensively for selection (e.g. sel.xpath('//ul/li'))\nWhen I run these commands in the scrapy shell, I get warning:\n<string>:1: ScrapyDeprecationWarning: \"sel\" shortcut is deprecated. Use \"response.xpath()\", \"response.css()\" or \"response.selector\" instead\nIf sel is deprecated, then the tutorial shouldn't use it.\n> scrapy --version\nScrapy 0.24.4 - project: tutorial", "issue_status": "Closed", "issue_reporting_time": "2015-03-26T19:16:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1269": {"issue_url": "https://github.com/scrapy/scrapy/issues/1099", "issue_id": "#1099", "issue_summary": "IPython NB Boto Loading Error", "issue_description": "thiagof commented on Mar 24, 2015\nProgramming a Crawler from ipython notebook is causing boto to raise an error\nERROR:boto:Caught exception reading instance data\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/boto/utils.py\", line 219, in retry_url\n    r = opener.open(req)\n  File \"/usr/lib/python2.7/urllib2.py\", line 404, in open\n    response = self._open(req, data)\n  File \"/usr/lib/python2.7/urllib2.py\", line 422, in _open\n    '_open', req)\n  File \"/usr/lib/python2.7/urllib2.py\", line 382, in _call_chain\n    result = func(*args)\n  File \"/usr/lib/python2.7/urllib2.py\", line 1214, in http_open\n    return self.do_open(httplib.HTTPConnection, req)\n  File \"/usr/lib/python2.7/urllib2.py\", line 1184, in do_open\n    raise URLError(err)\nURLError: <urlopen error timed out>\nERROR:boto:Unable to read instance data, giving up\n2015-03-24 00:58:54-0300 [fipe-periodo] INFO: Closing spider (finished)\n2015-03-24 00:58:54-0300 [fipe-periodo] INFO: Dumping Scrapy stats:\n        {'downloader/request_bytes': 308,\n         'downloader/request_count': 1,\n         'downloader/request_method_count/GET': 1,\n         'downloader/response_bytes': 124031,\n         'downloader/response_count': 1,\n         'downloader/response_status_count/200': 1,\n         'finish_reason': 'finished',\n         'finish_time': datetime.datetime(2015, 3, 24, 3, 58, 54, 646566),\n         'item_scraped_count': 171,\n         'response_received_count': 1,\n         'scheduler/dequeued': 1,\n         'scheduler/dequeued/memory': 1,\n         'scheduler/enqueued': 1,\n         'scheduler/enqueued/memory': 1,\n         'start_time': datetime.datetime(2015, 3, 24, 3, 58, 53, 978029)}\n2015-03-24 00:58:54-0300 [fipe-periodo] INFO: Spider closed (finished)\nWe solved the problem by removing boto from scrapy.optional_packages\nHere is the code\nfrom twisted.internet import reactor\nfrom scrapy.crawler import Crawler\nfrom scrapy import log, signals\nfrom scrapy.utils.project import get_project_settings\n\nfrom fipe_spiders.spiders.automoveis import FipePeriodo\n\nfrom scrapy import optional_features\n# uncommented fixes my issue\n# optional_features.remove('boto')\n\ndef setup_crawler(Spider):\n    spider = Spider()\n    settings = get_project_settings()\n\n    crawler = Crawler(settings)\n    crawler.signals.connect(reactor.stop, signal=signals.spider_closed)\n    crawler.configure()\n    crawler.crawl(spider)\n    crawler.start()\n\nsetup_crawler(FipePeriodo)\n\nlog.start()\nreactor.run()", "issue_status": "Closed", "issue_reporting_time": "2015-03-24T04:58:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1270": {"issue_url": "https://github.com/scrapy/scrapy/issues/1096", "issue_id": "#1096", "issue_summary": "Scrapy 0.26? Proposing a new stable release prior GSoC.", "issue_description": "Contributor\nnyov commented on Mar 24, 2015\nIt's been a while since the last stable release.\nIn fact it's been before the last GSoC project.\nWhile it seems from the various issues I read now and then, that there is a new distant milestone for a \"1.0\" release, I think it's bad to ignore the here and now.\nI'm not even sure if it's wise to put such a goal, where \"everything\" has to be \"ready\". It'll forever stay in the future and might take away from finishing more immediate developments.\nLots of projects have failed in the past to hang in for the long haul, after starting on or announcing a refactored \"version 2\" codebase, where frustration led to complete abandonment in the end.\nThere have been many bigger changes since 0.24 now than there were after 0.22, at least that is my feeling. I've been waiting for a new release since the last GSoC and finalization of SEP-19. My spiders still run on 0.24 stable mostly, and without a new stable version I couldn't really invest time to get to know all the great changes in 0.25/master yet.\nIf at all possible with current people's workloads, I would root for a small feature freeze now and finalizing a new stable release with all that's done by now, to have a good and common starting point for GSoC 2015, before that whirlwind starts hogging all ressources (I assume), if it hits scrapy ;)", "issue_status": "Closed", "issue_reporting_time": "2015-03-23T20:26:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1271": {"issue_url": "https://github.com/scrapy/scrapy/issues/1095", "issue_id": "#1095", "issue_summary": "Remove 'scrapy deploy' command", "issue_description": "Contributor\nrdowinton commented on Mar 23, 2015\nRemove 'scrapy deploy' command and instead direct users to the documentation when they try to run it.", "issue_status": "Closed", "issue_reporting_time": "2015-03-23T17:34:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1272": {"issue_url": "https://github.com/scrapy/scrapy/issues/1094", "issue_id": "#1094", "issue_summary": "Add deployment section to documentation", "issue_description": "Contributor\nrdowinton commented on Mar 23, 2015\nWe need to add a deployment section to the documentation covering scrapyd-deploy.", "issue_status": "Closed", "issue_reporting_time": "2015-03-23T17:33:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1273": {"issue_url": "https://github.com/scrapy/scrapy/issues/1093", "issue_id": "#1093", "issue_summary": "Add scrapyd-client package", "issue_description": "Contributor\nrdowinton commented on Mar 23, 2015\nWe need to add scrapyd-client package for users so they won't need to install the full scrapyd package to deploy their code.", "issue_status": "Closed", "issue_reporting_time": "2015-03-23T17:31:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1274": {"issue_url": "https://github.com/scrapy/scrapy/issues/1091", "issue_id": "#1091", "issue_summary": "telnet localhost 6023", "issue_description": "neo-hu commented on Mar 23, 2015\ntelnet localhost 6023\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\n?", "issue_status": "Closed", "issue_reporting_time": "2015-03-23T05:27:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1275": {"issue_url": "https://github.com/scrapy/scrapy/issues/1090", "issue_id": "#1090", "issue_summary": "i have been getting this errore although im trying to make same as scrapy website has provided an exapmle , can you anybody please tell me what am i doing wrong please ?", "issue_description": "zzeenn commented on Mar 21, 2015", "issue_status": "Closed", "issue_reporting_time": "2015-03-21T14:16:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1276": {"issue_url": "https://github.com/scrapy/scrapy/issues/1088", "issue_id": "#1088", "issue_summary": "ImportError: No module named twisted", "issue_description": "karan10 commented on Mar 21, 2015\nI installed the Scrapy by following http://doc.scrapy.org/en/latest/topics/ubuntu.html#topics-ubuntu. The installation went perfectly. But when I run\nscrapy startproject myproject\nI got the following error\nTraceback (most recent call last):\n  File \"/home/karan/coding/bin/scrapy\", line 7, in <module>\n    from scrapy.cmdline import execute\n  File \"/home/karan/coding/local/lib/python2.7/site-packages/scrapy/__init__.py\", line 50, in <module>\n    from twisted import version as _txv\nImportError: No module named twisted", "issue_status": "Closed", "issue_reporting_time": "2015-03-21T06:50:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1277": {"issue_url": "https://github.com/scrapy/scrapy/issues/1084", "issue_id": "#1084", "issue_summary": "0.24.5 release notes are truncated", "issue_description": "Member\nkmike commented on Mar 19, 2015\nIt seems the longer lines are truncated in 0.24.5 release notes - check \"S3DownloadHandler..\", \"Tentative attention message...\", \"Patches Twisted issue...\" and \"SgmlLinkExtractor...\" lines.\nIt is easy to fix, but I wonder why it happened. //cc @dangra", "issue_status": "Closed", "issue_reporting_time": "2015-03-18T23:05:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1278": {"issue_url": "https://github.com/scrapy/scrapy/issues/1080", "issue_id": "#1080", "issue_summary": "Exporters shouldn't encode unicode values to bytes by default", "issue_description": "Member\nkmike commented on Mar 18, 2015\nI think it makes more sense to send unicode to serializer libraries - encoding shouldn't be handled per-field.\nJsonLinesItemExporter and JsonItemExporter shouldn't encode values to bytes because JSON is a text format. The encoding is always utf8. In Python 3.x stdlib json module doesn't even support byte keys/values, and docs say that the result of json.dump is always str (aka unicode in 2.x). By the way, it makes no sense to inherit JsonItemExporter from JsonLinesItemExporter. JsonItemExporter even overrides all JsonLinesItemExporter methods.\nXmlItemExporter shouldn't encode values to bytes because since Python 2.7.6 xml.sax.saxutils wants unicode, and in Python 3.x it requires unicode. Currently this is worked around by decoding the encoded value. The problem is that before Python 2.7.6 xml.sax.saxutils required bytes; IMHO it is better to workaround this instead of encoding/decoding data multiple times.\nFor CsvItemExporter it makes sense to encode values because in Python 2.x csv module doesn't support unicode. In Python 3.x csv supports unicode, so encoding can be seen as a temporary workaround of stdlib issues.\nFor PickleItemExporter and MarshalItemExporter encoding unicode values is bad because instead of original values users will get bytes when the result is unpickled/unmarshalled. It is not convenient, and it is an information loss (it is not clear what encoding to use to decode the value).\nI'm not sure about PprintItemExporter. Its behaviour should depend on 'file' argument - if it can handle unicode the output should be unicode, if it can't - the output should be bytes.\nPythonItemExporter assumes that serialization library can't handle unicode and mentions json, msgpack and binc. JSON can't handle bytes; msgpack can handle both bytes and unicode; I'm not sure binc is of any use currently. I think leaving unicode values as-is is a better behaviour for PythonItemExporter.", "issue_status": "Closed", "issue_reporting_time": "2015-03-17T23:06:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1279": {"issue_url": "https://github.com/scrapy/scrapy/issues/1076", "issue_id": "#1076", "issue_summary": "Document interaction between priority and CONCURRENT_ITEMS", "issue_description": "alexgarel commented on Mar 16, 2015\nI wanted to use request priority to crawl a site based on a cookie session. I had to submit a search form and navigate through search results. Because current search was held in the cookie session, I had to first navigate first search submission, then the second.\nSo I wanted to do such a thing (simplifying):\nclass MySpider(BaseSpider):\n\n    self.start_urls = ['http://example.com/search']\n\n    def parse(self, response):\n        for data in DATA:\n            yield FormRequest.from_response(\n                formdata=data, callback=self.parse_result, priority=-10)\n\n    def parse_result(self, response):\n        hxs = HtmlXPathSelector(response)\n        # handle items\n        # compute next_page_url\n        if next_page_url:\n            yield Request(next_page_url, dont_filter=True, priority=10)\nI was expecting Scrapy to\ntake first form request, yielded by parse\ncall parse_result\nthen process yielded next_page requests of higher priority, till exhausted\nthen take next form request\nIt do not happens like that. Indeed, the form request are all processed before the other requests.\nIndeed, I think this happens because Scrapy merges requests yielded by parse()\ninto a single deferred using scrapy.utils.defer.parallel.\nThis merge is controlled by CONCURRENT_ITEMS settings (defaults to 100).\nI think this particularity should be mentioned in priority documentation here, isn't it ?\nNote : I also know I can change the cookie to have separate sessions for my searches. And yes it's the way I'd go for that particular problem.", "issue_status": "Closed", "issue_reporting_time": "2015-03-16T14:38:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1280": {"issue_url": "https://github.com/scrapy/scrapy/issues/1074", "issue_id": "#1074", "issue_summary": "0 delay inside utils/defer.py->defer_succeed makes reactor not to be async enough.", "issue_description": "aliowka commented on Mar 16, 2015\nThe problem appears when HTTPCACHE_ENABLED is True.\nSuppose I need to collect items while using httpcache with FilesystemCacheStorage.\nIn the items pipeline I want to send them out with some twisted based client.\nWhile having HTTPCACHE_ENABLED I observe that the items are sent very slow and the memory is growing very fast.\nFor instance:\nWhen cache is disabled: I have about 50 items/second being sent and the total process memory is 70M. When it's enabled I have 3 items/second and 4G of memory used (my items are big). Fortunately my task is small (ends after 3 min.)\nWhen the crawling ends and spider closes, I observe the bulk of items being transfered by the client with 2K items/s speed.\nIt's clear for me that when the cache is enabled the reactor is not released for enough time to accomplish it's async tasks (items sending) and that what causes this abnormal behavior, making using of cache very questionable.\nI found that there is a delay 0 in the scrapy/scrapy/utils/defer.py : defer_succeed function:\nreactor.callLater(0, d.callback, result)\nthat seems just not enough for the cached responses that does not obey to DOWNLOAD_DELAY and potentially may through big bulks of items very fast (it does)\nI tried to increase the delay to 0.1 and this fixed the issue.", "issue_status": "Closed", "issue_reporting_time": "2015-03-16T10:20:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1281": {"issue_url": "https://github.com/scrapy/scrapy/issues/1072", "issue_id": "#1072", "issue_summary": "Javascript web page not rendering in splash", "issue_description": "drprabhakar commented on Mar 14, 2015\nI am using scrapy script to load URL using \"yield\".\nMyUrl = \"www.example.com\"\nrequest = Request(MyUrl, callback=self.mydetail)\nyield request\ndef jobdetail(self, response):\nitem['Description'] = response.xpath(\".//table[@Class='list']//text()\").extract()\nreturn item\nThe URL seems to take minimum 5 seconds to load in browser because of javascript. So I want Scrapy to render javascript. I tried splash with scrapy but getting output as empty. I tried DOWNLOAD_DELAY also but not working.", "issue_status": "Closed", "issue_reporting_time": "2015-03-14T08:30:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1282": {"issue_url": "https://github.com/scrapy/scrapy/issues/1069", "issue_id": "#1069", "issue_summary": "Error downloading, twisted.python.failure.Failure, exceptions.ValueError", "issue_description": "StagnantIce commented on Mar 7, 2015\nPlease, try\nscrapy fetch https://www.mediamarkt.ru\n2015-03-07 15:00:40+0300 [mediamarkt_ru] ERROR: Error downloading <GET https://www.mediamarkt.ru>: [<twisted.python.failure.Failure <type 'exceptions.ValueError'>>]", "issue_status": "Closed", "issue_reporting_time": "2015-03-07T12:02:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1283": {"issue_url": "https://github.com/scrapy/scrapy/issues/1064", "issue_id": "#1064", "issue_summary": "allow spiders to return dicts instead of Items", "issue_description": "Member\nkmike commented on Mar 6, 2015\nIn many cases the requirement to define and yield Items from a spider is an unnecessary complication.\nAn example from Scrapy tutorial:\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\nIt can be made simpler with dicts instead of Items:\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            yield {\n                'title': sel.xpath('a/text()').extract(),\n                'link': sel.xpath('a/@href').extract(),\n                'desc': sel.xpath('text()').extract(),\n            }\nThe version with dicts gives a developer less concepts to learn, and it is easier to explain.\nWhen field metadata is not used and data is exported to JSON/XML yielding Python dicts should be enough. Even when you export to CSV dicts could be enough - columns can be set explicitly by an user.\nThis should also prevent tickets like #968.\n\ud83d\udc4d 3", "issue_status": "Closed", "issue_reporting_time": "2015-03-05T20:53:55Z", "fixed_by": "#1081", "pull_request_summary": "Allow spiders to return dicts.", "pull_request_description": "Member\nkmike commented on Mar 18, 2015\nA PR to fix GH-1064.\nDocs are missing.", "pull_request_status": "Merged", "issue_fixed_time": "2015-03-27T18:19:27Z", "files_changed": [["8", "docs/index.rst"], ["4", "docs/topics/architecture.rst"], ["18", "docs/topics/exporters.rst"], ["13", "docs/topics/images.rst"], ["15", "docs/topics/item-pipeline.rst"], ["17", "docs/topics/items.rst"], ["19", "docs/topics/practices.rst"], ["4", "docs/topics/signals.rst"], ["9", "docs/topics/spider-middleware.rst"]]}, "1284": {"issue_url": "https://github.com/scrapy/scrapy/issues/1063", "issue_id": "#1063", "issue_summary": "Relocate scrapy.contrib and remove scrapy.contrib_exp", "issue_description": "Member\nkmike commented on Mar 6, 2015\nWhat do you think about getting rid of contrib and contrib_exp? Most of contrib is enabled by default anyways, it is not really contrib.\nFor example, we can do the following:\nscrapy.contrib.downloadermiddleware -> scrapy.downloadermiddleware;\nscrapy.contrib.exporter -> scrapy.exporter;\nscrapy.contrib.linkextractors -> scrapy.linkextractors;\nscrapy.contrib.loader -> scrapy.loader;\nscrapy.contrib.pipeline -> scrapy.pipeline;\nscrapy.contrib.spidermiddleware -> scrapy.spidermiddleware;\nscrapy.contrib.spiders -> scrapy.spiders;\nfiles from scrapy.contrib top-level folder -> scrapy.extensions;\ncontrib_exp - should we just remove it, or move contrib_exp.iterators to scrapy.util.iterators and contrib_exp.downloadermiddleware to scrapy.downloadermiddleware?\nAlso, @dangra or @pablohoffman said something about moving scrapy.core to scrapy.http or to a separate scrapy-core package.\nI think we should also move scrapy.contrib.djangoitem to a separate repo, something like scrapy-django.\nWe are also inconsistent in singular-plural names (pipeline and loader, but spiders and linkextractors). I'm not sure if it worths fixing, but maybe it worths.", "issue_status": "Closed", "issue_reporting_time": "2015-03-05T20:37:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1285": {"issue_url": "https://github.com/scrapy/scrapy/issues/1062", "issue_id": "#1062", "issue_summary": "Error when HTML tag have \":\" character", "issue_description": "marcosmachado81 commented on Mar 5, 2015\nWhen i try use the contains(@xml:lang,\"en\") the result is null\nthe tag:\n< meta text=\"teste\" xml:lang=\"pt\" />\n< meta text=\"test\" xml:lang=\"en\" />\nthe code:\nresponse.xpath(\"//meta[contains(@xml:lang='en')/@text\").extract()\ndon't work", "issue_status": "Closed", "issue_reporting_time": "2015-03-05T01:48:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1286": {"issue_url": "https://github.com/scrapy/scrapy/issues/1061", "issue_id": "#1061", "issue_summary": "scrapy fail to load in both python 2.7 and 3.4", "issue_description": "gozes commented on Mar 4, 2015\nI just installed scrapy from pypi, when I try to run any command I get the following traceback\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 9, in\nload_entry_point('Scrapy==0.24.5', 'console_scripts', 'scrapy')()\nFile \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 356, in load_entry_point\nreturn get_distribution(dist).load_entry_point(group, name)\nFile \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 2476, in load_entry_point\nreturn ep.load()\nFile \"/usr/lib/python3/dist-packages/pkg_resources.py\", line 2190, in load\n['name'])\nFile \"/usr/local/lib/python3.4/dist-packages/scrapy/init.py\", line 28, in\nimport _monkeypatches\nImportError: No module named '_monkeypatches'\nthe same traceback is display for 2.7", "issue_status": "Closed", "issue_reporting_time": "2015-03-04T16:00:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1287": {"issue_url": "https://github.com/scrapy/scrapy/issues/1059", "issue_id": "#1059", "issue_summary": "Sort items", "issue_description": "Vanuan commented on Mar 1, 2015\nIs there an option to sort Item by field while exporting?\nCurrently, when you crawl, you get random order of scraped elements.", "issue_status": "Closed", "issue_reporting_time": "2015-03-01T03:36:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1288": {"issue_url": "https://github.com/scrapy/scrapy/issues/1057", "issue_id": "#1057", "issue_summary": "how to run tests", "issue_description": "Contributor\nnyov commented on Feb 28, 2015\nWhat happened to bin/runtests.sh|bat and how do we run tests standalone now (documentation)?\nHow about a Makefile for tests and extras/coverage-report.sh?", "issue_status": "Closed", "issue_reporting_time": "2015-02-27T19:32:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1289": {"issue_url": "https://github.com/scrapy/scrapy/issues/1054", "issue_id": "#1054", "issue_summary": "S3DownloadHandler causes 1s startup delay for people who don't use S3", "issue_description": "Member\nkmike commented on Feb 27, 2015\nS3DownloadHandler tries to connect to s3 on startup, with 1s timeout. This can be seen in Scrapy logs - note 1s delay between \"Enabled extensions\" and \"Enabled downloader middlewares\" messages:\n2015-02-27 04:49:42+0500 [scrapy] INFO: Scrapy 0.25.1 started (bot: scrapybot)\n2015-02-27 04:49:42+0500 [scrapy] INFO: Optional features available: ssl, http11, boto, django\n2015-02-27 04:49:42+0500 [scrapy] INFO: Overridden settings: {'DEFAULT_ITEM_CLASS': 'dirbot.items.Website', 'NEWSPIDER_MODULE': 'dirbot.spiders', 'SPIDER_MODULES': ['dirbot.spiders']}\n2015-02-27 04:49:42+0500 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, CoreStats, SpiderState\n2015-02-27 04:49:43+0500 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\nWith s3 download handler commented out the delay disappears.\nWhat do you think about making .conn attribute lazy? It won't be possible to raise NotConfigured exception earlier, but users will get an error later.\nAnother option is to disable s3 handler by default, but it is more intrusive.", "issue_status": "Closed", "issue_reporting_time": "2015-02-26T23:57:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1290": {"issue_url": "https://github.com/scrapy/scrapy/issues/1053", "issue_id": "#1053", "issue_summary": "ValueError: Attempted relative import in non-package", "issue_description": "re4lfl0w commented on Feb 23, 2015\n$ pwd\n~/test_scrapy/scrapy/tests/test_djangoitem\n$ python __init__.py\nTraceback (most recent call last):\n  File \"__init__.py\", line 10, in <module>\n    from .models import Person, IdentifiedPerson\nValueError: Attempted relative import in non-package", "issue_status": "Closed", "issue_reporting_time": "2015-02-23T10:47:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1291": {"issue_url": "https://github.com/scrapy/scrapy/issues/1052", "issue_id": "#1052", "issue_summary": "standalone Selector()", "issue_description": "Contributor\nnyov commented on Feb 23, 2015\nI'd love to be able to use scrapy's Selector outside of scrapy, as an xpath (and css?) selector library for xml, html, sgml documents.\nI suppose I should go looking at BeautifulSoup for that, but I'm familiar with Selector and like the way it works. I don't like plain lxml's way of doing things and usually don't need any write-abilities on the tree anyway.\nWould it be feasible and sensible to make the selector parts of scrapy not depend on SomeResponse and expect response.url objects (but SomeDocument instead) and put it together with things such as LxmlDocument, then have some glue-code to bind it to object_ref and response objects and such in scrapy?\nAt least in my mind this makes sense with the \"librarization\" goal of scrapy in mind, but maybe I'm wrong or that'd be too much of a design change when there is BeautifulSoup.\np.s. before anyone comments that it is possible: I know I can use it outside scrapy, with something like this (but thats still having all of scrapy inside another project or rewriting a lot to factor out the selector stuff):\nnamespaces = [\n    ('x',    'http://www.w3.org/1999/xhtml'),\n]\nwith open('file') as f:\n    data = f.read()\nxs = Selector(text=data, type='xml')\nfor namespace, scheme in namespaces:\n    xs.register_namespace(namespace, scheme)\nprint(xs.xpath('/x:html').extract())", "issue_status": "Closed", "issue_reporting_time": "2015-02-23T01:54:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1292": {"issue_url": "https://github.com/scrapy/scrapy/issues/1050", "issue_id": "#1050", "issue_summary": "Issue with CsvItemExporter when used as Feed Exporter with a crawler that yields more than one type of Item", "issue_description": "aaearon commented on Feb 18, 2015\nMy project includes a spider that yields several different types of Items. All my items inherit from a 'DefaultItem' that has fields such as 'url', 'time_crawled' and 'group_id'.\nWhen defining FEED_FORMAT = 'csv', the feed exporter uses the fields of the first type of Item yielded as the csv's header/column names and then for subsequent yielded items (despite the Item's type) tries to export those items with the fields of the first yielded item.\nSince all of the items have 'url', 'time_crawled', 'group_id' and fields that may share the same names, those columns in the CSV get exported but otherwise the column for particular field is empty.\ntim@lindev:~/kkc$ scrapy -v Scrapy 0.24.4 - project: kkc", "issue_status": "Closed", "issue_reporting_time": "2015-02-18T14:16:09Z", "fixed_by": "#1159", "pull_request_summary": "[MRG+1] FEED_EXPORT_FIELDS option", "pull_request_description": "Member\nkmike commented on Apr 15, 2015\nAs discussed in #1081, this option is nice to have if we're yielding items as dicts. This also fixes #1050.\nThere were no tests for FeedExporter class, so I've added some.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-21T18:54:24Z", "files_changed": [["28", "docs/topics/feed-exports.rst"], ["7", "scrapy/contrib/feedexport.py"], ["1", "scrapy/settings/default_settings.py"], ["4", "scrapy/utils/testproc.py"], ["1", "tests/mockserver.py"], ["26", "tests/test_commands.py"], ["180", "tests/test_contrib_feedexport.py"]]}, "1293": {"issue_url": "https://github.com/scrapy/scrapy/issues/1046", "issue_id": "#1046", "issue_summary": "Add option to the parse command to attach metadata to requests", "issue_description": "kutschkem commented on Feb 11, 2015\nThe scrapy parse command is a wonderful tool for debugging spiders. However in my project I pass information in the requests meta attribute that is expected by the parse commands. It would be nice if it was possible to include this data in the parse command, so that testing is possible without modifying the code to not expect the meta information (and risking to forget to change everything back afterwards).\nproposed syntax would be something like:\nscrapy parse --spider myspider -c parse -m meta1=a -m meta2=b\nwhich would create dictionary entries { meta1:a, meta2:b } in the meta attribute of the Request.", "issue_status": "Closed", "issue_reporting_time": "2015-02-11T11:20:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1294": {"issue_url": "https://github.com/scrapy/scrapy/issues/1040", "issue_id": "#1040", "issue_summary": "Issue with twisted==15.0.0", "issue_description": "mou55 commented on Feb 6, 2015\nIt seems that there's an issue with twisted==15.0.0, Scrapy used to work fine with the 14.0.2version.\nSo I think 15.0.0 should be removed from https://github.com/scrapy/scrapy/blob/master/requirements.txt#L1.\nThe stack trace:\nTraceback (most recent call last):\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py\", line 38, in process_request\n    return download_func(request=request, spider=spider)\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/scrapy/core/downloader/__init__.py\", line 123, in _enqueue_request\n    self._process_queue(spider, slot)\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/scrapy/core/downloader/__init__.py\", line 143, in _process_queue\n    dfd = self._download(slot, request, spider)\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/scrapy/core/downloader/__init__.py\", line 154, in _download\n    dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n--- <exception caught here> ---\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 39, in mustbe_deferred\n    result = f(*args, **kw)\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 40, in download_request\n    return handler(request, spider)\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py\", line 36, in download_request\n    return agent.download_request(request)\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py\", line 174, in download_request\n    d = agent.request(method, url, headers, bodyproducer)\n  File \"/home/moussa/Work/python/pb-env/local/lib/python2.7/site-packages/twisted/web/client.py\", line 1560, in request\n    endpoint = self._getEndpoint(parsedURI)\nexceptions.TypeError: _getEndpoint() takes exactly 4 arguments (2 given)", "issue_status": "Closed", "issue_reporting_time": "2015-02-06T17:08:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1295": {"issue_url": "https://github.com/scrapy/scrapy/issues/1037", "issue_id": "#1037", "issue_summary": "image pipeline - background converted to black", "issue_description": "Contributor\npawelmhm commented on Feb 6, 2015\nI have somewhat unusual bug related to Scrapy Images pipeline, it all happens here https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/pipeline/images.py#L92 in convert_image method.\nI have following image with white background: http://images.vitaminimages.com/hb/vf/productimages/HB/370/021837_A.png background is converted to black for some strange reason. This happens because image is .png with mode 'P' and we are not adding background explicitly for this mode even though this is possible.\nIn [24]: from PIL import Image\n\nIn [25]: image = Image.open('021837_A.png')\n\nIn [26]: image.mode\nOut[26]: 'P'\n\nIn [27]: im = image.convert('RGB')\n\nIn [28]: im.save('demo', 'JPEG')\n# you will get image with black background in 'demo' file\nAbove code is what Image pipeline is doing. It does this if it detects that image mode is not 'RGBA'. Black background is added by PIL in conversion.\nBut it seems that there is no problem in just converting image to this 'RGBA' mode and then adding background for it regardless of image mode.\nIn [54]: background = Image.new('RGBA', image.size, (255,255,255))\n\nIn [55]: img = image.convert('RGBA')\n\nIn [56]: background.paste(img, img)\n\nIn [57]: background.save('demo_other', 'JPEG')\n# nice white background as expected \nIt would be nice to add white background always when this is possible for .png images. If user is downloading lots of images he prefers to have all of them with some specific background and not other.", "issue_status": "Closed", "issue_reporting_time": "2015-02-06T13:17:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1296": {"issue_url": "https://github.com/scrapy/scrapy/issues/1034", "issue_id": "#1034", "issue_summary": "HTTP/1.1 requests cause bad calls w/ twisted 15", "issue_description": "mkb218 commented on Feb 5, 2015\nTwisted 15.0 appears to have changed the signature of the _getEndpoint method on twisted.web.client.Agent. This causes the http11 handler to throw exceptions like so:\nTraceback (most recent call last):\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py\", line 38, in process_request\n    return download_func(request=request, spider=spider)\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/scrapy/core/downloader/__init__.py\", line 123, in _enqueue_request\n    self._process_queue(spider, slot)\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/scrapy/core/downloader/__init__.py\", line 143, in _process_queue\n    dfd = self._download(slot, request, spider)\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/scrapy/core/downloader/__init__.py\", line 154, in _download\n    dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n--- <exception caught here> ---\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 39, in mustbe_deferred\n    result = f(*args, **kw)\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 40, in download_request\n    return handler(request, spider)\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py\", line 36, in download_request\n    return agent.download_request(request)\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py\", line 174, in download_request\n    d = agent.request(method, url, headers, bodyproducer)\n  File \"/usr/share/python/spotify-prelude2-directed-crawlers/local/lib/python2.7/site-packages/twisted/web/client.py\", line 1560, in request\n    endpoint = self._getEndpoint(parsedURI)\nexceptions.TypeError: _getEndpoint() takes exactly 4 arguments (2 given)\nThat method's signature in Twisted 15.0.0 is def _getEndpoint(self, uri): while in version 14.0.2 it is def _getEndpoint(self, scheme, host, port):", "issue_status": "Closed", "issue_reporting_time": "2015-02-04T22:29:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1297": {"issue_url": "https://github.com/scrapy/scrapy/issues/1031", "issue_id": "#1031", "issue_summary": "IPv6 support? Problem running home page example from an IPv6 network", "issue_description": "ronnix commented on Jan 31, 2015\nI'm running into problems while trying to run the example on the scrapy.org home page from the FOSDEM IPv6-only Wi-Fi network. (The scraper works fine from an IPv4 network.)\nIf both IPv4 and IPv6 are enabled on my computer (OS X Yosemite), and the IPv4 is configured with DHCP, and thus gets a self-assigned address (169.254.x.x), then I get timeout errors:\n$ scrapy runspider myspider.py\n:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named service_identity'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n2015-01-31 12:13:32+0100 [scrapy] INFO: Scrapy 0.24.4 started (bot: scrapybot)\n2015-01-31 12:13:32+0100 [scrapy] INFO: Optional features available: ssl, http11\n2015-01-31 12:13:32+0100 [scrapy] INFO: Overridden settings: {}\n2015-01-31 12:13:32+0100 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2015-01-31 12:13:32+0100 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-01-31 12:13:32+0100 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-01-31 12:13:32+0100 [scrapy] INFO: Enabled item pipelines:\n2015-01-31 12:13:32+0100 [blogspider] INFO: Spider opened\n2015-01-31 12:13:32+0100 [blogspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-01-31 12:13:32+0100 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-01-31 12:13:32+0100 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n2015-01-31 12:14:32+0100 [blogspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-01-31 12:14:48+0100 [blogspider] DEBUG: Retrying <GET http://blog.scrapinghub.com> (failed 1 times): TCP connection timed out: 60: Operation timed out.\n2015-01-31 12:15:32+0100 [blogspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-01-31 12:16:03+0100 [blogspider] DEBUG: Retrying <GET http://blog.scrapinghub.com> (failed 2 times): TCP connection timed out: 60: Operation timed out.\n2015-01-31 12:16:32+0100 [blogspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-01-31 12:17:18+0100 [blogspider] DEBUG: Gave up retrying <GET http://blog.scrapinghub.com> (failed 3 times): TCP connection timed out: 60: Operation timed out.\n2015-01-31 12:17:18+0100 [blogspider] ERROR: Error downloading <GET http://blog.scrapinghub.com>: TCP connection timed out: 60: Operation timed out.\n2015-01-31 12:17:18+0100 [blogspider] INFO: Closing spider (finished)\n2015-01-31 12:17:18+0100 [blogspider] INFO: Dumping Scrapy stats:\n    {'downloader/exception_count': 3,\n     'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,\n     'downloader/request_bytes': 657,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2015, 1, 31, 11, 17, 18, 494774),\n     'log_count/DEBUG': 5,\n     'log_count/ERROR': 1,\n     'log_count/INFO': 10,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2015, 1, 31, 11, 13, 32, 955024)}\n2015-01-31 12:17:18+0100 [blogspider] INFO: Spider closed (finished)\nIf I turn off IPv4 completely, then scrapy fails with \"No route to host\" errors:\n$ scrapy runspider myspider.py\n:0: UserWarning: You do not have a working installation of the service_identity module: 'No module named service_identity'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module and a recent enough pyOpenSSL to support it, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n2015-01-31 12:10:06+0100 [scrapy] INFO: Scrapy 0.24.4 started (bot: scrapybot)\n2015-01-31 12:10:06+0100 [scrapy] INFO: Optional features available: ssl, http11\n2015-01-31 12:10:06+0100 [scrapy] INFO: Overridden settings: {}\n2015-01-31 12:10:06+0100 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2015-01-31 12:10:06+0100 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-01-31 12:10:06+0100 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2015-01-31 12:10:06+0100 [scrapy] INFO: Enabled item pipelines:\n2015-01-31 12:10:06+0100 [blogspider] INFO: Spider opened\n2015-01-31 12:10:06+0100 [blogspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2015-01-31 12:10:06+0100 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2015-01-31 12:10:06+0100 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n2015-01-31 12:10:06+0100 [blogspider] DEBUG: Retrying <GET http://blog.scrapinghub.com> (failed 1 times): No route to host: 51: Network is unreachable.\n2015-01-31 12:10:06+0100 [blogspider] DEBUG: Retrying <GET http://blog.scrapinghub.com> (failed 2 times): No route to host: 51: Network is unreachable.\n2015-01-31 12:10:06+0100 [blogspider] DEBUG: Gave up retrying <GET http://blog.scrapinghub.com> (failed 3 times): No route to host: 51: Network is unreachable.\n2015-01-31 12:10:06+0100 [blogspider] ERROR: Error downloading <GET http://blog.scrapinghub.com>: No route to host: 51: Network is unreachable.\n2015-01-31 12:10:06+0100 [blogspider] INFO: Closing spider (finished)\n2015-01-31 12:10:06+0100 [blogspider] INFO: Dumping Scrapy stats:\n    {'downloader/exception_count': 3,\n     'downloader/exception_type_count/twisted.internet.error.NoRouteError': 3,\n     'downloader/request_bytes': 657,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2015, 1, 31, 11, 10, 6, 482287),\n     'log_count/DEBUG': 5,\n     'log_count/ERROR': 1,\n     'log_count/INFO': 7,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2015, 1, 31, 11, 10, 6, 463382)}\n2015-01-31 12:10:06+0100 [blogspider] INFO: Spider closed (finished)\nNote that I can open the blog.scrapinghub.com site in Safari, so the target web site does support IPv6 and the problem seems to be on scrapy's side.\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2015-01-31T11:18:33Z", "fixed_by": "#4227", "pull_request_summary": "Name resolver with IPv6 support", "pull_request_description": "Member\nelacuesta commented on Dec 12, 2019 \u2022\nedited\nFixes #1031. Based on #1031 (comment).\nAdded the ability to enable an experimental name resolver with IPv6 support (DNS_RESOLVER setting, which defaults to the current scrapy.resolver.CachingThreadedResolver class). So far I couldn't find a \"native\" way to enforce a specific timeout for DNS requests, the approach in f1c1846 is not working (see https://twistedmatrix.com/trac/ticket/9748). Still, I think we can document this fact properly and give users the chance to choose, knowing the implications.\nTo do:\nIPv6-specific tests (the current test suite works with the new resolver)\nDocs\nShell examples:\n$ scrapy shell http://ipv6.google.com -s DNS_RESOLVER=scrapy.resolver.CachingHostnameResolver\n(...)\n2020-01-16 03:58:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://ipv6.google.com> (referer: None)\n[s] Available Scrapy objects:\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n[s]   crawler    <scrapy.crawler.Crawler object at 0x101d635c0>\n[s]   item       {}\n[s]   request    <GET http://ipv6.google.com>\n[s]   response   <200 http://ipv6.google.com>\n[s]   settings   <scrapy.settings.Settings object at 0x104627ef0>\n[s]   spider     <DefaultSpider 'default' at 0x104b5bef0>\n[s] Useful shortcuts:\n[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n[s]   shelp()           Shell help (print this help)\n[s]   view(response)    View response in a browser\nIn [1]: response.url\nOut[1]: 'http://ipv6.google.com'\n$ scrapy shell http://ipv6.google.com\n(...)\n2020-01-16 03:59:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://ipv6.google.com> (failed 1 times): DNS lookup failed: no results for hostname lookup: ipv6.google.com.\n2020-01-16 03:59:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://ipv6.google.com> (failed 2 times): DNS lookup failed: no results for hostname lookup: ipv6.google.com.\n2020-01-16 03:59:38 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://ipv6.google.com> (failed 3 times): DNS lookup failed: no results for hostname lookup: ipv6.google.com.\nTraceback (most recent call last):\n  File \"/.../scrapy/venv-scrapy/bin/scrapy\", line 11, in <module>\n    load_entry_point('Scrapy', 'console_scripts', 'scrapy')()\n  File \"/.../scrapy/scrapy/cmdline.py\", line 145, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/.../scrapy/scrapy/cmdline.py\", line 99, in _run_print_help\n    func(*a, **kw)\n  File \"/.../scrapy/scrapy/cmdline.py\", line 153, in _run_command\n    cmd.run(args, opts)\n  File \"/.../scrapy/scrapy/commands/shell.py\", line 74, in run\n    shell.start(url=url, redirect=not opts.no_redirect)\n  File \"/.../scrapy/scrapy/shell.py\", line 46, in start\n    self.fetch(url, spider, redirect=redirect)\n  File \"/.../scrapy/scrapy/shell.py\", line 114, in fetch\n    reactor, self._schedule, request, spider)\n  File \"/.../scrapy/venv-scrapy/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\n    result.raiseException()\n  File \"/.../scrapy/venv-scrapy/lib/python3.6/site-packages/twisted/python/failure.py\", line 467, in raiseException\n    raise self.value.with_traceback(self.tb)\ntwisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: ipv6.google.com.\n\ud83c\udf89 2", "pull_request_status": "Merged", "issue_fixed_time": "2020-01-25T00:03:14Z", "files_changed": [["7", "docs/faq.rst"], ["13", "docs/topics/settings.rst"], ["1", "pytest.ini"], ["45", "scrapy/crawler.py"], ["75", "scrapy/resolver.py"], ["1", "scrapy/settings/default_settings.py"], ["15", "tests/CrawlerProcess/alternative_name_resolver.py"], ["12", "tests/CrawlerProcess/default_name_resolver.py"], ["18", "tests/test_crawler.py"]]}, "1298": {"issue_url": "https://github.com/scrapy/scrapy/issues/1030", "issue_id": "#1030", "issue_summary": "Adding more flexible crawl configuration (expanding start_url)?", "issue_description": "yangmillstheory commented on Jan 30, 2015\nI'm working on a scrapy project where a \"rabbit client\" and \"crawl worker\" work together to consume scrape requests from a queue. These requests have more configuration than a start_url - it could be something like url and a set of xpaths, or a domain-specific configuration, like site-specific product ID (from which we programmatically build the url) and optional identifiers like color, style, and size to further specify the item one wants to scrape.\nI ended up rolling my own generic abstract base classes to support this. These classes expose an interface that provide for crawl configuration validation and deserialization.\nI'm wondering if it would be desirable to have built-in support for more specific \"crawl configurations\" like this within the framework? If that's the case, I'd be more than happy to have a design discussion and hash out the details.\nI'm looking for specifically for replies from project maintainers/contributors to see whether or not this is feasible - I don't want to waste anyone's time, including mine. If it is, I'd love to tackle it myself with some design guidance, to make sure I'm working with the framework instead of against it.", "issue_status": "Closed", "issue_reporting_time": "2015-01-30T02:14:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1299": {"issue_url": "https://github.com/scrapy/scrapy/issues/1027", "issue_id": "#1027", "issue_summary": "Remove deploy command", "issue_description": "Member\npablohoffman commented on Jan 28, 2015\nscrapy deploy command was deprecated a while ago and is no longer maintained. Its functionality has been moved to scrapy-deploy (for scrapyd) and shub deploy (for scrapinghub).\nSo I think we should remove deploy command, specially for 1.0.", "issue_status": "Closed", "issue_reporting_time": "2015-01-28T15:40:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1300": {"issue_url": "https://github.com/scrapy/scrapy/issues/1026", "issue_id": "#1026", "issue_summary": "How to get the number of requests in queue in scrapy?", "issue_description": "hitalex commented on Jan 27, 2015\nI am using scrapy to crawl some websites. How to get the number of requests in queue in scrapy?\nI have posted this question in stackoverflow: http://stackoverflow.com/questions/28169756/how-to-get-the-number-of-requests-in-queue-in-scrapy", "issue_status": "Closed", "issue_reporting_time": "2015-01-27T11:47:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1301": {"issue_url": "https://github.com/scrapy/scrapy/issues/1025", "issue_id": "#1025", "issue_summary": "got a NotImplementedError with tutorial", "issue_description": "astwyg commented on Jan 27, 2015\ni am following Scrapy Documentation, Release 0.24.0\non a window7(x32)\nall required packages are installed using pip\ni meet a NotImplementedError error during step:\nrun scrapy crawl dmoz\nmy console looks like:\nE:\\DOCs\\46Scrapy\\tutorial>scrapy crawl dmoz\n2015-01-27 10:58:32+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: tutorial)\n2015-01-27 10:58:32+0800 [scrapy] INFO: Optional features available: ssl, http11\n, django\n2015-01-27 10:58:32+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE'\n: 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutor\nial'}\n2015-01-27 10:58:33+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetCons\nole, CloseSpider, WebService, CoreStats, SpiderState\n2015-01-27 10:58:35+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuth\nMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, Def\naultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, Redirec\ntMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2015-01-27 10:58:35+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMid\ndleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddlew\nare\n2015-01-27 10:58:35+0800 [scrapy] INFO: Enabled item pipelines:\n2015-01-27 10:58:35+0800 [dmoz] INFO: Spider opened\n2015-01-27 10:58:35+0800 [dmoz] INFO: Crawled 0 pages (at 0 pages/min), scraped\n0 items (at 0 items/min)\n2015-01-27 10:58:35+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6\n023\n2015-01-27 10:58:35+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n\n2015-01-27 10:58:36+0800 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Co\nmputers/Programming/Languages/Python/Resources/> (referer: None)\n2015-01-27 10:58:36+0800 [dmoz] ERROR: Spider error processing <GET http://www.d\nmoz.org/Computers/Programming/Languages/Python/Resources/>\n        Traceback (most recent call last):\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 12\n01, in mainLoop\n            self.runUntilCurrent()\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 82\n4, in runUntilCurrent\n            call.func(*call.args, **call.kw)\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 3\n82, in callback\n            self._startRunCallbacks(result)\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 4\n90, in _startRunCallbacks\n            self._runCallbacks()\n        --- <exception caught here> ---\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 5\n77, in _runCallbacks\n            current.result = callback(current.result, *args, **kw)\n          File \"C:\\Python27\\lib\\site-packages\\scrapy\\spider.py\", line 56, in par\nse\n            raise NotImplementedError\n        exceptions.NotImplementedError:\n\n2015-01-27 10:58:36+0800 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Co\nmputers/Programming/Languages/Python/Books/> (referer: None)\n2015-01-27 10:58:36+0800 [dmoz] ERROR: Spider error processing <GET http://www.d\nmoz.org/Computers/Programming/Languages/Python/Books/>\n        Traceback (most recent call last):\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 12\n01, in mainLoop\n            self.runUntilCurrent()\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\base.py\", line 82\n4, in runUntilCurrent\n            call.func(*call.args, **call.kw)\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 3\n82, in callback\n            self._startRunCallbacks(result)\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 4\n90, in _startRunCallbacks\n            self._runCallbacks()\n        --- <exception caught here> ---\n          File \"C:\\Python27\\lib\\site-packages\\twisted\\internet\\defer.py\", line 5\n77, in _runCallbacks\n            current.result = callback(current.result, *args, **kw)\n          File \"C:\\Python27\\lib\\site-packages\\scrapy\\spider.py\", line 56, in par\nse\n            raise NotImplementedError\n        exceptions.NotImplementedError:\n\n2015-01-27 10:58:36+0800 [dmoz] INFO: Closing spider (finished)\n2015-01-27 10:58:36+0800 [dmoz] INFO: Dumping Scrapy stats:\n        {'downloader/request_bytes': 516,\n         'downloader/request_count': 2,\n         'downloader/request_method_count/GET': 2,\n         'downloader/response_bytes': 16342,\n         'downloader/response_count': 2,\n         'downloader/response_status_count/200': 2,\n         'finish_reason': 'finished',\n         'finish_time': datetime.datetime(2015, 1, 27, 2, 58, 36, 731000),\n         'log_count/DEBUG': 4,\n         'log_count/ERROR': 2,\n         'log_count/INFO': 7,\n         'response_received_count': 2,\n         'scheduler/dequeued': 2,\n         'scheduler/dequeued/memory': 2,\n         'scheduler/enqueued': 2,\n         'scheduler/enqueued/memory': 2,\n         'spider_exceptions/NotImplementedError': 2,\n         'start_time': datetime.datetime(2015, 1, 27, 2, 58, 35, 186000)}\n2015-01-27 10:58:36+0800 [dmoz] INFO: Spider closed (finished)\nit seems something wrong with twisted,i change the version of twisted to 13.2.0 , but the error still there.\ni searched google got no answer about this question.\nSo, anyone with idea please help me.", "issue_status": "Closed", "issue_reporting_time": "2015-01-27T03:09:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1302": {"issue_url": "https://github.com/scrapy/scrapy/issues/1024", "issue_id": "#1024", "issue_summary": "No doc for the ExtensionManager", "issue_description": "hitalex commented on Jan 26, 2015\nThere is no doc for scrapy.extension.ExtensionManager.\nBesides, by looking at the code, ExtensionManager is derived from MiddlewareManager and both of them do not provide any useful information, except self.middlewares.\nI think the ExtensionManager should provide access to all the initiated extension objects by name, perhaps.", "issue_status": "Closed", "issue_reporting_time": "2015-01-26T14:58:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1303": {"issue_url": "https://github.com/scrapy/scrapy/issues/1019", "issue_id": "#1019", "issue_summary": "Crawl multiple domains", "issue_description": "rodrigoalviani commented on Jan 22, 2015\nHow to crawl multiple domains simultaneously?\nI have a CSV with 500k different domains, would like to get the home page of all, and with the largest number of possible parallel processes.", "issue_status": "Closed", "issue_reporting_time": "2015-01-21T20:38:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1304": {"issue_url": "https://github.com/scrapy/scrapy/issues/1015", "issue_id": "#1015", "issue_summary": "process_spider_exception not called with exception from spider", "issue_description": "Contributor\naufziehvogel commented on Jan 18, 2015\nAccording to the documentation, process_spider_exception should also be called when a spider throws an exception. To my understanding, this would include throwing an exception from any parse method like this:\n    def parse_item(self, response):\n        log.msg(\"[parse_item] Now in exceptional parse\", level=log.INFO)\n        raise Exception('foo')\nMy middleware looks like this:\nclass ManyExceptionsMiddleware(object):\n    def process_spider_output(self, response, result, spider):\n        log.msg(\"[process_spider_output] Shows that middleware IS installed\", level=log.INFO)\n        return result\n\n    def process_spider_exception(self, response, exception, spider):\n        log.msg(\"[process_spider_exception] Many exceptions on %s\" % spider.name, level=log.WARNING)\n        return []\nThis results in:\n2015-01-18 18:08:01+0100 [example] DEBUG: Crawled (200) <GET some-secret-url> (referer: some-other-url)\n2015-01-18 18:08:01+0100 [scrapy] INFO: [process_spider_output] Shows that middleware IS installed\n2015-01-18 18:08:01+0100 [scrapy] INFO: [parse_item] Now in exceptional parse\n2015-01-18 18:08:01+0100 [example] ERROR: Spider error processing <GET some-secret-url>\n    Traceback (most recent call last):\n[...]\n    exceptions.Exception: foo\nThen I added the following additional method to check that process_spider_exception works (because the only exception handling in scrapy itself is done like this).\ndef process_spider_input(self, response, spider):\n    raise Exception('foo')\nThen the output looks like this:\n2015-01-18 18:09:53+0100 [example] DEBUG: Crawled (200) <GET some-secret-url> (referer: None)\n2015-01-18 18:09:53+0100 [scrapy] WARNING: [process_spider_exception] Many exceptions on some-secret-domain\n2015-01-18 18:09:53+0100 [scrapy] INFO: [process_spider_output] Shows that middleware IS installed\nIf you could tell me, where this all should happen, I could look into the code to fix it (if I understand it well enough).\n\ud83d\udc4d 5", "issue_status": "Closed", "issue_reporting_time": "2015-01-18T17:12:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1305": {"issue_url": "https://github.com/scrapy/scrapy/issues/1010", "issue_id": "#1010", "issue_summary": "Expanding default list of processors?", "issue_description": "Contributor\nGranitosaurus commented on Jan 13, 2015\nProcessors for Item Loaders are really powerful and quite unavoidable. However the default set seems be rather barebones.\nI have few suggestions from few processors I use quite often to fix that, them being:\nStripChars(chars) - strips away leading/following chars.\nTakeNth(position) - Like TakeFirst() but for nth element.\nOnlyDigits() - gets rid of everything that is not a digit.\nOnlyChars() - gets rid of digits and punctuation characters(string.punctuation).\nCould any of those fit in scrapy processors?\nedit: see pull request for my attempt: #1012", "issue_status": "Closed", "issue_reporting_time": "2015-01-13T12:15:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1306": {"issue_url": "https://github.com/scrapy/scrapy/issues/1009", "issue_id": "#1009", "issue_summary": "Small Issue on the Extensions page of the documentation", "issue_description": "Contributor\nSudShekhar commented on Jan 13, 2015\nHi,\nIn the Sample Extension code given on the extensions page (http://doc.scrapy.org/en/latest/topics/extensions.html), we are setting self.item_count = 0 whereas (I think) it should be self.items_scraped =0 as items_scrapped is the counter", "issue_status": "Closed", "issue_reporting_time": "2015-01-13T06:51:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1307": {"issue_url": "https://github.com/scrapy/scrapy/issues/1005", "issue_id": "#1005", "issue_summary": "loader.add_json?", "issue_description": "Contributor\npawelmhm commented on Jan 8, 2015\nJust an idea.\nI'm dealing with lots of json so very often I have to write code like this:\nloader = SomeLoader()\ndata = json.loads(response.body)\ndata_i_need = data[\"some_key\"]\nloader.add_value(\"something\", data_i_need.get(\"some_other_key\"))\nloader.add_value(\"another_value\", data_i_need.get('something_else\"))\nthis can go on for many lines and can be repetitve.\nWould be really nice to have all this done in loader, so that I could write\nloader = SomeLoader(json=response.body, json_key=\"some_key\")\nloader.add_json(\"something\", \"some_other_key\")\ndon't know how difficult to implement this would be, just suggesting this as possible improvement.", "issue_status": "Closed", "issue_reporting_time": "2015-01-08T13:03:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1308": {"issue_url": "https://github.com/scrapy/scrapy/issues/1003", "issue_id": "#1003", "issue_summary": "How to get request/response meta info when errback is called?", "issue_description": "wlnirvana commented on Jan 6, 2015\nI wonder if it's possible to get the meta and response.url when errback is called. Here is my code:\nclass CdDvdSpider(scrapy.Spider):\n    ...\n    ...\n\n    def make_requests_from_url(self, url):\n        return Request(url, dont_filter=True, meta={'foo': 'foo'},\n                       callback=self.parse, errback=self.error_handler)\n\n    def error_handler(self, response):    # FIXME: argument is not response\n        # print response.url\n        # print response.meta['foo']\n        pass\nWhat I got is:\nexceptions.AttributeError: Failure instance has no attribute 'meta'\nI found a related post here on SO. It seems the latest Scrapy (0.24.4 as of this post) still doesn't provide relevant API. If this is true, is it possible to add this feature?", "issue_status": "Closed", "issue_reporting_time": "2015-01-06T01:58:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1309": {"issue_url": "https://github.com/scrapy/scrapy/issues/1001", "issue_id": "#1001", "issue_summary": "signal spider_closed won't fire", "issue_description": "chiyuan-goh commented on Jan 2, 2015\nHave been trying to register my function to the signal spider_closed either from within my middleware or from the main function. So far, only spider_opened works. Scrapy==0.24.4 twisted==14.0.2\ndef abc():\n    print 'works!'\nclass IgnoreDuplicatesMiddleware(object):\n    @classmethod\n    def from_crawler(cls, crawler):\n        crawler.signals.connect(abc, signal=signals.spider_closed)\n        crawler.signals.connect(abc, signal=signals.spider_opened)\n\n    def on_spider_closed(self):\n        self.conn.executemany(\"insert into crawled_urls(url) VALUES(?)\", [(url) for url in self.fresh_meat])\n        self.conn.close()\n        print(\"Spider closed. Saving %s new urls to already-crawled db.\"%len(self.fresh_meat))\n\n    def __init__(self):\n        self.crawled_urls = set()\n        self.fresh_meat = set()\n\n        db = os.path.join(os.path.dirname(os.path.abspath(__file__)),  'scrapy.db')\n        print db\n\n        self.conn= sqlite3.connect(db)\n        cur = self.conn.cursor()\n        cur.execute(\"\"\"SELECT url from crawled_urls;\"\"\")\n        self.crawled_urls.update(i[0] for i in cur.fetchall())\n\n        log.msg(\"There are %s already-crawled links.\" %len(self.crawled_urls), level=log.INFO)\n        SignalManager(dispatcher.Any).connect(abc, signals.spider_closed)\n\n    def process_request(self, request, spider):\n        if request.url in self.crawled_urls:\n            raise IgnoreRequest()\n        else:\n            self.crawled_urls.add(request.url)\n            self.fresh_meat.add(request.url)\nif __name__ == '__main__':\n        spider = AlphaSpider()\n        from scrapy.conf import settings\n        settings.set('DOWNLOADER_MIDDLEWARES', {'download.healthscraper.middlewares.IgnoreDuplicatesMiddleware': 543,},\n                        'cmdline')\n\n        crawler = Crawler(settings)\n        crawler.signals.connect(abc, signal=signals.spider_closed)\n        crawler.configure()\n        crawler.crawl(spider)\n        crawler.start()\n\n        py_observer = PythonLoggingObserver()\n        py_observer.start()\n        reactor.run()", "issue_status": "Closed", "issue_reporting_time": "2015-01-02T09:35:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1310": {"issue_url": "https://github.com/scrapy/scrapy/issues/997", "issue_id": "#997", "issue_summary": "Passing spider command line arguments to contracts", "issue_description": "dfockler commented on Dec 30, 2014\nJust wondering if there is a way for check to pass in command line arguments to a spider when running the contract. I couldn't find it anywhere online or in the docs. Something like\nscrapy check spider -a arg=argument\nor passing it as an argument to the @url\nThanks", "issue_status": "Closed", "issue_reporting_time": "2014-12-29T22:40:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1311": {"issue_url": "https://github.com/scrapy/scrapy/issues/996", "issue_id": "#996", "issue_summary": "Scrapy 1.0 release", "issue_description": "Contributor\nrdowinton commented on Dec 29, 2014\nI've created this ticket so we can discuss the Scrapy 1.0 release. @kmike has created a milestone so we can begin adding issues when we are ready. This ticket will be a good place to keep track of any discussions/ideas related to the release, including our motivations behind a 1.0 release, and of course what we would like to see go into it. It would be good if we could look at why we want to do a 1.0 release, so we know what's best to include.\nI know Mikhail is interested in seeing API and documentation related issues go into this release, as well as Python 3 support. Below are the examples he has provided:\nExamples of API-related tickets:\n#906 (move Selector or its parts to its own library)\n#548 (add a better support for relative URLs);\n#578, #568 and alike tickets - now there are link extractors, selectors and item loaders which do very similar things;\nallow to return dicts instead of Items from spiders (I don't think there is a ticket for that); it will make Scrapy easier to use for quick scripts and prevent tickets like #968.\nDocs-related tickets:\n#609 (improve the tutorial - I like http://hopefulramble.blogspot.ru/2014/08/web-scraping-with-scrapy-first-steps_30.html a lot);\n#713 (move docstrings from docs to the source code);\n@eliasdorneles has also mentioned this specific issue: #712\nThe 1.0 release will also be a good opportunity to improve things like versioning. Currently odd numbers correspond to development versions, and even numbers correspond to releases--this is something we probably want to change. Any changes which would require breaking backwards compatibility would be suited to this release too, as it would be reasonable to expect such changes in moving from 0.2x to 1.0.", "issue_status": "Closed", "issue_reporting_time": "2014-12-29T13:01:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1312": {"issue_url": "https://github.com/scrapy/scrapy/issues/995", "issue_id": "#995", "issue_summary": "One scrapy job is running forever, can't be stopped.", "issue_description": "cnglen commented on Dec 29, 2014\nOne scrapy job is running forever, can't be stopped.\nI have set downloader timeout to 30\nIn my code, I checked the running periodlly, if longer than some time, use 'http://localhost:6800/cancel.json' to cancel the job.\nHowever, the job is still running. How can I debug this situation?(Bug of my src, scrapy, or scrapyd?) Any ideas? Thanks.\nFinally, I run 'sudo kill -SIGKILL 13902' to kill the process. 'sudo kill -SIGTERM 13902' doesn't work.\n$ curl http://localhost:6800/cancel.json -d project=myproject -d job=5139c1ac8f0a11e4b0ed247703282fcc\n{\"status\": \"ok\", \"prevstate\": \"running\"} <--- It returns status ok, however, the job can't be stopped.\n$ ps aux|grep 13902\nscrapy 13902 0.0 0.8 172860 64636 ? S 11:25 0:01 /usr/bin/python -m scrapyd.runner crawl mysider -a _job=5139c1ac8f0a11e4b0ed247703282fcc\nscrapy.version\nu'0.24.4'\nscrapyd.version\n'1.0.1'\nlog:\n2014-12-29 11:25:26+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: myproject)\n2014-12-29 11:25:26+0800 [scrapy] INFO: Optional features available: ssl, http11\n2014-12-29 11:25:26+0800 [scrapy] INFO: Overridden settings:\n{'COOKIES_DEBUG': True, 'NEWSPIDER_MODULE': 'myproject.spiders',\n'FEED_URI':\n'/var/lib/scrapyd/items/myproject/myspider/5139c1ac8f0a11e4b0ed247703282fcc.jl',\n'SPIDER_MODULES': ['myproject.spiders'], 'RETRY_HTTP_CODES': [500,\n502, 503, 504, 400, 408, 403, 404], 'BOT_NAME': 'myproject',\n'DOWNLOAD_TIMEOUT': 30, 'COOKIES_ENABLED': False, 'LOG_FILE': <--------------------timeout\n'/var/log/scrapyd/myproject/myspider/5139c1ac8f0a11e4b0ed247703282fcc.log',\n'DOWNLOAD_DELAY': 2}\n2014-12-29 11:25:26+0800 [scrapy] INFO: Enabled extensions:\nFeedExporter, LogStats, TelnetConsole, CloseSpider, WebService,\nCoreStats, SpiderState\n2014-12-29 11:25:26+0800 [scrapy] INFO: Enabled downloader\nmiddlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware,\nRandomUserAgentMiddleware, RandomProxyMiddleware, RetryMiddleware,\nDefaultHeadersMiddleware, MetaRefreshMiddleware,\nHttpCompressionMiddleware, RedirectMiddleware,\nChunkedTransferMiddleware, DownloaderStats\n2014-12-29 11:25:26+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-12-29 11:25:26+0800 [scrapy] INFO: Enabled item pipelines: ImagesPipeline, WordpressPipeline, MySQLStorePipeline\n2014-12-29 11:25:26+0800 [myspider] INFO: Spider opened\n2014-12-29 11:25:26+0800 [myspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2014-12-29 11:25:26+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025\n2014-12-29 11:25:26+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082\n2014-12-29 11:45:52+0800 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force\n2014-12-29 11:46:22+0800 [scrapy] INFO: Received SIGTERM twice, forcing unclean shutdown <------------------------\n$ sudo kill -SIGTERM 13902\n$ ps aux|grep 13902\nscrapy 13902 0.0 0.8 172860 64636 ? S 11:25 0:01 /usr/bin/python -m scrapyd.runner crawl myspider -a _job=5139c1ac8f0a11e4b0ed247703282fcc\ntouch 18543 0.0 0.0 18248 2204 pts/5 S+ 14:24 0:00 grep 13902\n$ sudo kill -SIGKILL 13902\n$ ps aux|grep 13902\ntouch 18551 0.0 0.0 18244 2204 pts/5 S+ 14:25 0:00 grep 13902", "issue_status": "Closed", "issue_reporting_time": "2014-12-29T06:20:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1313": {"issue_url": "https://github.com/scrapy/scrapy/issues/993", "issue_id": "#993", "issue_summary": "Scrappy virtualenv django settings", "issue_description": "Eimis commented on Dec 27, 2014\nThought that may be interesting for you. I couldn't get Scrapy working for existing project - Scrapy was unable to find my projects settings file. The only way that I got it working was hardcoding my app's settings in ~/.virtualenvs/myproject/bin/scrapy:\n#!/home/eimantas/.virtualenvs/myproject/bin/python\n\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nimport os\n\nfrom scrapy.cmdline import execute\n\nif __name__ == '__main__':\n    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"myapp.settings\")\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(execute())", "issue_status": "Closed", "issue_reporting_time": "2014-12-26T22:11:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1314": {"issue_url": "https://github.com/scrapy/scrapy/issues/990", "issue_id": "#990", "issue_summary": "Scrapy will not export file when run from a script.", "issue_description": "ryancerf commented on Dec 20, 2014\nPlease see the following three unanswered stackoverflow questions:\nhttp://stackoverflow.com/questions/27573265/scrapy-from-script-will-not-export-data\nhttp://stackoverflow.com/questions/19080575/json-not-working-in-scrapy-when-calling-spider-through-a-python-script\nhttp://stackoverflow.com/questions/15483898/calling-scrapy-from-a-python-script-not-creating-json-output-file\nI have tried to get the file to export with a pipeline and with Feed Export. Both of these ways work when I run scrapy from the command line, but neither work when I run scrapy from a script.\nAfter I run scrapy from the script the log says: \"Stored csv feed (341 items) in: output.csv\", but there is no output.csv to be found.", "issue_status": "Closed", "issue_reporting_time": "2014-12-20T03:18:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1315": {"issue_url": "https://github.com/scrapy/scrapy/issues/985", "issue_id": "#985", "issue_summary": "Scrapy 0.25 hangs closing spider", "issue_description": "Member\ndangra commented on Dec 16, 2014\nA basic spider like this hangs and is only possible to close the spider by double Ctrl-C\nclass TestSpider1c(scrapy.Spider):\n    name = \"test1c\"\n    allowed_domains = ['productlibrary.brandbank.com']\n\n    start_urls = [\n        'https://productlibrary.brandbank.com/products/detail/949211',\n\n    ]\n\n    def parse(self, request):\n        return []\n$ scrapy crawl test1c\n2014-12-16 11:18:18-0200 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2014-12-16 11:18:18-0200 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-12-16 11:18:18-0200 [scrapy] INFO: Enabled item pipelines: \n2014-12-16 11:18:18-0200 [test1c] INFO: Spider opened\n2014-12-16 11:18:18-0200 [test1c] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2014-12-16 11:18:18-0200 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2014-12-16 11:18:18-0200 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n2014-12-16 11:18:18-0200 [scrapy] INFO: Scrapy 0.25.1 started (bot: hangtest)\n2014-12-16 11:18:18-0200 [scrapy] INFO: Scrapy 0.25.1 started (bot: hangtest)\n2014-12-16 11:18:18-0200 [scrapy] INFO: Optional features available: ssl, http11\n2014-12-16 11:18:18-0200 [scrapy] INFO: Optional features available: ssl, http11\n2014-12-16 11:18:18-0200 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'hangtest.spiders', 'SPIDER_MODULES': ['hangtest.spiders'], 'BOT_NAME': 'hangtest'}\n2014-12-16 11:18:18-0200 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'hangtest.spiders', 'SPIDER_MODULES': ['hangtest.spiders'], 'BOT_NAME': 'hangtest'}\n2014-12-16 11:18:19-0200 [test1c] DEBUG: Redirecting (302) to <GET https://secure.brandbank.com/users/issue.aspx?wa=wsignin1.0&wtrealm=https%3a%2f%2fproductlibrary.brandbank.com&wctx=https%3a%2f%2fproductlibrary.brandbank.com%2fproducts%2fdetail%2f949211> from <GET https://productlibrary.brandbank.com/products/detail/949211>\n2014-12-16 11:18:21-0200 [test1c] DEBUG: Redirecting (302) to <GET https://secure.brandbank.com/login.aspx?ReturnUrl=%2fusers%2fissue.aspx%3fwa%3dwsignin1.0%26wtrealm%3dhttps%253a%252f%252fproductlibrary.brandbank.com%26wctx%3dhttps%253a%252f%252fproductlibrary.brandbank.com%252fproducts%252fdetail%252f949211&wa=wsignin1.0&wtrealm=https%3a%2f%2fproductlibrary.brandbank.com&wctx=https%3a%2f%2fproductlibrary.brandbank.com%2fproducts%2fdetail%2f949211> from <GET https://secure.brandbank.com/users/issue.aspx?wa=wsignin1.0&wtrealm=https%3a%2f%2fproductlibrary.brandbank.com&wctx=https%3a%2f%2fproductlibrary.brandbank.com%2fproducts%2fdetail%2f949211>\n2014-12-16 11:18:21-0200 [test1c] DEBUG: Crawled (200) <GET https://secure.brandbank.com/login.aspx?ReturnUrl=%2fusers%2fissue.aspx%3fwa%3dwsignin1.0%26wtrealm%3dhttps%253a%252f%252fproductlibrary.brandbank.com%26wctx%3dhttps%253a%252f%252fproductlibrary.brandbank.com%252fproducts%252fdetail%252f949211&wa=wsignin1.0&wtrealm=https%3a%2f%2fproductlibrary.brandbank.com&wctx=https%3a%2f%2fproductlibrary.brandbank.com%2fproducts%2fdetail%2f949211> (referer: None)\n2014-12-16 11:18:21-0200 [test1c] INFO: Closing spider (finished)\n2014-12-16 11:18:21-0200 [test1c] INFO: Dumping Scrapy stats:\n    {'downloader/request_bytes': 1201,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'downloader/response_bytes': 6838,\n     'downloader/response_count': 3,\n     'downloader/response_status_count/200': 1,\n     'downloader/response_status_count/302': 2,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2014, 12, 16, 13, 18, 21, 833571),\n     'log_count/DEBUG': 5,\n     'log_count/INFO': 9,\n     'response_received_count': 1,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2014, 12, 16, 13, 18, 18, 551032)}\n2014-12-16 11:18:21-0200 [test1c] INFO: Spider closed (finished)\n^C^C\n$\ngit bisect between 0.25.0 and master branch:\nThere are only 'skip'ped commits left to test.\nThe first bad commit could be any of:\n39c6a80\nd7038b2\n3ae9714\n980e30a\na995727\nd402735\n870438e\neb0253e\n84fa004\nd0edad4\n89df18b\nWe cannot bisect more!", "issue_status": "Closed", "issue_reporting_time": "2014-12-16T13:17:23Z", "fixed_by": "#999", "pull_request_summary": "Patch hanging HTTPConnectionPool.closeCachedConnections call", "pull_request_description": "Member\ncurita commented on Dec 31, 2014\nThis PR fixes #985\nThe issue while closing the spider on that domain raised while closing the HTTP11DownloadHandler (it can be seen here). The server doesn't close the connection properly so the client waits for a confirmation that doesn't arrive. I've reported this on the Twisted issue tracker (#7738) since this a problem concerning their HTTPConnectionPool while cleaning persistent connections, but I've patched it externally by firing a deferred on a DelayedCalled.\nThis problem hasn't came up before the changes on the crawling API because the reactor was stopped along the download handlers on the engine_stop signal (CrawlerProcess@8fece4b and DownloadHandlers@8fece4b). Instead of that, now the reactor is stopped after the crawl deferreds has been fired (CrawlerProcess), which happens after each engine has stopped, so the HTTP11DownloadHandler.close isn't abruptly terminated.\nI chose a _disconnect_timeout of one second on a tradeoff between the previous instant termination and giving a little time to the connections to close in an orderly manner. It could be a new Scrapy setting, and that's why I set this variable on the class init, but I think that kind of parametrization is not needed right now.\nI struggled on mocking a server that mimics this behavior (Twisted doesn't provide a way to do it as they manage the sockets internally, and I'm still not sure how to do it otherwise), so that's why I'm submitting the PR as it is and later I'll try to add a proper test.", "pull_request_status": "Merged", "issue_fixed_time": "2014-12-31T03:52:44Z", "files_changed": [["20", "scrapy/core/downloader/handlers/http11.py"]]}, "1316": {"issue_url": "https://github.com/scrapy/scrapy/issues/981", "issue_id": "#981", "issue_summary": "fetch errors on some https sites", "issue_description": "toolking commented on Dec 12, 2014\n$ scrapy fetch 'https://flixbus.de'\n2014-12-12 10:01:50+0100 [scrapy] INFO: Scrapy 0.24.4 started (bot: scrapybot)\n2014-12-12 10:01:50+0100 [scrapy] INFO: Optional features available: ssl, http11\n2014-12-12 10:01:50+0100 [scrapy] INFO: Overridden settings: {}\n2014-12-12 10:01:50+0100 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole,  CloseSpider, WebService, CoreStats, SpiderState\n2014-12-12 10:01:50+0100 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2014-12-12 10:01:50+0100 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-12-12 10:01:50+0100 [scrapy] INFO: Enabled item pipelines: \n2014-12-12 10:01:50+0100 [default] INFO: Spider opened\n2014-12-12 10:01:50+0100 [default] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2014-12-12 10:01:50+0100 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2014-12-12 10:01:50+0100 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n2014-12-12 10:01:50+0100 [default] DEBUG: Retrying <GET https://flixbus.de> (failed 1 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2014-12-12 10:01:50+0100 [default] DEBUG: Retrying <GET https://flixbus.de> (failed 2 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2014-12-12 10:01:50+0100 [default] DEBUG: Gave up retrying <GET https://flixbus.de> (failed 3 times): [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2014-12-12 10:01:50+0100 [default] ERROR: Error downloading <GET https://flixbus.de>: [<twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>]\n2014-12-12 10:01:50+0100 [default] INFO: Closing spider (finished)\n2014-12-12 10:01:50+0100 [default] INFO: Dumping Scrapy stats:\n    {'downloader/exception_count': 3,\n     'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,\n     'downloader/request_bytes': 627,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2014, 12, 12, 9, 1, 50, 463722),\n     'log_count/DEBUG': 5,\n     'log_count/ERROR': 1,\n     'log_count/INFO': 7,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2014, 12, 12, 9, 1, 50, 288873)}\n2014-12-12 10:01:50+0100 [default] INFO: Spider closed (finished)\n$ pip freeze\nScrapy==0.24.4\nTwisted==14.0.2\nargparse==1.2.1\ncffi==0.8.6\ncharacteristic==14.2.0\ncryptography==0.6.1\ncssselect==0.9.1\ndistribute==0.7.3\nlxml==3.4.1\npyOpenSSL==0.14\npyasn1==0.1.7\npyasn1-modules==0.0.5\npycparser==2.10\nqueuelib==1.2.2\nservice-identity==14.0.0\nsix==1.8.0\nw3lib==1.10.0\nwsgiref==0.1.2\nzope.interface==4.1.1\nSame error on scrapinghub ;)", "issue_status": "Closed", "issue_reporting_time": "2014-12-12T09:01:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1317": {"issue_url": "https://github.com/scrapy/scrapy/issues/977", "issue_id": "#977", "issue_summary": "create ClientCreator inside scrapy spider", "issue_description": "ntolstikov commented on Dec 11, 2014\nHi! I want to solve this issue: connect to MQ inside a spider.\nIm trying to call ClientCreator, but i dont know where to get \"reactor\" variable inside a CrawlSpider...\nd = ClientCreator(reactor, AMQClient, delegate=delegate, vhost='/', spec=spec).connectTCP('localhost', 5672)\nd.addCallback(self.gotConnection, 'guest', 'guest')\nWhy i am connecting to MQ inside a spider and dont use Pipeline? I have a loop running spider that reads some jobs from MQ. Pipelines provides a storage, but i need to read and proccess MQ jobs. I cant read jobs and that run spider, cause i need to proccess mq jobs while spider is already logged in on website.\nDoes somebody have any suggessions how to connect to MQ inside a spider not to store parsed items but to read jobs?", "issue_status": "Closed", "issue_reporting_time": "2014-12-11T09:59:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1318": {"issue_url": "https://github.com/scrapy/scrapy/issues/975", "issue_id": "#975", "issue_summary": "Documentation for Contributing to Scrapy > Tests not up-to-date", "issue_description": "Contributor\naufziehvogel commented on Dec 11, 2014\nIn the documentation how to contribute to scrapy (Section testing), it is said that running tests can be done by bin/runtests.sh, however this file does not exist anymore.\nNeither can I find a file bin/trial as mentioned on the trial website (to which there is also a link).\nAlso the tests are not in scrapy.tests (scrapy/tests) anymore, but in tests.\nThis section should be updated to newest testing environment.", "issue_status": "Closed", "issue_reporting_time": "2014-12-10T21:23:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1319": {"issue_url": "https://github.com/scrapy/scrapy/issues/972", "issue_id": "#972", "issue_summary": "FTP password with not-so weird caracters not working", "issue_description": "jairot commented on Dec 7, 2014\nI have an ftp sever with a complex password, in the FEED_URI something like this:\nFEED_URI = \"ftp://user:ma!?luis!112@ftp.example.com/data.json\"\nThe spider gets broken and never starts, the traceback is this:\nTraceback (most recent call last):\n  File \"env/bin/scrapy\", line 9, in <module>\n    load_entry_point('Scrapy==0.24.4', 'console_scripts', 'scrapy')()\n  File \"env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"env/local/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"env/local/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 60, in run\n    self.crawler_process.start()\n  File \"env/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 92, in start\n    if self.start_crawling():\n  File \"env/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 124, in start_crawling\n    return self._start_crawler() is not None\n  File \"env/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 139, in _start_crawler\n    crawler.configure()\n  File \"env/local/lib/python2.7/site-packages/scrapy/crawler.py\", line 46, in configure\n    self.extensions = ExtensionManager.from_crawler(self)\n  File \"env/local/lib/python2.7/site-packages/scrapy/middleware.py\", line 50, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"env/local/lib/python2.7/site-packages/scrapy/middleware.py\", line 31, in from_settings\n    mw = mwcls.from_crawler(crawler)\n  File \"env/local/lib/python2.7/site-packages/scrapy/contrib/feedexport.py\", line 162, in from_crawler\n    o = cls(crawler.settings)\n  File \"env/local/lib/python2.7/site-packages/scrapy/contrib/feedexport.py\", line 144, in __init__\n    if not self._storage_supported(self.urifmt):\n  File \"env/local/lib/python2.7/site-packages/scrapy/contrib/feedexport.py\", line 214, in _storage_supported\n    self._get_storage(uri)\n  File \"env/local/lib/python2.7/site-packages/scrapy/contrib/feedexport.py\", line 225, in _get_storage\n    return self.storages[urlparse(uri).scheme](uri)\n  File \"env/local/lib/python2.7/site-packages/scrapy/contrib/feedexport.py\", line 110, in __init__\n    self.port = int(u.port or '21')\n  File \"/usr/lib/python2.7/urlparse.py\", line 110, in port\n    port = int(port, 10)\nValueError: invalid literal for int() with base 10: '22!'", "issue_status": "Closed", "issue_reporting_time": "2014-12-07T14:09:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1320": {"issue_url": "https://github.com/scrapy/scrapy/issues/971", "issue_id": "#971", "issue_summary": "Cannot install scrapy", "issue_description": "ludbek commented on Dec 7, 2014\nI get following error while installing Scrapy in virtualenv.\ni686-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security build/temp.linux-i686-2.7/src/lxml/lxml.etree.o -lxslt -lexslt -lxml2 -lz -lm -o build/lib.linux-i686-2.7/lxml/etree.so\n/usr/bin/ld: cannot find -lz\ncollect2: error: ld returned 1 exit status\nerror: command 'i686-linux-gnu-gcc' failed with exit status 1\nI have met all the dependencies as per documentation.", "issue_status": "Closed", "issue_reporting_time": "2014-12-07T11:44:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1321": {"issue_url": "https://github.com/scrapy/scrapy/issues/968", "issue_id": "#968", "issue_summary": "Add command for Items generation", "issue_description": "codeadict commented on Dec 6, 2014\nAdding items to scrapy can be a tedious work when there are lot of fields to create and at the and all items are same type scrapy.Field(). I think creating a command like Ruby on Rails have, like:\nrails generate model ad name:string description:text price:decimal seller_id:integer email:string img_url:string\nIn our case it can be like this:\nscrapy genitem Product name url price brand image\nThis will help a lot to focus on writing spiders itself and simplify the task of defining data items.\nWhat do you think?? @pablohoffman", "issue_status": "Closed", "issue_reporting_time": "2014-12-06T17:36:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1322": {"issue_url": "https://github.com/scrapy/scrapy/issues/964", "issue_id": "#964", "issue_summary": "loader.get_output_value returning None for a key that can be confirmed via loader.load_item", "issue_description": "lekhakpadmanabh commented on Dec 3, 2014\nimport scrapy\n\n\nclass MyItem(scrapy.Item):\n    item1 = scrapy.Field()\n    item_url = scrapy.Field()\n    item_address = scrapy.Field()\n    item_list = scrapy.Field()\n\n\nclass MyLoader(scrapy.contrib.loader.ItemLoader):\n    default_output_processor = TakeFirst()\n    default_input_processor = MapCompose(unicode.strip)\n    item_list_in = Identity()\n    item_list_out = Identity()\n\n\nclass MySpider(scrapy.Spider):\n\n    name = 'myspider'\n    start_urls = ['some-url']\n\n    def parse(self, response):\n        uls = response.xpath('//div[@id=\"someID\"]//ul')\n        for ul in uls:\n            url = ul.xpath('.//li[1]/a/@href').extract()[0]\n            loader = MyLoader(MyItem(), ul)\n            loader.add_xpath('item1', './/li[1]/a/text()')\n            loader.add_value('item_url', url)\n            request = scrapy.Request(url, callback=self.parse_more)\n            request.meta['item'] = loader.load_item()\n            yield request\n\n    def parse_more(self, response):\n        loader = MyLoader(response.meta['item'], response=response)\n        item_list = response.css('.nav-buttons.someclass').xpath('.//a/img/@alt').extract()\n        loader.add_xpath('item_address', '//address//div[@class=\"address-info\"]/text()')\n        loader.add_value('item_list', item_list)\n        #import pdb; pdb.set_trace()\n        if loader.get_output_value('item_address') == \"some address\":\n             loader.add_value('address_code', 'FR')\n        yield loader.load_item()\nIn pdb, for example, if we inspect a loader instance\n(pdb) loader.load_item()\n{'item_address': 'someval', ....}\n(pdb) type(loader.get_output_value('item_address'))\n<type: Nonetype>\nAs a result, the if-block is never executing.", "issue_status": "Closed", "issue_reporting_time": "2014-12-02T22:19:12Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1323": {"issue_url": "https://github.com/scrapy/scrapy/issues/962", "issue_id": "#962", "issue_summary": "Scrapy just stops unexpectedly with no error", "issue_description": "pigerla commented on Dec 1, 2014\nevery time Scrapy just stop after running a few minutes and I still cant figure out why ? spider added 69 . There are indeed lots of ParseException...is there too much spiders or ParseException ? Any help will be appreciated.\nstop like this :\n2014-12-01 12:34:06+0800 [scrapy] INFO: info : {'platform_name': '\\xe7\\xa7\\xaf\\xe6\\x9c\\xa8\\xe7\\x9b\\x92\\xe5\\xad\\x90', 'platform': 'jimubox', 'loanId': '13774', 'create_time': 1417408354133} exception : ParseException('type', 'list index out of range')\n2014-12-01 12:34:06+0800 [scrapy] INFO: info : {'platform_name': '\\xe7\\xa7\\xaf\\xe6\\x9c\\xa8\\xe7\\x9b\\x92\\xe5\\xad\\x90', 'platform': 'jimubox', 'loanId': '13773', 'create_time': 1417408354133} exception : ParseException('type', 'list index out of range')\n2014-12-01 12:34:07+0800 [eloancn] INFO: Crawled 19 pages (at 19 pages/min), scraped 0 items (at 0 items/min)\n2014-12-01 12:34:09+0800 [qmdai] INFO: Crawled 10 pages (at 10 pages/min), scraped 0 items (at 0 items/min)\nscrapy version : 0.24.4", "issue_status": "Closed", "issue_reporting_time": "2014-12-01T06:35:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1324": {"issue_url": "https://github.com/scrapy/scrapy/issues/959", "issue_id": "#959", "issue_summary": "how to get the item fields when rewriting \u2018file_path\u2019 function", "issue_description": "zhizunbao84 commented on Nov 25, 2014\ni want to custom image path when downloading images with\nreturn %s/%s' % (item['title'], image_guid)", "issue_status": "Closed", "issue_reporting_time": "2014-11-25T12:27:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1325": {"issue_url": "https://github.com/scrapy/scrapy/issues/958", "issue_id": "#958", "issue_summary": "ImportError: Error loading object 'scrapy.contrib.memusage.MemoryUsage': No module named mail.smtp", "issue_description": "addbook commented on Nov 25, 2014\n2014-11-25 09:19:19+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'prj.spiders', 'SPIDER_MODULES': ['prj.spiders'], 'RETRY_HTTP_CODES': [500, 503, 504, 400, 403, 404, 408], 'BOT_NAME': 'prj', 'DOWNLOAD_TIMEOUT': 1000, 'COOKIES_ENABLED': False, 'USER_AGENT': 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.43 Safari/537.31', 'DOWNLOAD_DELAY': 10}\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 4, in\nexecute()\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 143, in execute\n_run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 89, in _run_print_help\nfunc(_a, *_kw)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 150, in _run_command\ncmd.run(args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py\", line 60, in run\nself.crawler_process.start()\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 92, in start\nif self.start_crawling():\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 124, in start_crawling\nreturn self._start_crawler() is not None\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 139, in _start_crawler\ncrawler.configure()\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py\", line 46, in configure\nself.extensions = ExtensionManager.from_crawler(self)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 50, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/middleware.py\", line 29, in from_settings\nmwcls = load_object(clspath)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/misc.py\", line 42, in load_object\nraise ImportError(\"Error loading object '%s': %s\" % (path, e))\nImportError: Error loading object 'scrapy.contrib.memusage.MemoryUsage': No module named mail.smtp", "issue_status": "Closed", "issue_reporting_time": "2014-11-25T01:22:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1326": {"issue_url": "https://github.com/scrapy/scrapy/issues/957", "issue_id": "#957", "issue_summary": "'request_scheduled' signal execution behavior", "issue_description": "jacob1237 commented on Nov 24, 2014\nGreetings!\nDuring the making of my current scrapy project, I've found some interesting 'request_scheduled' signal behavior: it fires always, even if a request has been rejected by dupefilter or something else.\nSo for example, I can't count the number of scheduled requests by value increment, but only by crawler stats values.\nI think there is a need to split this signal in two pieces: 'request_scheduled' and 'request_dropped', like in pipeline items.\nThank you!", "issue_status": "Closed", "issue_reporting_time": "2014-11-24T08:57:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1327": {"issue_url": "https://github.com/scrapy/scrapy/issues/953", "issue_id": "#953", "issue_summary": "Missing scheme in request url: h", "issue_description": "wangeguo commented on Nov 19, 2014\nI wrote this code:\nimport scrapy\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors import LinkExtractor\nfrom bot.items import Product\n\nclass RevolveclothingSpider(CrawlSpider):\n    name = \"revolveclothing\"\n    allowed_domains = [\"www.revolveclothing.com\"]\n    start_urls = ('http://www.revolveclothing.com/daydreamer-rolling-stones-flag-tongue-sweatshirt-in-black/dp/DDRE-WO8/')\n    rules = (Rule(LinkExtractor(allow=('dp/[a-zA-Z0-9\\-]+/')), callback='parse_item'),)\n\n    def parse_item(self, response):\n        product = Product()\n\n        brand = response.xpath('//div[@class=\"designer_brand\"]/h2/a/text()').extract()\n        title = response.xpath('//div[@class=\"product_name\"]/h1/text()').extract()\n        product['name'] = '%s %s' % (brand[0], title[0].strip())\n\n        product['price'] = response.xpath('//div[@class=\"price_box\"]/span[@class=\"price\"]/text()').extract()[0]\n        product['link'] = response.url\n        #product['description'] = response.xpath('').extract()[0].strip()\n        product['image_urls'] = response.xpath('//div[@class=\"pdp_zoomed_product\"]/div/img/@src').extract()\n\n        yield product\nand here is result:\nscrapy crawl revolveclothing\n2014-11-19 21:54:17+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: bot)\n2014-11-19 21:54:17+0800 [scrapy] INFO: Optional features available: ssl, http11, django\n2014-11-19 21:54:17+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'bot.spiders', 'SPIDER_MODULES': ['bot.spiders'], 'BOT_NAME': 'bot'}\n2014-11-19 21:54:17+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2014-11-19 21:54:17+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2014-11-19 21:54:17+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-11-19 21:54:17+0800 [scrapy] INFO: Enabled item pipelines: MyImagesPipeline\n2014-11-19 21:54:17+0800 [revolveclothing] INFO: Spider opened\n2014-11-19 21:54:17+0800 [revolveclothing] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2014-11-19 21:54:17+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2014-11-19 21:54:17+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n2014-11-19 21:54:17+0800 [revolveclothing] ERROR: Obtaining request from start requests\n    Traceback (most recent call last):\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/Library/Python/2.7/site-packages/scrapy/utils/reactor.py\", line 41, in __call__\n        return self._func(*self._a, **self._kw)\n    --- <exception caught here> ---\n      File \"/Library/Python/2.7/site-packages/scrapy/core/engine.py\", line 112, in _next_request\n        request = next(slot.start_requests)\n      File \"/Library/Python/2.7/site-packages/scrapy/spider.py\", line 50, in start_requests\n        yield self.make_requests_from_url(url)\n      File \"/Library/Python/2.7/site-packages/scrapy/spider.py\", line 53, in make_requests_from_url\n        return Request(url, dont_filter=True)\n      File \"/Library/Python/2.7/site-packages/scrapy/http/request/__init__.py\", line 26, in __init__\n        self._set_url(url)\n      File \"/Library/Python/2.7/site-packages/scrapy/http/request/__init__.py\", line 61, in _set_url\n        raise ValueError('Missing scheme in request url: %s' % self._url)\n    exceptions.ValueError: Missing scheme in request url: h\n\n2014-11-19 21:54:17+0800 [revolveclothing] INFO: Closing spider (finished)\n2014-11-19 21:54:17+0800 [revolveclothing] INFO: Dumping Scrapy stats:\n    {'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2014, 11, 19, 13, 54, 17, 958055),\n     'log_count/DEBUG': 2,\n     'log_count/ERROR': 1,\n     'log_count/INFO': 7,\n     'start_time': datetime.datetime(2014, 11, 19, 13, 54, 17, 953648)}\n2014-11-19 21:54:17+0800 [revolveclothing] INFO: Spider closed (finished)\nENV\nscrapy version -v\nScrapy  : 0.24.4\nlxml    : 3.4.0.0\nlibxml2 : 2.9.0\nTwisted : 12.2.0\nPython  : 2.7.5 (default, Mar  9 2014, 22:15:05) - [GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)]\nPlatform: Darwin-13.4.0-x86_64-i386-64bit\nWho can tell me how to modify the problem?", "issue_status": "Closed", "issue_reporting_time": "2014-11-19T14:07:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1328": {"issue_url": "https://github.com/scrapy/scrapy/issues/951", "issue_id": "#951", "issue_summary": "Extraction of gzipped sitemap fails in Scrapy 0.24.4", "issue_description": "mercutio79 commented on Nov 15, 2014\nretrieving a gzipped sitemap xml (tested on amazon.de) fails.\nReproduce with :\nmodify /utils/gz.py gunzip method to write the incoming data to a file.\ngunzip the file on the command line.\nthe unzipped file contains garbled content\ngunzip that file with garbled content a second time and get the correct content\n-> I suspect that the content coming from the target server is already gzip compressed and scrapy has a bug that causes the gzip decompression to not work properly, resulting in a double compressed file arriving at the /utils/gz.py gunzip method", "issue_status": "Closed", "issue_reporting_time": "2014-11-14T18:48:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1329": {"issue_url": "https://github.com/scrapy/scrapy/issues/950", "issue_id": "#950", "issue_summary": "Scrapy built-in email system doesn't work for me", "issue_description": "Contributor\nLazar-T commented on Nov 14, 2014\nSo i asked this question on Stackoverflow and IRC channel but no luck, anyway when i try to send email when spider is done scraping i got it to work when using something custom like this: https://gist.github.com/Lazar-T/73694c9bfb609ee85a3a, but with MailSender i can't get it to work somehow, this is pseudo code that i got so far in spider: https://gist.github.com/Lazar-T/9b3fa684a13fb175e125", "issue_status": "Closed", "issue_reporting_time": "2014-11-13T22:11:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1330": {"issue_url": "https://github.com/scrapy/scrapy/issues/948", "issue_id": "#948", "issue_summary": "Concurrent requests return wrong response", "issue_description": "archroad commented on Nov 13, 2014\nHi Scrapy experts,\nI am not sure if this is an existing issue -- I searched in Google, github and didn't find anything similar.\nHere is my question. I wrote a crawler to crawl products on mango.com. I set CONCURRENT_REQUESTS = 16 in settings.py. However, I found sometimes the response passed to my parse function is ANOTHER page. I used inspect_response(response, self) to stop in my parse function to debug. I found response.url is correct. But if I type view(response), it is a completely different page from response.url. Later if I set CONCURRENT_REQUESTS = 1 and reran the crawler, the problem seems gone.\nI pasted my debug results in this thread. The response.url is http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123.\nHowever, I also printed out the response.boy (sorry for the mass page content). You will find it's completely different from the actual url content.\nDid I miss anything here?\nThanks,\nJun\nDebug results ======================================\n2014-11-13 00:06:06-0800 [spider_mango] DEBUG: File (uptodate): Downloaded image from <GET http://st.mngbcn.com/rcs/pics/static/T3/fotos/S6/31067584_02.jpg> referred in\n2014-11-13 00:06:06-0800 [spider_mango] DEBUG: File (uptodate): Downloaded image from <GET http://st.mngbcn.com/rcs/pics/static/T3/fotos/S6/31067584_02_R.jpg> referred in\n2014-11-13 00:06:06-0800 [spider_mango] DEBUG: File (uptodate): Downloaded image from <GET http://st.mngbcn.com/rcs/pics/static/T3/fotos/S6/31067584_02_D1.jpg> referred in\n2014-11-13 00:06:06-0800 [spider_mango] DEBUG: File (uptodate): Downloaded image from <GET http://st.mngbcn.com/rcs/pics/static/T3/fotos/outfit/S6/31067584_02-33035589_02-33013683_02-33000307_02-33090327_93.jpg> referred in\n2014-11-13 00:06:06-0800 [spider_mango] DEBUG: Now parsing http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123\n[s] Available Scrapy objects:\n[s] crawler <scrapy.crawler.Crawler object at 0x10a7ea9d0>\n[s] item {}\n[s] request <GET http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123>\n[s] response <200 http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123>\n[s] settings <scrapy.settings.Settings object at 0x109c33a50>\n[s] spider <SpiderMango 'spider_mango' at 0x10a7f82d0>\n[s] Useful shortcuts:\n[s] shelp() Shell help (print this help)\n[s] view(response) View response in a browser\nIn [1]: response.xpath('//div[@itemprop=\"name\"]/h1/text()').extract())\nFile \"\", line 1\nresponse.xpath('//div[@itemprop=\"name\"]/h1/text()').extract())\n^\nSyntaxError: invalid syntax\nIn [2]: response.xpath('//div[@itemprop=\"name\"]/h1/text()').extract()\nOut[2]: [u'\\n\\t\\t\\t\\t\\t\\t\\t\\tRibbed wool beanie\\n\\t\\t\\t\\t\\t\\t\\t']\nIn [3]: response.xpath('//div[@itemprop=\"name\"]/h1/text()').extract()\nOut[3]: [u'\\n\\t\\t\\t\\t\\t\\t\\t\\tRibbed wool beanie\\n\\t\\t\\t\\t\\t\\t\\t']\nIn [4]: view(response)\nOut[4]: True\nIn [5]: response.url\nOut[5]: 'http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123'\nIn [6]: view(response)\nOut[6]: True\nIn [7]: response.xpath('//span[@itemprop=\"description\"]/span/text()').extract()\nOut[7]: [u'Ribbed wool beanie with rolled-up edge. ']\nIn [8]: cat = crawl_util.get_substring(response.url, '/US/p0/', '/?id=', 0)\nNameError Traceback (most recent call last)\nin ()\n----> 1 cat = crawl_util.get_substring(response.url, '/US/p0/', '/?id=', 0)\nNameError: name 'crawl_util' is not defined\nIn [9]: response\nOut[9]: <200 http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123>\nIn [10]: view(response)\nOut[10]: True\nIn [11]: response.request\nOut[11]: <GET http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123>\nIn [12]: response.body\nOut[12]: '\\n\\n\\t\\t\\t<title>Wool fedora hat - Men | MANGO</title>\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n \\n <script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/vendor/modernizr-2.6.2.min.js?v=4_18_19&ts=267\"></script>\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t<script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/pluginsnew2.js?v=4_18_19&ts=267\"></script>\\n\\t\\t\\t<script type=\"text/javascript\" src=\"/javax.faces.resource/jsf.js.faces?ln=javax.faces&v=4_18_19&ts=267\"></script><script type=\"text/javascript\">\\n\\ttry{ document.domain='mango.com'; self.name='mainPage'; } catch(e) {} try{\\n (function(){\\n var opt = document.createElement('script');\\n opt.type = 'text/javascript';\\n opt.async = true;\\n opt.src = ('https:' == document.location.protocol ? 'https://': 'http://') + 'cdn.optimizely.com/js/1748450118.js'; var scrpt = document.getElementsByTagName('head')[0];\\n scrpt.appendChild(opt); })();\\n }catch(err){}\\n ; \\n this.setTracker3 = function() {\\n if(arguments.length>2){\\n ga(arguments[0], arguments[1], arguments[2], arguments[3], arguments[4]);\\n }else{\\n _gaq.push(['_setAccount','UA-855910-3'],['trackEvent',arguments[0],'Click',arguments[1]]);\\n }\\n };\\n ; \\n trackingEventAnalyticsUniversal = function() {\\n ga('send','event',arguments[0],arguments[1],arguments[2]);\\n };\\n \\n analyticsMenu = function() {\\n ga('send','event','menu',arguments[0],arguments[1]);\\n };\\n ; \\n (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\\n (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\\n m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\\n })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); \\n ga('create', 'UA-855910-26', 'mango.com'); \\n ga('set', { \\n'dimension3': 'mango',\\n'dimension4': 'sombreros y gorros',\\n'dimension5': 'h-robb-e c:sombrero fedora lana',\\n'dimension6': '33053675'\\n}); \\n window.optimizely = window.optimizely || [];\\n window.optimizely.push(\"activateUniversalAnalytics\");\\n ga('send', 'pageview');\\n ; \\n var img;\\n var dateTime = new Date().getTime();\\n if ('006'=='400'){\\n img = \"googleads.g.doubleclick.net/pagead/viewthroughconversion/983634403/?value=0&label=IQJUCLWi7wQQ46OE1QM&guid=ON&script=0&data=ecomm_prodid%3D33053675400%3Becomm_pagetype%3Dproduct%3Becomm_totalvalue%3D32.04&random=\" + dateTime;\\n }else if ('075'=='400'){\\n img = \"googleads.g.doubleclick.net/pagead/viewthroughconversion/977410496/?value=0&label=9XrICMit2AUQwLOI0gM&guid=ON&script=0\";\\n }else if ('720'=='400'){\\n img = \"googleads.g.doubleclick.net/pagead/viewthroughconversion/1008098799/?value=0&guid=ON&script=0\";\\n }else if ('001'=='400'){\\n img = \"googleads.g.doubleclick.net/pagead/viewthroughconversion/982291508/?value=0&label=rzYYCPTBklIQtKiy1AM&guid=ON&script=0&data=ecomm_prodid%3D33053675400%3Becomm_pagetype%3Dproduct%3Becomm_totalvalue%3D32.04&random=\" + dateTime;\\n }else if ('003'=='400'){\\n img = \"googleads.g.doubleclick.net/pagead/viewthroughconversion/978827098/?value=0&label=Id9OCK7B0wcQ2u7e0gM&guid=ON&script=0&data=ecomm_prodid%3D33053675400%3Becomm_pagetype%3Dproduct%3Becomm_totalvalue%3D32.04&random=\" + dateTime;\\n }else if ('400'=='400'){\\n img = \"googleads.g.doubleclick.net/pagead/viewthroughconversion/1016140217/?value=0&label=QUYVCKfHmAUQuaPE5AM&guid=ON&script=0&data=ecomm_prodid%3D33053675400%3Becomm_pagetype%3Dproduct%3Becomm_totalvalue%3D32.04&random=\" + dateTime;\\n } var gdr_url = ('https:' == document.location.protocol ? 'https://': 'http://')+img;\\n document.write(\"<div style='display:none'> <img height='1' width='1' alt='' style='border-style:none;' src=\" + gdr_url + \" />\");\\n \\n ; ;\\n\\t</script>\\n\\t\\t\\t\n\\n\\t\\n\\t\n\\n\\t\\t\n\\n\\t\\t\\t\nFree delivery from $50\\n\\t\\t\\t\n\\n\\t\\t\n\\t\\t\\n\\t\n\\n\\t\\n\\t\n\\n\\t\\t\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\n\\t\n\\n\\t\\t\\n\\t\\ts\\n\\t\\ts\\n\\t\\t\n\\n\\t\\tSearch\\n\\t\n\\n\\t\\n\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\\t\n\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\nRegister\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\n|\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\nStart session\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\n|\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\nOrders\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n|\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\nHelp\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t|\\n\\t\\t\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\nb\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(0\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\nYour shopping bag is empty\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\nx\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\nYOUR SHOPPING BAG HAS BEEN UPDATED\n\\n\\t\\t\\t\\t\nThe items you added in your last session have been included.\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\n\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\\t\\tMANGO\\n\\t\\t\\t\\n\\t\\t\n\\n\\t\n\\n\\t\\n\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tWomen\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tMen\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tVioleta\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\n\\n\\t\n\\n\\t\\t\\n\\t\n\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\tE\\n\\t\\t\\t\\t\\t\\n\\t\\n\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\\t\\tMANGO\\n\\t\\t\\t\\n\\t\\t\n\\n\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t E\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\tS\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tS\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tNEW\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpromotion\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOutlet\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tCASUAL\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tSUITS\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tBLACKSMITH\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tSPORT\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfeatured\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nKnit o'clock\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nLightweight down-feather jackets\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tCLOTHING\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nCoats\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nJackets\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nBlazers\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nLeather \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nCardigans and sweaters\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nShirt\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nT-shirts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nPolo shirts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nTrousers\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nJeans\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nBermudes\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nPyjamas\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nUnderwear and socks\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAccessories\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nShoes\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nBags\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nBelts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nLeather goods\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nBracelets\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nGloves, scarves and foulards\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nTies\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nSunglasses\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nHats and caps\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nOther accessories\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tCatalogue\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nWinter 2014\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n2014 Autumn\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tLookbooks\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n2014 November\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n2014 August\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tm\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tTHE POST\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t Women\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t Men\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t Violeta\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t-\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nRegister\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\to\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nStart session\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t7\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\nOrders\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tn\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\nHelp\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tu\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\nUSA \\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n0\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\n\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n \\t \\n \\t\\n \\t\n\\n \\t\\t\\n \\t\\t\n\\n\\t\\t \\t\\t\\tx\\n \\t\\t\n\\n \\t\\t\\n \\t\\t\n\\n\\t\n\\n\\t\\t\n\\n\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\tThe items stated are not available at this moment.\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\tWe will automatically eliminate these items from your order.\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\tYou can continue shopping process or proceed to checkout without these items.\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\nContinue shopping\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\n\\n \\t\\t\n\\n \\t\n\\n\\t\\t\\n\\n\n\\n\\t\n\\n\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\tItem AddedCHECKOUT \\n\\t\\t\n\\n\\t\n\\n\\t\n\\n\\t\\t \\n\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\n\\t\\t\n\\nAccessories > Ribbed wool beanie\\n\\t\\t\n\\n\\t\\t\nPrevious 6 / 7 Next\\n\\t\\t\n\\n\\t\n\\n\\t\\n\\t\n\\n\\t\\t\n\\n\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tRibbed wool beanie\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\tCap\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tREF. 33085700 - For-e c \\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t$39.99\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\nk6 de 7 l\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\nColor:\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tGreen\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n \\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\tChoose your size\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tSizes guide \\n\\t\\t\\t\\t\\t\\t\\tSizes guide \\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\nAdd to bag\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t^Choose the colour and size before adding it to the shopping bag.\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t^Choose your size before adding the item to your Wishlist.\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t \\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\nSearch in storeAdd to wishlist\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t^Click on `Add to Wishlist\\xc2\\xbf and we will notify you by e-mail if it becomes available again.\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t \\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t'\\n\\t\\t\\t\\t\\t\\t\\t\\tF\\n\\t\\t\\t\\t\\t\\t\\t\\t!\\n\\t\\t\\t\\t\\t\\t\\t\\tG\\n\\t\\t\\t\\t\\t\\t\\t\\tZ\\n\\t\\t\\t\\t\\t\\t\\t\\tR\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t<script type=\"text/javascript\"></script>\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ta\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tDescription\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\n\\t\\t\n\\n\\t\\t\\t\\t\\tRibbed wool beanie with rolled-up edge. \\t\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ta\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMaterial and washing instructions\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\n\\t\n\\n\\t\\t\nComposition: 100% wool \\n\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\nHAND WASHING MAX 30\\xc2\\xb0C / 85\\xc2\\xbaF\n\\n\\t\\t\\t\\t\\t\nDO NOT BLEACH\n\\n\\t\\t\\t\\t\\t\nDO NOT IRON\n\\n\\t\\t\\t\\t\\t\nDRY CLEANING PERCHLOROETHYLENE\n\\n\\t\\t\\t\\t\\t\nDO NOT TUMBLE DRY\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\n\\n\\n\\t\\t\\n\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\n\\n\\n\\n\\n\\t\\t\n\\n\\t\\t\\n\\t\\t\n\\t\\t\\n\\t\\t\\t\n\\n \\t\n\\n \\t\\t\\n \\t\n\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t \\t\n\\n\\t\\t\\t \\t\\n\\t\\t\\t \\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\n\\n\\t\n\\n\\n\\t\n\\n\n\\n\\t\n\\n\n\\n\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\tp\\n\\t\\t\\t\\t\\t\\tTHE LAST THING YOU VIEWED...\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\n\\n\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\n\\n\\t\\t\n\\n \\t \\n \\t\\n \\t\n\\n \\t\\t\\n \\t\\t\n\\n\\t\\t \\t\\t\\tx\\n \\t\\t\n\\n \\t\\t\\n \\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\xc2\\xbf\\n\\t\\t\\t\\t\nWe remind you that the limit on purchases is a maximum of $2,000 and/or 40 items, as indicated in the purchasing conditions.\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\nAccept\\n\\t\\t\\t\n\\n\\t\\t\n\\n \\t\\t\n\\n \\t\n\\n\\t\\t\n\\n\\t\\n\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t Joint purchases of garments from the current S/S collection with advance purchase items from the A/W season cannot be made.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\nIf you add this item to your shopping basket, the items currently in the shopping basket will automatically be deleted.\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\nWhat do you want to do?\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\nAdd to bag\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\nCancel\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tEXCLUSIVE!\\n\\t\\t\\t\\t\\t\\t\\t\nGet hold of these S/S'12 accessories before anyone else with 15% off.\\n\\t\\t\\t\\t\\t\\t\\t\nPurchase from 7 to 25 December and they will be delivered to you from 1 January 2012.\\n\\t\\t\\t\\t\\t\\t\\t\n- Promotion valid on a selection of 36 items. Cannot be used in conjunction with other offers or promotions.\\n\\t\\t\\t\\t\\t\\t\\t\n- Joint purchases of garments from the current A/W '11 collection with advance purchase items cannot be made.\\n\\t\\t\\t\\t\\t\\t\\t\n- Orders of advance purchase items will be dispatched from 1 January 2012.\\n\\t\\t\\t\\t\\t\\t\\t\n- Identical exchange and/or return policy for these articles.\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tEXCLUSIVE!\\n\\t\\t\\t\\t\\t\\t\\t\nMANGO Online is offering you the exclusive opportunity to be the first to purchase selected garments from the fashion show for our new Autumn/Winter 2011 collection. \\n\\t\\t\\t\\t\\t\\t\\t\nBuy before they go on sale in stores and you end up without your ideal garment. Only 100 units of each model are available.\\n\\t\\t\\t\\t\\t\\t\\t\nPurchases shall be done from 18 to 31 May and will be delivered to you from July 2011.\\n\\t\\t\\t\\t\\t\\t\\t\n- Joint purchases of garments from the current S/S '11 collection with pre-orders cannot be made.\\n\\t\\t\\t\\t\\t\\t\\t\n- Pre-orders will be dispatched from July 2011.\\n\\t\\t\\t\\t\\t\\t\\t\n- The exchange and/or return policy of these items is identical to the existing online policy.\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n \\t \\n \\t\\n \\t\n\\n \\t\\t\\n \\t\\t\n\\n\\t \\t\\t\\t\n\\n\\t \\t\\t\\t\n\\n \\t\\t\n\\n \\t\\t\\n \\t\\t\n\\n\\t\\t\nTo create your WishList, you must be registered.\\n\\t\\t\\t\nIf you are already registered, log in.\\n\\t\\t\n\\n\\t\\t\nStart session\nRegister\nClose\n\\n\\t\\t\n\\n \\t\\t\n\\n \\t\n\\n\\t\\t\n\\t\\t\\t\\n\\t\\t\n\\n\\t\\t\\tWe remind you that you can add up to 40 items to the Wish List.\\n\\t\\t\\t\nAccept\\n\\t\\t\\t\n\\n\\t\\t\n\\n \\t \\n \\t\\n \\t\n\\n \\t\\t\\n \\t\\t\n\\n\\t\\t \\t\\t\\tx\\n \\t\\t\n\\n \\t\\t\\n \\t\\t\n\\n\\n\\t\\t\\t\\t\nWishlist\n\\n\\t\\t\\t\\t\nSelect the Wishlist you wish to add your garment to:\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\nCreate a new WishList >>\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\nFirst NameCreate the WishList\\n\\t\\t\\t\\t\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n \\t\\t\n\\n \\t\n\\n\\t\\t\\n\\n\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\tExclusive news & updates\\n\\t\\t\\t\\tNewsletter\\n\\t\\t\\t\\t\\te\\n\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\tRECEIVE OUR NEWSLETTER!SUBSCRIBE\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t^\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\nFranchises\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\nStore Locator\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\nEngineering and Construction\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\nAbout MANGO\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\nWork with us\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\teGift card\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\nAffiliate\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tUSA \\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\tF\\n\\t\\t\\t\\t\\t\\t\\t!\\n\\t\\t\\t\\t\\t\\t\\tG\\n\\t\\t\\t\\t\\t\\t\\tq\\n\\t\\t\\t\\t\\t\\t\\tZ\\n\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\th\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\nORDERS BY PHONE: 1.866.6MNG.MNG (TOLL-FREE) FROM 6AM TO 8PM EST.\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\tF\\n\\t\\t\\t\\t\\t\\t\\t\\t!\\n\\t\\t\\t\\t\\t\\t\\t\\tG\\n\\t\\t\\t\\t\\t\\t\\t\\tq\\n\\t\\t\\t\\t\\t\\t\\t\\tZ\\n\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\th\\n\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\xc2\\xa9 2000-2014 MANGO-On Line, S.A.\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\xc2\\xa9 2014 Punto Fa, S.L All rights reserved\\n\\t\\t\\t\\t\\t\\t\\t\\xc2\\xa9 Mango-Online 2014Purchasing conditionsPrivacy\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\n\\n\\t\\t\n\\n\\t\\n\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tNEW\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tpromotion\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tOutlet\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tfeatured\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tPremium evening\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tRetro Sixties\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tOnline exclusive\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCardigans and sweaters\\n\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCLOTHING\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tDresses\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCoats\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJackets\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLeather\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCardigans and sweaters\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBlouses and shirts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tT-shirts and tops\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tTrousers\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJeans\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tSkirts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tShorts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJumpsuits\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tPremium\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tEssentials\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tAccessories\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tShoes\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBags\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJewellery\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLeather goods\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBelts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tHats and caps\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tFoulards and scarves\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tGloves\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tSunglasses\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tOther accessories\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCatalogue\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tWinter 2014\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tTouch 2014\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t2014 Autumn\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLookbooks\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tNovember\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tMango Premium\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tSeptember\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tTHE MAGAZINE\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tSOCIAL\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tTV\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tNEW\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tpromotion\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tOutlet\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCASUAL\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tSUITS\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBLACKSMITH\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tSPORT\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tfeatured\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tKnit o'clock\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLightweight down-feather jackets\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCLOTHING\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCoats\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJackets\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBlazers\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLeather \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCardigans and sweaters\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tShirt\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tT-shirts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tPolo shirts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tTrousers\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJeans\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBermudes\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tPyjamas\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tUnderwear and socks\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tAccessories\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tShoes\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBags\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBelts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLeather goods\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBracelets\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tGloves, scarves and foulards\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tTies\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tSunglasses\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tHats and caps\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tOther accessories\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCatalogue\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tWinter 2014\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t2014 Autumn\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLookbooks\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t2014 November\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t2014 August\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tTHE POST\\n\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\n\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tNEW\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tpromotion\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tfeatured\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tAutumn Weekend\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tWrap up in style\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCasual dresses\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tKnitwear collection\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCLOTHING\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tDresses\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCoats\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJackets\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCardigans and sweaters\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBlouses and shirts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tT-shirts and tops\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tTrousers\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJeans\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tSkirts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJumpsuits\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tAccessories\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tShoes\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBags\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tJewellery\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLeather goods\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBelts\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tHats and caps\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tFoulards and scarves\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tGloves\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tCatalogue\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t2014 Autumn\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tLookbooks\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tF/W 2014 Preview\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\tBROOKLYN GIRL\\n\\t\\t\\t\\t\\t\\t\\t\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\\t\n\\n\\t\\t\\t\\t\\t\n\\n\\t\\t\nAdd comment\n\\n\\t\\t\\n\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\nLeave your comment \\n\\t\\t\\t\\t\n<textarea id=\"SVPReview:FAddReview:textoReview\" name=\"SVPReview:FAddReview:textoReview\" class=\"txtInput7 inputReg1\" cols=\"58\" rows=\"6\"></textarea>\\n\\t\\t\\t\\t\n\\n\\t\\t\\t\\t\nLeave an anonymous comment \\n\\t\\t\\t\\t\nSend\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\n\\n\n\\n \\t\n\\n \\t\\t\n\\n\\n \\t\\t\\t\\t\\t\\t\\t\nX\nk\\n \\t\\t\\t\\t\\t\\t\\t\n\\n\\n \\t\\t\\t\\t\\t\\t\\t\nEnter your e-mail address and we will notify you if the item becomes available again.\\n \\t\\t\\t\\t\\t\\t\\t\n\\n\\n \\t\\t\\t\\t\\t\\t\\t\nE-mail\nReceive notification\n\\n\\t\n\\n\\t \\t\\t\\t\\t\\t\\t\\t\n\\n \\t\\t\\t\\t\\t\\t\\t\n\\n\\n\\n \\t\\t\\t\\t\\t\\t\\t\n\\n \\t\\t\\t\\t\\t\\t\\t\\t\nOr if you prefer, add this item to the Wishlist.\\n \\t\\t\\t\\t\\t\\t\\t\\t\n\\n \\t\\t\\t\\t\\t\\t\\t\\t\\n \\t\\t\\t\\t\\t\\t\\t\\tBy clicking on \"Receive notification\", you accept the confidentiality policy.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\n \\t\\t\\t\\t\\t\\t\\t\n\\n \\t\\t\\t\\t\n\\n \\t\n\\n\\t\\n\\n\\t\\t<script type=\"text/javascript\" src=\"/javax.faces.resource/panelBasic.js.faces?ln=panels&v=4_18_19&ts=267\"></script><script type=\"text/javascript\" src=\"/javax.faces.resource/events.js.faces?ln=common&v=4_18_19&ts=267\"></script><script type=\"text/javascript\" src=\"/javax.faces.resource/manager_1_2.js.faces?ln=ficha&v=4_18_19&ts=267\"></script><script type=\"text/javascript\" src=\"/javax.faces.resource/client.js.faces?ln=dom&v=4_18_19&ts=267\"></script><script type=\"text/javascript\" src=\"/javax.faces.resource/zoomPanel.js.faces?ln=zoom&v=4_18_19&ts=267\"></script><script>\\n\\t\\tvar mng_sb = null;\\n\\t\\t// Esta fucion modifica el href del linkImagen con la lupa (boton de buscar) para anadir el texto del input antes del submit\\n\\t\\tfunction addSearchText(input) {\\n\\t\\t\\t// Cogemos el anchor del linkImagen.\\n\\t\\t\\tvar anchor = document.getElementById('anchorBuscarBoton');\\n\\t\\t\\t// Limpiamos posibles busquedas anteriores\\n\\t\\t\\tvar base = anchor.value.substring(0, anchor.value.lastIndexOf('/'));\\n\\t\\t\\t// Creamos el regex para buscar caracteres no alfabeticos\\n\\t\\t\\tvar regex = XRegExp(\"[^\\p{L}\\-\\d]\");\\n\\t\\t\\tvar valor = input.value.replace(' ','-').replace('/','-');\\n\\t\\t\\t// Mientras tengamos caracteres no numericos filtramos el texto de busqueda\\n\\t\\t\\twhile(regex.test(valor)) {\\n\\t\\t\\t\\tvalor = valor.replace(regex, '');\\n\\t\\t\\t}\\n\\t\\t\\tif (valor != '') {\\n\\t\\t\\t\\tanchor.value = base+'/'+valor+'?kw='+input.value;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tanchor.value = base+'/empty?kw=';\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tfunction initAutocomplete() {\\n\\t\\t\\tif (_mng_sb == null && SearchBroker) {\\n\\t\\t\\t\\t_mng_sb = new SearchBroker(\"http://sbmango.colbenson.es/sb-mango/\", true, true).init();\\n\\t\\t\\t\\t_mng_sb.autocomplete(\"#Form\\:SVSecciones\\:buscadorTexto\", 2, 8, 'en', 'S', 'H', function() {document.location = document.getElementById('anchorBuscarBoton').href});\\n\\t\\t\\t}\\n\\t\\t}\\n\\tfunction _mng_preSubmit(name, value, formName) { \\n \\tif (document.getElementById(name)) document.getElementById(name).value = value; \\n \\telse { \\n \\t\\tvar _mngh = document.createElement('input'); \\n \\t\\t_mngh.setAttribute('type','hidden'); \\n \\t\\t_mngh.setAttribute('name',name); _mngh.setAttribute('id',name); \\n \\t\\t_mngh.setAttribute('value',value); \\n \\t\\tdocument.forms[formName].appendChild(_mngh); \\n \\t} \\n } \\n \\n\\t\\tfunction showBocataProducto(event) {\\n\\t\\t\\tdocument.getElementById('SVBodyHeader:SVUserMenu:userMenuForm:SVWishlistHeader:WishListAddProducto').style.display = 'block';\\t\\n\\t\\t\\tif (document.addEventListener) {\\n\\t\\t\\t\\tdocument.addEventListener('click', hideBocataProducto, false);\\n\\t\\t\\t}\\n\\t\\t\\telse if (document.attachEvent) {\\n\\t\\t\\t\\tdocument.attachEvent('onmouseover', hideBocataProducto);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tfunction hideBocataProducto(event) {\\n\\t\\t\\tdocument.getElementById('SVBodyHeader:SVUserMenu:userMenuForm:SVWishlistHeader:WishListAddProducto').style.display = 'none';\\t\\n\\t\\t}\\n\\tvar _mng_timerUpAndDown = null; \\nfunction bolsaDown(despl) { \\n\\tif (despl == null) { \\n\\t\\t_mng_timerUpAndDown = setTimeout('bolsaDown(0)', 45); \\n\\t} else { \\n\\t\\tvar offset = 15; \\n\\t\\tvar div = document.getElementById('SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa:j_id_4b'); \\n\\t\\tvar top = Number(div.style.top.substr(0,div.style.top.length-2)); \\n\\t\\tif (top < 0) { \\n\\t\\t\\tvar height = Number(div.parentNode.style.height.substr(0, div.parentNode.style.height.length-2)); \\n\\t\\t\\tif (despl + offset < 114) { \\n\\t\\t\\t\\tnewTop = top + offset; \\n\\t\\t\\t\\tdiv.style.top = newTop+'px';\\n\\t\\t\\t\\t_mng_timerUpAndDown = setTimeout('bolsaDown('+(despl+offset)+')', 45); \\n\\t\\t\\t} else { \\n\\t\\t\\t\\tnewTop = top + (114-despl); \\n\\t\\t\\t\\tdiv.style.top = newTop+'px';\\n\\t\\t\\t} \\n\\t\\t} \\n\\t} \\n} \\nfunction bolsaUp(despl) { \\n\\tif (despl == null) { \\n\\t\\t_mng_timerUpAndDown = setTimeout('bolsaUp(0)', 45); \\n\\t} else { \\n\\t\\tvar offset = 15; \\n\\t\\tvar div = document.getElementById('SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa:j_id_4b'); \\n\\t\\tvar top = Number(div.style.top.substr(0,div.style.top.length-2)); \\n\\t\\tvar heightParent = 0; \\n\\t\\tvar height = Number(div.offsetHeight); \\t\\n\\t\\tif ((top - offset) > (heightParent - height)) { \\n\\t\\t\\tif (despl + offset < 114) { \\n\\t\\t\\t\\tnewTop = top - offset; \\n\\t\\t\\t\\tdiv.style.top = newTop+'px';\\n\\t\\t\\t\\t_mng_timerUpAndDown = setTimeout('bolsaUp('+(despl+offset)+')', 45); \\n\\t\\t\\t} else { \\n\\t\\t\\t\\tnewTop = top - (114-despl); \\n\\t\\t\\t\\tdiv.style.top = newTop+'px';\\n\\t\\t\\t} \\n\\t\\t} \\n\\t} \\n} \\nfunction _mng_mouse_pos(event) {\\n\\tvar pos = new Object(); \\n\\tif (event.pageX && event.pageY) { \\n\\t\\tpos.x = event.pageX; \\n\\t\\tpos.y = event.pageY; \\n\\t\\tpos.multi = 1; \\n\\t} else if (event.touches) { \\n\\t\\tpos.x = event.touches[0].pageX; \\n\\t\\tpos.y = event.touches[0].pageY; \\n\\t\\tpos.xs = new Array(); \\n\\t\\tpos.ys = new Array(); \\n\\t\\tfor (i=0; i<event.touches.length; i=i+1) { \\n\\t\\t\\tpos.xs[i] = event.touches[i].pageX; \\n\\t\\t\\tpos.ys[i] = event.touches[i].pageY; \\n\\t\\t} \\n\\t\\tpos.multi = event.touches.length; \\n\\t} else { \\n\\t\\tpos.x = event.clientX + (document.documentElement.scrollLeft ? document.documentElement.scrollLeft : document.body.scrollLeft); \\n\\t\\tpos.y = event.clientY + (document.documentElement.scrollTop ? document.documentElement.scrollTop : document.body.scrollTop); \\n\\t\\tpos.multi = 1; \\n\\t} \\n\\treturn pos; \\n} \\nvar _mng_estaBolsaVisible = new Array(); \\nvar _mng_estaBolsaMov = new Array(); \\nfunction _mng_bolsaPageWidth() { \\n\\tif (document.documentElement && document.documentElement.clientWidth) return document.documentElement.clientWidth; \\n\\telse if (document.body) return document.body.clientWidth; \\n\\telse if (window.innerWidth) return window.innerWidth; \\n\\telse return 1024; \\n} \\nfunction _mng_bolsaShow(count, divId, posY, msecs, clip) { \\n\\tvar div = document.getElementById(divId); \\n\\tif (count == null) count = -1; \\n\\tif (count == -1 && !_mng_estaBolsaMov[divId]) { \\n\\t\\t_mng_estaBolsaMov[divId] = true; \\n\\t\\tvar top = posY; \\n\\t\\tdiv.style.top = '-2000px'; \\t\\n\\t\\tdiv.style.display = 'block'; \\n\\t\\tvar height = Number(div.offsetHeight);\\n\\t\\tvar width = Number(div.offsetWidth); \\n\\t\\tdiv.style.top = (top - height)+'px'; \\n\\t\\tvar bolsaTimer = setTimeout('_mng_bolsaShow(1,\"'+divId+'\",'+posY+','+msecs+','+clip+')', msecs); \\n\\t\\tdiv.style.clip = 'rect('+height+'px '+width+'px '+height+'px 0px)'; \\n\\t} else if (count != -1) { \\n\\t\\tvar height = Number(div.offsetHeight); \\n\\t\\tvar width = Number(div.offsetWidth); \\n\\t\\tvar offset = 32; \\n\\t\\tvar percent = Math.round((clip / height) * 100); \\n\\t\\tvar factor = 1; \\n\\t\\tif (percent >= 50) factor = (100 - percent) / 50; \\n\\t\\toffset = Math.round(offset * factor); if (offset < 2) offset = 2;\\n\\t\\tclip = clip + offset; \\n\\t\\tif ( clip < height ) { \\n\\t\\t\\tdiv.style.clip = 'rect('+(height-clip)+'px '+width+'px '+height+'px 0px)'; \\n\\t\\t\\tvar top = Number(div.style.top.substr(0,div.style.top.length-2)); \\n\\t\\t\\tdiv.style.top = (top + offset)+'px'; \\n\\t\\t\\tvar bolsaTimer = setTimeout('mng_bolsaShow('+(count+1)+',\"'+divId+'\",'+posY+','+msecs+','+clip+')', msecs); \\n\\t\\t} else { \\n\\t\\t\\toffsetTop = height - (offset(count-1)); \\n\\t\\t\\t_mng_estaBolsaMov[divId] = false; \\n\\t\\t\\t_mng_estaBolsaVisible[divId] = true; \\n\\t\\t\\tdiv.style.clip = ''; \\n\\t\\t\\tdiv.style.top = posY+'px'; \\n\\t\\t} \\n\\t} \\n} \\nfunction _mng_bolsaHide(count, divId, posY, msecs, clip) { \\n\\tif (count == null) count = -1; \\n\\tif (count == -1 && !_mng_estaBolsaMov[divId]) { \\n\\t\\t_mng_estaBolsaMov[divId] = true; \\n\\t\\tvar bolsaTimer = setTimeout('_mng_bolsaHide(1,\"'+divId+'\",'+posY+','+msecs+','+clip+')', msecs); \\n\\t} else if (count != -1) { \\n\\t\\tvar offset = 32; \\n\\t\\tvar div = document.getElementById(divId); \\n\\t\\tvar height = Number(div.offsetHeight); \\n\\t\\tvar width = Number(div.offsetWidth); \\n\\t\\tvar percent = Math.round((clip / height) * 100); \\n\\t\\tvar factor = 1; \\n\\t\\tif (percent >= 50) factor = (100 - percent) / 50; \\n\\t\\toffset = Math.round(offset * factor); if (offset < 2) offset = 2;\\n\\t\\tclip = clip + offset; \\n\\t\\tif (clip < height ) { \\n\\t\\t\\tvar bolsaTimer = setTimeout('_mng_bolsaHide('+(count+1)+',\"'+divId+'\",'+posY+','+msecs+','+clip+')', msecs); \\n\\t\\t\\tdiv.style.clip = 'rect('+clip+'px '+width+'px '+height+'px 0px)'; \\n\\t\\t\\tvar top = Number(div.style.top.substr(0,div.style.top.length-2)); \\n\\t\\t\\tdiv.style.top = (top - offset)+'px'; \\n\\t\\t} else { \\n\\t\\t\\tdiv.style.display ='none'; \\n\\t\\t\\tdiv.style.top = posY+'px'; \\n\\t\\t\\t_mng_estaBolsaMov[divId] = false; \\n\\t\\t\\t_mng_estaBolsaVisible[divId] = false; \\n\\t\\t} \\n\\t} \\n} \\n_mng_estaBolsaVisible['SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa'] = false; \\n_mng_estaBolsaMov['SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa'] = false; \\nfunction showHideBolsa() { \\n if (_mng_estaBolsaVisible['SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa'] == false) showBolsa(); \\n else hideBolsa();\\n} \\nfunction showBolsa() { \\nif (!_mng_estaBolsaMov['SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa']) { \\n\\t_mng_bolsaShow(null,'SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa',0,28,0); \\n} \\n} \\nfunction hideBolsa() { \\nif (!_mng_estaBolsaMov['SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa']) { \\n\\t_mng_bolsaHide(null,'SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa',0,28,0); \\n} \\n} \\nfunction bolsaSeMueve() { \\n\\treturn _mng_estaBolsaMov['SVBodyHeader:SVUserMenu:userMenuForm:SVBolsaHeader:SVBolsa:panelBolsa']; \\n} \\nfunction _mng_ButtonTRender(cell, src, color, style, styleClass) { \\n \\tif (src != 'null') cell.style.backgroundImage= \"url('\"+src+\"')\"; \\n \\tif (color != 'null') cell.style.color=color; \\n \\tif (style != 'null') cell.style.cssText=style; \\n \\tif (styleClass != 'null') cell.className=styleClass; \\n } \\n function _mng_autoscrollAdd(anchor) {\\n\\ttry { \\n \\t\\tif (window.pageYOffset) \\n \\t\\t\\tanchor.href = anchor.href + '?scroll=' + window.pageYOffset; \\n \\t\\telse if (window.scrollY) \\n \\t\\t\\tanchor.href = anchor.href + '?scroll=' + window.scrollY; \\n \\t\\telse if (document.documentElement.scrollTop) \\n \\t\\t\\tanchor.href = anchor.href + '?scroll=' + document.documentElement.scrollTop; \\n \\t\\telse if (document.body.scrollTop) \\n \\t\\t\\tanchor.href = anchor.href + '?scroll=' + document.body.scrollTop; \\n \\t} catch (ex) {} \\n } \\n function _mng_autoscrollSet() {\\ntry { \\n \\tif (document.location.href.indexOf('scroll=') != -1) { \\n\\t\\tvar scr = document.location.href.substring( document.location.href.indexOf('scroll=')+7); \\n \\t\\tif (scr.indexOf('&') != -1) scr = scr.substring(0, scr.indexOf('&')); \\n \\t\\twindow.scrollTo(0, scr); \\n \\t} \\n } catch (ex) {} \\n } \\n function _mng_add_onload(func) {\\n\\tvar oldonload = window.onload;\\n\\tif (typeof window.onload != 'function') {\\n\\t\\twindow.onload = func;\\n\\t} else {\\n\\t\\twindow.onload = function() {\\n\\t\\t\\tif (oldonload) {\\n\\t\\t\\t\\toldonload.call();\\n\\t\\t\\t}\\n\\t\\t\\tfunc.call();\\n\\t\\t}\\n\\t}\\n}\\n_mng_add_onload(mng_autoscrollSet); \\n\\n\\t\\t\\tfunction showNostock() {\\n\\t\\t\\t\\t_mng_showPanel('popupNostock','3%','2%','panel_visiblepopupNostock','','');\\n\\t\\t\\t}\\n\\t\\t\\tfunction hideNostock() {\\n\\t\\t\\t\\t_mng_closePanel('popupNostock','panel_visiblepopupNostock');\\n\\t\\t\\t}\\n\\t\\t\\tdocument.getElementById('popupNostock').style.left = '3%'; document.getElementById('popupNostock').style.top = '2%';\\n\\t\\t\\n\\t\\t\\tclick_color('ficha', '24', 'true', 'Green', true, 'http://st.mngbcn.com/rcs/pics/static/T3/fotos/S6/33085700_24.jpg\\', 'http://st.mngbcn.com/rcs/pics/static/T3/fotos/S20/33085700_24.jpg\\',\\'33085700\\',\\'imagenFicha\\',\\'_spanNombreColor\\'); void('24'); \\n\\t\\t\\n\\t\\t\\ttextoTallaPatternSingular = \"Violeta size $X is the equivalent of size $Y\";\\n\\t\\t\\ttextoTallaPatternPlural = \"Violeta size $X corresponds to sizes $Y and $Z\";\\n\\t\\t\\n\\t\\t\\t\\t\\t\\t\\tfunction showTallaNoSel() {\\n\\t\\t\\t\\t\\t\\t\\t\\t$('#bocataTallaNoSel').show();\\n\\t\\t\\t\\t\\t\\t\\t\\t$(document).on('mousedown',function(){\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t$('#bocataTallaNoSel').hide();\\n\\t\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t\\t};\\n\\t\\t\\t\\t\\t\\t\\tfunction showTallaNoSelWish() {\\n\\t\\t\\t\\t\\t\\t\\t\\t$('#bocataTallaNoSelWish').show();\\n\\t\\t\\t\\t\\t\\t\\t\\t$(document).on('mousedown',function(){\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t$('#bocataTallaNoSelWish').hide();\\n\\t\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t\\t};\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\tfunction showWishListTip() {\\n\\t\\t\\t\\t\\t\\t\\t$('#wishListTip').show();\\n\\t\\t\\t\\t\\t\\t\\t$(document).on('mousedown',function(){\\n\\t\\t\\t\\t\\t\\t\\t\\t$('#wishListTip').hide();\\n\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t};\\t\\n\\t\\t\\t\\t\\t\\trefreshNombreAvisame =\\n\\t\\t\\t\\t\\t\\t\\tfunction (){\\n\\t\\t\\t\\t\\t\\t\\t\\t$('span[id=\"nombreProductoAvisame\"]').text('Ribbed wool beanie');\\n\\t\\t\\t\\t\\t\\t\\t};\\n\\t\\t\\t\\t\\t\\tfunction showAvisamePopupFicha() {\\n\\t\\t\\t\\t\\t\\t\\t$('input[id=\"hiddenIdProd\"]').val('33085700');\\n\\t\\t\\t\\t\\t\\t\\t$('span[id=\"nombreProductoAvisame\"]').text('Ribbed wool beanie');\\n\\t\\t\\t\\t\\t\\t\\tshowAvisamePopup(); \\n\\t\\t\\t\\t\\t\\t};\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t//document.getElementById('addthis_boton_flike').setAttribute('fb:like:layout','false');\\n\\t\\t\\t\\t\\t\\t\\t\\tvar addthis_config = {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tdata_track_clickback: true,\\n\\t\\t\\t\\t\\t\\t\\t\\t ui_click: true,\\n\\t\\t\\t\\t\\t\\t\\t\\t ui_language: 'en',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tservices_compact : 'email, facebook, twitter, google_plusone_share, pinterest, tumblr'\\n\\t\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\tvar _mng_timerUpAndDown = new Array(); \\nfunction _mng_scrollUpvertical(id, count) { \\n\\tif (count == -1) { \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t_mng_timerUpAndDown[id] = setTimeout('_mng_scrollUpvertical(\"'+id+'\",1)', 18); \\n\\t} else { \\n\\t\\tvar offset = 12; \\n\\t\\tvar div = document.getElementById('_mng_divScrollHijo'+id); \\n\\tvar pos = Number(div.style.top.substr(0,div.style.top.length-2));\\n\\tvar large = Number(div.parentNode.style.height.substr(0, div.parentNode.style.height.length-2)); \\n\\t\\tif (pos < 0) { \\n\\t\\t\\tif (pos + offset > 0) newPos = 0; \\n\\t\\t\\telse newPos = pos + offset; \\n\\t\\tdiv.style.top = newPos+'px'; \\n\\t\\t\\t_mng_timerUpAndDown[id] = setTimeout('_mng_scrollUpvertical(\"'+id+'\",'+(count+1)+')', 18); \\n\\t\\t} \\n\\t} \\n} \\nfunction _mng_scrollDownvertical(id, count) {\\n\\tif (count == -1) { \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t_mng_timerUpAndDown[id] = setTimeout('_mng_scrollDownvertical(\"'+id+'\",1)', 18); \\t\\t\\t\\t\\t\\t\\t\\t\\n\\t} else { \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\tvar offset = 12; \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\tvar div = document.getElementById('_mng_divScrollHijo'+id);\\t\\n\\t\\tvar child = null; for (i=0; i<div.childNodes.length; i++) { if (typeof(div.childNodes[i].offsetHeight) != 'undefined') child = div.childNodes[i];}\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\tvar pos = Number(div.style.top.substr(0,div.style.top.length-2)); \\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\tvar large = Number(div.parentNode.style.height.substr(0, div.parentNode.style.height.length-2)); \\t\\n\\tvar largeChild = Number(child.offsetHeight) ; \\t\\t\\n\\t\\tif ( pos > ( large - largeChild ) ) { \\n\\t\\t\\tnewPos = pos - offset; \\n\\t\\tdiv.style.top = newPos+'px'; \\n\\t\\t\\t_mng_timerUpAndDown[id] = setTimeout('_mng_scrollDownvertical(\"'+id+'\",'+(count+1)+')', 18); \\n\\t\\t} \\n\\t} \\n} \\nfunction _mng_scrollStop(id) { \\n\\tclearTimeout(_mng_timerUpAndDown[id]); \\n} \\n_mng_timerUpAndDown['24'] = null; \\nfunction detallesDown_24() { \\n\\t_mng_scrollUpvertical('24', -1); \\n} \\nfunction detallesUp_24() {\\n\\t_mng_scrollDownvertical('24', -1); \\n} \\nfunction detallesStop_24() { \\n\\t_mng_scrollStop('24'); \\n} \\n_mng_timerUpAndDown['90'] = null; \\nfunction detallesDown_90() { \\n\\t_mng_scrollUpvertical('90', -1); \\n} \\nfunction detallesUp_90() {\\n\\t_mng_scrollDownvertical('90', -1); \\n} \\nfunction detallesStop_90() { \\n\\t_mng_scrollStop('90'); \\n} \\n\\n\\t\\t\\tfunction openZoomFicha() {\\n\\t\\t\\t\\tstartZoomPanel('zoomFicha','zoomFicha_img', 'zoomFicha_hid', false);\\n\\t\\t\\t}\\n\\n\\t\\t\\tfunction setSrcZoom(src) {\\n\\t\\t\\t\\tchangeSrcZoom(src, 'zoomFicha_img', 'zoomFicha_hid');\\n\\t\\t\\t}\\n\\t\\tfunction _mng_popupShow(container, content) { \\n\\tvar doc = document.documentElement, body = document.body; \\n\\tvar top = (doc && doc.scrollTop || body && body.scrollTop || 0); \\n\\tcontainer.style.height = (document.documentElement.offsetHeight-120)+'px'; \\n\\tcontainer.style.width = document.documentElement.offsetWidth+'px'; \\n\\tcontent.style.display = 'block'; \\n\\tif (window.innerHeight) wh = window.innerHeight; \\n\\telse if (document.body.clientHeight) wh = document.body.clientHeight; \\n\\telse if (document.documentElement.offsetHeight) wh = document.documentElement.clientHeight; \\n\\tcontent.style.top = (top+((wh/2)-(content.offsetHeight/2)))+'px'; \\n\\tcontainer.style.display = 'block'; \\n} \\nfunction _mng_popupHide(container, content) { \\n\\tcontainer.style.display = 'none'; \\n\\tcontent.style.display = 'none'; \\n} \\nfunction mostrarInfoPopup() { \\n\\tvar div1 = document.getElementById('Form:SVLoading:popupLoading'); \\n\\tvar div2 = document.getElementById('Form:SVLoading:popupLoading_t'); \\n\\tdocument.getElementById('_mng_dialogVisible_popupLoading').value = 'true'; \\n\\t_mng_popupShow(div1, div2); \\n} \\nfunction ocultarInfoPopup() { \\n\\tvar div1 = document.getElementById('Form:SVLoading:popupLoading'); \\n\\tvar div2 = document.getElementById('Form:SVLoading:popupLoading_t'); \\n\\tdocument.getElementById('_mng_dialogVisible_popupLoading').value = 'false'; \\n\\t_mng_popupHide(div1, div2); \\n} \\n\\n\\t\\t\\tfunction showAviso() {\\n\\t\\t\\t\\t_mng_showPanel('popupAviso','3%','2%','_panel_visiblepopupAviso','','');\\n\\t\\t\\t}\\n\\t\\t\\tfunction hideAviso() {\\n\\t\\t\\t\\t_mng_closePanel('popupAviso','_panel_visiblepopupAviso');\\n\\t\\t\\t}\\n\\t\\t\\tdocument.getElementById('popupAviso').style.left = '3%'; document.getElementById('popupAviso').style.top = '2%';\\n\\t\\tfunction showAviso() { \\n\\tvar div1 = document.getElementById('Form:SVPAvisoPre:popupAvisoPreVenta'); \\n\\tvar div2 = document.getElementById('Form:SVPAvisoPre:popupAvisoPreVenta_t'); \\n\\tdocument.getElementById('_mng_dialogVisible_popupAvisoPreVenta').value = 'true'; \\n\\t_mng_popupShow(div1, div2); \\n} \\nfunction hideAviso() { \\n\\tvar div1 = document.getElementById('Form:SVPAvisoPre:popupAvisoPreVenta'); \\n\\tvar div2 = document.getElementById('Form:SVPAvisoPre:popupAvisoPreVenta_t'); \\n\\tdocument.getElementById('_mng_dialogVisible_popupAvisoPreVenta').value = 'false'; \\n\\t_mng_popupHide(div1, div2); \\n} \\nfunction showAvisoInfoPreventa(evt, msecs) { \\n_mng_bocata_show('Form:SVPAvisoPre:avisoInfoPreventa', 'center top', 126, evt, msecs); \\n}\\nfunction hideAvisoInfoPreventa() { \\n_mng_bocata_hide('Form:SVPAvisoPre:avisoInfoPreventa'); \\n} \\nfunction _mng_object_pos(obj) {\\n\\tvar curpos = new Object(); \\n\\tvar objCopy = obj; \\n\\tcurpos.left = 0; \\n\\tcurpos.top = 0;\\t\\n\\tif(obj.offsetParent) \\n\\t\\twhile(1) { \\n\\t\\t\\tcurpos.left += obj.offsetLeft; \\n\\t\\t\\tcurpos.top += obj.offsetTop; \\n\\t\\t\\tif(!obj.offsetParent) break; \\n\\t\\t\\tobj = obj.offsetParent; \\n\\t\\t} \\n\\telse { \\n\\t\\tif(obj.x) curpos.left += obj.x; \\n\\t\\tif(obj.y) curpos.top += obj.y; \\n\\t} \\n\\treturn curpos;\\t\\n} \\nfunction _mng_bocata_link(divId, linkedId, posX, posY, origin, evt, msecs) { \\nvar div = document.getElementById(divId); \\nif (typeof(div) != 'undefined') { \\n\\tvar elem = document.getElementById(linkedId); \\n\\tif (div.style.display == 'none') { \\n\\t\\tvar elemPos = _mng_object_pos(elem); \\n\\t\\tdiv.style.display = 'block'; \\n\\t\\tvar heightDiv = Number(div.offsetHeight); \\n\\t\\tvar widthDiv = Number(div.offsetWidth); \\n\\t\\tvar heightElem = Number(elem.offsetHeight); \\n\\t\\tvar widthElem = Number(elem.offsetWidth); \\n\\t\\tif (msecs == null) { \\n\\t\\t\\tdocument.addEventListener('mouseup', function() {_mng_bocata_hide(divId); } , false);\\t\\n\\t\\t} \\n\\t\\tif (posY == 'bottom') div.style.top = (elemPos.top+heightElem)+'px'; \\n\\t\\telse if (posY == 'center') div.style.top = (elemPos.top+(heightElem/2))+'px'; \\n\\t\\telse div.style.top = (elemPos.top-heightDiv)+'px'; \\n\\t\\tvar posL = elemPos.left - origin; \\n\\t\\tif (posX == 'right') div.style.left = (posL+widthElem)+'px'; \\n\\t\\telse if (posX == 'center') div.style.left = (posL+(widthElem/2))+'px'; \\n\\t\\telse div.style.left = posL+'px'; \\n\\t} \\n\\tif (msecs != null) \\n\\t\\tsetTimeout('_mng_bocata_hide(\"'+divId+'\",\"xxx\")',msecs);\\n} \\n} \\nfunction _mng_bocata_show(divId, position, origin, evt, msecs) { \\nvar div = document.getElementById(divId); \\nif (typeof(div) != 'undefined') { \\n\\tif (div.style.display == 'none') { \\n\\tvar posX = evt.pageX; \\n\\tvar posY = evt.pageY; \\n\\t\\tdiv.style.display = 'block'; \\n\\t\\tvar height = Number(div.offsetHeight); \\n\\t\\tvar width = Number(div.offsetWidth); \\n\\t\\tif (msecs == null) { \\n\\t\\t\\tdocument.addEventListener('mouseup',function() {_mng_bocata_hide(divId); } , false);\\t\\n\\t\\t} \\n\\t\\tif (position == 'bottom') div.style.top = posY+'px'; \\n\\t\\telse div.style.top = (posY-height)+'px'; \\n\\t\\tvar posL = posX - origin; \\n\\t\\tdiv.style.left = posL+'px'; \\n\\t} \\n\\tif (msecs != null) \\n\\t\\tsetTimeout('_mng_bocata_hide(\"'+divId+'\",\"xxx\")',msecs);\\n} \\n}\\nfunction _mng_bocata_hide(bocataId) { \\n\\tif (document.getElementById(bocataId) != null && typeof(document.getElementById(bocataId)) != 'undefined') { \\n\\t\\tdocument.getElementById(bocataId).style.display = 'none'; \\n\\t\\t\\tdocument.removeEventListener('mouseup',function() {_mng_bocata_hide(divId); } , false);\\t\\n\\t} \\n} \\nfunction showAvisoInfoPreventaDesfile(evt, msecs) { \\n_mng_bocata_show('Form:SVPAvisoPre:avisoInfoPreventaDesfile', 'center top', 126, evt, msecs); \\n}\\nfunction hideAvisoInfoPreventaDesfile() { \\n_mng_bocata_hide('Form:SVPAvisoPre:avisoInfoPreventaDesfile'); \\n} \\n\\n\\t\\t\\tfunction showAvisoIdentWishList() {\\n\\t\\t\\t\\t_mng_showPanel('identificateWishListPopup','25%','40%','_panel_visibleidentificateWishListPopup','','');\\n\\t\\t\\t}\\n\\t\\t\\tfunction hideAvisoIdentWishList() {\\n\\t\\t\\t\\t_mng_closePanel('identificateWishListPopup','_panel_visibleidentificateWishListPopup');\\n\\t\\t\\t}\\n\\t\\t\\tdocument.getElementById('identificateWishListPopup').style.left = '25%'; document.getElementById('identificateWishListPopup').style.top = '40%';\\n\\t\\tfunction showAvisoMaxItems() { \\n\\tvar div1 = document.getElementById('Form:SVWishlistPanelMaxItems:popupmaxItems'); \\n\\tvar div2 = document.getElementById('Form:SVWishlistPanelMaxItems:popupmaxItems_t'); \\n\\tdocument.getElementById('_mng_dialogVisible_popupmaxItems').value = 'true'; \\n\\t_mng_popupShow(div1, div2); \\n} \\nfunction hideAvisoMaxItems() { \\n\\tvar div1 = document.getElementById('Form:SVWishlistPanelMaxItems:popupmaxItems'); \\n\\tvar div2 = document.getElementById('Form:SVWishlistPanelMaxItems:popupmaxItems_t'); \\n\\tdocument.getElementById('_mng_dialogVisible_popupmaxItems').value = 'false'; \\n\\t_mng_popupHide(div1, div2); \\n} \\n\\n\\t\\t\\tfunction showPanelWishList() {\\n\\t\\t\\t\\t_mng_showPanel('panelWishList','35%','50%','_panel_visiblepanelWishList','','');\\n\\t\\t\\t}\\n\\t\\t\\tfunction hidePanelWishLista() {\\n\\t\\t\\t\\t_mng_closePanel('panelWishList','_panel_visiblepanelWishList');\\n\\t\\t\\t}\\n\\t\\t\\tdocument.getElementById('panelWishList').style.left = '35%'; document.getElementById('panelWishList').style.top = '50%';\\n\\t\\tfunction _mng_panelShow(div) { \\n\\tif (div.style.display != 'block') div.style.display = 'block'; \\n\\tvar x = (document.all ? document.scrollTop : window.pageYOffset); \\n\\tif (x != null) { \\n\\t\\tvar pos = div.style.top; \\n\\t\\tif (x <= parseInt(pos)) { \\n\\t\\t var height = 0; \\n\\t\\t\\tif (document.documentElement.clientHeight) height = x + document.documentElement.clientHeight; \\n\\t\\t\\telse if (window.innerHeight) height = x + window.innerHeight; \\n\\t\\t\\tvar heightDiv = Number(div.offsetHeight); \\n\\t\\t\\tif (height > 0 && (parseInt(pos) + heightDiv > height)) div.style.top = (height-heightDiv)+'px'; \\n\\t\\t} \\n\\t} \\n} \\nfunction _mng_panelHide(div) { \\n\\tif (div.style.display == 'block') div.style.display = 'none'; \\n} \\nvar _mng_mouseX = null; \\nvar _mng_mouseY = null; \\nvar _mng_mouseX2 = null; \\nvar _mng_mouseY2 = null; \\nvar _mng_idPanel = null; \\nvar _mng_idPanelPos = null; \\nfunction _mng_panelMovePre(evt, divId, hidId) { \\n\\tvar div = document.getElementById(divId); \\n\\tif (div.style.display == 'block') { \\n\\t\\tvar m = _mng_mouse_pos(evt); \\n\\t\\tvar left = parseInt(div.style.left);\\n\\t\\t_mng_mouseX = m.x; \\n\\t\\t_mng_mouseY = m.y; \\n\\t\\t_mng_idPanel = divId; \\n\\t\\t_mng_idPanelPos = hidId; \\n\\tdiv.addEventListener('mousemove',_mng_panelMove ,false);\\t\\n\\tdiv.addEventListener('mouseup',_mng_panelMovePost, false);\\t\\n\\t} \\n} \\nfunction _mng_panelMove(event) { \\n\\tvar div = document.getElementById(_mng_idPanel); \\n\\tif (_mng_mouseX != null) { \\n\\t\\tvar top = parseInt(div.style.top); \\n\\t\\tvar left = parseInt(div.style.left); \\n\\t\\tvar m = _mng_mouse_pos(event); \\n\\t\\tvar moveX = left + (m.x - _mng_mouseX); \\n\\t\\tvar moveY = top + (m.y - _mng_mouseY); \\n\\t\\t_mng_mouseX = m.x; \\n\\t\\t_mng_mouseY = m.y; \\n\\tif (moveX > 0 && (moveX + div.offsetWidth) < document.body.clientWidth ) \\t\\t\\tdiv.style.left = moveX + 'px'; \\n\\tif (moveY > 0 && (moveY + div.offsetHeight) < document.body.clientHeight ) \\t\\t\\tdiv.style.top = moveY + 'px'; \\n\\t} \\n} \\nfunction _mng_touchMove(event) { \\n event.preventDefault(); \\n\\tvar div = document.getElementById(_mng_idPanel); \\n\\tvar m = _mng_mouse_pos(event); \\n\\t_mng_mouseX2 = m.x; \\n\\t_mng_mouseY2 = m.y; \\n\\tvar moveX = _mng_mouseX2 - _mng_mouseX; \\n\\tvar moveY = _mng_mouseY2 - _mng_mouseY; \\n\\tdiv.style.webkitTransform = 'translate(' + moveX + 'px, ' + moveY + 'px)'; \\n} \\nfunction _mng_panelMovePost() { \\n\\tvar div = document.getElementById(mng_idPanel); \\n\\tvar top = parseInt(div.style.top); \\n\\tvar left = parseInt(div.style.left); \\n\\t_mng_mouseX = null; \\n\\t_mng_mouseY = null; \\n\\tvar pos = left + '' + top; \\n document.getElementById('mng_panelXY'+div.id).value = pos; \\n\\tdiv.removeEventListener('mousemove',_mng_panelMove,false);\\t\\n\\tdiv.removeEventListener('mouseup',_mng_panelMovePost,false);\\t\\n} \\nfunction _mng_touchMovePost(event) { \\n\\tvar div = document.getElementById(_mng_idPanel); \\n\\tvar top = parseInt(div.style.top); \\n\\tvar left = parseInt(div.style.left); \\n\\tif (_mng_mouseX2 != null && _mng_mouseX != null) { \\n\\t\\tvar moveX = left + (_mng_mouseX2 - _mng_mouseX); \\n\\t\\tvar moveY = top + (_mng_mouseY2 - mng_mouseY); \\n\\t\\tdiv.style.webkitTransform = 'translate(0px, 0px)'; \\n\\t\\tdiv.style.left = moveX + 'px'; \\n\\t\\tdiv.style.top = moveY + 'px'; \\n\\t\\t_mng_mouseX = null; \\n\\t\\t_mng_mouseY = null; \\n\\t\\t_mng_mouseX2 = null; \\n\\t\\t_mng_mouseY2 = null; \\n\\t} \\n\\tvar pos = left + '' + top; \\n document.getElementById('mng_panelXY'+div.id).value = pos; \\n\\tdiv.removeEventListener('touchmove',_mng_panelMove, false);\\t\\n\\tdiv.removeEventListener('touchend',_mng_touchMovePost, false);\\t\\n} \\nfunction showAddReview() { \\n\\tvar div = document.getElementById('j_id_sw'); \\n\\t_mng_panelShow(div); \\n\\tdocument.getElementById('_mng_dialogVisible_j_id_sw').value = 'true'; \\n} \\nfunction hideAddReview() { \\n\\tvar div = document.getElementById('j_id_sw'); \\n\\t_mng_panelHide(div); \\n\\tdocument.getElementById('_mng_dialogVisible_j_id_sw').value = 'false'; \\n} \\nhideAddReview(); \\n\\n\\t\\tfunction showAvisamePopup() {\\n\\t\\t\\t$('#bocataAvisame').show();\\n\\t\\t\\t$('#blockedModal').show();\\n\\t\\t};\\n\\t\\tfunction hideAvisamePopup() {\\n\\t\\t\\t$('#bocataAvisame').hide();\\n\\t\\t\\t$('#blockedModal').hide();\\n\\t\\t};\\n\\t\\n\\t\\t// Script que recarga los iframes de paginas externas para mobile, por el bug del back en iPhone\\n\\t\\t\\tif(false && typeof(window.frames['bodyFrame']) != 'undefined') {\\n\\t\\t\\t\\tvar ts = new Date().getTime();\\n\\t\\t\\t\\tif (document.getElementById('bodyFrame').src.indexOf(\"?\") > -1){\\n\\t\\t\\t\\t\\tdocument.getElementById('bodyFrame').src = document.getElementById('bodyFrame').src+'&ts='+ts;\\n\\t\\t\\t\\t}else{\\n\\t\\t\\t\\t\\tdocument.getElementById('bodyFrame').src = document.getElementById('bodyFrame').src+'?ts='+ts;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t\\t dataLayer = [{\\n 'marinId': 'bntmtycmx0',\\n 'idPais': '400',\\n 'dispositivo': 'desktop',\\n 'acceso': 'web',\\n 'linea': 'mango',\\n 'pais': 'ee.uu',\\n 'idioma': 'usa english',\\n 'fichaProd': 'h-for-e c:gorro canale lana',\\n 'familia': 'sombreros y gorros',\\n 'referencia': '33085700'\\n }];\\n (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\\n new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\\n j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\\n '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\\n })(window,document,'script','dataLayer','GTM-TWFTD4')\\n ; \\n var _criteoLoader = \"static.criteo.net/js/ld/ld.js\";\\n var _criteoURL = location.protocol==\"https:\"?\"https://\"+_criteoLoader:\"http://\"+_criteoLoader;\\n if(location.protocol==\"https:\"){var nUrl=\"https://\"+_criteoLoader;}else{var nUrl=\"http://\"+_criteoLoader;}\\n document.writeln(\"<scr\"+\"ipt type='text/ja\"+\"vascript' src='\"+ _criteoURL +\"' async='true' ></scr\"+\"ipt>\")\\n \\n ; \\n window.criteo_q = window.criteo_q || []; \\n window.criteo_q.push( \\n { event: \"setAccount\", account: 11972 }, \\n { event: \"setCustomerId\", id: \"0\" }, \\n { event: \"setSiteType\", type: \"d\" }, \\n { event: \"viewItem\", item: \"33085700\" } \\n ); \\n ;\\n\\t\\t</script>\\n\\t\\n\\t<script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/topLayoutDivs.js?v=4_18_19&ts=267\"></script>\\n\\t<script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/oldLibs.js?v=4_18_19&ts=267\"></script>\\n\\t<script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/jquery.hoverIntent.js?v=4_18_19&ts=267\"></script>\\n\\t<script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/jquery-scrolltofixed-min.js?v=4_18_19&ts=267\"></script>\\n\\t\\n\\t<script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/picturefill.min.js?v=4_18_19&ts=267\" async=\"async\"></script>\\n\\t\\n <script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/plugins.js?v=4_18_19&ts=267\"></script>\\n <script type=\"text/javascript\" src=\"http://st.mngbcn.com/static/js/main.js?v=4_18_19&ts=267\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"http://tracking.lengow.com/tos_mango.js?v=4_18_19&ts=267\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"//assets.pinterest.com/js/pinit.js?v=4_18_19&ts=267\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"http://s7.addthis.com/js/250/addthis_widget.js#username=mangotest01?v=4_18_19&ts=267\" async=\"async\"></script>\\n'\nIn [13]: response.xpath('//span[@itemprop=\"description\"]/span/text()').extract()Out[13]: [u'Ribbed wool beanie with rolled-up edge. ']\nIn [14]: response.request.url\nOut[14]: 'http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123'\nIn [15]: response.url\nOut[15]: 'http://shop.mango.com/US/p0/men/accessories/wool-fedora-hat/?id=33053675_G5&ident=0_accesorio112_0_1415864699123&n=1&s=accesorios_he.sombreros_he&ts=1415864699123'", "issue_status": "Closed", "issue_reporting_time": "2014-11-13T09:10:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1331": {"issue_url": "https://github.com/scrapy/scrapy/issues/947", "issue_id": "#947", "issue_summary": "cannot use the xpath to extract the data", "issue_description": "hansonzh commented on Nov 12, 2014\nI want to extract the data \u5546\u54c1\u7f16\u53f7\uff1a1057746 from page whose url is \u201chttp://item.jd.com/1057746.html\u201d.\non the scrapy shell, the following xpath is used: response.xpath('//*[@id=\"summary-market\"]/div[2]/span'), but the blank list is returned. I have checked the xpath with XPath Helper on chrome, I can get the data with this tool. so is there a bug of scrapy?", "issue_status": "Closed", "issue_reporting_time": "2014-11-12T14:13:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1332": {"issue_url": "https://github.com/scrapy/scrapy/issues/945", "issue_id": "#945", "issue_summary": "Scrapy Only Close Spider and never dumped Scrapy status", "issue_description": "AlanChien commented on Nov 12, 2014\nHello,\nI have found some issue recently .\nI use Scrapyd to run my spider with python-scrapyd-api.\nSome times my spider log only show below:\n2014-11-12 07:40:59+0800 [FullInfo] DEBUG: Scraped from 200 http://www.indeed.com/jobtrends/industry?date=2010-08\n{'file_md5': '62aac088a1bc77d9abc3d32d92bf7168',\n'file_sha1': '50b625fe3d605bc27d601d1e1fe1c46ad2ebe39b',\n'file_size': 51722,\n'file_type': 'HTML document, ASCII text, with very long lines',\n'status_code': 200,\n'url_info': [{'http_status_code': 200,\n'url': 'http://www.indeed.com/jobtrends/industry?date=2010-08',\n'url_host': 'www.indeed.com',\n'url_host_ip': '50.97.195.27'}]}\n2014-11-12 07:40:59+0800 [FullInfo] INFO: Closing spider (finished)\nYou will see there is no 'Dumping Scrapy status'.\nI have get nothing error message...\nWhy...?\nP.s\nThe pipelines close_spider function was never be called , then just hang there", "issue_status": "Closed", "issue_reporting_time": "2014-11-12T01:46:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1333": {"issue_url": "https://github.com/scrapy/scrapy/issues/942", "issue_id": "#942", "issue_summary": "ImportError: cannot import name log", "issue_description": "natea commented on Nov 9, 2014\nI'm trying to ue scrapy with django-dynamic-scraper, and getting this error when I start up the Django server:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/django/utils/autoreload.py\", line 93, in wrapper\n    fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/django/core/management/commands/runserver.py\", line 102, in inner_run\n    self.validate(display_num_errors=True)\n  File \"/usr/local/lib/python2.7/site-packages/django/core/management/base.py\", line 310, in validate\n    num_errors = get_validation_errors(s, app)\n  File \"/usr/local/lib/python2.7/site-packages/django/core/management/validation.py\", line 34, in get_validation_errors\n    for (app_name, error) in get_app_errors().items():\n  File \"/usr/local/lib/python2.7/site-packages/django/db/models/loading.py\", line 196, in get_app_errors\n    self._populate()\n  File \"/usr/local/lib/python2.7/site-packages/django/db/models/loading.py\", line 78, in _populate\n    self.load_app(app_name)\n  File \"/usr/local/lib/python2.7/site-packages/django/db/models/loading.py\", line 99, in load_app\n    models = import_module('%s.models' % app_name)\n  File \"/usr/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 40, in import_module\n    __import__(name)\n  File \"/Users/nateaune/Dropbox/code/moocminder/moocminder/moocscraper/models.py\", line 3, in <module>\n    from scrapy.contrib.djangoitem import DjangoItem\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/__init__.py\", line 56, in <module>\n    from scrapy.spider import Spider\n  File \"/usr/local/lib/python2.7/site-packages/scrapy/spider.py\", line 6, in <module>\n    from scrapy import log\nImportError: cannot import name log", "issue_status": "Closed", "issue_reporting_time": "2014-11-08T19:56:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1334": {"issue_url": "https://github.com/scrapy/scrapy/issues/938", "issue_id": "#938", "issue_summary": "Is it common practice to create a scrapy project for each spider?", "issue_description": "roycehaynes commented on Nov 4, 2014\nFor example, I want to scrap goodreads and facebook. So, I plan on making two scrapy projects:\nscrapy startproject goodreads\nAND\nscrapy startproject facebook\nIs this the correct way of thinking of a scrapy project?\nTake care!", "issue_status": "Closed", "issue_reporting_time": "2014-11-04T18:02:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1335": {"issue_url": "https://github.com/scrapy/scrapy/issues/937", "issue_id": "#937", "issue_summary": "Windows installation guide", "issue_description": "matxtam commented on Nov 4, 2014\nHi,\nI\u00b4ve been installing the framework in Windows Xp and:\nC:\\Python2.7;C:\\Python2.7\\Scripts; (as it\u00b4s said on the installation guide)\nshould be:\nC:\\Python27;C:\\Python27\\Scripts;\nand pywin32 also needs to be installed\nthanks for an awesome framework", "issue_status": "Closed", "issue_reporting_time": "2014-11-04T01:27:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1336": {"issue_url": "https://github.com/scrapy/scrapy/issues/936", "issue_id": "#936", "issue_summary": "Scrapy thinks Python is under 2.7", "issue_description": "Sshuichi commented on Nov 4, 2014\nHello I've updated my python 2.6 to 2.7 but Scrapy thinks that Python is still under 2.6\nHow can I fix this ? Debian 6.0", "issue_status": "Closed", "issue_reporting_time": "2014-11-03T18:47:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1337": {"issue_url": "https://github.com/scrapy/scrapy/issues/935", "issue_id": "#935", "issue_summary": "Add support for depth per spider", "issue_description": "lihan commented on Nov 3, 2014\nCurrently DEPTH_LIMIT is a global setting, but some projects really need to have a limit per spider.", "issue_status": "Closed", "issue_reporting_time": "2014-11-03T13:25:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1338": {"issue_url": "https://github.com/scrapy/scrapy/issues/933", "issue_id": "#933", "issue_summary": "Documentation about running tests is obsolete", "issue_description": "cbourjau commented on Oct 29, 2014\nThe documentation about how to run tests seems to be obsolete. (http://doc.scrapy.org/en/latest/contributing.html#tests)\nI am actually not sure how they should be run nowadays... I found\n$ py.test --twisted scrapy\nin issue #788 but that does not work for me either.", "issue_status": "Closed", "issue_reporting_time": "2014-10-29T17:59:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1339": {"issue_url": "https://github.com/scrapy/scrapy/issues/931", "issue_id": "#931", "issue_summary": "Scrapy no such host crawler", "issue_description": "teodorag commented on Oct 28, 2014\nHello,\nI'm using this crawler as my base crowler https://github.com/alecxe/broken-links-checker/blob/master/broken_links_spider.py\nIt is created to catch 404 error domains and save them. I wanted to modify it a little bit and make it look for \"No such host\" error, which is error 12002.\nHowever, with this code, Scrapy is not receiving any responce (because there isn't a host to return a responce) and when scrapy encounters such domains it returns\nnot found: [Errno 11001] getaddrinfo failed.\nHow can I catch this not found error and save the domains?", "issue_status": "Closed", "issue_reporting_time": "2014-10-28T09:50:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1340": {"issue_url": "https://github.com/scrapy/scrapy/issues/929", "issue_id": "#929", "issue_summary": "Pass Request meta argument to other Requests in CrawlSpider", "issue_description": "jbinfo commented on Oct 25, 2014\nWhen using CrawlSpider and rules to automatically extract links and flow them, the parse method that generate Requests to follow don't pass meta argument.\nThe idea behind this is when using the start_requests method to generate some requests at beginning and set the meta argument, then let the rules and parse method to do things automatically.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2014-10-25T08:43:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1341": {"issue_url": "https://github.com/scrapy/scrapy/issues/926", "issue_id": "#926", "issue_summary": "twisted.web._newclient.ParseError", "issue_description": "AlanChien commented on Oct 24, 2014\nI used scrapy to crawl an url on \"http://sports.williamhill.com/bet/ja/betting/e/6710669/Adelaide-36ers-%E5%AF%BE-Wollongong-Hawks.html\"\nBut always through out an error message : <twisted.python.failure.Failure <class 'twisted.web._newclient.ParseError'>>\nI can browse this url through browser normally .\nAlso can get url content through \"wget\" .\nI can get this url content success, when I use python library \"Requests\" .\nCan some one help me?\nThe error message as below:\nGET http://sports.williamhill.com/bet/ja/betting/e/6710669/Adelaide-36ers-%E5%AF%BE-Wollongong-Hawks.html\n[Test] DEBUG: Retrying GET http://sports.williamhill.com/bet/ja/betting/e/6710669/Adelaide-36ers-%E5%AF%BE-Wollongong-Hawks.html (failed 1 times): [twisted.python.failure.Failure <class 'twisted.web._newclient.ParseError']\n2014-10-24 14:56:06+0800 [Test] DEBUG: User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5 GET http://sports.williamhill.com/bet/ja/betting/e/6710669/Adelaide-36ers-%E5%AF%BE-Wollongong-Hawks.html\n[Test] DEBUG: Retrying GET http://sports.williamhill.com/bet/ja/betting/e/6710669/Adelaide-36ers-%E5%AF%BE-Wollongong-Hawks.html (failed 2 times): [twisted.python.failure.Failure <class 'twisted.web._newclient.ParseError']\n[Test] DEBUG: User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5 GET http://sports.williamhill.com/bet/ja/betting/e/6710669/Adelaide-36ers-%E5%AF%BE-Wollongong-Hawks.html\n[Test] DEBUG: Gave up retrying GET http://sports.williamhill.com/bet/ja/betting/e/6710669/Adelaide-36ers-%E5%AF%BE-Wollongong-Hawks.html (failed 3 times): [twisted.python.failure.Failure <class 'twisted.web._newclient.ParseError']\nRequest failed: GET http://sports.williamhill.com/bet/ja/betting/e/6710669/Adelaide-36ers-%E5%AF%BE-Wollongong-Hawks.html", "issue_status": "Closed", "issue_reporting_time": "2014-10-24T07:12:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1342": {"issue_url": "https://github.com/scrapy/scrapy/issues/923", "issue_id": "#923", "issue_summary": "not able to create structured directory for images . plus scrapy not updating pipeline code", "issue_description": "igauravsehrawat commented on Oct 21, 2014 \u2022\nedited by redapple\ndef process_item(self, item, spider):\n        image_guid =  item\n\n        print \"me before\"\n        return \"/full/%s%s.jpg\"% (id,image_guid)\n\n    #this should work , exactly as per documentation \n\n    def get_media_requests(self, item, info):\n\n        print \"me before\"\n        for image_url in item['image_urls']:\n            yield scrapy.Request(image_url,meta={'id':item['Property_name']})\nImages are downloaded in one folder only . After updating code , scrapy doesn't updates . It there some cache related concept ?", "issue_status": "Closed", "issue_reporting_time": "2014-10-21T07:15:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1343": {"issue_url": "https://github.com/scrapy/scrapy/issues/922", "issue_id": "#922", "issue_summary": "Scrapy Not Working in Yosemite", "issue_description": "masterfung commented on Oct 20, 2014\nwhen I run scrapy crawl spider_name this comes up:\nTraceback (most recent call last):\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/cmdline.py\", line 122, in execute\ncmds = _get_commands_dict(settings, inproject)\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/cmdline.py\", line 46, in _get_commands_dict\ncmds = _get_commands_from_module('scrapy.commands', inproject)\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/cmdline.py\", line 29, in _get_commands_from_module\nfor cmd in _iter_command_classes(module):\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/cmdline.py\", line 20, in _iter_command_classes\nfor module in walk_modules(module_name):\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 68, in walk_modules\nsubmod = import_module(fullpath)\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/init.py\", line 37, in import_module\nimport(name)\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/commands/bench.py\", line 3, in\nfrom scrapy.tests.mockserver import MockServer\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/tests/mockserver.py\", line 6, in\nfrom twisted.internet import reactor, defer, ssl\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/twisted/internet/ssl.py\", line 59, in\nfrom OpenSSL import SSL\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/OpenSSL/init.py\", line 8, in\nfrom OpenSSL import rand, crypto, SSL\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/OpenSSL/rand.py\", line 11, in\nfrom OpenSSL._util import (\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/OpenSSL/_util.py\", line 3, in\nfrom cryptography.hazmat.bindings.openssl.binding import Binding\nImportError: No module named cryptography.hazmat.bindings.openssl.binding\nwhile if I were to just use the scrapy shell 'url' command, the traceback is:\nTraceback (most recent call last):\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/bin/scrapy\", line 11, in\nsys.exit(execute())\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/cmdline.py\", line 122, in execute\ncmds = _get_commands_dict(settings, inproject)\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/cmdline.py\", line 46, in _get_commands_dict\ncmds = _get_commands_from_module('scrapy.commands', inproject)\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/cmdline.py\", line 29, in _get_commands_from_module\nfor cmd in _iter_command_classes(module):\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/cmdline.py\", line 20, in _iter_command_classes\nfor module in walk_modules(module_name):\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 68, in walk_modules\nsubmod = import_module(fullpath)\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/init.py\", line 37, in import_module\nimport(name)\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/commands/bench.py\", line 3, in\nfrom scrapy.tests.mockserver import MockServer\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scrapy/tests/mockserver.py\", line 6, in\nfrom twisted.internet import reactor, defer, ssl\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/twisted/internet/ssl.py\", line 59, in\nfrom OpenSSL import SSL\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/OpenSSL/init.py\", line 8, in\nfrom OpenSSL import rand, crypto, SSL\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/OpenSSL/rand.py\", line 11, in\nfrom OpenSSL._util import (\nFile \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/OpenSSL/_util.py\", line 3, in\nfrom cryptography.hazmat.bindings.openssl.binding import Binding\nImportError: No module named cryptography.hazmat.bindings.openssl.binding\nmy pip freeze gives me this:\nJinja2==2.7.3\nMarkupSafe==0.23\nPillow==2.4.0\nPyYAML==3.11\nScrapy==0.24.4\nTwisted==14.0.2\nargparse==1.2.1\nawscli==1.4.4\nbackports.ssl-match-hostname==3.4.0.2\nbcdoc==0.12.2\nbeautifulsoup4==4.3.2\nbinaryornot==0.3.0\nbotocore==0.63.0\ncertifi==14.05.14\ncolorama==0.2.5\ncookiecutter==0.7.2\ncssselect==0.9.1\ndocutils==0.12\necdsa==0.11\neventbrite==0.45\ngnureadline==6.3.3\nhttplib2==0.9\nipython==2.1.0\njmespath==0.4.1\nlxml==3.3.5\nparamiko==1.15.0\npyOpenSSL==0.14\npyasn1==0.1.7\npycrypto==2.6.1\npython-dateutil==2.2\npyzmq==14.3.1\nqueuelib==1.1.1\nrequests==2.4.1\nrsa==3.1.2\nsix==1.7.3\nstevedore==1.0.0\nstripe==1.19.0\ntornado==4.0.2\nvboxapi==1.0\nvirtualenv==1.11.6\nvirtualenv-clone==0.2.5\nvirtualenvwrapper==4.3.1\nw3lib==1.10.0\nwsgiref==0.1.2\nzope.interface==4.1.1\nPlease let me know what to do next", "issue_status": "Closed", "issue_reporting_time": "2014-10-19T19:22:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1344": {"issue_url": "https://github.com/scrapy/scrapy/issues/921", "issue_id": "#921", "issue_summary": "Replace twisted logging with native python logging", "issue_description": "Member\ncurita commented on Oct 12, 2014\nI've created this issue since that functionality is referenced often between topics concerning logging and I didn't find a specific issue to track that (Closest is #433 but it's discussed in the comments).", "issue_status": "Closed", "issue_reporting_time": "2014-10-12T01:05:26Z", "fixed_by": "#1060", "pull_request_summary": "[MRG+1] Python logging", "pull_request_description": "Member\ncurita commented on Mar 2, 2015\nThis PR switches Twisted logging system to the Python standard one.\nIt's pretty much done, though it's missing documentation (UPDATE: All docs related to logging have been updated) since I want to hear feedback first. I implemented the bare minimum changes to support all previous use cases, but new functionality could definitely be added.\nThe pull request closes a few open issues and other PRs. This closes #921, closes #881, closes #877, closes #875, closes #483, closes #481, closes #433, and closes #265.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-29T18:20:34Z", "files_changed": [["30", "conftest.py"], ["2", "docs/index.rst"], ["17", "docs/intro/tutorial.rst"], ["30", "docs/topics/benchmarking.rst"], ["5", "docs/topics/debug.rst"], ["8", "docs/topics/downloader-middleware.rst"], ["9", "docs/topics/extensions.rst"]]}, "1345": {"issue_url": "https://github.com/scrapy/scrapy/issues/920", "issue_id": "#920", "issue_summary": "spider object not in shell", "issue_description": "aplicacionamedida commented on Oct 12, 2014\nIn the file shell.py I propose a change in this function \"inspect_response\":\ndef inspect_response(response, spider):\n    \"\"\"Open a shell to inspect the given response\"\"\"\n    Shell(spider.crawler).start(response=response)\nThis is my proposal:\ndef inspect_response(response, spider):\n    \"\"\"Open a shell to inspect the given response\"\"\"\n    Shell(spider.crawler).start(response=response, spider=spider)\nRationale: Without this change, when you call to the function inspect_response(), you obtain a None object inside the \"spider\" variable. None is the default value for the argument \"spider\" of the \"start()\" method of the Shell class.", "issue_status": "Closed", "issue_reporting_time": "2014-10-12T00:37:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1346": {"issue_url": "https://github.com/scrapy/scrapy/issues/913", "issue_id": "#913", "issue_summary": "Remove deprecated bin folder", "issue_description": "Contributor\nbrunsgaard commented on Oct 7, 2014\nAfter commit a8f45dc the bin folder can be removed as the entry_points keyword is now being used by setuptools to create the executable.\nI am not sure if this (removing the bin folder) has not happend because people like the bin folder instead of using setup install develop or if it has just been forgotten.", "issue_status": "Closed", "issue_reporting_time": "2014-10-07T00:15:07Z", "fixed_by": "#914", "pull_request_summary": "Deleted bin folder from root, fixes #913", "pull_request_description": "Contributor\nbrunsgaard commented on Oct 7, 2014\nPretty trivial, but fixes #913.", "pull_request_status": "Merged", "issue_fixed_time": "2014-10-09T16:58:36Z", "files_changed": [["4", "bin/scrapy"]]}, "1347": {"issue_url": "https://github.com/scrapy/scrapy/issues/910", "issue_id": "#910", "issue_summary": "HttpErrorMiddleware log level should be at least INFO, not DEBUG", "issue_description": "daveoncode commented on Oct 3, 2014\nI want to spot immediately if one of my spiders get 403 or 404 response codes. But the current implementation of HttpErrorMiddleware, sets log as DEBUG... debug?! It really doesn't matter if a spider is not able to scrape a page? It should matter! It should be a very important log to read!", "issue_status": "Closed", "issue_reporting_time": "2014-10-03T10:44:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1348": {"issue_url": "https://github.com/scrapy/scrapy/issues/908", "issue_id": "#908", "issue_summary": "Current caching of DjangoItem's instance property is buggy", "issue_description": "daveoncode commented on Sep 30, 2014\nThe current implementation of DjangoItem's instance should be rewritten in order to reflect further modifications of the item to the underlying django's model instance.\nThese are the steps to reproduce the issue:\ncreate a django item:\nitem = MyDjangoItem()\nset a field value:\nitem['foo'] = 1\nsave the item:\nmodel = item.save(commit=False)\nitem.instance is now cached and further modifications to the item are not reflected...\nthis returns \"1\"... and it's ok, because has been previously assigned to the instance\nprint model.foo\nnow... set a new item field\nitem['bar'] = 2\nthis prints None (because the underlying cache has not been updated!!)\nprint item.instance.bar\nIn conclusion... the cache should be purged each times the item is updated!\nCurrently I overrided the DjangoItem in this way to avoid the issue:\nfrom scrapy.contrib.djangoitem import DjangoItem as BaseDjangoItem\n\nclass DjangoItem(BaseDjangoItem):\n    def __setitem__(self, key, value):\n        self._instance = None\n        return super(DjangoItem, self).__setitem__(key, value)\n\n    def __delitem__(self, key):\n        self._instance = None\n        super(DjangoItem, self).__delitem__(key)", "issue_status": "Closed", "issue_reporting_time": "2014-09-30T08:52:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1349": {"issue_url": "https://github.com/scrapy/scrapy/issues/907", "issue_id": "#907", "issue_summary": "LinkExtractor chokes when only one link is bogus", "issue_description": "Contributor\nredapple commented on Sep 29, 2014\n_extract_links() either returns all extracted links (can be an empty list) or fails;\nIt would be nice to wrap a try/except and return what could be extracted and skip bogus links.\nExample session:\npaul@desktop:~$ scrapy shell\n2014-09-29 13:45:58+0200 [scrapy] INFO: Scrapy 0.24.4 started (bot: scrapybot)\n2014-09-29 13:45:58+0200 [scrapy] INFO: Optional features available: ssl, http11, boto\n2014-09-29 13:45:58+0200 [scrapy] INFO: Overridden settings: {'LOGSTATS_INTERVAL': 0}\n2014-09-29 13:45:58+0200 [scrapy] INFO: Enabled extensions: TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2014-09-29 13:45:58+0200 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2014-09-29 13:45:58+0200 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-09-29 13:45:58+0200 [scrapy] INFO: Enabled item pipelines: \n2014-09-29 13:45:58+0200 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2014-09-29 13:45:58+0200 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080\n[s] Available Scrapy objects:\n[s]   crawler    <scrapy.crawler.Crawler object at 0x7fab4bcd6fd0>\n[s]   item       {}\n[s]   settings   <scrapy.settings.Settings object at 0x7fab51714450>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser\n\nIn [1]: from scrapy.http import HtmlResponse\n\nIn [2]: r = HtmlResponse(body=\n'<html><body><a href=\"http://www.example.com/1\">link1</a><a href=\"http://www.example.com/2\">link2</a></body></html>', status=200, url=\"http://www.example.com\")\n\nIn [3]: from scrapy.contrib.linkextractors import LinkExtractor\n\nIn [4]: lx = LinkExtractor()\n\nIn [5]: lx.extract_links(r)\nOut[5]: \n[Link(url='http://www.example.com/1', text='link1', fragment='', nofollow=False),\n Link(url='http://www.example.com/2', text='link2', fragment='', nofollow=False)]\n\nIn [6]: r = HtmlResponse(body=\n'<html><body><a href=\"http://www.example.com/1\">link1</a><a href=\"http://[www.example.com/2\">link2</a></body></html>', status=200, url=\"http://www.example.com\")\n\nIn [7]: lx.extract_links(r)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-7-297e7bca14b8> in <module>()\n----> 1 lx.extract_links(r)\n\n/usr/local/lib/python2.7/dist-packages/scrapy/contrib/linkextractors/lxmlhtml.pyc in extract_links(self, response)\n    105         all_links = []\n    106         for doc in docs:\n--> 107             links = self._extract_links(doc, response.url, response.encoding, base_url)\n    108             all_links.extend(self._process_links(links))\n    109         return unique_list(all_links)\n\n/usr/local/lib/python2.7/dist-packages/scrapy/linkextractor.pyc in _extract_links(self, *args, **kwargs)\n     92 \n     93     def _extract_links(self, *args, **kwargs):\n---> 94         return self.link_extractor._extract_links(*args, **kwargs)\n\n/usr/local/lib/python2.7/dist-packages/scrapy/contrib/linkextractors/lxmlhtml.pyc in _extract_links(self, selector, response_url, response_encoding, base_url)\n     50         for el, attr, attr_val in self._iter_links(selector._root):\n     51             # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n---> 52             attr_val = urljoin(base_url, attr_val)\n     53             url = self.process_attr(attr_val)\n     54             if url is None:\n\n/usr/lib/python2.7/urlparse.pyc in urljoin(base, url, allow_fragments)\n    259             urlparse(base, '', allow_fragments)\n    260     scheme, netloc, path, params, query, fragment = \\\n--> 261             urlparse(url, bscheme, allow_fragments)\n    262     if scheme != bscheme or scheme not in uses_relative:\n    263         return url\n\n/usr/lib/python2.7/urlparse.pyc in urlparse(url, scheme, allow_fragments)\n    141     Note that we don't break the components up in smaller bits\n    142     (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n--> 143     tuple = urlsplit(url, scheme, allow_fragments)\n    144     scheme, netloc, url, query, fragment = tuple\n    145     if scheme in uses_params and ';' in url:\n\n/usr/lib/python2.7/urlparse.pyc in urlsplit(url, scheme, allow_fragments)\n    189                 if (('[' in netloc and ']' not in netloc) or\n    190                         (']' in netloc and '[' not in netloc)):\n--> 191                     raise ValueError(\"Invalid IPv6 URL\")\n    192             if allow_fragments and '#' in url:\n    193                 url, fragment = url.split('#', 1)\n\nValueError: Invalid IPv6 URL\n\nIn [8]: ", "issue_status": "Closed", "issue_reporting_time": "2014-09-29T11:47:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1350": {"issue_url": "https://github.com/scrapy/scrapy/issues/906", "issue_id": "#906", "issue_summary": "Scrapy.selector Enhancement Proposal", "issue_description": "zwidny commented on Sep 26, 2014\nMake scrapy.selector into a separate project.\nWhen we're scraping web pages, the most common task you need to perform is to extract data from the HTML source. There are several libraries available to achieve this: BeautifulSoup, lxml.\nAnd although scrapy selectors are built over the lxml library, it is a better way to extract data from HTML source. In my opinion, people will prefer selector to BeautifulSoup and lxml , if make the scrapy.selectors into a separate project,", "issue_status": "Closed", "issue_reporting_time": "2014-09-26T03:53:21Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1351": {"issue_url": "https://github.com/scrapy/scrapy/issues/901", "issue_id": "#901", "issue_summary": "Error: scrapy.webservice.WebService': No module named parse", "issue_description": "gitex commented on Sep 24, 2014\nHello. There is an error, which I can't get rid of. It prevents me from working.\nTraceback (most recent call last):\nFile \"C:/Users/Eugene/Scrapy/Test/Test_news.py\", line 37, in\ncrawler.configure()\nFile \"C:\\Python27\\lib\\site-packages\\scrapy\\crawler.py\", line 46, in configure\nself.extensions = ExtensionManager.from_crawler(self)\nFile \"C:\\Python27\\lib\\site-packages\\scrapy\\middleware.py\", line 50, in from_crawler\nreturn cls.from_settings(crawler.settings, crawler)\nFile \"C:\\Python27\\lib\\site-packages\\scrapy\\middleware.py\", line 29, in from_settings\nmwcls = load_object(clspath)\nFile \"C:\\Python27\\lib\\site-packages\\scrapy\\utils\\misc.py\", line 42, in load_object\nraise ImportError(\"Error loading object '%s': %s\" % (path, e))\nImportError: Error loading object 'scrapy.webservice.WebService': No module named parse\nWhat I've done:\nReinstall Scrapy.\nSearched for the same file \"webservice\". Another file no.\nChecked the original file \"webservice\" he has.", "issue_status": "Closed", "issue_reporting_time": "2014-09-24T06:27:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1352": {"issue_url": "https://github.com/scrapy/scrapy/issues/888", "issue_id": "#888", "issue_summary": "Handling corrupt image response in ImagesPipeline", "issue_description": "Contributor\npawelmhm commented on Sep 16, 2014\nCurrently when corrupt image response is downloaded, for example this one, and PIL fails to parse it PIL exception is propagated and raised with traceback here.\nI would suggest wrapping part that opens an image with PIL:\nImage.open(BytesIO(response.body))) \nwith try except block:\ntry:\n    Image.open(BytesIO(response.body))) \nexcept IOError as e:\n    raise ImageException(e.message)\nIn except block we would raise ImageException with original error message.\nThis seems logical to me in so far as we are already using ImageException for much less serious offence such as width smaller then minimal.\nMain benefit of this that this exception will be logged automatically as warning not as error with traceback, in my use case this is expected, client is getting unhandled errors in logs and is worried that he looses items, in reality nothing bad happens, just one of thousand images we download was corrupt and PIL failed to parse it. Developer can't really do much about it. Adding simple warning for cases like this seems much better to me.", "issue_status": "Closed", "issue_reporting_time": "2014-09-16T11:50:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1353": {"issue_url": "https://github.com/scrapy/scrapy/issues/887", "issue_id": "#887", "issue_summary": "ScrapyAgent should support infinite download timeout", "issue_description": "Contributor\nnyov commented on Sep 16, 2014\nThough the wisdom of this functionality is unclear,\nit would make sense if setting DOWNLOAD_TIMEOUT to zero, or None would in fact mean 'infinite' in accordance with other settings such as CLOSESPIDER_TIMEOUT, CLOSESPIDER_ITEMCOUNT et cetera.\nThe current ScrapyAgent source reads\ntimeout = request.meta.get('download_timeout') or self._connectTimeout\nwhere zero and None are false and invoke self._connectTimeout instead.\nAside from this, reactor.callLater(0, ...) would mean \"asap\" instead of \"never\", so this would need a different codepath.", "issue_status": "Closed", "issue_reporting_time": "2014-09-16T11:04:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1354": {"issue_url": "https://github.com/scrapy/scrapy/issues/883", "issue_id": "#883", "issue_summary": "[httpcache] FilesystemCacheStorage file organization", "issue_description": "larryxiao commented on Sep 14, 2014\nSituation\nI use FilesystemCacheStorage, and have saved about 1M requests, 30GB data (not sure, I'm still waiting for counting ...). And it can't proceed reporting no disk space, but I still have 10+GB on disk.\nInteresting thing is that I can't even use auto completion now.\n-bash: cannot create temp file for here-document: No space left on device\nComment\nI don't know if it is a valid use case, I use filecache to save call to APIs to allow reprocessing the response as needed (to lower burden on service).\nI think it's possible that fs has run out of inode. And the current implementation seem to create file for each request. Maybe can combine cache in save file.\nOr maybe the solution is to use database?", "issue_status": "Closed", "issue_reporting_time": "2014-09-14T07:48:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1355": {"issue_url": "https://github.com/scrapy/scrapy/issues/881", "issue_id": "#881", "issue_summary": "Accessing crawler from log before crawler is assigned to spider", "issue_description": "Contributor\nrocioar commented on Sep 11, 2014\nBug introduced in commit d513b5a\nTraceback:\nFile \"/Library/Python/2.7/site-packages/Scrapy-0.25.1-py2.7.egg/scrapy/log.py\", line 53, in _emit_with_crawler\n        ev = self._emit(eventDict)\n      File \"/Library/Python/2.7/site-packages/Scrapy-0.25.1-py2.7.egg/scrapy/log.py\", line 47, in _emit\n        self.crawler)\n      File \"/Library/Python/2.7/site-packages/Scrapy-0.25.1-py2.7.egg/scrapy/log.py\", line 83, in _adapt_eventdict\n        if crawler and (not spider or spider.crawler is not crawler):\n    exceptions.AttributeError: 'TestSpider' object has no attribute 'crawler'", "issue_status": "Closed", "issue_reporting_time": "2014-09-11T14:51:04Z", "fixed_by": "#1060", "pull_request_summary": "[MRG+1] Python logging", "pull_request_description": "Member\ncurita commented on Mar 2, 2015\nThis PR switches Twisted logging system to the Python standard one.\nIt's pretty much done, though it's missing documentation (UPDATE: All docs related to logging have been updated) since I want to hear feedback first. I implemented the bare minimum changes to support all previous use cases, but new functionality could definitely be added.\nThe pull request closes a few open issues and other PRs. This closes #921, closes #881, closes #877, closes #875, closes #483, closes #481, closes #433, and closes #265.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-29T18:20:34Z", "files_changed": [["30", "conftest.py"], ["2", "docs/index.rst"], ["17", "docs/intro/tutorial.rst"], ["30", "docs/topics/benchmarking.rst"], ["5", "docs/topics/debug.rst"], ["8", "docs/topics/downloader-middleware.rst"], ["9", "docs/topics/extensions.rst"]]}, "1356": {"issue_url": "https://github.com/scrapy/scrapy/issues/880", "issue_id": "#880", "issue_summary": "context is missing in selector", "issue_description": "nshigit commented on Sep 9, 2014\nscrapy shell http://basic.10jqka.com.cn/000635/holder.html\n>>> response.xpath('//*[@id=\"gdrsTable\"]/div/div/div[2]/table[1]/tbody/tr/th[1]/div')\n[]", "issue_status": "Closed", "issue_reporting_time": "2014-09-09T15:21:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1357": {"issue_url": "https://github.com/scrapy/scrapy/issues/877", "issue_id": "#877", "issue_summary": "Logging settings to stdout are overridden when using `log.start()`", "issue_description": "hartleybrody commented on Sep 6, 2014\nI was having issues suppressing loglines to stdout, as mentioned here:\nhttps://groups.google.com/forum/#!topic/scrapy-users/pJfN8BfF_fM\nThe logging settings I used were:\nLOG_LEVEL = 'WARNING'\nLOG_FILE = '/path/to/scrapy.log'\nLOG_STDOUT = False\nI was logging from a spider, and had set log.start(loglines=\"INFO\") in the spider's init method. When I tailed the LOG_FILE, I was only seeing messages that were WARNING or higher, but I was still seeing INFO on stdout.\n@nramirezuy made a great proof of concept here that shows the desired behavior:\nhttps://gist.github.com/nramirezuy/e75d8c041b07a8edb44f\nBut if you add a log.start() statement to the init method of the DummySpider, then you'll see everything on stdout, and only warnings and above in the log file.\n$> scrapy crawl dummy -s LOG_LEVEL=WARNING\n2014-09-05 17:13:48-0400 [scrapy] INFO: Scrapy 0.24.4 started (bot: chrome_store_crawler)\n2014-09-05 17:13:48-0400 [scrapy] INFO: Optional features available: ssl, http11\n2014-09-05 17:13:48-0400 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'crawler.spiders', 'LOG_LEVEL': 'WARNING', 'SPIDER_MODULES': ['crawler.spiders'], 'BOT_NAME': 'chrome_store_crawler', 'USER_AGENT': 'MagicMikeBot (+http://www.magicmike.io)', 'LOG_FILE': '/Users/hartley/Desktop/scrapy.log', 'DOWNLOAD_DELAY': 0.3}\n2014-09-05 17:13:48-0400 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2014-09-05 17:13:48-0400 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2014-09-05 17:13:48-0400 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-09-05 17:13:48-0400 [scrapy] INFO: Enabled item pipelines: CsvExporterPipeline\n2014-09-05 17:13:48-0400 [dummy] INFO: Spider opened\n2014-09-05 17:13:48-0400 [dummy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2014-09-05 17:13:49-0400 [scrapy] INFO: INFO\n2014-09-05 17:13:49-0400 [scrapy] WARNING: WARNING\n2014-09-05 17:13:49-0400 [scrapy] ERROR: ERROR\n2014-09-05 17:13:49-0400 [scrapy] CRITICAL: CRITICAL\n2014-09-05 17:13:49-0400 [dummy] INFO: Closing spider (finished)\n2014-09-05 17:13:49-0400 [dummy] INFO: Dumping Scrapy stats:\n    {'downloader/request_bytes': 219,\n     'downloader/request_count': 1,\n     'downloader/request_method_count/GET': 1,\n     'downloader/response_bytes': 1569,\n     'downloader/response_count': 1,\n     'downloader/response_status_count/200': 1,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2014, 9, 5, 21, 13, 49, 648787),\n     'log_count/CRITICAL': 1,\n     'log_count/ERROR': 1,\n     'log_count/WARNING': 1,\n     'response_received_count': 1,\n     'scheduler/dequeued': 1,\n     'scheduler/dequeued/memory': 1,\n     'scheduler/enqueued': 1,\n     'scheduler/enqueued/memory': 1,\n     'start_time': datetime.datetime(2014, 9, 5, 21, 13, 48, 914450)}\n2014-09-05 17:13:49-0400 [dummy] INFO: Spider closed (finished)\nThis might actually be a bug with Twisted, as I see Scrapy's log.msg() is a pretty thin wrapper around theirs. Just wanted to drop it here first as it might be a problem with Scrapy's use of their logger:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/log.py#L132", "issue_status": "Closed", "issue_reporting_time": "2014-09-05T21:21:39Z", "fixed_by": "#1060", "pull_request_summary": "[MRG+1] Python logging", "pull_request_description": "Member\ncurita commented on Mar 2, 2015\nThis PR switches Twisted logging system to the Python standard one.\nIt's pretty much done, though it's missing documentation (UPDATE: All docs related to logging have been updated) since I want to hear feedback first. I implemented the bare minimum changes to support all previous use cases, but new functionality could definitely be added.\nThe pull request closes a few open issues and other PRs. This closes #921, closes #881, closes #877, closes #875, closes #483, closes #481, closes #433, and closes #265.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-29T18:20:34Z", "files_changed": [["30", "conftest.py"], ["2", "docs/index.rst"], ["17", "docs/intro/tutorial.rst"], ["30", "docs/topics/benchmarking.rst"], ["5", "docs/topics/debug.rst"], ["8", "docs/topics/downloader-middleware.rst"], ["9", "docs/topics/extensions.rst"]]}, "1358": {"issue_url": "https://github.com/scrapy/scrapy/issues/875", "issue_id": "#875", "issue_summary": "log_observer is beign started twice within a crawl", "issue_description": "Contributor\nnramirezuy commented on Sep 4, 2014\nThe order of execution is as listed:\nhttps://github.com/scrapy/scrapy/pull/816/files#diff-017ca5ab6671590721d197e95de3cea3R91\nhttps://github.com/scrapy/scrapy/pull/816/files#diff-017ca5ab6671590721d197e95de3cea3R130\n/cc @curita", "issue_status": "Closed", "issue_reporting_time": "2014-09-03T20:04:12Z", "fixed_by": "#1060", "pull_request_summary": "[MRG+1] Python logging", "pull_request_description": "Member\ncurita commented on Mar 2, 2015\nThis PR switches Twisted logging system to the Python standard one.\nIt's pretty much done, though it's missing documentation (UPDATE: All docs related to logging have been updated) since I want to hear feedback first. I implemented the bare minimum changes to support all previous use cases, but new functionality could definitely be added.\nThe pull request closes a few open issues and other PRs. This closes #921, closes #881, closes #877, closes #875, closes #483, closes #481, closes #433, and closes #265.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-29T18:20:34Z", "files_changed": [["30", "conftest.py"], ["2", "docs/index.rst"], ["17", "docs/intro/tutorial.rst"], ["30", "docs/topics/benchmarking.rst"], ["5", "docs/topics/debug.rst"], ["8", "docs/topics/downloader-middleware.rst"], ["9", "docs/topics/extensions.rst"]]}, "1359": {"issue_url": "https://github.com/scrapy/scrapy/issues/874", "issue_id": "#874", "issue_summary": "Enabled extensions, middlewares, pipelines", "issue_description": "Contributor\nnramirezuy commented on Sep 4, 2014\nI found that this information isn't being printed anymore.\nThe responsible of this bug is this line, seems that spider is None on eventDict at the moment the components are instantiated.\nI'm not sure how to fix it because I 'm not quite sure what it is attempting to block.\n/cc @curita", "issue_status": "Closed", "issue_reporting_time": "2014-09-03T20:00:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1360": {"issue_url": "https://github.com/scrapy/scrapy/issues/872", "issue_id": "#872", "issue_summary": "FileFeedStorage creates empty file when no items are scraped", "issue_description": "gbirke commented on Sep 2, 2014\nWhen no items are scraped, the corresponding file is created none the less, because it is created in by the storage.open call in FeedExporter.open_spider. This behavior ignores the the setting of FEED_STORE_EMPTY when using file export.\nMy proposal for this would be to add a cleanup method to the IFeedStorage interface. Then FeedExporter.close_spider can call that method before returning in case slot.itemcount is zero and self.store_empty is False. cleanup could also be called internally from the store methods of the IFeedStorage implementations.", "issue_status": "Closed", "issue_reporting_time": "2014-09-02T15:12:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1361": {"issue_url": "https://github.com/scrapy/scrapy/issues/870", "issue_id": "#870", "issue_summary": "Fail to extract correct text from \u2018dd\u2019 if there's a left bracket inside the text", "issue_description": "antxash commented on Aug 31, 2014\nThe example html looks like the attachment.\nIf using xpath to extract the content \"just a test a < 10.5\" (without space inside)\n\nresponse.xpath(\"//dd/text()\").extract()[0]\n\nIt just return\n\nu' just a test a\\n\\n'", "issue_status": "Closed", "issue_reporting_time": "2014-08-31T12:21:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1362": {"issue_url": "https://github.com/scrapy/scrapy/issues/867", "issue_id": "#867", "issue_summary": "archive.scrapy.org apt key wrong", "issue_description": "Contributor\nnyov commented on Aug 29, 2014\n(Doesn't really belong here though I know no other place to put it.)\nhttp://archive.scrapy.org/ubuntu/archive.key isn't actually the key used for signing.\npub   2048R/90E2741A 2010-06-17 [expires: 2020-06-14]\nuid                  Scrapinghub (Package Signing Key) <info@scrapinghub.com>\nsub   2048R/D998B9C8 2010-06-17 [expires: 2020-06-14]\nThe key that is in fact used, is still this one:\npub   2048R/627220E7 2014-01-18 [expires: 2024-01-16]\nuid                  Scrapy Team (APT Signing Key) <info@scrapy.org>\nLuckily I have this older key on another machine I could export it from, but it would be nice to have this corrected.\nIn case anyone else needs the old key for now, here it is:\ncat << EOF | sudo apt-key add -\n-----BEGIN PGP PUBLIC KEY BLOCK-----\nVersion: GnuPG v1.4.12 (GNU/Linux)\n\nmQENBFLa0R4BCADSYpSARBQ3IQE2nxgXyqHITI73wh5EZ7tuKx+f94ChSPHPRSCw\negTYtFLU4Js3OBnqmg1IOLULkObmgLyjyQ5O+kLBrIdDmKbvGY6vr5Ey6WfIVvPp\nBbLmMG3fNg3BjLRaPPLp5QOvw5oJcPUmEjdcOmGAzJlurbBZ6gIyOWZ+9AeJTa6Z\nkJ47j5W5DQbct9u60YXWFA/EZ1X+mcr0gC7EXF3RWIi6TI3KmDsQAoVDOg8i4Hwx\n9DwvQvB7+aW5IYt5+kiM6RIx5ZNXDcAOg4MTTfV77t3kBwJ2XGO+XMFF9ABwImxw\nt0QiBYYokjs3sdepfkapQcTnO1G5hogX562pABEBAAG0L1NjcmFweSBUZWFtIChB\nUFQgU2lnbmluZyBLZXkpIDxpbmZvQHNjcmFweS5vcmc+iQE+BBMBAgAoBQJS2tEe\nAhsDBQkSzAMABgsJCAcDAgYVCAIJCgsEFgIDAQIeAQIXgAAKCRCPYssfYnIg5666\nCACgoUel/UqE28T8V6pvslgEKKC7UcSL8gbXvA0ZQ5qDVZ635p+vA8NmcRnxzm1n\nfCPhh9T2Asfr6zKtAQV1ro+he59xoqahT9h0vfNkaeFD8t73YO+JP6cBethfaDc4\nDxl3Kr2ayb5vmLkNr+z08A6SxhxNIUjfL9qc2QilGVQTAT4wlttHvn/UQFhN4vin\nO+fCVUX9G2foUgL2KeoxwWobPPMPLasqv6Jc4NLVBQ79KXxEOgrwCG4+6sVOsmzf\nlxlCaeSGrJJ4DLTFAA00onhLrNJ0Io7nE3odYylvAz7oqXg4UbMecUQwRDpah/Dg\naZkLFj09mmntq2GfexMxvKQ4\n=TYhN\n-----END PGP PUBLIC KEY BLOCK-----\nEOF", "issue_status": "Closed", "issue_reporting_time": "2014-08-29T05:07:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1363": {"issue_url": "https://github.com/scrapy/scrapy/issues/862", "issue_id": "#862", "issue_summary": "Error with scrapyd on commit jsonrpc webservice", "issue_description": "antwal commented on Aug 20, 2014\nHi,\nI have see the file utils/txweb.py is deleted and scrapyd not working.\na9292cf\nworking with code of 15 Aug.\nThanks.", "issue_status": "Closed", "issue_reporting_time": "2014-08-20T00:49:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1364": {"issue_url": "https://github.com/scrapy/scrapy/issues/857", "issue_id": "#857", "issue_summary": "boto versioning and bucket", "issue_description": "garrettvee commented on Aug 15, 2014\nDocumentation states that boto must be installed if Amazon S3 is to be used. However, I was wondering what version(s) is/are supported.\nAlso, while setting up an S3 bucket with a private policy, the FilesPipeline fails since the key's contents are set with 'public-read' per (\nscrapy/scrapy/contrib/pipeline/files.py\nLine 76 in f6b1e9b\n POLICY = 'public-read' \n)\nI'd like to propose a configuration which defaults to 'public-read'\nscrapy version -v\nUserWarning: You do not have the service_identity module installed. Please install it from <https://pypi.python.org/pypi/service_identity>. Without the service_identity module and a recent enough pyOpenSSL tosupport it, Twisted can perform only rudimentary TLS client hostnameverification.  Many valid certificate/hostname mappings may be rejected.\n  verifyHostname, VerificationError = _selectVerifyImplementation()\nScrapy  : 0.24.2\nlxml    : 3.3.5.0\nlibxml2 : 2.9.0\nTwisted : 14.0.0\nPython  : 2.7.3 (default, Oct 15 2013, 11:34:37) - [GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.2.78)]\nPlatform: Darwin-13.3.0-x86_64-i386-64bit", "issue_status": "Closed", "issue_reporting_time": "2014-08-14T21:12:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1365": {"issue_url": "https://github.com/scrapy/scrapy/issues/855", "issue_id": "#855", "issue_summary": "canonicalize_url : IDNA encoding for the netloc", "issue_description": "jvanasco commented on Aug 13, 2014\nfirst off, thanks for writing this function. it seems to be the best url standardizer i've found in python.\none small suggestion for improvement though -- the netloc could/should be standardized further.\nnow that there are internationalized domain names, the urls could either be the utf-8 encoded version or the punycode encoded idna version.\nfor example, I ran into this url shortener:\nnetloc_utf8 = \"\u27a1.ws\"\nscrapy canonicalizes it into:\n %E2%9E%A1.ws\nhowever the idna version (which most browsers not only support, but actually rewrite the percent-encoded version into) is this:\nxn--hgi.ws\nthis creates a problem, because the netloc isn't really standardized -- there are at least 2 versions that are possible.\nso it would be great if the function could be trivially improved.\nit would also be acceptable to convert IDNA domains to unicode and then to percent encoded, but I think the IDNA structure is more convenient as most web browsers prefer it as the native option.\nin any event, the standard library does all the hard work :\ndef uft8_to_idna(txt):\n    try:\n        txt = unicode(txt, 'utf-8')\n        txt = txt.encode('idna')\n    except UnicodeDecodeError:\n        raise\n    return txt\n\ndef unicode_to_idna(txt):\n    try:\n        txt = txt.encode('idna')\n    except UnicodeDecodeError:\n        raise\n    return txt", "issue_status": "Closed", "issue_reporting_time": "2014-08-13T18:25:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1366": {"issue_url": "https://github.com/scrapy/scrapy/issues/852", "issue_id": "#852", "issue_summary": "Handling exceptions in scrapy", "issue_description": "elmkarami commented on Aug 12, 2014 \u2022\nedited\nI'm working on a project with scrapy for a while now, and i wanted to integrate sentry,\nI've used scrapy-sentry but it does not work at all\ni tried also to implement it using Extensions but it works only if an error occurred in the spider's callback (not pipelines.py, items.py)...\nfrom scrapy import signals\n\nfrom raven import Client\n\n\nclass FailLogger(object):\n    client = Client(settings.get('SENTRY_DSN'))\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        ext = cls()\n\n        crawler.signals.connect(ext.spider_error, signal=signals.spider_error)\n        return ext\n\n    def spider_error(self, failure, response, spider):\n        try:\n            failure.raiseException()\n        except:\n            self.client.get_ident(self.client.captureException())\nis there any that i can log errors (in spiders, items, pipelines ...) to sentry, like in Django?\nThank you.", "issue_status": "Closed", "issue_reporting_time": "2014-08-12T11:28:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1367": {"issue_url": "https://github.com/scrapy/scrapy/issues/850", "issue_id": "#850", "issue_summary": "service_identity python module not listed as default dependency", "issue_description": "fconcklin commented on Aug 11, 2014\nI would add a pip requirement for service_identity", "issue_status": "Closed", "issue_reporting_time": "2014-08-11T08:52:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1368": {"issue_url": "https://github.com/scrapy/scrapy/issues/849", "issue_id": "#849", "issue_summary": "get_project_settings() failing with no error", "issue_description": "tanzaho commented on Aug 10, 2014\nI just struggled for a long time with my Django installation to understand that my problem was coming from Scrapy. When using scrapy as a script and following http://doc.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script , I could not get Django manage.py to run. No error messages were displayed.\nI traced down to a bad configuration on my scrapy.cfg [settings] path. That was a simple mistake, however, I was not told, at any moment that I used a wrong path.\nI suggest adding some exceptions in utils/project.py for the function get_project_settings().\nps : sorry if I'm not too clear, it's probably the first time I submit an issue/suggestion through GH.", "issue_status": "Closed", "issue_reporting_time": "2014-08-09T22:41:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1369": {"issue_url": "https://github.com/scrapy/scrapy/issues/848", "issue_id": "#848", "issue_summary": "ImportError: No module named tests.spiders running scrapy startproject", "issue_description": "larubbio commented on Aug 9, 2014\nI just installed 0.24.3 and anytime I run the scrapy cli I get 'ImportError: No module named tests.spiders'\nLooking at the source in the egg there is no spiders.py in the tests dir. Is this a bad package build?\nenv)21:12 rob@zippy:~/code/abett/Scrapy (master)$ pip install scrapy\nDownloading/unpacking scrapy\n  Downloading Scrapy-0.24.3-py2-none-any.whl (317kB): 317kB downloaded\nRequirement already satisfied (use --upgrade to upgrade): pyOpenSSL in ./env/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.5.2 in ./env/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): queuelib in ./env/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): lxml in ./env/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): w3lib>=1.8.0 in ./env/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cssselect>=0.9 in ./env/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): Twisted>=10.0.0 in ./env/lib/python2.7/site-packages (from scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cryptography>=0.2.1 in ./env/lib/python2.7/site-packages (from pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): zope.interface>=3.6.0 in ./env/lib/python2.7/site-packages (from Twisted>=10.0.0->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): cffi>=0.8 in ./env/lib/python2.7/site-packages (from cryptography>=0.2.1->pyOpenSSL->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in ./env/lib/python2.7/site-packages (from zope.interface>=3.6.0->Twisted>=10.0.0->scrapy)\nRequirement already satisfied (use --upgrade to upgrade): pycparser in ./env/lib/python2.7/site-packages (from cffi>=0.8->cryptography>=0.2.1->pyOpenSSL->scrapy)\nInstalling collected packages: scrapy\nSuccessfully installed scrapy\nCleaning up...\n(env)21:12 rob@zippy:~/code/abett/Scrapy (master)$ scrapy\nTraceback (most recent call last):\n  File \"/Users/rob/code/abett/Scrapy/env/bin/scrapy\", line 11, in <module>\n    sys.exit(execute())\n  File \"/Users/rob/code/abett/Scrapy/env/lib/python2.7/site-packages/scrapy/cmdline.py\", line 122, in execute\n    cmds = _get_commands_dict(settings, inproject)\n  File \"/Users/rob/code/abett/Scrapy/env/lib/python2.7/site-packages/scrapy/cmdline.py\", line 46, in _get_commands_dict\n    cmds = _get_commands_from_module('scrapy.commands', inproject)\n  File \"/Users/rob/code/abett/Scrapy/env/lib/python2.7/site-packages/scrapy/cmdline.py\", line 29, in _get_commands_from_module\n    for cmd in _iter_command_classes(module):\n  File \"/Users/rob/code/abett/Scrapy/env/lib/python2.7/site-packages/scrapy/cmdline.py\", line 20, in _iter_command_classes\n    for module in walk_modules(module_name):\n  File \"/Users/rob/code/abett/Scrapy/env/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 68, in walk_modules\n    submod = import_module(fullpath)\n  File \"/usr/local/Cellar/python/2.7.8/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/Users/rob/code/abett/Scrapy/env/lib/python2.7/site-packages/scrapy/commands/bench.py\", line 2, in <module>\n    from scrapy.tests.spiders import FollowAllSpider\nImportError: No module named tests.spiders\n(env)21:14 rob@zippy:~/code/abett/Scrapy (master)$ ls /Users/rob/code/abett/Scrapy/env/lib/python2.7/site-packages/scrapy/tests\nsample_data/        test_utils_misc/", "issue_status": "Closed", "issue_reporting_time": "2014-08-09T04:14:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1370": {"issue_url": "https://github.com/scrapy/scrapy/issues/847", "issue_id": "#847", "issue_summary": "Handle multiple same headers in response", "issue_description": "fangpenlin commented on Aug 9, 2014\nI encountered an issue when writing a spider with scrapy. Some websites, they writes multiple Set-Cookie in HTTP headers. Like this\nSet-Cookie: foo=bar\nSet-Cookie: eggs=spam\nSince the http headers is implemented as a dict like object, which means, only one key will survive in that dict. The last one will do. So the first Set-Cookie will be simply ignored.\nThe situation is mentioned here in this question - Set more than one HTTP header with the same name?\nI think this bug is from Twisted http agent, but since scrapy maintains a copy of twisted http agent, and reasonable usage for multiple header might only be cookie, so I think it's better to address the issue here. The way dict implemented is already widely used, mapping string from string is also widely used, it's pretty hard to change it (like use a list instead or set value as list of string instead of string). I think a better approach is to provide a new raw_headers attribute attached on response object. For cookie handler, it should scan Set-Cookie in the raw_headers instead of headers.\nI may create a pull request soon.", "issue_status": "Closed", "issue_reporting_time": "2014-08-08T23:50:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1371": {"issue_url": "https://github.com/scrapy/scrapy/issues/842", "issue_id": "#842", "issue_summary": "Support ::nth(n) and ::last extension for CSS selectors (non-standard)", "issue_description": "Contributor\nredapple commented on Aug 5, 2014\nIt would be nice to support CSS selectors based on positional conditions and that are not covered by ::nth-child() and ::nth-of-type()\n(triggered by a discussion with @krotkiewicz)\nExample HTML input:\n<div>\n<p class=\"red\">1</p>\n<p class=\"red\">2</p>\n<p class=\"blue\">3</p>\n<p class=\"red\">4</p>\n<p class=\"red\">5</p>\n<p class=\"blue\">6</p>\n</div>\n::nth(N) and ::last are only suggested names\n.red::nth(3) would select <p class=\"red\">4</p>\n.blue::nth(1) would select <p class=\"blue\">3</p>\n.red::last would select <p class=\"red\">5</p>\nRespective XPath expressions:\ndescendant-or-self::p[@class and contains(concat(' ', normalize-space(@class), ' '), ' red ')][3]\ndescendant-or-self::p[@class and contains(concat(' ', normalize-space(@class), ' '), ' blue ')][1]\ndescendant-or-self::p[@class and contains(concat(' ', normalize-space(@class), ' '), ' red ')][last()]\n::first could also be added as a shortcut to ::nth(1)\nSupporting this means extending cssselect like it was done for ::attr() and ::text\nReferences:\nhttp://stackoverflow.com/questions/10921809/css3-nth-of-type-restricted-to-class/10931957#10931957", "issue_status": "Closed", "issue_reporting_time": "2014-08-05T13:37:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1372": {"issue_url": "https://github.com/scrapy/scrapy/issues/838", "issue_id": "#838", "issue_summary": "Links with a preceding space are not parsed correctly.", "issue_description": "jpswade commented on Aug 2, 2014\nFor example:\nImagine on this page http://www.example.co.uk/?post=123 we have this markup:\n<a href=\" http://test.example.co.uk/\">Test</a>\nIt's subtle but there's a preceding space before the URL in the href.\nInstead of crawling to the URL http://test.example.co.uk/ it crawls this:\nhttp://www.example.co.uk/?post=123%20http://test.example.co.uk/\nFurther more, if this page exists, then it may likely contain the same link, which means there's a link like this:\nhttp://www.example.co.uk/?post=123%20http://test.example.co.uk/%20http://test.example.co.uk/\nThis causes an infinite loop.\nEssentially, the solution is to trim URLs before crawling them.", "issue_status": "Closed", "issue_reporting_time": "2014-08-02T11:11:53Z", "fixed_by": "#2547", "pull_request_summary": "[MRG+1] LinkExtractors: strip whitespaces", "pull_request_description": "Member\nkmike commented on Feb 8, 2017 \u2022\nedited\nI thought this fixes #838, but after re-reading the issue it was not only about link extractors :) But LinkExtractors should be fixed anyways, because html5 rules are clear.\nSee also: https://github.com/scrapy/scrapy/issues/1614, #1021, #1603.\nThis PR conflicts with #2537 - when one of them is merged the other have to be fixed.", "pull_request_status": "Merged", "issue_fixed_time": "2017-02-20T14:08:33Z", "files_changed": [["11", "docs/topics/link-extractors.rst"], ["2", "requirements-py3.txt"], ["2", "requirements.txt"], ["8", "scrapy/linkextractors/htmlparser.py"], ["18", "scrapy/linkextractors/lxmlhtml.py"], ["3", "scrapy/linkextractors/regex.py"], ["14", "scrapy/linkextractors/sgml.py"], ["2", "setup.py"], ["1", "tests/sample_data/link_extractor/sgml_linkextractor.html"], ["3", "tests/test_linkextractors.py"], ["20", "tests/test_linkextractors_deprecated.py"]]}, "1373": {"issue_url": "https://github.com/scrapy/scrapy/issues/831", "issue_id": "#831", "issue_summary": "'scrapy' is not recognized as an internal or external command, operable program or batch file.", "issue_description": "jpswade commented on Jul 31, 2014\nHi there,\nI've installed Python 2.7.2 on Windows 7 (32-bit), easyinstall and pip, as well as Scrapy using pip.\nWhen I open a command prompt and issue \"scrapy\" I am met with this error:\n'scrapy' is not recognized as an internal or external command,\noperable program or batch file.\nI've checked my %PATH% which has both the Python27 directory and the Python27 Scripts directory which is where the scrapy \"binary\" resides.\nC:\\>echo %PATH%\nC:\\Windows\\system32;C:\\Windows;C:\\Windows\\system32\\wbem;C:\\Program Files\\Common\nFiles\\Microsoft Shared\\Windows Live;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C\n:\\Program Files\\Windows Live\\Shared;C:\\Program Files\\TortoiseGit\\bin;C:\\Program\nFiles\\TortoiseSVN\\bin;C:\\Program Files\\QuickTime\\QTSystem;C:\\usr\\bin;c:\\Program\nFiles\\Microsoft SQL Server\\100\\Tools\\Binn\\;c:\\Program Files\\Microsoft SQL Server\n\\100\\DTS\\Binn\\;C:\\HashiCorp\\Vagrant\\bin;C:\\Program Files\\Git\\cmd;C:\\Program File\ns\\Git\\bin;C:\\Program Files\\Nmap;C:\\ProgramData\\chocolatey\\bin;C:\\Python27;C:\\Pyt\nhon27\\Scripts;\nThis is not the first instance of this issue I've seen:\nhttp://stackoverflow.com/questions/4988297/trying-to-get-scrapy-into-a-project-to-run-crawl-command\nhttps://groups.google.com/forum/#!topic/scrapy-users/NQGltXNNt1I\nhttps://www.youtube.com/watch?v=eEK2kmmvIdw\nYes the scrapy file exists:\nC:\\Python27\\Scripts>dir scrapy\n Volume in drive C has no label.\n Volume Serial Number is ACF1-762D\n\n Directory of C:\\Python27\\Scripts\n\n31/07/2014  13:58                72 scrapy\n               1 File(s)             72 bytes\n               0 Dir(s)   8,137,617,408 bytes free\nYes it has contents:\nC:\\Python27\\Scripts>type scrapy\n#!C:\\Python27\\python.exe\n\nfrom scrapy.cmdline import execute\nexecute()\nYes I'm using a normal windows command line prompt:\nMicrosoft Windows [Version 6.1.7601]\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\nYes I've rebooted.\nI can't put my finger on what I'm missing here...", "issue_status": "Closed", "issue_reporting_time": "2014-07-31T16:24:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1374": {"issue_url": "https://github.com/scrapy/scrapy/issues/826", "issue_id": "#826", "issue_summary": "Possible extension: machine learning classifier-based pipeline with scikit", "issue_description": "johncadigan commented on Jul 29, 2014\nI've made a classifier-based pipeline system for scrapy, and I was wondering if it could be featured as an extension. I'd appreciate any feedback:\nhttps://github.com/johncadigan/sciscrapy\nMy next step will be integrating it with the command system for scrapy. That seems like the most reasonable way to make it a viable extension. The packaging and unit-tests will come last.", "issue_status": "Closed", "issue_reporting_time": "2014-07-29T17:50:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1375": {"issue_url": "https://github.com/scrapy/scrapy/issues/825", "issue_id": "#825", "issue_summary": "dont_... meta flags should handle False values", "issue_description": "Member\nkmike commented on Jul 29, 2014\nCurrently several dont_... flags don't work as one could expect if their value is False: dont_merge_cookies=False is the same as dont_merge_cookies=True - only the presence of a flag is checked. What about checking actual values?\nAffected options:\ndont_merge_cookies #846\ndont_redirect\ndont_retry\ndont_cache from #821\nThere is also 'dont_filter' argument for Request; it handles False values.", "issue_status": "Closed", "issue_reporting_time": "2014-07-29T15:05:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1376": {"issue_url": "https://github.com/scrapy/scrapy/issues/824", "issue_id": "#824", "issue_summary": "working spider under linux is not working under windows", "issue_description": "armujahid commented on Jul 29, 2014\nHI I have scrapy 0.25.1 in windows. I have moved my working spider from linux to windows but it is not working in windows. I am using scrapy crawl myspider -o output.csv. It is producing empty csv file. I am not a noob. I am returning Item object from my spider so it should work. Same code is working flawlessly under my kali linux. Please help", "issue_status": "Closed", "issue_reporting_time": "2014-07-29T13:14:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1377": {"issue_url": "https://github.com/scrapy/scrapy/issues/823", "issue_id": "#823", "issue_summary": "Content-length not set when body is not specified", "issue_description": "juanriaza commented on Jul 29, 2014\nPOST requests with empty body lacks of Content-length header. Default behaviour should be to set that header with a value of zero.\nAlso worth noting that documentation states that:\nIf body is not given,, an empty string is stored\nBut implementation differs.", "issue_status": "Closed", "issue_reporting_time": "2014-07-29T12:08:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1378": {"issue_url": "https://github.com/scrapy/scrapy/issues/822", "issue_id": "#822", "issue_summary": "Can't find manytomanyfield in djangoitem", "issue_description": "Flowerowl commented on Jul 29, 2014\nIs there any way to handle manytomanyfield by djangoitems without use model directly?", "issue_status": "Closed", "issue_reporting_time": "2014-07-29T06:49:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1379": {"issue_url": "https://github.com/scrapy/scrapy/issues/813", "issue_id": "#813", "issue_summary": "crawlspider doesn't listen deny rule", "issue_description": "muhasturk commented on Jul 22, 2014\nHere is my spider\nfrom scrapy.contrib.linkextractors import LinkExtractor\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.selector import Selector\nfrom vitrinbot.items import ProductItem\nfrom vitrinbot.base import utils\nimport hashlib\n\nremoveCurrency = utils.removeCurrency\ngetCurrency = utils.getCurrency\n\nclass NautilusSpider(CrawlSpider):\n    name = 'nautilus'\n    allowed_domains = ['nautilusconcept.com']\n    start_urls = ['http://www.nautilusconcept.com/']\n    xml_filename = 'nautilus-%d.xml'\n    xpaths = {\n        'category' :'//tr[@class=\"KategoriYazdirTabloTr\"]//a/text()',\n        'title':'//h1[@class=\"UrunBilgisiUrunAdi\"]/text()',\n        'price':'//hemenalfiyat/text()',\n        'images':'//td[@class=\"UrunBilgisiUrunResimSlaytTd\"]//div/a/@href',\n        'description':'//td[@class=\"UrunBilgisiUrunBilgiIcerikTd\"]//*/text()',\n        'currency':'//*[@id=\"UrunBilgisiUrunFiyatiDiv\"]/text()',\n        'check_page':'//div[@class=\"ayrinti\"]'\n    }\n\n    rules = (\n\n        Rule(\n            LinkExtractor(allow=('com/[\\w_]+',),\n\n                          deny=('asp$',\n                                'login\\.asp'\n                                'hakkimizda\\.asp',\n                                'musteri_hizmetleri\\.asp',\n                                'iletisim_formu\\.asp',\n                                'yardim\\.asp',\n                                'sepet\\.asp',\n                                'catinfo\\.asp\\?brw',\n                          ),\n            ),\n            callback='parse_item',\n            follow=True\n        ),\n\n    )\n\n\n    def parse_item(self, response):\n        i = ProductItem()\n        sl = Selector(response=response)\n\n        if not sl.xpath(self.xpaths['check_page']):\n            return i\n\n        i['id'] = hashlib.md5(response.url.encode('utf-8')).hexdigest()\n        i['url'] = response.url\n        i['category'] = \" > \".join(sl.xpath(self.xpaths['category']).extract()[1:-1])\n        i['title'] = sl.xpath(self.xpaths['title']).extract()[0].strip()\n        i['special_price'] = i['price'] = sl.xpath(self.xpaths['price']).extract()[0].strip().replace(',','.')\n\n        images = []\n        for img in sl.xpath(self.xpaths['images']).extract():\n            images.append(\"http://www.nautilusconcept.com/\"+img)\n        i['images'] = images\n\n        i['description'] = (\" \".join(sl.xpath(self.xpaths['description']).extract())).strip()\n\n        i['brand'] = \"Nautilus\"\n\n        i['expire_timestamp']=i['sizes']=i['colors'] = ''\n\n        i['currency'] = sl.xpath(self.xpaths['currency']).extract()[0].strip()\n\n        return i\nHere is the peace of log\n2014-07-22 17:39:31+0300 [nautilus] DEBUG: Crawled (200) <GET http://www.nautilusconcept.com/catinfo.asp?brw=0&cid=64&direction=&kactane=100&mrk=1&offset=-1&order=&src=&typ=> (referer: http://www.nautilusconcept.com/catinfo.asp?brw=0&cid=64&direction=&kactane=100&mrk=1&offset=&offset=&order=&src=&stock=1)\n2014-07-22 17:39:31+0300 [nautilus] DEBUG: Crawled (200) <GET http://www.nautilusconcept.com/catinfo.asp?brw=0&chkBeden=&chkMarka=&chkRenk=&cid=64&direction=1&kactane=100&mrk=1&offset=-1&order=prc&src=&stock=1&typ=> (referer: http://www.nautilusconcept.com/catinfo.asp?brw=0&cid=64&direction=&kactane=100&mrk=1&offset=&offset=&order=&src=&stock=1)\n2014-07-22 17:39:32+0300 [nautilus] DEBUG: Crawled (200) <GET http://www.nautilusconcept.com/catinfo.asp?brw=0&chkBeden=&chkMarka=&chkRenk=&cid=64&direction=1&kactane=100&mrk=1&offset=-1&order=name&src=&stock=1&typ=> (referer: http://www.nautilusconcept.com/catinfo.asp?brw=0&cid=64&direction=&kactane=100&mrk=1&offset=&offset=&order=&src=&stock=1)\n2014-07-22 17:39:32+0300 [nautilus] DEBUG: Crawled (200) <GET http://www.nautilusconcept.com/catinfo.asp?brw=&chkBeden=&chkMarka=&chkRenk=&cid=64&direction=2&kactane=100&mrk=1&offset=-1&order=prc&src=&stock=1&typ=7> (referer: http://www.nautilusconcept.com/catinfo.asp?brw=&cid=64&direction=1&kactane=100&mrk=1&offset=-1&order=prc&src=&stock=1&typ=7)\n2014-07-22 17:39:32+0300 [nautilus] DEBUG: Crawled (200) <GET http://www.nautilusconcept.com/catinfo.asp?brw=&chkBeden=&chkMarka=&chkRenk=&cid=64&direction=2&kactane=100&mrk=1&offset=-1&order=name&src=&stock=1&typ=7> (referer: http://www.nautilusconcept.com/catinfo.asp?brw=&cid=64&direction=1&kactane=100&mrk=1&offset=-1&order=prc&src=&stock=1&typ=7)\n2014-07-22 17:39:33+0300 [nautilus] DEBUG: Crawled (200) <GET http://www.nautilusconcept.com/catinfo.asp?brw=0&chkBeden=&chkMarka=&chkRenk=&cid=64&cmp=&direction=1&grp=&kactane=100&model=&mrk=1&offset=-1&order=prc&src=&stock=1&typ=7> (referer: http://www.nautilusconcept.com/catinfo.asp?brw=&cid=64&direction=1&kactane=100&mrk=1&offset=-1&order=prc&src=&stock=1&typ=7)\n2014-07-22 17:39:33+0300 [nautilus] DEBUG: Crawled (200) <GET http://www.nautilusconcept.com/catinfo.asp?brw=1&chkBeden=&chkMarka=&chkRenk=&cid=64&cmp=&direction=1&grp=&kactane=100&model=&mrk=1&offset=-1&order=prc&src=&stock=1&typ=7> (referer: http://www.nautilusconcept.com/catinfo.asp?brw=&cid=64&direction=1&kactane=100&mrk=1&offset=-1&order=prc&src=&stock=1&typ=7)\n2014-07-22 17:39:33+0300 [nautilus] DEBUG: Crawled (200) <GET http://www.nautilusconcept.com/catinfo.asp?brw=1&cid=64&direction=1&kactane=100&mrk=1&offset=-1&order=name&src=&typ=7> (referer: http://www.nautilusconcept.com/catinfo.asp?brw=1&chkBeden=&chkMarka=&chkRenk=&cid=64&cmp=&direction=1&grp=&kactane=100&model=&mrk=1&offset=-1&order=name&src=&stock=1&typ=7)\nI don't want spider to try to crawl links that contains /\" catinfo.asp?brw \"/ but spider doesn't listen deny rule. Where is the problem?\nI'm using Scrapy==0.24.2 and python 2.7.6", "issue_status": "Closed", "issue_reporting_time": "2014-07-22T15:08:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1380": {"issue_url": "https://github.com/scrapy/scrapy/issues/808", "issue_id": "#808", "issue_summary": "Unable to run scrapy bench", "issue_description": "Moataz-E commented on Jul 17, 2014\nHello, I am trying to run scrapy bench but I keep getting this error. I tried on both Linux Mint and Ubuntu.\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/scrapy/tests/mockserver.py\", line 198, in <module>\n    os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/ssl.py\", line 104, in __init__\n    self.cacheContext()\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/ssl.py\", line 113, in cacheContext\n    ctx.use_certificate_file(self.certificateFileName)\nOpenSSL.SSL.Error: [('system library', 'fopen', 'No such file or directory'), ('BIO routines', 'FILE_CTRL', 'system lib'), ('SSL routines', 'SSL_CTX_use_certificate_file', 'system lib')]", "issue_status": "Closed", "issue_reporting_time": "2014-07-17T17:45:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1381": {"issue_url": "https://github.com/scrapy/scrapy/issues/807", "issue_id": "#807", "issue_summary": "ImportError: No module named cmdline", "issue_description": "MickeyBadBad commented on Jul 17, 2014\nC:\\Python27\\Lib\\site-packages and C:\\Python27 already added in PATH . still got this\ni have no idea .... any one can help ?", "issue_status": "Closed", "issue_reporting_time": "2014-07-17T16:53:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1382": {"issue_url": "https://github.com/scrapy/scrapy/issues/806", "issue_id": "#806", "issue_summary": "scrapyd deploy with slash in branch name causes problems", "issue_description": "kevinkirkup commented on Jul 17, 2014\nIf you try to deploy a project with scrapyd-deploy and your current branch has a slash in the name, it will say the deploy was successful, but doesn't update the project.\nExample Branch Name: feature/some-feature\nChanging the branch name solve the problem.", "issue_status": "Closed", "issue_reporting_time": "2014-07-17T04:47:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1383": {"issue_url": "https://github.com/scrapy/scrapy/issues/794", "issue_id": "#794", "issue_summary": "Scrapy shell[ipython] - NameError exception when using already imported module within a lambda", "issue_description": "Contributor\nandrix commented on Jul 10, 2014\nI was doing some testing on Scrapy shell and I found this bug (in Ipython standalone works OK):\n$ scrapy shell # shell must call ipython\n$ import re\n$ f1 = lambda n: re.match('xxx', n, re.IGNORECASE)\n$ f1('')\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n<ipython-input-4-a3ec2df8ae39> in <module>()\n----> 1 f1('')\n\n<ipython-input-3-8945a0244946> in <lambda>(n)\n----> 1 f1 = lambda n: re.match('serving', n, re.IGNORECASE)\n\nNameError: global name 're' is not defined\nI have confirmed this issue with @kmike and @kalessin.", "issue_status": "Closed", "issue_reporting_time": "2014-07-10T16:13:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1384": {"issue_url": "https://github.com/scrapy/scrapy/issues/790", "issue_id": "#790", "issue_summary": "Funny issue with MapCompose and itereable check.", "issue_description": "AndreiPashkin commented on Jul 9, 2014\nMapCompose class uses arg_to_iter function (link) with processed values.\narg_to_iter check if value is container or not with such method:\nhasattr(arg, '__iter__')\nIm oftenly pass lxml.html.HtmlElement instances through processors and use MapCompose with them. But instances of this class have attribute __iter__, and they are not being wrapped into list, so they are treated as iterables, and MapCompose iterates over children of an HtmlElement breaking inteded low.\nOne workaround comes to mind - is to use facilitating processor that would wrap all HtmlElements into lists.\nBut I think, it is better to replace check in arg_to_iter to\nisinstance(arg, (list, tuple))\nBecause it is more explicit, and would ensure, that such things as I described will never happen.", "issue_status": "Closed", "issue_reporting_time": "2014-07-09T17:40:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1385": {"issue_url": "https://github.com/scrapy/scrapy/issues/789", "issue_id": "#789", "issue_summary": "Debian packages for 0.25 shows 0.23.0 version", "issue_description": "Member\ndangra commented on Jul 9, 2014\nciting @cyberplant and myself from chat history:\nUnpacking scrapy-0.25 (from .../scrapy-0.25_0.23.0-340-g4bb0989+1404859260_all.deb) ...\nProcessing triggers for man-db ...\nSetting up scrapy-0.25 (0.23.0-340-g4bb0989+1404859260) ...\nProcessing triggers for python-support ...\n# scrapy\nScrapy 0.23.0-340-g4bb0989 - no active project\n\nthe version is inferred from git tags, the tag for 0.25.0 is missing and that's why it picks 0.23.0, but tagging 0.25.0 will trigger pypi deployment in travis-ci.\nThat's why I didn't fix yet, have to find a condition in .travis.yml to skip odd minor versions at https://github.com/scrapy/scrapy/blob/master/.travis.yml#L35. The condition is supposed to be a bash-like expression\n\ntravis docs are a bit scarse in this topic, and travis source code doesn't help too much to me. the only way to find a working condition is by trial and error but that takes time because you have to wait for travis on every commit", "issue_status": "Closed", "issue_reporting_time": "2014-07-09T04:54:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1386": {"issue_url": "https://github.com/scrapy/scrapy/issues/788", "issue_id": "#788", "issue_summary": "version -0.24.1 test failures; test_ajax_url, DocTestFailure", "issue_description": "idella commented on Jul 8, 2014\nplatform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\nplugins: twisted\ncollected 1008 items / 14 skipped\n\n================================= FAILURES==========================\n______________________________________________________________ RequestTest.test_ajax_url _______________________________________________________________\n\nself = <scrapy.tests.test_http_request.RequestTest testMethod=test_ajax_url>\n\n    def test_ajax_url(self):\n        # ascii url\n        r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n>       self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\")\nE       AssertionError: 'http://www.example.com/ajax.html?_escaped_fragment_=key=value' != 'http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n\n../../../../work/scrapy-0.24.1-python2_7/scrapy/tests/test_http_request.py:111: AssertionError\n____________________________________________________________ FormRequestTest.test_ajax_url _____________________________________________________________\n\nself = <scrapy.tests.test_http_request.FormRequestTest testMethod=test_ajax_url>\n\n    def test_ajax_url(self):\n        # ascii url\n        r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n>       self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\")\nE       AssertionError: 'http://www.example.com/ajax.html?_escaped_fragment_=key=value' != 'http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n\n../../../../work/scrapy-0.24.1-python2_7/scrapy/tests/test_http_request.py:111: AssertionError\n___________________________________________________________ XmlRpcRequestTest.test_ajax_url ____________________________________________________________\n\nself = <scrapy.tests.test_http_request.XmlRpcRequestTest testMethod=test_ajax_url>\n\n    def test_ajax_url(self):\n        # ascii url\n        r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n>       self.assertEqual(r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\")\nE       AssertionError: 'http://www.example.com/ajax.html?_escaped_fragment_=key=value' != 'http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n\n../../../../work/scrapy-0.24.1-python2_7/scrapy/tests/test_http_request.py:111: AssertionError\n________________________________________________________ [doctest] scrapy.utils.url.escape_ajax ________________________________________________________\n080     \"\"\"\n081     Return the crawleable url according to:\n082     http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n083 \n084     >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\nExpected:\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\nGot:\n    'www.example.com/ajax.html?_escaped_fragment_=key=value'\n\n/mnt/gen2/TmpDir/portage/dev-python/scrapy-0.24.1/work/scrapy-0.24.1-python2_7/scrapy/utils/url.py:84: DocTestFailure\n========= 4 failed, 1001 passed, 17 skipped in 229.15 seconds ========\nsed -e s':test_ajax_url:_&:' -i scrapy/tests/test_http_request.py takes out the first 3 failures. 4th. is a single doctest fail.", "issue_status": "Closed", "issue_reporting_time": "2014-07-08T08:32:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1387": {"issue_url": "https://github.com/scrapy/scrapy/issues/785", "issue_id": "#785", "issue_summary": "failed to re-schedule a request when download failed", "issue_description": "dabing1205 commented on Jul 4, 2014\nHi,\nI have a question about how to re-schedule a request which failed due to download error, such as proxy issue, I have request a question in stackoverflow, and I also would like to have a answer from here. I paste the link here, any suggestion and comments are high appreciated.\nRegards,\nBi", "issue_status": "Closed", "issue_reporting_time": "2014-07-04T05:08:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1388": {"issue_url": "https://github.com/scrapy/scrapy/issues/784", "issue_id": "#784", "issue_summary": "new IPolicyForHTTPS interface on Twisted 14.0.0", "issue_description": "Member\ndangra commented on Jul 3, 2014\nThis ticket is to track the work needed to remove the following deprecation warning introduced in Twisted 14.0.0 while still be compatible with older Twisted versions\nlib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py:160: DeprecationWarning: <scrapy.core.downloader.contextfactory.ScrapyClientContextFactory instance at 0x10a441b48> was passed as the HTTPS policy for an Agent, but it does not provide IPolicyForHTTPS.  Since Twisted 14.0, you must pass a provider of IPolicyForHTTPS.\n  connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)", "issue_status": "Closed", "issue_reporting_time": "2014-07-03T14:39:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1389": {"issue_url": "https://github.com/scrapy/scrapy/issues/783", "issue_id": "#783", "issue_summary": "Something wrong with the service_identity.", "issue_description": "xmduhan commented on Jul 3, 2014\nWhen I start scrapy(0.24.1) in ubuntu(12.04) shell,I found that is a warning,it suggest to install a service_identity module. I follow it.But when service_identity installation is finished,the scrapy do not work again.It complain the following:\n----------------------------------- Error Stack----------------------------------------------\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 4, in\nexecute()\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 122, in execute\ncmds = _get_commands_dict(settings, inproject)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 46, in _get_commands_dict\ncmds = _get_commands_from_module('scrapy.commands', inproject)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 29, in _get_commands_from_module\nfor cmd in _iter_command_classes(module):\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 20, in _iter_command_classes\nfor module in walk_modules(module_name):\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/misc.py\", line 68, in walk_modules\nsubmod = import_module(fullpath)\nFile \"/usr/lib/python2.7/importlib/init.py\", line 37, in import_module\nimport(name)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/bench.py\", line 3, in\nfrom scrapy.tests.mockserver import MockServer\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/tests/mockserver.py\", line 6, in\nfrom twisted.internet import reactor, defer, ssl\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/ssl.py\", line 223, in\nfrom twisted.internet._sslverify import (\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/_sslverify.py\", line 184, in\nverifyHostname, VerificationError = _selectVerifyImplementation()\nFile \"/usr/local/lib/python2.7/dist-packages/twisted/internet/_sslverify.py\", line 159, in _selectVerifyImplementation\nfrom service_identity import VerificationError\nFile \"/usr/local/lib/python2.7/dist-packages/service_identity/init.py\", line 11, in\nfrom . import pyopenssl\nFile \"/usr/local/lib/python2.7/dist-packages/service_identity/pyopenssl.py\", line 12, in\nfrom pyasn1_modules.rfc2459 import GeneralNames\nFile \"/usr/local/lib/python2.7/dist-packages/pyasn1_modules/rfc2459.py\", line 72, in\nclass AttributeValue(univ.Any): pass\nAttributeError: 'module' object has no attribute 'Any'\n----------------------------------- End Error Stack----------------------------------------------", "issue_status": "Closed", "issue_reporting_time": "2014-07-03T07:52:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1390": {"issue_url": "https://github.com/scrapy/scrapy/issues/780", "issue_id": "#780", "issue_summary": "_nons function is not used in scrapy.contrib.linkextractors.lxmlhtml", "issue_description": "Member\nkmike commented on Jul 3, 2014\nThere is either a bug or some stray code in https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/linkextractors/lxmlhtml.py#L37: tag = _nons(el.tag) local variable is not used, and so _nons function is also unused. @redapple - what was the intended behavior?", "issue_status": "Closed", "issue_reporting_time": "2014-07-02T22:26:30Z", "fixed_by": "#791", "pull_request_summary": "Extract links from XHTML documents with MIME-Type \"application/xml\"", "pull_request_description": "Contributor\nredapple commented on Jul 10, 2014\n\"application/xhtml+xml\" is already interpreted as HTML\nand link extractor is fine with it to extract links.\nOnly for XML documents can the namespaces in tags be an issue.\nFixes #780", "pull_request_status": "Merged", "issue_fixed_time": "2014-07-11T20:07:54Z", "files_changed": [["30", "scrapy/contrib/linkextractors/lxmlhtml.py"], ["47", "scrapy/tests/test_contrib_linkextractors.py"]]}, "1391": {"issue_url": "https://github.com/scrapy/scrapy/issues/778", "issue_id": "#778", "issue_summary": "rename str_to_unicode and unicode_to_str functions", "issue_description": "Member\nkmike commented on Jul 3, 2014\nI think that unicode_to_str and str_to_unicode names are misleading: unicode_to_str can accept input which is not unicode, and 'str' is ambiguous in Python 2.x / 3.x; str_to_unicode can accept input which is not str.\nWhat about renaming them to text_to_bytes and text_to_unicode? \"Text\" will mean \"either a bytestring or a unicode string\".\nDjango uses force_bytes / force_text names for similar utilities; the difference is that they acept any objects, not just bytes/unicode as input.\n\"Text\" means \"unicode\" in Django names. This makes sense because not all bytestrings are a representation of text. I think that for Scrapy bytestrings acceptable by text_to_bytes and text_to_unicode functions are representation of text, so \"either a bytestring or a unicode string\" meaning is OK in Scrapy.", "issue_status": "Closed", "issue_reporting_time": "2014-07-02T19:49:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1392": {"issue_url": "https://github.com/scrapy/scrapy/issues/774", "issue_id": "#774", "issue_summary": "Move JSON-RPC interface to separate package", "issue_description": "Member\npablohoffman commented on Jul 2, 2014\nWhat do you think about moving the JSON-RPC interface (ie. scrapy.webservice) to a separate scrapy-jsonrpc package?. Aside from working fine and having tests, I don't think it's something that Scrapy users use much (or at all). It is a good proven mechanism for IPC and something that scrapyd could use (but never did), but I'm not sure Scrapy is the place to leave such large piece of functionality, which involves the following files:\nscrapy/webservice.py - the JSON-RPC web service\ndocs/topics/webservice.rst - documentation\nscrapy/utils/jsonrpc.py - JSON-RPC helpers\nscrapy/tests/test_utils_jsonrpc.py - tests\nextras/scrapy-ws.py - a JSON-RPC command line client\nI think a separate package could bring more visibility to this (reasonable piece of) functionality and possibly ease its maintenance. If/when scrapyd decides to use JSON-RPC for communicating with scrapy processes, it can include the scrapy-jsonrpc package as dependency.\nThoughts?", "issue_status": "Closed", "issue_reporting_time": "2014-07-01T20:11:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1393": {"issue_url": "https://github.com/scrapy/scrapy/issues/769", "issue_id": "#769", "issue_summary": "CrawlerSettings is broken in scrapy 0.24", "issue_description": "Member\nkmike commented on Jun 27, 2014\nThis line raises an exception because overrides became a property: https://github.com/scrapy/scrapy/blob/master/scrapy/settings/__init__.py#L135", "issue_status": "Closed", "issue_reporting_time": "2014-06-27T13:11:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1394": {"issue_url": "https://github.com/scrapy/scrapy/issues/765", "issue_id": "#765", "issue_summary": "How to use customized selector?", "issue_description": "hszcg commented on Jun 26, 2014\nHi, i am new to scrapy recently. And i found that there used to be an option to set default selectors in https://github.com/scrapy/scrapy/blob/master/scrapy/selector/__init__.py\nHowever, in the latest version, i notice that this feature have already been dropped.\nI totally understand that sticking to lxml only will make the codebase much cleaner and simpler. However, i wonder that what if we want to use some customized selector, e.g. pyquery. (joehillen@6301adc)\nI mean i wonder whether scrapy could provide a easy interface for us to overwrite the default configuration.\nThanks in advance.", "issue_status": "Closed", "issue_reporting_time": "2014-06-26T09:22:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1395": {"issue_url": "https://github.com/scrapy/scrapy/issues/763", "issue_id": "#763", "issue_summary": "Broken test suite after LxmlLinkExtractor promotion", "issue_description": "Member\ndangra commented on Jun 26, 2014\n$ trial scrapy.tests.test_crawl.CrawlTestCase\nscrapy.tests.test_crawl\n  CrawlTestCase\n    test_delay ...                                                         [OK]\n    test_engine_status ...                                                 [OK]\n    test_follow_all ...                                                  [FAIL]\n    test_referer_header ...                                                [OK]\n    test_retry_503 ...                                                     [OK]\n    test_retry_conn_aborted ...                                            [OK]\n    test_retry_conn_failed ...                                             [OK]\n    test_retry_conn_lost ...                                               [OK]\n    test_retry_dns_error ...                                               [OK]\n    test_start_requests_bug_before_yield ...                               [OK]\n    test_start_requests_bug_yielding ...                                   [OK]\n    test_start_requests_dupes ...                                          [OK]\n    test_start_requests_lazyness ...                                       [OK]\n    test_timeout_failure ...                                               [OK]\n    test_timeout_success ...                                               [OK]\n    test_unbounded_response ...                                            [OK]\n\n===============================================================================\n[FAIL]\nTraceback (most recent call last):\n  File \"/home/daniel/envs/scrapy/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1099, in _inlineCallbacks\n    result = g.send(result)\n  File \"/home/daniel/src/scrapy/scrapy/tests/test_crawl.py\", line 26, in test_follow_all\n    self.assertEqual(len(spider.urls_visited), 11)  # 10 + start_url\n  File \"/home/daniel/envs/scrapy/lib/python2.7/site-packages/twisted/trial/_synctest.py\", line 356, in assertEqual\n    % (msg, pformat(first), pformat(second)))\ntwisted.trial.unittest.FailTest: not equal:\na = 31\nb = 11\n\n\nscrapy.tests.test_crawl.CrawlTestCase.test_follow_all\n-------------------------------------------------------------------------------\nRan 16 tests in 90.150s\n\nFAILED (failures=1, successes=15)\n\n$ trial scrapy.tests.test_crawl.CrawlTestCase.test_follow_all\nscrapy.tests.test_crawl\n  CrawlTestCase\n    test_follow_all ...                                                    [OK]\n\n-------------------------------------------------------------------------------\nRan 1 tests in 0.874s\n\nPASSED (successes=1)", "issue_status": "Closed", "issue_reporting_time": "2014-06-25T18:55:35Z", "fixed_by": "#764", "pull_request_summary": "no need to keep links as instance attribute", "pull_request_description": "Member\ndangra commented on Jun 26, 2014\nAttempt to fix #763", "pull_request_status": "Merged", "issue_fixed_time": "2014-06-25T19:29:15Z", "files_changed": [["11", "scrapy/contrib/linkextractors/lxmlhtml.py"]]}, "1396": {"issue_url": "https://github.com/scrapy/scrapy/issues/759", "issue_id": "#759", "issue_summary": "_", "issue_description": "ghost commented on Jun 25, 2014\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2014-06-25T07:36:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1397": {"issue_url": "https://github.com/scrapy/scrapy/issues/758", "issue_id": "#758", "issue_summary": "provide wheels", "issue_description": "Member\nkmike commented on Jun 24, 2014\nI think it is a good time to start providing wheels on pypi. There is not too many advantages for pure-python packages, but it is not hard to provide a wheel, and it makes installation faster (3.5s vs 2.6s in my local tests, with all data cached locally and without installing deps). Also, for some reason wheels are smaller than tar.gz archives - for Scrapy 0.23 it is 673K vs 744K, so downloading will also become a bit faster.\nTo create a wheel there is no need to add anything to setup.cfg now; pip install wheel; python setup.py bdist_wheel should do the right thing (until we gain Python 3 support).", "issue_status": "Closed", "issue_reporting_time": "2014-06-24T03:59:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1398": {"issue_url": "https://github.com/scrapy/scrapy/issues/755", "issue_id": "#755", "issue_summary": "SGMLParseError", "issue_description": "bijzz commented on Jun 22, 2014\nI often have problems with the SgmlLinkExtractor. Lets try:\nscrapy shell \"http://www.dachser.com/de/de/\"\n# in the shell\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nlink_extractor = SgmlLinkExtractor()\nlink_extractor.extract_links(response)\nThis will raise an exception with SGMLParseError: expected name token at '<!/IoRangeRedDotMode'. Whats happening here?", "issue_status": "Closed", "issue_reporting_time": "2014-06-22T08:27:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1399": {"issue_url": "https://github.com/scrapy/scrapy/issues/754", "issue_id": "#754", "issue_summary": "RobotsTxtMiddleware doesn't support wildcards in Disallow rules", "issue_description": "mattfullerton commented on Jun 22, 2014\nThis is because, as I understand, Python's robotparser module doesn't support them either. There is an alternative, drop-in module, Robotexclusionrulesparser:\nhttp://nikitathespider.com/python/rerp/\nThis line would need to be changed:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/robotstxt.py#L7", "issue_status": "Closed", "issue_reporting_time": "2014-06-21T20:51:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1400": {"issue_url": "https://github.com/scrapy/scrapy/issues/753", "issue_id": "#753", "issue_summary": "Select CSS classes using Xpath", "issue_description": "shirk3y commented on Jun 19, 2014\nCSS, or Xpath. That is the question. Even if you are CSS lover, you probably want more. XPath has the most what you probably want, but unfortunately, when you have to select by CSS class, expression looks like:\n//p[contains(concat( \" \", @class, \" \" ), concat( \" \", \"mb-author\", \" \" ))]\nor\ndescendant-or-self::p[@class and contains(concat(' ', normalize-space(@class), ' '), ' mb-author ')]\nIsn't it painful if we select by many classes in one expression? For example the CSS expression p.mb-content.initial.rendered, div.content is converted to something like this:\ndescendant-or-self::p[@class and contains(concat(' ', normalize-space(@class), ' '), ' mb-content ') and (@class and contains(concat(' ', normalize-space(@class), ' '), ' initial ')) and (@class and contains(concat(' ', normalize-space(@class), ' '), ' rendered '))]\nWhy not extend Xpath to handle CSS classes more easily and less verbose? The example above could be expressed in following way:\n//p[has-class(\"p.mb-content\", \"initial\", \"rendered\")]\nI've already done first step in this gist: https://gist.github.com/shirk3y/458224083ce5464627bc\nIf you like the idea, maybe it can be easily ported to Scrapy selector", "issue_status": "Closed", "issue_reporting_time": "2014-06-18T19:04:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1401": {"issue_url": "https://github.com/scrapy/scrapy/issues/749", "issue_id": "#749", "issue_summary": "scrapy shell segmentation fault", "issue_description": "franciscosamuel commented on Jun 16, 2014\nI'm getting segmentation fault when trying out xpath queries through scrapy shell.\nWhen using scrapy shell with any URL I always get a segmentation fault on the second experiment, for example with a \"simple\" page as google homepage:\nscrapy shell \"https://www.google.pt\"\n'>>> sel.xpath(\"//\")\n'>>> sel.xpath(\"//\")\nhttp://pastebin.com/7heNRzG9\nI'm using Mac OSX Mavericks with the stock lxml lib. Here is OSX's information about the crash: http://pastebin.com/2eSMRBZF", "issue_status": "Closed", "issue_reporting_time": "2014-06-15T19:20:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1402": {"issue_url": "https://github.com/scrapy/scrapy/issues/748", "issue_id": "#748", "issue_summary": "Support for socks5 proxy", "issue_description": "cydu commented on Jun 14, 2014\nSupport for socks5 proxy\nhttp://www.ietf.org/rfc/rfc1928.txt", "issue_status": "Closed", "issue_reporting_time": "2014-06-14T14:49:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1403": {"issue_url": "https://github.com/scrapy/scrapy/issues/744", "issue_id": "#744", "issue_summary": "OffsiteMiddleWare fails to process spider output if allowed_domains contains None", "issue_description": "Contributor\ncrlane commented on Jun 12, 2014\nIf you dynamically generate allowed domains (e.g., using urlparse(url).hostname) for your spiders, it is possible to generate a list of domains where some elements of the list are None. In our case it was because of some poorly formatted URLs in a large dataset.\nThis causes the OffsiteMiddleware object to error out when trying compile the regex for allowed hosts and the self.host_regex is never set for the OffsiteMiddleware instance. This is because of a call to re.escape(d) in get_host_regex, where d is some iterable. Since None is not iterable, we see two errors, one for the failing call to re.escape, and then subsequently lots of\n AttributeError: 'OffsiteMiddleware' object has no attribute 'host_regex' \nFor spiders that have a more than one url and allowed domain, this means that processing stops as soon as the first None (or really non-iterable) domain is encountered, i.e., it crashes the whole process and no results will be collected for that spider.\nI suppose it is also possible that something other non-iterable could be there, but it's most likely going to be a string or None. So the pull request is a quick fix that protects against None in the regex builder and a test to ensure that the spider doesn't crash if it encounters None in the list of allowed domains.", "issue_status": "Closed", "issue_reporting_time": "2014-06-12T15:41:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1404": {"issue_url": "https://github.com/scrapy/scrapy/issues/739", "issue_id": "#739", "issue_summary": "Random JSON formatting error, not consistent", "issue_description": "akurtovic commented on Jun 2, 2014\nI have an issue with JSON output where (seemingly at random) a few entries are corrupt. The script uses several spiders to crawl a list of news websites and pull a headline and link. As you can see below, there is one corrupt JSON entry amid a total of more than 70 entries that worked fine.\n...\n{\"headline\": \"Rocks Thrown From Festus Overpass Damage Vehicles\", \"link\": \"http://stlouis.cbslocal.com/2014/05/29/rocks-thrown-from-festus-overpass-damage-vehicles/\"}\nr-man-who-robbed-car-wash-shot-employee-261327581.html\"}\n{\"headline\": \"Police arrest man accused of threatening to kill wife with machete\", \"link\": \"http://www.kmov.com/news/crime/Man-accused-of-wielding-machete-threatening-to-kill-wife-261311461.html\"}\n...\nThis error is not consistent so I don't how to recreate it. When I re-ran the same project just a minute later, that particular entry was not corrupt anymore. And sometimes the JSON output file is completely clean, while at other times there are 3-4 errors out of 50-70 articles. This has happened on many attempts over the last couple of days. I thought it was a encoding issue or something in the URL field, but the JSON lines sometimes break on the headline field as well.\nBelow is my pipelines class (the project is here: https://github.com/akurtovic/Scrapy-Projects/tree/master/crimelog)\nclass JsonWriterPipeline(object):\n\n    def __init__(self):\n        self.file = codecs.open('crimelog.json', 'wb', encoding='utf-8')\n\n    def process_item(self, item, spider):\n        line = json.dumps(dict(item), ensure_ascii=False) + \"\\n\"\n        self.file.write(line)\n        return item", "issue_status": "Closed", "issue_reporting_time": "2014-06-02T17:15:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1405": {"issue_url": "https://github.com/scrapy/scrapy/issues/736", "issue_id": "#736", "issue_summary": "Sort spider names on 'scrapy list' command", "issue_description": "martnst commented on May 28, 2014\nHey there,\ni think the spider names on the scrapy list command should be order by name in the console output.", "issue_status": "Closed", "issue_reporting_time": "2014-05-28T10:46:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1406": {"issue_url": "https://github.com/scrapy/scrapy/issues/734", "issue_id": "#734", "issue_summary": "ItemLoader doesn't pass the loader context to input processors", "issue_description": "realmhamdy commented on May 25, 2014\nI have a couple of spiders here that reproduce this issue. I'll use the smaller one\nHere's spider.py. What's interesting is the last 4 lines:\nclass JobSpider(Spider):\n    start_urls = [\"http://www.egyptitjobs.com\"]\n    name = \"jobspider\"\n\n    def parse(self, response):\n        sel = Selector(response)\n        site_logo_url = sel.xpath(\"//a[@id='logo']/img/@src\").extract()[0]\n        if site_logo_url.startswith('/'):\n            site_logo_url = JobSpider.start_urls[0] + site_logo_url\n        #yield SiteLogo(image_urls = [site_logo_url])\n        job_links = sel.xpath(\"//a[contains(@class, 'jbj_title') and contains(@class, 'jobcriteria')]\")\n        job_target_urls = job_links.xpath(\"@href\").extract()\n        job_titles = job_links.xpath(\"text()\").extract()\n        for (job_title, job_target) in zip(job_titles, job_target_urls):\n            if job_target.startswith(\"/\"):\n                job_target = JobSpider.start_urls[0] + job_target\n            item_loader = MyItemLoader(EgyitjobsItem(), response=response, context=\"dacontext\")\n            item_loader.add_value(\"job_title\", job_title)\n            item_loader.add_value(\"job_url\", job_target)\n            yield item_loader.load_item()\nand now itemloaders.py:\ndef contextLogger(scraped, loaderContext):\n  logfile.write(\"loaderContext: %s\" % loaderContext)\n  return scraped\n\nclass MyItemLoader(ItemLoader):\n  job_title_in = MapCompose(contextLogger)\nUsing this simple MyItemLoader causes exceptions.TypeError: contextLogger() takes exactly 2 arguments (1 given).\nScrapy 0.22.2, Python 2.7.6\nHere's the whole spider project:\nhttps://www.dropbox.com/s/ase4s9wu84npshn/crawler.zip", "issue_status": "Closed", "issue_reporting_time": "2014-05-25T03:31:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1407": {"issue_url": "https://github.com/scrapy/scrapy/issues/731", "issue_id": "#731", "issue_summary": "SGMLParseError when looking for links with SGMLLinkExtractor", "issue_description": "dmoisset commented on May 24, 2014\nI'm having an exception when extracting links for a site. It can be reproduced by:\n$ scrapy shell 'http://www.cnea.gov.ar/'\n>>> from scrapy.contrib.linkextractors import sgml\n>>> e = sgml.SgmlLinkExtractor()\n>>> e.extract_links(response)\nThat produces the following traceback:\n---------------------------------------------------------------------------\nSGMLParseError                            Traceback (most recent call last)\n<ipython-input-3-276218303eda> in <module>()\n----> 1 e.extract_links(response)\n\n/home/dmoisset/caf/env/local/lib/python2.7/site-packages/scrapy/contrib/linkextractors/sgml.pyc in extract_links(self, response)\n    127             body = response.body\n    128\n--> 129         links = self._extract_links(body, response.url, response.encoding, base_url)\n    130         links = self._process_links(links)\n    131         return links\n\n/home/dmoisset/caf/env/local/lib/python2.7/site-packages/scrapy/contrib/linkextractors/sgml.pyc in _extract_links(self, response_text, response_url, response_encoding, base_url)\n     27         \"\"\" Do the real extraction work \"\"\"\n     28         self.reset()\n---> 29         self.feed(response_text)\n     30         self.close()\n     31\n\n/usr/lib/python2.7/sgmllib.pyc in feed(self, data)\n    102\n    103         self.rawdata = self.rawdata + data\n--> 104         self.goahead(0)\n    105\n    106     def close(self):\n\n/usr/lib/python2.7/sgmllib.pyc in goahead(self, end)\n    172                     # deployed,\" this should only be the document type\n    173                     # declaration (\"<!DOCTYPE html...>\").\n--> 174                     k = self.parse_declaration(i)\n    175                     if k < 0: break\n    176                     i = k\n\n/usr/lib/python2.7/markupbase.pyc in parse_declaration(self, i)\n    138             else:\n    139                 self.error(\n--> 140                     \"unexpected %r char in declaration\" % rawdata[j])\n    141             if j < 0:\n    142                 return j\n\n/usr/lib/python2.7/sgmllib.pyc in error(self, message)\n    109\n    110     def error(self, message):\n--> 111         raise SGMLParseError(message)\n    112\n    113     # Internal -- handle data as far as reasonable.  May leave state\n\nSGMLParseError: unexpected '=' char in declaration\nThis looks like some kind of problem with the doctype, except that I checked the beginning of response.body and it has a perfectly valid XHTML 1.0 transitional doctype", "issue_status": "Closed", "issue_reporting_time": "2014-05-23T21:48:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1408": {"issue_url": "https://github.com/scrapy/scrapy/issues/729", "issue_id": "#729", "issue_summary": "FilesPipeline FILES_STORE paths and scrapyd deployment", "issue_description": "hxu commented on May 23, 2014\nI've been working on a deployment of a spider that uses the FilesPipeline, and encountered a sort of 'gotcha' with respect to where files get downloaded when it gets deployed to scrapyd.\nWhen developing, I can set the FILES_STORE to whatever suits my development environment. However, when I deploy this to scrapyd, this setting gets deployed with my project and spider. If the path of FILES_STORE isn't already created on the scrapyd server, then the files will fail to save.\nIt seems like where the FilesPipeline stores files should be overridden by the settings in the scrapyd server. In the current situation, I have to make sure to set the FILES_STORE setting to something that works for the deployment server before I deploy, then switch it back when I'm in my dev environment. Is there a better way to do this?\n(I wasn't sure if this ticket was better suited for scrapy or scrapyd)", "issue_status": "Closed", "issue_reporting_time": "2014-05-23T10:58:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1409": {"issue_url": "https://github.com/scrapy/scrapy/issues/728", "issue_id": "#728", "issue_summary": "get_func_args maximum recursion", "issue_description": "Contributor\nnramirezuy commented on May 21, 2014\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/utils/python.py#L149\nToday I was working on a project were I have to skip the first item of a list, and then join the rest. Instead of writing the typical slice I tried something much more good looking Compose(itemgetter(slice(1, None)), Join()) but I found out this maximum recursion. I did some research and ask @dangra about it, but nothing came up.\nI think the main problem is that inspect isn't able recognize itemgetter as something.\n>>> inspect.getmembers(itemgetter(2))\n[('__call__',\n  <method-wrapper '__call__' of operator.itemgetter object at 0x7f79aeffb990>),\n ('__class__', <type 'operator.itemgetter'>),\n ('__delattr__',\n  <method-wrapper '__delattr__' of operator.itemgetter object at 0x7f79aeffb990>),\n ('__doc__',\n  'itemgetter(item, ...) --> itemgetter object\\n\\nReturn a callable object that fetches the given item(s) from its operand.\\nAfter, f=itemgetter(2), the call f(r) returns r[2].\\nAfter, g=itemgetter(2,5,3), the call g(r) returns (r[2], r[5], r[3])'),\n ('__format__',\n  <built-in method __format__ of operator.itemgetter object at 0x7f79aeffb990>),\n ('__getattribute__',\n  <method-wrapper '__getattribute__' of operator.itemgetter object at 0x7f79aeffb990>),\n ('__hash__',\n  <method-wrapper '__hash__' of operator.itemgetter object at 0x7f79aeffb990>),\n ('__init__',\n  <method-wrapper '__init__' of operator.itemgetter object at 0x7f79aeffb990>),\n ('__new__', <built-in method __new__ of type object at 0x8c1ec0>),\n ('__reduce__',\n  <built-in method __reduce__ of operator.itemgetter object at 0x7f79aeffb990>),\n ('__reduce_ex__',\n  <built-in method __reduce_ex__ of operator.itemgetter object at 0x7f79aeffb990>),\n ('__repr__',\n  <method-wrapper '__repr__' of operator.itemgetter object at 0x7f79aeffb990>),\n ('__setattr__',\n  <method-wrapper '__setattr__' of operator.itemgetter object at 0x7f79aeffb990>),\n ('__sizeof__',\n  <built-in method __sizeof__ of operator.itemgetter object at 0x7f79aeffb990>),\n ('__str__',\n  <method-wrapper '__str__' of operator.itemgetter object at 0x7f79aeffb990>),\n ('__subclasshook__',\n  <built-in method __subclasshook__ of type object at 0x8c1ec0>)]\n>>> inspect.getargspec(itemgetter(2).__call__)\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"/usr/lib/python2.7/inspect.py\", line 815, in getargspec\n    raise TypeError('{!r} is not a Python function'.format(func))\nTypeError: <method-wrapper '__call__' of operator.itemgetter object at 0xb3ddd0> is not a Python function\n>>> inspect.getargspec(itemgetter(slice(None, 2)).__init__)\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\n  File \"/usr/lib/python2.7/inspect.py\", line 815, in getargspec\n    raise TypeError('{!r} is not a Python function'.format(func))\nTypeError: <method-wrapper '__init__' of operator.itemgetter object at 0xb3de10> is not a Python function\nEDIT: Looks like the reason was C functions weren't covered by inspect module until Python 3.4 (http://bugs.python.org/issue17481)", "issue_status": "Closed", "issue_reporting_time": "2014-05-21T17:05:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1410": {"issue_url": "https://github.com/scrapy/scrapy/issues/725", "issue_id": "#725", "issue_summary": "test failures in recently released 0.18.2; test_validation, test_process_parallel_failure", "issue_description": "idella commented on May 20, 2014\n$ python -V\nPython 2.7.6\n....E.........................................................................................................\n===============================================================================\n[SKIPPED]\nAWS keys not found\n\nscrapy.tests.test_contrib_feedexport.S3FeedStorageTest.test_store\n===============================================================================\n[SKIPPED]\nTrue\n\nscrapy.tests.test_selector.XPathSelectorTestCase.test_nested_select_on_text_nodes\nscrapy.tests.test_selector_libxml2.Libxml2XPathSelectorTestCase.test_nested_select_on_text_nodes\nscrapy.tests.test_selector_lxml.LxmlXPathSelectorTestCase.test_nested_select_on_text_nodes\n===============================================================================\n[FAIL]\nTraceback (most recent call last):\n  File \"/mnt/gen2/TmpDir/portage/dev-python/scrapy-0.18.2/work/scrapy-0.18.2/scrapy/tests/test_djangoitem/__init__.py\", line 83, in test_validation\n    i.errors)\n  File \"/usr/lib64/python2.7/site-packages/twisted/trial/_synctest.py\", line 356, in assertEqual\n    % (msg, pformat(first), pformat(second)))\ntwisted.trial.unittest.FailTest: not equal:\na = {'age': [u'This field cannot be null.'],\n 'name': [u'Ensure this value has at most 255 characters (it has 300).']}\nb = {'age': [ValidationError([u'This field cannot be null.'])],\n 'name': [ValidationError([u'Ensure this value has at most 255 characters (it has 300).'])]}\n\n\nscrapy.tests.test_djangoitem.DjangoItemTest.test_validation\n===============================================================================\n[ERROR]\nTraceback (most recent call last):\nFailure: exceptions.TypeError: \n\nscrapy.tests.test_utils_defer.DeferUtilsTest.test_process_parallel_failure\n-------------------------------------------------------------------------------\nRan 898 tests in 166.884s\n\nFAILED (skips=4, failures=1, errors=1, successes=892)\nbin/runtests.sh: line 48:  6295 Terminated              vsftpd $vsftpd_conf\nDo you get these? So you require anything further. atm I don't see any advantage in spamming github with a full build.log.", "issue_status": "Closed", "issue_reporting_time": "2014-05-20T02:02:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1411": {"issue_url": "https://github.com/scrapy/scrapy/issues/723", "issue_id": "#723", "issue_summary": "middleware \"open_spider\" method - deprecated or not?", "issue_description": "Contributor\nnyov commented on May 18, 2014\nI've been pondering this some time.\nWhat is the recommended practice on middlewares here, explicitly including signals and binding to spider_opened|closed or defining open|close_spider which get called by the middleware manager on those signals?\nSEP13 says:\nremove methods: open_spider, close_spider. They should be replaced by using the spider_opened, spider_closed signals, but they weren't before because of a chicken-egg problem when open spiders (because of scheduler auto-open feature).\nSEP18 mentions open|close_spider() as part of a/the new spider API, nothing on deprecation here.", "issue_status": "Closed", "issue_reporting_time": "2014-05-18T07:23:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1412": {"issue_url": "https://github.com/scrapy/scrapy/issues/712", "issue_id": "#712", "issue_summary": "Promote a new CrawlSpider that allows overriding `parse`", "issue_description": "Contributor\nredapple commented on May 7, 2014\nI see a lot of StackOverflow questions and problems with CrawlSpider and overriden parse methods. (e.g. https://stackoverflow.com/questions/23511230)\nI'd like to see a new implementation, called CrawlSpider2 or CrawlingSpider or something, that uses another internal method with another name than parse,\nso that user could define their own parse method.\nThen, the question is if users will expect this parse method to be used by default for each downloaded page (is addition to being parsed for links with Rules),\nor if the reference to parse should be explicit.\nThoughts?", "issue_status": "Closed", "issue_reporting_time": "2014-05-07T08:35:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1413": {"issue_url": "https://github.com/scrapy/scrapy/issues/710", "issue_id": "#710", "issue_summary": "item_dropped signal should pass response arg as item_scraped does", "issue_description": "Contributor\nrafallo commented on May 2, 2014\nI highly use request and response.meta in item_scraped signal handler.\nWhy item_dropped doesn't pass response argument as well as item_scraper does?", "issue_status": "Closed", "issue_reporting_time": "2014-05-02T13:41:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1414": {"issue_url": "https://github.com/scrapy/scrapy/issues/709", "issue_id": "#709", "issue_summary": "I couldn't see the output file when running spider from scrapyd", "issue_description": "WilliamKinaan commented on Apr 30, 2014\nI am using scrapy 0.20 with python 2.7\nI deployed my project, which contains one spider, on scrapyd server.\nWhen I run my spider from cmd, I can see the output file in the root of the project, but when I run my spider from the scrapyd server, I can't see my output file.\nI am using windows 7. I created this scrapyd.conf file inside scrapyd folder in the c drive. and I put these values inside it.\n[scrapyd]\neggs_dir = eggs\nlogs_dir = logs\nitems_dir = items\njobs_to_keep = 5\ndbs_dir = dbs\nmax_proc = 0\nmax_proc_per_cpu = 4\nfinished_to_keep = 100\npoll_interval = 5\nhttp_port = 6800\ndebug = off\nrunner = scrapyd.runner\napplication = scrapyd.app.application\nlauncher = scrapyd.launcher.Launcher\n[services]\nschedule.json = scrapyd.webservice.Schedule\ncancel.json = scrapyd.webservice.Cancel\naddversion.json = scrapyd.webservice.AddVersion\nlistprojects.json = scrapyd.webservice.ListProjects\nlistversions.json = scrapyd.webservice.ListVersions\nlistspiders.json = scrapyd.webservice.ListSpiders\ndelproject.json = scrapyd.webservice.DeleteProject\ndelversion.json = scrapyd.webservice.DeleteVersion\nlistjobs.json = scrapyd.webservice.ListJobs\nI can see that the items_dir is set to \"Item\", but when I can't see the output file.\nIn my setting I do this:\nFEED_EXPORTERS = {\n'jsonlines': 'scrapy.contrib.exporter.JsonLinesItemExporter',\n}\nFEED_FORMAT = 'jsonlines'\nFEED_URI = 'ourput.json'\nCould u help please, Thanks in advance", "issue_status": "Closed", "issue_reporting_time": "2014-04-30T12:42:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1415": {"issue_url": "https://github.com/scrapy/scrapy/issues/706", "issue_id": "#706", "issue_summary": "scrapy engine can stop even if start_requests is not empty", "issue_description": "Contributor\nredapple commented on Apr 25, 2014\nConsider this example spider\nfrom scrapy.spider import Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\n\nclass DmozSpider(Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield Request(\n                url,\n                #dont_filter=True\n            )\n\n    def parse(self, response):\n        self.log(\"parse %r\" % response.url)\nand run it with CONCURRENT_REQUESTS=1 and DupeFilter enabled,\nit will only visit http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\nEven with higher concurrency settings, it can happen that the spider is considered idle because the next request from start_requests iterator was filtered.\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/core/engine.py#L120 or https://github.com/scrapy/scrapy/blob/master/scrapy/core/engine.py#L155 seems to be missing a test on slot.start_requests is None", "issue_status": "Closed", "issue_reporting_time": "2014-04-25T14:20:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1416": {"issue_url": "https://github.com/scrapy/scrapy/issues/704", "issue_id": "#704", "issue_summary": "Is there a way to pass meta information into CrawlSpider's requests?", "issue_description": "kvnn commented on Apr 24, 2014\nI want to pass along some meta information that belongs to the original start_url's request. I can not find a way to reference that original request from within process_request.\nI'm currently doing this by overriding _requests_to_follow in my CrawlSpider sub-class and passing meta=response.meta into Request (example below).\nIs there a better way? If not, is a better way (like a parameter in the Rule constructor) wanted?\n    def _requests_to_follow(self, response):\n        self.f.write('REQUESTS RESPONSE META: %s' % response.meta)\n        if not isinstance(response, HtmlResponse):\n            return\n        seen = set()\n        for n, rule in enumerate(self._rules):\n            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]\n            if links and rule.process_links:\n                links = rule.process_links(links)\n            for link in links:\n                seen.add(link)\n                r = Request(url=link.url, callback=self._response_downloaded, meta=response.meta)\n                r.meta.update(rule=n, link_text=link.text)\n                yield rule.process_request(r)", "issue_status": "Closed", "issue_reporting_time": "2014-04-23T23:22:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1417": {"issue_url": "https://github.com/scrapy/scrapy/issues/703", "issue_id": "#703", "issue_summary": "scrapy do not print log msg in cmd?", "issue_description": "eromoe commented on Apr 21, 2014\nI set up a scrapy project and run by scrapy crawl xxx.\nI do not see any ouotput in cmd,however log msg does write to the log file.\nI am using log like below:\nfrom scrapy import log\n\ndef Pipe(...):\n    ....\n    log.msg(\"Actor_id: %s wrote to database\" % (actor.id,),\n                level=log.DEBUG, spider=spider)\nHow can I make it works?", "issue_status": "Closed", "issue_reporting_time": "2014-04-21T11:53:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1418": {"issue_url": "https://github.com/scrapy/scrapy/issues/691", "issue_id": "#691", "issue_summary": "Scrapy can't follow specific kind of urls", "issue_description": "kamil-forys commented on Apr 16, 2014\ntl;dr\nScrapy can't follow links, which contains additional attribute with non-ASCII character.\nSo, here is my test website:\n<html>\n    <body>\n        <a href=\"test1.html\">test1</a>\n        <a foo=\"bar\" href=\"test2.html\">test2</a>\n        <a f\u00d8o=\"bar\" href=\"test3.html\">test3</a> <!-- Scrapy can't follow this one -->\n        <a href=\"test4.html\" f\u00d8o=\"bar\">test4</a>\n    </body>\n</html>\nFiles named test[].html looks almost the same, here is a sample:\n<html>\n    <body>\n        <h1>Test 1</h1>\n    </body>\n</html>\nHere is my spider code (simplified):\nclass followSpider(CrawlSpider):\n    name = 'follow'\n    allowed_domains = ['localhost']\n    start_urls = ['http://localhost/']\n\n    rules = [Rule(SgmlLinkExtractor(), callback='parse_follow')]\n\n    def parse_follow(self, response):\n        follow = FollowItem()\n        sel = Selector(response)\n        follow['url'] = response.url\n        follow['h1'] = sel.xpath(\"//h1/text()\").extract()\n        return follow\nExecuting of Scrapy (scrapy crawl follow -o follow.json), gives me follow.json file:\n{\"url\": \"http://localhost/test4.html\", \"h1\": [\"Test 4\"]}\n{\"url\": \"http://localhost/test2.html\", \"h1\": [\"Test 2\"]}\n{\"url\": \"http://localhost/test1.html\", \"h1\": [\"Test 1\"]}\nAs you can see, Scrapy didn't crawl 'Test 3' website. Take a look again into test3 url:\n<a f\u00d8o=\"bar\" href=\"test3.html\">test3</a>\nIt contains additional attribute with UTF character \u00d8. I think this might be essence of the problem.\nTake a look also into test4 url:\n<a href=\"test4.html\" f\u00d8o=\"bar\">test4</a>\nIt also contains additional attribute with UTF character \u00d8, but it is placed after href attribute. Scrapy can follow this kind of url.\nSummary: Scrapy can't follow links, which contains additional attribute with non-ASCII character, which is placed before href attribute. I know, that non-ASCII characters are not allowed in html attribute names, but I have to crawl a website which contains plenty of this ugly urls.\nCan you fix it?\n$ scrapy -version\nScrapy 0.22.2", "issue_status": "Closed", "issue_reporting_time": "2014-04-16T14:12:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1419": {"issue_url": "https://github.com/scrapy/scrapy/issues/689", "issue_id": "#689", "issue_summary": "Add a way for requests to prevent caching", "issue_description": "jwm commented on Apr 15, 2014\nI'm working on a speaker scraper (https://github.com/scrapinghub/pycon-speakers) for JavaOne. To paginate through its results, you load the same URL multiple times, each time receiving the next page of results.\nBecause of this, that URL can't be cached, but AFAICT the only way to disable caching in scrapy is to disable caching for the entire crawl (i.e., a global scrapy setting).\nIt looked like a meta flag was added a while ago for this (#19) but it's more suited for internal use, and should probably be replaced by checking for dont_cache in a caching policy.", "issue_status": "Closed", "issue_reporting_time": "2014-04-14T20:26:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1420": {"issue_url": "https://github.com/scrapy/scrapy/issues/680", "issue_id": "#680", "issue_summary": "Scrappy java application", "issue_description": "Spartacus2018 commented on Apr 4, 2014\nHello canwe use scrapy on java application, not j2ee application a real java application ?\nthanks to you", "issue_status": "Closed", "issue_reporting_time": "2014-04-03T19:11:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1421": {"issue_url": "https://github.com/scrapy/scrapy/issues/677", "issue_id": "#677", "issue_summary": "how to change the URL of a Request ?", "issue_description": "yanpeipan commented on Apr 2, 2014\nI want to change the url of current request,add dynamic apikeys\nI write a demo used replace,but it not work.\nhere is my code:\nmiddlewares.py\nclass ProxyMiddleware(object):\n  def process_request(self, request, spider):\n    request = request.replace(url = \"%s&apikey=00d8ef49d1c2b3bb028acddd75481b31\" % request.url)\n    request.meta['proxy'] = 'http://24.143.198.188:80'\nsettings.py\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110,\n    'Scrapy.middlewares.ProxyMiddleware': 100\n    }", "issue_status": "Closed", "issue_reporting_time": "2014-04-02T02:21:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1422": {"issue_url": "https://github.com/scrapy/scrapy/issues/671", "issue_id": "#671", "issue_summary": "Override of TEMPLATES_DIR does not work for \"startproject\" command", "issue_description": "marksoenen commented on Mar 27, 2014\nThe following documentation is incorrect:\nhttp://doc.scrapy.org/en/latest/topics/settings.html#templates-dir\nThe following command line does not work:\nscrapy startproject -s TEMPLATES_DIR=../lib/python2.7/sitepackages/mypackage/templates test_projectA\nThe same command line for \"gendspider\" does work. So the -s option appears to be working. It appears that the code in startproject.py does not check for overridden settings:\nTEMPLATES_PATH = join(scrapy.path[0], 'templates', 'project')\nas opposed to genspider.py which checks:\n    _templates_base_dir = self.settings['TEMPLATES_DIR'] or \\\n        join(scrapy.__path__[0], 'templates')\n    return join(_templates_base_dir, 'spiders')\nAt minimum, the documentation is incorrect and needs to be updated.", "issue_status": "Closed", "issue_reporting_time": "2014-03-26T22:02:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1423": {"issue_url": "https://github.com/scrapy/scrapy/issues/667", "issue_id": "#667", "issue_summary": "Support dropping default input values in from_response()", "issue_description": "Member\ndangra commented on Mar 25, 2014\nIt should be possible to ignore fields when submitting forms on FormRequest.from_response()\nA proposal to consider is to drop any field whose value is None as passed in formdata.\nsource: #412 (comment)", "issue_status": "Closed", "issue_reporting_time": "2014-03-25T14:11:27Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1424": {"issue_url": "https://github.com/scrapy/scrapy/issues/666", "issue_id": "#666", "issue_summary": "API of Request and Rule", "issue_description": "zhangfand commented on Mar 23, 2014\nI'm using scrapy to crawl a forum. I found that the mechanisms to transfer information to callback function of Request and Rule objects differ. Within my knowledge, in Rule i'm using cb_kwargs to transfer while in Request i'm using meta to pass information.\nI'm not sure if I'm doing it the RIGHT way. If I'm, i think maybe this is not a good API design? In my opinion, using meta as well in Rule is a feasible design.", "issue_status": "Closed", "issue_reporting_time": "2014-03-23T13:30:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1425": {"issue_url": "https://github.com/scrapy/scrapy/issues/665", "issue_id": "#665", "issue_summary": "Add more settings to startproject's generated settings.py", "issue_description": "Contributor\nredapple commented on Mar 23, 2014\nCurrently, scrapy/templates/project/module/settings.py.tmpl is rather succint.\n# Scrapy settings for $project_name project\n#\n# For simplicity, this file contains only the most important settings by\n# default. All the other settings are documented here:\n#\n#     http://doc.scrapy.org/en/latest/topics/settings.html\n#\nBOT_NAME = '$project_name'\n\nSPIDER_MODULES = ['$project_name.spiders']\nNEWSPIDER_MODULE = '$project_name.spiders'\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = '$project_name (+http://www.yourdomain.com)'\nIt would be nice to have many common and useful settings in the generated settings.py, probably commented.\nThe current file redirects the reader to http://doc.scrapy.org/en/latest/topics/settings.html\nbut useful settings like AUTOTHROTTLE or HTTPCACHE_xxx settings are not explained in this page\nsettings.py could include something like this:\n# The amount of time (in secs) that the downloader should wait\n# before downloading consecutive pages from the same website. \n#3 seconds corresponds to roughly 20 request/sec maximum\n# Default value is 0 so the downloader will ask for pages as fast as it can\n# within the constraints of CONCURRENT_REQUESTS_PER_DOMAIN\n#DOWNLOAD_DELAY=3\n#CONCURRENT_REQUESTS_PER_DOMAIN=16\n#CONCURRENT_REQUESTS_PER_IP=16\n\n# Enables the AutoThrottle extension. (Default: False)\n# See http://doc.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED=True\n# The initial download delay (in seconds). Default: 60.0 (Default: 5.0)\n#AUTOTHROTTLE_START_DELAY=3\n# The maximum download delay (in seconds) to be set in case of high latencies.\n#AUTOTHROTTLE_MAX_DELAY=90\n#AUTOTHROTTLE_DEBUG=True\n\n#HTTPCACHE_ENABLED=True\n#HTTPCACHE_EXPIRATION_SECS=3600\n#HTTPCACHE_DIR='httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES=[404]\n...", "issue_status": "Closed", "issue_reporting_time": "2014-03-23T09:23:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1426": {"issue_url": "https://github.com/scrapy/scrapy/issues/652", "issue_id": "#652", "issue_summary": "SgmlLinkExtractor default value for 'attrs' argument should be tuple", "issue_description": "Member\nkmike commented on Mar 18, 2014\nIt is now a string, and attrs_func doesn't make sense if attrs is a string. See https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/linkextractors/sgml.py#L98", "issue_status": "Closed", "issue_reporting_time": "2014-03-17T19:47:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1427": {"issue_url": "https://github.com/scrapy/scrapy/issues/629", "issue_id": "#629", "issue_summary": "Convert Alll SEPs to RST", "issue_description": "Contributor\naspidites commented on Mar 7, 2014\nIt would be nice to convert the older SEPs to rst format from their original trac versions. By doing so:\ndocumentation willl be more consistent\ngithub will automatically render them to html when browsed\nit would be possible to include them as an appendix to the official documentation\nI would be willing (and already have started) to do this myself.", "issue_status": "Closed", "issue_reporting_time": "2014-03-06T21:12:22Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1428": {"issue_url": "https://github.com/scrapy/scrapy/issues/620", "issue_id": "#620", "issue_summary": "When use cache, download_delay does not work", "issue_description": "tonydot28 commented on Mar 1, 2014\nIf a cached response is returned when the page is considered fresh without requesting file,\nit will return the response immediately. Thus, no download scheduling is performed, and hence download_delay does not used at all.\nHowever, when I want to repeatly check if a page, by request the same url in the parse function, then no delay exists. That makes the problem.", "issue_status": "Closed", "issue_reporting_time": "2014-03-01T04:41:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1429": {"issue_url": "https://github.com/scrapy/scrapy/issues/619", "issue_id": "#619", "issue_summary": "Missing archive key for ubuntu packages?", "issue_description": "raymondmaxwellsmith commented on Feb 28, 2014\n(Sorry if this is the wrong place for this report.)\nThe wiki page for Ubuntu packages at http://doc.scrapy.org/en/0.22/topics/ubuntu.html says to do the following to get the public signing key for the Ubuntu packages:\ncurl -s http://archive.scrapy.org/ubuntu/archive.key | sudo apt-key add \nbut this is returning a 404 at the moment:\ntanngrisnir:hypermancer raymond$ date && curl -s http://archive.scrapy.org/ubuntu/archive.key \nFri 28 Feb 2014 15:15:11 EST\n<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html><head>\n<title>404 Not Found</title>\n</head><body>\n<h1>Not Found</h1>\n<p>The requested URL /ubuntu/archive.key was not found on this server.</p>\n<hr>\n<address>Apache Server at archive.scrapy.org Port 80</address>\n</body></html>", "issue_status": "Closed", "issue_reporting_time": "2014-02-28T05:18:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1430": {"issue_url": "https://github.com/scrapy/scrapy/issues/616", "issue_id": "#616", "issue_summary": "ItemLoader should work with copy of item passed as an argument", "issue_description": "Contributor\nchekunkov commented on Feb 26, 2014\nSometimes it is needed to parse several variants of item from single page, where only a couple of fields differ. Obvious way to do this is to create base item with all common fields collected and then create new items (for example in a loop) using new ItemLoader instance with base item passed to it.\nThe problem I'm facing now is that ItemLoader.load_item() modifies base item - which is counterintuitive and can result in weird behavior (for example if variant items can have different fields - after those field were added to base item - they would appear in all loaded items).\nIn [5]: item = Item()\n\nIn [7]: item['url'] = 'foo'\n\nIn [8]: item\nOut[8]: {'url': 'foo'}\n\nIn [9]: l = ItemLoader(item)\n\nIn [12]: l.add_value('category', 'bar')\n\nIn [13]: item\nOut[13]: {'url': 'foo'}\n\nIn [14]: item_copy = l.load_item()\n\nIn [15]: item_copy\nOut[15]: {'category': 'bar', 'url': 'foo'}\n\nIn [16]: item\nOut[16]: {'category': 'bar', 'url': 'foo'}\n\nIn [17]: id(item)\nOut[17]: 49468304\n\nIn [18]: id(item_copy)\nOut[18]: 49468304\nNow I'm using workaround like this to suppress such behavior:\nloader = ItemLoader(selector=sel)\n...\nitem = loader.load_item()\nfor variant in variants:\n    item_copy = item.copy()\n    loader = ItemLoader(item_copy)\n    ...\n    yield loader.load_item()\nWhat do you think about using item copy inside ItemLoader by default?", "issue_status": "Closed", "issue_reporting_time": "2014-02-26T12:00:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1431": {"issue_url": "https://github.com/scrapy/scrapy/issues/615", "issue_id": "#615", "issue_summary": "It is not documented how to use per-request proxy server", "issue_description": "Member\nkmike commented on Feb 26, 2014\nI think Request.meta['proxy'] should be documented. It is not documented now, see http://doc.scrapy.org/en/latest/topics/request-response.html#request-meta-special-keys.\nThe only reference for scrapy+proxies is http://doc.scrapy.org/en/latest/topics/downloader-middleware.html?#module-scrapy.contrib.downloadermiddleware.httpproxy, and it is quite confusing.", "issue_status": "Closed", "issue_reporting_time": "2014-02-26T08:22:55Z", "fixed_by": "#1071", "pull_request_summary": "updating list of Request.meta special keys", "pull_request_description": "Member\neliasdorneles commented on Mar 11, 2015\nHey, folks!\nSo, here is an update for the meta special keys, this fixes #615.\nDoes this look good?\nThank you!", "pull_request_status": "Merged", "issue_fixed_time": "2015-03-13T11:38:19Z", "files_changed": [["7", "docs/topics/downloader-middleware.rst"], ["3", "docs/topics/request-response.rst"], ["2", "docs/topics/settings.rst"], ["5", "docs/topics/spider-middleware.rst"]]}, "1432": {"issue_url": "https://github.com/scrapy/scrapy/issues/614", "issue_id": "#614", "issue_summary": "Empty ItemLoader get_output_value() when instantiated with an item", "issue_description": "sardok commented on Feb 25, 2014\nHi, following code snippet does not work as i expected:\nIn [2]: from scrapy.contrib.loader import ItemLoader\n\nIn [3]: \n\nIn [3]: from scrapy.item import Item, Field\n\nIn [4]: \n\nIn [4]: class Product(Item):\n   ...:         name = Field()\n   ...:         comment = Field()\n   ...:     \n\nIn [5]: p = Product({'name': 'xx1', 'comment': 'hi!'})\n\nIn [6]: p\nOut[6]: {'comment': 'hi!', 'name': 'xx1'}\n\nIn [7]: l = ItemLoader(item=p)\n\nIn [8]: l.get_output_value('name')\nOut[8]: []\n\nIn [9]: l.context\nOut[9]: {'item': {'comment': 'hi!', 'name': 'xx1'}, 'response': None, 'selector': None}\nI would expect to see a result of 'xx1' from get_output_value() function intuitively. Am i right about my expectation here?\nRegarding the code at item loader (https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/loader/__init__.py#L88), is it correct to check the 'context' or 'item' of the loader if relevant field_name is not available in the loader?", "issue_status": "Closed", "issue_reporting_time": "2014-02-25T09:02:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1433": {"issue_url": "https://github.com/scrapy/scrapy/issues/611", "issue_id": "#611", "issue_summary": "Debug log for ignored/not followed pages", "issue_description": "Contributor\ndeed02392 commented on Feb 23, 2014\nI'm new to Scrapy and one thing that tripped me up was the crawler not following 401s and not giving any indication as to why. I've since figured it out after Googling and seeing a related StackOverflow question, but it strikes me as part of the scrapy crawl spider log output, there should be a log.DEBUG level output when a spider decides to ignore a request response.", "issue_status": "Closed", "issue_reporting_time": "2014-02-23T13:01:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1434": {"issue_url": "https://github.com/scrapy/scrapy/issues/609", "issue_id": "#609", "issue_summary": "Remove CrawlSpider example and other tutorial-like parts from overview.rst", "issue_description": "Member\nkmike commented on Feb 23, 2014\nA link to current overview: http://doc.scrapy.org/en/latest/intro/overview.html\nI think that using CrawlSpider in overview makes Scrapy look complex and confuses new users.\nThe overview relies on fact that there is a pattern in urls and that the spider should just crawl all urls with this pattern, making users wondering what to do in a general case.\nThe example scares me :)\nfrom scrapy.contrib.spiders import CrawlSpider, Rule  \nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\nfrom scrapy.selector import Selector\n\n# how can one find where to import stuff from? \n# SgmlLinkExtractor class has obscure name (not anyone has \n# a beard and knows what SGML is), and it is buried in \n#4-level-deep hierarchy, including \"contrib\".\n\n\nclass MininovaSpider(CrawlSpider):\n\n    name = 'mininova' \n    allowed_domains = ['mininova.org']\n    start_urls = ['http://www.mininova.org/today']\n    # this was fine (though not explained earlier)\n\n    # HERE BE DRAGONS\n    rules = [Rule(SgmlLinkExtractor(allow=['/tor/\\d+']), 'parse_torrent')]\n\n    # What to do if users want more control on what is spider \n    # doing - should they find an another class nested 4-levels deep? \n    # Should they override Rule or SgmlLinkExtractor or CrawlSpider\n    # to customize what spider is doing? Or maybe there are powerful options \n    # in these classes that cover 95% cases, and you are out of luck \n    # if your case is in remaining 5%? Is this kind of problems scrapy users \n    # are working on? Is it a scrapy style to always pass \n    # method names as strings? Readability one: is method name \n    # a parameter of Rule or SgmlLinkExtractor?\nAnd then there is an encouraging \".. but this is just the surface. \" in \"What else\" section. We all know scrapy is powerful :)\nOf course, there are different users reading this overview, and for someone it can be just what is wanted. I keep in mind another kind of users reading the overview: they want to scrape information from some webpage, but haven't done this before. They may understand how to hack a synchronous script using requests or urllib. Maybe they can parallelize it using threads (maybe via futures), via grequests or even using twisted or tornado or celery. Maybe they already checked their webpage and have a rough idea how to extract data using regexes and how to follow the links. But before hacking such script they want to check how can a framework help them. They go to scrapy webpage and end up reading this overview.\nOverview tells them they'll need to define an Item class, write some link extraction rules in a scrapy DSL, populate items using xpaths and to use a command-line tool to get the result. Most likely this code doesn't look like anything they thought of, except for xpaths. They can see that rules may save them some typing on link following code, but learning how to use them looks like a big time investment. They see that scrapy somehow downloads pages behind the scenes, but it is not clear what does it take to make scrapy download the pages they want. Also, it is not clear how does scrapy work (sync? threads? event loop?), so the advantage over e.g. sequential requests.get(url) calls are unclear. There is zero visible advantages of defining TorrentItem over using plain dicts.\nOverview omits some real-word steps like where to put the code we've written - I recall putting it all in a single file mininova.py and trying to run the scrapy crawl command - this didn't work. Overview gets into great details about mininova-specific XPaths, so I thought it is a tutorial while reading it; The purpose of this document... note in the beginning of the document was long forgotten.\nAfter a lengthy rant it is better to propose something :) What about removing tutorial-ish parts (including CrawlSpider example) from the overview? I think a good approximation would be to \"downgrade\" \"Pick a website\", \"Define the data you want to scrape\", \"Write a Spider to extract the data\" and \"Run the spider to extract the data\" sections to an ordered list with short comments about each item, and maybe create a \"CrawlSpider tutorial\" from the removed contents.\nTutorial has its issues (#608), but it is a better introduction IMHO.", "issue_status": "Closed", "issue_reporting_time": "2014-02-23T02:48:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1435": {"issue_url": "https://github.com/scrapy/scrapy/issues/608", "issue_id": "#608", "issue_summary": "Tutorial should have an example of how to issue requests", "issue_description": "Member\nkmike commented on Feb 23, 2014\nI think the main issue with existing tutorial is that it doesn't show how to follow links and how start_requests work, relying on parse convention instead. The \"under the hood\" explanation is not enough IMHO. Users may think the only (or the blessed) way to write spiders that need to follow links is a CrawlSpider, and Spider is just for scraping data from a single webpage or from a static list of webpages (start_urls).\nWhat do you think about extending the example with some link following code?", "issue_status": "Closed", "issue_reporting_time": "2014-02-23T02:45:22Z", "fixed_by": "#1180", "pull_request_summary": "Some improvements for Scrapy tutorial", "pull_request_description": "Member\neliasdorneles commented on Apr 21, 2015\nSo, here are some improvements for Scrapy tutorial, please let me know what you think.\nSummary of the changes:\nadds an example and explanation about following links (fixes #608 )\nadded note about CSS vs XPath, idea is to encourage people who know CSS to dig XPath too\nupdated recommend XPath tutorials to the ones I felt were most helpful as a XPath beginner\nadd a watch target to docs Makefile (note: depends on watchdog)\na few other small changes\nDoes this look good?", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-21T18:49:39Z", "files_changed": [["3", "docs/Makefile"], ["172", "docs/intro/tutorial.rst"]]}, "1436": {"issue_url": "https://github.com/scrapy/scrapy/issues/606", "issue_id": "#606", "issue_summary": "Redirection Links in docs", "issue_description": "taranjeet commented on Feb 21, 2014\nWhen i run\ncd doc;\nmake linkcheck\nIt shows a list of url some which are redirection ones and some broken ones.", "issue_status": "Closed", "issue_reporting_time": "2014-02-20T20:05:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1437": {"issue_url": "https://github.com/scrapy/scrapy/issues/600", "issue_id": "#600", "issue_summary": "Python-requests like API", "issue_description": "juanriaza commented on Feb 17, 2014\npython-requests API has become the de-facto way to make HTTP requests in python. treq is an HTTP library inspired by requests but written on top of Twisted's Agents.\nIt would be interesting to merge or just to abstract some functionality in order to become more welcoming for newcomers.", "issue_status": "Closed", "issue_reporting_time": "2014-02-17T16:31:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1438": {"issue_url": "https://github.com/scrapy/scrapy/issues/599", "issue_id": "#599", "issue_summary": "Document usage of FilesPipeline", "issue_description": "Contributor\nredapple commented on Feb 17, 2014\nFilesPipeline usage needs documentation\n#370\nJust like ImagesPipeline is documented in http://doc.scrapy.org/en/latest/topics/images.html", "issue_status": "Closed", "issue_reporting_time": "2014-02-17T12:40:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1439": {"issue_url": "https://github.com/scrapy/scrapy/issues/593", "issue_id": "#593", "issue_summary": "engine status util references removed engine.slots attribute", "issue_description": "Member\ndangra commented on Feb 14, 2014\nTraceback (most recent call last): Less\n      File \"/usr/lib/pymodules/python2.7/scrapy/xlib/pydispatch/robustapply.py\", line 54, in robustApply\n        return receiver(*arguments, **named)\n      File \"/usr/lib/pymodules/python2.7/scrapy/contrib/memusage.py\", line 63, in engine_started\n        tsk.start(60.0, now=True)\n      File \"/usr/lib/python2.7/dist-packages/twisted/internet/task.py\", line 163, in start\n        self()\n      File \"/usr/lib/python2.7/dist-packages/twisted/internet/task.py\", line 208, in __call__\n        d = defer.maybeDeferred(self.f, *self.a, **self.kw)\n    --- <exception caught here> ---\n      File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n        result = f(*args, **kw)\n      File \"/usr/lib/pymodules/python2.7/scrapy/contrib/memusage.py\", line 103, in _check_warning\n        self._send_report(self.notify_mails, subj)\n      File \"/usr/lib/pymodules/python2.7/scrapy/contrib/memusage.py\", line 116, in _send_report\n        s += pformat(get_engine_status(self.crawler.engine))\n      File \"/usr/lib/pymodules/python2.7/scrapy/utils/engine.py\", line 33, in get_engine_status\n        for spider in engine.slots.keys():\n    exceptions.AttributeError: 'ExecutionEngine' object has no attribute 'slots'", "issue_status": "Closed", "issue_reporting_time": "2014-02-14T16:46:48Z", "fixed_by": "#594", "pull_request_summary": "fix a reference to unexistent engine.slots.", "pull_request_description": "Member\ndangra commented on Feb 14, 2014\ncloses #593", "pull_request_status": "Merged", "issue_fixed_time": "2014-02-14T17:39:30Z", "files_changed": [["27", "docs/topics/telnetconsole.rst"], ["6", "scrapy/tests/spiders.py"], ["15", "scrapy/tests/test_crawl.py"], ["36", "scrapy/utils/engine.py"]]}, "1440": {"issue_url": "https://github.com/scrapy/scrapy/issues/591", "issue_id": "#591", "issue_summary": "Add-ons (SEP-021) Discussion", "issue_description": "Contributor\nnyov commented on Feb 12, 2014\nI totally dig this idea, but I also think this needs to be well thought-out or it could spell more trouble than it solves and be hard to deprecate and fix later.\nMy thoughts on the current proposal:\n[addons] in scrapy.cfg would not provide any way to pass configuration settings. So we still need to modify settings.py for, say, the database connection parameters.\nThat doesn't look very portable.\nAlso, if two different addons expose or use the same settings, it gets ugly.\nSay we have different addons, one with a MongoPipeline and one with a MongoQueue, which by chance both use the MONGO_COLLECTION setting name.\nThis is obviously already a problem, but I think with add-ons, people would more expect a \"plug-and-play\" system which \"just works\", and be less careful with checking individual addon-components for name clashes or dependency issues.\nUnless addons should be expected to be configured prior to use, or only as a bundle for hardcoded configuration settings, I think we need another layer deep to have addons and their settings in the same place.\nMaybe we can use a [addon_(name)] (or simply [(addon_name)]) ini setting which can then take multiple parameters.\nThese settings should be namespaced to the addon.\nExample:\n[httpcache] # (looking in python path, no settings)\n#enabled = True # (does ini-style require a parameter to recognize the section?)\n[mongodb_pipeline]\npath = /path/to/mongodb_pipeline.py # (explicit path)\nhost = 'localhost'\nport = 27017\nThese settings would then expand to (addon_name)_(setting), e.g. mongodb_pipeline_port or MONGODB_PIPELINE_PORT to prevent namespace clashes between addons.\n(Would this copy settings on scrapyd deployment?)\nAlternatively, maybe all the addon defining and configuration could be in settings.py, but I think it should be in a single place.", "issue_status": "Closed", "issue_reporting_time": "2014-02-12T04:17:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1441": {"issue_url": "https://github.com/scrapy/scrapy/issues/589", "issue_id": "#589", "issue_summary": "Enable latest stable branch on ReadTheDocs", "issue_description": "Member\ndangra commented on Feb 10, 2014\nDocs for 0.24 branch exposed at http://doc.scrapy.org/en/0.24/ are outdated compared to http://doc.scrapy.org/en/latest/ (\"latest\" is set to 0.24).\nReenable once readthedocs/readthedocs.org#635 is fixed.", "issue_status": "Closed", "issue_reporting_time": "2014-02-10T14:29:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1442": {"issue_url": "https://github.com/scrapy/scrapy/issues/588", "issue_id": "#588", "issue_summary": "Multi-host scraping.", "issue_description": "JerzySpendel commented on Feb 10, 2014\nI would like to ask a question, is multi-host scraping sth you would like to be implemented? I heard about scrapy-redis but it's not pure python therefore it can't be included in standard Scrapy.", "issue_status": "Closed", "issue_reporting_time": "2014-02-09T18:40:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1443": {"issue_url": "https://github.com/scrapy/scrapy/issues/586", "issue_id": "#586", "issue_summary": "Change the scrapy short description", "issue_description": "Contributor\nrmax commented on Feb 7, 2014\nThe Scrapy short description says:\nScrapy, a fast high-level screen scraping and web crawling framework.\nI think would be better:\nScrapy, a fast high-level web crawling and screen scraping framework.\nBecause it highlights first its difference with simple screen scraping tools (i.e. Nokogiri. Mechanize, etc).\nScreen scraping can be done even with curl and grep, but I don't think you could do web crawling with such simple tools.\nPerhaps this can be an alternative:\nScrapy, a fast and scalable web crawling and screen scraping framework.\nAlso the term \"web data mining\" can be a good fit for Scrapy (along with Scrapely and similar tools) and help to shape its roadmap.", "issue_status": "Closed", "issue_reporting_time": "2014-02-07T16:17:25Z", "fixed_by": "#1188", "pull_request_summary": "[MRG+1] Favoring web scraping over screen scraping in the descriptions", "pull_request_description": "Member\neliasdorneles commented on Apr 25, 2015\nThis fixes #586 that was reopened after more discussion, see last comments.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-29T19:49:43Z", "files_changed": [["2", "README.rst"], ["4", "debian/control"], ["8", "docs/intro/overview.rst"], ["2", "docs/topics/selectors.rst"], ["2", "scrapy/__init__.py"], ["2", "setup.py"]]}, "1444": {"issue_url": "https://github.com/scrapy/scrapy/issues/583", "issue_id": "#583", "issue_summary": "twisted.internet.defer.TimeoutError deprecated", "issue_description": "Member\nDigenis commented on Feb 5, 2014\ntwisted.internet.defer.TimeoutError class is still being imported by some scrapy modules.\nSome of them even introduced its use long before twisted deprecated it:\nhttps://github.com/scrapy/scrapy/blame/83dcf8aff91d0178ffbf4978fb757fc7e6d5d08c/scrapy/trunk/scrapy/contrib/downloadermiddleware/retry.py#L28\nhttps://github.com/scrapy/scrapy/blame/588a262b73e73f319edede130882f2520350d968/scrapy/core/downloader/webclient.py#L77", "issue_status": "Closed", "issue_reporting_time": "2014-02-05T12:17:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1445": {"issue_url": "https://github.com/scrapy/scrapy/issues/581", "issue_id": "#581", "issue_summary": "Deprecated class inheritance check bug", "issue_description": "Contributor\nnramirezuy commented on Feb 4, 2014\n>>> from scrapy.spider import BaseSpider\n>>> class A(BaseSpider):\n...     pass\n... \n>>> class B(BaseSpider):\n...     pass\n... \n>>> isinstance(A('foo'), B)\nTrue", "issue_status": "Closed", "issue_reporting_time": "2014-02-04T18:24:01Z", "fixed_by": "#584", "pull_request_summary": "Fix wrong checks on subclassing of deprecated classes", "pull_request_description": "Member\ndangra commented on Feb 5, 2014\ncloses #581", "pull_request_status": "Merged", "issue_fixed_time": "2014-02-05T21:48:40Z", "files_changed": [["11", "scrapy/tests/test_utils_deprecate.py"], ["10", "scrapy/utils/deprecate.py"]]}, "1446": {"issue_url": "https://github.com/scrapy/scrapy/issues/579", "issue_id": "#579", "issue_summary": "A XPath expression that worked with the libxml selector does not work with the lxml selector", "issue_description": "Contributor\nrmax commented on Feb 4, 2014\nThis is from a spider which was written before the lxml selector was merged.\nIn [1]: from scrapy.selector import Selector\n\nIn [2]: Selector(text='')\nOut[2]: <Selector xpath=None data=u'<html></html>'>\n\nIn [3]: sel = Selector(text='')\n\nIn [4]: sel.xpath('//*[contains(@id,\"StoreName\")][text()!=\"\\xc2\\xa0\" and text() !=\"\\xc2\\xa0\\xc2\\xa0\"]/parent::*')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-4-761f742aca4b> in <module>()\n----> 1 sel.xpath('//*[contains(@id,\"StoreName\")][text()!=\"\\xc2\\xa0\" and text() !=\"\\xc2\\xa0\\xc2\\xa0\"]/parent::*'\n      2 )\n\n/home/rolando/.virtualenvs/scrapy/lib/python2.7/site-packages/scrapy/selector/unified.pyc in xpath(self, query)\n     86 \n     87         try:\n---> 88             result = xpathev(query, namespaces=self.namespaces)\n     89         except etree.XPathError:\n     90             raise ValueError(\"Invalid XPath: %s\" % query)\n\n/usr/lib/python2.7/dist-packages/lxml/etree.so in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:46062)()\n\n/usr/lib/python2.7/dist-packages/lxml/etree.so in lxml.etree.XPathElementEvaluator.__call__ (src/lxml/lxml.etree.c:131314)()\n\n/usr/lib/python2.7/dist-packages/lxml/etree.so in lxml.etree._utf8 (src/lxml/lxml.etree.c:24233)()\n\nValueError: All strings must be XML compatible: Unicode or ASCII, no NULL bytes or control characters\nThe error comes from the non-breaking space character (a.k.a. nbsp). The curious thing is that the selector works with the unicode representation but not with the utf8 representation (as the said spider was using):\nIn [2]: from scrapy.http import HtmlResponse\n\nIn [4]: response = HtmlResponse('http://example.org', body='&nbsp;')\n\nIn [6]: from scrapy.selector import Selector\n\nIn [7]: sel = Selector(response)\n\nIn [8]: sel.extract()\nOut[8]: u'<html><body><p>\\xa0</p></body></html>'\n\nIn [11]: sel.xpath(u'//*[text()=\"\\xa0\"]').extract()\nOut[11]: [u'<p>\\xa0</p>']\n\nIn [15]: u'\\xa0'.encode('utf8')\nOut[15]: '\\xc2\\xa0'\n\nIn [16]: sel.xpath('//*[text()=\"\\xc2\\xa0\"]').extract()\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-16-af466045db16> in <module>()\n----> 1 sel.xpath('//*[text()=\"\\xc2\\xa0\"]').extract()\n\n/usr/lib/pymodules/python2.7/scrapy/selector/unified.pyc in xpath(self, query)\n     86 \n     87         try:\n---> 88             result = xpathev(query, namespaces=self.namespaces)\n     89         except etree.XPathError:\n     90             raise ValueError(\"Invalid XPath: %s\" % query)\n\n/usr/lib/python2.7/dist-packages/lxml/etree.so in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:46062)()\n\n/usr/lib/python2.7/dist-packages/lxml/etree.so in lxml.etree.XPathElementEvaluator.__call__ (src/lxml/lxml.etree.c:131314)()\n\n/usr/lib/python2.7/dist-packages/lxml/etree.so in lxml.etree._utf8 (src/lxml/lxml.etree.c:24233)()\n\nValueError: All strings must be XML compatible: Unicode or ASCII, no NULL bytes or control characters\nThis even causes weird errors, below is an invalid xpath which normally would raise a ValueError: Invalid XPath: ...:\nIn [11]: sel.xpath(u'//*[text()=\"\\xa0\"')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-11-332157574520> in <module>()\n----> 1 sel.xpath(u'//*[text()=\"\\xa0\"')\n\n/home/rolando/.virtualenvs/scrapy/lib/python2.7/site-packages/scrapy/selector/unified.pyc in xpath(self, query)\n     88             result = xpathev(query, namespaces=self.namespaces)\n     89         except etree.XPathError:\n---> 90             raise ValueError(\"Invalid XPath: %s\" % query)\n     91 \n     92         if type(result) is not list:\n\n<type 'str'>: (<type 'exceptions.UnicodeEncodeError'>, UnicodeEncodeError('ascii', u'Invalid XPath: //*[text()=\"\\xa0\"', 27, 28, 'ordinal not in range(128)'))\nCould be this considered a bug? It seems there is not a consistent handling of unicode/str strings.", "issue_status": "Closed", "issue_reporting_time": "2014-02-04T06:07:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1447": {"issue_url": "https://github.com/scrapy/scrapy/issues/576", "issue_id": "#576", "issue_summary": "exceptions.AttributeError: 'dict' object has no attribute 'fields'", "issue_description": "Sshuichi commented on Feb 3, 2014\nHello\nIn my pipeline i return this dict :\nmarkers={}\nmarkers['active']=1\nmarkers['author']=\"52cb29b0211b6fd9248b456b\"\nmarkers['contact']=item['contact']\nmarkers['personal']={\"name\":item['name'],\"lname\":item['lname']}\nmarkers['date']=strftime(\"%Y-%m-%d %H:%M\", gmtime())\nI want to export the data to a json file with command :\nscrapy crawl stories -o file.json -t json\nI get this error :\nexceptions.AttributeError: 'dict' object has no attribute 'fields'\nHow can I fix it ?", "issue_status": "Closed", "issue_reporting_time": "2014-02-02T21:44:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1448": {"issue_url": "https://github.com/scrapy/scrapy/issues/573", "issue_id": "#573", "issue_summary": "Log with utf-8, could it show the right character instead of code `\\uxx`?", "issue_description": "geekan commented on Jan 31, 2014\nI'm a newbie to scrapy, and when I crawl some web with utf-8 encoding, I see the log messages contain lots of \\uxx. I don't like showing unicode code, so I hack log.py, now it shows the right utf-8 character.\nHere's the code, repr(kw).decode(\"unicode-escape\"): is the only modification.\ndef msg(message=None, _level=INFO, **kw):\n    kw['logLevel'] = kw.pop('level', _level)\n    kw.setdefault('system', 'scrapy')\n    if message is None:\n        # log.msg(**kw)\n        print repr(kw).decode(\"unicode-escape\")\n    else:\n        # log.msg(message, **kw)\n        print message, repr(kw).decode(\"unicode-escape\")\nThe method I use may be crude, not a twisted-style log, but I'm not familier with twisted too. So is there any configuration I could use to make utf-8 log correct, or any better way to do it?", "issue_status": "Closed", "issue_reporting_time": "2014-01-31T08:15:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1449": {"issue_url": "https://github.com/scrapy/scrapy/issues/568", "issue_id": "#568", "issue_summary": "New selector method: extract_first()", "issue_description": "shirk3y commented on Jan 29, 2014\nI think about suggestion to improve scrapy Selector. I've seen this construction in many projects:\nresult = sel.xpath('//div/text()').extract()[0]\nAnd what about if result: and else:, or try: and except:, which should be always there?\nWhen we don't want ItemLoaders, the most common use of selector is retrieving only single element.\nMaybe there should be method xpath1 or xpath_one or xpath_first that returns first matched element or None?\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2014-01-29T16:15:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1450": {"issue_url": "https://github.com/scrapy/scrapy/issues/567", "issue_id": "#567", "issue_summary": "A response with status 429 get lost without calling the callback or any error at all.", "issue_description": "Contributor\nrmax commented on Jan 29, 2014\nI had a spider which crawled a few thousand of URLs but noticed it didn't extract any item at all. Digging into the logs I noticed the 429 status code (Too Many Requests) after a few dozen of requests:\n2014-01-27 13:15:05 DEBUG Crawled (200) <GET http://site/...>\n2014-01-27 13:15:06 DEBUG Crawled (200) <GET http://site/...>\n2014-01-27 13:15:06 DEBUG Crawled (429) <GET http://site/...>\n2014-01-27 13:15:06 DEBUG Crawled (429) <GET http://site/...>\nAnd this message for thousands of requests without calling the callback at all. I would expect to scrapy show an error or ignore message for non-200 response but in this case it didn't happen. Below is the technical information and how to reproduce this behavior. I haven't verified if this happens for other 4xx response status codes.\n$ scrapy version -v\nScrapy  : 0.23.0\nlxml    : 3.1.0.0\nlibxml2 : 2.9.0\nTwisted : 12.3.0\nPython  : 2.7.4 (default, Sep 26 2013, 03:20:26) - [GCC 4.7.3]\nPlatform: Linux-3.8.0-35-generic-x86_64-with-Ubuntu-13.04-raring\n# file: server.py\n# usage: python server.py\nfrom klein import run, route\n\nglobal counter\ncounter = 0\n\n@route('/')\ndef home(request):\n    global counter\n    counter += 1\n\n    if counter < 10:\n        return 'Hello, world %s!' % counter\n    else:\n        request.setResponseCode(429)\n        return 'Too Many Requests'\n\nrun(\"localhost\", 8080)\n# file: myspider.py\nfrom scrapy.http import Request\nfrom scrapy.spider import Spider\n\nclass MySpider(Spider):\n    name = 'myspider'\n    start_urls = ['http://localhost:8080']\n\n    def parse(self, response):\n        return Request(response.url, dont_filter=True)\n$ scrapy runspider myspider.py   \n2014-01-28 16:15:15-0400 [scrapy] INFO: Scrapy 0.23.0 started (bot: scrapybot)\n2014-01-28 16:15:15-0400 [scrapy] INFO: Optional features available: ssl, http11, boto, django\n2014-01-28 16:15:15-0400 [scrapy] INFO: Overridden settings: {}\n2014-01-28 16:15:15-0400 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2014-01-28 16:15:15-0400 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2014-01-28 16:15:15-0400 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2014-01-28 16:15:15-0400 [scrapy] INFO: Enabled item pipelines: \n2014-01-28 16:15:15-0400 [myspider] INFO: Spider opened\n2014-01-28 16:15:15-0400 [myspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2014-01-28 16:15:15-0400 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n2014-01-28 16:15:15-0400 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: None)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (200) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] DEBUG: Crawled (429) <GET http://localhost:8080> (referer: http://localhost:8080)\n2014-01-28 16:15:15-0400 [myspider] INFO: Closing spider (finished)\n2014-01-28 16:15:15-0400 [myspider] INFO: Dumping Scrapy stats:\n    {'downloader/request_bytes': 2368,\n     'downloader/request_count': 10,\n     'downloader/request_method_count/GET': 10,\n     'downloader/response_bytes': 1230,\n     'downloader/response_count': 10,\n     'downloader/response_status_count/200': 9,\n     'downloader/response_status_count/429': 1,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2014, 1, 28, 20, 15, 15, 740849),\n     'log_count/DEBUG': 12,\n     'log_count/INFO': 7,\n     'request_depth_max': 9,\n     'response_received_count': 10,\n     'scheduler/dequeued': 10,\n     'scheduler/dequeued/memory': 10,\n     'scheduler/enqueued': 10,\n     'scheduler/enqueued/memory': 10,\n     'start_time': datetime.datetime(2014, 1, 28, 20, 15, 15, 622068)}\n2014-01-28 16:15:15-0400 [myspider] INFO: Spider closed (finished)\n```.", "issue_status": "Closed", "issue_reporting_time": "2014-01-28T20:21:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1451": {"issue_url": "https://github.com/scrapy/scrapy/issues/562", "issue_id": "#562", "issue_summary": "UnicodeEncodeError in SgmlLinkExtractor when using restrict_xpaths", "issue_description": "Contributor\nrmax commented on Jan 24, 2014\nThis issue has been addressed before by #285 but at the time none of the proposed alternative solution have made it into Scrapy.\nEven though the solution proposed in #285 was a good workaround, it was discarded because it returned a different response.\nBut by the same argument, the restrict_xpaths argument already causes to modify the body content and thus the link extractor acts on a different body than the original.\nHere is an example:\nIn [6]: from scrapy.selector import Selector\n\nIn [7]: from scrapy.http import HtmlResponse\n\nIn [8]: resp = HtmlResponse('http://example.com', body='<html><body><p>&hearts;</p></body></html>')\n\nIn [9]: Selector(resp).extract()\nOut[9]: u'<html><body><p>\\u2665</p></body></html>'\nSo, the link extractor by using the selector to build a fragment of the html get all the entities converted and thus causing the failure when trying to re-encode the body.\nIn [10]: from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n\nIn [11]: resp = HtmlResponse('http://example.com', encoding='iso8859-15', body='<html><body><p>&hearts;</p></body></html>')\n\nIn [12]: SgmlLinkExtractor(restrict_xpaths='//p').extract_links(resp)\n---------------------------------------------------------------------------\nUnicodeEncodeError                        Traceback (most recent call last)\n<ipython-input-5-7d00fa2c152d> in <module>()\n----> 1 SgmlLinkExtractor(restrict_xpaths='//p').extract_links(resp)\n\n/home/rolando/Projects/scrapy/scrapy/contrib/linkextractors/sgml.pyc in extract_links(self, response)\n    122                             for x in self.restrict_xpaths\n    123                             for f in sel.xpath(x).extract()\n--> 124                             ).encode(response.encoding)\n    125         else:\n    126             body = response.body\n\n/usr/lib/python2.7/encodings/iso8859_15.pyc in encode(self, input, errors)\n     10 \n     11     def encode(self,input,errors='strict'):\n---> 12         return codecs.charmap_encode(input,errors,encoding_table)\n     13 \n     14     def decode(self,input,errors='strict'):\n\nUnicodeEncodeError: 'charmap' codec can't encode character u'\\u2665' in position 3: character maps to <undefined>\nIt's a bummer that the most widely used link extractor and the only one that supports the handy argument restrict_xpaths fails in such simple and very common case.\nI don't know what's the best solution (a solution that can be backported to 0.22), but it seems if the Selector have the support to work in other encodings than unicode by a given parameter it might work well for this case. This because lxml can handle the entities in different encodings nicely:\nIn [46]: import lxml\n\nIn [47]: parser = lxml.etree.HTMLParser()\n\nIn [48]: parser.feed('<p>&hearts;</p>')\n\nIn [52]: fragment = parser.close()\n\nIn [53]: lxml.html.tostring(fragment)\nOut[53]: '<html><body><p>&#9829;</p></body></html>'\n\nIn [54]: lxml.html.tostring(fragment, encoding='unicode')\nOut[54]: u'<html><body><p>\\u2665</p></body></html>'\n\nIn [55]: lxml.html.tostring(fragment, encoding='ascii')\nOut[55]: '<html><body><p>&#9829;</p></body></html>'\n\nIn [56]: lxml.html.tostring(fragment, encoding='iso8859-15')\nOut[56]: '<html><body><p>&#9829;</p></body></html>'\nIn this way, even though the entities still are being converted, the re-encoding of the extracted fragment won't fail.", "issue_status": "Closed", "issue_reporting_time": "2014-01-24T17:28:23Z", "fixed_by": "#565", "pull_request_summary": "replace unencodeable codepoints with html entities", "pull_request_description": "Member\ndangra commented on Jan 27, 2014\nfixes #562 and #285", "pull_request_status": "Merged", "issue_fixed_time": "2014-01-28T06:01:39Z", "files_changed": [["2", "scrapy/contrib/linkextractors/sgml.py"], ["7", "scrapy/tests/test_contrib_linkextractors.py"]]}, "1452": {"issue_url": "https://github.com/scrapy/scrapy/issues/560", "issue_id": "#560", "issue_summary": "Confusing `default` argument for `Field` class in docs", "issue_description": "Contributor\nrmax commented on Jan 23, 2014\nAn item loaders example shows this code:\nfrom scrapy.item import Item, Field\nfrom scrapy.contrib.loader.processor import MapCompose, Join, TakeFirst\n\nfrom scrapy.utils.markup import remove_entities\nfrom myproject.utils import filter_prices\n\nclass Product(Item):\n    name = Field(\n        input_processor=MapCompose(remove_entities),\n        output_processor=Join(),\n    )\n    price = Field(\n        default=0,\n        input_processor=MapCompose(remove_entities, filter_prices),\n        output_processor=TakeFirst(),\n    )\nThe argument default=0 currently does nothing in that example. What's the purpose of having default=0 in that example?\nFurthermore, that particular example needs a little more work to use it. It might be good to have as a policy that all examples should work out of the box by simply copy pasting the content.\nI was attempting to make a working example but even remove_entities is misleading because actually it doesn't remove the entities but convert them to unicode.\nAnyway, I worked out an example that I think it make sense:\nfrom scrapy.item import Item, Field\nfrom scrapy.contrib.loader.processor import MapCompose, Join, TakeFirst\nfrom w3lib.html import remove_tags\n\n\ndef filter_price(value):\n    \"\"\"Ignores non-number value.\"\"\"\n    if value.isdigit():\n        return value\n\n\nclass Product(Item):\n    \"\"\"An item with input/output processors declared in the fields.\"\"\"\n    name = Field(\n        input_processor=MapCompose(remove_tags),\n        output_processor=Join(),\n    )\n    price = Field(\n        input_processor=MapCompose(remove_tags, filter_price),\n        output_processor=TakeFirst(),\n    )\n>>> from scrapy.contrib.loader import ItemLoader\n>>> il = ItemLoader(item=Product())\n>>> il.add_value('name', [u'Welcome to my', u'<strong>website</strong>'])\n>>> il.add_value('price', [u'&euro;', u'<span>1000</span>'])\n>>> il.load_item()\n{'name': u'Welcome to my website', 'price': u'1000'}\nIf looks good to you I can make a PR for this issue.", "issue_status": "Closed", "issue_reporting_time": "2014-01-23T16:40:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1453": {"issue_url": "https://github.com/scrapy/scrapy/issues/554", "issue_id": "#554", "issue_summary": "Make \"xpath\" and \"css\" Selector methods available on response", "issue_description": "Member\nkmike commented on Jan 22, 2014\nI understand that this is a compromise: HTTP response shouldn't be tied to lxml. But combined with #548 and #494, it allows to reduce the ceremony even further. Instead of this:\nimport urlparse\nfrom scrapy.spider impoty Spider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request\n\nclass MySpider(Spider):\n    # ...\n    def parse(self, response):\n        sel = Selector(response)\n        for href in sel.xpath('//a/@href').extract():\n            url = urlparse.urljoin(response.url, href)\n            yield Request(url)\nwe'll be able to write this:\nimport scrapy\n\nclass MySpider(scrapy.Spider):\n    # ...\n    def parse(self, response):\n        for href in response.xpath('//a/@href').extract():\n            yield scrapy.Request(url)\nThe latter variant is 2x shorter, and readability is still fine (I'd say it is improved).\nI think that increased internal complexity is justified by better API here.\nIn browser js document knows about css and xpath selectors, and they are doing just fine. The distinction between \"response\" and \"document\" is not very clear - document is the response js code works on. It doesn't provide raw http data though. But our TextResponse also doesn't work only on raw http data - it checks some headers an provides the decoded body.\nAlso check ItemLoader class - it can be initialized with either response or selector; this tells us Selector and Response already can act similar.\nImplementation may involve renaming existing Response to something like RawResponse or HttpResponse, or adding xpath and css methods only for TextResponse. Selector could be created only on demand internally. We may also ditch LxmlDocument cache and store parsed tree as a response attribute.", "issue_status": "Closed", "issue_reporting_time": "2014-01-22T10:11:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1454": {"issue_url": "https://github.com/scrapy/scrapy/issues/548", "issue_id": "#548", "issue_summary": "Support relative urls better", "issue_description": "Member\nkmike commented on Jan 18, 2014\nBuilding an URL relative to current URL is a very common task; currently users are required to do that themselves - import urlparse and then urlparse.urljoin(response.url, href).\nWhat do you think about adding a shortcut for that - something like response.urljoin(href) or response.absolute_url(href)?\nAnother crazy idea is to support relative urls directly, by assuming they are relative to a response.url. I think it can be implemented as a spider middleware. I actually like this idea :)\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2014-01-18T00:42:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1455": {"issue_url": "https://github.com/scrapy/scrapy/issues/546", "issue_id": "#546", "issue_summary": "\"scrapy crawl myspider -o data.csv\" should write csv", "issue_description": "Member\nkmike commented on Jan 18, 2014\nWhat do you think about removing a need to write this?\n$ scrapy crawl myspider -t csv -o data.csv\nOf course, other file extensions could be also supported.", "issue_status": "Closed", "issue_reporting_time": "2014-01-18T00:15:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1456": {"issue_url": "https://github.com/scrapy/scrapy/issues/545", "issue_id": "#545", "issue_summary": "Make 0.22 branch default in RTFD", "issue_description": "Member\nkmike commented on Jan 18, 2014\nWhat do you think about making 0.22 branch default in RTFD?\nThis will make \"latest\" point to latest stable release instead of pointing to master. The option can be found in \"advanced settings\" menu - it is not the same as making 0.22 doc version default. It may be necessary to enable docs for \"master\" branch manually after doing that.\nWe can also enable canonical URLs (main settings menu) - I think this should solve issues with Google searches pointing users to versions of Scrapy that are too old or too modern.", "issue_status": "Closed", "issue_reporting_time": "2014-01-17T19:36:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1457": {"issue_url": "https://github.com/scrapy/scrapy/issues/542", "issue_id": "#542", "issue_summary": "Requests scheduled when idle never go through the spider middlewares", "issue_description": "nside commented on Jan 17, 2014\nWhen my spider enters the idle state I schedule a request through the engine like this:\nself.crawler.engine.schedule( req , self)\nand I expect that request to go through the spider middlewares. But after some investigation it looks like only requests returned by Spider.start_requests and those returned from callbacks are processed by these middlewares.\nMaybe I'm scheduling the request in a bad way but if so it shouldn't be public (ie I'd prefix schedule with _).", "issue_status": "Closed", "issue_reporting_time": "2014-01-17T06:16:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1458": {"issue_url": "https://github.com/scrapy/scrapy/issues/540", "issue_id": "#540", "issue_summary": "invalid", "issue_description": "Member\ndangra commented on Jan 17, 2014\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2014-01-16T22:03:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1459": {"issue_url": "https://github.com/scrapy/scrapy/issues/532", "issue_id": "#532", "issue_summary": "ItemExporter serialize_field but not recursive (item encoding problem)", "issue_description": "thiagof commented on Jan 15, 2014\nHello\nI'm using the JsonItemExporter. My item has the following structure:\nitem = {\n    'content': u'O presente curso de No\\xc3\\xa7\\xc3\\xb5es de Rela\\xc3\\xa7\\xc3\\xb5es Internacionais tem como foco as rela\\xc3\\xa7\\xc3\\xb5es econ\\xc3\\xb4micas internacionais',\n    'title': u'No\\xc3\\xa7\\xc3\\xb5es de Rela\\xc3\\xa7\\xc3\\xb5es Internacionais',\n    'terms_names': {\n        'category': u'Administra\\xe7\\xe3o'\n    }\n}\nThe problem is on BaseItemExporter.serialize_field. The default serializer converts only unicode to str.\nSo in this example, where I have a dict value for a field (terms_names), serialize_field do nothing and the resulting item have content and title as utf-8 str but category property remains unicode. And this cant be encoded/mixed by json! - http://pastebin.com/mfkhHYc9\nPossible solution: recursively convert properties.", "issue_status": "Closed", "issue_reporting_time": "2014-01-15T02:32:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1460": {"issue_url": "https://github.com/scrapy/scrapy/issues/529", "issue_id": "#529", "issue_summary": "MemoryUsage extension works incorrectly on OS X", "issue_description": "Member\nkmike commented on Jan 15, 2014\nThe problem is that it uses ru_maxrss which is non-standard. It is in KB on Linux, but on OS X 10.9.1 it is in bytes:\n$ python -c \"import resource; print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\"\n4235264\nI don't know if it is in bytes or in kilobytes in earlier OS X versions. It'll be great if somebody who have not updated to Mavericks check that. Anyone here? If not supporting OS X versions earlier than Maverick is fine, we could add a sys.platform check.", "issue_status": "Closed", "issue_reporting_time": "2014-01-14T21:16:13Z", "fixed_by": "#530", "pull_request_summary": "Fix MemoryUsage extension for OS X", "pull_request_description": "Member\nkmike commented on Jan 15, 2014\nThis fixes GH-529.", "pull_request_status": "Merged", "issue_fixed_time": "2014-01-14T21:56:02Z", "files_changed": [["10", "scrapy/contrib/memusage.py"]]}, "1461": {"issue_url": "https://github.com/scrapy/scrapy/issues/528", "issue_id": "#528", "issue_summary": "lxml based link extractor with similar functionnality to SgmlLinkExtractor", "issue_description": "Contributor\nredapple commented on Jan 13, 2014\nSgmlLinkExtractor can choke on some pages that lxml is fine with.\nhttps://groups.google.com/forum/#!topic/scrapy-users/iA1VzcJYpJE\nCurrently, LxmlParserLinkExtractor doesnt have some of SgmlLinkExtractor's useful constructor parameters like allow, deny etc.\nAlso, as lxml is already used for the Selector implementation, it would make sense to reuse the same lxml-parsed document (LxmlParserLinkExtractor doesnt use `LxmlDocument``at the moment)", "issue_status": "Closed", "issue_reporting_time": "2014-01-13T11:08:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1462": {"issue_url": "https://github.com/scrapy/scrapy/issues/526", "issue_id": "#526", "issue_summary": "Look at Issue #527", "issue_description": "Contributor\nferdyrod commented on Jan 13, 2014\nError.", "issue_status": "Closed", "issue_reporting_time": "2014-01-13T06:10:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1463": {"issue_url": "https://github.com/scrapy/scrapy/issues/518", "issue_id": "#518", "issue_summary": "Improve structure of Scrapy settings documentation", "issue_description": "Member\npablohoffman commented on Jan 8, 2014\n#515 got me thinking that some Scrapy settings are missed because they aren't documented in the main settings page (docs/topics/settings.rst).\nOne approach I came up for fixing this was to generate a list of all settings (regardless of which file/doc they're documented) but i couldn't find a way in sphinx to generate a list of all entities of a single role (in this case, \"setting\").\nAn alternative approach would to adopt the policy/convention of documenting all settings in the settings.rst file (perhaps splitting them in one section per component/middleware/etc) and refer to that file from the other docs (for example, donwloader middlewares in downloader-middleware.rst would refer to settings.rst for their settings documentation).\nCurrently, we used a mixed (probably half-transitioned) approach where we document settings separately (in the file where their component is documented) and also mix core & component settings in settings.rst. This proposal also aims to resolve that inconsistency.\nThoughts?", "issue_status": "Closed", "issue_reporting_time": "2014-01-08T06:06:24Z", "fixed_by": "#1220", "pull_request_summary": "Building settings list from docs", "pull_request_description": "Member\neliasdorneles commented on May 10, 2015\nHey, folks!\nSo, this PR intends to fix #518, changing the scrapydocs extension to do the following:\ncollect all setting definitions in the documentation in the env variable\nadds a directive settingslist which will render a list of settings defined anywhere in the documentation except in the current document (I added this restriction to make it less verbose, the complete list is too big).\nDoes this look good?", "pull_request_status": "Merged", "issue_fixed_time": "2015-05-13T17:43:19Z", "files_changed": [["83", "docs/_ext/scrapydocs.py"], ["10", "docs/topics/settings.rst"]]}, "1464": {"issue_url": "https://github.com/scrapy/scrapy/issues/516", "issue_id": "#516", "issue_summary": "AttributeError: 'ItemLoader' object has no attribute 'add_xpath'", "issue_description": "busla commented on Jan 6, 2014\nHi all.\nI just started using Scrapy and things were working fine until I tried moving from items to ItemLoaders.\nI get an error that ItemLoader doesn\u00b4t have an 'add_xpath' attribute which makes no sense since it\u00b4s a method.\nHere is my bPaste, hopefully you can look at it:\nhttp://bpaste.net/show/8HB1lYL7rZ0xuWSkguPT/\nI\u00b4m parsing Facebook events from public fan pages (yes, my IP will probably get blocked one day).", "issue_status": "Closed", "issue_reporting_time": "2014-01-05T21:52:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1465": {"issue_url": "https://github.com/scrapy/scrapy/issues/513", "issue_id": "#513", "issue_summary": "Scrapy tests fails under pip 1.5", "issue_description": "Member\ndangra commented on Jan 4, 2014\nmitmproxy depends on PIL but in pip 1.5 it won't install because of pypa/pip#1055\nDownloading/unpacking PIL (from mitmproxy)\nCould not find any downloads that satisfy the requirement PIL (from mitmproxy)\nSome externally hosted files were ignored (use --allow-external PIL to allow).\nCleaning up...\nNo distributions at all found for PIL (from mitmproxy)", "issue_status": "Closed", "issue_reporting_time": "2014-01-03T19:56:00Z", "fixed_by": "#514", "pull_request_summary": "Mitmproxy in pip15", "pull_request_description": "Member\ndangra commented on Jan 4, 2014\nfixes #513", "pull_request_status": "Merged", "issue_fixed_time": "2014-01-03T23:31:30Z", "files_changed": [["9", "tests-requirements.txt"], ["11", "tox.ini"]]}, "1466": {"issue_url": "https://github.com/scrapy/scrapy/issues/511", "issue_id": "#511", "issue_summary": "ImportError: cannot import name Spider", "issue_description": "C47Joe commented on Jan 1, 2014\nUPDATE: If you are experiencing this issue, please install Scrapy 0.22 (apt-get install scrapy-0.22)\nI'm running the Scrapy Intro tutorial so I know very little about the program, but I keep getting an import error with Spider. I'm just doing a simple copy/paste of the code in the tutorial.\njoeys-imac:tutorial cappuccino$ scrapy crawl dmoz\nTraceback (most recent call last):\n  File \"/Users/cappuccino/anaconda/bin/scrapy\", line 4, in <module>\n    execute()\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\n    func(*a, **kw)\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\n    cmd.run(args, opts)\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 47, in run\n    crawler = self.crawler_process.create_crawler()\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/crawler.py\", line 87, in create_crawler\n    self.crawlers[name] = Crawler(self.settings)\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/crawler.py\", line 25, in __init__\n    self.spiders = spman_cls.from_crawler(self)\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/spidermanager.py\", line 35, in from_crawler\n    sm = cls.from_settings(crawler.settings)\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/spidermanager.py\", line 31, in from_settings\n    return cls(settings.getlist('SPIDER_MODULES'))\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/spidermanager.py\", line 22, in __init__\n    for module in walk_modules(name):\n  File \"/Users/cappuccino/anaconda/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 68, in walk_modules\n    submod = import_module(fullpath)\n  File \"/Users/cappuccino/anaconda/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/Users/cappuccino/Documents/Python Scraping/tutorial/tutorial/spiders/dmoz_spider.py\", line 1, in <module>\n    from scrapy.spider import Spider\nImportError: cannot import name Spider", "issue_status": "Closed", "issue_reporting_time": "2014-01-01T16:46:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1467": {"issue_url": "https://github.com/scrapy/scrapy/issues/509", "issue_id": "#509", "issue_summary": "Ubuntu repositories", "issue_description": "Member\ndangra commented on Dec 31, 2013\nCurrent repositories setup requires maintaining an apt repository per codename (precise, quantal, raring, saucy, trusty,...) and it bugs us on every new ubuntu release.\nThe true is that we build debian packages on a Precise host and upload the same package to all repositories. There is a legacy reason for using multiples repos per codename, we started publishing and building debian packages in Lucid (Python 2.6), when Precise arrived we had to build for Python 2.7. Lucid packages were published for Karmic, Maverick and Natty, while Precise for the others. There was also the ubuntu switch to Upstart that affected Scrapyd packaging by that time.\nThere are two ideas flowing around:\nUnify repositories and install instructions to:\ndeb http://archive.scrapy.org/ubuntu scrapy main\nMove repositories to Ubuntu PPA managed by Scrapy team.\noption (1) is simple and will work as far as Python2.7 is available in ubuntu.\noption (2) has the advantage that a new debian package is built per codename, and we don't rely on ScrapingHub infrastructure to build and distribute debs.\nI intentionally left out the discussion about renaming scrapy-VERSION to scrapy, but it may be related if we want to publish oldstable/stable/trunk versions under the same name but in different repository components.", "issue_status": "Closed", "issue_reporting_time": "2013-12-31T16:41:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1468": {"issue_url": "https://github.com/scrapy/scrapy/issues/500", "issue_id": "#500", "issue_summary": "dbm-based cache can become corrupted when spider is killed", "issue_description": "Member\nkmike commented on Dec 27, 2013\nHi,\nScrapy uses shelve-based cache by default; this could lead to a bad experience with scrapy because shelve databases can become corrupted if spider is stopped forcefully.\nWhat do you think about the following?\nCheck if it can be fixed - for example, by closing databases at certain points;\nif it can't be fixed, then switch to an another cache backend.\nThis ticket could be related: #491 - I haven't looked at that ticket in details, but user mentions close_spider is not called on middleware - this could cause DB corruption for dbm backend. Any ideas?\nFile-based cache doesn't seem to have this issue. Another option is sqlite - in my experience it is much harder to get sqlite database corrupted, compared to dbm.", "issue_status": "Closed", "issue_reporting_time": "2013-12-27T13:01:51Z", "fixed_by": "#626", "pull_request_summary": "[WIP] Add a LevelDB cache backend", "pull_request_description": "Member\ndangra commented on Mar 6, 2014\nThe great part about LevelDB is its resilience to crashes, I killed the process multiple times while it was putting keys and the db never corrupted.\nThe not so good part is that installing python-leveldb from pypi is not a just-works experience. And the version in pypi is outdated compared to https://code.google.com/p/py-leveldb/. UPDATE: pypi release was updated to pyleveldb 0.193\nfix #500", "pull_request_status": "Merged", "issue_fixed_time": "2014-06-23T18:30:14Z", "files_changed": [["21", "docs/topics/downloader-middleware.rst"], ["63", "scrapy/contrib/httpcache.py"], ["7", "scrapy/tests/test_downloadermiddleware_httpcache.py"], ["1", "tox.ini"]]}, "1469": {"issue_url": "https://github.com/scrapy/scrapy/issues/495", "issue_id": "#495", "issue_summary": "rename BaseSpider to Spider", "issue_description": "Member\nkmike commented on Dec 19, 2013\nWhat do you think about renaming BaseSpider to Spider?\n\"Base\" prefix usually suggests that the class is somewhat low-level and most likely shouldn't be used in user code, unless heavy customization is needed.\nFor example, scrapy has BaseItem and Item, and users should use Item; django has internal models.BaseManager and user-facing Manager, werkzeug has BaseRequest and Request, there are unittest.BaseTestSuite and unittest.TestSuite in standard library, etc.\nI think Spider is a better name, because it is fine to use BaseSpider in user code instead of CrawlSpider or other BaseSpider subclasses, but the name suggests the opposite.", "issue_status": "Closed", "issue_reporting_time": "2013-12-18T23:56:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1470": {"issue_url": "https://github.com/scrapy/scrapy/issues/494", "issue_id": "#494", "issue_summary": "improve scrapy top-level namespace", "issue_description": "Member\nkmike commented on Dec 19, 2013\nHey,\nI think that @redapple is right in #488 that base spider template needs more imports by default, but IMHO introducing more code generation is not a solution.\nScrapy top-level namespace is a bit funky now:\n>>> import scrapy\n>>> dir(scrapy)\n['__builtins__',\n '__doc__',\n '__file__',\n '__name__',\n '__package__',\n '__path__',\n '__version__',\n '_txv',\n 'boto',\n 'django',\n 'optional_features',\n 'os',\n 'pkgutil',\n 'print_function',\n 'sys',\n 'twisted_version',\n 'urlparse_monkeypatches',\n 'version_info',\n 'warnings',\n 'xlib']\nit contains boto and django, but doesn't contain shortcuts like Request, Selector and BaseSpider. What do you think about making them available as scrapy.Request etc and removing unintended exports?\nCurrently to write a basic spider user has to remember a lot of import locations. Scrapy docs omit imports sometimes (see #493), this also doesn't help.\nMy current list is BaseSpider (why isn't it named Spider, by the way?), Request, FormRequest and Selector, but I don't see much downsides in being more generous (Item? Field?).", "issue_status": "Closed", "issue_reporting_time": "2013-12-18T23:20:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1471": {"issue_url": "https://github.com/scrapy/scrapy/issues/493", "issue_id": "#493", "issue_summary": "Required imports are sometimes missing in docs", "issue_description": "Member\nkmike commented on Dec 19, 2013\nSome examples of omitted imports:\nhttp://doc.scrapy.org/en/0.20/topics/spiders.html#spider-arguments\nhttp://doc.scrapy.org/en/0.20/intro/overview.html#write-a-spider-to-extract-the-data - check the first example\nhttp://doc.scrapy.org/en/0.20/topics/spiders.html#scrapy.spider.BaseSpider.start_requests - this is the first occurance of FormRequest, by the way;\nhttp://doc.scrapy.org/en/0.20/topics/images.html#usage-example\nI think we should review docs and add missing imports.", "issue_status": "Closed", "issue_reporting_time": "2013-12-18T23:16:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1472": {"issue_url": "https://github.com/scrapy/scrapy/issues/492", "issue_id": "#492", "issue_summary": "Provide Ubuntu package for Saucy Salamander", "issue_description": "boite commented on Dec 18, 2013\nAre there plans to provide a package for Saucy?\nThe latest install docs currently suggest:-\n\"Don\u2019t use thepython-scrapy package provided by Ubuntu, they are typically too old and slow to catch up with latest Scrapy.\"\nBut the most recent version of Ubuntu isn't yet supported in the official Ubuntu Packages.", "issue_status": "Closed", "issue_reporting_time": "2013-12-18T13:53:06Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1473": {"issue_url": "https://github.com/scrapy/scrapy/issues/491", "issue_id": "#491", "issue_summary": "RaiseSpider Exception may not close current db connection", "issue_description": "michaelnguyen547 commented on Dec 18, 2013\nIt is just a theory I derived from a high aborted clients stats when i schedule to have 200 spiders running via scrapyd.\nFull explanation/description is posted on Stackoverflow ( http://stackoverflow.com/questions/20647382/scrapyd-schedule-200-spiders-cause-massive-mysql-aborted-clients ) and on Scrapy user group (https://groups.google.com/forum/#!topic/scrapy-users/HEaXJT3d4iQ)", "issue_status": "Closed", "issue_reporting_time": "2013-12-18T00:24:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1474": {"issue_url": "https://github.com/scrapy/scrapy/issues/488", "issue_id": "#488", "issue_summary": "Make \"basic\" the default template for genspider", "issue_description": "Member\npablohoffman commented on Dec 12, 2013\nWhat do you think about making \"basic\" the default template for the genspider command?", "issue_status": "Closed", "issue_reporting_time": "2013-12-12T17:56:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1475": {"issue_url": "https://github.com/scrapy/scrapy/issues/486", "issue_id": "#486", "issue_summary": "RTFD documentation search is broken in Firefox 25", "issue_description": "yourcelf commented on Dec 8, 2013\nSearching the documentation in Firefox 25 is busted (returns no results for any query), with the following error in the console:\nError: Syntax error, unrecognized expression: #rtd-search-form input[name='q' @ https://media.readthedocs.org/javascript/jquery/jquery-2.0.3.min.js:4\nExample URL that doesn't work: http://doc.scrapy.org/en/latest/search.html?q=duplication&check_keywords=yes&area=default\nIt seems to be working fine in Chromium 30.", "issue_status": "Closed", "issue_reporting_time": "2013-12-07T20:37:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1476": {"issue_url": "https://github.com/scrapy/scrapy/issues/483", "issue_id": "#483", "issue_summary": "Logging system writes lots of Error when initiated", "issue_description": "verganis commented on Dec 5, 2013\nWhen I try to use the scrapy logging system and I run the scraper I get a lot of error messages on STDOUT.\nI call log.start() in the init method of my spider only once.\nCode Below:\nclass BilancioSpider(BaseSpider):\n    name = \"bilancio\"\n    allowed_domains = [\"http://finanzalocale.interno.it\"]\n    start_urls = [\"http://finanzalocale.interno.it\",]\n    lista_comuni = []\n\n    def __init__(self,**kwargs):\n        log.start()\n        super(BilancioSpider, self).__init__(self.name, **kwargs)\n\n        # initialize start_urls with all comune codes, years and type of bilancio\n        udr = None\n        udr = UnicodeDictReader(f=open(FILE_PATH,mode='r'), dialect=\"excel\",)\n        return\n\n    def parse(self, response):\n        return None", "issue_status": "Closed", "issue_reporting_time": "2013-12-05T14:57:54Z", "fixed_by": "#1060", "pull_request_summary": "[MRG+1] Python logging", "pull_request_description": "Member\ncurita commented on Mar 2, 2015\nThis PR switches Twisted logging system to the Python standard one.\nIt's pretty much done, though it's missing documentation (UPDATE: All docs related to logging have been updated) since I want to hear feedback first. I implemented the bare minimum changes to support all previous use cases, but new functionality could definitely be added.\nThe pull request closes a few open issues and other PRs. This closes #921, closes #881, closes #877, closes #875, closes #483, closes #481, closes #433, and closes #265.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-29T18:20:34Z", "files_changed": [["30", "conftest.py"], ["2", "docs/index.rst"], ["17", "docs/intro/tutorial.rst"], ["30", "docs/topics/benchmarking.rst"], ["5", "docs/topics/debug.rst"], ["8", "docs/topics/downloader-middleware.rst"], ["9", "docs/topics/extensions.rst"]]}, "1477": {"issue_url": "https://github.com/scrapy/scrapy/issues/477", "issue_id": "#477", "issue_summary": "OffsiteMiddleware allowing any link with the domain in it even if it is a link to another site", "issue_description": "manuchandraprasad commented on Nov 27, 2013\nNot sure if its a bug..\nOffsiteMiddleware currently allows any link if it just has an allowed domain somewhere in it. This causes the links which go to other sites with the allowed domain it being allowed.\nYou might have to nodify the regex checking to limit only the links which start with the allowed domains.", "issue_status": "Closed", "issue_reporting_time": "2013-11-26T21:20:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1478": {"issue_url": "https://github.com/scrapy/scrapy/issues/476", "issue_id": "#476", "issue_summary": "New Selector produce the wrong results", "issue_description": "OlgaCh commented on Nov 26, 2013\nFor the site\nhttps://careers.nscorp.com/sap/bc/webdynpro/sap/hrrcf_a_unreg_job_search?%20sap-client=100&sap-language=EN&sap-wd-configId=Z_P_ER_A_UNREG_JOB_SEARCH\nnew Selector and old HtmlXPathSelector are created the different resulting objects.\nObject s=Selector(response) has the wrong data.", "issue_status": "Closed", "issue_reporting_time": "2013-11-26T08:56:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1479": {"issue_url": "https://github.com/scrapy/scrapy/issues/473", "issue_id": "#473", "issue_summary": "Ability to not send specific headers in HTTP requests", "issue_description": "Contributor\nredapple commented on Nov 25, 2013\nSome web servers behave differently when they are receive or don't receive specific headers.\nFor example FeedBurner (http://feeds.feedburner.com/someblog) sends out XML RSS feeds only is you do not set the \"Referer\" header.\nThe idea would be to use the headers dict with some keys with a None value, and skip these headers when sending the HTTP request.\nCurrently, for the \"Referer\" example:\nheaders={\"Referer\": None} sends \"Referer: None\"\nheaders={\"Referer\": \"\"} sends \"Referer: \" (which works for the FeedBurner case, but is not satisfactory)\ndisable RefererMiddleware but that feels a bit heavy\n(for this FeedBurner thing, apparently adding ?format=xml also does the trick)", "issue_status": "Closed", "issue_reporting_time": "2013-11-25T16:03:20Z", "fixed_by": "#475", "pull_request_summary": "[MRG] Do not set Referer by default when its value is None", "pull_request_description": "Member\ndangra commented on Nov 25, 2013\nAnother implementation for #473, alternative #474.\ncloses #473.", "pull_request_status": "Merged", "issue_fixed_time": "2013-12-24T13:20:36Z", "files_changed": [["4", "scrapy/http/headers.py"], ["13", "scrapy/tests/mockserver.py"], ["19", "scrapy/tests/spiders.py"], ["32", "scrapy/tests/test_crawl.py"], ["8", "scrapy/tests/test_http_headers.py"]]}, "1480": {"issue_url": "https://github.com/scrapy/scrapy/issues/470", "issue_id": "#470", "issue_summary": "Support some EXSLT extensions by default in `Selector` when using XPath", "issue_description": "Contributor\nredapple commented on Nov 22, 2013\nSome EXSLT extensions are supported by default in lxml, provided one registers the corresponding namespaces when using XPath.\nSee http://www.exslt.org/ and http://lxml.de/xpathxslt.html#regular-expressions-in-xpath\nSelector could register these by default:\nset manipulation (http://www.exslt.org/set/index.html, namespace http://exslt.org/sets)\nand regular expressions (http://www.exslt.org/regexp/index.html, namespace http://exslt.org/regular-expressions)\nSome examples on how to use set operations:\nhttp://stackoverflow.com/questions/17722110/xpath-descendants-but-not-by-traversing-this-node/17727726#17727726\nhttp://stackoverflow.com/questions/18050803/what-is-the-next-tag-after-the-specific-tag-in-html-using-xpath/18055420#18055420\nRegarding implementation it would mean registering default namespaces and merging user-provided namespaces.", "issue_status": "Closed", "issue_reporting_time": "2013-11-22T17:29:57Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1481": {"issue_url": "https://github.com/scrapy/scrapy/issues/467", "issue_id": "#467", "issue_summary": "Document HANDLE_HTTPSTATUS_* settings", "issue_description": "Member\ndangra commented on Nov 21, 2013\nPlease, document settings added by #466.\n/cc @kalessin", "issue_status": "Closed", "issue_reporting_time": "2013-11-21T15:47:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1482": {"issue_url": "https://github.com/scrapy/scrapy/issues/463", "issue_id": "#463", "issue_summary": "[scrapy.telnet.TelnetConsole] Unhandled Error", "issue_description": "1000camels commented on Nov 19, 2013\nI've seen at least one other person with this exact error (http://blog.gmane.org/gmane.comp.python.scrapy.user). I am using 0.21.0 and they were using 0.20.0.\nIn general, when I try to telnet to the port, no prompt is returned. Then this error arrives from the spider.\nI assume it is entirely possible this is an issue on my machine. I am running OS X Mavericks (10.9)\n[scrapy.telnet.TelnetConsole] Unhandled Error\nTraceback (most recent call last):\nFile \"/Users/darcy/Dev/python/ve/lib/python2.7/site-packages/twisted/python/log.py\", line 73, in callWithContext\nreturn context.call({ILogContext: newCtx}, func, _args, *_kw)\nFile \"/Users/darcy/Dev/python/ve/lib/python2.7/site-packages/twisted/python/context.py\", line 118, in callWithContext\nreturn self.currentContext().callWithContext(ctx, func, _args, *_kw)\nFile \"/Users/darcy/Dev/python/ve/lib/python2.7/site-packages/twisted/python/context.py\", line 81, in callWithContext\nreturn func(args,*kw)\nFile \"/Users/darcy/Dev/python/ve/lib/python2.7/site-packages/twisted/internet/selectreactor.py\", line 151, in _doReadOrWrite\nwhy = getattr(selectable, method)()\n--- ---\nFile \"/Users/darcy/Dev/python/ve/lib/python2.7/site-packages/twisted/internet/tcp.py\", line 1063, in doRead\nprotocol = self.factory.buildProtocol(self._buildAddr(addr))\nFile \"/Users/darcy/Dev/python/ve/lib/python2.7/site-packages/twisted/internet/protocol.py\", line 123, in buildProtocol\np = self.protocol()\nFile \"/Users/darcy/Dev/python/ve/lib/python2.7/site-packages/scrapy/telnet.py\", line 56, in protocol\ntelnet_vars = self._get_telnet_vars()\nFile \"/Users/darcy/Dev/python/ve/lib/python2.7/site-packages/scrapy/telnet.py\", line 62, in _get_telnet_vars\nslots = self.crawler.engine.slots\nexceptions.AttributeError: 'ExecutionEngine' object has no attribute 'slots'", "issue_status": "Closed", "issue_reporting_time": "2013-11-19T02:44:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1483": {"issue_url": "https://github.com/scrapy/scrapy/issues/459", "issue_id": "#459", "issue_summary": "Homebrew package", "issue_description": "rossedman commented on Nov 11, 2013\nDoes anyone know who maintains the scrapy brew installation? It is currently still at 18.4 and I was wondering if and when it would be updated? Thanks a lot!", "issue_status": "Closed", "issue_reporting_time": "2013-11-11T16:23:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1484": {"issue_url": "https://github.com/scrapy/scrapy/issues/458", "issue_id": "#458", "issue_summary": "Test suite is failing under Twisted 13.2.0", "issue_description": "Member\ndangra commented on Nov 9, 2013\nTwisted 13.2.0 was released at 2013-11-08 (a day before Scrapy 0.20.0) and now one test is failing:\n$ trial scrapy.tests.test_utils_defer.DeferUtilsTest\nscrapy.tests.test_utils_defer\n  DeferUtilsTest\n    test_process_chain ...                                                 [OK]\n    test_process_chain_both ...                                            [OK]\n    test_process_parallel ...                                              [OK]\n    test_process_parallel_failure ...                                   [ERROR]\n\n===============================================================================\n[ERROR]\nTraceback (most recent call last):\nFailure: exceptions.TypeError: \n\nscrapy.tests.test_utils_defer.DeferUtilsTest.test_process_parallel_failure\n-------------------------------------------------------------------------------\nRan 4 tests in 0.007s\n\nFAILED (errors=1, successes=3)", "issue_status": "Closed", "issue_reporting_time": "2013-11-09T02:52:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1485": {"issue_url": "https://github.com/scrapy/scrapy/issues/453", "issue_id": "#453", "issue_summary": "https request with proxies.", "issue_description": "dustinthughes commented on Nov 5, 2013\nCurrently not having https support with proxy are really hurting getting info that is based on ip address location. Some site use ip address geo location deliver content. Example Google or Kobobooks.com uses this method.\nIf you can give us some insight our staff is willing to write this section for you so you can add this back to the active development. We are using older version of scrapy 0.16.5. We would like to add the fix to the 0.16.X branch.", "issue_status": "Closed", "issue_reporting_time": "2013-11-04T22:42:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1486": {"issue_url": "https://github.com/scrapy/scrapy/issues/451", "issue_id": "#451", "issue_summary": "item_scraped signal is not fired when crawler is called via API", "issue_description": "ghost commented on Nov 2, 2013\nWhen using the example from http://doc.scrapy.org/en/latest/topics/practices.html \"Run Scrapy from a script\" using Windows XP and Python 2.7 it seems the item_scraped passed signal is not fired. The scraping works and shows in its stats that \"item_scraped_count\" is 60, yet not a single time the associated function gets called.\nI connected it using\ncrawler.signals.connect(self._item_passed, signal=signals.item_scraped)\nI also tried\ncrawler.signals.connect(self._item_passed, signal=signals.item_passed)\ndispatcher.connect(self._item_passed_2, signals.item_scraped)\ndispatcher.connect(self._item_passed_2, signals.item_passed)\nwhich didn't get called either.\nitem_dropped isn't called either.\nI have no explicit pipelines defined, just a single basic spider which works fine as seen in the debug logs.\nConnecting other signals like\ncrawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\nworks on the other hand.\nThe main call is:\ndef _item_passed(self, item, response, spider):\n    print \"PASSED\"\n\ndef _item_passed_2(self, item, response, spider):\n    print \"PASSED-DISP\"\n\ndef _item_dropped(self, item, spider, exception):\n    print \"dropped\"\n\ndef _spider_opened(spider):\n    print \"spider opened\"\n\ndef run(self):\n    spider = TestSpider()\n    settings = get_project_settings()\n    crawler = Crawler(settings)\n    crawler.install()\n    crawler.signals.connect(reactor.stop, signal=signals.spider_closed)\n    crawler.signals.connect(self._item_passed, signal=signals.item_scraped)\n    crawler.signals.connect(self._item_passed, signal=signals.item_passed)\n    crawler.signals.connect(self._item_dropped, signal=signals.item_dropped)\n    crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n    crawler.configure()\n    crawler.crawl(spider)\n    dispatcher.connect(self._item_passed_2, signals.item_passed)\n    dispatcher.connect(self._item_passed_2, signals.item_scraped)\n\n    crawler.start()\n\n    log.start(loglevel=log.INFO)\n    reactor.run()", "issue_status": "Closed", "issue_reporting_time": "2013-11-01T20:55:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1487": {"issue_url": "https://github.com/scrapy/scrapy/issues/450", "issue_id": "#450", "issue_summary": "Crawlers won't shutdown gracefully on SIGINT", "issue_description": "demji commented on Nov 1, 2013\nHello,\nCrawlers won't shutdown gracefully since CrawlerProcess' _start_crawler method pops crawlers off the self.crawlers list, which is where the stop method looks for crawlers to stop.\nSteps to reproduce:\nscrapy crawl\nSend SIGINT via Ctrl-c\nCrawler continues running after displaying \"2013-11-01 09:24:49-0400 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force\"\n$ scrapy version -v\nScrapy : 0.18.4\nlxml : 3.2.3.0\nlibxml2 : 2.9.1\nTwisted : 13.1.0\nPython : 2.7.5 (default, Aug 17 2013, 13:35:16) - [GCC 4.6.3]\nPlatform: Linux-3.10.7-gentoo-r1-x86_64-Intel-R-Core-TM-2_Quad_CPU_Q9550@_2.83GHz-with-gentoo-2.2\nRegards,\ndemji", "issue_status": "Closed", "issue_reporting_time": "2013-11-01T13:57:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1488": {"issue_url": "https://github.com/scrapy/scrapy/issues/449", "issue_id": "#449", "issue_summary": "url_is_from_any_domain should use url.hostname instead of url.netloc", "issue_description": "vinayan3 commented on Oct 29, 2013\nFrom scrapy.utils.url:\ndef url_is_from_any_domain(url, domains):\n\"\"\"Return True if the url belongs to any of the given domains\"\"\"\nhost = parse_url(url).netloc\n[ . . .]\nAn example from the command line on the difference between netloc and hostname.\nimport urlparse\nurlparse.urlparse('http://www.iol.co.za:80/news/crime-courts/finally-perhaps-justice-for-anene-1.1598738').netloc\n'www.iol.co.za:80'\nurlparse.urlparse('http://www.iol.co.za:80/news/crime-courts/finally-perhaps-justice-for-anene-1.1598738').hostname\n'www.iol.co.za'\nAccording to the URI spec the port is not part of the hostname aka domain portion of the URL.\nhttp://en.wikipedia.org/wiki/URI_scheme#Generic_syntax", "issue_status": "Closed", "issue_reporting_time": "2013-10-29T17:00:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1489": {"issue_url": "https://github.com/scrapy/scrapy/issues/444", "issue_id": "#444", "issue_summary": "What are the best ways to scrap ads from social media websites using scrapy?", "issue_description": "subintp commented on Oct 28, 2013\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2013-10-27T20:22:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1490": {"issue_url": "https://github.com/scrapy/scrapy/issues/443", "issue_id": "#443", "issue_summary": "Problem with instaling on windows 7", "issue_description": "danio987 commented on Oct 26, 2013\nHello all, I am trying to install scrappy but i constantly getting this error:\nNameError: global name 'f' is not defined\nCould someone help me with that?", "issue_status": "Closed", "issue_reporting_time": "2013-10-26T17:22:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1491": {"issue_url": "https://github.com/scrapy/scrapy/issues/442", "issue_id": "#442", "issue_summary": "doc.scrapy.org \"latest\" version is confusing", "issue_description": "Contributor\nrmax commented on Oct 25, 2013\nWhen searching for scrapy docs, Google (the most used search engine) return links to latest version of the docs, for example: https://www.google.com/search?q=scrapy+downloader+middleware\nReturns as first result: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html\nEven though the title says 0.19, this can be misleading and many users start trying the examples with the stable version of scrapy (for instance 0.18) and it's likely are going to get errors due backward incompatible changes.\nSo, I propose one of the following:\nChange the \"latest\" version to a name that reflects that is the development version. (i.e., just \"dev\")\nEach \"latest\" page should include a notice/warning pointing out that the current page is for the development version and add a link to the latest stable version which is most likely the version that most users are using at the time.\nand 2.\nNote: I have no idea if 2. it's possible.", "issue_status": "Closed", "issue_reporting_time": "2013-10-25T14:41:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1492": {"issue_url": "https://github.com/scrapy/scrapy/issues/441", "issue_id": "#441", "issue_summary": "Allow sending HTML emails with scrapy.mail.MailSender", "issue_description": "Contributor\nBlender3D commented on Oct 24, 2013\nI've patched this locally by changing the send method:\ndef send(self, to, subject, body, cc=None, attachs=(), mime='text/plain', _callback=None):\n    if attachs:\n        msg = MIMEMultipart()\n    else:\n        msg = MIMENonMultipart(*mime.split('/'))\nBut it seems fragile. Any thoughts?", "issue_status": "Closed", "issue_reporting_time": "2013-10-24T16:25:05Z", "fixed_by": "#602", "pull_request_summary": "Added a mimetype parameter to `MailSender.send`", "pull_request_description": "Contributor\nBlender3D commented on Feb 18, 2014\nFixes #441", "pull_request_status": "Merged", "issue_fixed_time": "2014-02-20T20:05:14Z", "files_changed": [["5", "docs/topics/email.rst"], ["4", "scrapy/mail.py"], ["11", "scrapy/tests/test_mail.py"]]}, "1493": {"issue_url": "https://github.com/scrapy/scrapy/issues/439", "issue_id": "#439", "issue_summary": "AttributeError: 'ParseError' object has no attribute 'msg'", "issue_description": "subintp commented on Oct 23, 2013\nAm getting error like this.I doubt this error caused by a unknown exception.\nzoomcar@zoomcar-1:/media/d_drive/code/python/myhabit/myhabit$ scrapy crawl myhabit\n2013-10-23 11:38:28+0530 [scrapy] INFO: Scrapy 0.12.0.2546 started (bot: myhabit)\n2013-10-23 11:38:28+0530 [scrapy] DEBUG: Enabled extensions: TelnetConsole, SpiderContext, WebService, CoreStats, MemoryUsage, CloseSpider\n2013-10-23 11:38:28+0530 [scrapy] DEBUG: Enabled scheduler middlewares: DuplicatesFilterMiddleware\n2013-10-23 11:38:28+0530 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, RedirectMiddleware, CookiesMiddleware, HttpCompressionMiddleware, DownloaderStats\n2013-10-23 11:38:28+0530 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2013-10-23 11:38:28+0530 [scrapy] DEBUG: Enabled item pipelines:\n2013-10-23 11:38:28+0530 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n2013-10-23 11:38:28+0530 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n2013-10-23 11:38:28+0530 [myhabit] INFO: Spider opened\n2013-10-23 11:38:30+0530 [myhabit] DEBUG: Crawled (200) <GET https://www.amazon.com/ap/signin?openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&pageId=quarterdeck&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&clientContext=183-6909322-8613518&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.mode=checkid_setup&marketPlaceId=A39WRC2IB8YGEK&openid.assoc_handle=quarterdeck&openid.return_to=https%3A%2F%2Fwww.myhabit.com%2Fsignin&&siteState=http%3A%2F%2Fwww.myhabit.com%2Fhomepage%3Fhash%3Dpage%253Db%2526dept%253Dwomen%2526sale%253DA1VZ6QH7N57X0T%2526ref%253Dqd_nav_women_cur_0_A1VZ6QH7N57X0T> (referer: None)\n2013-10-23 11:38:30+0530 [myhabit] ERROR: Spider error processing https://www.amazon.com/ap/signin?openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&pageId=quarterdeck&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&clientContext=183-6909322-8613518&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.mode=checkid_setup&marketPlaceId=A39WRC2IB8YGEK&openid.assoc_handle=quarterdeck&openid.return_to=https%3A%2F%2Fwww.myhabit.com%2Fsignin&&siteState=http%3A%2F%2Fwww.myhabit.com%2Fhomepage%3Fhash%3Dpage%253Db%2526dept%253Dwomen%2526sale%253DA1VZ6QH7N57X0T%2526ref%253Dqd_nav_women_cur_0_A1VZ6QH7N57X0T (referer: )\nTraceback (most recent call last):\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1178, in mainLoop\nself.runUntilCurrent()\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\ncall.func(_call.args, *_call.kw)\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 362, in callback\nself._startRunCallbacks(result)\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 458, in _startRunCallbacks\nself._runCallbacks()\n--- ---\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 545, in _runCallbacks\ncurrent.result = callback(current.result, _args, *_kw)\nFile \"/media/d_drive/code/python/myhabit/myhabit/spiders/myhabit_spider.py\", line 12, in parse\ncallback=self.after_login)]\nFile \"/usr/lib/python2.7/dist-packages/scrapy/http/request/form.py\", line 44, in from_response\nencoding=encoding, backwards_compat=False)\nFile \"/usr/lib/python2.7/dist-packages/scrapy/xlib/ClientForm.py\", line 1085, in ParseFile\nreturn _ParseFileEx(file, base_uri, _args, *_kwds)[1:]\nFile \"/usr/lib/python2.7/dist-packages/scrapy/xlib/ClientForm.py\", line 1105, in _ParseFileEx\nfp.feed(data)\nFile \"/usr/lib/python2.7/dist-packages/scrapy/xlib/ClientForm.py\", line 877, in feed\nraise ParseError(exc)\nscrapy.xlib.ClientForm.ParseError: <ParseError instance at 0x2797a50 with str error:\nTraceback (most recent call last):\nFile \"/usr/lib/python2.7/dist-packages/twisted/python/reflect.py\", line 546, in _safeFormat\nreturn formatter(o)\nFile \"/usr/lib/python2.7/HTMLParser.py\", line 64, in str\nresult = self.msg\nAttributeError: 'ParseError' object has no attribute 'msg'\n>", "issue_status": "Closed", "issue_reporting_time": "2013-10-23T06:10:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1494": {"issue_url": "https://github.com/scrapy/scrapy/issues/433", "issue_id": "#433", "issue_summary": "Should Scrapy have logging shortcuts? (i.e. `log.debug`, `log.info`)", "issue_description": "WoLpH commented on Oct 21, 2013\nTo me it seems pretty tedious to have to do log.msg(something, logging.DEBUG) all of the time.\nIt would be very useful if there would be specific logging methods for the different levels.\nThis could be as simple as:\ndebug = lambda message: msg(message, DEBUG)\ninfo = lambda message: msg(message, INFO)\nOr a bit more complex like this:\ndef debug(message, *args, **kwargs):\n    assert not (args and kwargs), 'Cannot use named and unnamed parameters at the same time'\n    return msg(message % args or kwargs, DEBUG)\nIf desired I can create a pull request.", "issue_status": "Closed", "issue_reporting_time": "2013-10-21T12:01:22Z", "fixed_by": "#1060", "pull_request_summary": "[MRG+1] Python logging", "pull_request_description": "Member\ncurita commented on Mar 2, 2015\nThis PR switches Twisted logging system to the Python standard one.\nIt's pretty much done, though it's missing documentation (UPDATE: All docs related to logging have been updated) since I want to hear feedback first. I implemented the bare minimum changes to support all previous use cases, but new functionality could definitely be added.\nThe pull request closes a few open issues and other PRs. This closes #921, closes #881, closes #877, closes #875, closes #483, closes #481, closes #433, and closes #265.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-29T18:20:34Z", "files_changed": [["30", "conftest.py"], ["2", "docs/index.rst"], ["17", "docs/intro/tutorial.rst"], ["30", "docs/topics/benchmarking.rst"], ["5", "docs/topics/debug.rst"], ["8", "docs/topics/downloader-middleware.rst"], ["9", "docs/topics/extensions.rst"]]}, "1495": {"issue_url": "https://github.com/scrapy/scrapy/issues/428", "issue_id": "#428", "issue_summary": "Project named test causes error", "issue_description": "Contributor\nsrmaximiano commented on Oct 16, 2013\nWhen creating a new project with the command 'startproject' and setting the name of the project to 'test', the project is created but it will cause future errors, such as the error below (reported in issue #54):\n Traceback (most recent call last):\n  File \"/home/max/testscrapy/venv/bin/scrapy\", line 8, in \n    execfile(__file__)\n  File \"/home/max/workspace/scrapy/bin/scrapy\", line 4, in \n    execute()\n  File \"/home/max/workspace/scrapy/scrapy/cmdline.py\", line 108, in execute\n    settings = get_project_settings()\n  File \"/home/max/workspace/scrapy/scrapy/utils/project.py\", line 57, in get_project_settings\n    settings_module = __import__(settings_module_path, {}, {}, [''])\nImportError: No module named settings\nIn this case test is the name of a python library, and as such there is no settings module.\nTo avoid this problem we would like to add a validation, at least for 'test'. Is this something worth doing? We took some time before realizing, but on the other hand we can't cover all cases (i.e. project named 're'). What do you think?\nScrapy : 0.19.0\nlxml : 3.2.3.0\nlibxml2 : 2.7.8\nTwisted : 13.1.0\nPython : 2.7.3 (default, Sep 26 2013, 20:03:06) - [GCC 4.6.3]\nPlatform: Linux-3.8.0-31-generic-x86_64-with-Ubuntu-12.04-precise", "issue_status": "Closed", "issue_reporting_time": "2013-10-16T17:33:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1496": {"issue_url": "https://github.com/scrapy/scrapy/issues/427", "issue_id": "#427", "issue_summary": "FormRequest clickdata argument is not properly documented", "issue_description": "Member\nkmike commented on Oct 16, 2013\nDocs say: \"Also, if you want to change the control clicked (instead of disabling it) you can also use the clickdata argument\", but it is not documented how to use this argument.", "issue_status": "Closed", "issue_reporting_time": "2013-10-16T14:07:17Z", "fixed_by": "#645", "pull_request_summary": "Documentation for FormRequest.from_response clickdata parameter", "pull_request_description": "Member\ncurita commented on Mar 12, 2014\nFixes #427\nDocumentation about:\nclickdata parameter in Formrequest.from_response\nnr attribute in clickdata dict\ndefault behaviour when clickdata is None", "pull_request_status": "Merged", "issue_fixed_time": "2014-03-13T19:18:26Z", "files_changed": [["9", "docs/topics/request-response.rst"]]}, "1497": {"issue_url": "https://github.com/scrapy/scrapy/issues/417", "issue_id": "#417", "issue_summary": "`python setup.py bdist_rpm` fails with `cp: cannot stat 'examples': No such file or directory`", "issue_description": "ciupicri commented on Oct 8, 2013\n$ python setup.py bdist_rpm\n...\n+ cd /home/ciupicri/3rdparty-projects/scrapy/build/bdist.linux-x86_64/rpm/BUILD\n+ cd Scrapy-0.19.0\n+ DOCDIR=/home/ciupicri/3rdparty-projects/scrapy/build/bdist.linux-x86_64/rpm/BUILDROOT/Scrapy-0.19.0-1.x86_64/usr/share/doc/Scrapy-0.19.0\n+ export DOCDIR\n+ /usr/bin/mkdir -p /home/ciupicri/3rdparty-projects/scrapy/build/bdist.linux-x86_64/rpm/BUILDROOT/Scrapy-0.19.0-1.x86_64/usr/share/doc/Scrapy-0.19.0\n+ cp -pr docs /home/ciupicri/3rdparty-projects/scrapy/build/bdist.linux-x86_64/rpm/BUILDROOT/Scrapy-0.19.0-1.x86_64/usr/share/doc/Scrapy-0.19.0\n+ cp -pr examples /home/ciupicri/3rdparty-projects/scrapy/build/bdist.linux-x86_64/rpm/BUILDROOT/Scrapy-0.19.0-1.x86_64/usr/share/doc/Scrapy-0.19.0\ncp: cannot stat 'examples': No such file or directory\nerror: Bad exit status from /var/tmp/rpm-tmp.eslvBl (%doc)\n\n\nRPM build errors:\n    Bad exit status from /var/tmp/rpm-tmp.eslvBl (%doc)\nerror: command 'rpmbuild' failed with exit status 1\nIf I remove examples from setup.cfg it stops complaining.", "issue_status": "Closed", "issue_reporting_time": "2013-10-08T13:31:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1498": {"issue_url": "https://github.com/scrapy/scrapy/issues/415", "issue_id": "#415", "issue_summary": "DjangoItem + Postgresql fails with \"max_locks_per_transaction\" if run with several parallel crawls on scrapyd", "issue_description": "cbourjau commented on Oct 8, 2013\nThis is a follow up to http://stackoverflow.com/questions/19068308/access-django-models-with-scrapy-defining-path-to-django-project/19083978#19083978\nThe problem occurred on scrapy 0.18.0 and 0.18.1. Even though the title might suggest that the problem is scrapyd related I am fairly sure it is associated with DjangoItem\nThe problem occurred when running several spiders scrapping to a DjangoItem in parallel on scrapyd.\nThe DjangoItem is defined as usual:\nitems.py\nclass MyItem(DjangoItem):\ndjango_model = MyModel\nThe item is processed like this in the pipeline:\npipeline.py\n def process_item(self, item, spider):\n     try:\n        with transaction.commit_on_success():\n            item.save()\n     except IntegrityError, e:\n         spider.log(str(e), log.DEBUG)\n         return None\n     return item\nWhen several spiders are run in parallel at some point all spiders stop working with a \"max_locks_per_transaction\" error in the spiders log.\nI fixed the problem for myself by not using DjangoItem at the moment and instead am creating the model object directly prior to the save() in the pipeline:\nerror is not occurring when not using DjangoItem and instead having:\npipeline.py\nclass Django_pipeline(object):\ndef process_item(self, item, spider):\ntry:\nwith transaction.commit_on_success():\nobj = MyModel(**item)\nobj.save()\nexcept IntegrityError, e:\nspider.log(str(e), log.DEBUG)\nraise DropItem('Dropped item after IntegrityError')\nreturn item\nI believe the error is caused by DjangoItem creating to many model objects in parallel exceeding the postgresql max_locks_per_transaction limit but I have to little knowledge of Postgresql (and Django for that sake) to be certain.\nI would like to submit an actual example but am a little uncertain about which pages to crawl sufficiently long and in parallel to reproduce the issue while not having any legal issues with publishing the code...\nI hope this helps anyhow!", "issue_status": "Closed", "issue_reporting_time": "2013-10-07T21:11:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1499": {"issue_url": "https://github.com/scrapy/scrapy/issues/414", "issue_id": "#414", "issue_summary": "\"pip install Scrapy\" fails to build dependencies on Fedora 19", "issue_description": "castedo commented on Oct 7, 2013\nI was able to work around this failure of \"pip install Scrapy\" by manually using yum with the usual Fedora repositories:\nyum install libxslt-devel\nyum install pyOpenSSL\nyum install python-lxml\nyum install python-twisted\nand then doing pip install Scrapy.\nThe main error is that dependencies want gcc-4.5 but the default gcc on Fedora 19 is version 4.8:\n\"\n...\ncreating build/temp.linux-i686-2.7/src/lxml\ngcc-4.5 -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -grecord-gcc-switches -m32 -march=i686 -mtune=atom -fasynchronous-unwind-tables -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -grecord-gcc-switches -m32 -march=i686 -mtune=atom -fasynchronous-unwind-tables -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/libxml2 -I/tmp/pip-build-castedo/lxml/src/lxml/includes -I/usr/include/python2.7 -c src/lxml/lxml.etree.c -o build/temp.linux-i686-2.7/src/lxml/lxml.etree.o\nunable to execute gcc-4.5: No such file or directory\n\nerror: command 'gcc-4.5' failed with exit status 1\n....\n\"\nYou might want to update install instructions to warn folks that they might need to manually run yum for those dependencies.", "issue_status": "Closed", "issue_reporting_time": "2013-10-07T16:01:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1500": {"issue_url": "https://github.com/scrapy/scrapy/issues/411", "issue_id": "#411", "issue_summary": "Scrapy hangs if an exception raises in start_requests", "issue_description": "Member\nkmike commented on Oct 3, 2013\nSymptoms are the same as in #83, but the reason is different, so I opened a new ticket. This issue starts to happen after this commit: alexcepoi@902208c. @alexcepoi any ideas?", "issue_status": "Closed", "issue_reporting_time": "2013-10-03T00:59:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1501": {"issue_url": "https://github.com/scrapy/scrapy/issues/407", "issue_id": "#407", "issue_summary": "Twisted exception `AlreadyCalledError` when I follow the documentation", "issue_description": "gkb commented on Oct 2, 2013\nI worked through an example from the documentation on the scrapy shell. One of the commands there did not work however and I got an AlreadyCalledError from a deferred in Twisted.\nMore specifically, this example failed during the call to request.replace(method=\"POST\"). The method should return a new object according to its doc, but I think the reason Twisted throws the AlreadyCalledError is because it has run through the deferred's callbacks already in the new object. In other words, request.replace() is not returning an independent clone.\nHere's a gist containing the exact error message.", "issue_status": "Closed", "issue_reporting_time": "2013-10-02T00:43:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1502": {"issue_url": "https://github.com/scrapy/scrapy/issues/403", "issue_id": "#403", "issue_summary": "logging in middleware/spiders doesn't produce output", "issue_description": "em-cliqz commented on Sep 30, 2013\nI'm unable to get the spider.log function to produce any output when the LOG_FILE is specified on the command line.\n/usr/local/bin/scrapy crawl web -a job_name=test_job -a input_file=/tmp/test_job.json -s \"LOG_FILE=/tmp/test.log\" -s 'FEED_URI=/tmp/test.jl'\nI have a Filter with this function:\ndef process_start_requests(self, start_requests, spider):\nprint( \"HERE\" )\nspider.log( \"HERE\" )\nreturn start_requests\n\"HERE\" is written to stdout but not to the logfile. The Logfile is generated, it has the basic stats. A log statement in the spider as well is not recording any data.", "issue_status": "Closed", "issue_reporting_time": "2013-09-30T12:08:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1503": {"issue_url": "https://github.com/scrapy/scrapy/issues/398", "issue_id": "#398", "issue_summary": "Dynamic Creation of Item Classes", "issue_description": "Contributor\nLorenDavie commented on Sep 26, 2013\nI have a need to dynamically create item classes based on user input. It's not clear to me how to approach that with Scrapy, it seems to assume that a developer will be coding item classes manually.\nI created this utility function:\nfrom scrapy.item import DictItem, Field\n\ndef create_item_class(class_name,field_list):\n    field_dict = {}\n    for field_name in field_list:\n        field_dict[field_name] = Field()\n\n    return type(class_name,DictItem,field_dict)\nNot sure if this is the best way to approach this kind of problem with Scrapy, but I leave it here for your consideration. Thanks!", "issue_status": "Closed", "issue_reporting_time": "2013-09-26T14:41:06Z", "fixed_by": "#1018", "pull_request_summary": "Updates documentation on dynamic item classes.", "pull_request_description": "Contributor\nbarraponto commented on Jan 20, 2015\nFixes #398\nUsed dict comprehension for more pythonic code.", "pull_request_status": "Merged", "issue_fixed_time": "2015-01-19T20:12:02Z", "files_changed": [["6", "docs/topics/practices.rst"]]}, "1504": {"issue_url": "https://github.com/scrapy/scrapy/issues/396", "issue_id": "#396", "issue_summary": "inspect_response(response) yields incorrect response in IPython shell", "issue_description": "xEtherealx commented on Sep 25, 2013\nExample case (requires registration at example site, and even then would be hard to use as a use-case; modify to suit your needs): http://pastebin.com/GT8N893q\nIn the above example, the response.meta printout in after_submit callback does not match that within the inspect_response shell on the second iteration (the first is correct). It appears that inspect_response has a stale response the second time.", "issue_status": "Closed", "issue_reporting_time": "2013-09-24T19:06:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1505": {"issue_url": "https://github.com/scrapy/scrapy/issues/395", "issue_id": "#395", "issue_summary": "Selectors unified API", "issue_description": "Member\ndangra commented on Sep 24, 2013\nThe imminent addition of CSS selectors (#176) to Scrapy arises some questions about how inconvenient is the current Selectors API when it needs to support more than one query language.\nThe current interface for selectors has the following requirements:\nSelector must accept a scrapy.http.Response as first constructor argument\nSelector must implement a method .select(query) that matches nodes and returns a flattened list of elements of the same Selector type.\nThe list returned by .select() must implement Selector interface too (except for its constructor). i.e.: XPathSelectorList implements it for XPathSelectors.\nSelector must implement a method .extract() that serializes the matched nodes and returns a flattened list of unicode strings.\nA major drawback is that XPath and CSS selectors can't be nested, and auxiliary methods like .re() (regex parsing) has to be re-implemented for each query language.\nAlso, DOM parsers for XML and HTML differs, that is why XPath selectors have two classes: HtmlXPathSelector and XmlXPathSelector. Merging CSS selectors (#176) in current shape will add two more classes to the mix: HtmlCSSSelector and XmlCSSSelector.\nAlso, we used to support multiples backends for XPath Selectors (lxml and libxml2) but after migrating and using lxml backend as default since Scrapy 0.16, it is quite obvious that we are not going back to libxml2.\nThis ticket propose simplifying public and internal Selectors API while keeping backward compatibility on the public API (ie. the XPathSelector's family)\nProposal\nExpose a single public Selector class that:\nAccepts a response as first constructor argument\nImplements one method to select nodes per query language: .xpath() and .css()\nDeprecate .select() in favor of .xpath()\nSelection methods must return a list of Selector instances (as it does now).\nThe list is a SelectorList instance that forwards calls to its elements (as it does now)\nKeeps the .extract() method that serializes matching nodes (as it does now)\nImplements methods to handle namespaces: .register_namespace(), .remove_namespaces()\nKeeps the .re() method to extract nodes and parse the output with regular expressions (as it does now)\nPublic API changes are minimal, basically renaming .select() to .xpath() and unifying XML and HTML selectors into a single more powerful class.\ni.e.::\n>>> selector = Selector(response)\n>>> selector.css('#content span.title').xpath('a[contains(@href, \"foo\")]').extract()\n['<a href=\"http://coolsite.com/foo-link\"/>', ...] \nInternally, it will drop libxml2 backend and completely remove xpath backend selection by sticking to lxml.\nSelector class will switch between HTML and XML parsers based on response type (HtmlResponse vs XmlResponse) and also provide a way to force what parser to use.", "issue_status": "Closed", "issue_reporting_time": "2013-09-24T18:25:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1506": {"issue_url": "https://github.com/scrapy/scrapy/issues/394", "issue_id": "#394", "issue_summary": "scrapy selector return null when parsing url but ok when parsing saved url", "issue_description": "gtg944s commented on Sep 24, 2013\nI'm trying to scrape data-table from the web using scrapy selector but got an empty array. The funny thing is when I tried to save the file and scrape it I got the expected array (non-null). Information on Scrapy version, selector command, and expected response can be found below.\nScrapy Version\nScrapy : 0.18.2 lxml : 3.2.3.0 libxml2 : 2.9.0 Twisted : 13.1.0 Python : 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)] Platform: Windows-8-6.2.9200\nselector\nhxs.select('//table[contains(@Class,\"mnytbl\")]//tbody//td[contains(@headers,\"tbl\\x34\\x37a\")]//span/text()').extract()\nExpected Response\n[u'\\n1.26 Bil\\n \\n', u'\\n893.90 Mil\\n \\n', u'\\n924.87 Mil\\n \\n', u'\\n1.18 Bil\\n \\n', u'\\n1.55 Bil\\n \\n', u'\\n2.91 Bil\\n \\n', u'\\n3.96 Bil\\n \\n', u'\\n4.01 Bil\\n \\n', u'\\n3.35 Bil\\n \\n', u'\\n2.36 Bil\\n \\n']\n: http://investing.money.msn.com/investments/financial-statements?symbol=SPF\nShell Command to connect to the web\n\u2022scrapy shell\nRunning the selector on return an empty array ([]). If I save the html output into a file (e.g. C:\\src.html) and use the selector I got the expected response.\nThx!", "issue_status": "Closed", "issue_reporting_time": "2013-09-23T20:18:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1507": {"issue_url": "https://github.com/scrapy/scrapy/issues/392", "issue_id": "#392", "issue_summary": "Proxy HTTPS traffic using CONNECT method", "issue_description": "Member\ndangra commented on Sep 21, 2013\nThe new HTTP1.1 downloader handler landed into Scrapy 0.18, but it lacks proper support for HTTPS downloads trough proxies, it must use HTTP tunneling trough CONNECT method instead of absolute URI mechanism described by RFC2616\nThere were previous attempts to bring this support to Scrapy but they were implemented for (deprecated) HTTP1.0 download handler. i.e.: #45 #237 #16\nScrapy is using new Twisted Agents as backend for downloads, proxied requests instsanciate a t.w.c.ProxyAgent in s.c.d.h.http11#L58\nCurrent ProxyAgent implementation by Twisted doesn't support CONNECT mechanism (Twisted ticket #5324).\nThis ticket is about implementing CONNECT mechanism using Twisted endpoints as described by ProxyAgent.request() comments::\n    # To support proxying HTTPS via CONNECT, we will use key\n    # (\"http-proxy-CONNECT\", scheme, host, port), and an endpoint that\n    # wraps _proxyEndpoint with an additional callback to do the CONNECT.\n    return self._requestWithEndpoint(key, self._proxyEndpoint, method,\n                                     _URI.fromBytes(uri), headers,\n                                     bodyProducer, uri)", "issue_status": "Closed", "issue_reporting_time": "2013-09-20T19:49:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1508": {"issue_url": "https://github.com/scrapy/scrapy/issues/389", "issue_id": "#389", "issue_summary": "Got 502 bad gateway when using some proxies", "issue_description": "staszek-arsdata commented on Sep 17, 2013\nThere's something wrong. I can't get it.\nIf I set proxy to 223.202.2.85:8888 scrapy always returns 502 bad gateway.\nThis proxy works OK even with command line wget. This proxy works OK with Firefox.\nI've even checked whether it's twisted problem. No. Simple twisted http client with this proxy also works OK.\nI'm having so much problems with proper proxy support, spent a lot of time for this.\nAny clue?\nBtw, How to enabled full request/response debug, so I can see what is client sending and getting back on the wire (http level)?", "issue_status": "Closed", "issue_reporting_time": "2013-09-17T14:16:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1509": {"issue_url": "https://github.com/scrapy/scrapy/issues/385", "issue_id": "#385", "issue_summary": "Proxy support not working unless env variable is set", "issue_description": "staszek-arsdata commented on Sep 13, 2013\nThere's na issue with contrib/downloadermiddleware/httpproxy.py.\nWhile I was setting proxy using request.meta['proxy] in my spider for each yield request, the processing was blocked.\nOnly setting env variable http_proxy didn't block the spider. But this kind of setting limits to one proxy per spider, so it's not as useful as request.meta['proxy'].\nAfter reading the stackoverflow thread;\nhttp://stackoverflow.com/questions/14945873/enabling-httpproxymiddleware-in-scrapyd I come into guess that the problem is somewhere in contrib/downloadermiddleware/httpproxy.py.\nI see It raises NotConfigured in init if no env proxy variables are set.\nAfter I commened out the:\n    if not self.proxies:\n        raise NotConfigured\nIt started working OK.\nI don't know the internals of scrapy well, so don't know whether it's really needed, but for the code of contrib/downloadermiddleware/httpproxy.py it's not needed, as self.proxies stays as empty dict and the rest of the code within the module is OK with that.", "issue_status": "Closed", "issue_reporting_time": "2013-09-12T23:54:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1510": {"issue_url": "https://github.com/scrapy/scrapy/issues/384", "issue_id": "#384", "issue_summary": "A little problem when trying scrapy shell command", "issue_description": "t-benze commented on Sep 11, 2013\nI'm new to scrapy, and when I was trying to use the command 'scrapy shell' to crawl a url like this:\nscrapy shell http://www.cuyoo.com/home/portal.php?mod=view_both&aid=18984\nI found the second parameter after '&' lost in the information displayed, and it failed to get the page. It took me quite a while to realize that '&' is a special character for shell, then I tried:\nscrapy shell 'http://www.cuyoo.com/home/portal.php?mod=view_both&aid=18984'\nAnd it works! This is just a little problem, but would you please mention about it somewhere in the toutorial, I believe other rookies like me would appreciate it!", "issue_status": "Closed", "issue_reporting_time": "2013-09-11T13:11:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1511": {"issue_url": "https://github.com/scrapy/scrapy/issues/383", "issue_id": "#383", "issue_summary": "Close spider using jsonrpc is broken", "issue_description": "askender commented on Sep 10, 2013\nhttp://doc.scrapy.org/en/0.18/topics/webservice.html\njsonrpc_call(opts, 'crawler/engine', 'close_spider', args[0])\nreturn:\ncode: -32603\ndata: \"Traceback (most recent call last): File \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/jsonrpc.py\", line 74, in jsonrpc_server_call return jsonrpc_result(id, method(_a, *_kw)) File \"/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py\", line 252, in close_spider slot = self.slots[spider] KeyError: u'dianping' \"\nMany thanks~", "issue_status": "Closed", "issue_reporting_time": "2013-09-10T08:11:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1512": {"issue_url": "https://github.com/scrapy/scrapy/issues/382", "issue_id": "#382", "issue_summary": "Can scrapy provide a common way to save the port of webservice listening on?", "issue_description": "askender commented on Sep 10, 2013\nBy default, each spider will provide webservice which listening different port\nIn webservice.py:\nlog.msg(format='Web service listening on %(host)s:%(port)d',\nlevel=log.DEBUG, host=h.host, port=h.port)\nI wish to use jsonrpc to get the detail info of a running spider, which scrapyd do not provide.", "issue_status": "Closed", "issue_reporting_time": "2013-09-10T08:02:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1513": {"issue_url": "https://github.com/scrapy/scrapy/issues/379", "issue_id": "#379", "issue_summary": "Bug on BaseItemExporter Constructor", "issue_description": "thiagof commented on Sep 5, 2013\nThe documentation states that additional parameters on the BaseItemExporter class, are left to the JSONEncoder (for example).\nhttp://doc.scrapy.org/en/latest/topics/exporters.html#jsonitemexporter\n... and the leftover arguments to the JSONEncoder constructor, so you can use any JSONEncoder constructor argument to customize this exporter.\nDespite the following is looking implemented at scrapy/contrib/exporter/__init__.py:96 it is not working because the BaseItemExporter constructor halts before telling that there's wrong argument set.\nSeems like a bug on the line 32 of that file. Can someone from the core team confirm it?", "issue_status": "Closed", "issue_reporting_time": "2013-09-04T20:29:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1514": {"issue_url": "https://github.com/scrapy/scrapy/issues/377", "issue_id": "#377", "issue_summary": "incorrect permissions in the packaged tar", "issue_description": "schwehr commented on Sep 2, 2013\nAll files and directories in the distribution tar should be group and world readable. By not doing this, an installed instance of scrapy that is first built as a user and then installed by root (this is what mac osx fink does) sets up scrapy such that it is not usable by non-root users.\nI setup the fink package for scrapy like this to work around the issue:\nCompileScript: <<\n  #!/bin/bash -ev\n  chmod 644 scrapy/{VERSION,mime.types}\n  find . -name \\*.tmpl -o -name \\*.cfg -o -name \\*.xml -o -name \\*.tar | xargs chmod 644\n  find . -name \\*.zip -o -name \\*.csv -o -name \\*.html -o -name \\*.egg | xargs chmod 644\n  find . -name \\*.gz -o -name \\*.bz2 -o -name \\*.bin -o -name \\*.txt | xargs chmod 644\n  python%type_raw[python] setup.py build\n<<\nHow to see this issue:\nwget https://pypi.python.org/packages/source/S/Scrapy/Scrapy-0.18.1.tar.gz#md5=63f84dd460cc3eb4f8c5b8bc907f6f39\ntar xf Scrapy-0.18.1.tar.gz\n\nfind . | xargs ls -l | grep '\\---' | wc -l\n   871 \nfind . | xargs ls -l | grep '\\---' | head\n\n-rw-------  1 schwehr  5000   1273 Aug 27 14:46 ./AUTHORS\n-rw-------  1 schwehr  5000    154 Aug 27 14:46 ./INSTALL\n-rw-------  1 schwehr  5000   1521 Aug 27 14:46 ./LICENSE\n-rw-------  1 schwehr  5000    385 Aug 27 14:46 ./MANIFEST.in\n-rw-------  1 schwehr  5000   2671 Aug 27 14:49 ./PKG-INFO\n-rw-------  1 schwehr  5000   1348 Aug 27 14:46 ./README.rst\n-rw-------  1 schwehr  5000    114 Aug 27 14:46 ./bin/runtests.bat\n-rwx------  1 schwehr  5000   1271 Aug 27 14:46 ./bin/runtests.sh\n-rwx------  1 schwehr  5000     68 Aug 27 14:46 ./bin/scrapy\n-rw-------  1 schwehr  5000   2225 Aug 27 14:46 ./docs/Makefile", "issue_status": "Closed", "issue_reporting_time": "2013-09-02T17:31:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1515": {"issue_url": "https://github.com/scrapy/scrapy/issues/376", "issue_id": "#376", "issue_summary": "\"scrapy check\" regression", "issue_description": "Contributor\nnyov commented on Sep 1, 2013\nscrapy check fails in a project with more than one spider, that have a 'name' property.\n$ scrapy check\nUnhandled Error\nTraceback (most recent call last):\n  File \"[...]/scrapy.git/scrapy/core/engine.py\", line 247, in _spider_idle\n    self.close_spider(spider, reason='finished')\n  File \"[...]/scrapy.git/scrapy/core/engine.py\", line 259, in close_spider\n    dfd.addBoth(lambda _: self.downloader.close())\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 327, in addBoth\n    callbackKeywords=kw, errbackKeywords=kw)\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 293, in addCallbacks\n    self._runCallbacks()\n--- <exception caught here> ---\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 575, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"[...]/scrapy.git/scrapy/core/engine.py\", line 259, in <lambda>\n    dfd.addBoth(lambda _: self.downloader.close())\n  File \"[...]/scrapy.git/scrapy/core/downloader/__init__.py\", line 180, in close\n    self._slot_gc_loop.stop()\n  File \"/usr/lib/python2.7/dist-packages/twisted/internet/task.py\", line 181, in stop\n    assert self.running, (\"Tried to stop a LoopingCall that was \"\nexceptions.AssertionError: Tried to stop a LoopingCall that was not running.\nThis was introduced by commit af5c13f.\nscrapy check also hangs forever in a (new) project without any spiders,\nbut this already seemed to be the case in 0.16\nedit: tested with Twisted 12.0 and 13.0", "issue_status": "Closed", "issue_reporting_time": "2013-09-01T04:04:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1516": {"issue_url": "https://github.com/scrapy/scrapy/issues/373", "issue_id": "#373", "issue_summary": "Correctly catch and retry on request exceptions", "issue_description": "AJamesPhillips commented on Aug 21, 2013\nScrapy's retry middleware does not retry when twisted internet raises a ConnectionDone exception. I can't get the tests to pass but I believe I've fixed this issue with this: AJamesPhillips@1604505\nThe \"exception\" passed to RetryMiddleware.process_exception is actually wrapped and comes from the twisted deferred errback using code similar to: https://github.com/scrapy/scrapy/blob/master/scrapy/xlib/tx/_newclient.py#L540", "issue_status": "Closed", "issue_reporting_time": "2013-08-21T16:47:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1517": {"issue_url": "https://github.com/scrapy/scrapy/issues/371", "issue_id": "#371", "issue_summary": "fatal error: 'libxml/xmlversion.h' file not found", "issue_description": "mrsinguyen commented on Aug 17, 2013\nI'm getting this error when trying to install it with pip in Mac OS X Mountain Lion (10.8.4).\nsudo pip install Scrapy --upgrade\nThis is complete pip log, see the link bellow:\nhttps://gist.github.com/mrsinguyen/6255937\nThe latest XCode Command Line Tools installed.", "issue_status": "Closed", "issue_reporting_time": "2013-08-17T08:41:38Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1518": {"issue_url": "https://github.com/scrapy/scrapy/issues/368", "issue_id": "#368", "issue_summary": "get_func_args() from scrapy.utils.python crashes on partial functions", "issue_description": "amir-f commented on Aug 15, 2013\nI use Python 2.7 and used a partial function (functools.partial) as part of an input_processor for the ItemLoader. For instance,\nprice = Field(input_processor=Compose(some_partial_func, some_other_func))\nDuring execution, get_func_args() from scrapy.util.python goes into an infinite loop until\n/usr/local/lib/python2.7/dist-packages/scrapy/utils/python.pyc in get_func_args(func, stripself)\n    161             return []\n    162         else:\n--> 163             return get_func_args(func.__call__, True)\n    164     else:\n    165         raise TypeError('%s is not callable' % type(func))\n\n/usr/local/lib/python2.7/dist-packages/scrapy/utils/python.pyc in get_func_args(func, stripself)\n    161             return []\n    162         else:\n--> 163             return get_func_args(func.__call__, True)\n    164     else:\n    165         raise TypeError('%s is not callable' % type(func))\n\n/usr/local/lib/python2.7/dist-packages/scrapy/utils/python.pyc in get_func_args(func, stripself)\n    151     if inspect.isfunction(func):\n    152         func_args, _, _, _ = inspect.getargspec(func)\n--> 153     elif inspect.isclass(func):\n    154         return get_func_args(func.__init__, True)\n    155     elif inspect.ismethod(func):\n\n/usr/lib/python2.7/inspect.pyc in isclass(object)\n     63         __doc__         documentation string\n     64         __module__      name of module in which this class was defined\"\"\"\n---> 65     return isinstance(object, (type, types.ClassType))\n     66 \n     67 def ismethod(object):\n\nRuntimeError: maximum recursion depth exceeded while calling a Python object\nhappens. Looks like get_func_args needs to handle partial functions as a separate case.", "issue_status": "Closed", "issue_reporting_time": "2013-08-15T07:05:22Z", "fixed_by": "#506", "pull_request_summary": "get func args partial support - based on #504", "pull_request_description": "Member\ndangra commented on Dec 30, 2013\nfixes #368 based on #504.", "pull_request_status": "Merged", "issue_fixed_time": "2013-12-30T16:45:38Z", "files_changed": [["24", "scrapy/tests/test_contrib_loader.py"], ["9", "scrapy/tests/test_utils_python.py"], ["5", "scrapy/utils/python.py"]]}, "1519": {"issue_url": "https://github.com/scrapy/scrapy/issues/365", "issue_id": "#365", "issue_summary": "Scrapyd not loading the latest deployed egg", "issue_description": "ivandir commented on Aug 13, 2013\nWhen multiple eggs are located in /var/lib/scrapyd/eggs//*.egg\nI restart scrapyd service\nThen I execute the command \"scrapy deploy scrapyd -p project1 --version GIT\" on the same folder level as setup.py and scrapyd.conf.\nHowever after looking at my application scrapyd hasn't transitioned to the new egg even thought i get a status response of 200 (OK)\nHow can I check and how does scrapyd decide what egg to load when it's restarted?\nDeploying a new egg should make scrapyd load it right?\nThanks,\n~Ivan", "issue_status": "Closed", "issue_reporting_time": "2013-08-13T03:58:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1520": {"issue_url": "https://github.com/scrapy/scrapy/issues/364", "issue_id": "#364", "issue_summary": "Item constructor", "issue_description": "Sean-Brown commented on Aug 7, 2013\nI can't get an item to have a constructor that takes arguments because scrapy.item always ends up calling getattr(self, name) which throws. Are items not meant to have constructors, or am I not doing it right (good possibility, I am a Python novice).\nMy item class has the form shown below:\nclass TestItem(Item):\nf1 = Field()\nf2 = Field()\ndef init(self, f1=None, f2=None):\nself['f1'] = f1\nself['f2'] = f2\nis there something that I'm leaving out, or is this not a proper way to subclass Item?", "issue_status": "Closed", "issue_reporting_time": "2013-08-07T14:07:10Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1521": {"issue_url": "https://github.com/scrapy/scrapy/issues/360", "issue_id": "#360", "issue_summary": "SitemapSpider should support rel=\"alternate\" hreflang=\"x\"", "issue_description": "eshao commented on Aug 2, 2013\nMany sitemaps contain URLs in multiple languages. You can find specs here.\nExample:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n  xmlns:xhtml=\"http://www.w3.org/1999/xhtml\">\n  <url>\n    <loc>http://www.example.com/english/</loc>\n    <xhtml:link \n                 rel=\"alternate\"\n                 hreflang=\"de\"\n                 href=\"http://www.example.com/deutsch/\"\n                 />\n    <xhtml:link \n                 rel=\"alternate\"\n                 hreflang=\"de-ch\"\n                 href=\"http://www.example.com/schweiz-deutsch/\"\n                 />\n    <xhtml:link \n                 rel=\"alternate\"\n                 hreflang=\"en\"\n                 href=\"http://www.example.com/english/\"\n                 />\n  </url>\n</urlset>\nI would expect to have support to make a request for each of these three language pages rather than only the URL in the loc tag.", "issue_status": "Closed", "issue_reporting_time": "2013-08-02T09:38:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1522": {"issue_url": "https://github.com/scrapy/scrapy/issues/357", "issue_id": "#357", "issue_summary": "absolute path for cmd feed exports", "issue_description": "panfayang commented on Jul 30, 2013\ni am trying to call the spiders from one directory and save the output files to another in the windows cmd.\n\"scrapy crawl spidername -o c:/some/abs/path/results.csv -t csv\" failed, but\n\"scrapy crawl spidername --output=open('c:/some/abs/path/results.csv' -t csv\" succeeded.\nwhy it works under open parentheses?\nwhy it doesn't work for direct supply of abs path?\nI tried but could not find where the related codes were written...", "issue_status": "Closed", "issue_reporting_time": "2013-07-29T21:39:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1523": {"issue_url": "https://github.com/scrapy/scrapy/issues/351", "issue_id": "#351", "issue_summary": "Scrapy failing to retrieve cookies from cookiejar", "issue_description": "Contributor\narijitchakraborty commented on Jul 18, 2013\nWhile working with domains like \"https://example.com:4443/\", scrapy cookiejar seems to store incoming cookies against \"example.com\", where as while retrieving cookies from cookiejar to send back to server, its looking for cookies stored against \"example.com:4443\"\nThis behavior is different from what browsers do.", "issue_status": "Closed", "issue_reporting_time": "2013-07-17T19:26:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1524": {"issue_url": "https://github.com/scrapy/scrapy/issues/349", "issue_id": "#349", "issue_summary": "Spider arguments example in documentation should include super call", "issue_description": "Contributor\ndellis23 commented on Jul 16, 2013\nI just ran into an issue with one of the examples because it did not mention that the init method needs to call super. It looks like there's a couple of other people with the same confusion in the scrapy-users group as well. I'll submit a documentation patch.", "issue_status": "Closed", "issue_reporting_time": "2013-07-16T18:23:59Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1525": {"issue_url": "https://github.com/scrapy/scrapy/issues/348", "issue_id": "#348", "issue_summary": "Unable to send email with any other charset than us-ascii", "issue_description": "mortenlj commented on Jul 14, 2013\nSending an email-body with characters that doesn't exist in the us-ascii encoding fails with the following error:\n  File \"/home/fimojoha/.virtualenvs/obos-alert/lib/python2.7/site-packages/scrapy/mail.py\", line 69, in send\n    dfd = self._sendmail(rcpts, msg.as_string())\n  File \"/usr/lib64/python2.7/email/message.py\", line 137, in as_string\n    g.flatten(self, unixfrom=unixfrom)\n  File \"/usr/lib64/python2.7/email/generator.py\", line 83, in flatten\n    self._write(msg)\n  File \"/usr/lib64/python2.7/email/generator.py\", line 108, in _write\n    self._dispatch(msg)\n  File \"/usr/lib64/python2.7/email/generator.py\", line 134, in _dispatch\n    meth(msg)\n  File \"/usr/lib64/python2.7/email/generator.py\", line 180, in _handle_text\n    self._fp.write(payload)\nexceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\\xe6' in position 66: ordinal not in range(128)\nExample code that should illustrate the problem (untested because I don't know how to set up the entire twisted environment):\nfrom scrapy.mail import MailSender\n\nmailer = MailSender()\nbody = u\"\\u00e5 \\u03b1-\\u03c9 \\u1128 \\u114e\"\nsubject = u\"Subject\"\nmailer.send([\"example@example.com\"], subject, body)", "issue_status": "Closed", "issue_reporting_time": "2013-07-13T18:42:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1526": {"issue_url": "https://github.com/scrapy/scrapy/issues/347", "issue_id": "#347", "issue_summary": "SitemapSpider prone to invalid xml sitemap", "issue_description": "cbourjau commented on Jul 13, 2013\nHello,\nThe SitemapSpider is very prone to invalid xml which like on www.wired.com/sitemap.xml. Trying to scrap that site using the Sitemap spider in release 0.17 gives the error\ncElementTree.ParseError: XML or text declaration not at start of entity: line 2, column 0\nsince the xml file starts with a blank line. This behaviour was discussed on the scrapy mailing list (https://groups.google.com/forum/#!topic/scrapy-users/BAHTPS-91VA) where Paul Tremberth pointed out that lxml's parser provides a recovery possibility which is easily enabled. Copying from his post:\n>>> import lxml.etree\n>>> lxml.etree.parse('http://www.wired.com/sitemap.xml')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"lxml.etree.pyx\", line 3197, in lxml.etree.parse (src/lxml/lxml.etree.c:65042)\n  File \"parser.pxi\", line 1571, in lxml.etree._parseDocument (src/lxml/lxml.etree.c:93101)\n  File \"parser.pxi\", line 1600, in lxml.etree._parseDocumentFromURL (src/lxml/lxml.etree.c:93388)\n  File \"parser.pxi\", line 1500, in lxml.etree._parseDocFromFile (src/lxml/lxml.etree.c:92445)\n  File \"parser.pxi\", line 1047, in lxml.etree._BaseParser._parseDocFromFile (src/lxml/lxml.etree.c:89329)\n  File \"parser.pxi\", line 577, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:84711)\n  File \"parser.pxi\", line 676, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:85816)\n  File \"parser.pxi\", line 616, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:85138)\nlxml.etree.XMLSyntaxError: XML declaration allowed only at the start of the document, line 2, column 6\n>>> xmlp = lxml.etree.XMLParser(recover=True)\n>>> lxml.etree.parse('http://www.wired.com/sitemap.xml', parser=xmlp)\n<lxml.etree._ElementTree object at 0x7fdc3d13ea70>\n>>> root = lxml.etree.parse('http://www.wired.com/sitemap.xml', parser=xmlp).getroot()\n>>> print lxml.etree.tostring(root, pretty_print=True)\n<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n\n<!-- cache: cached = yes name = sitemap_jspCache key = sitemap -->\n<sitemap>\n<loc>http://www.wired.com/sitemap1.xml</loc>\n<lastmod>2013-07-11</lastmod>\n</sitemap>\n\n<sitemap>\n<loc>http://www.wired.com/sitemap2.xml</loc>\n<lastmod>2013-07-11</lastmod>\n</sitemap>\n\n<sitemap>\n<loc>http://www.wired.com/sitemap3.xml</loc>\n<lastmod>2013-07-11</lastmod>\n</sitemap>\n\n<!-- end cache -->\n</sitemapindex>\nThis is my first ever bug report and hence I do not know how to post a possible patch correctly, but for the time being, the following did the trick for me:\ndiff --git a/scrapy/utils/sitemap.py b/scrapy/utils/sitemap.py\nindex 71d8122..d14d93e 100644\n--- a/scrapy/utils/sitemap.py\n+++ b/scrapy/utils/sitemap.py\n@@ -5,17 +5,16 @@ Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n\n-from cStringIO import StringIO\n-from xml.etree.cElementTree import ElementTree\n+import lxml.etree\n+\n\n class Sitemap(object):\n     \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n     (type=sitemapindex) files\"\"\"\n\n     def __init__(self, xmltext):\n-        tree = ElementTree()\n-        tree.parse(StringIO(xmltext))\n-        self._root = tree.getroot()\n+        xmlp = lxml.etree.XMLParser(recover=True)\n+        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n         rt = self._root.tag\n         self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt\n\n@@ -26,6 +25,8 @@ class Sitemap(object):\n                 tag = el.tag\n                 name = tag.split('}', 1)[1] if '}' in tag else tag\n                 d[name] = el.text.strip() if el.text else ''\n+            if not 'loc' in d.keys():\n+                continue\n             yield d\n\n def sitemap_urls_from_robots(robots_text):\nI am happy to provide more information if needed.\nSpecs:\nScrapy : 0.17.0\nlxml : 3.1.0.0\nlibxml2 : 2.9.0\nTwisted : 12.3.0\nPython : 2.7.4 (default, Apr 19 2013, 18:28:01) - [GCC 4.7.3]\nPlatform: Linux-3.6.11-030611-generic-x86_64-with-Ubuntu-13.04-raring", "issue_status": "Closed", "issue_reporting_time": "2013-07-12T21:26:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1527": {"issue_url": "https://github.com/scrapy/scrapy/issues/345", "issue_id": "#345", "issue_summary": "Scrapy chokes on HTTP response status lines without a Reason phrase", "issue_description": "tonal commented on Jul 11, 2013\nTry fetch page:\n$ scrapy fetch 'http://www.gidroprofmontag.ru/bassein/sbornue_basseynu'\noutput:\n2013-07-11 09:15:37+0400 [scrapy] INFO: Scrapy 0.17.0-304-g3fe2a32 started (bot: amon)\n/home/tonal/amon/amon/amon/downloadermiddleware/blocked.py:6: ScrapyDeprecationWarning: Module `scrapy.stats` is deprecated, use `crawler.stats` attribute instead\n  from scrapy.stats import stats\n2013-07-11 09:15:37+0400 [amon_ra] INFO: Spider opened\n2013-07-11 09:15:37+0400 [amon_ra] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2013-07-11 09:15:37+0400 [amon_ra] ERROR: Error downloading <GET http://www.gidroprofmontag.ru/bassein/sbornue_basseynu>: [<twisted.python.failure.Failure <class 'scrapy.xlib.tx._newclient.ParseError'>>]\n2013-07-11 09:15:37+0400 [amon_ra] INFO: Closing spider (finished)\n2013-07-11 09:15:37+0400 [amon_ra] INFO: Dumping Scrapy stats:\n        {'downloader/exception_count': 1,\n         'downloader/exception_type_count/scrapy.xlib.tx._newclient.ResponseFailed': 1,\n         'downloader/request_bytes': 256,\n         'downloader/request_count': 1,\n         'downloader/request_method_count/GET': 1,\n         'finish_reason': 'finished',\n         'finish_time': datetime.datetime(2013, 7, 11, 5, 15, 37, 512010),\n         'log_count/ERROR': 1,\n         'log_count/INFO': 4,\n         'scheduler/dequeued': 1,\n         'scheduler/dequeued/memory': 1,\n         'scheduler/enqueued': 1,\n         'scheduler/enqueued/memory': 1,\n         'start_time': datetime.datetime(2013, 7, 11, 5, 15, 37, 257898)}\n2013-07-11 09:15:37+0400 [amon_ra] INFO: Spider closed (finished)", "issue_status": "Closed", "issue_reporting_time": "2013-07-11T05:16:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1528": {"issue_url": "https://github.com/scrapy/scrapy/issues/340", "issue_id": "#340", "issue_summary": "Enable Travis CI for pull requests", "issue_description": "Member\nkmike commented on Jul 8, 2013\nHi,\nCurrently Travis CI is run only for master and release branches (see 79cb031). It doesn't run on pull requests/feature branches so contributors are not notified when their pull request is breaking tests.\nWhy was this change made? What do you think about restoring CI for pull requests by removing \"branches\" section from travis.yml?", "issue_status": "Closed", "issue_reporting_time": "2013-07-08T17:56:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1529": {"issue_url": "https://github.com/scrapy/scrapy/issues/335", "issue_id": "#335", "issue_summary": "support EventSource/long polling in scrapyd", "issue_description": "graingert commented on Jul 6, 2013\nWhen I run a long running crawl task, I'd like to be notified when it's done through the API. I know this is possible with the callbacks, but I'd rather it was built in.\nThe solution to this in REST/HTTP is the EventSource API.\nhttps://developer.mozilla.org/en-US/docs/Server-sent_events/Using_server-sent_events", "issue_status": "Closed", "issue_reporting_time": "2013-07-06T16:06:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1530": {"issue_url": "https://github.com/scrapy/scrapy/issues/327", "issue_id": "#327", "issue_summary": "Support SSL in email sender.", "issue_description": "zac-zhou commented on Jun 26, 2013\nSometime I need to set a gmail address as the sender of the email in the scrapy's mail sender, but gmail require to use port 465 and set the SSL to true. But when I check the code of scrapy/mail.py, I found it use ESMTPSenderFactory to send the email, but it set the requireTransportSecurity fixed to False. Can the future version of scrapy change this into a property that can be changed via the settings?", "issue_status": "Closed", "issue_reporting_time": "2013-06-26T14:37:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1531": {"issue_url": "https://github.com/scrapy/scrapy/issues/324", "issue_id": "#324", "issue_summary": "scrapyd items_dir: when empty returns access to root level folder", "issue_description": "ivandir commented on Jun 22, 2013\nHi,\nThe \"items_dir\" setting when left empty and used in the default scrapyd server you get from OS repo exposes the whole server filesystem starting with /\nI am storing my parsed data in a database and don't need and items feed. When I set \"items_dir\" to an empty value and start the scrapyd server clicking on the /items/ url in the webpage leads me to my servers root path. The scrapyd server I am using is the default one installed with Ubuntu precise32\n(dev2)vagrant@precise32:/vagrant$ scrapy version -v\nScrapy : 0.16.3\nlxml : 3.2.1.0\nlibxml2 : 2.7.8\nTwisted : 12.2.0\nPython : 2.7.3 (default, Aug 1 2012, 05:16:07) - [GCC 4.6.3]\nPlatform: Linux-3.2.0-23-generic-pae-i686-with-Ubuntu-12.04-precise\n(dev2)vagrant@precise32:/vagrant$ apt-cache show scrapyd-0.16\nPackage: scrapyd-0.16\nSource: scrapy-0.16\nVersion: 0.16.5+1369956345\nArchitecture: all\nMaintainer: Scrapinghub Team info@scrapinghub.com\nInstalled-Size: 93\nDepends: scrapy-0.16 (>= 0.16.5+1369956345), python-setuptools\nConflicts: scrapyd, scrapyd-0.11\nProvides: scrapyd\nHomepage: http://scrapy.org/\nPriority: optional\nSection: python\nFilename: pool/main/s/scrapy-0.16/scrapyd-0.16_0.16.5+1369956345_all.deb\nSize: 4016\nSHA256: 9a361297122a7149a3e91d8262303d8a1d27878c5e189b8cb9e4b15c2703a20a\nSHA1: 6e8f00c084d6c94957c3b546c710c2cd91719498\nMD5sum: fd51653e8d4524d9d89a07931e5748e2\nDescription: Scrapy Service\nThe Scrapy service allows you to deploy your Scrapy projects by building\nPython eggs of them and uploading them to the Scrapy service using a JSON API\nthat you can also use for scheduling spider runs. It supports multiple\nprojects also.\nThe only modification I have done to the actual startup script is the location of twisted so that it uses then one I have in my virtual environment.\nexec ~/.myvirtualenv/bin/twistd -ny /usr/share/scrapyd/scrapyd.tac ..........\nHere is the config file:\n[scrapyd]\nhttp_port = 6800\ndebug = off\nmax_proc = 0\nmax_proc_per_cpu = 4\neggs_dir = /var/lib/scrapyd/eggs\ndbs_dir = /var/lib/scrapyd/dbs\nitems_dir =\nlogs_dir = /var/log/scrapyd\nlogs_to_keep = 5\nrunner = scrapyd.runner\napplication = scrapyd.app.application\n[services]\nschedule.json = scrapyd.webservice.Schedule\ncancel.json = scrapyd.webservice.Cancel\naddversion.json = scrapyd.webservice.AddVersion\nlistprojects.json = scrapyd.webservice.ListProjects\nlistversions.json = scrapyd.webservice.ListVersions\nlistspiders.json = scrapyd.webservice.ListSpiders\ndelproject.json = scrapyd.webservice.DeleteProject\ndelversion.json = scrapyd.webservice.DeleteVersion\nlistjobs.json = scrapyd.webservice.ListJobs\nThis is what my virtual environment contains (removing unecessary modules not related to discussion)\n(dev2)vagrant@precise32:/vagrant$ pip freeze\nScrapy==0.16.3\nTwisted==12.2.0\nThanks for your help.", "issue_status": "Closed", "issue_reporting_time": "2013-06-22T13:12:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1532": {"issue_url": "https://github.com/scrapy/scrapy/issues/319", "issue_id": "#319", "issue_summary": "download_delay is not used dynamically", "issue_description": "eode commented on Jun 6, 2013\nThe concurrent_requests option is read and processed when dynamically changed, but the download_delay is only used to set the initial delay time.", "issue_status": "Closed", "issue_reporting_time": "2013-06-06T14:11:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1533": {"issue_url": "https://github.com/scrapy/scrapy/issues/315", "issue_id": "#315", "issue_summary": "MetaRefreshMiddleware can't handle URL correctly", "issue_description": "umrashrf commented on May 31, 2013\nIf meta refresh url has \"../\" in it, it can't handle it correctly whereas Chrome does.\nTry this URL https://mbcips.com:4443/ where Chrome gives 302 but Scrapy 404.", "issue_status": "Closed", "issue_reporting_time": "2013-05-31T14:27:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1534": {"issue_url": "https://github.com/scrapy/scrapy/issues/314", "issue_id": "#314", "issue_summary": "The closespider extension should support spider overrides", "issue_description": "Contributor\nalexcepoi commented on May 31, 2013\nI.e. override the 'CLOSESPIDER_PAGECOUNT' setting per spider by using the closespider_pagecount attribute in the spider, etc.", "issue_status": "Closed", "issue_reporting_time": "2013-05-31T12:51:35Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1535": {"issue_url": "https://github.com/scrapy/scrapy/issues/310", "issue_id": "#310", "issue_summary": "select not working properly", "issue_description": "Sean-Brown commented on May 28, 2013\nInside the HTML sent back by one website, there's one \"a\" element structured as such:\na class=\"title threadtitle_unread\" href=\"showthread.php?t=154247171\" id=\"thread_title_154247171\">Here is some text</a\nit seems that trying to make a selection based on the class does not work, my initial suspicion is that it has to do with there being a space between \"title\" and \"threadtitle_unread\", but my Python's not good enough yet for me to accurately debug it.\nEDIT: ignore that there's no opening '<' and closing '>', I didn't know how to get this page to not parse it literally so I left them out", "issue_status": "Closed", "issue_reporting_time": "2013-05-28T12:02:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1536": {"issue_url": "https://github.com/scrapy/scrapy/issues/306", "issue_id": "#306", "issue_summary": "--output-format raise on invalid format", "issue_description": "Contributor\nDeaconDesperado commented on May 22, 2013\nUsing version 0.16.4\nCurrently, if an invalid format is passed to the -t or --output-format options, the spider will proceed with its crawling operation, but no output will be saved or produced. This could be seen as frustrating on large scrape runs, in the event the user running it passed a mistyped format to the option and assumed their operation was saving output only to find later the scraped data has only been logged to stdout.\nShould we make the output format option raise or fail+exit if an invalid or unknown format is passed?", "issue_status": "Closed", "issue_reporting_time": "2013-05-21T20:33:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1537": {"issue_url": "https://github.com/scrapy/scrapy/issues/304", "issue_id": "#304", "issue_summary": "initspider vs crawlspider", "issue_description": "vortic commented on May 17, 2013\nI currently have a CrawlSpider to crawl through a website, but I needed to change it to InitSpider in order to get an authenticated session.\nWith CrawlSpider, it would correctly crawl all of the pages, but would fail on the ones that required a login.\nWith InitSpider, it successfully logs in, but then does not do anything else.\nI know InitSpider used to inherit from CrawlSpider, but I'm a little unhappy with the (hacky) solution posted here:\nhttp://stackoverflow.com/questions/5851213/crawling-with-an-authenticated-session-in-scrapy\n(the last post with 2 votes).\nAs the poster there asked, is there another solution?\nThanks in advance!", "issue_status": "Closed", "issue_reporting_time": "2013-05-16T20:16:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1538": {"issue_url": "https://github.com/scrapy/scrapy/issues/302", "issue_id": "#302", "issue_summary": "logging in pipelines breaks scrapy deploy success message", "issue_description": "plainas commented on May 8, 2013\nNot sure if this should be reported here on on scrapyd's issue tracker.\nusing the logging facility on scrapy breaks the spider count on deploy, it returns 0 all the time.\nTo reproduce the error, place the snippet below on your pipelines module and try to deploy to a scrapyd target\nfrom scrapy import log\n\nclass MyPipeline(object):    \n    def __init__(self):\n        log.start()\n\n...", "issue_status": "Closed", "issue_reporting_time": "2013-05-08T07:41:05Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1539": {"issue_url": "https://github.com/scrapy/scrapy/issues/300", "issue_id": "#300", "issue_summary": "Contract for item key-value testing", "issue_description": "gcmalloc commented on May 7, 2013\nIt would be nice to also test for the values of the parsed item, not only their keys. For the moment, I use a yaml description for the item in a separate file. I call the contract using:\n\"\"\"Comment for a spider callback\n@contain_item filename.yaml\n\"\"\"\nIn the contract, I check for the equivalence of the item in the output structure of the post_process method.\nIs there a better approach for this ? if not I would promptly propose this code for a pull request.", "issue_status": "Closed", "issue_reporting_time": "2013-05-07T18:12:08Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1540": {"issue_url": "https://github.com/scrapy/scrapy/issues/298", "issue_id": "#298", "issue_summary": "import unittest vs from twisted.trial import unittest", "issue_description": "yogeshc commented on Apr 29, 2013\nI see in the test-code that certain places use import unittest and few use from twisted.trial import unittest. If tests are implemented using Twisted unit-testing framework, then all occurrences of \"import unittest\" should be replaced with \"from twisted.trial import unittest\". Kindly comment if this change can be made without any other issues. I will send a pull request if it is the case.", "issue_status": "Closed", "issue_reporting_time": "2013-04-29T07:25:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1541": {"issue_url": "https://github.com/scrapy/scrapy/issues/294", "issue_id": "#294", "issue_summary": "Feed Storage (S3) Not Working?", "issue_description": "rdegges commented on Apr 25, 2013\nHey all, I'm using the latest stable release of scrapy, and I'm having problems getting feed storage working. In my settings.py file, I've got the following defined:\nLOG_ENABLED = True\nLOG_FILE = BOT_NAME + '.log'\nLOG_LEVEL = 'DEBUG'\nLOG_STDOUT = True\nSTATS_DUMP = True\nFEED_URI = 's3://mybucker/%(name)s/%(time)s.json'\nFEED_FORMAT = 'jsonlines'\nFEED_STORE_EMPTY = False\nAWS_ACCESS_KEY_ID = 'blah'\nAWS_SECRET_ACCESS_KEY = 'blah'\nAccording to the documentation, http://doc.scrapy.org/en/0.16/topics/feed-exports.html#settings it looks as if this should work.\nI'm defining my desired S3 URI, I've got boto installed (so that shouldn't be a problem), I'm telling scrapy to use jsonlines for serialization, and I've got my AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY clearly defined.\nWhat am I doing wrong? When the scraper has finished running, if I inspect the .log file it creates locally, I see no reference to s3 at all.\nThank you.", "issue_status": "Closed", "issue_reporting_time": "2013-04-25T08:47:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1542": {"issue_url": "https://github.com/scrapy/scrapy/issues/289", "issue_id": "#289", "issue_summary": "before scrapy download, how to set a ip in request to download.", "issue_description": "zhiying8710 commented on Apr 18, 2013\nThere are some physical public IP address on my computer, when I run scrapy, I want to randomly choose one of them which will be used to download. how am I supposed to do?", "issue_status": "Closed", "issue_reporting_time": "2013-04-18T02:28:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1543": {"issue_url": "https://github.com/scrapy/scrapy/issues/282", "issue_id": "#282", "issue_summary": "Error at initialization of module heapyc", "issue_description": "gnemoug commented on Apr 8, 2013\nwhen start the scaper,then output:\nError at initialization of module heapyc\nI know it because the guppy app,but I can't find out the reason!How to solve this?", "issue_status": "Closed", "issue_reporting_time": "2013-04-08T07:52:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1544": {"issue_url": "https://github.com/scrapy/scrapy/issues/280", "issue_id": "#280", "issue_summary": "Downloader Middleware process_request documentation is inaccurate", "issue_description": "gnemoug commented on Apr 2, 2013\nThe process_request(self, request, spider) method of DownloaderMiddleware document that:\n      \"If it returns a Request object, the returned request will be rescheduled (in the Scheduler) to \nbe downloaded in the future. The callback of the original request will always be called. If the new\n request has a callback it will be called with the response downloaded, and the output of that \ncallback will then be passed to the original callback. If the new request doesn\u2019t have a callback, the \nresponse downloaded will be just passed to the original request callback.\"\nBut actually is that if it returns a Request object,then the original request will be dropped, so you must make sure that the new request object's callback is the original callback.", "issue_status": "Closed", "issue_reporting_time": "2013-04-02T06:00:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1545": {"issue_url": "https://github.com/scrapy/scrapy/issues/278", "issue_id": "#278", "issue_summary": "Selectors from XPathSelectorList doesn't contain response data", "issue_description": "gEndelf commented on Mar 29, 2013\nThere is an issue in XPathSelector, on selector.select('xpath') we have a list of selectors without response in them.\ntest:\nselectors = hxs.select('//body')\nassert(selectors[0].response, 'response is undefined')\nfix: to add response=self.response as argument\n#XPathSelector.select#line 52\n        result = [self.__class__(_root=x, _expr=xpath, namespaces=self.namespaces, response=self.response)\n                  for x in result]", "issue_status": "Closed", "issue_reporting_time": "2013-03-29T15:22:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1546": {"issue_url": "https://github.com/scrapy/scrapy/issues/272", "issue_id": "#272", "issue_summary": "Contracts: exceptions.UnboundLocalError: local variable 'output' referenced before assignment", "issue_description": "Contributor\ncoagulant commented on Mar 16, 2013\nWhenever any of my contracts fail I get 2 errors insted of one: one real and one stated in issue title. (Scrapy 0.16.4)\nscrapy check alpha\nI suppose only relevant exceptions should be printed to stdout.\nTraceback (most recent call last):\n  File \"/Users/prophet/.envs/project/lib/python2.7/site-packages/twisted/internet/base.py\", line 1182, in mainLoop\n    self.runUntilCurrent()\n  File \"/Users/prophet/.envsproject/lib/python2.7/site-packages/twisted/internet/base.py\", line 805, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"/Users/prophet/.envs/project/lib/python2.7/site-packages/twisted/internet/defer.py\", line 381, in callback\n    self._startRunCallbacks(result)\n  File \"/Users/prophet/.envs/project/lib/python2.7/site-packages/twisted/internet/defer.py\", line 489, in _startRunCallbacks\n    self._runCallbacks()\n--- <exception caught here> ---\n  File \"/Users/prophet/.envs/project/lib/python2.7/site-packages/twisted/internet/defer.py\", line 576, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"/Users/prophet/.envs/project/lib/python2.7/site-packages/scrapy/commands/check.py\", line 18, in wrapper\n    output = cb(response)\n  File \"/Users/prophet/.envs/project/lib/python2.7/site-packages/scrapy/contracts/__init__.py\", line 114, in wrapper\n    return output\nexceptions.UnboundLocalError: local variable 'output' referenced before assignment\nF\n======================================================================\nERROR: [alpha] parse_list (@returns post-hook)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/prophet/.envs/project/lib/python2.7/site-packages/scrapy/contracts/__init__.py\", line 103, in wrapper\n    output = list(iterate_spider_output(cb(response)))\nTypeError: parse_list() takes exactly 3 arguments (2 given)", "issue_status": "Closed", "issue_reporting_time": "2013-03-16T18:21:42Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1547": {"issue_url": "https://github.com/scrapy/scrapy/issues/266", "issue_id": "#266", "issue_summary": "Running scrapy from script is giving \"cannot import name crawler\" error on pipelines", "issue_description": "bslima commented on Mar 13, 2013\nI'm trying to run scrapy spiders within a python script as the documentation says (http://doc.scrapy.org/en/latest/topics/practices.html#run-from-script)\nHere is a snippet of my code (https://gist.github.com/bslima/5151215)\nBut when i run the python script i get a\nImportError: Error loading object 'aqueleimovel.pipelines.ValidationImovelPipeline': cannot import name crawler\nFirst of all, i only use scrapy crawler reference in the python script mentioned above. Second, scrapy finds my spider but is missing my pipelines.\nFrom the traceback, It seems like scrapy is having a problem with python class path.\n$ python main.py\nTraceback (most recent call last):\n  File \"main.py\", line 24, in <module>\n    crawler.configure()\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/crawler.py\", line 44, in configure\n    self.engine = ExecutionEngine(self, self._spider_closed)\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/core/engine.py\", line 63, in __init__\n    self.scraper = Scraper(crawler)\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/core/scraper.py\", line 66, in __init__\n    <b>self.itemproc = itemproc_cls.from_crawler(crawler)</b>\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/middleware.py\", line 50, in from_crawler\n    <b>return cls.from_settings(crawler.settings, crawler)</b>\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/middleware.py\", line 29, in from_settings\n   <b>mwcls = load_object(clspath)</b>\n  File \"/Library/Python/2.7/site-packages/Scrapy-0.17.0-py2.7.egg/scrapy/utils/misc.py\", line 40, in load_object\n    raise ImportError, \"Error loading object '%s': %s\" % (path, e)\nImportError: Error loading object 'aqueleimovel.pipelines.ValidationImovelPipeline': cannot import name crawler\nMy directory struct is like this:\n|- main.py\n|- scrapy.cfg\n|- local_settings.py\n|---- project\n      |- settings.py\n      |- pipelines.py\n      |---- spiders\n            |- spider.py\nI've searched the web and the only reference i could find was:\nhttp://stackoverflow.com/questions/13112520/importing-scrapy-conf-settings-generates-error\nThe guy didnt get any help and dropped scrapy as solution.\nI also asked the scrapy-users mailing list, lots of helps but none of them worked for me. To see more info about this look here: (https://groups.google.com/forum/?fromgroups=#!topic/scrapy-users/UR6vyKQeDw4)\nI also tried to create the tutorial procedure and simply created the script to run the spider and im getting the same traceback error.\nIm running the master clone fo scrapy on a OSX 10.8.2.\nI'm only reporting this, because with all the help in the scrapy-user list we couldn't find a solution, so it may be a scrapy issue.", "issue_status": "Closed", "issue_reporting_time": "2013-03-13T11:40:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1548": {"issue_url": "https://github.com/scrapy/scrapy/issues/265", "issue_id": "#265", "issue_summary": "LOG_STDOUT does not work", "issue_description": "Contributor\nstav commented on Mar 9, 2013\nIt seems the LOG_STDOUT command line setting does not work. Here is the spider I'm using to test:\nfrom scrapy import log\nfrom scrapy.spider import BaseSpider\n\nclass TestSider(BaseSpider):\n    name = \"logtest\"\n    start_urls = [\"http://example.com/\"]\n\n    def parse(self, response):\n        log.msg('LOG: This is a log message')\n        print 'PRINT: This is a print message'\nAnd here is the terminal output which includes both stdout & sttderr, notice the PRINT statement gets written to stdout which appears in the terminal:\nstav@maia:$ /srv/scrapy/scrapy.sh crawl logtest\n2013-03-08 15:25:28-0600 [scrapy] INFO: Scrapy 0.17.0 started (bot: oneoff)\n2013-03-08 15:25:28-0600 [scrapy] DEBUG: Overridden settings: {'NEWSPIDER_MODULE': 'oneoff.spiders', 'SPIDER_MODULES': ['oneoff.spiders'], 'USER_AGENT': 'Chromium OneOff 24.0.1312.56 Ubuntu 12.04 (24.0.1312.56-0ubuntu0.12.04.1)', 'BOT_NAME': 'oneoff'}\n2013-03-08 15:25:28-0600 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2013-03-08 15:25:28-0600 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2013-03-08 15:25:28-0600 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2013-03-08 15:25:28-0600 [scrapy] DEBUG: Enabled item pipelines:\n2013-03-08 15:25:28-0600 [logtest] INFO: Spider opened\n2013-03-08 15:25:28-0600 [logtest] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2013-03-08 15:25:28-0600 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6025\n2013-03-08 15:25:28-0600 [scrapy] DEBUG: Web service listening on 0.0.0.0:6082\n2013-03-08 15:25:28-0600 [logtest] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example/> from <GET http://example.com/>\n2013-03-08 15:25:29-0600 [logtest] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example> from <GET http://www.iana.org/domains/example/>\n2013-03-08 15:25:29-0600 [logtest] DEBUG: Crawled (200) <GET http://www.iana.org/domains/example> (referer: None)\n2013-03-08 15:25:29-0600 [scrapy] INFO: LOG: This is a log message\nPRINT: This is a print message\n2013-03-08 15:25:29-0600 [logtest] INFO: Closing spider (finished)\n2013-03-08 15:25:29-0600 [logtest] INFO: Dumping Scrapy stats:\n    {'downloader/request_bytes': 801,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'downloader/response_bytes': 1204,\n     'downloader/response_count': 3,\n     'downloader/response_status_count/200': 1,\n     'downloader/response_status_count/302': 2,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2013, 3, 8, 21, 25, 29, 292089),\n     'log_count/DEBUG': 10,\n     'log_count/INFO': 5,\n     'response_received_count': 1,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2013, 3, 8, 21, 25, 28, 602254)}\n2013-03-08 15:25:29-0600 [logtest] INFO: Spider closed (finished)\nNow here is the output with the setting enabled:\nstav@maia:$ /srv/scrapy/scrapy.sh crawl -s LOG_STDOUT=True logtest\n2013-03-08 15:25:17-0600 [scrapy] INFO: Scrapy 0.17.0 started (bot: oneoff)\n2013-03-08 15:25:17-0600 [scrapy] DEBUG: Overridden settings: {'NEWSPIDER_MODULE': 'oneoff.spiders', 'LOG_STDOUT': 'True', 'SPIDER_MODULES': ['oneoff.spiders'], 'USER_AGENT': 'Chromium OneOff 24.0.1312.56 Ubuntu 12.04 (24.0.1312.56-0ubuntu0.12.04.1)', 'BOT_NAME': 'oneoff'}\n2013-03-08 15:25:17-0600 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n2013-03-08 15:25:17-0600 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2013-03-08 15:25:17-0600 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2013-03-08 15:25:17-0600 [scrapy] DEBUG: Enabled item pipelines:\n2013-03-08 15:25:17-0600 [logtest] INFO: Spider opened\n2013-03-08 15:25:17-0600 [logtest] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2013-03-08 15:25:17-0600 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6025\n2013-03-08 15:25:17-0600 [scrapy] DEBUG: Web service listening on 0.0.0.0:6082\n2013-03-08 15:25:17-0600 [logtest] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example/> from <GET http://example.com/>\n2013-03-08 15:25:18-0600 [logtest] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example> from <GET http://www.iana.org/domains/example/>\n2013-03-08 15:25:18-0600 [logtest] DEBUG: Crawled (200) <GET http://www.iana.org/domains/example> (referer: None)\n2013-03-08 15:25:18-0600 [scrapy] INFO: LOG: This is a log message\n2013-03-08 15:25:18-0600 [logtest] INFO: Closing spider (finished)\n2013-03-08 15:25:18-0600 [logtest] INFO: Dumping Scrapy stats:\n    {'downloader/request_bytes': 801,\n     'downloader/request_count': 3,\n     'downloader/request_method_count/GET': 3,\n     'downloader/response_bytes': 1204,\n     'downloader/response_count': 3,\n     'downloader/response_status_count/200': 1,\n     'downloader/response_status_count/302': 2,\n     'finish_reason': 'finished',\n     'finish_time': datetime.datetime(2013, 3, 8, 21, 25, 18, 337809),\n     'log_count/DEBUG': 10,\n     'log_count/INFO': 5,\n     'response_received_count': 1,\n     'scheduler/dequeued': 3,\n     'scheduler/dequeued/memory': 3,\n     'scheduler/enqueued': 3,\n     'scheduler/enqueued/memory': 3,\n     'start_time': datetime.datetime(2013, 3, 8, 21, 25, 17, 592098)}\n2013-03-08 15:25:18-0600 [logtest] INFO: Spider closed (finished)\nThere should be a line in there for the PRINT statement, but it is not printed anywhere as I can tell. I have traced the issue to scrapy.log._adapt_eventdict() which I have not been able to debug yet.\nAlso twisted.python.log.startLoggingWithObserver() also has this code:\nif setStdout:\n    sys.stdout = logfile\n    sys.stderr = logerr\nwhich also might be causing some issues. Should it be: sys.stderr = logfile?", "issue_status": "Closed", "issue_reporting_time": "2013-03-08T21:38:24Z", "fixed_by": "#1060", "pull_request_summary": "[MRG+1] Python logging", "pull_request_description": "Member\ncurita commented on Mar 2, 2015\nThis PR switches Twisted logging system to the Python standard one.\nIt's pretty much done, though it's missing documentation (UPDATE: All docs related to logging have been updated) since I want to hear feedback first. I implemented the bare minimum changes to support all previous use cases, but new functionality could definitely be added.\nThe pull request closes a few open issues and other PRs. This closes #921, closes #881, closes #877, closes #875, closes #483, closes #481, closes #433, and closes #265.", "pull_request_status": "Merged", "issue_fixed_time": "2015-04-29T18:20:34Z", "files_changed": [["30", "conftest.py"], ["2", "docs/index.rst"], ["17", "docs/intro/tutorial.rst"], ["30", "docs/topics/benchmarking.rst"], ["5", "docs/topics/debug.rst"], ["8", "docs/topics/downloader-middleware.rst"], ["9", "docs/topics/extensions.rst"]]}, "1549": {"issue_url": "https://github.com/scrapy/scrapy/issues/263", "issue_id": "#263", "issue_summary": "Python 3 support", "issue_description": "extesy commented on Mar 7, 2013\nPython 3 is several years old and most of packages now support it (even django!). It would be really nice to support it in scrapy as well.\n\ud83d\udc4d 1", "issue_status": "Closed", "issue_reporting_time": "2013-03-06T22:48:47Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1550": {"issue_url": "https://github.com/scrapy/scrapy/issues/259", "issue_id": "#259", "issue_summary": "CrawlSpider.allowed_domains \u0441an not be set or tuple", "issue_description": "Contributor\nnuklea commented on Mar 3, 2013\nMy spider:\nclass MySpider(CrawlSpider):\n    allowed_domains = {'hostname.com'}\nand scrapy fetch smth raised:\nTraceback (most recent call last):\n  File \"/home/nuklea/.virtualenvs/default/bin/scrapy\", line 4, in <module>\n    execute()\n  File \"/home/nuklea/.virtualenvs/default/lib/python2.7/site-packages/scrapy/cmdline.py\", line 131, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/nuklea/.virtualenvs/default/lib/python2.7/site-packages/scrapy/cmdline.py\", line 76, in _run_print_help\n    func(*a, **kw)\n  File \"/home/nuklea/.virtualenvs/default/lib/python2.7/site-packages/scrapy/cmdline.py\", line 138, in _run_command\n    cmd.run(args, opts)\n  File \"/home/nuklea/.virtualenvs/default/lib/python2.7/site-packages/scrapy/commands/fetch.py\", line 55, in run\n    default_spider=BaseSpider('default'))\n  File \"/home/nuklea/.virtualenvs/default/lib/python2.7/site-packages/scrapy/utils/spider.py\", line 38, in create_spider_for_request\n    snames = spidermanager.find_by_request(request)\n  File \"/home/nuklea/.virtualenvs/default/lib/python2.7/site-packages/scrapy/spidermanager.py\", line 48, in find_by_request\n    if cls.handles_request(request)]\n  File \"/home/nuklea/.virtualenvs/default/lib/python2.7/site-packages/scrapy/spider.py\", line 61, in handles_request\n    return url_is_from_spider(request.url, cls)\n  File \"/home/nuklea/.virtualenvs/default/lib/python2.7/site-packages/scrapy/utils/url.py\", line 28, in url_is_from_spider\n    getattr(spider, 'allowed_domains', []))\nTypeError: can only concatenate list (not \"set\") to list", "issue_status": "Closed", "issue_reporting_time": "2013-03-03T06:49:30Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1551": {"issue_url": "https://github.com/scrapy/scrapy/issues/253", "issue_id": "#253", "issue_summary": "-bash scrapy command not found after pip install", "issue_description": "mcooganj commented on Feb 22, 2013\nI pip installed scrapy, but it does not appear to have installed the startup script.\nAs a result, when i use the command $ scrapy\ni see the error: --bash: scrapy: command not found\nI am able to import the file inside a python session with\nimport scrapy\nso i am pretty sure it's installed. can someone please help?", "issue_status": "Closed", "issue_reporting_time": "2013-02-22T11:16:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1552": {"issue_url": "https://github.com/scrapy/scrapy/issues/248", "issue_id": "#248", "issue_summary": "Can't add item into another item field", "issue_description": "Contributor\nnuklea commented on Feb 15, 2013\nScrapy 0.16.4 using. This doctest illustrates problem:\n>>> from scrapy.item import Item, Field\n>>> from scrapy.contrib.loader import ItemLoader\n>>> class Product(Item):\n...     seller = Field()\n>>> class Seller(Item):\n...     name = Field()\n>>> pl, sl = ItemLoader(Product()), ItemLoader(Seller())\n>>> sl.add_value('name', 'Seller name')\n>>> pl.add_value('seller', sl.load_item())\n>>> pl.load_item()\n{'seller': [{'name': 'Seller name'}]}\nWith result:\nFailure\n**********************************************************************\nFailed example:\n    pl.load_item()\nExpected:\n    {'seller': [{'name': 'Seller name'}]}\nGot:\n    {'seller': ['name']}", "issue_status": "Closed", "issue_reporting_time": "2013-02-15T08:19:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1553": {"issue_url": "https://github.com/scrapy/scrapy/issues/247", "issue_id": "#247", "issue_summary": "scrapy.spider.spiders", "issue_description": "jackdied commented on Feb 15, 2013\nspiders = ObsoleteClass(\"\"\"\n\"from scrapy.spider import spiders\" no longer works - use \"from scrapy.project import crawler\" and then access crawler.spiders attribute\"\n\"\"\")\nfrom scrapy.project import spiders\nTraceback (most recent call last):\nFile \"\", line 1, in\nImportError: cannot import name spiders", "issue_status": "Closed", "issue_reporting_time": "2013-02-14T20:31:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1554": {"issue_url": "https://github.com/scrapy/scrapy/issues/246", "issue_id": "#246", "issue_summary": "outdated doc", "issue_description": "noah commented on Feb 14, 2013\nTutorial links to diveintopython.org which is a site that belongs to Mark Pilgrim, who ragequit the internet a while ago... http://doc.scrapy.org/en/0.16/intro/tutorial.html", "issue_status": "Closed", "issue_reporting_time": "2013-02-14T12:54:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1555": {"issue_url": "https://github.com/scrapy/scrapy/issues/245", "issue_id": "#245", "issue_summary": "Downloader class memory leaking", "issue_description": "Contributor\nMimino666 commented on Feb 13, 2013\nIn Downloader class, slots are only created, but never deleted. When scrapy visits many different domains, during crawl, it can take a good amount of memory, storing all those empty slots.", "issue_status": "Closed", "issue_reporting_time": "2013-02-13T05:00:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1556": {"issue_url": "https://github.com/scrapy/scrapy/issues/243", "issue_id": "#243", "issue_summary": "FormRequest.from_response breaks on bad html", "issue_description": "mbilalf commented on Feb 12, 2013\nBad HTML causes from_response to throw an exception.\n  File \"/usr/lib64/python2.6/site-packages/twisted/internet/defer.py\", line 576, in _runCallbacks\n    current.result = callback(current.result, *args, **kw)\n  File \"crawler.py\", line 41, in parse\n    callback=self.parse_page)\n  File \"/usr/lib/python2.6/site-packages/scrapy/http/request/form.py\", line 36, in from_response\n    form = _get_form(response, formname, formnumber)\n  File \"/usr/lib/python2.6/site-packages/scrapy/http/request/form.py\", line 53, in _get_form\n    raise ValueError(\"No `<form>` element found in %s\" % response)\n==========HTML=========\n<html>\n    <head>\n        <link rel=\"stylesheet\" href=\"stylesheets/style.css\">\n            <title>Search</title>\n    </head>\n    <body>\n    </body>\n</html>\n<form name=\"formSearch\" method=\"post\" action=\"Search.aspx\" id=\"formSearch\">\n<input type=\"hidden\" name=\"__EVENTTARGET\" value=\"\" />\n</form>", "issue_status": "Closed", "issue_reporting_time": "2013-02-12T09:59:04Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1557": {"issue_url": "https://github.com/scrapy/scrapy/issues/236", "issue_id": "#236", "issue_summary": "There is no method to view the status of a particular job in scrapyd webservice", "issue_description": "egrachev commented on Feb 7, 2013\nNo description provided.", "issue_status": "Closed", "issue_reporting_time": "2013-02-07T13:17:13Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1558": {"issue_url": "https://github.com/scrapy/scrapy/issues/230", "issue_id": "#230", "issue_summary": "Scrapy does not retry on error from HttpCompressionMiddleware.", "issue_description": "Contributor\naivarsk commented on Jan 24, 2013\nI'm using Scrapy with a random proxy from a list of verified proxies. By the time Scrapy uses them some proxies are already closed/disabled/reconfigured. If connection fails Scrapy retries the same URL (middleware chooses another proxy). However if proxy returns invalid content and decompression fails Scrapy does not retry the same URL. I get the following error:\nERROR: Error downloading <GET http://xxx>: Error -3 while decompressing: invalid stored block lengths\nCurrently I've solved this by disabling HttpCompressionMiddleware. Is it possible to retry on such errors as well? Or turn off compression for host(+proxy) and retry?\nScrapy : 0.17.0-108-g71d7df9\nlxml : 2.3.2.0\nlibxml2 : 2.7.8\nTwisted : 11.1.0\nPython : 2.7.3 (default, Aug 1 2012, 05:16:07) - [GCC 4.6.3]\nPlatform: Linux-3.2.0-26-generic-pae-i686-with-Ubuntu-12.04-precise", "issue_status": "Closed", "issue_reporting_time": "2013-01-24T09:29:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1559": {"issue_url": "https://github.com/scrapy/scrapy/issues/225", "issue_id": "#225", "issue_summary": "spider_idle signal not called everytime", "issue_description": "Contributor\nMimino666 commented on Jan 9, 2013\nI have noticed that spider_idle signal is not called when the last issued request is filtered out by dedupe filter.\nExample spider:\nfrom scrapy import log, signals\nfrom scrapy.http import Request\nfrom scrapy.spider import BaseSpider\n\n\nclass IdleSpider(BaseSpider):\n    name = 'idle_spider'\n\n    def set_crawler(self, crawler):\n        super(IdleSpider, self).set_crawler(crawler)\n        self.crawler.signals.connect(self._spider_idle, signal=signals.spider_idle)\n\n    def _spider_idle(self, spider):\n        self.log('Enqueuing...', log.INFO)\n        self.crawler.engine.crawl(Request('http://httpbin.org/'), self)\n\n    def start_requests(self):\n        return []\n\n    def parse(self, response):\n        self.log('Parsing...', log.INFO)\n        pass\nMessage \"Enqueuing...\" is printed out exactly two times and \"Parsing...\" exactly once. After that spider_idle signal is sent no more. To me it seems like a bug, but if it is an intention, please reconsider it. It is rather unfortunate in the scenario, when _spider_idle is issuing new requests in batches and suddenly the last request was redirected to previously visited url (i.e. it wasn't even directly duplicate request).\nOf course the simple solution can be to use dont_filter on the issued requests, but it doesn't seem right to me.", "issue_status": "Closed", "issue_reporting_time": "2013-01-08T23:44:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1560": {"issue_url": "https://github.com/scrapy/scrapy/issues/224", "issue_id": "#224", "issue_summary": "spider_closed signal gets way more keyword params than described in docs.", "issue_description": "dchaplinsky commented on Jan 9, 2013\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/core/engine.py#L264\nhttp://doc.scrapy.org/en/latest/topics/signals.html#spider-closed\nAs usual exception is only happens when full crawl is finished, so you can understand how annoying it was to add one param to signal handler and run full crawl just to find that there is another one. And another one", "issue_status": "Closed", "issue_reporting_time": "2013-01-08T23:37:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1561": {"issue_url": "https://github.com/scrapy/scrapy/issues/223", "issue_id": "#223", "issue_summary": "Headers lose their order when passed to a DownloaderMiddleware", "issue_description": "iramari commented on Jan 6, 2013\nWhen passing the Headers to a DownloaderMiddleware, the order of the headers is lost.\nscrapy\\http\\headers.py uses a CaselessDict which inherits from dict. Using an OrderedDict instead of a dict would preserve the order.", "issue_status": "Closed", "issue_reporting_time": "2013-01-05T18:37:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1562": {"issue_url": "https://github.com/scrapy/scrapy/issues/220", "issue_id": "#220", "issue_summary": "process_spider_exception() not invoked for generators", "issue_description": "Contributor\nMimino666 commented on Jan 3, 2013\nWhen a spider implements parse() function through generator (i.e. returning results with yields) and an exception raises in parse(), then spidermiddleware's process_spider_exception() is not called.\nThe reason is that exception is finally invoked in one of the spidermiddlewares.\n\ud83d\udc4d 6", "issue_status": "Closed", "issue_reporting_time": "2013-01-03T03:23:03Z", "fixed_by": "#2061", "pull_request_summary": "[MRG+1] process_spider_exception on generators", "pull_request_description": "Member\nelacuesta commented on Jun 18, 2016 \u2022\nedited\nThis PR is a starting point to fix #220.\nIt could probably use some more test cases, mostly to figure out what exactly is the desired behaviour when processing the exceptions.\nI can't take much credit for this: if it breaks, blame @dangra \ud83d\ude1b\n\ud83d\udc4d 3\n\ud83d\ude04 1", "pull_request_status": "Merged", "issue_fixed_time": "2019-04-01T07:43:19Z", "files_changed": [["10", "docs/topics/spider-middleware.rst"], ["27", "scrapy/core/downloader/middleware.py"], ["1", "scrapy/core/scraper.py"], ["89", "scrapy/core/spidermw.py"], ["7", "scrapy/exceptions.py"], ["20", "scrapy/utils/python.py"], ["64", "tests/test_downloadermiddleware.py"], ["102", "tests/test_spidermiddleware.py"], ["380", "tests/test_spidermiddleware_output_chain.py"], ["14", "tests/test_utils_python.py"]]}, "1563": {"issue_url": "https://github.com/scrapy/scrapy/issues/211", "issue_id": "#211", "issue_summary": "Scrapy changing the animated gif to static gif after downloading", "issue_description": "ghost commented on Dec 21, 2012\nI am using scrapy to downlaod the images from site using image pipeline.\nThe images are downloaded but animated gif have become static gif . those images are not animated any more.\nhow can i retain the animated gif after downloading\nThis is my pipeline\nclass MyImagesPipeline(ImagesPipeline):\n\n    # name download version\n    def image_key(self, url):\n        image_guid = url.split('/')[-1]\n        return 'full/%s' % (image_guid)\n\n    def get_media_requests(self, item, info):\n        if item['image_urls']:\n        for image_url in item['image_urls']:\n            yield Request(image_url)", "issue_status": "Closed", "issue_reporting_time": "2012-12-20T21:46:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1564": {"issue_url": "https://github.com/scrapy/scrapy/issues/209", "issue_id": "#209", "issue_summary": "Parse commando doesn't work without specifying concrete spider", "issue_description": "enagorny commented on Dec 20, 2012\nIt worked in 0.14.\nSTR:\nscrapy parse \"http://www.example.com/foobar\" -c parse_item\nAnd now it throws exception:\nTraceback (most recent call last):\nFile \"/usr/local/bin/scrapy\", line 4, in <module>\n  execute()\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 131, in execute\n  _run_print_help(parser, _run_command, cmd, args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 76, in _run_print_help\n  func(*a, **kw)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/cmdline.py\", line 138, in _run_command\n  cmd.run(args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/parse.py\", line 194, in run\n  self.set_spider(url, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/commands/parse.py\", line 128, in set_spider\n  self.spider = create_spider_for_request(self.crawler.spiders, url)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/utils/spider.py\", line 38, in create_spider_for_request\n  snames = spidermanager.find_by_request(request)\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/spidermanager.py\", line 48, in find_by_request\n  if cls.handles_request(request)]\nFile \"/usr/local/lib/python2.7/dist-packages/scrapy/spider.py\", line 62, in handles_request\n  return url_is_from_spider(request.url, cls)\nAttributeError: 'str' object has no attribute 'url'", "issue_status": "Closed", "issue_reporting_time": "2012-12-20T10:53:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1565": {"issue_url": "https://github.com/scrapy/scrapy/issues/207", "issue_id": "#207", "issue_summary": "How can i follow the link and save items manually in InitSpider", "issue_description": "ghost commented on Dec 17, 2012\nThe crawl spider works fine because the logic of following links is already implemented in that with rules.\nBut i am inheriting from InitSpider and that don't have the rules option so i manually have to follow the links.\nBut then i get stuck whether i should return my item or Request.\nMy problem is written below\nI am using InitSpider for my scraping.\nI have the list page where\nI want to grab the list item and then makinga request to go into that list page and populate my item and then resturning my that item too save in pipeline\nI also grab the next page link so that i can further follow the link\nBut i am stuck at this\n    def parse(self, response):\n        soup = BeautifulSoup(response.body)\n        hxs = HtmlXPathSelector(response)\n        sites = hxs.select('//div[@class=\"row\"]')\n        items = []\n\n        for site in sites[:5]:\n            item = TestItem()\n            item['username'] = \"test5\"\n            request =  Request(\"http://www.example.org/profile.php\",  callback = self.parseUserProfile)\n            request.meta['item'] = item\n            **yield item**\n\n        mylinks= soup.find_all(\"a\", text=\"Next\")\n        if mylinks:\n            nextlink = mylinks[0].get('href')\n            yield Request(urljoin(response.url, nextlink), callback=self.parse)\n\n    def parseUserProfile(self, response):\n        item = response.meta['item']\n        item['image_urls'] = \"test3\"\n        return item\nNow my above works but with that i am not getting value of item['image_urls'] = \"test3\"\nIt is coming as null\nNow if use return request instead ofyield item\nThen get error thatcannot use return with generator\nIf i remove this line\nyield Request(urljoin(response.url, nextlink), callback=self.parse)Then my code works fine and i can get image_urls but then i canot follow the links\nSo is there any way so that i can use return request and yield together so that i get the item_urls", "issue_status": "Closed", "issue_reporting_time": "2012-12-17T02:53:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1566": {"issue_url": "https://github.com/scrapy/scrapy/issues/200", "issue_id": "#200", "issue_summary": "why 'image_downloaded hook failed'", "issue_description": "rlog commented on Nov 29, 2012\nwhen i use ImagesPipeline, some warning show like this, and there no images download:\nDEBUG: Crawled (200) <GET http://file8.mafengwo.net/M00/93/3D/wKgByU_RaouMmj-8AAT0RBRDt0g26.groupinfo.w600.jpeg> (referer: None)\nDEBUG: Image (downloaded): Downloaded image from <GET http://file8.mafengwo.net/M00/93/3D/wKgByU_RaouMmj-8AAT0RBRDt0g26.groupinfo.w600.jpeg> referred in <None>\nWARNING: 'image_downloaded hook failed'", "issue_status": "Closed", "issue_reporting_time": "2012-11-29T06:30:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1567": {"issue_url": "https://github.com/scrapy/scrapy/issues/199", "issue_id": "#199", "issue_summary": "Exception UnicodeDecodeError in linkextractors", "issue_description": "dzyao commented on Nov 27, 2012\nFile \"/lib/python2.6/site-packages/Scrapy-0.16.2-py2.6.egg/scrapy/contrib/linkextractors/sgml.py\"\nline 84, in handle_data \nself.current_link.text = self.current_link.text + data.strip()\nexceptions.UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128).\nI find other one also have the same issues on this line. My suggestion is to create a Unicode detection code like this.\n if isinstance(self.current_link.text, unicode):\n                self.current_link.text = self.current_link.text + data.strip()\nelse:\n                self.current_link.text = self.current_link.text.decode('utf-8') + data.strip()\n#self.current_link.text = self.current_link.text + data.strip()", "issue_status": "Closed", "issue_reporting_time": "2012-11-27T01:07:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1568": {"issue_url": "https://github.com/scrapy/scrapy/issues/197", "issue_id": "#197", "issue_summary": "Suggestion for cleaning Item data (removing or opt-in pipelining)", "issue_description": "Hedde commented on Nov 23, 2012\nI don't really like the pipeline cycle as a basic clean cycle because most of the time it requires some sort of differentiation per Item leaf class.\nMy suggestion is to create something similar as django's forms.Form clean loop. Which does something similar like my usual work-around, e.g.:\nclass ExtendedItem(Item):\n    def _process(self):\n        # Runs every '_process_FIELD_NAME' before being reached.\n        [getattr(self, func)() for func in dir(self) if func.split('_')[-1] in self.fields and callable(getattr(self, func))]\nNow in my class definitions I can do something like (normally extended with optional arguments):\nclass Necklace(ExtendedItem):\n    pearls = Field()\n    diamonds = Field()\n\n    def _process_pearls(self):\n        # clean field\n\n    def _process(self):\n        # override clean all\n        super(Necklace, self)._process()\nThis plays nicely without intervening other classes their data unless you want to.. but that's where the pipeline is a nice opt-in \ud83d\udc4d", "issue_status": "Closed", "issue_reporting_time": "2012-11-23T16:50:16Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1569": {"issue_url": "https://github.com/scrapy/scrapy/issues/194", "issue_id": "#194", "issue_summary": "ssl handshake failure (openssl bug in Ubuntu 12.04)", "issue_description": "Member\npablohoffman commented on Nov 15, 2012\nDue to this openssl bug:\nhttps://bugs.launchpad.net/ubuntu/+source/openssl/+bug/965371\nScrapy is failing to retrieve some pages via https such as this one:\nhttps://www.onekingslane.com/login\nMore information about client/server version compatibility can be found here:\nhttp://docs.python.org/dev/library/ssl.html#ssl.wrap_socket\nOther pages that talk about this issue:\nhttp://askubuntu.com/questions/116020/python-https-requests-urllib2-to-some-sites-fail-on-ubuntu-12-04-without-proxy\nThe error goes like this:\n2012-11-15 16:22:57-0200 [default] ERROR: Error downloading <GET https://www.onekingslane.com/login>: [('SSL routines', 'SSL23_READ', 'ssl handshake failure')]", "issue_status": "Closed", "issue_reporting_time": "2012-11-15T18:23:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1570": {"issue_url": "https://github.com/scrapy/scrapy/issues/193", "issue_id": "#193", "issue_summary": "Another IOError: \"Not a gzipped file\"", "issue_description": "Contributor\nvkrest commented on Nov 13, 2012\nTake a look at 'httpcompression' middleware and 'sitemap' middleware.\nIf you will try to download some gzipped file then 'httpcompression' middleware will decompress it first.\nSee it here:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/httpcompression.py#L36\nThen 'sitemap' will try to decompress it again and will rise the IOError here:\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/contrib/spiders/sitemap.py#L57\nIn my point of view we have to leave only one place for decompression and it will be reasonable if it will be 'httpcompression'.", "issue_status": "Closed", "issue_reporting_time": "2012-11-13T10:52:24Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1571": {"issue_url": "https://github.com/scrapy/scrapy/issues/191", "issue_id": "#191", "issue_summary": "aggressive mechanism for spider initiation in 0.16.x?", "issue_description": "shadowing commented on Nov 11, 2012\nHi,\nI've been working on a complicated data scraping project for several months, the development are mainly based on 0.14.x stable and 0.15.x unstable. Last time I run it's around early Oct, all spiders ran flawlessly.\nBut after I upgraded to 0.16.x, it just printed endless ERROR, not doing anything. The only thing I can do is actually to kill -9. Then I tried downgrading to 0.14.4, it runs back to normal.\nthe error msg is like follows:\n2012-11-09 00:39:33+0800 [scrapy] INFO: Scrapy 0.16.1 started (bot: mybot)\n2012-11-09 00:39:33+0800 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, CoreStats, SpiderState, FailLogger\n2012-11-09 00:39:33+0800 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, RandomUserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, RedirectMiddleware, HttpCompressionMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2012-11-09 00:39:33+0800 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2012-11-09 00:39:33+0800 [scrapy] DEBUG: Enabled item pipelines: sanitizePipeline\n2012-11-09 00:39:34+0800 [-] ERROR: 2012-11-09 00:39:34+0800 [-] ERROR: 2012-11-09 00:39:34+0800\nmessage like [-] ERROR: 2012-11-09 00:39:34+0800 just went on forever with the same timestamp, and ^C cannot stop it.\nSo, I start a new project using scrapy startproject mybot, copied one spider class, scrapy crawl spider, runs great; copy another, great; then one by one, until one, it started to print all these messages again. BTW, I run the same spider all the time.\nThought it could be some problem in my __init__() implementation, I brutally commented it, still wont work\nSo, you see, here I got a perfect running project and some spiders under scrapy 0.15.1/0.14.x, but in 0.16.x, all of the spiders won't work because of some certain spiders. My best guess is, is there some aggressive mechanism for spider initiation in 0.16.x?", "issue_status": "Closed", "issue_reporting_time": "2012-11-11T05:19:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1572": {"issue_url": "https://github.com/scrapy/scrapy/issues/188", "issue_id": "#188", "issue_summary": "Signal dispatching doesn't work on pypy", "issue_description": "royisme commented on Nov 10, 2012\nThere are some exception message when I use pypy1.9 to run scrapy ,but use python2.7 is ok.\n% /usr/local/share/pypy/scrapy shell http://www.baidu.com             \nzsh: correct 'shell' to 'shells' [nyae]? n\n2012-11-09 16:40:06+0800 [scrapy] INFO: Scrapy 0.16.1 started (bot: scrapybot)\n2012-11-09 16:40:06+0800 [scrapy] DEBUG: Enabled extensions: TelnetConsole, WebService, CloseSpider, CoreStats, SpiderState\n2012-11-09 16:40:06+0800 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, RedirectMiddleware, CookiesMiddleware, HttpCompressionMiddleware, ChunkedTransferMiddleware, DownloaderStats\n2012-11-09 16:40:06+0800 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2012-11-09 16:40:06+0800 [scrapy] DEBUG: Enabled item pipelines: \n2012-11-09 16:40:06+0800 [scrapy] ERROR: Error caught on signal handler: <bound method instance.start_listening of <scrapy.telnet.TelnetConsole instance at 0x00000001063f0bc0>>\n    Traceback (most recent call last):\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 1045, in _inlineCallbacks\n        result = g.send(result)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/core/engine.py\", line 75, in start\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/signalmanager.py\", line 23, in send_catch_log_deferred\n        return signal.send_catch_log_deferred(*a, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/utils/signal.py\", line 53, in send_catch_log_deferred\n        *arguments, **named)\n    --- <exception caught here> ---\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n        result = f(*args, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 47, in robustApply\n        return receiver(*arguments, **named)\n    exceptions.TypeError: start_listening() got 2 unexpected keyword arguments\n\n2012-11-09 16:40:06+0800 [scrapy] ERROR: Error caught on signal handler: <bound method instance.start_listening of <scrapy.webservice.WebService instance at 0x0000000103f35240>>\n    Traceback (most recent call last):\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 1045, in _inlineCallbacks\n        result = g.send(result)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/core/engine.py\", line 75, in start\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/signalmanager.py\", line 23, in send_catch_log_deferred\n        return signal.send_catch_log_deferred(*a, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/utils/signal.py\", line 53, in send_catch_log_deferred\n        *arguments, **named)\n    --- <exception caught here> ---\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n        result = f(*args, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 47, in robustApply\n        return receiver(*arguments, **named)\n    exceptions.TypeError: start_listening() got 2 unexpected keyword arguments\n\n2012-11-09 16:40:06+0800 [default] INFO: Spider opened\n2012-11-09 16:40:06+0800 [default] ERROR: Error caught on signal handler: <bound method CoreStats.spider_opened of <scrapy.contrib.corestats.CoreStats object at 0x0000000105ff8f38>>\n    Traceback (most recent call last):\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 1045, in _inlineCallbacks\n        result = g.send(result)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/core/engine.py\", line 225, in open_spider\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/signalmanager.py\", line 23, in send_catch_log_deferred\n        return signal.send_catch_log_deferred(*a, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/utils/signal.py\", line 53, in send_catch_log_deferred\n        *arguments, **named)\n    --- <exception caught here> ---\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n        result = f(*args, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 47, in robustApply\n        return receiver(*arguments, **named)\n    exceptions.TypeError: spider_opened() got 2 unexpected keyword arguments\n\n2012-11-09 16:40:06+0800 [default] ERROR: Error caught on signal handler: <bound method SpiderState.spider_opened of <scrapy.contrib.spiderstate.SpiderState object at 0x0000000105ff9088>>\n    Traceback (most recent call last):\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 1045, in _inlineCallbacks\n        result = g.send(result)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/core/engine.py\", line 225, in open_spider\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/signalmanager.py\", line 23, in send_catch_log_deferred\n        return signal.send_catch_log_deferred(*a, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/utils/signal.py\", line 53, in send_catch_log_deferred\n        *arguments, **named)\n    --- <exception caught here> ---\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n        result = f(*args, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 47, in robustApply\n        return receiver(*arguments, **named)\n    exceptions.TypeError: spider_opened() got 2 unexpected keyword arguments\n\n2012-11-09 16:40:06+0800 [default] ERROR: Error caught on signal handler: <bound method OffsiteMiddleware.spider_opened of <scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware object at 0x0000000104821d38>>\n    Traceback (most recent call last):\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 1045, in _inlineCallbacks\n        result = g.send(result)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/core/engine.py\", line 225, in open_spider\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/signalmanager.py\", line 23, in send_catch_log_deferred\n        return signal.send_catch_log_deferred(*a, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/utils/signal.py\", line 53, in send_catch_log_deferred\n        *arguments, **named)\n    --- <exception caught here> ---\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n        result = f(*args, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 47, in robustApply\n        return receiver(*arguments, **named)\n    exceptions.TypeError: spider_opened() got 2 unexpected keyword arguments\n\n2012-11-09 16:40:07+0800 [default] DEBUG: Crawled (200) <GET http://www.baidu.com> (referer: None)\n2012-11-09 16:40:07+0800 [default] ERROR: Error caught on signal handler: <bound method CoreStats.response_received of <scrapy.contrib.corestats.CoreStats object at 0x0000000105ff8f38>>\n    Traceback (most recent call last):\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 464, in _startRunCallbacks\n        self._runCallbacks()\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/twisted/internet/defer.py\", line 551, in _runCallbacks\n        current.result = callback(current.result, *args, **kw)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/core/engine.py\", line 200, in _on_success\n        response=response, request=request, spider=spider)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/signalmanager.py\", line 19, in send_catch_log\n        return signal.send_catch_log(*a, **kw)\n    --- <exception caught here> ---\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/utils/signal.py\", line 22, in send_catch_log\n        *arguments, **named)\n      File \"/usr/local/Cellar/pypy/1.9/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 47, in robustApply\n        return receiver(*arguments, **named)\n    exceptions.TypeError: response_received() got 4 unexpected keyword arguments\n\n[s] Available Scrapy objects:\n[s]   hxs        <HtmlXPathSelector xpath=None data=u'<html><head><meta http-equiv=\"Content-Ty'>\n[s]   item       {}\n[s]   request    <GET http://www.baidu.com>\n[s]   response   <200 http://www.baidu.com>\n[s]   settings   <CrawlerSettings module=None>\n[s]   spider     <BaseSpider 'default' at 0x10482b980>\n[s] Useful shortcuts:\n[s]   shelp()           Shell help (print this help)\n[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n[s]   view(response)    View response in a browser", "issue_status": "Closed", "issue_reporting_time": "2012-11-10T02:14:50Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1573": {"issue_url": "https://github.com/scrapy/scrapy/issues/187", "issue_id": "#187", "issue_summary": "request.replace not working in process_response, return response never gets called", "issue_description": "adaschevici commented on Nov 8, 2012\nHello. I have searched for this but i haven't found any satisfactory answer:\nThis is my code. I found some replies that say this should work.\nBut when i run it as part of my spider i get an infinite loop that apparently get stuck on the line that prints rt.url .\nI suspect that there may be a problem with the priority of middlewares but i am not sure.\nfrom auctionzip.settings import USER_AGENT_LIST\nimport random\nfrom scrapy import log\nclass RandomUserAgentMiddleware(object):\n   def process_request(self, request, spider):\n        ua  = random.choice(USER_AGENT_LIST)\n        tmp = \"http://www.freelancer.com\"\n        rt = request.replace(url = tmp)\n        #if ua:\n        #    request.headers.setdefault('User-Agent', ua)\n        #    request.headers.setdefault('Referrer', 'http://www.auctionzip.com')\n        print rt.url\n        return rt\n        #log.msg('>>>> UA %s'%request.headers)\n\n    def process_response(self, request, response, spider):\n        print \"in response handler\"\n        return response\nThis is the order of my middlewares:\nDOWNLOADER_MIDDLEWARES = {\n    'auctionzip.middlewares.random_user_agent.RandomUserAgentMiddleware': 100,\n    #'wines_crawler.middlewares.tor_anonymizer.TorMiddleWare' : 401,\n    #'wines_crawler.middlewares.random_proxy.ProxyMiddleware' : 401,\n    #'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,\n    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 402,\n    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware' : 600,\n    #'wines_crawler.middlewares.retry_change.RetryTORChangeProxyMiddleWare' : 600,\n    #'wines_crawler.middlewares.retry_change.RetryChangeProxyMiddleware' : 600,\n}\nI tried this with scrapy 0.16 and 0.17.", "issue_status": "Closed", "issue_reporting_time": "2012-11-07T21:16:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1574": {"issue_url": "https://github.com/scrapy/scrapy/issues/184", "issue_id": "#184", "issue_summary": "Offsite not working for urls containing domain name in url parameters", "issue_description": "greatwitenorth commented on Nov 6, 2012\nI've noticed that despite setting allowed_domains correctly that scrapy is still passing urls that contain the allowed domain, but is not in the hostname. For example:\nclass SitemapgenSpider(CrawlSpider):\n    name = \"sitemapgen\"\n    allowed_domains = [\"makesomecode.com\"]\n    start_urls = [\n            \"http://makesomecode.com\"\n        ]\n\n    rules = (\n        # Extract links matching 'category.php' (but not matching 'subsection.php')\n        # and follow links from them (since no callback means follow=True by default).\n        Rule(SgmlLinkExtractor(allow=()), callback='parse_item', follow=True),\n    )\n\n    def parse_item(self, response):\n        item = SitemapgenItem()\n\n        item['url'] = response.url\n        item['content'] = response.headers['content-type']\n        return item\nDespite having the domain set I'm seeing urls like this parsed in the logs:\n2012-11-05 16:53:07-0600 [sitemapgen] DEBUG: Crawled (200) <GET http://digg.com/submit?url=http%3A%2F%2Fmakesomecode.com%2Fabout%2F&title=About> (referer: http://makesomecode.com/about/)\nI'm not sure if the Offsite Middleware is incorrectly parsing it because 'makesomecode.com' appears as a parameter value in the url.\nI'm also seeing stuff like this show up:\n2012-11-05 17:00:04-0600 [sitemapgen] DEBUG: Redirecting (302) to <GET http://www.reddit.com/submit?url=http%3A%2F%2Fmakesomecode.com%2F2011%2F09%2F02%2Fproducteev-php-library%2F&title=Producteev+PHP+Library> from <GET http://reddit.com/submit?url=http%3A%2F%2Fmakesomecode.com%2F2011%2F09%2F02%2Fproducteev-php-library%2F&title=Producteev+PHP+Library>\nShouldn't the url not even make it to the redirect if it's not allowed?", "issue_status": "Closed", "issue_reporting_time": "2012-11-05T23:01:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1575": {"issue_url": "https://github.com/scrapy/scrapy/issues/183", "issue_id": "#183", "issue_summary": "Tutorial bug", "issue_description": "dangayle commented on Nov 2, 2012\nI'm doing the tutorial here (http://doc.scrapy.org/en/latest/intro/tutorial.html#trying-selectors-in-the-shell) and I'm getting this error: https://gist.github.com/03ee4a3f8f7d6db942ef\nCan anyone help me debug this?", "issue_status": "Closed", "issue_reporting_time": "2012-11-01T20:55:15Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1576": {"issue_url": "https://github.com/scrapy/scrapy/issues/182", "issue_id": "#182", "issue_summary": "items_dir setting overwritten when run as \"scrapy server\"", "issue_description": "mike-athene commented on Oct 24, 2012\nHi,\nI store parsed data in database on remote machine and don't need to have items feed on same server as scrapyd. According to docs this can be done by setting \"items_dir\" to empty value, which doesn't seem to work when launching scrapyd with \"scrapy server\":\nitems_dir would be set anyway in https://github.com/scrapy/scrapy/blob/master/scrapyd/script.py#L32\nIf I remove items_dir from script.py I still get \"Items\" link in web admin which leads me to root of scrapy project due to wrong static files configuration (which is definitely bad thing as I can download any file from it)\nI can see \"Items\" link for every spider in web admin\nSame thing for logs_dir\nAm I doing something wrong, or is this an actual bug? If so, would be happy to provide a patch.\n$scrapy version -v\nScrapy  : 0.17.0\nlxml    : 3.0.1.0 \nlibxml2 : 2.7.8 \nTwisted : 12.2.0\nPython  : 2.6.6 (r266:84292, Dec 26 2010, 22:31:48) - [GCC 4.4.5] \nPlatform: Linux-2.6.32-12-pve-x86_64-with-debian-6.0.4", "issue_status": "Closed", "issue_reporting_time": "2012-10-24T12:43:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1577": {"issue_url": "https://github.com/scrapy/scrapy/issues/180", "issue_id": "#180", "issue_summary": "scrapy.bat uses wrong path for interpreter in virtualenv", "issue_description": "keccs commented on Oct 6, 2012\nPlatform: windows vista 64-bit\nPython version: 2.7.3 32-bit\nVirtualenv version: 1.8.2\nScrapy version: 0.14.4\nscrapy.bat uses the following path:\n\"%~dp0..\\python\"\nwhich results in an error when running any scrapy commands\nthe interpreter is actually at:\n\"%~dp0\"python", "issue_status": "Closed", "issue_reporting_time": "2012-10-06T09:21:14Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1578": {"issue_url": "https://github.com/scrapy/scrapy/issues/178", "issue_id": "#178", "issue_summary": "scrapy check -l fails in some cases", "issue_description": "Member\npablohoffman commented on Oct 3, 2012\nSee how scrapy list works but scrapy check -l fails in this case:\nprh@prh-laptop:~/src/testspiders [17:42:25]\n$ scrapy list\ntimed\ndummy\nnoop\nfollowall\nmad\nprh@prh-laptop:~/src/testspiders [17:43:39]\n$ scrapy check -l\nTraceback (most recent call last):\n  File \"/home/prh/src/scrapy/bin/scrapy\", line 4, in <module>\n    execute()\n  File \"/home/prh/src/scrapy/scrapy/cmdline.py\", line 127, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/home/prh/src/scrapy/scrapy/cmdline.py\", line 76, in _run_print_help\n    func(*a, **kw)\n  File \"/home/prh/src/scrapy/scrapy/cmdline.py\", line 134, in _run_command\n    cmd.run(args, opts)\n  File \"/home/prh/src/scrapy/scrapy/commands/check.py\", line 48, in run\n    spider = self.crawler.spiders.create(spider)\n  File \"/home/prh/src/scrapy/scrapy/spidermanager.py\", line 44, in create\n    return spcls(**spider_kwargs)\n  File \"/home/prh/src/testspiders/testspiders/spiders/timed.py\", line 14, in __init__\n    super(TimedSpider, self).__init__(url)\nTypeError: __init__() takes exactly 1 argument (2 given)\nRegardless of weather the timed spider should require a constructor argument or not, why does scrapy check -l need to actually instantiate the spiders?. Should it be enough to inspect the classes like scrapy list does?", "issue_status": "Closed", "issue_reporting_time": "2012-10-02T20:45:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1579": {"issue_url": "https://github.com/scrapy/scrapy/issues/173", "issue_id": "#173", "issue_summary": "Persist job data in Scrapyd", "issue_description": "Member\npablohoffman commented on Sep 13, 2012\nJob data in scrapyd is only kept in memory and thus removed when Scrapyd is restarted. We should store job data (in a sqlite or something) so that it persists after restarts. This is particularly important for completed jobs.", "issue_status": "Closed", "issue_reporting_time": "2012-09-13T18:21:01Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1580": {"issue_url": "https://github.com/scrapy/scrapy/issues/170", "issue_id": "#170", "issue_summary": "Scrapy working in Wrong Directory", "issue_description": "alexw23 commented on Sep 11, 2012\nI recently installed Python 2.7 on Mountain Lion. However, my Scrapy version is referencing another folder or something because when I cd into the directory and type scrapy list - the outputted list is an older version of my project.\nIs there something I am missing?\nSimilarly when I do scrapy crawl <spider_name>.py I get spider not found i.e. scrapy crawl groupy.py however the spider does exist.\nIf its an older spider (from previous revisions) it loads the spider - however changes to the file in spiders/ is not seen.\nI even updated BOT_VERSION to 1.5 in settings.py and when I run scrapy settings --get BOT_VERSION I returns 1.0\nRunning Scrapy 0.14", "issue_status": "Closed", "issue_reporting_time": "2012-09-11T00:02:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1581": {"issue_url": "https://github.com/scrapy/scrapy/issues/169", "issue_id": "#169", "issue_summary": "Schedule or callLater Request()", "issue_description": "franciscolourenco commented on Sep 1, 2012\nIs there a way to schedule a Request for a specific url for later, instead of having the yield Request() be processed asap?", "issue_status": "Closed", "issue_reporting_time": "2012-09-01T00:27:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1582": {"issue_url": "https://github.com/scrapy/scrapy/issues/166", "issue_id": "#166", "issue_summary": "MemoryStatsCollector not JSON serializable", "issue_description": "pprett commented on Aug 17, 2012\nWhen I try to access the crawler stats via the web interface (http://localhost:6080/stats) I get a TypeError::\nexceptions.TypeError: <scrapy.statscol.MemoryStatsCollector object at 0x14bfe50> is not JSON serializable", "issue_status": "Closed", "issue_reporting_time": "2012-08-17T09:25:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1583": {"issue_url": "https://github.com/scrapy/scrapy/issues/163", "issue_id": "#163", "issue_summary": "Empty results from SgmlLinkExtractor", "issue_description": "turian commented on Aug 5, 2012\nSgmlLinkExtractor returns no links for http://metaoptimize.com/qa/", "issue_status": "Closed", "issue_reporting_time": "2012-08-05T02:15:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1584": {"issue_url": "https://github.com/scrapy/scrapy/issues/162", "issue_id": "#162", "issue_summary": "Small documentation fix", "issue_description": "turian commented on Aug 5, 2012\nIn http://doc.scrapy.org/en/latest/topics/debug.html\nin section parsing, 'parse' should come right after 'scrapy'.", "issue_status": "Closed", "issue_reporting_time": "2012-08-05T01:37:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1585": {"issue_url": "https://github.com/scrapy/scrapy/issues/161", "issue_id": "#161", "issue_summary": "Failed to determine memory usage on FreeBSD", "issue_description": "kcwu commented on Aug 4, 2012\nI am using FreeBSD, not Linux.\nJust follow tutorial, I got exception from scrapy.\n2012-08-05 00:05:03+0800 [scrapy] ERROR: Error caught on signal handler: <bound method ?.engine_started of <scrapy.contrib.memusage.MemoryUsage object at 0x80935bf10>>\nTraceback (most recent call last):\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1187, in unwindGenerator\nreturn _inlineCallbacks(None, gen, Deferred())\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1045, in _inlineCallbacks\nresult = g.send(result)\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/scrapy/core/engine.py\", line 74, in start\nyield send_catch_log_deferred(signal=signals.engine_started)\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/scrapy/utils/signal.py\", line 53, in send_catch_log_deferred\n_arguments, *_named)\n--- ---\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\nresult = f(_args, *_kw)\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/scrapy/xlib/pydispatch/robustapply.py\", line 47, in robustApply\nreturn receiver(_arguments, *_named)\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/scrapy/contrib/memusage.py\", line 47, in engine_started\nstats.set_value('memusage/startup', self.get_virtual_size())\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/scrapy/contrib/memusage.py\", line 44, in get_virtual_size\nreturn get_vmvalue_from_procfs('VmSize')\nFile \"/home/data/virtualenv/scrapy/lib/python2.7/site-packages/scrapy/utils/memory.py\", line 26, in get_vmvalue_from_procfs\ni = v.index(vmkey + ':')\nexceptions.ValueError: substring not found\nOn FreeBSD, the format of /proc/[pid]/status is different to Linux.\nAnd no memory usage inside.\nref: http://www.unix.com/man-page/FreeBSD/5/procfs/", "issue_status": "Closed", "issue_reporting_time": "2012-08-04T16:10:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1586": {"issue_url": "https://github.com/scrapy/scrapy/issues/159", "issue_id": "#159", "issue_summary": "Error in the doc (page named \"Stats Collection\")", "issue_description": "mlorant commented on Jul 20, 2012\nHi,\nOn the \"Stats collection\", there's a little mistake, probably due to a copy/paste. In the part \"Common Stats Collector uses\" [ http://doc.scrapy.org/en/latest/topics/stats.html#common-stats-collector-uses ], the last method shown is stats.get_stats(spider=None). But the example given is this one :\n>>> stats.get_stats('pages_crawled', spider=some_spider) {'pages_crawled': 1238, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}\nThe first argument 'pages_crawled' shouldn't be here, the method is just stats.get_stats(spider=some_spider) ;)", "issue_status": "Closed", "issue_reporting_time": "2012-07-20T07:54:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1587": {"issue_url": "https://github.com/scrapy/scrapy/issues/158", "issue_id": "#158", "issue_summary": "Spider contracts (SEP-017)", "issue_description": "Member\npablohoffman commented on Jul 20, 2012\nThis ticket for adding the functionality of spider contracts (based on SEP-017) to test spiders.", "issue_status": "Closed", "issue_reporting_time": "2012-07-19T23:07:40Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1588": {"issue_url": "https://github.com/scrapy/scrapy/issues/156", "issue_id": "#156", "issue_summary": "scrapyd: job timeout", "issue_description": "alexw23 commented on Jul 9, 2012\nJust a small issue I've been having with scrapyd. Four of my jobs encountered MySQL errors and then grinded to a halt. Wondering how to handle this? The jobs have been running for over 2 days and therefore bottlenecked the queue.\nIs it possible to set a timeout variable for jobs? Or alternatively, how would I exit the crawler on MySQL error (from the PipeLine)", "issue_status": "Closed", "issue_reporting_time": "2012-07-09T08:21:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1589": {"issue_url": "https://github.com/scrapy/scrapy/issues/153", "issue_id": "#153", "issue_summary": "Can't modify AWS headers in settings.py", "issue_description": "alexw23 commented on Jul 3, 2012\nLooking to edit the headers configuration in images.py for amazon s3. Hard coded :-(", "issue_status": "Closed", "issue_reporting_time": "2012-07-03T11:48:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1590": {"issue_url": "https://github.com/scrapy/scrapy/issues/150", "issue_id": "#150", "issue_summary": "Telnet help command gives broken link to documentation", "issue_description": "JoshRosen commented on Jun 24, 2012\nCurrently, typing help in the telnet console prints the message\nThis is Scrapy telnet console. For more info see: http://doc.scrapy.org/topics/telnetconsole.html\nbut this link is broken. I believe the correct link is http://doc.scrapy.org/en/latest/topics/telnetconsole.html", "issue_status": "Closed", "issue_reporting_time": "2012-06-23T23:18:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1591": {"issue_url": "https://github.com/scrapy/scrapy/issues/144", "issue_id": "#144", "issue_summary": "Do not use urlparse.uses_query when it does not exist", "issue_description": "Arfrever commented on Jun 14, 2012\nuses_query list in urlparse module has been removed in 2.7 branch of Python:\nhttp://hg.python.org/cpython/rev/79e6ff3d9afd\nscrapy/xlib/urlparse_monkeypatches.py should be modified to not use uses_query when it does not exist.\n$ python2.7 setup.py build\nTraceback (most recent call last):\n  File \"setup.py\", line 87, in <module>\n    version = __import__('scrapy').__version__\n  File \"/tmp/scrapy/scrapy/__init__.py\", line 18, in <module>\n    from scrapy.xlib import urlparse_monkeypatches\n  File \"/tmp/scrapy/scrapy/xlib/urlparse_monkeypatches.py\", line 1, in <module>\n    from urlparse import urlparse, uses_netloc, uses_query\nImportError: cannot import name uses_query\nSuggested fix:\n--- scrapy/xlib/urlparse_monkeypatches.py\n+++ scrapy/xlib/urlparse_monkeypatches.py\n@@ -1,7 +1,13 @@\n-from urlparse import urlparse, uses_netloc, uses_query\n+from urlparse import urlparse, uses_netloc\n+\n+try:\n+    # Python <2.7.4\n+    from urlparse import uses_query\n+except ImportError:\n+    uses_query = None\n\n # workaround for http://bugs.python.org/issue7904\n if urlparse('s3://bucket/key').netloc != 'bucket':\n     uses_netloc.append('s3')\n-if urlparse('s3://bucket/key?key=value').query != 'key=value':\n+if uses_query is not None and urlparse('s3://bucket/key?key=value').query != 'key=value':\n     uses_query.append('s3')", "issue_status": "Closed", "issue_reporting_time": "2012-06-14T16:22:09Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1592": {"issue_url": "https://github.com/scrapy/scrapy/issues/142", "issue_id": "#142", "issue_summary": "RedirectMiddleware - sometimes not redirecting", "issue_description": "Contributor\nwarvariuc commented on Jun 12, 2012\nSome webpages (google, facebook) have meta refresh tag inside noscript:\n<noscript> <meta http-equiv=\"refresh\" content=\"0; URL=/homedepot?_fb_noscript=1\" /> </noscript>\nFix:\ndiff --git a/scrapy/utils/response.py b/scrapy/utils/response.py\nindex 85d0b60..03c353b 100644\n--- a/scrapy/utils/response.py\n+++ b/scrapy/utils/response.py\n@@ -41,7 +41,6 @@ def get_meta_refresh(response):\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.body_as_unicode()[0:4096]\n-        text = _noscript_re.sub(u'', text)\n         text = _script_re.sub(u'', text)\n         _metaref_cache[response] = html.get_meta_refresh(text, response.url, \\\n             response.encoding)", "issue_status": "Closed", "issue_reporting_time": "2012-06-12T17:12:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1593": {"issue_url": "https://github.com/scrapy/scrapy/issues/141", "issue_id": "#141", "issue_summary": "Spider not found - confusion", "issue_description": "Contributor\nwarvariuc commented on Jun 12, 2012\nif you put in settings.py something like import local_settings and local_settings doesn't exist or contains errors you get:\n/usr/local/lib/python2.7/dist-packages/Scrapy-0.15.1-py2.7.egg/scrapy/utils/project.py:17: UserWarning: Cannot import scrapy settings module settings\nwarnings.warn(\"Cannot import scrapy settings module %s\" % scrapy_module)\n...\nFile \"/usr/local/lib/python2.7/dist-packages/Scrapy-0.15.1-py2.7.egg/scrapy/cmdline.py\", line 117, in _run_command\ncmd.run(args, opts)\nFile \"/usr/local/lib/python2.7/dist-packages/Scrapy-0.15.1-py2.7.egg/scrapy/commands/crawl.py\", line 43, in run\nspider = self.crawler.spiders.create(spname, **opts.spargs)\nFile \"/usr/local/lib/python2.7/dist-packages/Scrapy-0.15.1-py2.7.egg/scrapy/spidermanager.py\", line 43, in create\nraise KeyError(\"Spider not found: %s\" % spider_name)\nKeyError: 'Spider not found: fb_spider'\nWhich is not very descriptive.", "issue_status": "Closed", "issue_reporting_time": "2012-06-12T16:46:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1594": {"issue_url": "https://github.com/scrapy/scrapy/issues/137", "issue_id": "#137", "issue_summary": "Endless loop with duplicate values in query string", "issue_description": "fcoelho commented on May 26, 2012\nPages which have duplicate values in their query string are treated as different pages:\nhttp://www.example.com/?q=\nhttp://www.example.com/?q=&q=\nhttp://www.example.com/?q=&q=&q=\n...\nIf the first page has a link to the second and the second a link to the third, scrapy enters an infinite loop, requesting each page in succession. I've put a simple page here to test the issue. Using a CrawlSpider with a rule like Rule(SgmlLinkExtractor(), callback='parse_page', follow=True) will cause this infinite recursion. Here is a sample spider:\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\nfrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n\nclass LoopingSpider(CrawlSpider):\n    name = \"loop\"\n    start_urls = [\"http://fcoelho.alwaysdata.net/scrapy/page.php\"]\n    allowed_domains = [\"fcoelho.alwaysdata.net\"]\n    rules = (\n        Rule(SgmlLinkExtractor(), callback='parse_page', follow=True),\n    )\n\n    def parse_page(self, response):\n        print \"Page: %s\" % response.url\n        return []\nIt seems like in the end the problem \"arises\" because scrapy.utils.url.canonicalize_url only sorts the query string keys, but don't remove duplicates. As far as I can tell, duplicates are \"wrong\", and shouldn't be counted as normal web page behavior. Would it be sane to remove arguments from the query string if their key is repeated?", "issue_status": "Closed", "issue_reporting_time": "2012-05-25T23:43:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1595": {"issue_url": "https://github.com/scrapy/scrapy/issues/136", "issue_id": "#136", "issue_summary": "stats.get_value('myvariable') returns None when myvariable is not updated", "issue_description": "tsouras commented on May 20, 2012\nLets say I create a statistics variable in a spider and then I use it in a pipeline.\nclass XXXSpider(BaseSpider):\n    name = \"xxx\"\n    allowed_domains = [\"http://xxx.com\"]\n\n    def start_requests(self):\n         stats.set_value('myvariable', 0, spider=self)\n         bla\n         bla\n\n\nclass MyPipeline(object):     \n    def close_spider(self, spider):\n        myvariable=stats.get_value('myvariable', spider=spider)\n        print myvariable\nIf \"myvariable\" is not changed at all then stats.get_value('myvariable', spider=spider) returns None.\nIf somewhere in the code I update myvariable using the function stats.inc_value('done_matches', spider=myspider) then stats.get_value('myvariable', spider=spider) returns a number correctly.\nMy system is the following:\nScrapy : 0.14.3\nTwisted : 11.0.0\nPython : 2.7.1 (r271:86832, Nov 27 2010, 18:30:46) [MSC v.1500 32 bit (Intel)]\nPlatform: Windows-XP-5.1.2600-SP3", "issue_status": "Closed", "issue_reporting_time": "2012-05-20T18:20:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1596": {"issue_url": "https://github.com/scrapy/scrapy/issues/133", "issue_id": "#133", "issue_summary": "segmentation fault occurs, when scrapy using HtmlXpathSelector parse HTML", "issue_description": "linxianping commented on May 16, 2012\nscrapy version -v result:\nScrapy : 0.14.3\nTwisted : 12.0.0\nPython : 2.6.8 (unknown, May 3 2012, 11:31:34) - [GCC 4.1.2 20071124 (Red Hat 4.1.2-42)]\nPlatform: Linux-2.6.18-92.el5-x86_64-with-redhat-5.2-Tikanga\nBug detail:\nThe 'segmentation fault' happens, and scrapy stop running. After checking the website url, I found it should belong to one tag with href attribute but no value. like this: \" dummy \" I suspect this is caused by lxml dynamic c lib, how to deal\nwith this? At least prevent the spider stop issue. My code snippet:\nhxs = HtmlXPathSelector(response)\nsites = hxs.select('//a')\nfor site in sites:\nlist_title= site.select('text()').extract()\nlist_link = site.select('./@href').extract() #????(crash here when parsing )\nBTW, The site scraped is 'http://mil.news.sohu.com/' . The issue also replicate under scrapy 0.15.1 and python 2.7.3,.\npre-install openssl0.12 because the openssl0.13 default for scrapy can't be installed sucessfully.\nI am urging waiting for your reply to decide if my project will use scrapy or not.\nI will appreciate if you can reply me ASAP.\nlinxianping", "issue_status": "Closed", "issue_reporting_time": "2012-05-16T08:22:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1597": {"issue_url": "https://github.com/scrapy/scrapy/issues/131", "issue_id": "#131", "issue_summary": "IgnoreRequest logged at ERROR level", "issue_description": "pilwon commented on May 8, 2012\nI'm filtering out some requests using IgnoreRequest in download middleware, but the ignored requests are logged at ERROR level as follows: \"ERROR: Error downloading...\" Is there a way to mute this or bring it down to at most DEBUG, or INFO?", "issue_status": "Closed", "issue_reporting_time": "2012-05-08T10:03:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1598": {"issue_url": "https://github.com/scrapy/scrapy/issues/126", "issue_id": "#126", "issue_summary": "Override request_fingerprint with meta field?", "issue_description": "jcushman commented on Apr 28, 2012\nWould it make sense to add something to the top of scrapy.utils.request.request_fingerprint like:\nif 'fingerprint' in request.meta:\n    return request.meta['fingerprint']\nThis is useful, for example, if you consider two pages to be identical if they share the same productID query parameter:\ndef parse(self, response):\n    ...\n    request = Request(url)\n    m = re.search(r'productID=([^&]+)', url)\n    if m:\n        request.meta['fingerprint'] = m.group(1)\nI'm currently handling this with a custom duplicate filter, but it seems like it would be broadly useful.", "issue_status": "Closed", "issue_reporting_time": "2012-04-28T01:26:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1599": {"issue_url": "https://github.com/scrapy/scrapy/issues/125", "issue_id": "#125", "issue_summary": "Support limiting the number of requests per interval", "issue_description": "honglei commented on Apr 27, 2012\nMany Web site's Open API limits the maximum number of requests in a certain interval from an IP address, Like 40 requests per minute.\nHow ever, the current arguments are CONCURRENT_REQUESTS,CONCURRENT_REQUESTS_PER_DOMAIN\nCONCURRENT_REQUESTS_PER_IP, and DOWNLOAD_DELAY. Which depend on the duration of completing requests, so I feel difficult to adjust according to the threshold in API.\nTo achieve high performance and don't exceed the threshold of API, I suggests adding arguments like MAX_REQUESTS_PER_MINUTE.\nThanks!", "issue_status": "Closed", "issue_reporting_time": "2012-04-27T09:37:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1600": {"issue_url": "https://github.com/scrapy/scrapy/issues/124", "issue_id": "#124", "issue_summary": "HtmlXpathSelector regex issue", "issue_description": "amir-f commented on Apr 27, 2012\nHere is a page I have got:\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\" />\n</head>\n<body>\n<p>12 is 12.0</p>\n<p>5.2 is not 5</p>\n</body>\n</html>\nI want to extract the numbers. Here is what I run and get:\nMy Scrapy is 0.14.2\n> scrapy shell http://people.tamu.edu/~afayazi/cr/main.html\n\n>>> hxs.select('//p/text()').re(r'\\d+(\\.\\d+)?')\n[u'', u'.0', u'.2', u'']\nI am expecting:\n[u'12', u'12.0', u'5.2', u'5']\nAm I missing something?", "issue_status": "Closed", "issue_reporting_time": "2012-04-27T08:04:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1601": {"issue_url": "https://github.com/scrapy/scrapy/issues/123", "issue_id": "#123", "issue_summary": "HtmlResponse doesn't detect the encoding correctly", "issue_description": "amir-f commented on Apr 25, 2012\nI'm new to Scrapy. I have this issue with the 0.14 version:\nThere is a MySpider(BaseSpider) with a start url which points to a simple HTML webpage stored in UTF-8.\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\" />\n</head>\n</html>\nThe server doesn't return encoding in its headers and inside the HtmlResponse._body_declared_encoding none of the regexes match.\nSo, when the code reaches my MySpider.parse method, I end up with '\\xef\\xbb\\xbf' (UTF-8 byte order mark) left at the beginning of the response.body with ascii being the inferred encoding of the response.", "issue_status": "Closed", "issue_reporting_time": "2012-04-25T04:59:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1602": {"issue_url": "https://github.com/scrapy/scrapy/issues/122", "issue_id": "#122", "issue_summary": "Unicode Error when logging dropped item", "issue_description": "domguard commented on Apr 21, 2012\nI have python objects in my scraped item that do have accents in their unicode representation,\nThrough the debugger, I can see that the text fields containing UTF-8 accents are correctly ascii-encoded but the object representation, like the category field here keeps its accents :\n{'category': <Category: Electro-m\u00e9nager :: TV>,\n 'description': u' Ecran monochrome\\r\\nFonction \\xe9co \\r\\nPr\\xe9sentation du num\\xe9ro \\r\\n1 combin\\xe9\\r\\nAutonomie : 10 h en communication / 100 h en veille\\r\\nSonneries : 10\\r\\nVerrouillage du clavier \\r\\nR\\xe9pertoire : 20 noms\\r\\nFonctionnalit\\xe9 main-libres\\r\\n',\n...\nWhen using a simple pipeline to drop them and raising the DropItem exception to do so, I get :\n2012-04-21 16:43:08+0200 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/Users/dom/django/.virtualenvs/cheze/lib/python2.7/site-packages/scrapy/utils/defer.py\", line 57, in <genexpr>\n        work = (callable(elem, *args, **named) for elem in iterable)\n      File \"/Users/dom/django/.virtualenvs/cheze/lib/python2.7/site-packages/scrapy/core/scraper.py\", line 173, in _process_spidermw_output\n        dfd.addBoth(self._itemproc_finished, output, response, spider)\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/defer.py\", line 320, in addBoth\n        callbackKeywords=kw, errbackKeywords=kw)\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/defer.py\", line 286, in addCallbacks\n        self._runCallbacks()\n    --- <exception caught here> ---\n      File \"/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/defer.py\", line 542, in _runCallbacks\n        current.result = callback(current.result, *args, **kw)\n      File \"/Users/dom/django/.virtualenvs/cheze/lib/python2.7/site-packages/scrapy/core/scraper.py\", line 199, in _itemproc_finished\n        log.msg(log.formatter.dropped(item, ex, response, spider), \\\n      File \"/Users/dom/django/.virtualenvs/cheze/lib/python2.7/site-packages/scrapy/logformatter.py\", line 22, in dropped\n        return u\"Dropped: %s%s%s\" % (exception, os.linesep, item)\n    exceptions.UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 33: ordinal not in range(128)\nThis is not coming from my exception message, its an ascii string\nI've been looking for long how to avoid this bug which clutters my log, but it's not really clear to me where I have to modifiy Scrapy.", "issue_status": "Closed", "issue_reporting_time": "2012-04-21T14:48:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1603": {"issue_url": "https://github.com/scrapy/scrapy/issues/120", "issue_id": "#120", "issue_summary": "comment mismatch in selector/__init__.py ?", "issue_description": "jwfang commented on Apr 19, 2012\ncomments says priority libxml2 > lxml,\nbut the code try lxml first.", "issue_status": "Closed", "issue_reporting_time": "2012-04-19T01:51:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1604": {"issue_url": "https://github.com/scrapy/scrapy/issues/118", "issue_id": "#118", "issue_summary": "Pypi tarball for Scrapy-0.14.2 miss .egg files used by test suite", "issue_description": "idella commented on Apr 18, 2012\nHaving discovered the bash script bin/runtests.sh, running it;\n[system python2.7]\narchtester scrapy # ebuild scrapy-0.14.2.ebuild clean test\n------------------------------------------------------------------------------------\n[FAIL]\n\nexceptions.AssertionError: '{\"spider:23c8050:name1\": \"1000.12\"}' != '{\"spider:23c8050:name1\": 1000.12}'\nscrapy.tests.test_utils_serialize.JsonEncoderTestCase.test_encode_decode\n------------------------------------------------------------------------------------------\n[ERROR]\n\nexceptions.IOError: [Errno 2] No such file or directory: '/mnt/gen2/TmpDir/portage/dev-python/scrapy-0.14.2/work/Scrapy-0.14.2/scrapyd/tests/mybot.egg'\nscrapyd.tests.test_utils.GetSpiderListTest.test_get_spider_list\n------------------------------------------------------------------------------------------\n[ERROR]\n\nexceptions.IOError: [Errno 2] No such file or directory: '/mnt/gen2/TmpDir/portage/dev-python/scrapy-0.14.2/work/Scrapy-0.14.2/scrapyd/tests/mybot.egg'\nscrapy.tests.test_utils_misc.UtilsMiscTestCase.test_walk_modules_egg\n-----------------------------------------------------------------------------------------\n\nRan 894 tests in 26.926s\n\nFAILED (skips=2, failures=1, errors=2, successes=889)\nCANNOT find facility to upload the logs, but you have them in the email.", "issue_status": "Closed", "issue_reporting_time": "2012-04-18T11:08:52Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1605": {"issue_url": "https://github.com/scrapy/scrapy/issues/112", "issue_id": "#112", "issue_summary": "Minor and insignificant typo in signal.py", "issue_description": "pabluk commented on Apr 11, 2012\nI was reading the code in https://github.com/scrapy/scrapy/blob/master/scrapy/utils/signal.py#L1 and i found a minor and insignificant typo in the first line:\n\"\"\"Helper functinos for working with signals\"\"\"\nmust be\n\"\"\"Helper functions for working with signals\"\"\"\nCheers and thanks for Scrapy!", "issue_status": "Closed", "issue_reporting_time": "2012-04-11T12:45:39Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1606": {"issue_url": "https://github.com/scrapy/scrapy/issues/110", "issue_id": "#110", "issue_summary": "DOWNLOAD_DELAY has no effect when scrap from a single domain SERIALLY", "issue_description": "jwfang commented on Apr 9, 2012\nFor example, when scraping Google's search result,\nin the spider code, return scrapped result as well as make_request_from_url with the \"Next Page\" link.\nThe Slot in Downloader will get \"created .. deleted .. created .. deleted ...\" in a loop, with lastseen always been 0.\nWhen lastseen is 0, delay = delay - now + 0 will be negative so the DELAY logic in _process_queue will be skipped.\nI think make Slot.lastseen initialized to time() in init could fix this case, but not quite sure if this change has other effects.", "issue_status": "Closed", "issue_reporting_time": "2012-04-09T03:18:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1607": {"issue_url": "https://github.com/scrapy/scrapy/issues/107", "issue_id": "#107", "issue_summary": "Tutorial needs an update", "issue_description": "johnmee commented on Apr 3, 2012\nI'm just trying to run through the tutorial and am thinking quite a few things have changed since it was last revised...\nFor example:\nit talks about creating a directory \"dmoz/spiders\" which contrasts with the comment in \"spiders/init.py\" suggesting we run \"scrapy genspider ...\".\nthe code in the tutorial subclasses \"BaseSpider\", yet the genspider creates almost identical but subclasses \"CrawlSpider\"\ndoing as instructed (creating dmoz/spiders/dmoz_spider.py and running scrapy crawl...) fails with \"Spider not found: dmoz\" but following the comments in the codebase works ok.\nThat area seems to be the only bit that has changed significantly.", "issue_status": "Closed", "issue_reporting_time": "2012-04-03T08:22:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1608": {"issue_url": "https://github.com/scrapy/scrapy/issues/105", "issue_id": "#105", "issue_summary": "Scheduler's duplicate filtering is too silent", "issue_description": "amckinlay commented on Mar 30, 2012\nIn version 14.2, if the duplicate filter should detect whether the URL its ignoring has been sent to a Spider previously. If not, it should not silently ignore the duplicate URL. A recent Request I did had a 302 redirect to an identical URL, before redirecting to a unique URL. It took half an hour before I realized that the scheduler was ignoring the first redirect. There were no debug indications or exceptions raised. I set dont_filter to True, which fixed the situation, as expected.\nScrapy should either raise a quiet exception of some sort or realize that a duplicate URL had not previously been sent to a spider instead of silently ignoring duplicates.\nP.S.: I don't know why the site sent me to a duplicate URL, but it had modified cookies in the response.", "issue_status": "Closed", "issue_reporting_time": "2012-03-30T05:19:34Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1609": {"issue_url": "https://github.com/scrapy/scrapy/issues/104", "issue_id": "#104", "issue_summary": "Docs should explain STATSMAILER_RCPTS", "issue_description": "matthewleon commented on Mar 29, 2012\nhttp://doc.scrapy.org/en/latest/topics/settings.html#std:setting-STATSMAILER_RCPTS\nThe page above does not explain how to fill this list in settings order to properly receive stats e-mails.", "issue_status": "Closed", "issue_reporting_time": "2012-03-29T15:18:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1610": {"issue_url": "https://github.com/scrapy/scrapy/issues/102", "issue_id": "#102", "issue_summary": "Suggestion: different pipeline processor for each item type", "issue_description": "Contributor\nScorpil commented on Mar 18, 2012\nI didn't find a good way to use different 'process_item' methods to different classes of Items.\nFor example we could create two item classes to crawl and then store in db:\nclass Headers(item):\n    url = Field()\n    response_code = Field()\n    content_type = Field()\n\nclass Body(item):\n    title = Field()\n    h1 = Field()\nThan in item pipeline we would need to do something like this:\nclass StoreInDB(object):\n\n    def process_item(self. item, spider):\n        if isinstance(item, Headers):\n            return self.storeHeaders(item, spider)\n        elif isinstance(item, Body):\n            return self.storeBody(item, spider)\n\n    def storeHeaders(item, spider):\n        pass # make some things with Headers item here\n\n    def storeBody(item, spider):\n        pass # make some things with Body item here\nWouldn't it be nice to put this functionality in base class, so we would have some dict or function to map item to the correct processor? Sure, current behavior would stay as default. Here is what i'm talking about:\nfrom project.items import Headers, Body\nfrom scrapy.contrib.pipeline import Pipeline\n\nclass StoreInDB(Pipeline):\n\n    def __init__(self):\n        self.assignItemProcessor(itemclass=Headers, processor=self.storeHeaders)\n        self.assignItemProcessor(itemclass=Body, processor=self.storeBody)\n\n    def storeHeaders(item, spider):\n        pass # make some things with Headers item here\n\n    def storeBody(item, spider):\n        pass # make some things with Body item here\nI can write the code if you guys think it's useful. It certainly is for me.\n\ud83d\udc4d 2", "issue_status": "Closed", "issue_reporting_time": "2012-03-18T11:21:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1611": {"issue_url": "https://github.com/scrapy/scrapy/issues/101", "issue_id": "#101", "issue_summary": "Suggestion: different pipeline processor for each item type", "issue_description": "Contributor\nScorpil commented on Mar 18, 2012\nI didn't find a good way to use different 'process_item' methods to different classes of Items.\nFor example we could create two item classes to crawl and then store in db:\nclass Headers(item):\n    url = Field()\n    response_code = Field()\n    content_type = Field()\n\nclass Body(item):\n    title = Field()\n    h1 = Field()\nThan in item pipeline we would need to do something like this:\nclass StoreInDB(object):\n\n    def process_item(self. item, spider):\n        if isinstance(item, Headers):\n            return self.storeHeaders(item, spider)\n        elif isinstance(item, Body):\n            return self.storeBody(item, spider)\n\n    def storeHeaders():\n        pass # make some things with Headers item here\n\n    def storeBody():\n        pass # make some things with Body item here\nWouldn't it be nice to put this functionality in base class, so we would have some dict or function to map item to the correct processor? Sure, current behavior would stay as default. Here is what i'm talking about:\nfrom project.items import Headers, Body\nfrom scrapy.contrib.pipeline import Pipeline\n\nclass StoreInDB(Pipeline):\n\n\n    self.assignItemProcessor(itemclass=Headers, processor=self.storeHeaders)\n    self.assignItemProcessor(itemclass=Body, processor=self.storeBody)\n\n    def storeHeaders():\n        pass # make some things with Headers item here\n\n    def storeBody():\n        pass # make some things with Body item here\nI can write the code if you guys think it's useful. It certainly is for me.", "issue_status": "Closed", "issue_reporting_time": "2012-03-18T11:21:36Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1612": {"issue_url": "https://github.com/scrapy/scrapy/issues/100", "issue_id": "#100", "issue_summary": "Error in scrapy shell: SQLite objects created in a thread can only be used in that same thread", "issue_description": "santagada commented on Mar 16, 2012\nI get this error everytime I exit the scrapy shell. I think it is most probably an ipython error but maybe scrapy is doing something strange with threads. This is only in IPython 0.12 the latest stable version (because I think it's the first with sqlite as a history backend).\nProgrammingError                          Traceback (most recent call last)\n/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/atexit.pyc in _run_exitfuncs()\n     22         func, targs, kargs = _exithandlers.pop()\n     23         try:\n---> 24             func(*targs, **kargs)\n     25         except SystemExit:\n     26             exc_info = sys.exc_info()\n\n/Library/Python/2.6/site-packages/IPython/core/interactiveshell.pyc in atexit_operations(self)\n   2732         # this must be *before* the tempfile cleanup, in case of temporary\n\n   2733         # history db\n\n-> 2734         self.history_manager.end_session()\n   2735 \n   2736         # Cleanup all tempfiles left around\n\n\n/Library/Python/2.6/site-packages/IPython/core/history.pyc in end_session(self)\n    428     def end_session(self):\n    429         \"\"\"Close the database session, filling in the end time and line count.\"\"\"\n--> 430         self.writeout_cache()\n    431         with self.db:\n    432             self.db.execute(\"\"\"UPDATE sessions SET end=?, num_cmds=? WHERE\n\n/Library/Python/2.6/site-packages/IPython/core/history.pyc in writeout_cache(self, conn)\n\n/Library/Python/2.6/site-packages/IPython/core/history.pyc in needs_sqlite(f, *a, **kw)\n     59         return []\n     60     else:\n---> 61         return f(*a,**kw)\n     62 \n     63 class HistoryAccessor(Configurable):\n\n/Library/Python/2.6/site-packages/IPython/core/history.pyc in writeout_cache(self, conn)\n    602         with self.db_input_cache_lock:\n    603             try:\n--> 604                 self._writeout_input_cache(conn)\n    605             except sqlite3.IntegrityError:\n    606                 self.new_session(conn)\n\n/Library/Python/2.6/site-packages/IPython/core/history.pyc in _writeout_input_cache(self, conn)\n    586             for line in self.db_input_cache:\n    587                 conn.execute(\"INSERT INTO history VALUES (?, ?, ?, ?)\",\n--> 588                                 (self.session_number,)+line)\n    589 \n    590     def _writeout_output_cache(self, conn):\n\nProgrammingError: SQLite objects created in a thread can only be used in that same thread.The object was created in thread id 4332220416 and this is thread id 140735088327872", "issue_status": "Closed", "issue_reporting_time": "2012-03-16T02:03:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1613": {"issue_url": "https://github.com/scrapy/scrapy/issues/92", "issue_id": "#92", "issue_summary": "Timeouts on S3ImageStore due to seek() being run on different threads", "issue_description": "ryross commented on Feb 24, 2012\nI'm on OS X 10.7.3 with python 2.7.2+, boto 2.2.2, and scrapy 0.14.1\nWhen I try to store an image on s3 with the images pipeline I get this S3 Response Error:\n\"Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\"\nOften, the first couple stores will create keys in the bucket, but most of the files have 0 bytes, or if they are not empty, they are garbled and not in a jpg format.\nI found that if I changed line 124 of the images pipeline to not use twisted threads, it works fine.\nFrom:\nreturn threads.deferToThread(k.set_contents_from_file, buf, \\ headers=self.HEADERS, policy=self.POLICY)\nTo:\nk.set_contents_from_file( buf, headers=self.HEADERS, policy=self.POLICY)\nI talked to @dangra in irc and he suggested I comment out lines 258 and 257 with the reasoning that \"I am afraid that seek(0) runs in main thread while persist_image does the same in another thread\"\nhttps://github.com/scrapy/scrapy/blob/0.14.1/scrapy/contrib/pipeline/images.py#L257", "issue_status": "Closed", "issue_reporting_time": "2012-02-23T19:31:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1614": {"issue_url": "https://github.com/scrapy/scrapy/issues/89", "issue_id": "#89", "issue_summary": "Support for gzipped sitemaps in SitemapSpider", "issue_description": "rosner commented on Feb 12, 2012\nSince a lot of sites use gzipped sitemaps defined in robots.txt it would be great if the SitemapSpider would support those when a robots.txt is given in the sitemap_url property.", "issue_status": "Closed", "issue_reporting_time": "2012-02-12T12:38:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1615": {"issue_url": "https://github.com/scrapy/scrapy/issues/87", "issue_id": "#87", "issue_summary": "struct.error on http compression middleware", "issue_description": "Member\npablohoffman commented on Feb 12, 2012\nThis issue is discussed more in this thread: https://groups.google.com/d/topic/scrapy-users/-RWh97fpq5I/discussion\nBy running:\nscrapy shell http://mail.ru/\nWe get:\n2012-02-11 20:22:56-0200 [scrapy] ERROR: Shell error\n    Traceback (most recent call last):\n      File \"/usr/lib/python2.7/threading.py\", line 525, in __bootstrap\n        self.__bootstrap_inner()\n      File \"/usr/lib/python2.7/threading.py\", line 552, in __bootstrap_inner\n        self.run()\n      File \"/usr/lib/python2.7/threading.py\", line 505, in run\n        self.__target(*self.__args, **self.__kwargs)\n    --- <exception caught here> ---\n      File \"/usr/lib/python2.7/dist-packages/twisted/python/threadpool.py\", line 207, in _worker\n        result = context.call(ctx, function, *args, **kwargs)\n      File \"/usr/lib/python2.7/dist-packages/twisted/python/context.py\", line 59, in callWithContext\n        return self.currentContext().callWithContext(ctx, func, *args, **kw)\n      File \"/usr/lib/python2.7/dist-packages/twisted/python/context.py\", line 37, in callWithContext\n        return func(*args,**kw)\n      File \"/home/prh/src/scrapy/scrapy/shell.py\", line 47, in _start\n        self.fetch(url, spider)\n      File \"/home/prh/src/scrapy/scrapy/shell.py\", line 88, in fetch\n        self._schedule, request, spider)\n      File \"/usr/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 118, in blockingCallFromThread\n        result.raiseException()\n      File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 542, in _runCallbacks\n        current.result = callback(current.result, *args, **kw)\n      File \"/home/prh/src/scrapy/scrapy/core/downloader/middleware.py\", line 46, in process_response\n        response = method(request=request, response=response, spider=spider)\n      File \"/home/prh/src/scrapy/scrapy/contrib/downloadermiddleware/httpcompression.py\", line 20, in process_response\n        decoded_body = self._decode(response.body, encoding.lower())\n      File \"/home/prh/src/scrapy/scrapy/contrib/downloadermiddleware/httpcompression.py\", line 36, in _decode\n        body = gunzip(body)\n      File \"/home/prh/src/scrapy/scrapy/utils/gz.py\", line 14, in gunzip\n        chunk = f.read(8196)\n      File \"/usr/lib/python2.7/gzip.py\", line 252, in read\n        self._read(readsize)\n      File \"/usr/lib/python2.7/gzip.py\", line 316, in _read\n        self._read_eof()\n      File \"/usr/lib/python2.7/gzip.py\", line 334, in _read_eof\n        crc32 = read32(self.fileobj)\n      File \"/usr/lib/python2.7/gzip.py\", line 25, in read32\n        return struct.unpack(\"<I\", input.read(4))[0]\n    struct.error: unpack requires a string argument of length 4", "issue_status": "Closed", "issue_reporting_time": "2012-02-11T22:25:43Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1616": {"issue_url": "https://github.com/scrapy/scrapy/issues/86", "issue_id": "#86", "issue_summary": "exceptions.AttributeError: 'dict' object has no attribute 'fields", "issue_description": "Contributor\ndfdeshom commented on Feb 3, 2012\nHi,\nwe were running into problems where we would get the following exception when crawling urls:\nTraceback (most recent call last):\n[...]\n\n    field = item.fields[field_name]\nexceptions.AttributeError: 'dict' object has no attribute 'fields'\nAfter some digging, we found that the exception was caused by the newly-introduced item exporters, where the default is to export the item as json. This exception makese more sense:\n ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.contrib.feedexport.FeedExporter object at 0x2a90cd0>>\nTraceback (most recent call last):\n[...]\nFile \"/usr/lib/python2.7/json/encoder.py\", line 264, in iterencode\n    return _iterencode(o, 0)\n  File \"/usr/lib/python2.7/json/encoder.py\", line 178, in default\n    raise TypeError(repr(o) + \" is not JSON serializable\")\nexceptions.TypeError: datetime.datetime(2012, 2, 2, 0, 0, tzinfo=<UTC>) is not JSON serializable\nThe item exporters extension scrapy.contrib.feedexport.FeedExporter should be either disabled by default or changed to a format that is guaranteed not to fail (like pickle), since not all items are json-serializable.", "issue_status": "Closed", "issue_reporting_time": "2012-02-03T06:13:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1617": {"issue_url": "https://github.com/scrapy/scrapy/issues/85", "issue_id": "#85", "issue_summary": "Outdated imports in documentation", "issue_description": "lefterav commented on Jan 30, 2012\nin page: http://readthedocs.org/docs/scrapy/en/0.14/topics/spiders.html\nfrom scrapy.contrib.spiders.crawl import CrawlSpider\nit should be\nfrom scrapy.contrib.spiders.crawl import CrawlSpider, Rule", "issue_status": "Closed", "issue_reporting_time": "2012-01-30T13:56:46Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1618": {"issue_url": "https://github.com/scrapy/scrapy/issues/84", "issue_id": "#84", "issue_summary": "exceptions.AssertionError: No free spider slots when opening 'default'", "issue_description": "ghost commented on Jan 30, 2012\nIn scrapy shell, when fetch() is run for the second time, it crashes with the following exception:\nexceptions.AssertionError: No free spider slots when opening 'default'\nTraceback:\n2012-01-29 23:45:01-0600 [-] ERROR: Unhandled error in Deferred:\n2012-01-29 23:45:01-0600 [-] Unhandled Error\nTraceback (most recent call last):\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 113, in _callFromThread\nresult = defer.maybeDeferred(f, _a, *_kw)\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 133, in maybeDeferred\nresult = f(_args, *_kw)\nFile \"/usr/lib/pymodules/python2.7/scrapy/shell.py\", line 64, in _schedule\nself.crawler.engine.open_spider(spider, close_if_idle=False)\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1141, in unwindGenerator\nreturn _inlineCallbacks(None, f(_args, *_kwargs), Deferred())\n--- ---\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 1020, in _inlineCallbacks\nresult = g.send(result)\nFile \"/usr/lib/pymodules/python2.7/scrapy/core/engine.py\", line 214, in open_spider\nspider.name\nexceptions.AssertionError: No free spider slots when opening 'default'\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/usr/lib/pymodules/python2.7/scrapy/shell.py\", line 80, in fetch\nself._schedule, request, spider)\nFile \"/usr/lib/python2.7/dist-packages/twisted/internet/threads.py\", line 118, in blockingCallFromThread\nresult.raiseException()\nFile \"/usr/lib/python2.7/dist-packages/twisted/python/failure.py\", line 338, in raiseException\nraise self.type, self.value, self.tb\nAssertionError: Spider 'default' not opened when crawling: <GET http://www.economist.com/blogs/freeexchange/2011/10/generational-warfare>", "issue_status": "Closed", "issue_reporting_time": "2012-01-30T05:50:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1619": {"issue_url": "https://github.com/scrapy/scrapy/issues/83", "issue_id": "#83", "issue_summary": "Scrapy hangs if an exception raises in start_requests", "issue_description": "Member\ndangra commented on Jan 28, 2012\nWhen start_requests iterator throws an exception, it makes engine._next_request fail with an UnhandledError and prevents scrapy from correctly stop the engine hanging forever\nIt is requried Ctrl-C to stop it.\n2012-01-27 17:10:09-0200 [scrapy] INFO: Scrapy 0.15.1 started (bot: testbot)\n2012-01-27 17:10:09-0200 [spidername.com] INFO: Spider opened\n2012-01-27 17:10:09-0200 [spidername.com] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2012-01-27 17:10:09-0200 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/home/daniel/src/scrapy/scrapy/commands/crawl.py\", line 45, in run\n        self.crawler.start()\n      File \"/home/daniel/src/scrapy/scrapy/crawler.py\", line 76, in start\n        reactor.run(installSignalHandlers=False) # blocking call\n      File \"/home/daniel/envs/mytestenv/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/home/daniel/envs/mytestenv/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/home/daniel/envs/mytestenv/local/lib/python2.7/site-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/home/daniel/src/scrapy/scrapy/utils/reactor.py\", line 41, in __call__\n        return self._func(*self._a, **self._kw)\n      File \"/home/daniel/src/scrapy/scrapy/core/engine.py\", line 108, in _next_request\n        request = slot.start_requests.next()\n      File \"/home/daniel/src/testbot/testbot/spiders_dev/myspider.py\", line 32, in start_requests\n        'spidername.com does not support url mapping'\n    exceptions.AssertionError: spidername.com does not support url mapping\n\n^C2012-01-27 17:10:11-0200 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force unclean shutdown\n2012-01-27 17:10:11-0200 [spidername.com] INFO: Closing spider (shutdown)\n2012-01-27 17:10:11-0200 [spidername.com] INFO: Dumping spider stats:\n    {'finish_reason': 'shutdown',\n     'finish_time': datetime.datetime(2012, 1, 27, 19, 10, 11, 757102),\n     'start_time': datetime.datetime(2012, 1, 27, 19, 10, 9, 487178)}\n2012-01-27 17:10:11-0200 [spidername.com] INFO: Spider closed (shutdown)\n2012-01-27 17:10:11-0200 [scrapy] INFO: Dumping global stats:\n    {'memusage/max': 111865856, 'memusage/startup': 111865856}", "issue_status": "Closed", "issue_reporting_time": "2012-01-27T19:14:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1620": {"issue_url": "https://github.com/scrapy/scrapy/issues/82", "issue_id": "#82", "issue_summary": "SSL compatibility issues with some servers", "issue_description": "johtso commented on Jan 26, 2012\nAttempting to fetch this page over HTTPS resulted in the not very enlightening error Connection was closed cleanly.\nERROR: Error downloading <GET https://rn.ftc.gov/pls/textilern/wrnquery$.startup>: Connection was closed cleanly.\nSome very helpful people on #twisted managed to work out that it was an issue with the server not liking empty fragments in the SSL communication. This could be fixed by specifying the OP_DONT_INSERT_EMPTY_FRAGMENTS option when making the OpenSSL context.\nCurrently it seems that in order to be able to scrape a website that has any SSL compatibility issues, you have to subclass ClientContextFactory in order to specify the compatibility options, and then subclass HTTPDownloadHandler and tell it to use that context factory.\nCould this be made easier? Would it even be a good idea to set some compatibility options by default? Maybe even SSL.OP_ALL?", "issue_status": "Closed", "issue_reporting_time": "2012-01-25T19:48:48Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1621": {"issue_url": "https://github.com/scrapy/scrapy/issues/76", "issue_id": "#76", "issue_summary": "Plugable Link Extractor backends", "issue_description": "Member\npablohoffman commented on Jan 5, 2012\nWe want to had pluggable link extractor backends, maybe having a LINKEXTRACTOR_CLASS setting.\nSome backends that come to mind: pure-regex, scrapely, libxml2, lxml, sgml\nThe sgml backend is not working very well, as there have been some issues reported about it:\nhttp://stackoverflow.com/questions/6443485/scrapy-parsing-issue-with-malformed-br-tags\nhttps://groups.google.com/d/topic/scrapy-users/E_-cbj7-Wyk/discussion\nThe scrapely backend workrks quite well and it's pure-python, so it's a good choice. But we'd have to add the dependency to scrapely.", "issue_status": "Closed", "issue_reporting_time": "2012-01-05T16:41:32Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1622": {"issue_url": "https://github.com/scrapy/scrapy/issues/73", "issue_id": "#73", "issue_summary": "scrapy shell raising an exception in ipython", "issue_description": "tknapen commented on Dec 29, 2011\nI'm running a standard scrapy shell command and receiving the following response, which occurs after it has finished parsing and enters into the shell\nscrapy shell http://doc.scrapy.org/_static/selectors-sample1.html\n2011-12-28 19:37:34+0100 [scrapy] INFO: Scrapy 0.12.0.2543 started (bot: botname)\n2011-12-28 19:37:34+0100 [scrapy] DEBUG: Enabled extensions: TelnetConsole, SpiderContext, WebService, CoreStats, CloseSpider\n2011-12-28 19:37:34+0100 [scrapy] DEBUG: Enabled scheduler middlewares: DuplicatesFilterMiddleware\n2011-12-28 19:37:34+0100 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, RedirectMiddleware, CookiesMiddleware, HttpCompressionMiddleware, DownloaderStats\n2011-12-28 19:37:34+0100 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n2011-12-28 19:37:35+0100 [scrapy] DEBUG: Enabled item pipelines: DuvelPipeline\n2011-12-28 19:37:35+0100 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n2011-12-28 19:37:35+0100 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n2011-12-28 19:37:35+0100 [default] INFO: Spider opened\n2011-12-28 19:37:35+0100 [default] DEBUG: Crawled (404) <GET http://doc.scrapy.org/_static/selectors-sample1.html> (referer: None)\n2011-12-28 19:37:35+0100 [default] INFO: Closing spider (finished)\n2011-12-28 19:37:35+0100 [default] INFO: Spider closed (finished)\n[s] Available Scrapy objects:\n[s] hxs <HtmlXPathSelector xpath=None data=u'<script type=\"text'>\n[s] item DuvelItem()\n[s] request <GET http://doc.scrapy.org/_static/selectors-sample1.html>\n[s] response <404 http://doc.scrapy.org/_static/selectors-sample1.html>\n[s] settings <CrawlerSettings module=<module 'spidertree.settings' from '/Volumes/HDD/home/duvel/spidertree/settings.pyc'>>\n[s] spider <BaseSpider 'default' at 0x10fcdda10>\n[s] Useful shortcuts:\n[s] shelp() Shell help (print this help)\n[s] fetch(req_or_url) Fetch request (or URL) and update local objects\n[s] view(response) View response in a browser\n2011-12-28 19:37:35+0100 [scrapy] ERROR: Shell error\nTraceback (most recent call last):\nFile \"/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/threading.py\", line 504, in __bootstrap\nself.__bootstrap_inner()\nFile \"/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/threading.py\", line 532, in __bootstrap_inner\nself.run()\nFile \"/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/threading.py\", line 484, in run\nself.__target(_self.__args, *_self.__kwargs)\n--- ---\nFile \"/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/twisted/python/threadpool.py\", line 207, in _worker\nresult = context.call(ctx, function, _args, *_kwargs)\nFile \"/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/twisted/python/context.py\", line 118, in callWithContext\nreturn self.currentContext().callWithContext(ctx, func, _args, *_kw)\nFile \"/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/twisted/python/context.py\", line 81, in callWithContext\nreturn func(args,*kw)\nFile \"/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/scrapy/shell.py\", line 56, in _start\nstart_python_console(self.vars)\nFile \"/opt/local/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packages/scrapy/utils/console.py\", line 14, in start_python_console\nshell = IPython.Shell.IPShellEmbed(argv=[], user_ns=namespace)\nexceptions.AttributeError: 'module' object has no attribute 'Shell'\nEvidently there's a problem with IPython's shell.\nI'm running version 0.12 of ipython and 0.12.0.2543 of scrapy", "issue_status": "Closed", "issue_reporting_time": "2011-12-28T18:47:26Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1623": {"issue_url": "https://github.com/scrapy/scrapy/issues/72", "issue_id": "#72", "issue_summary": "CrawlSpider shouldn't follow links from non-HTML responses", "issue_description": "Member\npablohoffman commented on Dec 28, 2011\nWhen CrawlSpider tries to extract links from a non-HTML response, it fails.\nCrawlSpider should only follow links from HTML responses.", "issue_status": "Closed", "issue_reporting_time": "2011-12-27T23:21:56Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1624": {"issue_url": "https://github.com/scrapy/scrapy/issues/71", "issue_id": "#71", "issue_summary": "joining (base, relative) urls give wrong urls in some cases!", "issue_description": "mdebbar commented on Dec 22, 2011\nI was trying to crawl a website, and they had a <a></a> with @href starts with a space!\nthe @href is an absolute url \" http://...\"\nbut because it starts with a blank, it will be considered as a relative url.", "issue_status": "Closed", "issue_reporting_time": "2011-12-21T18:30:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1625": {"issue_url": "https://github.com/scrapy/scrapy/issues/70", "issue_id": "#70", "issue_summary": "scrapy.item.Item - allow item.url along with item['url]", "issue_description": "Contributor\nwarvariuc commented on Dec 20, 2011\nscrapy.item.Item:\ndef __getattr__(self, name):\n    if name in self.fields:\n        raise AttributeError(\"Use item[%r] to get field value\" % name)\n    raise AttributeError(name)\n\ndef __setattr__(self, name, value):\n    if not name.startswith('_'):\n        raise AttributeError(\"Use item[%r] = %r to set field value\" % \\\n            (name, value))\n    super(DictItem, self).__setattr__(name, value)\nI suggest to allow using item.url=response.url along with item['url'] - this allows IDEs to autocomplete field names:\ndef __getattr__(self, name):\n    if name in self.fields:\n        return self.__getitem__(name)\n    raise AttributeError(name)\n\ndef __setattr__(self, name, value):\n    if not name.startswith('_'):\n        return self.__setitem__(name, value)\n    super(DictItem, self).__setattr__(name, value)", "issue_status": "Closed", "issue_reporting_time": "2011-12-20T11:22:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1626": {"issue_url": "https://github.com/scrapy/scrapy/issues/64", "issue_id": "#64", "issue_summary": "Allow misc.load_object() to take a reference to an actual object", "issue_description": "johtso commented on Dec 10, 2011\nIt's nice to have the flexibility of giving a reference to the actual object when setting settings, as well as specifying paths to objects.\nMaybe something as simple as:\nif not isinstance(path, basestring):\n    return path\n..at the beginning of misc.load_object(path)?", "issue_status": "Closed", "issue_reporting_time": "2011-12-10T14:31:19Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1627": {"issue_url": "https://github.com/scrapy/scrapy/issues/63", "issue_id": "#63", "issue_summary": "RegEx gets too big when allowed_domains long", "issue_description": "guybowden commented on Dec 7, 2011\nlines 49 and 50 of scrapy.contrib.spidermiddleware.offsite:\n    regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n    return re.compile(regex)\nTested with a list of 3000 allowed domains -\nCaught OverflowError while rendering: regular expression code size limit exceeded\nFixed by subclassing and using url_is_from_any_domain\nclass MyOffsiteMiddleware(OffsiteMiddleware):\n\ndef should_follow(self, request, spider):\n    allowed_domains = getattr(spider, 'allowed_domains', None)\n    return url_is_from_any_domain(request.url, allowed_domains)\n\ndef spider_opened(self, spider):\n    self.host_regexes[spider] = re.compile('')\n    self.domains_seen[spider] = set()", "issue_status": "Closed", "issue_reporting_time": "2011-12-07T00:45:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1628": {"issue_url": "https://github.com/scrapy/scrapy/issues/58", "issue_id": "#58", "issue_summary": "_disconnectedDeferred error using twisted 11.1.0", "issue_description": "Contributor\nrmax commented on Nov 29, 2011\nSeen with Scrapy 14.0, Twisted 11.1.0, Python 2.6/2.7. After downgrade to twisted 11.0.0 the error does not show up.\nIn my case, happened in a long running spider which doesn't perform anything unusual, and after the error the crawler hangs up.\nSee the log below:\n2011-11-23 09:32:40-0600 [projects] DEBUG: Crawled (200) <GET http://www.example.net/p/foo> (referer: http://www.example.net/p?page=51903&sort=users)\n2011-11-23 09:32:40-0600 [projects] DEBUG: Crawled (200) <GET http://www.example.net/p/bar> (referer: http://www.example.net/p?page=51903&sort=users)\n2011-11-23 09:38:10-0600 [projects] INFO: Crawled 89600 pages (at 42 pages/min), scraped 31075 items (at 16 items/min)\n2011-11-23 09:38:11-0600 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 45, in run\n        self.crawler.start()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/crawler.py\", line 76, in start\n        reactor.run(installSignalHandlers=False) # blocking call\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/tcp.py\", line 337, in failIfNotConnected\n        self.connector.connectionFailed(failure.Failure(err))\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1055, in connectionFailed\n        self.factory.clientConnectionFailed(self, reason)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/web/client.py\", line 413, in clientConnectionFailed\n        self._disconnectedDeferred.callback(None)\n    exceptions.AttributeError: ScrapyHTTPClientFactory instance has no attribute '_disconnectedDeferred'\n\n2011-11-23 09:38:13-0600 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 45, in run\n        self.crawler.start()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/crawler.py\", line 76, in start\n        reactor.run(installSignalHandlers=False) # blocking call\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/tcp.py\", line 337, in failIfNotConnected\n        self.connector.connectionFailed(failure.Failure(err))\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1055, in connectionFailed\n        self.factory.clientConnectionFailed(self, reason)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/web/client.py\", line 413, in clientConnectionFailed\n        self._disconnectedDeferred.callback(None)\n    exceptions.AttributeError: ScrapyHTTPClientFactory instance has no attribute '_disconnectedDeferred'\n\n2011-11-23 09:38:13-0600 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 45, in run\n        self.crawler.start()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/crawler.py\", line 76, in start\n        reactor.run(installSignalHandlers=False) # blocking call\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/tcp.py\", line 337, in failIfNotConnected\n        self.connector.connectionFailed(failure.Failure(err))\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1055, in connectionFailed\n        self.factory.clientConnectionFailed(self, reason)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/web/client.py\", line 413, in clientConnectionFailed\n        self._disconnectedDeferred.callback(None)\n    exceptions.AttributeError: ScrapyHTTPClientFactory instance has no attribute '_disconnectedDeferred'\n\n2011-11-23 09:38:13-0600 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 45, in run\n        self.crawler.start()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/crawler.py\", line 76, in start\n        reactor.run(installSignalHandlers=False) # blocking call\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/tcp.py\", line 337, in failIfNotConnected\n        self.connector.connectionFailed(failure.Failure(err))\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1055, in connectionFailed\n        self.factory.clientConnectionFailed(self, reason)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/web/client.py\", line 413, in clientConnectionFailed\n        self._disconnectedDeferred.callback(None)\n    exceptions.AttributeError: ScrapyHTTPClientFactory instance has no attribute '_disconnectedDeferred'\n\n2011-11-23 09:38:13-0600 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 45, in run\n        self.crawler.start()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/crawler.py\", line 76, in start\n        reactor.run(installSignalHandlers=False) # blocking call\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/tcp.py\", line 337, in failIfNotConnected\n        self.connector.connectionFailed(failure.Failure(err))\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1055, in connectionFailed\n        self.factory.clientConnectionFailed(self, reason)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/web/client.py\", line 413, in clientConnectionFailed\n        self._disconnectedDeferred.callback(None)\n    exceptions.AttributeError: ScrapyHTTPClientFactory instance has no attribute '_disconnectedDeferred'\n\n2011-11-23 09:38:13-0600 [-] Unhandled Error\n    Traceback (most recent call last):\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 45, in run\n        self.crawler.start()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/scrapy/crawler.py\", line 76, in start\n        reactor.run(installSignalHandlers=False) # blocking call\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1169, in run\n        self.mainLoop()\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1178, in mainLoop\n        self.runUntilCurrent()\n    --- <exception caught here> ---\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n        call.func(*call.args, **call.kw)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/tcp.py\", line 337, in failIfNotConnected\n        self.connector.connectionFailed(failure.Failure(err))\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/internet/base.py\", line 1055, in connectionFailed\n        self.factory.clientConnectionFailed(self, reason)\n      File \"/home/josemanuel/envs/global/lib/python2.7/site-packages/twisted/web/client.py\", line 413, in clientConnectionFailed\n        self._disconnectedDeferred.callback(None)\n    exceptions.AttributeError: ScrapyHTTPClientFactory instance has no attribute '_disconnectedDeferred'\n\n2011-11-23 09:38:58-0600 [projects] INFO: Crawled 89600 pages (at 0 pages/min), scraped 31075 items (at 0 items/min)\n2011-11-23 09:39:58-0600 [projects] INFO: Crawled 89600 pages (at 0 pages/min), scraped 31075 items (at 0 items/min)\n2011-11-23 09:40:58-0600 [projects] INFO: Crawled 89600 pages (at 0 pages/min), scraped 31075 items (at 0 items/min)", "issue_status": "Closed", "issue_reporting_time": "2011-11-29T01:03:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1629": {"issue_url": "https://github.com/scrapy/scrapy/issues/53", "issue_id": "#53", "issue_summary": "changed scrapyd data dir to be \".scrapy/scrapyd\" instead of \".scrapy/.scrapy/scrapyd\"", "issue_description": "Contributor\nwarvariuc commented on Nov 18, 2011\nchanged scrapyd data dir to be \".scrapy/scrapyd\" instead of \".scrapy/.scrapy/scrapyd\"\n--- a/scrapyd/script.py\n+++ b/scrapyd/script.py\n@@ -14,7 +14,7 @@ from scrapyd import get_application\nfrom scrapyd.config import Config\n\ndef _get_config():\n-    datadir = os.path.join(project_data_dir(), '.scrapy', 'scrapyd')\n+    datadir = os.path.join(project_data_dir(), 'scrapyd')\n    conf = {\n        'eggs_dir': os.path.join(datadir, 'eggs'),\n        'logs_dir': os.path.join(datadir, 'logs'),", "issue_status": "Closed", "issue_reporting_time": "2011-11-18T13:17:11Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1630": {"issue_url": "https://github.com/scrapy/scrapy/issues/52", "issue_id": "#52", "issue_summary": "Real support for returning iterators on parse() method", "issue_description": "Contributor\nwarvariuc commented on Nov 18, 2011\nNew features and settings:\n...\nReal support for returning iterators on start_requests() method. The iterator is now consumed during the crawl when the spider is getting idle (r2704)\n...\nThis works! Thanks!\nBut the issue with iterators on parse() methods still exists:\nclass AmazonSpider(BaseSpider):\n    name = 'amazon'\n    allowed_domains = ['amazon.com']\n    start_urls = ['http://www.amazon.com/dp/B005890G8Y/']\n\n    def parse(self, response):\n        print 'parse'\n        for i in xrange(100000000):\n            url = 'http://www.amazon.com/dp/%i/' % i\n            print i,\n            yield self.make_requests_from_url(url)\nThis causes memory consumption to grow very fast.\nAlso (i guess), because of this when scrapyd in 'scrapy server' is interrupted one of my 'scrapy crawl' subprocesses cannot stop for a long time:\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 102, model 978, years 2007-2007\n2011-11-18 14:14:32+0200 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force unclean shutdown\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 622, model 3404, years 2002-2002\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 108, model 1451, years 2003-2003\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 82, model 2293, years 2007-2007\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 42, model 805, years 2011-2011\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 25, model 3345, years 2011-2011\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 33, model 665, years 2011-2011\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 130, model 2139, years 2009-2009\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 92, model 1975, years 2009-2009\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 134, model 3453, years 2010-2010\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 21, model 375, years 2011-2011\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 8, model 720, years 2011-2011\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 95, model 644, years 2009-2009\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 72, model 2117, years 2007-2007\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 32, model 636, years 2011-2011\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 70, model 654, years 2011-2011\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 106, model 2843, years 2011-2011\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Found item link: http://www.carbusiness.it/527781/auto-nuova/MICROCAR_MC2.ashx\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Redirecting (302) to <GET http://www.carbusiness.it/ricerca/risultati.aspx?idM=106&idMM=&PDa=0&PA=0&idTCb=0&idTCz=0&CC=0&CDa=0&CA=0&idTC=0&EA=0&nr=50&ADa=2011&Aa=2011&KmDa=-1&KmA=-1&G=0> from <POST http://www.carbusiness.it/Default.aspx>\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Crawled (200) <GET http://www.carbusiness.it/ricerca/risultati.aspx?idM=82&idMM=&PDa=0&PA=0&idTCb=0&idTCz=0&CC=0&CDa=0&CA=0&idTC=0&EA=0&nr=50&ADa=2011&Aa=2011&KmDa=-1&KmA=-1&G=0> (referer: http://www.carbusiness.it/jp/jp.aspx?action=load_modelli&id_marca=82)\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Crawled (200) <GET http://www.carbusiness.it/ricerca/risultati.aspx?idM=25&idMM=&PDa=0&PA=0&idTCb=0&idTCz=0&CC=0&CDa=0&CA=0&idTC=0&EA=0&nr=50&ADa=2011&Aa=2011&KmDa=-1&KmA=-1&G=0> (referer: http://www.carbusiness.it/jp/jp.aspx?action=load_modelli&id_marca=25)\n2011-11-18 14:14:32+0200 [carbusiness_it] INFO: Closing spider (shutdown)\n2011-11-18 14:14:32+0200 [carbusiness_it] DEBUG: Requesting search: brand 131, model 2152, years 2010-2010\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 79, model 2164, years 2007-2007\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 102, model 2464, years 2007-2007\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 622, model 3403, years 2002-2002\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 108, model 2853, years 2002-2002\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 82, model 266, years 2007-2007\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 42, model 2432, years 2011-2011\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 25, model 413, years 2011-2011\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 33, model 658, years 2011-2011\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 130, model 2137, years 2009-2009\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 92, model 1977, years 2008-2008\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 134, model 3452, years 2010-2010\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 21, model 383, years 2011-2011\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 8, model 2404, years 2011-2011\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 95, model 3214, years 2009-2009\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 72, model 3477, years 2007-2007\n2011-11-18 14:14:33+0200 [carbusiness_it] DEBUG: Requesting search: brand 32, model 2388, years 2011-2011\nBecause in a parse method there is really big loop:\ndef parse2(self, response):\n    '''Request search my brands, models and year.'''\n\n    hxs = HtmlXPathSelector(response)\n    models = hxs.select(\"//option/@value\").extract()[1:]\n    random.shuffle(models)\n\n    brandId = response.meta['brand_id']\n    self.log('parse2, brandId=%s, models=%s' % (brandId, models))\n\n    searchPageResponse = response.meta['prev_response']\n    hxs = HtmlXPathSelector(searchPageResponse)\n    years = hxs.select(\"//*[@id='AnnoDa']/option/@value\").extract()[1:]\n    years = list(map(int, years))\n    years.sort(reverse= True) # be sure its sorted desc\n\n    for i in range(len(years) - 1): #\n        yearTo = years[i]\n        yearFrom = years[i + 1]\n        if yearFrom == yearTo - 1:\n            yearFrom = yearTo # search is inclusive. so do not search two consecutive years\n\n        for modelId in models:\n            formdata = {'ddlMarca': brandId, 'ddlModello': modelId,\n                        'AnnoDa': yearFrom, 'Annoa': yearTo, 'ddlRisultatiPerPagina': 50}\n            formRequest = FormRequest.from_response(searchPageResponse, 'ctl00', formdata= formdata, # we specify which submit button to click\n                                callback= self.parseBrand, clickdata= {'name': 'btnRicerca'}, priority= -i)\n            self.log('Requesting search: brand %s, model %s, years %d-%d' %\n                     (brandId, modelId, yearFrom, yearTo), log.DEBUG)\n            yield formRequest", "issue_status": "Closed", "issue_reporting_time": "2011-11-18T12:56:45Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1631": {"issue_url": "https://github.com/scrapy/scrapy/issues/49", "issue_id": "#49", "issue_summary": "exception handling", "issue_description": "Contributor\nwarvariuc commented on Nov 14, 2011\nscrapy/spidermanager.py\ndef create(self, spider_name, **spider_kwargs):\n    try:\n        return self._spiders[spider_name](**spider_kwargs)\n    except KeyError:\n        raise KeyError(\"Spider not found: %s\" % spider_name)\nchange to\ndef create(self, spider_name, **spider_kwargs):\n    try:\n        spcls = self._spiders[spider_name]\n    except KeyError:\n        raise KeyError(\"Spider not found: %s\" % spider_name)\n    return spcls(**spider_kwargs)\ni had a case when spider.__init__ had a KeyError causing a misleading exception - 'spider not found'", "issue_status": "Closed", "issue_reporting_time": "2011-11-14T14:08:58Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1632": {"issue_url": "https://github.com/scrapy/scrapy/issues/47", "issue_id": "#47", "issue_summary": "start_requests() and parse() should always be generators", "issue_description": "Contributor\nwarvariuc commented on Oct 27, 2011\nstart_requests() and parse() in BaseSpider are expected to return iterables. I suggest to modify this behaviour and force them to return generators. I start_requests wants to generate requests for url by a pattern it may eat a lot of memory:\nfrom scrapy.conf import settings\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.spider import BaseSpider\n\n\nclass TestSpider(BaseSpider):\n    name = \"test_spider\"\n    start_urls = ['http://www.amazon.com/dp/B005890G8Y/']\n\n    def parse(self, response):\n        print 'parse'\n        for i in xrange(100000000):\n            url = 'http://www.amazon.com/dp/%i/' % i\n            print i,\n            yield self.make_requests_from_url(url)\n\n\ncrawler = CrawlerProcess(settings)\ncrawler.install()\ncrawler.configure()\n\nspider = TestSpider()\ncrawler.queue.append_spider(spider)\ncrawler.start()\nor\nfrom scrapy.conf import settings\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.spider import BaseSpider\n\n\nclass TestSpider(BaseSpider):\n    name = \"test_spider\"\n    #start_urls = ['http://www.amazon.com/dp/B005890G8Y/']\n\n    def start_requests(self):\n        for i in xrange(100000000):\n            url = 'http://www.amazon.com/dp/%i/' % i\n            print 'yielding a start url: %s' % url\n            yield self.make_requests_from_url(url)\n\n    def parse(self, response):\n        '''does nothing'''\n        print 'parse' \n\n\ncrawler = CrawlerProcess(settings)\ncrawler.install()\ncrawler.configure()\n\nspider = TestSpider()\ncrawler.queue.append_spider(spider)\ncrawler.start()", "issue_status": "Closed", "issue_reporting_time": "2011-10-27T07:41:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1633": {"issue_url": "https://github.com/scrapy/scrapy/issues/43", "issue_id": "#43", "issue_summary": "scrapy shell raising an exception with iPython 0.11", "issue_description": "dchaplinsky commented on Oct 4, 2011\n2011-10-04 11:33:47+0200 [scrapy] ERROR: Shell error\n        Traceback (most recent call last):\n          File \"/usr/lib/python2.6/threading.py\", line 504, in __bootstrap\n            self.__bootstrap_inner()\n          File \"/usr/lib/python2.6/threading.py\", line 532, in __bootstrap_inner\n            self.run()\n          File \"/usr/lib/python2.6/threading.py\", line 484, in run\n            self.__target(*self.__args, **self.__kwargs)\n        --- <exception caught here> ---\n          File \"/usr/local/lib/python2.6/dist-packages/twisted/python/threadpool.py\", line 207, in _worker\n            result = context.call(ctx, function, *args, **kwargs)\n          File \"/usr/local/lib/python2.6/dist-packages/twisted/python/context.py\", line 59, in callWithContext\n            return self.currentContext().callWithContext(ctx, func, *args, **kw)\n          File \"/usr/local/lib/python2.6/dist-packages/twisted/python/context.py\", line 37, in callWithContext\n            return func(*args,**kw)\n          File \"/usr/local/lib/python2.6/dist-packages/scrapy/shell.py\", line 56, in _start\n            start_python_console(self.vars)\n          File \"/usr/local/lib/python2.6/dist-packages/scrapy/utils/console.py\", line 14, in start_python_console\n            shell = IPython.Shell.IPShellEmbed(argv=[], user_ns=namespace)\n        exceptions.AttributeError: 'module' object has no attribute 'Shell'\nHowever, works fine with 0.10.1", "issue_status": "Closed", "issue_reporting_time": "2011-10-04T09:36:51Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1634": {"issue_url": "https://github.com/scrapy/scrapy/issues/41", "issue_id": "#41", "issue_summary": "scrapy doesn't respect CLOSESPIDER_ITEMCOUNT", "issue_description": "kodemi commented on Sep 19, 2011\nscrapy crawl example --set CLOSESPIDER_ITEMCOUNT=1\n<...>\n2011-09-19 19:06:40+0400 [example] INFO: Dumping spider stats:\n        {'downloader/request_bytes': 337,\n         'downloader/request_count': 2,\n         'downloader/request_method_count/GET': 2,\n         'downloader/response_bytes': 715698,\n         'downloader/response_count': 2,\n         'downloader/response_status_count/200': 2,\n         'finish_reason': 'closespider_itemcount',\n         'finish_time': datetime.datetime(2011, 9, 19, 15, 6, 40, 574142),\n         'item_scraped_count': 892,\n         'scheduler/memory_enqueued': 2,\n         'start_time': datetime.datetime(2011, 9, 19, 15, 6, 38, 560994)}\n2011-09-19 19:06:40+0400 [example] INFO: Spider closed (closespider_itemcount)\n<...>\n'item_scraped_count': 892\n'INFO: Closing spider (closespider_itemcount)' appears in log after first (CLOSESPIDER_ITEMCOUNT value in example) scraped item.", "issue_status": "Closed", "issue_reporting_time": "2011-09-19T15:35:02Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1635": {"issue_url": "https://github.com/scrapy/scrapy/issues/37", "issue_id": "#37", "issue_summary": "Some pages are missing in the documentation pdf", "issue_description": "akompotis commented on Sep 12, 2011\nSome pages are blank(contents and part of chapter 1)", "issue_status": "Closed", "issue_reporting_time": "2011-09-12T16:26:37Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1636": {"issue_url": "https://github.com/scrapy/scrapy/issues/35", "issue_id": "#35", "issue_summary": "xml iternodes bug with nested nodes", "issue_description": "Member\npablohoffman commented on Sep 9, 2011\nOriginally reported by Damian Canabal on Trac: http://dev.scrapy.org/ticket/275\nIf itertag = 'product' then returned nodes will be incomplete:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<merchandiser xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"merchandiser.xsd\">\n  <product product_id=\"95\" name=\"Pure Shetland Wool Throw\" sku_number=\"10095\" manufacturer_name=\"Biome Lifestyle\" part_number=\"\">\n    <URL>\n      <product>http://click.linksynergy.com/fs-bin/click?id=bNgl5*KPhYY&amp;offerid=211619.95&amp;type=15&amp;subid=0</product>\n      <productImage>http://assets1.notonthehighstreet.com/system/product_images/images/000/000/368/normal_95_pure_shetland_wool_throw_main.jpg</productImage>\n    </URL>\n    <description>\nMade from the purest and softest Shetland wool, this throw remains undyedto stay as eco-friendly as possible. She\n    </description>\n  </product>\n</merchandiser>\nreturned node:\n<product product_id=\"95\" name=\"Pure Shetland Wool Throw\" sku_number=\"10095\" manufacturer_name=\"Biome Lifestyle\" part_number=\"\">\n    <URL>\n      <product>\nhttp://click.linksynergy.com/fs-bin/click?id=bNgl5*KPhYY&amp;offerid=211619.95&amp;type=15&amp;subid=0\n      </product>\n    </URL>\n</product>", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:43:53Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1637": {"issue_url": "https://github.com/scrapy/scrapy/issues/34", "issue_id": "#34", "issue_summary": "Crawl spider crawls previously visited URL on redirect", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nPreviously reported by michaelvmata on Trac http://dev.scrapy.org/ticket/299\nIf a crawl spider is redirected to an already visited page, it will still crawl it.\nFrom the mailing list http://groups.google.com/group/scrapy-users/browse_thread/thread/ee9ad68f5dbacc6d:\n\"...the dupe filter only catches requests after they leave the spider, so redirected pages are ignored by the dupe filter.\nSince the dupefilter and the redirect middleware components are decoupled now, it would be awkward to implement what you suggest, but nevertheless I think it would be useful\"", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:43:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1638": {"issue_url": "https://github.com/scrapy/scrapy/issues/33", "issue_id": "#33", "issue_summary": "Probe command", "issue_description": "Member\npablohoffman commented on Sep 9, 2011\nSometimes pages depend on certain HTTP request headers sent, for rendering the expected result, and it's a manual and tedious job to find out which headers those are.\nSo, here's an idea for automating this probing mechanism: create a new scrapy command probe which takes a url as argument and a text to look for.\nScrapy then tries several combinations of HTTP headers (user-agent, accept, etc) and return a set that works (where works mean that the text passed is found).\nHere's a real world example to illustrate:\nhttp://www.storage-cabinets-online.com/IVG2/N/ProductID-118021.htm\nThe page should contain a string 'var sFeatures', but that string is not returned with the default Scrapy HTTP request headers. So we run scrapy probe on it:\n$ scrapy probe http://www.storage-cabinets-online.com/IVG2/N/ProductID-118021.htm 'var sFeatures'\nFound set of working headers:\n{'Host': 'www.storage-cabinets-online.com', 'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.2.10) Gecko/20100915 Ubuntu/10.04 (lucid)\nFirefox/3.6.10', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Accept-Encoding': 'gzip,deflate',\n'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.7', 'Keep-Alive': '115'}\nThe Scrapy probe command would try a different list of well known user-agents, along with Accept headers.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:41:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1639": {"issue_url": "https://github.com/scrapy/scrapy/issues/32", "issue_id": "#32", "issue_summary": "inside_project does not work with SCRAPY_SETTINGS_MODULE", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nI previously reported this on Trac http://dev.scrapy.org/ticket/300\nThe Assertion error in scrapy/utils/project.py inside project_data_dir incorrectly fails when running with SCRAPY_SETTINGS_MODULE instead of a scrapy.cfg.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:41:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1640": {"issue_url": "https://github.com/scrapy/scrapy/issues/31", "issue_id": "#31", "issue_summary": "Make Request class configurable setting", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nPreviously reported by wecacuee on Trac http://dev.scrapy.org/ticket/301\nRationale\nCurrently we use scrapy.http.Request as the default class for Request throughout scrapy. This makes scrapy quite bound to HTTP protocol requests. I understand that we have download handlers for \"http\", \"ftp\" and \"s3\", but this enforces request differentiation only by \"URI\" scheme.\nWe should have a common protocol general request: scrapy.Request and the request class should be configurable by settings.\nDEFAULT_REQUEST_CLASS = 'scrapy.http.Request'", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:39:41Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1641": {"issue_url": "https://github.com/scrapy/scrapy/issues/30", "issue_id": "#30", "issue_summary": "Set the 'nofollow' attribute in Link objects", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nI previously reported this on Trac: http://dev.scrapy.org/ticket/310\nThe Link object now has a 'nofollow' attribute. This needs support in the link extractors", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:38:09Z", "fixed_by": "#190", "pull_request_summary": "Adding support to 'nofollow' in the SGML link extractors", "pull_request_description": "Contributor\npedrofaustino commented on Nov 10, 2012\nPlease let me know if coding style is correct. Added a test case as well. Should close #30.", "pull_request_status": "Merged", "issue_fixed_time": "2012-11-12T18:09:31Z", "files_changed": [["2", "scrapy/contrib/linkextractors/sgml.py"], ["11", "scrapy/tests/test_contrib_linkextractors.py"]]}, "1642": {"issue_url": "https://github.com/scrapy/scrapy/issues/29", "issue_id": "#29", "issue_summary": "scrapyd should allow multiple jobs to be scheduled with one URL", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nPreviously reported by agtilden on Trac\nscrapyd only allows one job to be scheduled per URL invocation. This makes scheduling lots of jobs needlessly time consuming.\nI propose adding a file upload option that would contain a json string with the following structure:\n[{\"project\" : {\"spider\": {\"spider_arg_name\": \"spider_arg_value\"}}},\n    {\"another_project\" : {\"another_spider\": {\"spider_arg_name\": \"spider_arg_value\"}}}]\nThe return value would be a json list of the same length. Elements in the list would be either the jobid assigned by scrapyd or null in case the scheduler encountered an exception.\nThis can be implemented so that the existing parameter passing continues to work for scheduling one job at a time.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:37:20Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1643": {"issue_url": "https://github.com/scrapy/scrapy/issues/28", "issue_id": "#28", "issue_summary": "SqlitePriorityQueue.pop() return None may crash Poller.poll()", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nPreviously reported by mrkschan on Trac http://dev.scrapy.org/ticket/313\nI have a scrapy project that has several spiders in it. Those spiders are scheduled to execute on an hourly-basis.\nThe scrapyd eventually report unhandled error (as shown below). When I dig through the source of scrapy, I suspect the concurrent control of sqlite3 access is not well guarded as the error below is caused by popping from an empty queue.\nThe case can simply be explained by the scenario with two spiders A and B (also refer to the source - http://is.gd/ht4HSs).\nSpider A's poller poll() get to line 21 of scrapyd/poller.py.\nMeanwhile, Spider B's poller also get to line 21.\nSpider A's poller poll() get to line 23, the queue in sqlite becomes empty and sqlite3 is locked (according to py doc - http://is.gd/67PKpn).\nSpider B's poller poll() also get to line 23, wait the lock to be released.\nSpider A commit and release the sqlite lock.\nSpider B pop() from an empty queue. Raise Error.\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/dist-packages/twisted/internet/base.py\", line 778, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"/usr/lib/python2.6/dist-packages/twisted/internet/task.py\", line 194, in __call__\n    d = defer.maybeDeferred(self.f, *self.a, **self.kw)\n  File \"/usr/lib/python2.6/dist-packages/twisted/internet/defer.py\", line 117, in maybeDeferred\n    result = f(*args, **kw)\n  File \"/usr/lib/python2.6/dist-packages/twisted/internet/defer.py\", line 944, in unwindGenerator\n    return _inlineCallbacks(None, f(*args, **kwargs), Deferred())\n--- <exception caught here> ---\n  File \"/usr/lib/python2.6/dist-packages/twisted/internet/defer.py\", line 823, in _inlineCallbacks\n    result = g.send(result)\n  File \"/usr/lib/pymodules/python2.6/scrapyd/poller.py\", line 24, in poll\n    returnValue(self.dq.put(self._message(msg, p)))\n  File \"/usr/lib/pymodules/python2.6/scrapyd/poller.py\", line 33, in _message\n    d = queue_msg.copy()\nexceptions.AttributeError: 'NoneType' object has no attribute 'copy'", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:35:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1644": {"issue_url": "https://github.com/scrapy/scrapy/issues/27", "issue_id": "#27", "issue_summary": "Add Proxy CONNECT support (fixes bug with https urls and proxies)", "issue_description": "Member\npablohoffman commented on Sep 9, 2011\nProxy CONNECT support is required for https urls to work.\nWhen proxy support was added to Scrapy, urllib still didn't supported CONNECT, but that has been fixed now:\nhttp://bugs.python.org/issue1424152\nNow we need to add support to Scrapy (which doesn't use urllib at all).\nThis link contains useful tips on how to add CONNECT support with Twisted:\nhttp://twistedmatrix.com/pipermail/twisted-web/2008-August/003878.html\nSee originally reported ticket on Trac for more comments: http://dev.scrapy.org/ticket/159", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:35:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1645": {"issue_url": "https://github.com/scrapy/scrapy/issues/26", "issue_id": "#26", "issue_summary": "SSL error 'sslv3 alert illegal parameter' is generated on certain URLs", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nI previously reported this issue on Trac: http://dev.scrapy.org/ticket/315\nFor example:\n$scrapy fetch \"https://ui2web1.apps.uillinois.edu/BANPROD1/bwskfcls.P_GetCrse\"\n...\n2011-03-24 10:58:03+0000 [default] ERROR: Error downloading <https://ui2web1.apps.uillinois.edu/BANPROD1/bwskfcls.P_GetCrse>: [Failure instance: Traceback (failure with no frames): <class 'OpenSSL.SSL.Error'>: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert illegal parameter')]\nThis issue is discussed here http://bugs.python.org/issue11220\nIt would be nice to be able to specify the SSL method and options on requests or use scrapy defaults, instead of those hardcoded in twisted.internet.ssl.ClientContextFactor\nAnother option might be to try SSLv3 when an error is encountered with SSLv23.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:34:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1646": {"issue_url": "https://github.com/scrapy/scrapy/issues/25", "issue_id": "#25", "issue_summary": "Add tests for Feed export extension", "issue_description": "Member\npablohoffman commented on Sep 9, 2011\nSome feed storages are tested, but we need to tests the main FeedExport extension too.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:34:00Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1647": {"issue_url": "https://github.com/scrapy/scrapy/issues/24", "issue_id": "#24", "issue_summary": "SgmlLinkExtractor mangles some url encoding (specifically \"/\" encoded)", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nReported by kurtjx on Trac http://dev.scrapy.org/ticket/316\nIn [10]: fetch('http://www.last.fm/music/AC%252FDC/+images')\nIn [11]: lx = SgmlLinkExtractor(restrict_xpaths=('//a[@class=\"nextlink\"]'))\n\nIn [12]: lx.extract_links(response)\nOut[12]: [<Link url='http://www.last.fm/music/AC%2FDC/+images?page=2' text=u'Next' >]\nthe found link gets mangled to have \"AC%2FDC\" but it should be \"AC%252FDC\"", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:31:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1648": {"issue_url": "https://github.com/scrapy/scrapy/issues/23", "issue_id": "#23", "issue_summary": "Failing test case for get_meta_refresh function", "issue_description": "Member\npablohoffman commented on Sep 9, 2011\nOriginally reported in Trac by Daniel: http://dev.scrapy.org/ticket/111\ndiff --git a/scrapy/tests/test_utils_response.py b/scrapy/tests/test_utils_response.py\n--- a/scrapy/tests/test_utils_response.py\n+++ b/scrapy/tests/test_utils_response.py\n@@ -62,11 +62,16 @@\n         response = Response(url='http://example.org', body=body)\n         self.assertEqual(get_meta_refresh(response), (1, 'http://example.org/newpage'))\n\n         # entities in the redirect url\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=&#39;http://www.example.com/other&#39;\">\"\"\"\n         response = Response(url='http://example.com', body=body)\n         self.assertEqual(get_meta_refresh(response), (3, 'http://www.example.com/other'))\n\n+        # entities in the redirect url with single quotes\n+        body = \"\"\"<meta http-equiv=\"refresh\" content='3; url=&#39;http://www.example.com/other&#39;'>\"\"\"\n+        response = Response(url='http://example.com', body=body)\n+        self.assertEqual(get_meta_refresh(response), (3, 'http://www.example.com/other'))\n+\n         # relative redirects\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=other.html\">\"\"\"\n         response = Response(url='http://example.com/page/this.html', body=body)", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:28:17Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1649": {"issue_url": "https://github.com/scrapy/scrapy/issues/22", "issue_id": "#22", "issue_summary": "Transfer-Encoding chunked hasn't been handled", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nReported by MichaelPeng on Trac http://dev.scrapy.org/ticket/322\nI tried scrapy 0.12 to download some site and got a gzip error. I found the error was due to 'chunked' transfer encoding hadn't been handled. Transfer encoding 'chunked' was part of http1.1 and it should be supported by spiders.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:28:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1650": {"issue_url": "https://github.com/scrapy/scrapy/issues/21", "issue_id": "#21", "issue_summary": "Defect in FormRequest constructor", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nReported by cdeyoung on Trac http://dev.scrapy.org/ticket/323\nIn the constructor of the FormRequest? class in scrapy/http/request.form.py -- in the IF statement that handles the \"formdata\" argument -- there is a line that says:\n    self.method = 'POST'\nThat line either shouldn't be there, falling back to the base class's method variable, or it should be handled differently if you want FormRequest? to default to POST rather that GET, as the Response base class does. Currently, you are hard-coding FormRequest? objects to be submitted via POST, and that isn't always valid. You should still allow the developer to specify method='GET' when using a FormRequest? object, I think.\nIf you want FormRequest? to default to submitting forms via POST, then I would recommend the following change, or something like it:\nclass FormRequest(Request):\n    __slots__ = ()\n    def __init__(self, *args, **kwargs):\n        formdata = kwargs.pop('formdata', None)\n        method = kwargs.pop('method', 'POST')\n\n        super(FormRequest, self).__init__(*args, **kwargs)\n\n        if formdata:\n            self.method = method\n            ...", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:26:25Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1651": {"issue_url": "https://github.com/scrapy/scrapy/issues/20", "issue_id": "#20", "issue_summary": "Cookies middleware is slow when crawling many domains from the same spider", "issue_description": "Member\ndangra commented on Sep 9, 2011\nWhen crawling many domains from the same spider, cookies middleware slows down.\nThe patch is attached.\nThe main difference is that it only checks for cookies belonging to relevant domains (the\npotential_domain_matches) instead of all domains, but to do this required a bit of refactoring. It also\nonly calls clear_expired_cookies periodically instead of every request.\nThe performance problems are very large and noticeable pretty quickly if a single spider has to manage\nmany cookies. This happened when we had a spider that crawled many sites with the cookies middleware\nenabled.\nIt would be great if you could review and even if we can maybe then try it on more websites.\nsee http://dev.scrapy.org/ticket/333 for more info", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:24:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1652": {"issue_url": "https://github.com/scrapy/scrapy/issues/19", "issue_id": "#19", "issue_summary": "Add dont_cache flag", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nReported by binarybug on Trac http://dev.scrapy.org/ticket/325\nIf a website is using a session to maintain the client's state than resuming a crawl doesn't work when cache is enabled. If we can instruct scrapy not to cache some requests than resuming a crawl would create a session when those requests are encountered and subsequent requests wouldn't fail e.g.\nyield Request(' http://www.example.com', meta={'dont_cache': True}) ", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:24:44Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1653": {"issue_url": "https://github.com/scrapy/scrapy/issues/18", "issue_id": "#18", "issue_summary": "Ignore meta-refresh tag embedded inside noscript tag", "issue_description": "Member\ndangra commented on Sep 9, 2011\nSome sites uses a meta-refresh redirect to detect browsers without javascript support, ie: http://www.surlatable.com/product/id/195440.do\n<NOSCRIPT>\n <META HTTP-EQUIV=\"refresh\" CONTENT=\"1; URL=/jsmessage.html\">\n</NOSCRIPT>\nsee http://dev.scrapy.org/ticket/185 for more info", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:22:55Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1654": {"issue_url": "https://github.com/scrapy/scrapy/issues/17", "issue_id": "#17", "issue_summary": "Documentation speaks of re.match while meaning re.search", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nReported by Vasily Alexeev on Trac http://dev.scrapy.org/ticket/328\nIn link extractor reference we see passages like\n\"allow (str or list) \u2013 a single regular expression (or list of regular expressions) that the (absolute) urls must match in order to be extracted. If not given (or empty), it will match all links.\"\nThere's two quite different methods for working with regexps: matching and searching. A quick look in sources reveals that in this case we deal with searching, not matching:\n_matches = lambda url, regexs: any((r.search(url) for r in regexs))\nSo documentation is clearly misleading and should be corrected.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:21:33Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1655": {"issue_url": "https://github.com/scrapy/scrapy/issues/16", "issue_id": "#16", "issue_summary": "Add Proxy CONNECT support", "issue_description": "Member\ndangra commented on Sep 9, 2011\nProxy CONNECT support is required for https urls to work.\nWhen proxy support was added to Scrapy, urllib still didn't supported CONNECT, but that has been fixed now:\nhttp://bugs.python.org/issue1424152\nNow we need to add support to Scrapy (which doesn't use urllib at all).\nThis link contains useful tips on how to add CONNECT support with Twisted:\nhttp://twistedmatrix.com/pipermail/twisted-web/2008-August/003878.html\nsee http://dev.scrapy.org/ticket/159 for more info", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:21:31Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1656": {"issue_url": "https://github.com/scrapy/scrapy/issues/15", "issue_id": "#15", "issue_summary": "Offsite middleware doesn't filter redirected responses", "issue_description": "Member\npablohoffman commented on Sep 9, 2011\nReported by fencer on Trac: http://dev.scrapy.org/ticket/100\nUsing a BaseSpider to harvest links. The spider evaluates every anchor link on the page, processes them and applies an algorithm to it. The spider's parse function returns both items for output and request with the harvested links for further crawling.\nExtra domains were not specified, only the domain_name value was set in the spider to \"agd.org\". In testing the spider, I noticed it was crawling URLs outside the domain_name.\nIn examining the log file, I noticed that there were 302 redirects from an URL inside the domain to an URL outside the domain. All domains crawled outside of the original domain_name correlated with a 302 redirect.\n2009-09-01 12:44:25-0700 [agd.org] DEBUG: Redirecting (302) to \n   <http://www.goarmy.com/amedd/dental/index.jsp?iom=9618-ITBP-MCDE-07012009-16-09021-180AD1> \n   from <http://www.agd.org/adtracking/a.aspx?ZoneID=18&Task=Click&Mode=HTML&SiteID=1&PageID=28659>\nI have not examined the SpiderMiddleware in detail, but I am guessing that the 302 redirect is somehow circumventing the\nscrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware\nNot sure if this is a bug or way it was intentionally designed when handling 302 redirects.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:19:18Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1657": {"issue_url": "https://github.com/scrapy/scrapy/issues/14", "issue_id": "#14", "issue_summary": "Support for binding interface to another ip", "issue_description": "Member\ndangra commented on Sep 9, 2011\nI have a suggestion for an improvement. I've added this to my local scrapy installation. Im sure it can be done more elegant, but its a start :)\nThe adress to bind to the socket needs to be passed to reactor.connectTCP in core.downloader.handlers.http._connect as bindAddress\nSee the diff file for an example.\nJust add ip_bind = (ip-number, port-number) to your spider if you want to override the default.\nsee http://dev.scrapy.org/ticket/153 for more info", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:18:07Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1658": {"issue_url": "https://github.com/scrapy/scrapy/issues/13", "issue_id": "#13", "issue_summary": "canonicalize_url() function breaks some urls", "issue_description": "Member\ndangra commented on Sep 9, 2011\nCurrent behavior of Scrapy when finding links like:\n/fclick.php?variable\nis to canonicalize them to:\n/fclick.php?variable=\nThis however makes Scrapy follow an incorrect link and cause an error page to load. This is really fault of web script programmers really who use variables without value. But for the sake of robustness Scrapy should follow the correct links.\nI made a small patch for this. All it does really is that when it faces variables with 0 length value it crops out the =.\nsee http://dev.scrapy.org/ticket/133 for more info", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:17:03Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1659": {"issue_url": "https://github.com/scrapy/scrapy/issues/12", "issue_id": "#12", "issue_summary": "Can't register namespaces in XMLFeedSpider when using 'iternodes' iterator", "issue_description": "Member\npablohoffman commented on Sep 9, 2011\nReported by manuelaristaran on Trac: http://dev.scrapy.org/ticket/98\nSince scrapy.utils.iterators.xmliter instances XmlXPathSelector only upon matching nodename via regexes, it can't register the namespaces.\nNot very important, of course :)", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:16:29Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1660": {"issue_url": "https://github.com/scrapy/scrapy/issues/11", "issue_id": "#11", "issue_summary": "Allow LinkExtractor.process_value to be a string", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nReported by Vasily Alexeev on Trac http://dev.scrapy.org/ticket/329\nIt is sometimes handy to pass LinkExtractor.process_value a string which then would map to a method of the spider object, but Scrapy unfortunately doesn't support it.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:16:28Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1661": {"issue_url": "https://github.com/scrapy/scrapy/issues/10", "issue_id": "#10", "issue_summary": "Add support for FTP downloads", "issue_description": "Member\ndangra commented on Sep 9, 2011\nWe should add support for following FTP links like:\nftp://www.example.com/somedir/somefile.xml\nI suppose Requests will only use the URL attribute (and perhaps some data in meta, if it's needed).\nAs for Responses, they will contain the file contents in the body, as one would expect.\nhere should be a flag to enable/disable passive FTP, perhaps even per spider.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:14:23Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1662": {"issue_url": "https://github.com/scrapy/scrapy/issues/9", "issue_id": "#9", "issue_summary": "Interrupted system call (OSError) on \"scrapy deploy\"", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nThe following problem happens when running \"scrapy deploy\" on MAC:\nDraixBook:ibcrest martin$ python2.6 $(which scrapy) deploy\nBuilding egg of 51-1311257580\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/bin/scrapy\", line 5, in <module>\n    pkg_resources.run_script('Scrapy==0.13.0', 'scrapy')\n  File \"/Library/Python/2.6/site-packages/setuptools-0.6c9-py2.6.egg/pkg_resources.py\", line 448, in run_script\n\n  File \"/Library/Python/2.6/site-packages/setuptools-0.6c9-py2.6.egg/pkg_resources.py\", line 1166, in run_script\n    script_code = compile(script_text,script_filename,'exec')\n  File \"/Library/Python/2.6/site-packages/Scrapy-0.13.0-py2.6.egg/EGG-INFO/scripts/scrapy\", line 4, in <module>\n    execute()\n  File \"/Library/Python/2.6/site-packages/Scrapy-0.13.0-py2.6.egg/scrapy/cmdline.py\", line 131, in execute\n    _run_print_help(parser, _run_command, cmd, args, opts)\n  File \"/Library/Python/2.6/site-packages/Scrapy-0.13.0-py2.6.egg/scrapy/cmdline.py\", line 97, in _run_print_help\n    func(*a, **kw)\n  File \"/Library/Python/2.6/site-packages/Scrapy-0.13.0-py2.6.egg/scrapy/cmdline.py\", line 138, in _run_command\n    cmd.run(args, opts)\n  File \"/Library/Python/2.6/site-packages/Scrapy-0.13.0-py2.6.egg/scrapy/commands/deploy.py\", line 98, in run\n    egg, tmpdir = _build_egg()\n  File \"/Library/Python/2.6/site-packages/Scrapy-0.13.0-py2.6.egg/scrapy/commands/deploy.py\", line 208, in _build_egg\n    check_call([sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/subprocess.py\", line 457, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/subprocess.py\", line 444, in call\n    return Popen(*popenargs, **kwargs).wait()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/subprocess.py\", line 1137, in wait\n    pid, sts = os.waitpid(self.pid, 0)\nOSError: [Errno 4] Interrupted system call\nThe problem was solved when upgrading to Python 2.7.2", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:13:54Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}, "1663": {"issue_url": "https://github.com/scrapy/scrapy/issues/7", "issue_id": "#7", "issue_summary": "Display stats in scrapyd web interface", "issue_description": "Member\nshaneaevans commented on Sep 9, 2011\nIt could be very useful to display the scrapy stats in scrapyd web interface.", "issue_status": "Closed", "issue_reporting_time": "2011-09-09T05:09:49Z", "fixed_by": "", "pull_request_summary": "", "pull_request_description": "", "pull_request_status": "", "issue_fixed_time": "", "files_changed": []}}}