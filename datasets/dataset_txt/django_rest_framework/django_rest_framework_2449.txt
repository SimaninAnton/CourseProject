ArTiSTiX commented on 8 Aug 2014
I have spent a lot of time trying to understand why DRF is using so much memory when I try to serialize a big amount of data (it's not even huge, only 24K records). Indeed, to serialize ~16MB of json data (24K objects * 700bytes), it takes far more than 1.5GB. Thus, my server swaps, the processes crashes because of a Memory Error.
Here are the things i checked:
I'm not using DEBUG=True
I tried to replace the queryset by an iterator() (in get_serializer(), I replace the QuerySet object by it's iterator to avoid allocation of all data).
I tried to monitor objects sizes: Same amount before and after HTTP Request (~30M) but during the process, there are many SortedDict, str and other objects, growing fastly during serialization.
I tried to remove all related fields, i succedded to serialize ~10K objects (but it takes more thant 700M)
I optimized my related queries with prefetch related. (6 queries are involved in process).
I tried to serialize as XML, or CSV: same issue.
So I'm now blocked with this issue, I need help to find its rootcause, i think it's not usual to have such a big process for a simple serialization, if it's DRF's fault, this must be fixed.
(NB: I'm using DRF 2.3.13, 2.3.14 has regression that blocks me).