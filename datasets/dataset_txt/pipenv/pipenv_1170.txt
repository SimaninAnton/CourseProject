LukeMathWalker commented on 20 Apr 2018 â€¢
edited
Good morning!
After having spent the last three or four days reviewing all possible tools to manage Python environments and dependencies I have finally come to pipenv: it seems to be in a good position to solve my problem but I have a final dilemma to solve.
We have three repos - a library and two applications.
The library is used by the first application to train a machine learning model, which is then serialized using joblib.
The second application is meant to serve the model behind a RESTful API - to do so it needs to be able to reproduce the exact environment used by the first application to train the model, otherwise it might fail to deserialize it or the resulting behaviour might differ.
The first (dirty) solution that I have come up with is:
merge the two applications in a single repository;
merge their Pipfiles;
install the merged Pipfile;
train+serialize the model;
commit the resulting Pipfile.lock to git;
fetch the commit with the Pipfile.lock used to train the model and install all dependencies from the frozen Pipfile.lock when I have to serve the model.
This is of course suboptimal: the two applications serve different purposes, and I am installing a lot of unnecessary dependencies at training time (when I don't need Flask, etc.).
Is there a way to let my second application depend on my first application in order to install everything as specified in first application Pipfile.lock plus the missing libraries required to do the serving job?