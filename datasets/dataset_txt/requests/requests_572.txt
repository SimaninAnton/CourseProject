le1nux commented on Jul 12, 2017
I'm crawling a lot of different URLs using the requests library and I encountered that the process takes more and more RAM over time.
Basically all I do is calling this iteratively from multiple threads:
r  = requests.get(url=url, timeout=timeout)
content = r.text
when I comment out the second line this issue does not occur ... am I using the library fundamentally wrong or could this actually be a bug?