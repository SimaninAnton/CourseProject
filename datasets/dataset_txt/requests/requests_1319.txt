davechallis commented on Aug 25, 2015
Using python 3.4 and requests 2.7.0 (on both Ubuntu 14.04 and Mac OS X), I've noticed that fetching raw content of large files is significantly slower (~5x) with requests than many other applications and python modules, performance as below is typical:
import requests                                                             
import urllib.request                                                       
import time                                                                 

# returns a 100Mb JSON file                                                                     
url = 'http://some.host.on.local.network/'

def with_urllib():                                                          
    with urllib.request.urlopen(url) as f:                                  
        return f.read()                                                     

def with_requests():                                                        
    return requests.get(url).content                                        

s = time.time()                                                             
a = with_urllib()                                                           
print('urllib:', time.time() - s, 'seconds')                                

s = time.time()                                                             
b = with_requests()                                                         
print('requests:', time.time() - s, 'seconds')                              

assert a == b
This outputs:
urllib: 1.113746166229248 seconds
requests: 5.458410978317261 seconds
curl, wget, pycurl and urllib all complete in similar times (~1.1s on average), requests consistently takes ~5.4s on average, even with various combinations of parameters I've tried (e.g. with streaming, with different streaming chunksizes, etc.).