laurento commented on Nov 14, 2014
Debugging a nasty "300: Bad request" error I noticed an invalid behaviour when requests-2.4 is used in combination with HTTP/1.0.
Tring to reproduce the issue this is what I found...
import httplib
import requests

# Workaround for the IncompleteRead issue.
# See http://bugs.python.org/issue14044
httplib.HTTPConnection._http_vsn = 10
httplib.HTTPConnection._http_vsn_str = 'HTTP/1.0'

r = requests.get('https://www.google.com')
r.request.headers
On requests-2.4.3 (from PyPi) the request headers are
{'Connection': 'keep-alive',
'Accept-Encoding': 'gzip, deflate',
'Accept': '*/*',
'User-Agent': 'python-requests/2.4.3 CPython/2.7.8 Linux/3.16.0-24-generic'}
On requests-2.3.0 (Ubuntu 14.10 official repos) the request headers are (notice no keep-alive here)
{'Accept-Encoding': 'gzip, deflate',
'Accept': '*/*',
'User-Agent': 'python-requests/2.3.0 CPython/2.7.8 Linux/3.16.0-24-generic'}
Reading the documentation I guess HTTP/1.0 is not supported by the requests library but I'd like to suggest at least a nice WARN log message to inform the result can be weird.
Some setups don't seem to like the invalid combination "keep-alive" in the headers and HTTP/1.0 and even if in most cases everything is working fine at least once I received a "300: Bad request" error.