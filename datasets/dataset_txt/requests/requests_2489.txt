jessicald commented on May 8, 2012
requests' method for detecting a document's encoding when the Content-Type header does not have it seems to work differently than external methods.
Referencing #480, chardet is used to detect the encoding of a document and presumably the string.decode() for converting a response.content to response.text uses this guess, again if the encoding is not in the HTTP headers. However, testing reveals that the guess provided by requests is subpar compared to a standalone call to chardet.detect().
Using this code:
import requests
import urllib
import chardet

page_requests = requests.get('http://www2.ocn.ne.jp/~cast/')
page_urllib = urllib.FancyURLopener().open('http://www2.ocn.ne.jp/~cast/').read()

print page_requests.encoding
print chardet.detect(page_urllib)['encoding']
produces as output:
$ python2 encoding_test.py
ISO-8859-1
SHIFT_JIS
This becomes a problem when relying on response.text for input, as improperly decoded Python Unicode strings break a lot of other modules. The present solution would be to rely on response.content and do the response.content.decode(chardet.detect(response.content)['encoding']) manually, but if requests is to have a pre-decoded Unicode string and an encoding field, the detection should be as accurate as possible.