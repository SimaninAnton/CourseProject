MikeVlad6 commented on Oct 13, 2017 â€¢
edited
I'm trying to get 4 Stack-overflow pages simultaneously with Requests and multiprocessing.
Actual Result
Instead of the processes finishing, i get 4 zombie processes. I really tried 100 things, i read everything i could find(or understand) including the documentation, especially here. I frankly don't know if this is a matter of requests or of multiprocessing but...
Reproduction Steps
import multiprocessing as mp
import requests
from bs4 import BeautifulSoup
from random import randint
import threading
import sys   
sys.setrecursionlimit(10000)




class Spider(object):
  
 # define a example function
 def rand_string(self,length, output):
  s = requests.session()
  #s.keep_alive=False
  random_post=randint(1000000,9999999)
  response=s.get('https://stackoverflow.com/questions/'+str(random_post))
  soup=BeautifulSoup(response.content,'lxml')
  try:
   title=soup.find('a',{'class':'question-hyperlink'}).string
  except:
   title="not found"

  output.put(title)
 
 # Setup a list of processes that we want to run
 def run(self,output):

  processes = [mp.Process(target=self.rand_string, args=(x, output)) for x in range(3)]

  for p in processes:
   p.start()
 
  # Exit the completed processes
  for p in processes:
   p.join()
   p.terminate()
 
  # Get process results from the output queue
  results = [output.get() for p in processes]
 
  print(results)
 
 # Run processes

if __name__ == '__main__':

 output = mp.Queue()

 spider=Spider()
 spider.run(output)


   