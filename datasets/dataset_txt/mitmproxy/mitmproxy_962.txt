Contributor
Schamper commented on 1 Sep 2016
Steps to reproduce the problem:
Start mitmdump/own proxy similar to this example. Note that in the "own proxy" the flow was also deleted, similar to mitmdump.
Start browsing (a lot).
Observe memory usage growing steadily with each request, reaching multiple GBs within a couple minutes, never coming back down.
What is the expected behavior?
Memory usage should ideally stay relatively the same over time.
What went wrong?
Memory is being held hostage somewhere.
Any other comments? What have you tried so far?
I'm also using a modified version of the har_dump script, but it doesn't matter if I have it enabled or disabled, suggesting the issue is somewhere in mitmproxy.
Tried inspecting heap memory using guppy, but I've never really used it so I didn't get very far with it. Printing the summary using guppy/other similar memory inspection libraries show that an insanely high number of dict and str objects are kept in memory, which seem to never stop growing.
I've tried digging into the source of mitmproxy but I'm not too familiar with it so I'm not able to pinpoint where the issue might be.
Mitmproxy Version: master (0.18)
Operating System: Ubuntu 16.04