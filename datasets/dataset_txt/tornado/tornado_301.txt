zpoint commented on 4 Aug 2017 â€¢
edited
My handler file
# -*- coding:utf-8 -*-
import sys
from tornado import gen, web, httpclient

url = "https://mdetail.tmall.com/templates/pages/desc?id=527485572414"

class SearchHandler(web.RequestHandler):
    @gen.coroutine
    def get(self):
        async_client = httpclient.AsyncHTTPClient()
        print sys.getrefcount(async_client) # The first time less than 10, then always bigger than 200
        req = httpclient.HTTPRequest(url, "GET", headers=headers)
        req_lists = [async_client.fetch(req) for _ in range(200)]
        r = yield req_lists
        print sys.getrefcount(async_client) # always bigger than 200
        # The longer req_lists, the more memory will be consumed, and never decrease
configure file
tornado.httpclient.AsyncHTTPClient.configure(client, max_clients=1000)
if my client is "tornado.curl_httpclient.CurlAsyncHTTPClient", Then when I visit my handler in broswer, htop shows memory increase about 6GB,as long as the process running, memory usage never decrease
If I set range(200) to range(500) or higher, Memory usage grows higher
if my cline is None, memory barely increase
I found only fetch the https:// will have memory issue
How can I slove the momory problem with CurlAsyncHTTPClient?
Environment:
Ubuntu 16.10 x64
python2.7.12
Tornado 4.5.1