bindlock commented on 13 Sep 2016 â€¢
edited
Hello. I have a problem with web-crawler written on Tornado. It's simple version.
Ubuntu 14.04 (16.04)
Python 3.5.2
Tornado 4.4.1
Test links: https://gist.github.com/pyvim/ba9abc4ff0615a345b1c71e9d23735ea
Code
#!/usr/bin/env python
# coding=utf-8

import time

import tornado.gen
import tornado.web
import tornado.ioloop
from tornado.queues import Queue
import tornado.httpclient


async def main():
    start = time.time()
    q = Queue()
    client = tornado.httpclient.AsyncHTTPClient()
    results = {'results': []}

    @tornado.gen.coroutine
    def worker():
        while True:
            yield consumer()

    async def consumer():
        async for url in q:
            try:
                response = await client.fetch(url, raise_error=False)

                if response.body:
                    results['results'].append(response.body.decode('utf-8'))
            finally:
                q.task_done()


    async def producer():
        with open('urls') as f:
            for link in f:
                await q.put(link.strip())

    for __ in range(10):
        worker()

    await producer()
    await q.join()

    print('Done in {} seconds'.format(time.time() - start))

    del results  # Release memory (without memory up on 20MB per request)


class MainHandler(tornado.web.RequestHandler):

    async def get(self):
        await main()


if __name__ == '__main__':
    app = tornado.web.Application(
        (tornado.web.url(r'/', MainHandler),), debug=True
    )
    app.listen(8000)

    tornado.ioloop.IOLoop.current().start()
Top statistics
Before first call:
First call:
Second call: