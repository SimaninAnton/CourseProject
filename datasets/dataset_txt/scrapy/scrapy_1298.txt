yangmillstheory commented on Jan 30, 2015
I'm working on a scrapy project where a "rabbit client" and "crawl worker" work together to consume scrape requests from a queue. These requests have more configuration than a start_url - it could be something like url and a set of xpaths, or a domain-specific configuration, like site-specific product ID (from which we programmatically build the url) and optional identifiers like color, style, and size to further specify the item one wants to scrape.
I ended up rolling my own generic abstract base classes to support this. These classes expose an interface that provide for crawl configuration validation and deserialization.
I'm wondering if it would be desirable to have built-in support for more specific "crawl configurations" like this within the framework? If that's the case, I'd be more than happy to have a design discussion and hash out the details.
I'm looking for specifically for replies from project maintainers/contributors to see whether or not this is feasible - I don't want to waste anyone's time, including mine. If it is, I'd love to tackle it myself with some design guidance, to make sure I'm working with the framework instead of against it.