YanzhongSu commented on Dec 24, 2019
I just started using scrapy. Now I am trying to migrate some of the old crawlers (simple python requests) to scrapy.
One thing I am struggling with is figuring out a way to capture the status of scrappy spider. In the old crawl script, I have a decorator called task that would decorates the script start function. The code snippet is like this:
@task(task_name='tutorial',
      alert_name='tutorial')
def start():
    raw_data = download_data()
    data = parse(raw_data)
    push_to_db(data)

if if __name__ == "__main__":
    start() 
So this task decorator would send a success or failure message depending on the running status of the script.
Now I want to use this decorator for scrapy spider. I am struggling to find the right place to put this decorator so that it would capture the running status of the spider from request download all the way to pipeline.
I use this command to start the spider:
scrapy crawl spider_name
The original question was posted here on Stackoverflow