KrisB88 commented on Feb 22, 2016
I'm currently working on a project for a client that is a rather broad crawl, something around 200-300 urls. I've been tweaking this project to maximize the scrapy performance because my item pipelines really don't add much overhead if any to the performance. However, I ran into a problem with scrapy's built-in queue strategy.
The problem has been that the url queue becomes loaded with really long sequences of urls from a single domain. Because I'm trying to maintain some kind of politeness to these sites with a reasonable download rate, the performance suffers greatly because every url has some kind of download delay which essentially stalls the scraper. To fix this I implemented a simple queue that uses a heap which holds a PriorityQueue from queuelib to further filter urls by their domain. Domains are managed separately in this class with heapq which allows for choosing the next url based on the domain which has been called least recently. By doing so, I was able to increase the crawler's performance by over 500 percent using the exact same settings and only running a single instance of the spider. This is just a first implementation that can be improved with some custom extensions, but as a first run has shown fantastic improvements in both download rates and cpu utilization.
There was one issue I ran into when making this change to scrapy which is that the scheduler initializes its memory queue to a PriorityQueue. Therefore, I had to create a custom scheduler which only deviated from the built-in scheduler in its from_crawler. init, and open methods. Seems like a decent candidate for changes to the scheduler class and something I would be happy to do. Anyways, just my thoughts and I'd love to hear what everyone else thinks. I can share my code as well for anyone that wants to see the queue I'm using for this broad crawl.