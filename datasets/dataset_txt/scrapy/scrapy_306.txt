foomoto commented on Sep 4, 2018
I am running out of memory after crawling 60k pages on an 8GB server, so I am attempting the JOBSDIR settings to reduce memory consumption. If I stop scrapy and attempt to resume I get the below error: Please help.
2018-09-03 21:24:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.therosefactory.com/bloom-box> (referer: None)
Traceback (most recent call last):
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
yield next(it)
GeneratorExit
Exception ignored in: <generator object iter_errback at 0x109230a98>
RuntimeError: generator ignored GeneratorExit
Exception ignored in: <generator object GlobalSpider.parse_new at 0x108aff1a8>
RuntimeError: generator ignored GeneratorExit
Unhandled error in Deferred:
2018-09-03 21:24:06 [twisted] CRITICAL: Unhandled error in Deferred:
2018-09-03 21:24:06 [twisted] CRITICAL:
Traceback (most recent call last):
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/twisted/internet/task.py", line 517, in _oneWorkUnit
result = next(self._iterator)
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/utils/defer.py", line 63, in
work = (callable(elem, *args, **named) for elem in iterable)
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/scraper.py", line 183, in _process_spidermw_output
self.crawler.engine.crawl(request=output, spider=spider)
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/engine.py", line 210, in crawl
self.schedule(request, spider)
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/engine.py", line 216, in schedule
if not self.slot.scheduler.enqueue_request(request):
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/scheduler.py", line 57, in enqueue_request
dqok = self._dqpush(request)
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/scheduler.py", line 86, in _dqpush
self.dqs.push(reqd, -request.priority)
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/queuelib/pqueue.py", line 33, in push
self.queues[priority] = self.qfactory(priority)
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/scrapy/core/scheduler.py", line 114, in _newdq
return self.dqclass(join(self.dqdir, 'p%s' % priority))
File "/Users/mrfobilli/IdeaProjects/octopus-gatherer/venv1/lib/python3.6/site-packages/queuelib/queue.py", line 142, in init
self.size, = struct.unpack(self.SIZE_FORMAT, qsize)
struct.error: unpack requires a buffer of 4 bytes