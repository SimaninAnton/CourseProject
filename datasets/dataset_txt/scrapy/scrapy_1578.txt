Member
pablohoffman commented on Oct 3, 2012
See how scrapy list works but scrapy check -l fails in this case:
prh@prh-laptop:~/src/testspiders [17:42:25]
$ scrapy list
timed
dummy
noop
followall
mad
prh@prh-laptop:~/src/testspiders [17:43:39]
$ scrapy check -l
Traceback (most recent call last):
  File "/home/prh/src/scrapy/bin/scrapy", line 4, in <module>
    execute()
  File "/home/prh/src/scrapy/scrapy/cmdline.py", line 127, in execute
    _run_print_help(parser, _run_command, cmd, args, opts)
  File "/home/prh/src/scrapy/scrapy/cmdline.py", line 76, in _run_print_help
    func(*a, **kw)
  File "/home/prh/src/scrapy/scrapy/cmdline.py", line 134, in _run_command
    cmd.run(args, opts)
  File "/home/prh/src/scrapy/scrapy/commands/check.py", line 48, in run
    spider = self.crawler.spiders.create(spider)
  File "/home/prh/src/scrapy/scrapy/spidermanager.py", line 44, in create
    return spcls(**spider_kwargs)
  File "/home/prh/src/testspiders/testspiders/spiders/timed.py", line 14, in __init__
    super(TimedSpider, self).__init__(url)
TypeError: __init__() takes exactly 1 argument (2 given)
Regardless of weather the timed spider should require a constructor argument or not, why does scrapy check -l need to actually instantiate the spiders?. Should it be enough to inspect the classes like scrapy list does?