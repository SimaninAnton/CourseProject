flaiming commented on Nov 23, 2017 â€¢
edited
In RetryMiddleware docs there is line
Failed pages are collected on the scraping process and rescheduled at the end, once the spider has finished crawling all regular (non failed) pages.
But I don't see anything like it in code https://github.com/scrapy/scrapy/blob/master/scrapy/downloadermiddlewares/retry.py when there is for example TimeoutError exception raised more than max_retry_times. It just throws request away, which is not what docs are saying. Is it a bug or mistake in docs?