ivandir commented on Jun 22, 2013
Hi,
The "items_dir" setting when left empty and used in the default scrapyd server you get from OS repo exposes the whole server filesystem starting with /
I am storing my parsed data in a database and don't need and items feed. When I set "items_dir" to an empty value and start the scrapyd server clicking on the /items/ url in the webpage leads me to my servers root path. The scrapyd server I am using is the default one installed with Ubuntu precise32
(dev2)vagrant@precise32:/vagrant$ scrapy version -v
Scrapy : 0.16.3
lxml : 3.2.1.0
libxml2 : 2.7.8
Twisted : 12.2.0
Python : 2.7.3 (default, Aug 1 2012, 05:16:07) - [GCC 4.6.3]
Platform: Linux-3.2.0-23-generic-pae-i686-with-Ubuntu-12.04-precise
(dev2)vagrant@precise32:/vagrant$ apt-cache show scrapyd-0.16
Package: scrapyd-0.16
Source: scrapy-0.16
Version: 0.16.5+1369956345
Architecture: all
Maintainer: Scrapinghub Team info@scrapinghub.com
Installed-Size: 93
Depends: scrapy-0.16 (>= 0.16.5+1369956345), python-setuptools
Conflicts: scrapyd, scrapyd-0.11
Provides: scrapyd
Homepage: http://scrapy.org/
Priority: optional
Section: python
Filename: pool/main/s/scrapy-0.16/scrapyd-0.16_0.16.5+1369956345_all.deb
Size: 4016
SHA256: 9a361297122a7149a3e91d8262303d8a1d27878c5e189b8cb9e4b15c2703a20a
SHA1: 6e8f00c084d6c94957c3b546c710c2cd91719498
MD5sum: fd51653e8d4524d9d89a07931e5748e2
Description: Scrapy Service
The Scrapy service allows you to deploy your Scrapy projects by building
Python eggs of them and uploading them to the Scrapy service using a JSON API
that you can also use for scheduling spider runs. It supports multiple
projects also.
The only modification I have done to the actual startup script is the location of twisted so that it uses then one I have in my virtual environment.
exec ~/.myvirtualenv/bin/twistd -ny /usr/share/scrapyd/scrapyd.tac ..........
Here is the config file:
[scrapyd]
http_port = 6800
debug = off
max_proc = 0
max_proc_per_cpu = 4
eggs_dir = /var/lib/scrapyd/eggs
dbs_dir = /var/lib/scrapyd/dbs
items_dir =
logs_dir = /var/log/scrapyd
logs_to_keep = 5
runner = scrapyd.runner
application = scrapyd.app.application
[services]
schedule.json = scrapyd.webservice.Schedule
cancel.json = scrapyd.webservice.Cancel
addversion.json = scrapyd.webservice.AddVersion
listprojects.json = scrapyd.webservice.ListProjects
listversions.json = scrapyd.webservice.ListVersions
listspiders.json = scrapyd.webservice.ListSpiders
delproject.json = scrapyd.webservice.DeleteProject
delversion.json = scrapyd.webservice.DeleteVersion
listjobs.json = scrapyd.webservice.ListJobs
This is what my virtual environment contains (removing unecessary modules not related to discussion)
(dev2)vagrant@precise32:/vagrant$ pip freeze
Scrapy==0.16.3
Twisted==12.2.0
Thanks for your help.