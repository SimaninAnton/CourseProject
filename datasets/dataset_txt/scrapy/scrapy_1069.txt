raspatan commented on Dec 9, 2015
I'm starting with Scrapy in order to automatise file downloading from websites. As a test, I want to download the jpg files from this website. Perhaps I'm too noob for this, or perhaps the Wiki regarding downloading files is not very clear on the issue. In any case, here is the problem:
My code is this:
In settings.py, I have added these lines:
ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}

IMAGES_STORE = '/home/lucho/Scrapy/jpg/'
My items.py file is:
import scrapy

class JpgItem(scrapy.Item):
    image_urls = scrapy.Field()
    images = scrapy.Field()
    pass
My pipeline file is:
import scrapy
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem

class JpgPipeline(object):
#    def process_item(self, item, spider):
#        return item
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            yield scrapy.Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
        if not image_paths:
            raise DropItem("Item contains no images")
        item['image_paths'] = image_paths
        return item
Finally, my spider file is:
import scrapy

class JpgSpider(scrapy.Spider):
    name = "jpg"
    allowed_domains = ["http://www.kevinsmedia.com"]
    start_urls = [
        "http://www.kevinsmedia.com/km/mp3z/Fluke/Risotto/"
    ]

    def parse(self, response):
        yield JpgItem(
            file_urls=[
                'http://www.kevinsmedia.com/km/mp3z/Fluke/Risotto/AlbumArtSmall.jpg',
            ]
        )
(I added a def parse() to my spider, but this is restrictive, since I don't want to specify every file I need but to detect a class of files (.jpg) and download them all)
The output of "scrapy crawl jpg" is:
2015-12-07 10:29:39 [scrapy] INFO: Scrapy 1.0.3.post6+g2d688cd started (bot: jpg)
2015-12-07 10:29:39 [scrapy] INFO: Optional features available: ssl, http11
2015-12-07 10:29:39 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'jpg.spiders', 'SPIDER_MODULES': ['jpg.spiders'], 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3, 'BOT_NAME': 'jpg'}
2015-12-07 10:29:39 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-12-07 10:29:40 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-12-07 10:29:40 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-12-07 10:29:40 [scrapy] INFO: Enabled item pipelines: ImagesPipeline
2015-12-07 10:29:40 [scrapy] INFO: Spider opened
2015-12-07 10:29:40 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-12-07 10:29:40 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-12-07 10:29:40 [scrapy] DEBUG: Crawled (200) <GET http://www.kevinsmedia.com/km/mp3z/Fluke/Risotto/> (referer: None)
2015-12-07 10:29:40 [scrapy] ERROR: Spider error processing <GET http://www.kevinsmedia.com/km/mp3z/Fluke/Risotto/> (referer: None)
Traceback (most recent call last):
  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/pymodules/python2.7/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/usr/lib/pymodules/python2.7/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/pymodules/python2.7/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/pymodules/python2.7/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/lucho/Documents/Academia/Research/Database Chile/Code/Scrapy/jpg/jpg/spiders/jpg.py", line 12, in parse
    yield JpgItem(
NameError: global name 'JpgItem' is not defined
2015-12-07 10:29:41 [scrapy] INFO: Closing spider (finished)
2015-12-07 10:29:41 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 254,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2975,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 7, 13, 29, 41, 1049),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NameError': 1,
 'start_time': datetime.datetime(2015, 12, 7, 13, 29, 40, 106049)}
2015-12-07 10:29:41 [scrapy] INFO: Spider closed (finished)
So, there is an error, as indicated there, but google is not helping me on this. In case it matters, I'm using Ubuntu.
PS: I have also posted this on SE, here.