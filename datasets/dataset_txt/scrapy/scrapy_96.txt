Contributor
anubhavp28 commented on Aug 3, 2019
The current implementation of RobotsTxtMiddleware (link) uses USER_AGENT setting and User-Agent header of the request to determine the user agent to use for matching in robots.txt. There is no easy way for a user to send one user-agent header value but have a different user agent string evaluated to decide which pages to crawl. This use case seems to be more common that we thought previously for example - while Googlebot uses "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)" as User-Agent header of the request, it actually uses user agent "Googlebot" in robots.txt for deciding which pages to crawl. Here is the an article describing this. The same seems to be the case with Bing - link. Infact, Google made this distinction very clear in their proposed robots.txt specification where they described the user agent used in robots.txt as the "product token".
The proposed solutions are implementing a new ROBOTSTXT_USER_AGENT setting, implementing a robotstxt_user_agent meta key for overriding user agent per request or both. Here is a thread that has previous discussion on the topic - #3796 (comment).
I'm creating this issue as a place for discussion regarding whether to add this functionality into Scrapy? If yes, which route we should take?