teodorag commented on Oct 28, 2014
Hello,
I'm using this crawler as my base crowler https://github.com/alecxe/broken-links-checker/blob/master/broken_links_spider.py
It is created to catch 404 error domains and save them. I wanted to modify it a little bit and make it look for "No such host" error, which is error 12002.
However, with this code, Scrapy is not receiving any responce (because there isn't a host to return a responce) and when scrapy encounters such domains it returns
not found: [Errno 11001] getaddrinfo failed.
How can I catch this not found error and save the domains?