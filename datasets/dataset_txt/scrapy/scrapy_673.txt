milescrawford commented on Mar 10, 2017 â€¢
edited
If you have a polite DOWNLOAD_DELAY and CONCURRENT_REQUESTS_PER_DOMAIN, Scrapy will happily fill up all of its slots with requests going to a single domain, and essentially sit idle.
A crawl will last at least as long as it takes to crawl the most numerous site, but when waiting for a particular site, Scrapy ignores possible concurrent activity which prolongs the crawl a lot.
Scrapy has all the information required to re-queue requests going to a domain which is already waiting on delays or concurrency limits. Could it be extended to do this?