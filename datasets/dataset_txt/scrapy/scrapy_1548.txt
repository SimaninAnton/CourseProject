Contributor
stav commented on Mar 9, 2013
It seems the LOG_STDOUT command line setting does not work. Here is the spider I'm using to test:
from scrapy import log
from scrapy.spider import BaseSpider

class TestSider(BaseSpider):
    name = "logtest"
    start_urls = ["http://example.com/"]

    def parse(self, response):
        log.msg('LOG: This is a log message')
        print 'PRINT: This is a print message'
And here is the terminal output which includes both stdout & sttderr, notice the PRINT statement gets written to stdout which appears in the terminal:
stav@maia:$ /srv/scrapy/scrapy.sh crawl logtest
2013-03-08 15:25:28-0600 [scrapy] INFO: Scrapy 0.17.0 started (bot: oneoff)
2013-03-08 15:25:28-0600 [scrapy] DEBUG: Overridden settings: {'NEWSPIDER_MODULE': 'oneoff.spiders', 'SPIDER_MODULES': ['oneoff.spiders'], 'USER_AGENT': 'Chromium OneOff 24.0.1312.56 Ubuntu 12.04 (24.0.1312.56-0ubuntu0.12.04.1)', 'BOT_NAME': 'oneoff'}
2013-03-08 15:25:28-0600 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2013-03-08 15:25:28-0600 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2013-03-08 15:25:28-0600 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2013-03-08 15:25:28-0600 [scrapy] DEBUG: Enabled item pipelines:
2013-03-08 15:25:28-0600 [logtest] INFO: Spider opened
2013-03-08 15:25:28-0600 [logtest] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2013-03-08 15:25:28-0600 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6025
2013-03-08 15:25:28-0600 [scrapy] DEBUG: Web service listening on 0.0.0.0:6082
2013-03-08 15:25:28-0600 [logtest] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example/> from <GET http://example.com/>
2013-03-08 15:25:29-0600 [logtest] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example> from <GET http://www.iana.org/domains/example/>
2013-03-08 15:25:29-0600 [logtest] DEBUG: Crawled (200) <GET http://www.iana.org/domains/example> (referer: None)
2013-03-08 15:25:29-0600 [scrapy] INFO: LOG: This is a log message
PRINT: This is a print message
2013-03-08 15:25:29-0600 [logtest] INFO: Closing spider (finished)
2013-03-08 15:25:29-0600 [logtest] INFO: Dumping Scrapy stats:
    {'downloader/request_bytes': 801,
     'downloader/request_count': 3,
     'downloader/request_method_count/GET': 3,
     'downloader/response_bytes': 1204,
     'downloader/response_count': 3,
     'downloader/response_status_count/200': 1,
     'downloader/response_status_count/302': 2,
     'finish_reason': 'finished',
     'finish_time': datetime.datetime(2013, 3, 8, 21, 25, 29, 292089),
     'log_count/DEBUG': 10,
     'log_count/INFO': 5,
     'response_received_count': 1,
     'scheduler/dequeued': 3,
     'scheduler/dequeued/memory': 3,
     'scheduler/enqueued': 3,
     'scheduler/enqueued/memory': 3,
     'start_time': datetime.datetime(2013, 3, 8, 21, 25, 28, 602254)}
2013-03-08 15:25:29-0600 [logtest] INFO: Spider closed (finished)
Now here is the output with the setting enabled:
stav@maia:$ /srv/scrapy/scrapy.sh crawl -s LOG_STDOUT=True logtest
2013-03-08 15:25:17-0600 [scrapy] INFO: Scrapy 0.17.0 started (bot: oneoff)
2013-03-08 15:25:17-0600 [scrapy] DEBUG: Overridden settings: {'NEWSPIDER_MODULE': 'oneoff.spiders', 'LOG_STDOUT': 'True', 'SPIDER_MODULES': ['oneoff.spiders'], 'USER_AGENT': 'Chromium OneOff 24.0.1312.56 Ubuntu 12.04 (24.0.1312.56-0ubuntu0.12.04.1)', 'BOT_NAME': 'oneoff'}
2013-03-08 15:25:17-0600 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2013-03-08 15:25:17-0600 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2013-03-08 15:25:17-0600 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2013-03-08 15:25:17-0600 [scrapy] DEBUG: Enabled item pipelines:
2013-03-08 15:25:17-0600 [logtest] INFO: Spider opened
2013-03-08 15:25:17-0600 [logtest] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2013-03-08 15:25:17-0600 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6025
2013-03-08 15:25:17-0600 [scrapy] DEBUG: Web service listening on 0.0.0.0:6082
2013-03-08 15:25:17-0600 [logtest] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example/> from <GET http://example.com/>
2013-03-08 15:25:18-0600 [logtest] DEBUG: Redirecting (302) to <GET http://www.iana.org/domains/example> from <GET http://www.iana.org/domains/example/>
2013-03-08 15:25:18-0600 [logtest] DEBUG: Crawled (200) <GET http://www.iana.org/domains/example> (referer: None)
2013-03-08 15:25:18-0600 [scrapy] INFO: LOG: This is a log message
2013-03-08 15:25:18-0600 [logtest] INFO: Closing spider (finished)
2013-03-08 15:25:18-0600 [logtest] INFO: Dumping Scrapy stats:
    {'downloader/request_bytes': 801,
     'downloader/request_count': 3,
     'downloader/request_method_count/GET': 3,
     'downloader/response_bytes': 1204,
     'downloader/response_count': 3,
     'downloader/response_status_count/200': 1,
     'downloader/response_status_count/302': 2,
     'finish_reason': 'finished',
     'finish_time': datetime.datetime(2013, 3, 8, 21, 25, 18, 337809),
     'log_count/DEBUG': 10,
     'log_count/INFO': 5,
     'response_received_count': 1,
     'scheduler/dequeued': 3,
     'scheduler/dequeued/memory': 3,
     'scheduler/enqueued': 3,
     'scheduler/enqueued/memory': 3,
     'start_time': datetime.datetime(2013, 3, 8, 21, 25, 17, 592098)}
2013-03-08 15:25:18-0600 [logtest] INFO: Spider closed (finished)
There should be a line in there for the PRINT statement, but it is not printed anywhere as I can tell. I have traced the issue to scrapy.log._adapt_eventdict() which I have not been able to debug yet.
Also twisted.python.log.startLoggingWithObserver() also has this code:
if setStdout:
    sys.stdout = logfile
    sys.stderr = logerr
which also might be causing some issues. Should it be: sys.stderr = logfile?