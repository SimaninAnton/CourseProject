lolobosse commented on Feb 1, 2018
Hello there,
I wrote the following code:
 process = CrawlerProcess()
 results = []

 def crawler_results(parse_result):
      results.append(parse_result)

 # The line stuff are some params, not interesting ;)
 process.crawl(BASpider, self.server, line[0], line[2], line[1])
 for p in process.crawlers:
     p.signals.connect(crawler_results, signal=scrapy.signals.item_dropped)
 process.start()
But the method crawler_results is never triggered and I do not understand why. Is it not supposed to work like this? (I mean, it's not working with item_scrapped or engine_started either) I just get no events triggered
Else, how would your retrieve the items of a spider with CrawlerProcess? (having a pipe is not something handy because I need to send all the data at once).