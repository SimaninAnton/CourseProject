rdegges commented on Apr 25, 2013
Hey all, I'm using the latest stable release of scrapy, and I'm having problems getting feed storage working. In my settings.py file, I've got the following defined:
LOG_ENABLED = True
LOG_FILE = BOT_NAME + '.log'
LOG_LEVEL = 'DEBUG'
LOG_STDOUT = True
STATS_DUMP = True
FEED_URI = 's3://mybucker/%(name)s/%(time)s.json'
FEED_FORMAT = 'jsonlines'
FEED_STORE_EMPTY = False
AWS_ACCESS_KEY_ID = 'blah'
AWS_SECRET_ACCESS_KEY = 'blah'
According to the documentation, http://doc.scrapy.org/en/0.16/topics/feed-exports.html#settings it looks as if this should work.
I'm defining my desired S3 URI, I've got boto installed (so that shouldn't be a problem), I'm telling scrapy to use jsonlines for serialization, and I've got my AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY clearly defined.
What am I doing wrong? When the scraper has finished running, if I inspect the .log file it creates locally, I see no reference to s3 at all.
Thank you.