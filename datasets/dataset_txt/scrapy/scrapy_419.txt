ariasuni commented on Mar 15, 2018
Thereâ€™s no easy/not hackish way to log an error when reaching max retry times with standard scrapy RetryMiddleware. Itâ€™s useful for me to be able to see right away if a page I tried to crawl has not been downloaded.
I think itâ€™s sensible to change this line to log to error level instead:
scrapy/scrapy/downloadermiddlewares/retry.py
Line 89 in 6cc6bbb
 logger.debug("Gave up retrying %(request)s (failed %(retries)d times): %(reason)s", 
ğŸ‘ 1