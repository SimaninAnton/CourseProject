ariasuni commented on Mar 15, 2018
There’s no easy/not hackish way to log an error when reaching max retry times with standard scrapy RetryMiddleware. It’s useful for me to be able to see right away if a page I tried to crawl has not been downloaded.
I think it’s sensible to change this line to log to error level instead:
scrapy/scrapy/downloadermiddlewares/retry.py
Line 89 in 6cc6bbb
 logger.debug("Gave up retrying %(request)s (failed %(retries)d times): %(reason)s", 
👍 1