greatwitenorth commented on Nov 6, 2012
I've noticed that despite setting allowed_domains correctly that scrapy is still passing urls that contain the allowed domain, but is not in the hostname. For example:
class SitemapgenSpider(CrawlSpider):
    name = "sitemapgen"
    allowed_domains = ["makesomecode.com"]
    start_urls = [
            "http://makesomecode.com"
        ]

    rules = (
        # Extract links matching 'category.php' (but not matching 'subsection.php')
        # and follow links from them (since no callback means follow=True by default).
        Rule(SgmlLinkExtractor(allow=()), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        item = SitemapgenItem()

        item['url'] = response.url
        item['content'] = response.headers['content-type']
        return item
Despite having the domain set I'm seeing urls like this parsed in the logs:
2012-11-05 16:53:07-0600 [sitemapgen] DEBUG: Crawled (200) <GET http://digg.com/submit?url=http%3A%2F%2Fmakesomecode.com%2Fabout%2F&title=About> (referer: http://makesomecode.com/about/)
I'm not sure if the Offsite Middleware is incorrectly parsing it because 'makesomecode.com' appears as a parameter value in the url.
I'm also seeing stuff like this show up:
2012-11-05 17:00:04-0600 [sitemapgen] DEBUG: Redirecting (302) to <GET http://www.reddit.com/submit?url=http%3A%2F%2Fmakesomecode.com%2F2011%2F09%2F02%2Fproducteev-php-library%2F&title=Producteev+PHP+Library> from <GET http://reddit.com/submit?url=http%3A%2F%2Fmakesomecode.com%2F2011%2F09%2F02%2Fproducteev-php-library%2F&title=Producteev+PHP+Library>
Shouldn't the url not even make it to the redirect if it's not allowed?