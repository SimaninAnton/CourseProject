Member
kmike commented on Feb 23, 2014
A link to current overview: http://doc.scrapy.org/en/latest/intro/overview.html
I think that using CrawlSpider in overview makes Scrapy look complex and confuses new users.
The overview relies on fact that there is a pattern in urls and that the spider should just crawl all urls with this pattern, making users wondering what to do in a general case.
The example scares me :)
from scrapy.contrib.spiders import CrawlSpider, Rule  
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.selector import Selector

# how can one find where to import stuff from? 
# SgmlLinkExtractor class has obscure name (not anyone has 
# a beard and knows what SGML is), and it is buried in 
#4-level-deep hierarchy, including "contrib".


class MininovaSpider(CrawlSpider):

    name = 'mininova' 
    allowed_domains = ['mininova.org']
    start_urls = ['http://www.mininova.org/today']
    # this was fine (though not explained earlier)

    # HERE BE DRAGONS
    rules = [Rule(SgmlLinkExtractor(allow=['/tor/\d+']), 'parse_torrent')]

    # What to do if users want more control on what is spider 
    # doing - should they find an another class nested 4-levels deep? 
    # Should they override Rule or SgmlLinkExtractor or CrawlSpider
    # to customize what spider is doing? Or maybe there are powerful options 
    # in these classes that cover 95% cases, and you are out of luck 
    # if your case is in remaining 5%? Is this kind of problems scrapy users 
    # are working on? Is it a scrapy style to always pass 
    # method names as strings? Readability one: is method name 
    # a parameter of Rule or SgmlLinkExtractor?
And then there is an encouraging ".. but this is just the surface. " in "What else" section. We all know scrapy is powerful :)
Of course, there are different users reading this overview, and for someone it can be just what is wanted. I keep in mind another kind of users reading the overview: they want to scrape information from some webpage, but haven't done this before. They may understand how to hack a synchronous script using requests or urllib. Maybe they can parallelize it using threads (maybe via futures), via grequests or even using twisted or tornado or celery. Maybe they already checked their webpage and have a rough idea how to extract data using regexes and how to follow the links. But before hacking such script they want to check how can a framework help them. They go to scrapy webpage and end up reading this overview.
Overview tells them they'll need to define an Item class, write some link extraction rules in a scrapy DSL, populate items using xpaths and to use a command-line tool to get the result. Most likely this code doesn't look like anything they thought of, except for xpaths. They can see that rules may save them some typing on link following code, but learning how to use them looks like a big time investment. They see that scrapy somehow downloads pages behind the scenes, but it is not clear what does it take to make scrapy download the pages they want. Also, it is not clear how does scrapy work (sync? threads? event loop?), so the advantage over e.g. sequential requests.get(url) calls are unclear. There is zero visible advantages of defining TorrentItem over using plain dicts.
Overview omits some real-word steps like where to put the code we've written - I recall putting it all in a single file mininova.py and trying to run the scrapy crawl command - this didn't work. Overview gets into great details about mininova-specific XPaths, so I thought it is a tutorial while reading it; The purpose of this document... note in the beginning of the document was long forgotten.
After a lengthy rant it is better to propose something :) What about removing tutorial-ish parts (including CrawlSpider example) from the overview? I think a good approximation would be to "downgrade" "Pick a website", "Define the data you want to scrape", "Write a Spider to extract the data" and "Run the spider to extract the data" sections to an ordered list with short comments about each item, and maybe create a "CrawlSpider tutorial" from the removed contents.
Tutorial has its issues (#608), but it is a better introduction IMHO.