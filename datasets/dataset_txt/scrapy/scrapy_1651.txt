Member
dangra commented on Sep 9, 2011
When crawling many domains from the same spider, cookies middleware slows down.
The patch is attached.
The main difference is that it only checks for cookies belonging to relevant domains (the
potential_domain_matches) instead of all domains, but to do this required a bit of refactoring. It also
only calls clear_expired_cookies periodically instead of every request.
The performance problems are very large and noticeable pretty quickly if a single spider has to manage
many cookies. This happened when we had a spider that crawled many sites with the cookies middleware
enabled.
It would be great if you could review and even if we can maybe then try it on more websites.
see http://dev.scrapy.org/ticket/333 for more info