jsvachon2 commented on Jun 26, 2019 â€¢
edited
I'm getting a lot of 503 from a site I am scrapping with Scrapy and I can't get it to record a list of all the failures. I can always parse the logs at the very end but I'd like to simply requeue the failed URL when needed.
I've enabled the Retry module using RETRY_ENABLED = True and set RETRY_TIMES = 2 for testing purposes.
I've added the from_crawler class method and registered three callbacks...
    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(MySpider, cls).from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(spider.item_error, signal=signals.item_error)
        crawler.signals.connect(spider.item_dropped, signal=signals.item_dropped)

        return spider
The first callback is working fine but the other two are not working at all. They are defined as:
def item_error(self, item, response, spider, failure):
        self.log("************************ ERROR ********", logging.ERROR)
        self.log(item, logging.ERROR)

def item_dropped(self, item, response, exception, spider):
        self.log("*********************** DROPPED *********", logging.ERROR)
        self.log(item, logging.ERROR)
Am I doing something wrong? My spider reads the Urls to process from a queue so upon failure, I'd like to simply requeue them. To make it faster to test and to prevent hitting the remote server too hard, I am using a fake server that returns 503 for everything...
while true; do echo -e "HTTP/1.1 503 FAILED\n\n $(date)" | nc -l -p 1500 -q 1; done
Thanks