cockcrow commented on Aug 8, 2016
Problem should be reproduced by following steps:
scrapy startproject testing
cd testing
scrapy genspider http httpbin.org
scrapy edit http
# -*- coding: utf-8 -*-
import logging
import scrapy
from scrapy.utils.reqser import request_to_dict


class HttpSpider(scrapy.Spider):
    name = "http"
    allowed_domains = ["httpbin.org"]
    start_urls = (
        'http://httpbin.org/get',
    )

    def parse(self, response):
        """
        Default parse callback

        @url http://httpbin.org/get
        @returns request 0
        @returns item 0
        """
        request = request_to_dict(response.request, spider=self)
        self.log(request, level=logging.INFO)
Then the scrapy crawl http command works well, while the scrapy check blocks.
$ scrapy crawl http
2016-08-08 12:01:41 [scrapy] INFO: Scrapy 1.1.1 started (bot: testing)
2016-08-08 12:01:41 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'testing.spiders', 'SPIDER_MODULES': ['testing.spiders'], 'ROBOTSTXT_OBEY': True, 'BOT_NAME': 'testing'}
2016-08-08 12:01:41 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-08-08 12:01:41 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-08-08 12:01:41 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-08-08 12:01:41 [scrapy] INFO: Enabled item pipelines:
[]
2016-08-08 12:01:41 [scrapy] INFO: Spider opened
2016-08-08 12:01:41 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-08-08 12:01:41 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024
2016-08-08 12:01:41 [scrapy] DEBUG: Crawled (200) <GET http://httpbin.org/robots.txt> (referer: None)
2016-08-08 12:01:42 [scrapy] DEBUG: Crawled (200) <GET http://httpbin.org/get> (referer: None)
2016-08-08 12:01:42 [http] INFO: {'body': '', '_encoding': 'utf-8', 'cookies': {}, 'meta': {'download_timeout': 180.0, 'download_latency': 0.30795884132385254, 'download_slot': 'httpbin.org'}, 'headers': {'Accept-Language': ['en'], 'Accept-Encoding': ['gzip,deflate'], 'Accept': ['text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'], 'User-Agent': ['Scrapy/1.1.1 (+http://scrapy.org)']}, 'url': u'http://httpbin.org/get', 'dont_filter': True, 'priority': 0, 'callback': None, 'method': 'GET', 'errback': None}
2016-08-08 12:01:42 [scrapy] INFO: Closing spider (finished)
2016-08-08 12:01:42 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 431,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 713,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 8, 8, 4, 1, 42, 251024),
 'log_count/DEBUG': 3,
 'log_count/INFO': 8,
 'response_received_count': 2,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2016, 8, 8, 4, 1, 41, 314722)}
2016-08-08 12:01:42 [scrapy] INFO: Spider closed (finished)
$ scrapy check
E
======================================================================
ERROR: [http] parse (callback)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/contracts/__init__.py", line 79, in cb_wrapper
    output = cb(response)
  File "/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/contracts/__init__.py", line 131, in wrapper
    output = list(iterate_spider_output(cb(response)))
  File "/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/contracts/__init__.py", line 131, in wrapper
    output = list(iterate_spider_output(cb(response)))
  File "/Users/jianhaochen/testing/testing/spiders/http.py", line 22, in parse
    request = request_to_dict(response.request, spider=self)
  File "/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/utils/reqser.py", line 18, in request_to_dict
    cb = _find_method(spider, cb)
  File "/Users/jianhaochen/.pyenv/versions/2.7.10/envs/testing/lib/python2.7/site-packages/scrapy/utils/reqser.py", line 73, in _find_method
    raise ValueError("Function %s is not a method of: %s" % (func, obj))
ValueError: Function <function parse at 0x10c09d938> is not a method of: <HttpSpider 'http' at 0x10be58bd0>

----------------------------------------------------------------------
Ran 0 contracts in 0.983s

FAILED (errors=1)
Here the version info:
$ python -V
Python 2.7.10
$ scrapy version -v
Scrapy    : 1.1.1
lxml      : 3.6.1.0
libxml2   : 2.9.2
Twisted   : 16.3.0
Python    : 2.7.10 (default, Nov 20 2015, 11:35:08) - [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.1.76)]
pyOpenSSL : 16.0.0 (OpenSSL 1.0.2h  3 May 2016)
Platform  : Darwin-15.6.0-x86_64-i386-64bit