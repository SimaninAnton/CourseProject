sampeka commented on Apr 10, 2018 â€¢
edited
I have a crawler that scrapes hundreds of items per request that need to be processed by the item pipeline. Usually this involves downloading files, uploading them to a cloud provider, then posting some json to a web service. Due to this, an item can take a long time to pass through the whole pipeline.
This means that most of the time the spider is finished crawling new urls before the items have finished being processed. I may be misreading this, but after a while it looks like the process is killed and some items are left unprocessed. My settings are pretty much out of the box, no max depth etc.
Is this a bug or intended behaviour? Is there a way to prevent the scrapy process from exiting until all items have been processed?