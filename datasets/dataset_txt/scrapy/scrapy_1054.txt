mvl22 commented on Jan 11, 2016
We are seeing sites receiving crawls with this bot's signature which disobey robots.txt, falling into our honeytrap.
Please set support for robots.txt to be on by default. Currently the default is do not obey:
http://doc.scrapy.org/en/latest/topics/settings.html#robotstxt-obey
I would argue that crawlers have a responsibility to ensure that they act as a well-behaved web citizen by default. This applies even more to open-source code crawlers, which are likely to be used by people the author has no control over.
So please kindly ensure this behaviour is on by default. If someone is installing this crawler, and they are using it on their own website, they will have full knowledge of why robots.txt rules are in place, and so can decide to disable the feature. If they are using it on another site, then the robots.txt should be respected, because that is what the site owner has specifically defined as being reasonable usage.