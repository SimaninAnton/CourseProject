Member
pablohoffman commented on Sep 9, 2011
Sometimes pages depend on certain HTTP request headers sent, for rendering the expected result, and it's a manual and tedious job to find out which headers those are.
So, here's an idea for automating this probing mechanism: create a new scrapy command probe which takes a url as argument and a text to look for.
Scrapy then tries several combinations of HTTP headers (user-agent, accept, etc) and return a set that works (where works mean that the text passed is found).
Here's a real world example to illustrate:
http://www.storage-cabinets-online.com/IVG2/N/ProductID-118021.htm
The page should contain a string 'var sFeatures', but that string is not returned with the default Scrapy HTTP request headers. So we run scrapy probe on it:
$ scrapy probe http://www.storage-cabinets-online.com/IVG2/N/ProductID-118021.htm 'var sFeatures'
Found set of working headers:
{'Host': 'www.storage-cabinets-online.com', 'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.2.10) Gecko/20100915 Ubuntu/10.04 (lucid)
Firefox/3.6.10', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en-us,en;q=0.5', 'Accept-Encoding': 'gzip,deflate',
'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.7', 'Keep-Alive': '115'}
The Scrapy probe command would try a different list of well known user-agents, along with Accept headers.