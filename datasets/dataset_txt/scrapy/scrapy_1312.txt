cnglen commented on Dec 29, 2014
One scrapy job is running forever, can't be stopped.
I have set downloader timeout to 30
In my code, I checked the running periodlly, if longer than some time, use 'http://localhost:6800/cancel.json' to cancel the job.
However, the job is still running. How can I debug this situation?(Bug of my src, scrapy, or scrapyd?) Any ideas? Thanks.
Finally, I run 'sudo kill -SIGKILL 13902' to kill the process. 'sudo kill -SIGTERM 13902' doesn't work.
$ curl http://localhost:6800/cancel.json -d project=myproject -d job=5139c1ac8f0a11e4b0ed247703282fcc
{"status": "ok", "prevstate": "running"} <--- It returns status ok, however, the job can't be stopped.
$ ps aux|grep 13902
scrapy 13902 0.0 0.8 172860 64636 ? S 11:25 0:01 /usr/bin/python -m scrapyd.runner crawl mysider -a _job=5139c1ac8f0a11e4b0ed247703282fcc
scrapy.version
u'0.24.4'
scrapyd.version
'1.0.1'
log:
2014-12-29 11:25:26+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: myproject)
2014-12-29 11:25:26+0800 [scrapy] INFO: Optional features available: ssl, http11
2014-12-29 11:25:26+0800 [scrapy] INFO: Overridden settings:
{'COOKIES_DEBUG': True, 'NEWSPIDER_MODULE': 'myproject.spiders',
'FEED_URI':
'/var/lib/scrapyd/items/myproject/myspider/5139c1ac8f0a11e4b0ed247703282fcc.jl',
'SPIDER_MODULES': ['myproject.spiders'], 'RETRY_HTTP_CODES': [500,
502, 503, 504, 400, 408, 403, 404], 'BOT_NAME': 'myproject',
'DOWNLOAD_TIMEOUT': 30, 'COOKIES_ENABLED': False, 'LOG_FILE': <--------------------timeout
'/var/log/scrapyd/myproject/myspider/5139c1ac8f0a11e4b0ed247703282fcc.log',
'DOWNLOAD_DELAY': 2}
2014-12-29 11:25:26+0800 [scrapy] INFO: Enabled extensions:
FeedExporter, LogStats, TelnetConsole, CloseSpider, WebService,
CoreStats, SpiderState
2014-12-29 11:25:26+0800 [scrapy] INFO: Enabled downloader
middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware,
RandomUserAgentMiddleware, RandomProxyMiddleware, RetryMiddleware,
DefaultHeadersMiddleware, MetaRefreshMiddleware,
HttpCompressionMiddleware, RedirectMiddleware,
ChunkedTransferMiddleware, DownloaderStats
2014-12-29 11:25:26+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2014-12-29 11:25:26+0800 [scrapy] INFO: Enabled item pipelines: ImagesPipeline, WordpressPipeline, MySQLStorePipeline
2014-12-29 11:25:26+0800 [myspider] INFO: Spider opened
2014-12-29 11:25:26+0800 [myspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2014-12-29 11:25:26+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
2014-12-29 11:25:26+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6082
2014-12-29 11:45:52+0800 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force
2014-12-29 11:46:22+0800 [scrapy] INFO: Received SIGTERM twice, forcing unclean shutdown <------------------------
$ sudo kill -SIGTERM 13902
$ ps aux|grep 13902
scrapy 13902 0.0 0.8 172860 64636 ? S 11:25 0:01 /usr/bin/python -m scrapyd.runner crawl myspider -a _job=5139c1ac8f0a11e4b0ed247703282fcc
touch 18543 0.0 0.0 18248 2204 pts/5 S+ 14:24 0:00 grep 13902
$ sudo kill -SIGKILL 13902
$ ps aux|grep 13902
touch 18551 0.0 0.0 18244 2204 pts/5 S+ 14:25 0:00 grep 13902