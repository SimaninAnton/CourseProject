wybigsea9 commented on Jul 17, 2018
Scrapy_redis: when I run two crawlers at the same time, redis_key is passed to one of the crawlers, and ideally the other crawler won't start, but it starts up and reports an error.
[scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
Unhandled Error
Traceback (most recent call last):
File "c:\python27\lib\site-packages\scrapy\commands\crawl.py", line 58, in run
self.crawler_process.start()
File "c:\python27\lib\site-packages\scrapy\crawler.py", line 291, in start
reactor.run(installSignalHandlers=False) # blocking call
File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1261, in run
self.mainLoop()
File "c:\python27\lib\site-packages\twisted\internet\base.py", line 1270, in mainLoop
self.runUntilCurrent()
--- ---
File "c:\python27\lib\site-packages\twisted\internet\base.py", line 896, in runUntilCurrent
call.func(*call.args, **call.kw)
File "c:\python27\lib\site-packages\scrapy\utils\reactor.py", line 41, in call
return self._func(*self._a, **self._kw)
File "c:\python27\lib\site-packages\scrapy\core\engine.py", line 122, in _next_request
if not self._next_request_from_scheduler(spider):
File "c:\python27\lib\site-packages\scrapy\core\engine.py", line 149, in _next_request_from_scheduler
request = slot.scheduler.next_request()
File "c:\python27\lib\site-packages\scrapy_redis\scheduler.py", line 172, in next_request
request = self.queue.pop(block_pop_timeout)
File "c:\python27\lib\site-packages\scrapy_redis\queue.py", line 117, in pop
return self._decode_request(results[0])
File "c:\python27\lib\site-packages\scrapy_redis\queue.py", line 48, in _decode_request
return request_from_dict(obj, self.spider)
File "c:\python27\lib\site-packages\scrapy\utils\reqser.py", line 50, in request_from_dict
cb = _get_method(spider, cb)
File "c:\python27\lib\site-packages\scrapy\utils\reqser.py", line 87, in _get_method
raise ValueError("Method %r not found in: %s" % (name, obj))
exceptions.ValueError: Method 'parse_item' not found in: <SpiderSpider 'spider' at 0x59cb710>
The crawler project is written The data from The yield link in The parse function to The parse_item function
I don't know how to make this mistake, please help me to see it
Scrapy 1.5.0 started (bot: chongqingribao)
[scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0h 27 Mar 2018), cryptography 2.2.2, Platform Windows-8-6.2.9200