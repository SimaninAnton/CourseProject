birla commented on Oct 27, 2016 â€¢
edited
I'm creating a Scrapy.Item with the following structure:
Item1
property (string)
property (string)
property (Item2[])
(Item2)
(Item2)
....
Now the console's debug log displays the resulting JSON structure property, however in the export I get the following error:
2016-10-27 15:17:55 [scrapy] ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7f0edde6ebd0>>
Traceback (most recent call last):
  File "/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/twisted/internet/defer.py", line 149, in maybeDeferred
    result = f(*args, **kw)
  File "/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/extensions/feedexport.py", line 221, in item_scraped
    slot.exporter.export_item(item)
  File "/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/exporters.py", line 116, in export_item
    itemdict = dict(self._get_serialized_fields(item))
  File "/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/exporters.py", line 75, in _get_serialized_fields
    value = self.serialize_field(field, field_name, item[field_name])
  File "/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/exporters.py", line 47, in serialize_field
    return serializer(value)
  File "/home/birla/work/coolstore/data/env/local/lib/python2.7/site-packages/scrapy/item.py", line 52, in __init__
    for k, v in six.iteritems(dict(*args, **kwargs)):
ValueError: dictionary update sequence element #0 has length 12; 2 is required
Item2 has a 12 properties. As far as I understand it is being raised while converting Item2[] into a dict object. Also, if I run the crawl without exporting the code works fine.