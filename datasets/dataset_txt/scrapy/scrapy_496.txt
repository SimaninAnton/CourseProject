guangbaowan commented on Nov 20, 2017 â€¢
edited
Hi, We are deploying the scrapy on docker (scrapyd (1.1.0), scrapyd-client (1.0.1))
we restart(cronjob) the job by send message to 'http://0.0.0.0:6800/cancel.json' and got
Traceback (most recent call last):
File "stop_spiders.py", line 31, in
main()
File "stop_spiders.py", line 8, in main
jobs = requests.get(job_url)
File "/usr/local/lib/python2.7/site-packages/requests/api.py", line 67, in get
return request('get', url, params=params, **kwargs)
File "/usr/local/lib/python2.7/site-packages/requests/api.py", line 53, in request
return session.request(method=method, url=url, **kwargs)
File "/usr/local/lib/python2.7/site-packages/requests/sessions.py", line 468, in request
resp = self.send(prep, **send_kwargs)
File "/usr/local/lib/python2.7/site-packages/requests/sessions.py", line 576, in send
r = adapter.send(request, **kwargs)
File "/usr/local/lib/python2.7/site-packages/requests/adapters.py", line 437, in send
raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='0.0.0.0', port=6800): Max retries exceeded with url: /listjobs.json?project=chaos (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7fa3b8b71b10>: Failed to establish a new connection: [Errno 111] Connection refused',))