medecau commented on Aug 7, 2018
Python 3.6.6
Scrapy 1.5.1
I'm trying to add the ROBOTS and HTTPCACHE middle-wares to a self-contained spider but for some reason these are not being added to the list of downloader middle-wares.
I have tried generating this self-contained spider using the scrapy cli tool and the same thing happens. But when I create it using the project tree structure it does work. I have also tried for a few days to find this information on the docs but all attempts to set these values have failed.
For my setup I prefer to have as flat a folder structure with as few files as possible. What would then be the preferred way to activate the robots.txt and http-cache middle-wares in this circumstance?