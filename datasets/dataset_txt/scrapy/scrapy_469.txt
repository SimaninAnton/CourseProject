roboostify commented on Jan 11, 2018
I usually test my spiders with the command scrapy parse --spider=myspider -c parse_whatever <url_to_scrape>.
So far it worked out well, but when I want to apply a pipeline on my items it just doesn't work, at least it seems like the pipeline is skipped during execution, however the log says that the pipeline is enabled.
So for example, I have the following Item, Spider and Pipeline to test the issue:
Item:
class MyItem(scrapy.Item):
    status = scrapy.Field()
    date_created = scrapy.Field()
    site = scrapy.Field()
Spider:
class TestSpider(Spider):
    name = 'test'
    start_urls = ['http://www.example.com',]

    def parse(self, response):
        item = ProductPriceItem()
        yield item
Pipeline:
class DefaultValuePipeline(object):
    def process_item(self, item, spider):
        item.setdefault('status', 'New')
        item.setdefault('date_created', '01/12/2018')
        item.setdefault('site',  'example.com')

        return item
And my problem is when I execute the spider as:
scrapy parse --spider=test -c parse http://example.com
It will output:
# Scraped Items  ------------------------------------------------------------
[{}]
While using:
scrapy crawl test
Will output:
2018-01-11 12:05:54 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.example.com>
{'date_created': '01/12/2018',
 'site': 'example.com',
 'status': 'New'}
As expected.
Am I missing something or it's a bug?