iagent commented on Sep 21, 2017 â€¢
edited
I am currently using scrapy for an application involving broad crawling.
One of my teammates contested to use a combination of django + gunicorn in place of Scrapy as most of the team is conversant with Django and its ORM. The number of websites involved are to the tune of 100K. He suggests to use Gunicorn if we want concurrency and Django Q for queueing requests.
I am not comfortable with the idea as Django and Scrapy are built for completely different problems but not sure how can I make a strong argument.
Could you help me out here? My understanding is that queueing at each stage of crawling and scraping sort of helps in performance. As Scrapy takes care of each stage, even if we temporarily drop performance concern, wouldn't it involve more code writing?