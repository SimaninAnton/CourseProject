linxianping commented on May 16, 2012
scrapy version -v result:
Scrapy : 0.14.3
Twisted : 12.0.0
Python : 2.6.8 (unknown, May 3 2012, 11:31:34) - [GCC 4.1.2 20071124 (Red Hat 4.1.2-42)]
Platform: Linux-2.6.18-92.el5-x86_64-with-redhat-5.2-Tikanga
Bug detail:
The 'segmentation fault' happens, and scrapy stop running. After checking the website url, I found it should belong to one tag with href attribute but no value. like this: " dummy " I suspect this is caused by lxml dynamic c lib, how to deal
with this? At least prevent the spider stop issue. My code snippet:
hxs = HtmlXPathSelector(response)
sites = hxs.select('//a')
for site in sites:
list_title= site.select('text()').extract()
list_link = site.select('./@href').extract() #????(crash here when parsing )
BTW, The site scraped is 'http://mil.news.sohu.com/' . The issue also replicate under scrapy 0.15.1 and python 2.7.3,.
pre-install openssl0.12 because the openssl0.13 default for scrapy can't be installed sucessfully.
I am urging waiting for your reply to decide if my project will use scrapy or not.
I will appreciate if you can reply me ASAP.
linxianping