chrismp commented on Apr 18, 2018
I want to use Scrapy to download files and navigate folders at ftp://ftp.co.palm-beach.fl.us/Building%20Permits/.
Here's my spider:
# -*- coding: utf-8 -*-
import scrapy
from scrapy.http import Request

class LatestPermitsSpider(scrapy.Spider):
 name=   "latest_permits"
 allowed_domains=["ftp.co.palm-beach.fl.us"]
 handle_httpstatus_list = [404]
 
 ftpUser=  "the_username"
 ftpPW=   "the_password"
 permitFilesDir= "ftp://ftp.co.palm-beach.fl.us/Building%20Permits/"

 def start_requests(self):
  yield Request(
   url=self.permitFilesDir,
   meta={
    "ftp_user": self.ftpUser,
    "ftp_password": self.ftpPW
   }
  )

 def parse(self,response):
  print response.body
When I run scrapy crawl latest_permits, I get this error:
ConnectionLost: ('FTP connection lost', <twisted.python.failure.Failure twisted.protocols.ftp.CommandFailed: ['530 Sorry, no ANONYMOUS access allowed.']>)
Why does this error come up even when I supply the correct username and password?
I do not know the FTP server's port number.