Contributor
Granitosaurus commented on Jul 16, 2015
ScrapyRT lets you provide url and a callback to crawl a single page. I think this feature should exist in scrapy itself in a form of new Spider class/modification
People usually comment out a test start_request() method with test Request to crawl a single for testing purposes. Though maybe a new Spider class or addition to scrapy.Spider class would be appropriate solution ?
Something like:
class SingleSpider(scrapy.Spider):
    single_kwargs = {'url': '',
                     'callback': '',
                     'meta': {}}

    def __init__(self, **kwargs):
        super(SingleSpider, self).__init__(**kwargs)
        single_kwargs = kwargs.get('single', '{}')
        self.single_kwargs.update(eval(single_kwargs))

    def start_requests(self):
        callback = getattr(self, self.single_kwargs.pop('callback'), None)
        if self.single_kwargs['url']:
            return [scrapy.Request(callback=callback, **self.single_kwargs)]
        else:
            return self.generate_start_requests()

    def generate_start_requests(self):
        """normal start_requests from scrapy.Spider"""
        return super(SingleSpider, self).start_requests()

class ShBlogSpider(SingleSpider):
    name = "blog.scrapinghub"
    single_kwargs = {'callback': 'parse_blog', 'meta': {'index': '0'}}
    ...
which allows to scrape single page with something like:
 scrapy crawl blog.scrapinghub -a single="{'url': 'http://blog.scrapinghub.com/2015/06/19/link-analysis-algorithms-explained/'}"
There are several issues with the spider above (like no formrequest support and potentially breaking on some spider kwargs) but those could be resolved.