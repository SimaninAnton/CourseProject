thousfeet commented on Oct 25, 2019 •
edited
Description
I tried to run multiple spiders using CrawlerProcess following this doc. But can‘t get one valid XML file.
this is my code:
class Spider1(spider):
....

class Spider2(spider):
....

class Spider3(spider):
....

process = CrawlerProcess(settings={
    'FEED_FORMAT': 'xml',
    'FEED_URI': 'output.xml',
})

process.crawl(Spider1)
process.crawl(Spider2)
process.crawl(Spider3)
process.start()
I get a output.xml which can't be parsed by xml.etree.ElementTree. Then I found this output file is not a valid XML file.
...<author><value>Dorany Pineda</value></author><content><?xml version="1.0" encoding="utf-8"?>\n<items>\n<item><web_url>...
It seems one spider wrote the file but another spider hasn't finished writing....
Versions
Scrapy : 1.6.0
lxml : 4.4.1.0
libxml2 : 2.9.9
cssselect : 1.1.0
parsel : 1.5.2
w3lib : 1.21.0
Twisted : 19.7.0
Python : 3.6.7 (default, Feb 28 2019, 07:28:18) [MSC v.1900 64 bit (AMD64)]
pyOpenSSL : 19.0.0 (OpenSSL 1.1.1d 10 Sep 2019)
cryptography : 2.7
Platform : Windows-10-10.0.18362-SP0