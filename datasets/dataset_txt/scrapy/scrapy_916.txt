GoingMyWay commented on Jun 1, 2016 •
edited
Sorry, it is not a programming error.
As I know, Scrapy crawls the data into buffer, and when the program exit, it saves the data to disk.
My machine has 8G memory, and the data I am crawling maybe 8-10G.
How can I set the scrapy to save data while crawling so that it can avoid memory error？
There are 2200000 pages to be crawled, And how can I set that after crawled 10000 pages, it saves the data to disk?