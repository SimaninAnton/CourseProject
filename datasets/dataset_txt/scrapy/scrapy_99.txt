Urahara commented on Jul 31, 2019 â€¢
edited
I read the docs and i see the only method that exists is "spider_closed" but this is executed after the termination , i want to ignore signal and keep my spider running?
Output when i run kill -TERM {spider_pid} command:
[scrapy.crawler] INFO: Received SIGTERM, shutting down gracefully. Send again to force
Diging a lit bit i found out that crawler.py file its where the signals are handled.
scrapy/scrapy/crawler.py
Lines 272 to 277 in 0d51f9c
 def _signal_shutdown(self, signum, _): 
     install_shutdown_handlers(self._signal_kill) 
     signame = signal_names[signum] 
     logger.info("Received %(signame)s, shutting down gracefully. Send again to force ", 
                 {'signame': signame}) 
     reactor.callFromThread(self._graceful_stop_reactor) 
Is possible to make this configurable? Like configure_logging?