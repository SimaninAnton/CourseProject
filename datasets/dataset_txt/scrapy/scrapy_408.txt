colinfike commented on Mar 28, 2018 â€¢
edited
I was looking around to see if this functionality existed and didn't come across anything.
This particular scrape I am doing will take a day or two and I am uploading the files to S3 since I am hosting this on Heroku. I was wondering if there was a way to set the spider to upload to S3 every X pages scraped or something like that. I want to avoid the situation where my dyno crashes and the temp output file is lost before being uploaded to S3.
Does something like this already exist? Otherwise I may take a stab at trying to figure out a good way to implement something like this.
Closing as this does not exist. Probably going to add a PR for this.