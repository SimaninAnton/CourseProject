ramwin commented on May 26, 2016 ‚Ä¢
edited by kmike
I'm using mysql to store my spider data, but when I set the piplines to store data into local mysql server, it raise an error.
pipelines.py
import json
import requests
from mysql.connector import connection

MYSQL_SERVER = '192.168.1.90'   # using this, the spider can run
MYSQL_SERVER = 'localhost'   # using this, the spider raise an error
MYSQL_DB = 'scrapy'
MYSQL_USER = 'crawler'
MYSQL_PASS = 'crawl'
MYSQL_TABLE = 'pm25in'

class Pm25InPipeline(object):
    def __init__(self):                                                                
        pass

    def process_item(self, item, spider):                                              
        command = '''insert into {table} (monitortime, monitorcity, monitorpoint,      
            AQIindex, airsituation, primarypullutant, PM25content, PM10content,        
            CO, NO2, O3_1h, O3_8h, SO2)                                                
            values ( "{monitortime}", "{monitorcity}","{monitorpoint}", "{AQIindex}",  
            "{airsituation}", "{primarypollutant}", {PM25content}, {PM10content},      
            {CO}, {NO2}, {O3_1h}, {O3_8h}, {SO2} );                                    
            '''.format(table=MYSQL_TABLE, **dict(item))                                
        self.cursor.execute(command)                                                   
        return item                                                                    

    def open_spider(self, spider):                                                     
        self.cnx = connection.MySQLConnection(                                         
            host=MYSQL_SERVER,                                                         
            user=MYSQL_USER,                                                           
            password=MYSQL_PASS,                                                       
            database=MYSQL_DB,                                                         
            charset='utf8')                                                            
        self.cursor = self.cnx.cursor()                                                

    def close_spider(self,spider):                                                     
        self.cnx.commit()                                                              
        self.cnx.close()                                                               
the Traceback:
wangx@wangx-PC:~/github/pm25in$ scrapy crawl pm25spider
Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 57, in run
    self.crawler_process.crawl(spname, **opts.spargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 163, in crawl
    return self._crawl(crawler, *args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 167, in _crawl
    d = crawler.crawl(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1274, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py", line 1126, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python2.7/dist-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/crawler.py", line 87, in crawl
    yield self.engine.close()
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 100, in close
    return self._close_all_spiders()
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 340, in _close_all_spiders
    dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 298, in close_spider
    dfd = slot.close()
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 44, in close
    self._maybe_fire_closing()
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 51, in _maybe_fire_closing
    self.heartbeat.stop()
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py", line 202, in stop
    assert self.running, ("Tried to stop a LoopingCall that was "
exceptions.AssertionError: Tried to stop a LoopingCall that was not running.
üëç 2