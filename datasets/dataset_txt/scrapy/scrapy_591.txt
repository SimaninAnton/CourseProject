madspark commented on Jul 11, 2017 â€¢
edited
Here is the problem that I am having:
I want to crawl some URLs with CrawlSpider and I expect some of them to give DNS lookup errors.
I want to catch these errors and handle them gracefully (log them, etc).
For a generic spider, this can be done as shown in the documentation:
https://doc.scrapy.org/en/latest/topics/request-response.html?highlight=failure.check#using-errbacks-to-catch-exceptions-in-request-processing
For sublinks of my initial pages, it can be done as suggested on stackoverflow:
https://stackoverflow.com/questions/35866873/scrapy-get-website-with-error-dns-lookup-failed
But for the initial URL visited (i.e. before rules are used for extraction), I can't figure out a way to gracefully catch errors. Best I can do is write a downloader middleware, which can at least log them.
The following seemed promising, but does not work - I overwrote the start_requests function to include callback and errback in the initial request. This catches the errors, but it also makes the CrawlSpider not attempt to parse subpages for valid requests. I am actually not sure if this is a bug or if I am missing something.
Can anyone point out if there is an obvious approach that I am missing to catching all request download errors (including DNS lookup errors) using CrawlSpider?