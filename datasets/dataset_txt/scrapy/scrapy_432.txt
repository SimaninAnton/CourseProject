Contributor
virmht commented on Feb 25, 2018 â€¢
edited
Hi! I have already introduced myself here.
I am interested in working on the project: "Scrapy spider autorepair". As suggested by you, @cathalgarvey , I have read about Scrapely, its API, how it works, and have a decent understanding of its source code.
I have also read 2 research papers cited on the Github Scrapely page.
As per my understanding, scrapely has the following drawbacks when comapared to what is required as stated on the GSoC 2018 ideas page:
Scrapely currently does not output rules in the form of XPaths or CSS Selectors.
Scrapely currently requires the structure of the test page to remain same or at least similar to the page on which it was trained. In our case, (as described on the ideas page), the content remains same but the structure can change. Currently scrapely looks at the order of elements in the prefix and suffix of the element to be extracted. Hence, it might produce wrong results if the order changes quite drastically but the content remains same.
One way to deal with issue 2. is to do the following: instead of looking at the order of elements in the prefix and suffix of the element to be extracted(which scrapely does), we need to look at the information or the content of the page around the region of interest and figure out its path in the new page. For example:
**Old version of page:**
<parent>
               <child>ABC</child>
               <child>XYZ</child>
</parent>

**New version of page:**
<parent>
               <child>XYZ</child>
               <child>ABC</child>
</parent>
for each element in old version:
        value = element.value // example: value of first child element = 'ABC'
        position = find position of value in new version // position of 'ABC' in new version = /parent/child[2]
        return position
I want to start writing my GSoC 2018 proposal for this project. @cathalgarvey can you provide some more info about the tasks that should be completed ?
Thanks.