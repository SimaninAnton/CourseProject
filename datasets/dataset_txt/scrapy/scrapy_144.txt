luoqishuai commented on May 28, 2019 •
edited
I used to have a spider, but the key parameters in setting are different. At that time, I copied and pasted 30 copies one by one, and then used docker to start the crawl one by one.
Now I want to use a python program to run 30 spiders in a row, using 30 threads, each spider with a different Settings, and everything else is the same.
At first, I tried to build a thread pool, but python told me builtins.valueerror: signal only works in main thread.
Then I start the way of using https://doc.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process, because each spiders will require different Settings, so there is no success
How can you implement this kind of train of thought please?
Did I have to copy the code one by one and run the shell commands to boot it up one by one?
It's stupid, but that's all I can think of.
I would appreciate it if you could help me. If my description is not very clear, please let me know.
def task(i):
# settings=get_project_settings()
# settings.update({'IDQUEUE_MOD':i,'LOG_FILE' : "scrapy_spider_{}.log".format(i)})
# runner = CrawlerRunner(settings)
# runner.crawl(SpiderSpider)
# reactor.run(installSignalHandlers=False)
cmdline.execute('scrapy crawl spider -s IDQUEUE_MOD={} -s LOG_FILE=scrapy_spider_{}.log'.format(i,i).split())
return 1
settings = get_project_settings()
queuesize = settings.getint('IDQUEUE_SIZE') # 队列大小 代表起几个爬虫
pool=ThreadPoolExecutor(queuesize)
name_list=['name_{}'.format(i) for i in range(queuesize)]
for i in range(queuesize):
name_list[i]=pool.submit(task,i)
b=[i.result() for i in name_list]
print(b)