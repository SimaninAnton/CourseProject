wpxgit commented on Jul 28, 2017
I've a crawl where every subpage has 300+ links on it.
It slows down after a minute or so / sometimes it crawls 0 pages / min.
I've configured 10 concurrent_requests and 10 processed items. That means up to 3.000 yields per 10 processed items...
As far as i can see the parse function takes 70+ seconds per page.
That long time results from yielding these 300+ links and each of the links seems to wait until the engine? or something similar has done one task and is ready to process the new yield request?
Adding the requests to scheduler don't take long so it seems to me that the yield is waiting for something other.
Any hints?
Is it possible to yield the requests in bulk instead of yielding each on its own.
Is it possible to add them to scheduler without yielding them?
Thanks!