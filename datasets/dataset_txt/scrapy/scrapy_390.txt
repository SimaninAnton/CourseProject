Member
eliasdorneles commented on Apr 20, 2018
Hi folks!
So today I needed a simple way to evaluate the performance for all the pages of a website, and I thought scrapy would be a great tool for quickly visiting all the pages and getting this information.
Basically, I'm interested in the time of downloading a page, and maybe more details if possible (time to start a connection, and etc).
While it was fairly easy to get the spider visiting all the pages in no time (+700 pages in ~3 min), getting the response download times per URL seems quite a challenge.
I found this question in SO which made me feel less alone, but the recommended approach of using a downloader middleware in the existing answer doesn't really work because the requests are often processed way before they are actually sent.
I tried looking at the code to see where one would need to hook to get this information, I noticed that there is some code collecting time information (like here and here), but I didn't have time to dig much.
In the end, I gave up and went with a requests-based script that takes 30 minutes to run but it was easy to get the information, but I would much prefer to use Scrapy for this. =)
I'd love to hear if anyone has any thoughts about this -- thanks in advance!