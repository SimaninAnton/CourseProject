abeagomez commented on Nov 15, 2017
I have a .csv files and I need to fill some of my items with data from the .csv and some other with data from a list of URLs I will built using info from the csv. For an item to be send to the pipeline (and then written to the data base), seems like I need to yield it. I have been trying different approaches and so far, I don't know if this is a bug in Scrapy.
If I try yielding the items at every level, like this (commented lines means that I also tried that way):
def start_requests(self):
          #yield self.parse_csv()
          for item in self.parse_csv():
              yield item

def parse_csv(self):
        directory = os.path.join(os.path.dirname(__file__),"example.csv")
        with open(directory) as data:
            reader = csv.DictReader(data)
            for row in reader:
                #yield self.item_filler(row)
                for item in self.item_filler(row):
                    yield item

def item_filler(self, row, pin):
        item = ExampleItem()
        item['some_field'] = row['some_field']
        yield prop_item
I get the following exception:
2017-11-14 13:10:40 [twisted] CRITICAL: Unhandled Error
Traceback (most recent call last):
  [.... ... .... tracebacks .... ..... ....]
    raise AttributeError(name)
exceptions.AttributeError: dont_filter
If I try without the "yields" in the start_request and the parse functions, like this:
def start_requests(self):
          print("before parse")
          self.parse_csv()
          print("after parse")
        
def parse_csv(self):
        directory = os.path.join(os.path.dirname(__file__),"example.csv")
        with open(directory) as data:
            reader = csv.DictReader(data)
            print("Before cycle")
            for row in reader:
                self.item_filler(row)
            print("After cycle")

def item_filler(self, row, pin):
        item = ExampleItem()
        item['some_field'] = row['some_field']
        print("yielding")
        yield prop_item
I get something like:
[...]
2017-11-14 13:31:21 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
before parse
Before cycle
After cycle
after parse
2017-11-14 13:31:26 [scrapy.core.engine] INFO: Closing spider (finished)
[...]
No exceptions or errors, the item_filler function never was executed.
I'm trying to figure out how to solve this issue. Is this the expected behavior? Is this a bug? Thanks in advance.