dsandesari commented on Apr 12, 2017
Version:
Scrapy    : 1.1.1
lxml      : 3.6.1.0
libxml2   : 2.9.1
Twisted   : 16.3.0
Python    : 2.7.6 (default, Jun 22 2015, 17:58:13) - [GCC 4.8.2]
pyOpenSSL : 0.13 (OpenSSL 1.0.1f 6 Jan 2014)
Platform  : Linux-3.13.0-92-generic-x86_64-with-Ubuntu-14.04-trusty
Exception:
2017-04-07 07:00:21 [scrapy] DEBUG: Crawled (200) <GET http://www.xyz.com/people_directory/professional_profile/B-2-14893> (referer: http://www.xyz.com/people_directory/professional_profile/B-1-149)
2017-04-07 07:00:21 [scrapy] DEBUG: Scraped from <200 http://www.xyz.com/people_directory/professional_profile/B-2-14893>
{'url': 'http://www.xyz.com/people_directory/professional_profile/B-2-14893'}
2017-04-07 07:00:21 [scrapy] ERROR: Spider error processing <GET http://www.xyz.com/people_directory/professional_profile/B-2-14893> (referer: http://www.xyz.com/people_directory/professional_profile/B-1-149)
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
GeneratorExit
Exception RuntimeError: 'generator ignored GeneratorExit' in <generator object iter_errback at 0x7f767e516370> ignored
Unhandled error in Deferred:
2017-04-07 07:00:21 [twisted] CRITICAL: Unhandled error in Deferred:


Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py", line 1194, in run
    self.mainLoop()
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py", line 1203, in mainLoop
    self.runUntilCurrent()
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py", line 825, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py", line 671, in _tick
    taskObj._oneWorkUnit()
--- <exception caught here> ---
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 209, in crawl
    self.schedule(request, spider)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 215, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/scheduler.py", line 54, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/dupefilters.py", line 51, in request_seen
    self.fingerprints.add(fp)
exceptions.MemoryError: 
2017-04-07 07:00:21 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py", line 517, in _oneWorkUnit
    result = next(self._iterator)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/utils/defer.py", line 63, in <genexpr>
    work = (callable(elem, *args, **named) for elem in iterable)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/scraper.py", line 183, in _process_spidermw_output
    self.crawler.engine.crawl(request=output, spider=spider)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 209, in crawl
    self.schedule(request, spider)
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/engine.py", line 215, in schedule
    if not self.slot.scheduler.enqueue_request(request):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/core/scheduler.py", line 54, in enqueue_request
    if not request.dont_filter and self.df.request_seen(request):
  File "/usr/local/lib/python2.7/dist-packages/scrapy/dupefilters.py", line 51, in request_seen
    self.fingerprints.add(fp)
MemoryError