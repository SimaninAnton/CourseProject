ghost commented on Jun 26, 2017 â€¢
edited by ghost
2017-06-25 18:21:19 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://forum.wincenter.pl/> (failed 3 times): Connection was refused by other side: 111: Connection refused. 2017-06-25 18:21:19 [scrapy.core.scraper] ERROR: Error downloading <GET http://forum.wincenter.pl/> Traceback (most recent call last): File "/usr/local/lib/python3.5/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks result = result.throwExceptionIntoGenerator(g) File "/usr/local/lib/python3.5/site-packages/twisted/python/failure.py", line 393, in throwExceptionIntoGenerator return g.throw(self.type, self.value, self.tb) File "/usr/local/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request defer.returnValue((yield download_func(request=request,spider=spider))) twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.




Then scraper freezes for 30seconds - 1 minute and continues to crawl, then, after few websites, other or the same errors happen.
Shouldnt the site be skipped and crawl should continue?