Urahara commented on Feb 6, 2018 â€¢
edited
I have a server that have endpoints that call Scrapy using this below syntax, that i copied from reddit and use since then.
process = Popen(
    ['scrapy', 'crawl', 'product_list', '-s', 'LOG_ENABLED=False', '-t', 'json', '-o' '-',
        '-a', 'username={}'.format(username),
        '-a', 'password={}'.format(password),
        stdout=PIPE, stderr=PIPE)

stdout, _ = process.communicate()

stdout.decode("utf-8")

# parse string to json, code ommited.
My doubt is if have a better way to call my crawl and get the data returned. The above example that i use works but don't seens a good code.
UPDATE: I trying using CrawlerProcess but i don't know how to get items:
import scrapy
from scrapy.crawler import  CrawlerProcess
from scrapy.utils.project import get_project_settings

process = CrawlerProcess(get_project_settings())
process.crawl('product_list', **{ 'username' : 'test', 'password' : '123456'})
process.start()