zymtech commented on Jun 23, 2016
Hello,
I am new to scrapy , hope to find some help here. I was using scrapy with unstable proxies,spider always close early than expected for network problem. So, how to ignore all kinds of errors and let spider goes on. I have tried adding errback function to handle httperror, decreasing CONCURRENT_REQUESTS setting and increasing the DOWNLOAD_TIMEOUT setting,etc, it didn't work eventually. So, can anybody tell me how to handle errors like "user timeout caused connection failure"?
THANKS :) .