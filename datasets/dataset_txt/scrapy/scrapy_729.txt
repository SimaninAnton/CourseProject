mohmad-null commented on Jan 3, 2017
A feature request.
I've got my settings set up so that Scrapy is being nice and polite to any given site. For example:
# Very high
CONCURRENT_REQUESTS = 128

CONCURRENT_REQUESTS_PER_IP = 3
DOWNLOAD_DELAY = 2

AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_TARGET_CONCURRENCY = 3
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 15

#because I want to try and focus on breadth - I'm mostly making few a requests to many many domains
#With a few domains getting many requests
DEPTH_PRIORITY = 1
SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'
SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'
As you can see, for any given IP, scrapy is going to be slow, but in theory scrapy can handle many different IP's at once - that's why CONCURRENT_REQUESTS is very high. The problem is, it doesn't seem to.
Consider the following hypothetical queue (assume all domains are on different IPs):
example.com/1
example.com/2
example.com/3
example.com/4
example.com/5
example.com/6
example.com/7
example.com/8
google.example/1
google.example/2
google.example/3
google.example/4
google.example/5
google.example/6
google.example/7
blue.example
yellow.example
red.example
green.example
purple.example
pink.example
blank.example
Assuming scrapy is parsing the above top-down (and ignoring whether we're doing FIFO or reverse), what seems to happen is that scrapy will scrape all of the example.com pages first, then the google.example ones, in both instances hitting the per-ip/autothrottle limits and waiting for them to clear down.
However, the optimal way to do the above list is to note all the different domains, and then try and do them all concurrently (within the limit of CONCURRENT_REQUESTS). This doesn't appear to happen at present.
(At least, I'm guessing that's the reason I'm seeing very slow crawl rates, a low number of tcp/ip threads, as well as negligible CPU usage (<20% of a single core), and almost non-existent network usage (about 1MB per min - 130k bits/s!!!))
ðŸ‘ 2