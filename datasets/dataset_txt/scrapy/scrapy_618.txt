xhujerr commented on May 29, 2017
When I enable in settings.py
IMAGES_MIN_HEIGHT = 350
IMAGES_MIN_WIDTH = 350
I start getting unhandled exceptions:
Unhandled error in Deferred:
Failure: scrapy.pipelines.images.ImageException: Image too small (62x59 < 350x350)
The exception looks like this:
2017-05-28 19:06:43 [scrapy.pipelines.files] WARNING: File (error): Error processing file from <GET http://example.com/images/0.jpg> referred in <http://www.example.com/>: Image too small (600x200 < 350x350)
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/scrapy/pipelines/files.py", line 356, in media_downloaded
    checksum = self.file_downloaded(response, request, info)
  File "/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py", line 98, in file_downloaded
    return self.image_downloaded(response, request, info)
  File "/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py", line 102, in image_downloaded
    for path, image, buf in self.get_images(response, request, info):
  File "/usr/lib/python2.7/dist-packages/scrapy/pipelines/images.py", line 120, in get_images
    (width, height, self.min_width, self.min_height))
ImageException: Image too small (600x200 < 350x350)
When I comment except statements:
https://github.com/scrapy/scrapy/blob/1.3/scrapy/pipelines/files.py#L364
https://github.com/scrapy/scrapy/blob/1.3/scrapy/pipelines/files.py#L372
the exceptions aren't unhandled anymore.
It seems to me like a bug. Isn't is? Or am I doing something wrong?
It may be something similar to:
#1884
but in my case scrapy really crashes
python-w3lib: 1.17.0-1exp1
python-scrapy: 1.3.3-1exp1
both from a debian package
Python 2.7.10+
ðŸ‘ 2