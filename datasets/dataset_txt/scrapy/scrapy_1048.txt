jschilling1 commented on Jan 17, 2016
I need to call django.setup() to use django_item, when i do, Scrapy's memory usage keeps growing with the number of returned items.
My actual spider returns about 100 pages per html, but 1000 makes it grow to a gigabyte in minutes for the purposes of this.
def parse(self, response):
return ({'url': 'http://localhost/=%s' % randint(1, 9999999)} for i in xrange(1000))
Disabling 'debug_toolbar' in INSTALLED_APPS fixes this problem.
I'd really like to know how its affecting Scrapy. It took me a while to track it down django, guppy and trackrefs didn't point to a specific problem except a unicode() and dict() object count build up.