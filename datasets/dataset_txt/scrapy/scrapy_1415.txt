Contributor
redapple commented on Apr 25, 2014
Consider this example spider
from scrapy.spider import Spider
from scrapy.selector import Selector
from scrapy.http import Request

class DmozSpider(Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def start_requests(self):
        for url in self.start_urls:
            yield Request(
                url,
                #dont_filter=True
            )

    def parse(self, response):
        self.log("parse %r" % response.url)
and run it with CONCURRENT_REQUESTS=1 and DupeFilter enabled,
it will only visit http://www.dmoz.org/Computers/Programming/Languages/Python/Books/
Even with higher concurrency settings, it can happen that the spider is considered idle because the next request from start_requests iterator was filtered.
https://github.com/scrapy/scrapy/blob/master/scrapy/core/engine.py#L120 or https://github.com/scrapy/scrapy/blob/master/scrapy/core/engine.py#L155 seems to be missing a test on slot.start_requests is None