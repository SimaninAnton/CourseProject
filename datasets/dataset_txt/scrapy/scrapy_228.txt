HelloEdit commented on Jan 10, 2019
Hi,
I wanted to know if Scrapy had an internal mechanism, using Twisted or other, to make request without callback like this:
import scrapy
from scrapy import spiders, FormRequest


class PagesSpider(spiders.SitemapSpider):
    name = 'pages'


    sitemap_urls = ['http://site.com/sitemap.xml']


    def parse(self, response):
        id = response.css('span.id::text').extract_first()

        # here i would like to do something like this in JS
        # let [content, comments] = Promise.all([
        #         request('http://raw.mysite.com/{}'.format(id)),
        #         request('http://comments.mysite.com/{}'.format(id))
        # ])
        # Which is the equivalent of making several requests in parallel, waiting for it to be
        # executed and recovering the body of each one (Promise.all is like asyncio.gather I think)

        yield { 'id': id, 'content': content, 'comments': comments }
I don't know if I was clear, but for me, scrapy cruelly lacks the possibility of being able to make requests without callback directly in a function, in parallel if necessary and non-blocking.
Thanks,