Contributor
orangain commented on Feb 5, 2016
The BlogSpider seen at http://scrapy.org/ now crawls only the home page of https://blog.scrapinghub.com/. This is because links to archives on the sidebar of the blog have been replaced with a dropdown list.
Steps to Reproduce
As seen at http://scrapy.org/:
$ pip install scrapy
$ cat > myspider.py <<EOF
import scrapy

class BlogSpider(scrapy.Spider):
    name = 'blogspider'
    start_urls = ['http://blog.scrapinghub.com']

    def parse(self, response):
        for url in response.css('ul li a::attr("href")').re(r'.*/\d\d\d\d/\d\d/$'):
            yield scrapy.Request(response.urljoin(url), self.parse_titles)

    def parse_titles(self, response):
        for post_title in response.css('div.entries > ul > li a::text').extract():
            yield {'title': post_title}
EOF
$ scrapy runspider myspider.py
Expected Result
The spider crawls several tens of archive pages.
Actual Result
$ scrapy runspider myspider.py
2016-02-05 22:12:15 [scrapy] INFO: Scrapy 1.0.5 started (bot: scrapybot)
2016-02-05 22:12:15 [scrapy] INFO: Optional features available: ssl, http11
2016-02-05 22:12:15 [scrapy] INFO: Overridden settings: {}
2016-02-05 22:12:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2016-02-05 22:12:15 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2016-02-05 22:12:15 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2016-02-05 22:12:15 [scrapy] INFO: Enabled item pipelines:
2016-02-05 22:12:15 [scrapy] INFO: Spider opened
2016-02-05 22:12:15 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-02-05 22:12:15 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-02-05 22:12:15 [scrapy] DEBUG: Redirecting (301) to <GET https://blog.scrapinghub.com/> from <GET http://blog.scrapinghub.com>
2016-02-05 22:12:15 [scrapy] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/> (referer: None)
2016-02-05 22:12:15 [scrapy] INFO: Closing spider (finished)
2016-02-05 22:12:15 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 436,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 38260,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/301': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 2, 5, 13, 12, 15, 983486),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'response_received_count': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2016, 2, 5, 13, 12, 15, 383464)}
2016-02-05 22:12:15 [scrapy] INFO: Spider closed (finished)
Workaround
Replacing the CSS selector in the parse() method ul li a::attr("href") with option::attr("value") should fix the problem. However, extracting URLs from <option> elements seems a little bit funny as a tutorial.