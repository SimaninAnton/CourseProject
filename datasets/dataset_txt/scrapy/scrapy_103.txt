MikhailKavaliou commented on Jul 30, 2019
The RobotsTxtMiddleware is enabled with the ROBOTSTXT_OBEY=True.
Nonetheless, the built-in filter in the middleware passes URLs, that are basically disallowed.
For example there is the robots.txt rules like the following:
...
Disallow: /beta/
Disallow: /inventory/srp.html?*
Disallow: /search/*
...
In this case, all /beta/ links will be ignored as expected, but the 2 next rules do not filter correct URLs, because the asterisk at the end is interpreted as a symbol to be included in the URL.
Regarding that the condition in the RobotsTxtMiddleware#the process_request_2 is False for disallowed URLs:
if not rp.can_fetch(to_native_str(self._useragent), request.url):
            logger.debug("Forbidden by robots.txt: %(request)s",
                         {'request': request}, extra={'spider': spider})
            self.crawler.stats.inc_value('robotstxt/forbidden')
            raise IgnoreRequest("Forbidden by robots.txt")