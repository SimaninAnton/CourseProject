Contributor
pawelmhm commented on Aug 3, 2015
Looks like Scrapy is silently failing in case user attempts to make requests to url which meets two conditions:
request url is incorrect, e.g. no netloc
request url is outside allowed_domains
consider following simple spider
from scrapy import Spider, Request


class SimpleSpider(Spider):
    name = "simple"
    start_urls = [
        "http://httpbin.org/get"
    ]
    allowed_domains = ["httpbin.org"]

    def parse(self, response):
        url_not_allowed = "http:/github.com"
        yield Request(url_not_allowed)
requested url is not in allowed_domains and is additionally incorrect. When you run this spider, you get following output (silent failure)
2015-08-03 16:20:20 [scrapy] INFO: Spider opened
2015-08-03 16:20:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-08-03 16:20:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-08-03 16:20:21 [scrapy] DEBUG: Crawled (200) <GET http://httpbin.org/get> (referer: None)
2015-08-03 16:20:21 [scrapy] INFO: Closing spider (finished)
no exception is printed despite the fact that url is incorrect and not allowed.
Expected output: there should be either exception about incorrect url, or warning saying Filtered offsite request to 'github.com'.