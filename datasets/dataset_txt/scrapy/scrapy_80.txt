floggle commented on May 7, 2018
I was trying to extract links using the LinkExtractor (https://doc.scrapy.org/en/latest/topics/link-extractors.html) and some xpaths, and it's failing to find any links at all if I include an Xpath.
These two URL's:
https://coastalmap.marine.usgs.gov/mapservices/wms_svc.html
https://nationalmap.gov/small_scale/infodocs/webservices.html
link_extractor = LinkExtractor( restrict_xpaths=my_xpath) tmp_links = link_extractor.extract_links(response)
Even with something as simple as /html as the xpath, I get no links back.
If I remove the restrict_xpaths keyword arg, it works and returns all of the links on the page.
I understand that the HTML on those two pages seems a bit broken (I tried some online xpath extractors to see what was going on, they all choked on it), but in those circumstances I'd expect it to raise an exception or something to indicate the problem is with the page and not a valid-but-bad xpath which is what just I spent a bunch of time investigating.
Also a docs problem: the link extractors page (linked above) says this:
"restrict_xpaths (str or list) – is an XPath (or list of XPath’s) which defines regions inside the response where links should be extracted from. If given, only the text selected by those XPath will be scanned for links. See examples below."
But there are no "examples below", or anywhere.