rodrigogonegit commented on Apr 24, 2019
Hey, I'm wondering what would be the correct way of suspending the crawler in a scenario like this:
Crawl a table page per page
Stop once an item on the page has already been processed
Let's say I have 5 pages defined in the start_urls, these pages urls look something like:
target.com/list-0
target.com/list-50
target.com/list-100
target.com/list-150
target.com/list-200
The items on the pages change over time, the number means "0 to 49 most recent", "50 to 99 most recent", and so on.
Assuming the last processed item is on page 100, should I check for duplicates in the item Pipeline and stop the crawler there? To avoid the older page's items from being processed and not wasting the target's resources.