alanbchristie commented on Feb 23, 2019
https://www.idealista.it has a robots.txt which appears complex but essentially has the following: -
User-agent: *
Allow: /en/geo/
Scrapy (1.6.0) keeps telling me that where-ever I go on this site that I'm Forbidden by robots.tx: -
2019-02-23T11:06:44.226Z scrapy.downloadermiddlewares.robotstxt DEBUG # Forbidden by robots.txt: <GET https://www.idealista.it/en/geo/vendita-case/molise/>
I'm confused. I don't think I should blocked and I suspect that Scrapy may be thrown by other instructions in the robots.txt file.
I'm no expert by any means but when I validate an apparently legitimate URL (https://www.idealista.it/en/geo/vendita-case/molise/) using an independent tool like http://tools.seobook.com/robots-txt/analyzer/ (and I've tried more than one to gain confidence) I'm told...
Url: https://www.idealista.it/en/geo/vendita-case/molise/
Multiple robot rules found 
Robots allowed: All robots
So, is the robot.txt analysis in scrapy broken?
Scrapy tells me that everywhere on this site is blocked by the robots.txt. Just looking at the file myself, and not fully understanding the order of precedence, that just doesn't seem right.
If the answer is "Scrapy is correct" then why does it conflict with other analysers?
Is there more I need to configure in Scrapy?
Is there some middlewhere I'm missing?
And, most importantly, how do I continue to use Scrapy now and analyse sites like this? Suggestions I don't want are: circumvent robots with set ROBOTSTXT_OBEY = False or write your own robots.txt analyser.