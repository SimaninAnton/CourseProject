DANIYELS commented on May 31, 2017 â€¢
edited by redapple
Hi! I can't crawl https websites; http sites work just fine. I tried updating the scrapy.cfg file with DOWNLOADER_CLIENTCONTEXTFACTORY='testproject.CustomContext.CustomClientContextFactory' and creating the CustomContext.py file in the project directory, but it didn't solve the problem. See console log below.
I am grateful for solutions!
C:\Users\UserX\Documents\Python Scripts\Test\tutorials> **scrapy version -v**
Scrapy    : 1.3.3
lxml      : 3.7.2.0
libxml2   : 2.9.4
cssselect : 1.0.0
parsel    : 1.1.0
w3lib     : 1.17.0
Twisted   : 17.1.0
Python    : 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 11:57:41) [MSC v.1900 64 bit (AMD64)]
pyOpenSSL : 16.2.0 (OpenSSL 1.0.2k  26 Jan 2017)
Platform  : Windows-2012ServerR2-6.3.9600-SP0
C:\Users\UserX\Documents\Python Scripts\Test\tutorials> scrapy shell 'https://www.stackoverflow.com'

2017-05-31 13:18:28 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorials)
2017-05-31 13:18:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorials', 'DUPEFILTER_CLASS': 'scrapy.
dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'NEWSPIDER_MODULE': 'tutorials.spiders', 'ROBOTSTXT_OBEY': True, 'S
PIDER_MODULES': ['tutorials.spiders']}
2017-05-31 13:18:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2017-05-31 13:18:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-05-31 13:18:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-05-31 13:18:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-05-31 13:18:28 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-05-31 13:18:28 [scrapy.core.engine] INFO: Spider opened
2017-05-31 13:18:29 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stackoverflow.com/robots.txt>
(failed 1 times): Connection was refused by other side: 10061: No connection could be made because the target machine ac
tively refused it..
2017-05-31 13:18:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stackoverflow.com/robots.txt>
(failed 2 times): Connection was refused by other side: 10061: No connection could be made because the target machine ac
tively refused it..
2017-05-31 13:18:31 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://www.stackoverflow.com/robo
ts.txt> (failed 3 times): Connection was refused by other side: 10061: No connection could be made because the target ma
chine actively refused it..
2017-05-31 13:18:31 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://www.stackoverflow.com
/robots.txt>: Connection was refused by other side: 10061: No connection could be made because the target machine active
ly refused it..
Traceback (most recent call last):
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\twisted\internet\defer.py", line 1299,
in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\twisted\python\failure.py", line 393, i
n throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\scrapy\core\downloader\middleware.py",
line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: No connection could be made
because the target machine actively refused it..
2017-05-31 13:18:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stackoverflow.com> (failed 1 t
imes): Connection was refused by other side: 10061: No connection could be made because the target machine actively refu
sed it..
2017-05-31 13:18:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stackoverflow.com> (failed 2 t
imes): Connection was refused by other side: 10061: No connection could be made because the target machine actively refu
sed it..
2017-05-31 13:18:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://www.stackoverflow.com> (fa
iled 3 times): Connection was refused by other side: 10061: No connection could be made because the target machine activ
ely refused it..
Traceback (most recent call last):
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\Scripts\scrapy-script.py", line 5, in <module>
    sys.exit(scrapy.cmdline.execute())
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\scrapy\cmdline.py", line 142, in execut
e
    _run_print_help(parser, _run_command, cmd, args, opts)
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\scrapy\cmdline.py", line 88, in _run_pr
int_help
    func(*a, **kw)
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\scrapy\cmdline.py", line 149, in _run_c
ommand
    cmd.run(args, opts)
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\scrapy\commands\shell.py", line 73, in
run
    shell.start(url=url, redirect=not opts.no_redirect)
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\scrapy\shell.py", line 48, in start
    self.fetch(url, spider, redirect=redirect)
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\scrapy\shell.py", line 115, in fetch
    reactor, self._schedule, request, spider)
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\twisted\internet\threads.py", line 122,
 in blockingCallFromThread
    result.raiseException()
  File "C:\Users\UserX\AppData\Local\Continuum\Anaconda3\lib\site-packages\twisted\python\failure.py", line 372, i
n raiseException
    raise self.value.with_traceback(self.tb)
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061: No connection could be made
because the target machine actively refused it..
C:\Users\UserX\Documents\Python Scripts\Test\tutorials> scrapy shell 'http://www.stackoverflow.com'

2017-05-31 13:18:47 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorials)
2017-05-31 13:18:47 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorials', 'DUPEFILTER_CLASS': 'scrapy.
dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'NEWSPIDER_MODULE': 'tutorials.spiders', 'ROBOTSTXT_OBEY': True, 'S
PIDER_MODULES': ['tutorials.spiders']}
2017-05-31 13:18:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2017-05-31 13:18:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-05-31 13:18:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-05-31 13:18:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2017-05-31 13:18:48 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-05-31 13:18:48 [scrapy.core.engine] INFO: Spider opened
2017-05-31 13:18:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://stackoverflow.com/ro
bots.txt> from <GET http://www.stackoverflow.com/robots.txt>
2017-05-31 13:18:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://stackoverflow.com/r
obots.txt> from <GET http://stackoverflow.com/robots.txt>
2017-05-31 13:18:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://stackoverflow.com/robots.txt> (referer: None)

2017-05-31 13:18:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://stackoverflow.com/>
from <GET http://www.stackoverflow.com>
2017-05-31 13:18:48 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://stackoverflow.com/r
obots.txt> from <GET http://stackoverflow.com/robots.txt>
2017-05-31 13:18:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://stackoverflow.com/robots.txt> (referer: None)

2017-05-31 13:18:49 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://stackoverflow.com/>
 from <GET http://stackoverflow.com/>
2017-05-31 13:18:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://stackoverflow.com/> (referer: None)
2017-05-31 13:18:49 [traitlets] DEBUG: Using default logger
2017-05-31 13:18:49 [traitlets] DEBUG: Using default logger
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x000000ED0AEDB8D0>
[s]   item       {}
[s]   request    <GET http://www.stackoverflow.com>
[s]   response   <200 https://stackoverflow.com/>
[s]   settings   <scrapy.settings.Settings object at 0x000000ED0C022908>
[s]   spider     <DefaultSpider 'default' at 0xed0c2b9cc0>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
In [1]: