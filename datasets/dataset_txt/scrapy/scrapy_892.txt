rampage644 commented on Jun 24, 2016
Hi,
I was doing broad crawl and noticed constantly increasing memory consumption for a spider. Pruning my spider to most simple form doesn't help me here (memory still increases constantly).
I also noticed that others spiders (with much smaller crawl rates, CONCURRENT_REQUESTS = 16) don't have such problem.
So i was wondering if I misuse scrapy or there is a problem. Brief issue search didn't show anything, so I went ahead and created experimental spider for tests: https://github.com/rampage644/experimental
First, I'd like to know if someone has experienced memory problems with high rate crawl or another memory problem.
Second, I'd like to figure out why this simple spider leaks and can we do anything about that?