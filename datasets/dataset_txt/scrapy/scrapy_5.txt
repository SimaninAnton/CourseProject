wahidaa commented 28 days ago
Hi everyone ,
i'm beginner with scrapy
i'm trying to scrape multiple pages
but it doesn't work with me
this is the spider:
import scrapy
class pageSpider(scrapy.Spider):
name = 'page'
allowed_domains = ['www.bureauxlocaux.com']
start_url = ['https://www.bureauxlocaux.com/immobilier-d-entreprise/annonces/boulogne-billancourt-92100/location-bureaux']
def parse(self, response):
for product in response.xpath('//h4[@Class="item-card__title"]'):
yield {
'title' : product.xpath('.//span/text()').extract_first()
}
next_url_path = response.xpath('.//a[@Class="next-page-selector"]/@href').extract_first()
if next_url_path:
yield scrapy.Request(response.urljoin(next_url_path),callback=self.parse)
and this is the output:
2020-01-10 09:58:55 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: pages)
2020-01-10 09:58:55 [scrapy.utils.log] INFO: Versions: lxml 4.4.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.5.2 (default, Oct 8 2019, 13:06:37) - [GCC 5.4.0 20160609], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d 10 Sep 2019), cryptography 2.8, Platform Linux-4.4.0-170-generic-x86_64-with-Ubuntu-16.04-xenial
2020-01-10 09:58:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'pages', 'ROBOTSTXT_OBEY': True, 'NEWSPIDER_MODULE': 'pages.spiders', 'FEED_FORMAT': 'csv', 'SPIDER_MODULES': ['pages.spiders'], 'FEED_URI': 'page.csv'}
2020-01-10 09:58:55 [scrapy.extensions.telnet] INFO: Telnet Password: fc54acd3636fa0d8
2020-01-10 09:58:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
'scrapy.extensions.corestats.CoreStats',
'scrapy.extensions.logstats.LogStats',
'scrapy.extensions.feedexport.FeedExporter',
'scrapy.extensions.memusage.MemoryUsage']
2020-01-10 09:58:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
'scrapy.downloadermiddlewares.retry.RetryMiddleware',
'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-01-10 09:58:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
'scrapy.spidermiddlewares.referer.RefererMiddleware',
'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-01-10 09:58:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2020-01-10 09:58:55 [scrapy.core.engine] INFO: Spider opened
2020-01-10 09:58:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-01-10 09:58:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2020-01-10 09:58:55 [scrapy.core.engine] INFO: Closing spider (finished)
2020-01-10 09:58:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.004772,
'finish_reason': 'finished',
'finish_time': datetime.datetime(2020, 1, 10, 8, 58, 55, 842405),
'log_count/INFO': 10,
'memusage/max': 53555200,
'memusage/startup': 53555200,
'start_time': datetime.datetime(2020, 1, 10, 8, 58, 55, 837633)}
2020-01-10 09:58:55 [scrapy.core.engine] INFO: Spider closed (finished)
any hel please !!!