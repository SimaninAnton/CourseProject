ghost commented on Dec 17, 2012
The crawl spider works fine because the logic of following links is already implemented in that with rules.
But i am inheriting from InitSpider and that don't have the rules option so i manually have to follow the links.
But then i get stuck whether i should return my item or Request.
My problem is written below
I am using InitSpider for my scraping.
I have the list page where
I want to grab the list item and then makinga request to go into that list page and populate my item and then resturning my that item too save in pipeline
I also grab the next page link so that i can further follow the link
But i am stuck at this
    def parse(self, response):
        soup = BeautifulSoup(response.body)
        hxs = HtmlXPathSelector(response)
        sites = hxs.select('//div[@class="row"]')
        items = []

        for site in sites[:5]:
            item = TestItem()
            item['username'] = "test5"
            request =  Request("http://www.example.org/profile.php",  callback = self.parseUserProfile)
            request.meta['item'] = item
            **yield item**

        mylinks= soup.find_all("a", text="Next")
        if mylinks:
            nextlink = mylinks[0].get('href')
            yield Request(urljoin(response.url, nextlink), callback=self.parse)

    def parseUserProfile(self, response):
        item = response.meta['item']
        item['image_urls'] = "test3"
        return item
Now my above works but with that i am not getting value of item['image_urls'] = "test3"
It is coming as null
Now if use return request instead ofyield item
Then get error thatcannot use return with generator
If i remove this line
yield Request(urljoin(response.url, nextlink), callback=self.parse)Then my code works fine and i can get image_urls but then i canot follow the links
So is there any way so that i can use return request and yield together so that i get the item_urls