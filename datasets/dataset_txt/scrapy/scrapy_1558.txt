Contributor
aivarsk commented on Jan 24, 2013
I'm using Scrapy with a random proxy from a list of verified proxies. By the time Scrapy uses them some proxies are already closed/disabled/reconfigured. If connection fails Scrapy retries the same URL (middleware chooses another proxy). However if proxy returns invalid content and decompression fails Scrapy does not retry the same URL. I get the following error:
ERROR: Error downloading <GET http://xxx>: Error -3 while decompressing: invalid stored block lengths
Currently I've solved this by disabling HttpCompressionMiddleware. Is it possible to retry on such errors as well? Or turn off compression for host(+proxy) and retry?
Scrapy : 0.17.0-108-g71d7df9
lxml : 2.3.2.0
libxml2 : 2.7.8
Twisted : 11.1.0
Python : 2.7.3 (default, Aug 1 2012, 05:16:07) - [GCC 4.6.3]
Platform: Linux-3.2.0-26-generic-pae-i686-with-Ubuntu-12.04-precise