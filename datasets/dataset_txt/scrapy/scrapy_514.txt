tobads commented on Oct 28, 2017
I want to obtain some data from a website, so I wrote a Scrapy project
Code below:
settings.py
CONCURRENT_REQUESTS = 4
DOWNLOAD_TIMEOUT = 5
TestSpiders.py
import scrapy
from datetime import datetime
from scrapy.linkextractors import LinkExtractor

class TestSpider(scrapy.Spider):
    name = "Test"
    allowed_domains = ["scrapy.org"]
    start_urls = ["https://doc.scrapy.org"]

    def parse(self, response):
        time_start = datetime.now()
        
        links = LinkExtractor(allow=()).extract_links(response)
        for link in links:
            yield scrapy.http.Request(url = link.url)
            
        print "parse method time consumed: ", (datetime.now() - time_start).total_seconds(), "seconds\n"
Output
2017-10-28 16:50:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/1.4/topics/signals.html> (referer: https://doc.scrapy.org/en/1.4/intro/overview.html)
2017-10-28 16:50:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/latest/_static/selectors-sample1.html> (referer: None)
parse method time consumed:  0.106 seconds

2017-10-28 16:50:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/1.4/news.html> (referer: https://doc.scrapy.org/en/1.4/intro/overview.html)
2017-10-28 16:50:29 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite request to 'example.com': <GET http://example.com/image1.html>
parse method time consumed:  0.003 seconds

parse method time consumed:  0.072 seconds

parse method time consumed:  0.357 seconds

2017-10-28 16:50:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.scrapy.org/en/master/genindex.html> (referer: https://docs.scrapy.org/en/master/)
2017-10-28 16:50:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/1.4/topics/djangoitem.html#topics-djangoitem> (referer: https://doc.scrapy.org/en/1.4/news.html)
parse method time consumed:  0.044 seconds

parse method time consumed:  0.347 seconds

2017-10-28 16:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.scrapy.org/en/master/versioning.html> (referer: https://docs.scrapy.org/en/master/)
parse method time consumed:  0.051 seconds

2017-10-28 16:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.scrapy.org/en/master/py-modindex.html> (referer: https://docs.scrapy.org/en/master/)
parse method time consumed:  0.073 seconds
As you can see, everything is fine.
But now I want to improve this project, not only obtaining data, but also parsing it, storing it, using it.
Anyway, it's a time-consuming job.
So TestSpiders.py need to be changed a little bit
TestSpiders.py
import time
import scrapy
from datetime import datetime
from scrapy.linkextractors import LinkExtractor

class TestSpider(scrapy.Spider):
    name = "Test"
    allowed_domains = ["scrapy.org"]
    start_urls = ["https://doc.scrapy.org"]

    def parse(self, response):
        time_start = datetime.now()
        
        links = LinkExtractor(allow=()).extract_links(response)
        for link in links:
            yield scrapy.http.Request(url = link.url)
            
        time.sleep(10) #simulating time-consuming job
        print "parse method time consumed: ", (datetime.now() - time_start).total_seconds(), "seconds\n"
Output
2017-10-28 17:00:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/latest/> (referer: https://doc.scrapy.org/en/latest/)
2017-10-28 17:00:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/latest/topics/webservice.html> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/latest/topics/webservice.html took longer than 5.0 seconds..
2017-10-28 17:00:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/latest/topics/telnetconsole.html> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/latest/topics/telnetconsole.html took longer than 5.0 seconds..
2017-10-28 17:00:18 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/latest/topics/email.html> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/latest/topics/email.html took longer than 5.0 seconds..
parse method time consumed:  10.071 seconds

2017-10-28 17:00:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/master/> (referer: https://doc.scrapy.org/en/latest/)
2017-10-28 17:00:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/xpath-tutorial/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/xpath-tutorial/ took longer than 5.0 seconds..
2017-10-28 17:00:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.9/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.9/ took longer than 5.0 seconds..
2017-10-28 17:00:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.10.3/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.10.3/ took longer than 5.0 seconds..
parse method time consumed:  10.054 seconds

2017-10-28 17:00:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://doc.scrapy.org/en/0.12/> (referer: https://doc.scrapy.org/en/latest/)
2017-10-28 17:00:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.14/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.14/ took longer than 5.0 seconds..
2017-10-28 17:00:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.16/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.16/ took longer than 5.0 seconds..
2017-10-28 17:00:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://doc.scrapy.org/en/0.18/> (failed 1 times): User timeout caused connection failure: Getting https://doc.scrapy.org/en/0.18/ took longer than 5.0 seconds..
parse method time consumed:  10.028 seconds
Oops, so many Retrying.
You can easily find the pattern, always a Crawled and then 3 Retrying.
Why?
Because 3 + 1 = 4, and CONCURRENT_REQUESTS = 4 in settings.py
So when spider processing a response, other 3 concurrent_requests are timeout.
So, I am curious, while spider processing a response, what other concurrent_requests are doing?
Are they still receive data from server? If so, when all data was received, they should generate a response, and wait for spider to finish processing current response, and then send response to spider. In this case, timeout shouldn't happen.
If they are not receive data from server? Why not?