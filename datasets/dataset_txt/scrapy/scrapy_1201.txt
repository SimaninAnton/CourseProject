seozed commented on Jun 12, 2015
My spider returns the error.
I tried to add sys.setrecursionlimit(10000). but not work.
2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [scrapy] NOLEVEL: Current UA:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3
2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [scrapy] NOLEVEL: Current UA:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3
2015-06-12 11:49:52+0800 [-] ERROR: Unable to format event {'log_namespace': 'stderr', 'log_level': <LogLevel=error>, 'format': '%(log_legacy)s', 'log_logger': <Logger 'stderr'>, 'log_source': None, 'system': '-', 'log_io': u'2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [-] ERROR: 2015-06-12 11:49:52+0800 [scrapy] NOLEVEL: Current UA:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3', 'time': 1434080992.928, 'log_format': u'{log_io}', 'message': (), 'log_time': 1434080992.928}: maximum recursion depth exceeded while calling a Python object
2015-06-12 11:49:52+0800 [-] ERROR: Temporarily disabling observer LegacyLogObserverWrapper(<bound method ScrapyFileLogObserver._emit_with_crawler of <scrapy.log.ScrapyFileLogObserver instance at 0x05626210>>) due to exception: [Failure instance: Traceback: <type 'exceptions.RuntimeError'>: maximum recursion depth exceeded in __instancecheck__
    D:\Python27\lib\site-packages\twisted\python\log.py:557:emit
    D:\Python27\lib\site-packages\twisted\python\util.py:830:untilConcludes
    D:\Python27\lib\site-packages\twisted\logger\_io.py:170:write
    D:\Python27\lib\site-packages\twisted\logger\_logger.py:132:emit
    --- <exception caught here> ---
    D:\Python27\lib\site-packages\twisted\logger\_observer.py:131:__call__
    D:\Python27\lib\site-packages\twisted\logger\_legacy.py:93:__call__
    D:\Python27\lib\site-packages\scrapy\log.py:52:_emit_with_crawler
    D:\Python27\lib\site-packages\scrapy\log.py:48:_emit
    D:\Python27\lib\site-packages\twisted\python\log.py:557:emit
    D:\Python27\lib\site-packages\twisted\python\util.py:830:untilConcludes
    D:\Python27\lib\site-packages\twisted\logger\_io.py:170:write
    D:\Python27\lib\site-packages\twisted\logger\_logger.py:132:emit
    D:\Python27\lib\site-packages\twisted\logger\_observer.py:140:__call__
    D:\Python27\lib\site-packages\twisted\logger\_logger.py:178:failure
    D:\Python27\lib\site-packages\twisted\logger\_logger.py:132:emit
    D:\Python27\lib\site-packages\twisted\logger\_observer.py:133:__call__
    D:\Python27\lib\site-packages\twisted\python\failure.py:203:__init__
    ]
    Traceback (most recent call last):
      File "D:\Python27\lib\site-packages\twisted\python\log.py", line 557, in emit
        util.untilConcludes(self.write, timeStr + " " + msgStr)
      File "D:\Python27\lib\site-packages\twisted\python\util.py", line 830, in untilConcludes
        return f(*a, **kw)
      File "D:\Python27\lib\site-packages\twisted\logger\_io.py", line 170, in write
        self.log.emit(self.level, format=u"{log_io}", log_io=line)
      File "D:\Python27\lib\site-packages\twisted\logger\_logger.py", line 132, in emit
        self.observer(event)
    --- <exception caught here> ---
      File "D:\Python27\lib\site-packages\twisted\logger\_observer.py", line 131, in __call__
        observer(event)
      File "D:\Python27\lib\site-packages\twisted\logger\_legacy.py", line 93, in __call__
        self.legacyObserver(event)
      File "D:\Python27\lib\site-packages\scrapy\log.py", line 52, in _emit_with_crawler
        ev = self._emit(eventDict)
      File "D:\Python27\lib\site-packages\scrapy\log.py", line 48, in _emit
        log.FileLogObserver.emit(self, ev)
      File "D:\Python27\lib\site-packages\twisted\python\log.py", line 557, in emit
        util.untilConcludes(self.write, timeStr + " " + msgStr)
      File "D:\Python27\lib\site-packages\twisted\python\util.py", line 830, in untilConcludes
        return f(*a, **kw)
      File "D:\Python27\lib\site-packages\twisted\logger\_io.py", line 170, in write
        self.log.emit(self.level, format=u"{log_io}", log_io=line)
      File "D:\Python27\lib\site-packages\twisted\logger\_logger.py", line 132, in emit
        self.observer(event)
      File "D:\Python27\lib\site-packages\twisted\logger\_observer.py", line 140, in __call__
        observer=brokenObserver,
      File "D:\Python27\lib\site-packages\twisted\logger\_logger.py", line 178, in failure
        self.emit(level, format, log_failure=failure, **kwargs)
      File "D:\Python27\lib\site-packages\twisted\logger\_logger.py", line 132, in emit
        self.observer(event)
      File "D:\Python27\lib\site-packages\twisted\logger\_observer.py", line 133, in __call__
        brokenObservers.append((observer, Failure()))
      File "D:\Python27\lib\site-packages\twisted\python\failure.py", line 203, in __init__
        if isinstance(exc_value, str) and exc_type is None:
    exceptions.RuntimeError: maximum recursion depth exceeded in __instancecheck__
my spider code:
import json
import sys
from scrapy.spider import BaseSpider
from scrapy.http import Request
from soufang.items import SoufangItem
from scrapy import log
reload(sys)
sys.setdefaultencoding('utf8')

class SoufangSpider(BaseSpider):

    name = 'soufang'
    allowed_domains = ['fang.com']
    start_urls = ['http://fang.com/SoufunFamily.htm']

    log.start('crawl.log', loglevel=log.DEBUG)

    def parse(self, response):
        """分析城市列表页，提取城市关键词，并构造json页面请求"""

        for link in response.css('table#senfe1 tr td a').xpath('@href').extract():
            self.rnum = True
            pagenum = 0

            for i in xrange(300):
                if not link:
                    continue
                city = link.split('//')[1].split('.')[0]
                pagenum += 1
                cityRootUrl = r'http://newhouse.%s.fang.com/house/s/?x1=111.168964&x2=115.446336&y1=21.921421&y2=24.307946&strDistrict=&strRoundStation=&railway=&strPurpose=&strPrice=&strHuxing=&saling=&strStartDate=&isyouhui=&strOrderBy=&strKeyword=&railway_station=&strComarea=&housetag=&strSort=mobileyh&a=ajaxXfMapSearch&city=%s&PageNo=%d' % (city, city, pagenum)
                yield Request(cityRootUrl, callback=self.parse_json)

                if not self.rnum:
                    break


    def parse_json(self, response):
        """解析JSON数据"""

        if not response.body:
            return
        response_data = json.loads(response.body)

        print response_data['rnum']
        if response_data['rnum'] <= 1:
            self.rnum = False
            return
        for houses in response_data['list']:
            item = SoufangItem()
            item.setdefault('title','Null')
            item.setdefault('x','Null')
            item.setdefault('y','Null')
            item.setdefault('district','Null')
            item.setdefault('address','Null')
            item.setdefault('newCode','Null')
            item.setdefault('houseurl','Null')
            item.setdefault('purpose','Null')
            item.setdefault('price_num','Null')
            item.setdefault('price_unit','Null')
            item.setdefault('tel400','Null')
            item.setdefault('tagcount','Null')
            item.setdefault('tags','Null')
            item.setdefault('alias','Null')
            item.setdefault('city','Null')
            item.setdefault('propertyCategory','Null')
            item.setdefault('projectFeatures','Null')
            item.setdefault('decorateCondition','Null')
            item.setdefault('loopLineLocal','Null')
            item.setdefault('CBDCategory','Null')
            item.setdefault('plotRatio','Null')
            item.setdefault('landscapingRatio','Null')
            item.setdefault('openingDate','Null')
            item.setdefault('checkInDate','Null')
            item.setdefault('propertyCompany','Null')
            item.setdefault('developers','Null')
            item.setdefault('saleAddress','Null')
            item.setdefault('propertyAddress','Null')
            item.setdefault('floorSpace','Null')
            item.setdefault('areaOfStructure','Null')
            item.setdefault('equityYears','Null')
            item.setdefault('agent','Null')
            item.setdefault('investor','Null')
            item.setdefault('projectState','Null')
            item.setdefault('completionTime','Null')
            item.setdefault('projectConfig','Null')
            item.setdefault('trafficInfo','Null')
            item.setdefault('storeInfo','Null')
            item.setdefault('carportInfo','Null')
            item.setdefault('projectIntro','Null')
            item.setdefault('storeNumber','Null')
            item.setdefault('projectPics','Null')
            item.setdefault('developersAbout','Null')
            item.setdefault('buildingCategory','Null')
            item.setdefault('licence','Null')
            item.setdefault('decoreteMaterial','Null')
            item.setdefault('commercialSpace','Null')
            item.setdefault('saleState','Null')
            item.setdefault('usedRatio','Null')
            item.setdefault('ROI','Null')
            item.setdefault('storeyModelSpace','Null')
            item.setdefault('StoreySpace','Null')
            item.setdefault('officeSpace','Null')
            item.setdefault('houseSpace','Null')
            item.setdefault('houseFacility','Null')
            item.setdefault('targetState','Null')
            item.setdefault('stateArrange','Null')
            item.setdefault('developersPageUrl','Null')
            item['title'] = houses.setdefault('title', 'Null')
            item['x'] = houses.setdefault('x', 'Null')
            item['y'] = houses.setdefault('y', 'Null')
            item['district'] = houses.setdefault('district', 'Null')
            item['address'] = houses.setdefault('address', 'Null')
            item['newCode'] = houses.setdefault('newCode', 'Null')
            item['houseurl'] = houses.setdefault('houseurl', 'Null')
            item['purpose'] = houses.setdefault('purpose', 'Null')
            item['price_num'] = houses.setdefault('price_num', 'Null')
            item['price_unit'] = houses.setdefault('price_unit', 'Null')
            item['tel400'] = houses.setdefault('tel400', 'Null')
            item['tagcount'] = houses.setdefault('tagcount', 'Null')
            if not houses['houseurl']:
                from scrapy.shell import inspect_response
                inspect_response(response, self)
            detailurl = '{0[houseurl]}house/{0[newCode]}/housedetail.htm'.format(houses)
            yield Request(detailurl, meta={'item': item}, callback=self.parse_detail)


    def parse_detail(self, response):
        """解析楼盘详情页，最多字段的页面"""

        item = response.meta['item']
        fields = {
            u'标签组': 'tags',
            u'别名': 'alias',
            u'物业类别': 'propertyCategory',
            u'写字楼类型': 'propertyCategory',
            u'商铺类型': 'propertyCategory',
            u'项目特色': 'projectFeatures',
            u'装修状况': 'decorateCondition',
            u'环线位置': 'loopLineLocal',
            u'所属商圈': 'CBDCategory',
            u'容积率': 'plotRatio',
            u'绿 化 率': 'landscapingRatio',
            u'开盘时间': 'openingDate',
            u'交房时间': 'checkInDate',
            u'入住时间': 'checkInDate',
            u'住房时间': 'checkInDate',
            u'物业管理公司': 'propertyCompany',
            u'物业公司': 'propertyCompany',
            u'开 发 商': 'developers',
            u'开发商': 'developers',
            u'售楼地址': 'saleAddress',
            u'物业地址': 'propertyAddress',
            u'占地面积': 'floorSpace',
            u'建筑面积': 'areaOfStructure',
            u'产权年限': 'equityYears',
            u'产权描述': 'equityYears',
            u'代理商': 'agent',
            u'投资商': 'investor',
            u'工程进度': 'projectState',
            u'竣工时间': 'completionTime',
            u'项目配套': 'projectConfig',
            u'交通状况': 'trafficInfo',
            u'楼层状况': 'storeInfo',
            u'车位信息': 'carportInfo',
            u'项目简介': 'projectIntro',
            u'商铺总套数': 'storeNumber',
            u'楼盘图片': 'projectPics',
            '#住宅类': None,
            u'建筑类别': 'buildingCategory',
            u'预售许可证': 'licence',
            u'建材装修': 'decoreteMaterial',
            '#商铺类': None,
            u'商业面积': 'commercialSpace',
            u'租售状态': 'saleState',
            u'使用率': 'usedRatio',
            u'投资回报率': 'ROI',
            u'标准层面积': 'storeyModelSpace',
            u'单层商铺面积': 'StoreySpace',
            u'办公面积': 'officeSpace',
            u'单套面积': 'houseSpace',
            u'内部设施': 'houseFacility',
            u'目标业态': 'targetState',
            u'业态规划': 'stateArrange',
        }
        #提取开发商的链接
        tmep_companys = {
                    u'开发商': None,
                    u'投资商': None,
                    u'代理商': None,
                    u'物业管理公司': None,
                    u'物业公司': None,
                    }
        #标签组
        item['tags'] = self.filterBlank(response.css('.lpicon.tf a').xpath('text()').extract())
        #别名
        item['alias'] = self.filterBlank(response.css('.h1_label').xpath('text()').extract()).replace(u'别名：', '')
        #当前城市
        item['city'] = self.filterBlank(response.css('div.s2 .s4Box a').xpath('text()').extract())
        #主要信息所在模块
        content = response.css('.besic_inform')


        # 基本信息块
        for info_field in content.css('table').xpath('tr/td'):
            field_title = self.filterBlank(info_field.xpath('strong/text()').extract())
            field_text = self.filterBlank(info_field.xpath('text()').extract())
            #如果字段名称在字典内,就存储他的文本
            if fields.has_key(field_title):
                item[fields[field_title]] = field_text
            if field_title == u'所属商圈':
                item['CBDCategory']= self.filterBlank(info_field.xpath('span/a/text()').extract())
                print item['CBDCategory']

        #项目配套
        item['projectConfig'] = self.filterBlank(content.css('#xq_xmpt_anchor+div').xpath('text()').extract(), separator='\n')
        #交通状况
        item['trafficInfo'] = self.filterBlank(content.css('#xq_jtzk_anchor+div').xpath('text()').extract(), separator='\n')
        #车位信息
        item['carportInfo'] = self.filterBlank(content.css('#xq_cwxx_anchor+div').xpath('text()').extract(), separator='\n')
        #项目简介
        item['projectIntro'] = self.filterBlank(content.css('#xq_xmjs_anchor+div').xpath('text()').extract(), separator='\n')
        #内部设施/建材装修
        houseFacility = []

        #遍历、抽取字符串，最后再用换行符拼接字符串
        for tag in content.css('#xq_jczx_anchor+div strong'):
            houseFacility.append('{0}{1}'.format(
                self.filterBlank(tag.xpath('text()[1]').extract()),
                self.filterBlank(tag.xpath('following-sibling::text()[1]').extract())
            ))

        item['houseFacility'] = '\n'.join(houseFacility)
        #相关信息
        for tag in content.css('#xq_xgxx_anchor+div strong'):
            s = self.filterBlank(tag.xpath('text()').extract()).replace(':', '')
            if fields.has_key(s):
                item[fields[s]] = self.filterBlank(tag.xpath('following-sibling::text()[1]').extract())
                if s in tmep_companys:
                    item[fields[s]] = self.filterBlank(tag.xpath('following-sibling::a[1]/text()').extract())
                    tmep_companys[s] = self.filterBlank(tag.xpath('following-sibling::a[1]/@href').extract())
        item['developersPageUrl'] = tmep_companys[u'开发商']
        yield Request(tmep_companys[u'开发商'], meta={'item': item}, callback=self.parse_companypage)

    def parse_companypage(self, response):
        """
        解析公司detail页面模块,目前，只解析开发商页面
        """

        item = response.meta['item']
        temp_str_company = u'公司简介'
        item['developersAbout'] = self.filterBlank(response.xpath("//p/span[contains(text(),'%s')]/following-sibling::text()[1]" % temp_str_company).extract())
        yield Request(item['houseurl'], meta={'item': item}, callback=self.parsepic)

    def parsepic(self, response):
        """收集楼盘图片，跳到楼盘主页收集页面轮播的图片。"""

        item = response.meta['item']
        item['projectPics'] = self.filterBlank(response.css('ul#imageShowSmall li img').xpath('@src').extract())
        print 'done'
        yield item

    def filterBlank(self, l, separator=','):
        """过滤空白标签且合并所有列表项"""
        return separator.join(l).strip()