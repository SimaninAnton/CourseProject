wangjingpei01 commented on Dec 18, 2017 â€¢
edited
it's in centos env, when we exec scrapy fetch url or we scrapy crawl ourproject, it always coredump, following is the result:(help-wanted)
2017-12-18 12:03:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 578,
'downloader/request_count': 2,
'downloader/request_method_count/GET': 2,
'downloader/response_bytes': 33201,
'downloader/response_count': 2,
'downloader/response_status_count/200': 1,
'downloader/response_status_count/302': 1,
'finish_reason': 'finished',
'finish_time': datetime.datetime(2017, 12, 18, 4, 3, 12, 341518),
'log_count/DEBUG': 1,
'log_count/INFO': 1,
'memusage/max': 43687936,
'memusage/startup': 43687936,
'response_received_count': 1,
'scheduler/dequeued': 2,
'scheduler/dequeued/memory': 2,
'scheduler/enqueued': 2,
'scheduler/enqueued/memory': 2,
'start_time': datetime.datetime(2017, 12, 18, 4, 3, 8, 952436)}
Segmentation fault (core dumped)
since I can't parse the core file, so I don't know how to fix this prob, please hlep, thanks