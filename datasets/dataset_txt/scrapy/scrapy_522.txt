addisonwebb commented on Oct 20, 2017
Disclaimer: I am very new to the Python world so this issue might be caused by me not understanding how the pieces fit together. ðŸ™ƒ
Background
I have create a couple Spiders and I would like to start them using a script. (vs having to start them individually from the command line) I tried to follow the example in the Scrapy docs here. I am confident both of my spiders work as I would expect. When I run them like this, scrapy crawl data_1, I get the behavior I expect.
When I run the script I get this error:
$ python crawlScript.py 
Traceback (most recent call last):
  File "crawlScript.py", line 3, in <module>
    import scrapy
ImportError: No module named scrapy
Any help figuring out the issue would be greatly appreciated!
Script Content
#!/usr/bin/env python

import scrapy

from scrapy.crawler import CrawlerProcess

# import spiders
from spiders.data_1 import Data1Spider

process = CrawlerProcess(get_project_settings())

process.crawl(Data1Spider)
process.start()
Project Structure
MyScraper
|____.DS_Store
|______init__.py
|______init__.pyc
|____crawlScript.py     <--- my script
|____items.py
|____items.pyc
|____middlewares.py
|____pipelines.py
|____pipelines.pyc
|____settings.py
|____settings.pyc
|____spiders
| |____.DS_Store
| |______init__.py
| |______init__.pyc
| |____data_1.py        <--- spider file 1
| |____data_1.pyc
| |____data_2.py       <--- spider file 2
| |____data_2.pyc
Other Info
Python version: 2.7.10
Scrapy version: 1.4.0
OS: macOS 10.12.6