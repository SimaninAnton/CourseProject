nautilus28 commented on Jul 7, 2015
When I ran a simple spider on my virtualenv with boto installed, although I don't use any S3 related middlewares, Scrapy raised this exception:
Traceback (most recent call last):
File "/.../lib/python2.7/site-packages/boto/utils.py", line 210, in retry_url
r = opener.open(req, timeout=timeout)
File "/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py", line 431, in open
response = self._open(req, data)
File "/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py", line 449, in _open
'_open', req)
File "/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py", line 409, in _call_chain
result = func(*args)
File "/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py", line 1227, in http_open
return self.do_open(httplib.HTTPConnection, req)
File "/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib2.py", line 1197, in do_open
raise URLError(err)
URLError:
2015-07-07 09:20:26 [boto] ERROR: Unable to read instance data, giving up
environment:
OS X Yosemite 10.10.4
python 2.7.10
scrapy 1.0.1
boto 2.38.0
Any helps?
Thanks in advance,
Canh