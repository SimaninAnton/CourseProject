amirduran commented on Jun 24, 2019 â€¢
edited
This is question to understand deeper scrapy engine. On my server I can run multiple scrapy processes in parallel (we can start it by using scrapy API or just starting two scrapy instances in command line). That sounds good, but let's refer to the following scrapy architecture photo:
As far as I can understand, every scrapy process (in my case 2 processes) will have its own engine, scheduler, downloader etc. There is option to instruct/configure scrapy process to perform multiple crawls in parallel. This means if I instruct every process to run 50 requests in parallel my server will do 100 requests in parallel (because I have 2 processes running). What is the best way to instruct scrapy to run parallel crawls? Should I define all URLs in start URLs and then scrapy's scheduler will schedule crawling automatically for me or there is another way?
Also, if I have a mixture of 2 domains in my start urls attribute: 20 URLS belonging domain A and 50 URLs belonging domain B and if I configure scrapy that maximum 10 request in parallel should be performed on domain A and 5 on domain B, is scrapy's scheduler going to recognise this? Is there also way to say, if there are X different domains available in start URLs please always send max 10 requests in parallel for each of X domains?
Thx for answering my questions.