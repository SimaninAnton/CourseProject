petulla commented on 28 Oct 2019 â€¢
edited
I've been stuck on Step 2 of the entity linking docs and could use some help. I'm sure I'm making a basic error.
How to reproduce the problem
I create the knowledge base and training data. This seemed to work as it said "Done!" after running for 12 hours.
python wikidata_pretrain_kb.py 'latest-all.json.bz2' 'enwiki-latest-pages-articles-multistream.xml.bz2' './output2' 'en_core_web_lg'
The second step is dying off for me. Checking the logs, I'm running out of memory (and a lot of memory is being used). I have 32gb of RAM.
I'm running:
python wikidata_train_entity_linker.py output2:
output2 is the directory with the knowledge base. Its contents are:
entity_alias.csv gold_entities.jsonl
entity_defs.csv  kb
entity_descriptions.csv nlp_kb
entity_freq.csv  prior_prob.csv
Here is the command line output after running step 2's command. It simply escapes to the command line after running for 6+ hrs and running out of memory:
2019-10-27 19:52:44,320 - INFO - __main__ - Creating Entity Linker with Wikipedia and WikiData
2019-10-27 19:52:44,320 - INFO - __main__ - STEP 1a: Loading model from output2/nlp_kb
I1027 19:52:44.486585 4612883904 file_utils.py:39] PyTorch version 1.2.0 available.
I1027 19:52:45.090944 4612883904 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
I1027 19:52:59.587772 4612883904 wikidata_train_entity_linker.py:67] STEP 1b: Loading KB from output2/kb
I1027 19:53:07.744759 4612883904 wikidata_train_entity_linker.py:75] STEP 2: Reading training dataset from output2/gold_entities.jsonl
I1027 19:53:07.744909 4612883904 wikipedia_processor.py:473] Reading train data with limit None
2247226it [8:19:35,  3.76s/it]Killed: 9
Your Environment
Operating System: os Mojave
Python Version Used: 3.7.4
spaCy Version Used: 2.2.2.dev1 (built from source)
Environment Information: PyEnv