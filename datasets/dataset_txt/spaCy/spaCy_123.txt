absolution54321 commented on 22 Nov 2019
I decided to create a pattern/regex based matching approach with Matcher embedded inside pipeline. For small file MAX_TOKEN < 10000 it performs great but texts with size > 2 mb takes time for processing.
I have tried to speed up processing by chunking text into smaller batch, i.e every batch contain maximum of 10000 tokens, text processing was faster but some of the matcher rules missed out matches because chunking process moved part entities to different batch.
Example:
I want to match New York Boston
A text file of 2 mb
'text started here some blah blah ....... New York Boston .....'
for faster processing I am using nlp.pipe with batches of large file
above text becomes
texts = [ 'text started here some blah blah ....', '......... New York', 'Boston ..........']
nlp.pipe/ matcher.pipe(texts) this speeds up the process but 'New York Boston' is not to be found in matches because entity exists in two different batch.
I have disabled all memory consuming pipe components - NER, POS, DEPS
Currently, I have to trade off functionality for scaling. Do anyone have a suggestion, how to tackle large files? I want to use PhraseMatcher & Matcher.
Your Environment
Operating System: Linux-3.10.0-1062.1.1.el7.x86_64-x86_64-with-centos-7.7.1908-Core
Python Version Used: 3.6.8
spaCy Version Used: 2.1.8
Environment Information: 16 GB, 8 Cores No GPU