wavymazy commented on 5 Dec 2019
How to reproduce the behaviour
So I've been working with the en_trf_bertbaseuncased_lg model and ended up getting different classification results depending on how I processed text initially.
If I process text with the Language object using only 1 string
for ex: nlp(my_text) or nlp.pipe(my _text, batchsize=1)
I end up with different doc.tensor had I used :
nlp.pipe(my_texts, batchsize=500)
I compared the resulting doc, for each string, on a 1 by 1 basis, and the resulting vectors weren'T the same. I believe this to be the reason my classification results vary, and possibly why I get bad results when ingesting the same data by batch.
Thanks for reading and please let me know if I'm missing something, or if this is expected results.
cheers!
Your Environment
Operating System: ubuntu 18.04
Python Version Used: 3.6
spaCy Version Used: 2.2.1
Environment Information: I'm working with RASA.