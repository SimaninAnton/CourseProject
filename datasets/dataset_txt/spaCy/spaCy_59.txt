jerilkuriakose commented on 17 Dec 2019
I have the following code to train ner using spacy for our custom dataset.
spacy.prefer_gpu()
if model_dir:
    print('loading an existing model')
    nlp = spacy.load(model_dir)
else:
    print('creating a new model')
    nlp = spacy.blank('en')  # create blank Language class
# create the built-in pipeline components and add them to the pipeline
# nlp.create_pipe works for built-ins that are registered with spaCy
if 'ner' not in nlp.pipe_names:
    ner = nlp.create_pipe(pipe_name)
    nlp.add_pipe(ner, last=True)
# otherwise, get it so we can add labels
else:
    ner = nlp.get_pipe("ner")
# add labels
for __, annotations in TRAIN_DATA:
    for ent in annotations.get('entities'):
        ner.add_label(ent[2])

# get names of other pipes to disable them during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
with nlp.disable_pipes(*other_pipes):  # only train NER
    if not model_dir:
        print('Starting a new training')
        optimizer = nlp.begin_training()
    else:
        optimizer = nlp.entity.create_optimizer()
    for itn in range(iterations):
        print("Starting iteration " + str(itn + 1))
        random.shuffle(TRAIN_DATA)
        losses = {}
        batches = minibatch(TRAIN_DATA,
                            size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(
                texts,  # batch of texts
                annotations,  # batch of annotations
                drop=0.2,  # dropout - make it harder to memorise data
                sgd=optimizer,  # callable to update weights
                losses=losses)
        print(losses)
        # if losses[pipe_name] < constants.LOSSES_THRESHOLD:
        #     break
    print('Completed training')
And following the code for prediction:
nlp = spacy.load(model_dir)
doc = nlp(page_data)
nlp_output = [(ent.text, ent.label_) for ent in doc.ents]
Now we used the above mentioned code in two spacy versions: spacy 2.1.8 and spacy 2.2.3, and following are the results:
Tasks Version 2.1.8 Version 2.2.3
Dataset size 10 10
Iterations 500 500
Training Success Success
Prediction Success Did not predict a single entity
Loss 20.35429 1056.54712
Operating System: Windows 7
Python Version Used: 3.6.5
We have also noticed the losses are decreasing at a good rate in spacy 2.1.8, whereas it takes more iterations in spacy 2.2.3.
To summarise Spacy 2.2.3 did not return any predictions whereas we were getting the expected results in Spacy 2.1.8, what might be the issue, kindly help.
1