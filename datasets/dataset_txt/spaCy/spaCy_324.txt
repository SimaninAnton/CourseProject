H20Watermelon commented on 17 Sep 2019
Your Environment
Operating System: Win10
Python Version Used: 3.7
spaCy Version Used: 2.1.8
Environment Information:
I trained a couple of textcat models on approximately 8000 passages (most of which are one or two sentences long (max token count is about 60) for binary classification ("PRESENT": a passage contains descriptions of stock information; "NOT_PRESENT": a passage does not contain stock information). One model used the simple_cnn mode and the other the default ensemble mode.
When applying the models to the test set, I noticed that the models would predict "PRESENT" with very high probabilities (> 0.999) for passages that are not even remotely related to stock information. In fact, the models would give high probabilities to sentences with either garbled text or very short text. A couple of the examples are below:
3wqedsad \n sadsads
(etc
Both of the passages above received 1.0 for the PRESENT class.
It also seems that punctuation/symbols have a huge influence on predictions. For example, in the second example above, if the left parenthesis is removed, the probability for PRESENT drops to close to 0 (2.2164e-06).
I wonder if there are ways to improve the model performances, in particular in the area of reducing false positives.