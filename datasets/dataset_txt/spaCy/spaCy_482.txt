Contributor
FallakAsad commented on 27 Jul 2019 â€¢
edited
I want to train NER with FastText vectors, I tried 2 approaches:
1st Approach:
Load blank 'en' model
Load fasttext vectors for 2M vocabulary using nlp.vocab.set_vector() function
Call begin_training() followed by the code that iterate over batches and call update function.
2nd Approach:
Load blank 'en' model
Add a custom component i.e 'FastTextModel' in pipeline which sets the user_token_hooks with key 'vector' as shown in following code
Call begin_training() followed by the code that iterate over batches and call update function.
LABEL = ['label_1', 'label_2']

def train_model(model):
    nlp = spacy.load('en_core_web_sm')  # load existing spacy model

    # Load Fasttext component
    fasttext_component = FastTextModel(fasttext.load_model("cc.en.300.bin"))
    nlp.add_pipe(fasttext_component, first=True) 

    if 'ner' not in nlp.pipe_names:           
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner)
    else:
        ner = nlp.get_pipe('ner')

    # Add new entity labels to entity recognizer
    for i in LABEL:
        ner.add_label(i)   

    optimizer = nlp.resume_training()
    
    #Get names of other pipes to disable them during training to train only NER
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner' and pipe != 'FastTextModel']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        # Training code here
    save_model(nlp, 'model_name', 'output_dir', 0)

class FastTextModel(object):
    def __init__(self, model):
        self._model = model
        
    def __call__(self, doc):
        doc.user_token_hooks["vector"] = self.vector
        return doc;

    def vector(self, obj1):
        return self._model.get_word_vector(obj1.text)
        
if __name__ == '__main__':
    train_model()
My questions are:
In first approach, is it correct way to load vectors using nlp.vocab.set_vector() function? All the vectors loaded by this function will be utilized by NER while training?
In second approach, will NER utilize 'vector' function of FastTextModel to get word vectors for feeding it to CNN while training? During training, will NER learn new representation of words? if so, If I load word vector using following code after training is finished, will model return new learned word representations or the once generated by FastTextModel's 'vector' function
doc = nlp('payment')
for d in doc:
    print(d.vector) 
In second approach, will NER be able to generate word representation of OOV words using user_token_hooks since FastTextModel's vector() function generates representation of OOV words as well.
If NER does not use user_token_hooks during training, then only way to use FastText embeddings it to load it as a static embedding table using npl.vocab.set_vector() function?
Is there is some way to log what vector are being fed to embed part of NER while training or testing?
Your Environment
Platform: Linux-4.15.0-55-generic-x86_64-with-Ubuntu-16.04-xenial
spaCy version: 2.1.6
Python version: 3.5.2