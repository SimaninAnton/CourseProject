DSLituiev commented on 13 Nov 2019
Feature description
I am trying to clean / normalize a text with potential missing spaces around punctuation, like:
word,digit or digit/digit.
I could not find any documentation how to tweak it with the current default tokenizer.
import spacy
nlp = spacy.load("en_core_web_sm")

[(x, x.pos_) for x in nlp('fibrous type,5 mm in diameter,partially resected')]

# [(fibrous, 'ADJ'),
#  (type,5, 'PROPN'),
#  (mm, 'PROPN'),
#  (in, 'ADP'),
#  (diameter, 'NOUN'),
#  (,, 'PUNCT'),
#  (partially, 'ADV'),
#  (resected, 'VERB')]
This also relates to the problem of inconsistent tokenization of / in fractions:
[(x, x.pos_) for x in nlp('0/1 lymph nodes, 1/2 full')]
# [(0/1, 'PUNCT'),
#  (lymph, 'PROPN'),
#  (nodes, 'PROPN'),
#  (,, 'PUNCT'),
#  (1/2, 'NUM'),
#  (full, 'ADJ')]