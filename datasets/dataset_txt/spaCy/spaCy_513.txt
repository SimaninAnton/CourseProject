YannDubs commented on 18 Jul 2019 â€¢
edited
TL;DR: Improving HashEmbed class by using a weighted second matrix.
I was recently looking at the HashEmbed class. I was wondering if there was any reason not to use a weighted second matrix as in Hash Embeddings for Efficient Word Representations (NIPS 2017)?
The second matrix would be long but narrow (so not many parameters, usually same order of magnitude than first matric) and contains for each word the weights it should give for different hashes. E.g. "dog: [0.7, 0.2, 0.9]" such that word dog is not 0.7*hash_1(dog) + 0.2 * hash_2(dog) + 0.9 * has_3(dog). I've implemented it for the NIPS implementation challenge in 2017 and it worked very well (https://github.com/YannDubs/Hash-Embeddings). Small drawing I made back then to explain the method:
I was thus wondering if there is any reason not to use that instead ? Have you tried it?