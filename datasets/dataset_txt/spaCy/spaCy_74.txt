Contributor
mr-bjerre commented on 10 Dec 2019 â€¢
edited
I'd like to ship my model with a custom tokenizer but it doesn't load properly.
from spacy import load
from spacy.tokenizer import Tokenizer
from spacy.lang.en import English


class CustomTokenizer(Tokenizer):
    def __call__(self, text):
        return super().__call__(text.replace("\u2013", "-"))


nlp = English()
nlp.tokenizer = CustomTokenizer(nlp.vocab)

assert nlp("\u2013").text == "-"  # passes

nlp.to_disk("test-model")
nlp = load("test-model")
assert nlp("\u2013").text == "-"  # fails
How is this achieved? I assume I have to create my own language, i.e. extend English? I tried the following as well with same result
from spacy import load
from spacy.lang.en import English


class CustomEnglish(English):

    def make_doc(self, text):
        return self.tokenizer(text.replace("\u2013", "-"))


nlp = CustomEnglish()

assert nlp("\u2013").text == "-"  # passes

nlp.to_disk("test-model")
nlp = load("test-model")
assert nlp("\u2013").text == "-"  # fails
I also want to ship custom components and I recall Ines writing a post with Snake but I can't find it now. Link anyone?
Btw. I do realise that right now I only preprocess the text but I intent to change the tokenizer as well.
Info about spaCy
spaCy version: 2.2.3
Platform: Darwin-19.0.0-x86_64-i386-64bit
Python version: 3.7.5