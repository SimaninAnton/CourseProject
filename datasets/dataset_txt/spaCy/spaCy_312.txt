Contributor
alvaroabascar commented on 27 Sep 2019
The current implementation of get_lca_matrix was done by myself, so apologies for the following bug: on really big texts, get_lca_matrix produces a matrix with size (len(doc), len(doc)). This means RAM consumption will be O(n*n) and unbounded, which can produce MemoryError (it has happened to me with an abnormal text in a production environment).
I think we should modify the function, maybe using a sparse matrix to avoid this problem.
How to reproduce the behaviour
Run top or htop in a terminal.
Run the following python code.
import spacy
nlp = spacy.load('en')
text = 'word ' * 100000
nlp(text)
See how your computer explodes.
Your Environment
Spacy 2.0.17, linux.