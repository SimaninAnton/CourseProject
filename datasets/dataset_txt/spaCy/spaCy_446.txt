iCHAIT commented on 7 Aug 2019
I trained a custom NER model with 6 Entities and later tested it on an unseen sample.
This is the code I am using to evaluate my model on some unseen examples -
def evaluate(ner_model, examples):

    scorer = Scorer()
    for input_, annot in examples:
        doc_gold_text = ner_model.make_doc(input_)
        gold = GoldParse(doc_gold_text, entities=annot.get('entities'))
        pred_value = ner_model(input_)
        scorer.score(pred_value, gold)
    return scorer.scores
And here is the result I get -
{
    "uas": 0.0, "las": 0.0,
    "ents_p": 90.66397423978736,
    "ents_r": 90.26112736858471,
    "ents_f": 90.46210231583312,
    "ents_per_type": {
                        "ENTITY1": {
                                        "p": 90.15041861869855,
                                        "r": 88.68202816927871,
                                        "f": 89.4101949283755
                                   },
                        "ENTITY2":    {
                                        "p": 82.64149214221386,
                                        "r": 85.28465990316725,
                                        "f": 83.94227421270608
                                   },
                        "ENTITY3": {
                                        "p": 98.7231590403657,
                                        "r": 98.44665796656699,
                                        "f": 98.5847146278393
                                    }, 
                        "ENTITY4": {
                                        "p": 89.59285008653282,
                                        "r": 88.74727538604445,
                                        "f": 89.16805814934497
                                    }, 
                        "ENTITY5": {
                                        "p": 72.83978334689753,
                                        "r": 75.61103504529963,
                                        "f": 74.199542595769
                                    }, 
                        "ENTITY6":{
                                        "p": 88.18408901672848,
                                        "r": 88.07075899606895,
                                        "f": 88.12738757139074}
                                    },
    "tags_acc": 0.0,
    "token_acc": 100.0
}
I want to understand how is the overall F Score of the model calculated? I thought that the overall F score may be the average of all individual entity F Score, but that is not the case. See screenshot below -
Can someone help me better understand how is the overall F Score getting calculated?
Info about spaCy
spaCy version: 2.1.6
Platform: Darwin-18.5.0-x86_64-i386-64bit
Python version: 3.7.3
Models: en