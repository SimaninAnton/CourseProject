mihail911 commented on 14 Nov 2019
How to reproduce the behaviour
So I'm using one of the non-english models to do some tokenization. Specifically I'm interested in using the french tokenizer. While it works pretty well out-of-the-box, there are a few extra cases I'd like to add as special rules. Using this doc, I added a custom tokenizer as follows:
import re
from spacy.tokenizer import Tokenizer
import spacy

nlp = spacy.load("fr_core_news_sm", disable=["tagger", "parser", "ner"])
hash_re = re.compile(r"#[\w]+")
def custom_tokenizer(nlp):
         return Tokenizer(nlp.vocab,
                   rules=nlp.Defaults.tokenizer_exceptions,
                   prefix_search=spacy.util.compile_prefix_regex(nlp.Defaults.prefixes).search,
                   suffix_search=spacy.util.compile_suffix_regex(nlp.Defaults.suffixes).search,
                   infix_finditer=spacy.util.compile_infix_regex(nlp.Defaults.infixes).finditer,
                   token_match=hash_re.search)

nlp.tokenizer = custom_tokenizer(nlp)
doc = nlp("C'est une j'ai #blah")
print([t.text for t in doc])
However, this produces:
["C'", 'est', 'une', "j'", 'ai', '#', 'blah']
thereby not keeping the #blah token intact. My understanding given the docs is that the token_match field provides a regex that takes precedence over other rules (in this case, the trouble-maker i think is the prefix_search default regex). However, it seems like that regex precedence isn't being respected here.
Am I doing something wrong? If this is not the way to add custom regex rules on top of existing tokenizers, then what is the proper way?
Your Environment
spaCy version: 2.1.8
Platform: Darwin-18.7.0-x86_64-i386-64bit
Python version: 3.6.9
Thanks for the help!