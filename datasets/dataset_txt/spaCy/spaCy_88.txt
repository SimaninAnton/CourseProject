joanPlepi commented on 4 Dec 2019
How to reproduce the behaviour
from spacy.tokenizer import Tokenizer
from spacy.pipeline import Sentencizer
from spacy.lang.de import German
nlp = German()
nlp.add_pipe(Tokenizer(nlp.vocab))
nlp.add_pipe(Sentencizer())
print(nlp.pipeline) # for debugging
doc = nlp.make_doc("Er beteiligte sich in zahlreichen Gremien und Kreisen und hatte 1908 entscheidenden Anteil an der Gr체ndung der Gesellschaft der B체cherfreunde zu Hamburg. W채hrend des Ersten Weltkriegs diente M체nzel als Hauptmann der Landwehr.")
for sent in doc.sents:
    print(sent) 
I am trying to use spacy for the german language, however I am getting an error when I try to access to sentences and is not clear to me why as I have alread added the Sentencizer component.
ValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start.
I even tried to add the sentencizer using the code below, but I still have the same bug.
nlp.add_pipe(nlp.create_pipe("sentencizer"))
Your Environment
Operating System: Ubuntu 18.04
Python Version Used: 3.7.0
spaCy Version Used: 2.2.3
Environment Information: