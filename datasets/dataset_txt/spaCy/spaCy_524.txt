RonRademaker commented on 16 Jul 2019
If I understand correctly, the tok2vec property that was created for the bert-like pretraining is a model that creates an embedding vector for a token instead of just getting one from a word2vec model. Is there an API or something I could implement to create my own tok2vec model?
Which page or section is this issue related to?
#3448