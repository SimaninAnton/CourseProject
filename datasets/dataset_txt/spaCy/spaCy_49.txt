m-marina commented on 25 Dec 2019
When I try:
import spacy
nlp = spacy.load("xx_ent_wiki_sm")
text = "..."
doc = nlp(text)
for sent in doc.sents:
    print(sent)
Output: ValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start.
But if I add nlp.add_pipe(nlp.create_pipe('sentencizer')) - it works.
If we compare the English-language model (en_core_web_sm), it divides the text into sentences without adding 'sentencizer' component.
The question is how to do the same for the Multi-language model?
Environment
Operating System: Ubuntu 18.04
Python Version: 3.6
spaCy Version: 2.2.3