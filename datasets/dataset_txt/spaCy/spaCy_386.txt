erotavlas commented on 27 Aug 2019 â€¢
edited
I've noticed in my training data that the text hasn't been exactly entered correctly by whatever computer system or user has entered it. So say I'm dealing with medical reports and I've chosen to identify date as a named entity.
I'll have the majority of cases like this which are perfectly fine and the date is treated as individual tokens from their heading. (separated by whitespace)
RUN DATE: 11/21/11
DOB: 03/07/56
But then I'll get cases like this which are treated as one whole token because there is no whitespace between the entity and preceding token)
REPORTED:02/28/11
Although there is a colon character at the end of the preceding token, it is being treated as one whole token during training. Thus say I annotate all my dates correctly, the model learns that that whole chunk of text is a date because I'm guessing it gets fed into the algorithm as a single token. Similarly when I evaluate any text which has a similar structure, the model picks up the entire set of tokens as a single unit.
I've found other instances of this happening with other entity types. Although low occurrence, it is problematic if we are trying to achieve best results and identify as many as we can.
How do you recommend dealing with this issue?
(For now I have avoided to annotate anything like this for fear it might be learning the wrong patterns)
EDIT:
I just noticed that the model was correctly identifying person names that were connected to a preceding token followed by a colon., For example
SURGEON:John,Smith
Correctly identified as John Smith without including the preceding token.
So I have a feeling this issue might apply only to entities with numeric? (So far I noticed it happens with date and specimen id)