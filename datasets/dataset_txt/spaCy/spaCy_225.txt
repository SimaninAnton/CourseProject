ewaldatsensentia commented on 21 Oct 2019
This happens for "thats" and "whens". Both those words are in the nlp.vocab. They are split into tokens "that" and "s" by both nlp and nlp.make_doc.
How to reproduce the behaviour
import spacy
nlp = spacy.load('en_core_web_md')
for t in nlp("How many thats are in this sentence?"): print (t,t.lemma_)
output
How how
many many
that that
s s
are be
in in
this this
sentence sentence
? ?
Your Environment
spaCy version: 2.2.1
Platform: Windows-10-10.0.18362-SP0
Python version: 3.7.3