Contributor
mr-bjerre commented on 29 Aug 2019
I am trying to tokenize US$50 into ['US', '$', '50'] but there seems to be an issue or I am misunderstanding something!?
import spacy.util
import spacy.lang.en

nlp = spacy.lang.en.English()

infixes = nlp.Defaults.infixes + (
    r"(?<=\S)\$",
)
nlp.tokenizer.infix_finditer = spacy.util.compile_infix_regex(infixes).finditer

assert [t.text for t in nlp('US$50')] == ['US', '$', '50']  # fails
E.g. replacing $ with X works as expected
infixes = nlp.Defaults.infixes + (
    r"(?<=\S)X", 
)
nlp.tokenizer.infix_finditer = spacy.util.compile_infix_regex(infixes).finditer

assert [t.text for t in nlp('USX50')] == ['US', 'X', '50']  # passes
spaCy version: 2.1.8
Platform: Linux-5.0.0-25-generic-x86_64-with-Ubuntu-18.04-bionic
Python version: 3.7.3