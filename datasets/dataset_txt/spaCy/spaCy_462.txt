damianoporta commented on 1 Aug 2019 •
edited
Hello,
i am training a new NER model with the code:
with nlp.disable_pipes(*other_pipes):  # only train NER
    # reset and initialize the weights randomly – but only if we're
    # training a new model
    nlp.begin_training(component_cfg={"ner": {"conv_depth": 8}})

    for itn in range(N_ITER):
        random.shuffle(TRAIN_DATA)
        losses = {}
        # batch up the examples using spaCy's minibatch
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))

        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(
                texts,  # batch of texts
                annotations,  # batch of annotations
                drop=0.2,  # dropout - make it harder to memorise data
                losses=losses,
            )
i only have changed the conv_depth setting as said by @honnibal here: #3798
Then, loading the model i get:
>>> nlp = spacy.load('/home/nlp/0')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/__init__.py", line 27, in load
    return util.load_model(name, **overrides)
  File "/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py", line 133, in load_model
    return load_model_from_path(Path(name), **overrides)
  File "/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py", line 173, in load_model_from_path
    return nlp.from_disk(model_path)
  File "/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/language.py", line 791, in from_disk
    util.from_disk(path, deserializers, exclude)
  File "/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/util.py", line 630, in from_disk
    reader(path / key)
  File "/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy/language.py", line 787, in <lambda>
    deserializers[name] = lambda p, proc=proc: proc.from_disk(p, exclude=["vocab"])
  File "nn_parser.pyx", line 634, in spacy.syntax.nn_parser.Parser.from_disk
  File "/home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/thinc/neural/_classes/model.py", line 371, in from_bytes
    dest = getattr(layer, name)
AttributeError: 'HashEmbed' object has no attribute 'G'
What is this problem?
Your Environment
(.prodigy-1.8.3) root@damiano:/home/nlp# python -m spacy info

============================== Info about spaCy ==============================

spaCy version    2.1.4                         
Location         /home/nlp/.prodigy-1.8.3/lib/python3.6/site-packages/spacy
Platform         Linux-4.15.0-55-generic-x86_64-with-Ubuntu-18.04-bionic
Python version   3.6.7                         
Models