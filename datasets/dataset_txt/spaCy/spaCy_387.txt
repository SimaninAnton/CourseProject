H20Watermelon commented on 26 Aug 2019
Your Environment
Operating System: Windows 10
Python Version Used: 3.8
spaCy Version Used: 2.1.8
Additional Info: 256 Gb RAM; CPU: Intel Xeon E5-2687W v4 3.00GHz
I have a data set consisting of over 1.5 million sentences or passages. The average length of these sentences/passages is around 100 tokens. I am trying to use the Text Categorizer pipeline to train a model for 6 classes that are not mutually exclusive.
I used the following code, which largely follows the sample code in the documentation:
nlp = spacy.blank("en")
textcat = nlp.create_pipe("textcat", = { "exclusive_classes": False, "architecture": "ensemble", "ngram_size": 2}

nlp.add_pipe(textcat, last=True)
for label in labels:
    textcat.add_label(label)

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "textcat"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    #optimizer = nlp.resume_training()
    #if init_tok2vec is not None:
    print("Training the model...")
    print("{}\t{:^5}\t{:^5}\t{:^5}\t{:^5}".format("LABEL", "LOSS", "P", "R", "F"))
    batch_sizes = compounding(4.0, 32.0, 1.001)
    for i in range(n_iter):
        losses = {}
        random.shuffle(bias_train_data)
        batches = minibatch(bias_train_data, size = batch_sizes)
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer, drop=0.3, losses = losses)
After 5 hours, it did not finish a single iteration. I wonder if there is a way to increase the training speed (perhaps by increasing the batch size? or some other ways.)
I cannot use GPU for training because I have not been able to successfully compile thinc_gpu_ops. I tried all the suggested solutions in the Issues section to no avail.
Any suggestions would be greatly appreciated. Thanks!