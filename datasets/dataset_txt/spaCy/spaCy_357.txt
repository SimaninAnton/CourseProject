nguyenminhtuanfit commented on 5 Sep 2019 â€¢
edited
Hi,
I have 5 thousand emails and i want to create a custom name entity recognition.
if model is None:
        nlp.begin_training()
    optimizer = nlp.begin_training()    
    for itn in range(n_iter):
        random.shuffle(TRAIN_DATA)
        losses = {}

        # batch up the examples using spaCy's minibatch
        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(
                texts,  # batch of texts
                annotations,  # batch of annotations
                drop=0.5,  # dropout - make it harder to memorise data
                losses=losses,
                sgd=optimizer,
            )
        print("Losses", losses)`

> What is the ideal batch size of training process?
I will do sent_tokenize for all dataset and pass it as a single batch or do i need to split in a smaller single batch?
Thanks 
 