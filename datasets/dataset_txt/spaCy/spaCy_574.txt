aryehgigi commented on 2 Jul 2019 â€¢
edited
How to reproduce the behaviour
This is according to the following tokenizer initialization example.
import spacy
from spacy.tokenizer import Tokenizer
nlp = spacy.load("en_core_web_lg") # for example
test_case = "I don't know."
tokenizer_lame = Tokenizer(nlp.vocab)
[token for token in tokenizer_lame(test_case)] # will produce first output
tokenizer_awesome = nlp.Defaults.create_tokenizer(nlp)
[token for token in tokenizer_awesome(test_case)] # will produce second output
First output:
[I, don't, know.] # BAD
Second output:
[I, do, n't, know, .] # GOOD
Notes:
Not sure if this is the expected behavior (?), but even so, presenting both initializations as the same is misleading (IMHO).
I came across this when I used spacy_conll{ @BramVanroy } (and passed the flag is_tokenized=False), as they use the first method I posted as their tokenizer initialization step.
Your Environment
spaCy version: 2.1.4
Platform: Windows-10-10.0.17134-SP0
Python version: 3.6.5