DomHudson commented on 4 Sep 2019 â€¢
edited
Summary
Hi!
I was reading the description of how the tokenizer works and noticed a slight inconsistency between the described logic and what happens with the real code.
Regarding substrings, the documentation says that there is a "check whether we have an explicitly defined rule for this substring. If we do, use it."
I'm interested in the case where there is a suffix and a special case, for example the token _SPECIAL_ (which should not be tokenized) within parentheses: (_SPECIAL_).
If I follow the logic in the example code with this case, the special case would never match (as the string being presented to the special case logic still has the suffix(es) attached), so the rule would not apply.
First loop, input: "(_SPECIAL_)"
No special case matched identically so the prefix "(" matched and stripped
Second loop, input: _SPECIAL_)"
No special case matched identically, so the prefix "_" matched and stripped.
Third loop, input: SPECIAL_)
No longer resembles the special case so the remaining suffix punctuation is tokenized away from the main string.
With the real code the correct output is returned with the parentheses in their own tokens, but _SPECIAL_ as a single token.
Is there special logic happening within the special case logic to match the first part of the string in case there is a trailing suffix? Or is the order of logic slightly differently to account for this? Or is there a different solution?
Many thanks!
Code illustrating inconsistency
import spacy
from spacy.symbols import ORTH

TEST_STRING = '(_SPECIAL_)'

def print_tokens(nlp):
    """ Illustrative function to show the tokens detected on the test string
    with a given spacy tokenizer.

    :return void:
    """
    print([token.text for token in nlp(TEST_STRING)])


nlp = spacy.load('en_core_web_sm')
print_tokens(nlp)
# This is the default tokenizer, so the prefix and suffix rules separate the
# punctuation into their own tokens.
# Output: ['(', '_', 'SPECIAL', '_', ')']


# Now I define '-SPECIAL-' as its own token which should not be segmented.
nlp.tokenizer.add_special_case('_SPECIAL_', [{ORTH: '_SPECIAL_'}])
print_tokens(nlp)
# The output is correct. The punctuation outside of the special case is
# segmented, but the special case returned as a single token.
# Output: ['(', '_SPECIAL_', ')']
Which page or section is this issue related to?
https://spacy.io/usage/linguistic-features#how-tokenizer-works