H20Watermelon commented on 21 Aug 2019 â€¢
edited
Your Environment
Operating System: Windows 10
Python Version Used: 3.7
spaCy Version Used: 2.18
Environment Information:
I am trying to use regular expressions to find certain phrases in a set of data. Because of the way the text data is generated and extracted, oftentimes the phrases that I am trying to extract might be preceded and/or followed immediately by some symbols (with no white space in between).
Let me use a couple of made-up examples to illustrate what I mean: Say what I want to find is the phrase home address. In the example below, the regex match perfectly coincides with the two tokens produced by spaCy tokenization. As a result, I am able to use the start and end values from re.search to create a span with char_span,
nlp = spacy.load("en_core_web_lg")
doc = nlp("Home address / Phone number")
print(list(doc))
#[Home, address, /, Phone, Number]
start, end = re.search(r"\b[Hh]ome address\b", doc.text).span()
span = doc.char_span(doc, start, end, label="HomeAddressLabel")
print(span)
#Home address
In the example below however, because the slash / immediately follows "address", the slash is grouped with "address" as a single token "address/". As a result, the start and end values from re.search cannot be used to create a span (at least with the default tokenization scheme):
doc = nlp("Home address/ Phone number")
print(list(doc))
#[Home, address/, Phone, Number]
start, end = re.search(r"\b[Hh]ome address\b", doc.text).span()
span = doc.char_span(doc, start, end, label="HomeAddressLabel")
print(span)
So my question:
In my actual data, the trailing and/or leading symbols, and the number of them (e.g. "Home address///", "Home address--", could vary quite a bit, so ideally I don't want to create different regex patterns to deal with different scenarios, so I wonder if there is a more streamlined way to create a span when a regex match does not fully encompass all the corresponding tokens. For example, is there a way to, say, efficiently determine if a start value and an end value from re.search are, respectively, not at the start and the end of some tokens, and adjust the start and end values accordingly (e.g., move the start value to the left if not aligned, and move the end value to the right if not aligned).
Or does it make sense to simply change the tokenization rules. My preference is definitely not to change the tokenization rules (e.g., split on all slashes, all hyphens, or simply split on all symbols that are not underscore.)
Thanks!