levon003 commented on 3 Oct 2019
How to reproduce the behaviour
I identified a text that causes the tokenizer to [apparently] hang. See the sample script below.
text = "https://si0.twimg.com/profile_images/2711056064/4399ea260e5608718ba4f54960b51627.jpeg?awesome=3dhiQfF57zkZOHOuyKnORdmiqZSVDU08swJ1KmtpbNdWZbTKt1anNwJJB99a9stVShtj59XpA3yLxYRigamlXlxfwDWZ2LH2xKOqs2mHN46qgLBqb1H156JGhBryicZb1gmlKuC2vLMonWOCCA8ngGOSMwlSQai5LBNB4ZVAVrVPz2JoYwxUNDmtdGdGEgcdHuFFrHk7☃" + "☃" * 13200
print(text)
import spacy
nlp = spacy.load("en_core_web_lg")
tokenizer = nlp.Defaults.create_tokenizer(nlp)
print("loaded")
print(spacy.__version__)
for doc in tokenizer.pipe((text,)):
    print(doc)
I'm not sure if the tokenizer will ever terminate; from hanging up a batch job that was given about 48 hours for processing, I'm fairly certain that this text causes the tokenizer to at least block for 35+ hours. (The test code above blocks for at least > 5 mins; in contrast, the string of 13200 Snowman emoji '☃' tokenizes in about 22 seconds. [The link without the emoji tokenizes rapidly, as expected.])
Note that neither the Snowman emoji nor the link are sufficient on their own to cause the tokenizer to hang; it's something about the combination of the link and the long string of emoji.
On interrupt:
  File "tokenizer.pyx", line 142, in pipe
  File "tokenizer.pyx", line 125, in spacy.tokenizer.Tokenizer.__call__
  File "tokenizer.pyx", line 167, in spacy.tokenizer.Tokenizer._tokenize
  File "tokenizer.pyx", line 184, in spacy.tokenizer.Tokenizer._split_affixes
Maybe related to issues #2835 #2744
Note that this is a naturally occurring text provided by a user in a social media environment; to decrease the size of the text, I multiplied out the snowman emoji by the number of times it occurred in the original text.
Your Environment
spaCy version: 2.1.8
Platform: Linux-3.10.0-957.27.2.el7.x86_64-x86_64-with-centos-7.6.1810-Core
Python version: 3.7.3
Models: en