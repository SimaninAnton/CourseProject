gabeorlanski commented on 17 Nov 2019 â€¢
edited
The code I had used when this error appeared:
nlp = en_core_web_sm.load()
nlp.pipe(reports)
reports is a list of 2617 documents averaging 27K characters and 360 lines. In total, it has 71 million characters and is ~ 140 megabytes of data. This represents a single group of around 200 groups with a total of 400K documents. It is on the smaller side, so I assume it would also break for the groups with a lot more documents in them. One issue that could be causing this is that these reports are the result of parsing pdfs that include tables and tend to have strange characters in them. I am decoding with utf-8, but that could possibly be causing issues in the parser. The tables in the document do not have a uniform format, so they are quite hard to get rid of.
Environment:
spaCy version: 2.2.2
Platform: Linux-3.10.0-1062.4.1.el7.x86_64-x86_64-with-centos-7.7.1908-Core
Python version: 3.7.4
128 GB of ram
One other thing that I noticed was that I was getting an error when trying to individually run nlp(report) using multiprocessing on the batched reports. The error I was getting:
error("'i' format requires -2147483648 <= number <= 2147483647")
It came after it had printed out the entire document. The code I had used for that was:
with mp.Pool(cores) as p:
    p.imap_unordered(parseBatch, (nlp,batches,))

def parseBatch(nlp, doc_batch):
    return [nlp(report) for report in doc_batch]
I had googled the report and found that it related to multiprocessing and joblib so that the segfault could be on their end. In terms of memory usage, while running pipe, it had not gone above 50%. I also tested this with and without documents that have more than 100K characters, but the issue persisted.