Contributor
ajrader commented on 10 Aug 2019
Observation
I noticed if I tried to use spacy for tokenization only (disabling 'ner', 'parser' and 'tagger') that i was getting an erroneous mapping for the english word 'spun'. As long as tagger is not disabled, this past tense verb is correctly mapped to 'spin'. But when 'tagger' is disabled then it gets mapped to 'spin-dry' which in my estimation is wrong.
I think the source of this error is due to line 35297 in https://github.com/explosion/spaCy/tree/master/spacy/lang/en/lemmatizer/lookup.py
"spun": "spin-dry"
How to reproduce the behavior
import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp('spun', disable=['tagger'])
for tok in doc0:
print(tok.text, tok.lemma_)
The output of this is
spun spin-dry
I noticed the same behavior when using 'en_core_web_lg' and 'en_core_web_md' because I think this is the default lemmatizer mapping in all 'en' dictionaries.
Your Environment
Operating System: Windows 10
Python Version Used: 3.7.3
spaCy Version Used: 2.1.7
Environment Information: installed with conda
1