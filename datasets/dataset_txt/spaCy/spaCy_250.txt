nsaef commented on 15 Oct 2019
Hi! I'm trying to add special cases to the tokenizer, but they're only applied if the special tokens are separated by whitespace.
Code example:
import spacy
from spacy.symbols import ORTH, LEMMA, POS, DEP

nlp = spacy.load("en_core_web_sm")

word = "<TITLE>"
rule = [{ORTH: word}]
nlp.tokenizer.add_special_case(word, rule)

word = "</TITLE>"
rule = [{ORTH: word}]
nlp.tokenizer.add_special_case(word, rule)

result = [t.text for t in nlp("<TITLE>Hello World</TITLE>")]
Expected result: ['<TITLE>', 'Hello', 'World', '</TITLE>']
Actual result: ['<', 'TITLE', '>', 'Hello', 'World</TITLE', '>']
This, however, works:
result = [t.text for t in nlp("<TITLE> Hello World </TITLE>")]
>> ['<TITLE>', 'Hello', 'World', '<TITLE>']
This bug seems to have come up in the past already (#1061) and crept back in? At least the linked issue is what pointed me in the right direction.
Environment
spaCy version: 2.2.1
Platform: Windows-10-10.0.17763-SP0
Python version: 3.7.4
Models: en