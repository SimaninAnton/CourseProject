ibarshai commented on 4 Oct 2019
I used Spacy 1.9 in the past for tokenizing large quantities of small strings. When 2.0 came out, the tokenization speed for these smaller strings dropped dramatically. I believe tokenizer speed was targeted in 2.1 and was hoping to see more improvements on this front in 2.2. Is there an intent to address this in the future? Am I doing something wrong with disabling pipeline elements that's keeping the processing of these strings very heavy?
To illustrate my point, here is a comparison of tokenizing a small sentence with most of the pipeline disabled showing a 40x difference is speed:
Spacy 1.9
import spacy
nlp = spacy.load('en', parser=False, tagger=False, entity=False)
s = 'this is a sentence to tokenize'
%timeit nlp(s)

83.3 µs ± 418 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
Spacy 2.2
import spacy
nlp = spacy.load('en_core_web_sm', 
                 disable=['parser' 'tagger', 'entity', 'ner', 
                          'entity_linker', 'entity_ruler', 
                          'textcat', 'sentencizer', 
                          'merge_noun_chunks', 'merge_entities', 'merge_subtokens'])
s = 'this is a sentence to tokenize'
%timeit nlp(s)

3.27 ms ± 47.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
My Environment
spaCy version: 1.9.0
Platform: Linux-4.13.0-45-generic-x86_64-with-debian-stretch-sid
Python version: 3.6.2
Installed models: en
spaCy version: 2.2.1
Platform: Windows-10-10.0.18362-SP0
Python version: 3.7.4
Installed models: en_core_web_sm