bklippstein commented on 10 Jul 2019
tokenizer.add_special_case is not working for me.
import spacy

from spacy.symbols import ORTH, POS, X, LEMMA

nlp = spacy.load('en')

nlp.tokenizer.add_special_case('»', [{ORTH: '»', POS: X, LEMMA:'Test'  }])
nlp.tokenizer.add_special_case('«', [{ORTH: '«', POS: X, LEMMA:'Test' }])

doc = nlp('He said: »I am lying.«')

for token in doc:
    print('{:10}{:10}{:10}'.format(token.text, token.pos_, token.lemma_))
returns
He        PRON      -PRON-    
said      VERB      say       
:         PUNCT     :         
»         PUNCT     »         
I         PRON      -PRON-    
am        VERB      be        
lying     VERB      lie       
.         PUNCT     .         
«         PUNCT     «   
The reason why I try to change the POS tags is that they are wrong for '»' and '«' in the german model.
Even the code from here is not working:
import spacy

from spacy.symbols import ORTH, POS, NOUN, VERB

nlp = spacy.load('en')

nlp.tokenizer.add_special_case('{G}', [{ORTH: '{G}', POS: NOUN}])
nlp.tokenizer.add_special_case('{T}', [{ORTH: '{T}', POS: VERB}])

doc = nlp('This {G} a noun and this is a {T}')

for token in doc:
    print('{:10}{:10}'.format(token.text, token.pos_))
  
# Wanted output:
# This      DET       
# {G}       NOUN      
# a         DET       
# noun      NOUN      
# and       CCONJ     
# this      DET       
# is        VERB      
# a         DET       
# {T}       VERB

# My output:
# This      DET       
# {G}       NOUN      
# a         DET       
# noun      NOUN      
# and       CCONJ     
# this      DET       
# is        VERB      
# a         DET       
# {T}       NOUN   <<<<<<<<<<<<<<<<<<<
Your Environment
Operating System: Linux-5.0.0-20-generic-x86_64-with-debian-buster-sid
Python Version Used: 3.6.8
spaCy Version Used: 2.1.4 [cuda100]
Environment Information: