Contributor
tjanez commented on Oct 25, 2012
In the exercise in section on Cross-validation estimators one has to find an optimal regularization parameter alpha for Lasso regression.
I think the exercise is misleading, to say the least. The main issue is that the scores for different alphas differ insignificantly, yet the plot of the solution portrays them as something significant.
To be more concrete, the scores are:
[0.48803450860197256, 0.48803099969675356, 0.48802573754364903, 0.48801773427191092, 0.48800529096977491, 0.48798554724918719, 0.48795319169977985, 0.48789864301061825, 0.48790979589257422, 0.48810545077662604, 0.48833952711517042, 0.4885892181426163, 0.48879397803865515, 0.48900347108427633, 0.48895024452530594, 0.48886339735804762, 0.48836850577232954, 0.48762167781541343, 0.48712963815557303, 0.48692470890140377]
Yet the plot is something like .
It hides the y scale so that the observer can't see just how similar the scores are.
How I encountered the bug was that I programmed my own solution to the exercise. The plot this solution generates is this one:
The other problem with the exercise is the Bonus question: "How much can you trust the selection of alpha?".
The answer given in the solution is:
k_fold = cross_validation.KFold(len(X), 3)
print [lasso.fit(X[train], y[train]).alpha for train, _ in k_fold]
which produces:
[0.10000000000000001, 0.10000000000000001, 0.10000000000000001].
In essence, the code outputs the last used alpha from the alphas list. This is obviously not the answer to the question.
The correct solution would preferably say something like:
"The process used to find the optimal value of parameter alpha is prone to over-fitting.
A possible way to avoid this problem is to perform nested cross-validation: select the value of alpha using internal cross-validation on the current training data and compute the estimator's score on separate test data obtained via external cross-validation."
The code to perform this would be:
lasso_cv = linear_model.LassoCV()
k_fold = cross_validation.KFold(len(X), 3)
for k, (train, test) in enumerate(k_fold):
    lasso_cv.fit(X[train], y[train])
    print "Fold {}: best alpha obtained with internal CV: {}, score: {}".\
        format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test]))
This was my first issue with scikit-learn since I started using it a week ago.
I'm trying to be a helpful user and contribute something back, so please read my comments in a constructive light.