RyanCarey commented on Oct 23, 2017
I think LogisticRegression and LogisticRegressionCV are fitting the intercepts very differently. Possibly LogisticRegression is regularizing the intercept? When you have an imbalanced dataset, you generally don't want the intercept to be subject to regularization, so I figure this is a bug, and LogisticRegression should behave like LogisticRegressionCV?
Steps/Code to Reproduce
￼
import numpy as np
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
In [2]:
￼
10
np.random.seed(0)
X = np.random.rand(10,4)
y = np.random.choice(2, 10)
y
Out[2]:
array([0, 1, 0, 1, 1, 1, 1, 1, 1, 0])
In [3]:
￼
,
lr = LogisticRegression(C=1e-7)
lr.fit(X, y)
lr.intercept_, lr.coef_
Out[3]:
(array([ 1.99999899e-07]),
array([[ 4.81118353e-08, 1.55535981e-07, -8.16391637e-09,
1.60519085e-07]]))
In [4]:
￼
,
lrcv = LogisticRegressionCV(Cs=[1e-7])
lrcv.fit(X, y)
lrcv.intercept_, lrcv.coef_
Out[4]:
(array([ 0.84189173]),
array([[ -5.55390663e-08, 2.54505776e-08, -9.79413429e-08,
2.68588815e-08]]))
Expected Results
I think what I'd expect is for LogisticRegression to behave like LogisticRegressionCV, and to allow lr.intercept_ to be fitted to a nonzero value.
Versions
Linux-4.10.0-37-generic-x86_64-with-debian-stretch-sid
Python 3.5.3 |Continuum Analytics, Inc.| (default, Mar 6 2017, 11:58:13)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
NumPy 1.13.1
SciPy 0.19.1
Scikit-Learn 0.18.2