tvvignesh commented on Jan 1, 2019
I have been using Isolation Forest to do anomaly detection. While everything works out fine, I have a large dataset which I have to do prediction for. eg. I have to run predictions close to 1 lakh times for a dataset which I have and I have been using loops to predict one at a time and identify if its a outlier with score.
Is there a way to do bulk predictions to avoid this? I would like to make this faster and efficient. I am currently able to do around 2.66 predictions/second and keeping this metric into account, it would take close to 10 hours for all the predictions to complete.
Is there a way to improve this? (I am thinking of doing parallel processing using multiple cores/containers but was wondering if there is any inbuilt way to handle that.)
Some Stats:
Total records used for training - 176232
Total records to predict - 100000
Memory used after Training - 1.8GB
Love your great work btw. Thanks.