dgehlhaar commented on Feb 4, 2016
scikit-learn version: 0.16.1
python: 3.4.4 (anaconda 2.3.0)
machine: Linux RH 6, more memory than you can shake a stick at
There should be an option to store random forest classifiers that can be used for predictions, in a condensed representation, e.g. with each decision point having the descriptor, operator, and cutoff value, and the leaves having the classification. I trained a 300 tree model using default parameters, with about 100K data points (about 20 descriptors each). This trained fine but the resulting model, output with compression by joblib, is 182 MB (!). I tried training another model that had ten times the data points (same number of trees). This finished building, but joblib choked with the dreaded "OverflowError: Size does not fit in an unsigned int" error (supposedly this was fixed with python 3?).
By the looks of it, scikit-learn is storing the data used to build the model as part of the model, with no options not to do this. Any way around this? I want to build a model with 4M data points...