Contributor
kno10 commented on Jan 31, 2016
While the balltree accepts custom distance functions (at least if they are metric), this appears to come at a massive performance cost.
I have been running DBSCAN on this data set (110250 instances, 8 dimensions, color histograms - eventually I would like to use the version with 14x3x3 dimensions, for example).
With the balltree and Euclidean distance, this takes 11.4 seconds on average - reasonable. (The k-d-tree is still faster; but I would like to eventually use more advanced distance functions and higher dimensionality!)
However, when I use this code:
def euclid(a1, a2): # Two numpy arrays
    return numpy.sqrt(numpy.sum((a1 - a2)**2))
db = DBSCAN(min_samples=20, eps=0.01, algorithm="ball_tree", metric=euclid).fit(X)
it takes 5741 seconds now, which is unacceptable. 500 times slower than before - and I don't see much optimization potential in above code, do you? If I'm not mistaken, regular euclidean distance in the balltree is compiled from Cython, but using a custom metric like this will call back into the interpreter and copy data, right? To be useful, the overhead needs to be reduced.
Because of #5275, DBSCAN cannot be used with linear scan anymore (out of memory) on data this big. In earlier versions, the runtime with linear scan was around 1500 seconds with Euclidean distance.