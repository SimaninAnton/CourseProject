dave1g commented on Mar 21, 2015
I ran a large gridsearch (base don the code here http://scikit-learn.org/stable/auto_examples/grid_search_digits.html#example-grid-search-digits-py) and the process locked up, I killed it and tried to rerun and got this error. I later figured out it is because /dev/shm was full of old files from numpy arrays
$ ls -lrt /dev/shm/*joblib*
/dev/shm/joblib_memmaping_pool_64060_189405392:
total 13528188
-rw------- 1 general 13852862624 Mar 20 09:09 64060-190150864-190138048-0.pkl_01.npy
-rw------- 1 general         152 Mar 20 09:09 64060-190150864-190138048-0.pkl
4 or 5 similar files/directories were present
obviously its not possible to cleanup when i killed the task (which was probably stuck because of the lack of space in the first place. (which should have triggered an exception that when caught would cleanup this space)
But when I try to rerun the script these files should be cleaned up in an intelligent manner instead of also crashing, Or should at least fail with a better message as to which directory is full. (Ionly was able to figure it out form reading this: #3313 )
Traceback (most recent call last):
  File ".../python/linux/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py", line 240, in save
    obj, filename = self._write_array(obj, filename)
  File ".../python/linux/lib/python2.6/site-packages/sklearn/externals/joblib/numpy_pickle.py", line 203, in _write_array
    self.np.save(filename, array)
  File ".../python/linux/lib/python2.6/site-packages/numpy/lib/npyio.py", line 453, in save
    format.write_array(fid, arr)
  File ".../python2.6/site-packages/numpy/lib/format.py", line 521, in write_array
    array.tofile(fp)
IOError: 1731607818 requested and 502 written