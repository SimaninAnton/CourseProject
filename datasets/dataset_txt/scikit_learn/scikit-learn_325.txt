avitase commented on Jul 8, 2019 â€¢
edited
Correct me, if I am wrong, but the idea of a PCA is to decorrelate a given data sample X, right? Typically, this is achieved by diagonalizing the covariance matrix C of X, i.e. finding an orthonormal matrix R, such that (R^T C R) becomes diagonal. If X is centralized, this is equivalent with the SVD of X though, since C = (X^T X) / (n-1) in this case. Given X = (U S V^T) is the result of the SVD, then V is equivalent with R (hence X -> XR = XV is decorrelated).
My point here is, that if X is a (m x n) matrix with (m > n, i.e. more data (rows) than features), the former approach is much cheaper than the latter, since finding the covariance matrix roughly scales like O(mn^2) and the SVD (aka diagonalization) of the square (n x n) covariance matrix is basically for free for small n. This makes it way more efficient than the (bi)diagonalization via SVD of a full-blown (m x n) matrix (i.e. X), which is O(mn^2 + nm^2). (Note the m^2!)
My proposal: Replace the SVD of the (m x n) matrix X with a SVD of the (n x n) covariance matrix of X. You can then even relax the centralization requirement of X since the proposed solution works always. (N.B: The ordering of the PCA components is preserved if you use the SVD of the covariance matrix.)