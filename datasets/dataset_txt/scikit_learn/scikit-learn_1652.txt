baozi-lala commented on Dec 28, 2017
Due to the data set is too big to load it all at once. I need to normalize, extracted features and train it in batches. I chose iris as the data set and scikit-learn in python to validate my ideas.
The first step,I normalized the batches using standarScaler.particial_fit(),
def batch_normalize(data):
    scaler = StandardScaler()
    dataset=[]
    for i in data:
        sc = scaler.partial_fit(i)
    for i in data:
        dataset.append(scaler.transform(i))
    return dataset
The second step,I extracted features using IncrementalPCA.particial_fit()
def batch_feature_extracrton(dataset):
    ipca = IncrementalPCA(n_components=4)
    dataset_1=[]
    for i in dataset:
        ipca.partial_fit(i)
    for i in dataset:
        dataset_1.extend(ipca.transform(i))
    return dataset_1
The third step,I trained the data using MLPClassifier.particial_fit()
def batch_classify(X_train, X_test, y_train, y_test):
    batch_mlp = MLPClassifier(hidden_layer_sizes=(50,10), max_iter=500,
                    solver='sgd', alpha=1e-4,  tol=1e-4, random_state=1,
                    learning_rate_init=.01)
    for i,j in zip(X_train,y_train):
        batch_mlp.partial_fit(i, j,[0,1,2])
    print("batch Test set score: %f" % batch_mlp.score(X_test, y_test))
Below is my main function that calls the defined three functions above:
def batch(iris,batch_size):
    dataset=batch_normalize(list(chunks(iris.data, batch_size)))
    dataset=batch_feature_extracrton(dataset)
    X_train, X_test, y_train, y_test = train_test_split(dataset, iris.target, test_size=0.2)
    batch_data = list(chunks(X_train, batch_size))
    batch_label = list(chunks(y_train, batch_size))
    batch_classify(batch_data, X_test, batch_label, y_test)
However,in this method,every step, including the normalization and feature extraction, I have to go through all batches of data twice. Is there any other methods to Simplify the process? (for example, a batch can go directly from step 1 to 3)