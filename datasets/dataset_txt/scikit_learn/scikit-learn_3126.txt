Contributor
ssaeger commented on Mar 8, 2016
In commit 6bf63f6 error was replaced by kl_divergence, but line 850 was missed so that the final kl_divergence value is actually an old value that misses the final optimization step. This is especially a problem since people can access the kl_divergence in the current version as described in #6477.
Even in the verbose output one can see that the error/kl_divergence value from iteration 100 is kept as the final value at iteration 200.
[t-SNE] Iteration 25: error = 8.0242269, gradient norm = 0.0014062
[t-SNE] Iteration 50: error = 7.9942951, gradient norm = 0.0014150
[t-SNE] Iteration 75: error = 7.9053034, gradient norm = 0.0014402
[t-SNE] Iteration 100: error = 7.8706691, gradient norm = 0.0014515
[t-SNE] KL divergence after 100 iterations with early exaggeration: 7.870669
[t-SNE] Iteration 125: error = 0.5025748, gradient norm = 0.0009721
[t-SNE] Iteration 150: error = 0.4773589, gradient norm = 0.0009048
[t-SNE] Iteration 175: error = 0.4710594, gradient norm = 0.0008880
[t-SNE] Iteration 200: error = 0.4693623, gradient norm = 0.0008834
[t-SNE] Error after 200 iterations: 7.870669
If this confirms to be a bug, I will create a PR for it.
üëç 1