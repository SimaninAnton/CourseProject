h4k1m0u commented on Sep 11, 2015
Hi,
I'm trying to apply k_mean clustering and many other clustering methods (gaussian mixture model, Mean shift) included with sklearn on a large number of samples (500M), but I always get a Memory Error maybe because these algorithms requires data to be in float 32 format which takes a lot of space.
Here is the error I get when I try to fit the gaussian mixture model:
Traceback (most recent call last):
  File "gaussian_mixture_model.py", line 32, in <module>
    g.fit(X)
  File "/usr/local/lib/python2.7/dist-packages/sklearn/mixture/gmm.py", line 443, in fit
    random_state=self.random_state).fit(X).cluster_centers_
  File "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/k_means_.py", line 794, in fit
    n_jobs=self.n_jobs)
  File "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/k_means_.py", line 255, in k_means
    X = as_float_array(X, copy=copy_x)
  File "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py", line 93, in as_float_array
    return X.copy('F' if X.flags['F_CONTIGUOUS'] else 'C') if copy else X
MemoryError
Note that I'm using a AmazonEC2 instance with just 8Go of Ram which I guess is not enough.
Can anyone tell me how much Ram is required to apply one of the clustering algorithms on a data of this size (500M)?
Thanks.