alexisrosuel commented on Nov 6, 2017
Hi ! I am using the scikit learn implementation of the gaussian process regressor object and I have found something weird with the space required to save this object.
Description
Scikit gaussian process objects are very huge when pickling them (about 100 Mo for 7000 training data points)
Steps/Code to Reproduce
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import ConstantKernel, RBF, WhiteKernel

trend = RBF(length_scale=3000, length_scale_bounds=(1500, 6000), nu=1.5)
cst = ConstantKernel(constant_value=0.05, constant_value_bounds=(0.001, 10.0))
noise = WhiteKernel(noise_level=0.05, noise_level_bounds=(0.01, 10))

kernel = cst * trend + noise

gp = sklearn.gaussian_process.GaussianProcessRegressor(kernel=kernel, 
                                                  alpha=0, 
                                                  optimizer='fmin_l_bfgs_b', 
                                                  n_restarts_optimizer=0, 
                                                  normalize_y=True, 
                                                  copy_X_train=True, 
                                                  random_state=None)

gp.fit(data[:6000][['x','y']], data[:6000]['target'])

# pickle it 
pickle.dumps(gp, file=file)
Expected Result vs Actual result
6000 floats for x and y, 6000 int for target, I wouldn't think that it would require more than 150Mo of space...
Versions
import platform; print(platform.platform())
import sys; print("Python", sys.version)
import numpy; print("NumPy", numpy.__version__)
import scipy; print("SciPy", scipy.__version__)
import sklearn; print("Scikit-Learn", sklearn.__version__)
Darwin-17.0.0-x86_64-i386-64bit
Python 3.6.2 (v3.6.2:5fd33b5926, Jul 16 2017, 20:11:06) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]
NumPy 1.13.3
SciPy 1.0.0
Scikit-Learn 0.19.1
Thanks for your help !