blaine-alan-nelson commented on Jun 19, 2011
I am running experiments with an SVM for which I need to create SVMs in a loop on different datasets. Training these SVMs causes the memory of my python process to grow every time I retrain (even though I'm careful to delete the data). To show that the memory problem is in your implementation of SVM learning, I've included very simple sample code where I simply retrain on the same data in a loop (see below). In testing, it's clear that the memory problem clearly becomes worse as Is this a memory leak or is there some function I need to call to release the memory? I've noticed that the code released by the LIBSVM group implements a python del constructor that calls a C function to deallocate memory whereas this interface does not.
EXAMPLE OF MEMORY USAGE PROBLEM (After pressing enter, you can watch the python process rapidly grow. To make the problem worse, increase the size of the trainset regulated by posSize and negSize)
#! /usr/bin/env python

from scikits.learn import svm
import numpy

if __name__ == "__main__":
    raw_input("Press Enter")
    
    posSize = 100
    negSize = 100
    dimen = 10
    dataPos = numpy.random.uniform(0, 1, dimen*posSize).reshape(posSize,dimen)
    dataNeg = numpy.random.uniform(0, 1, dimen*negSize).reshape(negSize,dimen)
    dataNeg[:,0] *= -1.0
    
    trdata = numpy.concatenate((dataNeg,dataPos))
    trlabels = numpy.array([0]*len(dataNeg) + [1]*len(dataPos))
    
    m = svm.SVC(kernel='linear')
    for k in range(100000):
        print "Training new model " + str(k)
        m.fit(trdata,trlabels)
        del m
        m = svm.SVC(kernel='linear')
    
    raw_input("Press Enter")