Member
amueller commented on Apr 27, 2018
I just talked with the authors of EigenPro about including it in sklearn:
https://arxiv.org/abs/1703.10622
It's basically a kernel model with least squares loss using an SVD of the subset of the data.
It's very fresh for sklearn standards, but it's much faster than SVMs or Kernel Ridge (~10x speedup) with similar performance. It seems to work much better than doing kernel approximation.
A related method (from my relatively superficial understanding) is FALKON https://papers.nips.cc/paper/6978-falkon-an-optimal-large-scale-kernel-method.pdf
Misha Belkin promised a numpy implementation of EigenPro for inclusion into sklearn (they have a tensor-flow based one right now). I wanted to test the waters for this, given that it's more fresh than what we usually accept.
They showed me pretty convincing benchmarks against our SVC (and also ran comparisons against RBFSampler and Nystroem).