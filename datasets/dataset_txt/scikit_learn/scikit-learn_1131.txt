Contributor
agamemnonc commented on Aug 28, 2018
This is related to #11931.
Currently it is not possible to use the score() method in any classifier and specify the normalize argument, as we can otherwise do if we use the accuracy_score() function directly.
For example, this works fine:
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score 

X, Y = make_classification(n_samples=1000, n_features=100, n_informative=30, 
                    n_classes=2, random_state=10)


X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)

lr = LogisticRegression().fit(X_train, Y_train)
score = lr.score(X_test, Y_test)
assert score == accuracy_score(Y_test, lr.predict(X_test), 
                         normalize = True)
However, the following is not currently possible :
score = lr.score(X_test, Y_test, nomrmalize=False)
assert score == accuracy_score(Y_test, lr.predict(X_test), 
                         normalize = False)
All that needs to be done to fix this is to include the normalize arugment in the score() method of the RegressorMixin class.
Happy to submit a PR.