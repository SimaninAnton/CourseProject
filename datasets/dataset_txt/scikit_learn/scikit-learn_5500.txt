Member
ogrisel commented on Jan 9, 2012
Average SGD is an old computational trick (namely Polyak Rupert averaging applied to SGD) that was recently rediscovered and theoretically studied by Wei Xu (arxiv link) and Francis Bach & Eric Moulines (hal link) and further empirically validated in the public implementation of SGD by Leon Bottou (SGD project page).
This trick makes the SGD algorithm converge much faster with a very small modification of the original SGD code. It would be great to have it implemented in the scikit along with some examples that reproduce Leon Bottou's experiments on the RCV1 and Alpha datasets (that would require 2 new dataset loaders).
@npinto also has a pure numpy implementation that could also serve as a reference for additional comparison & benchmark.
@pprett the original author of the scikit-learn SGD module does not plan to work on this in the short term so anyone motivated enough, please feel free to embark (just send an email on the mailing to avoid // implementation efforts).
Edit: See also this thread on the mailing list.