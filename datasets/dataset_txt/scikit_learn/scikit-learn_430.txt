Green-16 commented on Jul 3, 2019
Meaning no offence.
I think there is an erro in function inplace_relu_derivative in sklearn.neural_network._base.py
def inplace_relu_derivative(Z, delta):
delta[Z == 0] = 0
I think the relu activation's derivative should be zero when x < 0.
delta[Z <= 0] =0 is it right ?
for the relu function:
f'(x)=1 x>0
f'(x)=0 x<=0
I can accept the result 1 will be omitted when f'(x) =1 ,but why the result 0 also be omitted ?
In sklearn.neural_network.multilayer_perception.py function _backprop:
inplace_derivative = DERIVATIVES[self.activation]
inplace_derivative(activations[i], deltas[i - 1])
the result will be wrong .
My English is not well,my apology.
Look forward to your replyã€‚
Best wishes!