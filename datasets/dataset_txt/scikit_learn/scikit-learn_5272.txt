Contributor
conradlee commented on Aug 21, 2012
This post is to follow up a discussion on the mailing list with @pprett .
This issue is not about a general monitoring API, just about the ability to add trees
The motivation for this feature request is monitoring (see below), but let's avoid discussion of a general monitoring API here (which should be the same API as used for monitoring other estimators). It might take a long time for us to all agree on a monitoring API, but in the meantime if someone implements a way to add more trees (or use a warm_start option in the fit method), then we'll have made some progress in the right direction.
Motivation: monitoring and early termination
Two of the most important parameters for the boosting estimators are learn_rate and n_estimators. In this document, Greg Ridgeway provides advice for settings these:
My rule of thumb is to set shrinkage as small as possible while still being able to fit the model in a reasonable amount of time and storage. I usually aim for 3,000 to 10,000 iterations with shrinkage rates between 0.01 and 0.001.
Often such a large number of iterations will be an overkill, and this will be clear if one monitors the accuracy on the out-of-bag estimate. In this situation (which is perhaps the usual situation), one should set the learning rate to be small, add some number of trees, check to see if the accuracy is still improving, and if so, keep adding trees. This requires the ability to add more estimators (perhaps using a warm_start option in the fit function, or a more() method).
@pprett mentioned
The original pull-request supported an additional parameter monitor which was called after each iteration using the current state of the model and allowed proper early stopping, however, we removed the parameter because we could not agree on the API.