ivic-katharina commented on Jun 19, 2015
Hi,
I am using sklearn.cross_validation.StratifiedKFold and sklearn.lda. I recently noticed different results on different machines. Also the behavior changed based on where I put the skf.
I marked the different places for the skf in the code (either Line 171 or Line 175)
The machines I used for testing were:
machine 1:
Xubuntu: 14.04
python 2.7.6
numpy 1.8.2
scipy 0.13.3
sklearn 0.14.1
machine2:
Redhat 7.1
python 2.7.9
numpy 1.9.1
scipy 0.15.0
sklearn 0.15.2
machine 3:
Redhat 6.6
python 2.7.9
numpy 1.9.2
scipy 0.15.1
sklearn 0.16.1
running the code with skf in line 171 (I shuffle the result vector afterwards) gives me the following output (I have 4 classes, so chance should be 0.25):
machine 1:
0
0.23125
0.24375
0.24375
0.2375
1
0.24375
0.2625
0.2625
0.24375
2
0.2625
0.23125
0.23125
0.24375
3
0.2625
0.2625
0.2625
0.24375
4
0.24375
0.2375
0.2375
0.2375
machine 2:
0
0.23125
0.24375
0.24375
0.2375
1
0.14375
0.16875
0.16875
0.14375
2
0.15
0.13125
0.13125
0.1375
3
0.1
0.11875
0.11875
0.10625
4
0.125
0.11875
0.11875
0.11875
machine 3:
0
0.23125
0.24375
0.24375
0.2375
1
0.1625
0.1375
0.1375
0.14375
2
0.15625
0.1375
0.1375
0.1375
3
0.19375
0.1875
0.1875
0.15625
4
0.16875
0.175
0.175
0.1625
If I put the sklearn on line 175, I get:
machine 1: same as if it were in line 171
machine 2:
0
0.23125
0.24375
0.24375
0.2375
1
0.24375
0.2625
0.2625
python/2.7.9_cns/lib/python2.7/site-packages/sklearn/lda.py:161: UserWarning: Variables are collinear
warnings.warn("Variables are collinear")
** On entry to DGESDDPM parameter number 10 had an illegal value
Traceback (most recent call last):
File "LDA_test.py", line 177, in
scores = cv.cross_val_score(clf, x[:,t:t+1], y, cv=skf).mean()
/python/2.7.9_cns/lib/python2.7/site-packages/sklearn/cross_validation.py", line 1151, in cross_val_score
for train, test in cv)
python/2.7.9_cns/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py", line 653, in call
self.dispatch(function, args, kwargs)
devel/python/2.7.9_cns/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py", line 400, in dispatch
job = ImmediateApply(func, args, kwargs)
python/2.7.9_cns/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py", line 138, in init
self.results = func(_args, *_kwargs)
python/2.7.9_cns/lib/python2.7/site-packages/sklearn/cross_validation.py", line 1239, in _fit_and_score
estimator.fit(X_train, y_train, **fit_params)
python/2.7.9_cns/lib/python2.7/site-packages/sklearn/lda.py", line 175, in fit
_, S, V = linalg.svd(X, full_matrices=0)
python/2.7.9_cns/lib/python2.7/site-packages/scipy/linalg/decomp_svd.py", line 101, in svd
raise ValueError('work array size computation for internal gesdd failed: %d' % info)
ValueError: work array size computation for internal gesdd failed: -10
machine 3:
ValueError Traceback (most recent call last)
LDA_test.py in ()
177
178 for t in range(x.shape[1]):
--> 179 scores = cv.cross_val_score(clf, x[:,t:t+1], y, cv=skf).mean()
180 print scores
181 numpy.random.shuffle(y)
/opt/hpc/cns/python/2.7.9/lib/python2.7/site-packages/sklearn/cross_validation.pyc in cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)
1359 train, test, verbose, None,
1360 fit_params)
-> 1361 for train, test in cv)
1362 return np.array(scores)[:, 0]
1363
/opt/hpc/cns/python/2.7.9/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in call(self, iterable)
657 self._iterating = True
658 for function, args, kwargs in iterable:
--> 659 self.dispatch(function, args, kwargs)
660
661 if pre_dispatch == "all" or n_jobs == 1:
/opt/hpc/cns/python/2.7.9/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in dispatch(self, func, args, kwargs)
404 """
405 if self._pool is None:
--> 406 job = ImmediateApply(func, args, kwargs)
407 index = len(self._jobs)
408 if not _verbosity_filter(index, self.verbose):
/opt/hpc/cns/python/2.7.9/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in init(self, func, args, kwargs)
138 # Don't delay the application, to avoid keeping the input
139 # arguments in memory
--> 140 self.results = func(_args, *_kwargs)
141
142 def get(self):
/opt/hpc/cns/python/2.7.9/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)
1457 estimator.fit(X_train, *_fit_params)
1458 else:
-> 1459 estimator.fit(X_train, y_train, *_fit_params)
1460
1461 except Exception as e:
/opt/hpc/cns/python/2.7.9/lib/python2.7/site-packages/sklearn/lda.pyc in fit(self, X, y, store_covariance, tol)
424 if self.shrinkage is not None:
425 raise NotImplementedError('shrinkage not supported')
--> 426 self._solve_svd(X, y, store_covariance=store_covariance, tol=tol)
427 elif self.solver == 'lsqr':
428 self._solve_lsqr(X, y, shrinkage=self.shrinkage)
/opt/hpc/cns/python/2.7.9/lib/python2.7/site-packages/sklearn/lda.pyc in _solve_svd(self, X, y, store_covariance, tol)
380 # Use SVD to find projection in the space spanned by the
381 # (n_classes) centers
--> 382 _, S, V = linalg.svd(X, full_matrices=0)
383
384 rank = np.sum(S > tol * S[0])
/opt/hpc/cns/python/2.7.9/lib/python2.7/site-packages/scipy/linalg/decomp_svd.pyc in svd(a, full_matrices, compute_uv, overwrite_a, check_finite)
97
98 # compute optimal lwork
---> 99 lwork, info = gesdd_lwork(a1.shape[0], a1.shape[1], compute_uv=compute_uv, full_matrices=full_matrices)
100 if info != 0:
101 raise ValueError('work array size computation for internal gesdd failed: %d' % info)
ValueError: On entry to DGESDD parameter number 10 had an illegal value
I get the error with the co-linearity and also I guess why it is different for the xubuntu and the redhat machines, however I would like to understand why I get values below chance, if I put the skf in line 171 for the redhat machines but not for the xubuntu machine.
Thank you,
Katharina
Here is the example-code I used:
import sklearn.lda as lda
import sklearn.cross_validation as cv
import numpy as np
import numpy.random

n_folds=10
y = np.arange(0,4).repeat(40)
numpy.random.seed(1253)


x = np.array([[ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.05,  0.05,  0.05,  0.05],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.05,  0.05,  0.05],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.05,  0.05,  0.05,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.05,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  ,  0.  ]])
skf = cv.StratifiedKFold(y, n_folds=n_folds)  # version 1
clf = lda.LDA()
for m in range(5):
    print m
    #skf = cv.StratifiedKFold(y, n_folds=n_folds) # version 2

    for t in range(x.shape[1]):
        scores = cv.cross_val_score(clf, x[:,t:t+1], y, cv=skf).mean()
        print scores
    numpy.random.shuffle(y)