Contributor
alexitkes commented on Aug 31, 2018
The Leaky-ReLU activation function for MLPs (max(x, alpha *x) where alpha is a small number around 0.01) is rumoured to work better than ReLU in some cases. Unfortunately, I din't find any way to use if tor MPLClassifier or MLPRegressor. I can try to implement it myself, but I am still not familiar enough with SciKit-Learn source code to do it quickly. Maybe there is a proper way to use it?