Contributor
rhiever commented on May 19, 2017
sklearn currently provides model-based feature importances for tree-based models and linear models. However, models such as e.g. SVM and kNN don't provide feature importances, which could be useful.
What if we added a feature importance based on shuffling of the features? e.g.:
Evaluate the model accuracy based on the original dataset
For each feature in the dataset:
a) Make a copy of the dataset
b) Randomly shuffle the current target feature
c) Evaluate the model accuracy based on the dataset with the shuffled feature
d) Compute the difference in the accuracies---this is the feature importance, where higher is better
Here's a (hacky) prototype that identifies the correct informative feature using a kNN:
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import numpy as np

X, y = make_classification(n_samples=1000, n_informative=1,
                           n_features=20, n_clusters_per_class=1, n_redundant=0)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)

clf = KNeighborsClassifier()
clf.fit(X_train, y_train)
clean_dataset_score = clf.score(X_test, y_test)

for index in range(X.shape[1]):
    X_train_noisy = X_train.copy()
    np.random.shuffle(X_train_noisy[:, index])
    X_test_noisy = X_test.copy()
    np.random.shuffle(X_test_noisy[:, index])
    clf.fit(X_train_noisy, y_train)
    noisy_score = clf.score(X_test_noisy, y_test)
    print(clean_dataset_score - noisy_score, clean_dataset_score, noisy_score)
Any interest for sklearn?