coderfi commented on Sep 19, 2016
Description
scikit-learn==0.18rc2 joblib.dump(...) creates ginormous serialized files compared to scikit-learn==0.17.1
Steps/Code to Reproduce
from sklearn import svm
from sklearn.externals import joblib
from random import randint
import cPickle
import numpy as np
import os
import tempfile

model = svm.SVC()
model.fit(np.random.rand(500, 50), [randint(0, 1) for _ in range(500)])

fn = tempfile.mktemp()
joblib.dump(model, fn)

pickle_sz = len(cPickle.dumps(model, protocol=2))
disk_sz = os.stat(fn).st_size
print "pickle size is %d bytes" % pickle_sz
print "on disk size is %d bytes" % disk_sz
print "joblib space usage is %dx better" % (pickle_sz / disk_sz)
os.remove(fn)
Expected Results
scikit-learn==0.17.1
pickle size is 205207 bytes
on disk size is 1099 bytes
joblib space usage is 186x better
Actual Results
scikit-learn==0.18.rc2
pickle size is 204820 bytes
on disk size is 204975 bytes
joblib space usage is 0x better
From 0.17.x, we basically lost the space saving benefits of using the optimized joblib serialization pipeline (compared to pickling).
Versions
Darwin-15.6.0-x86_64-i386-64bit
('Python', '2.7.9 (default, Dec 11 2014, 02:36:08) \n[GCC 4.2.1 Compatible Apple LLVM 5.1 (clang-503.0.40)]')
('NumPy', '1.11.1')
('SciPy', '0.18.0')
('Scikit-Learn', '0.17.1') (as well as '0.18rc2')