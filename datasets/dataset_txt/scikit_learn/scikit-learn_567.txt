arthurpaulino commented on Apr 23, 2019 â€¢
edited
Some metrics (e.g.: roc_auc_score) are compatible with probabilities, which may result in a better evaluation of the model depending on the application. And using such metrics with labels usually underestimates the accuracy of the model if we're intending to use it for outputting probabilities in production.
A solution for this problem is to allow the user to input a method parameter (detault='predict'), just like cross_val_predict does, on the cross_val_score and cross_validate functions.