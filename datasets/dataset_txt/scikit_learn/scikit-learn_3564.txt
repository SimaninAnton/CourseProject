peterrickwood commented on Oct 12, 2015
I think there is a memory leak or other unnecessary data copying going on when warm_start is used for GBM's. Here is a minimal program which reproduces the issue:
import sklearn
import sklearn.ensemble
import numpy as np
import random


#create a 2GB data set
train = np.random.random((500000, 2000))
traintarget = np.array([1 if random.random() < 0.1 else 0 for i in range(500000)])
print "Created initial data set"

#build a GBM classifier, warm start, 1 initial iteration
gbm=sklearn.ensemble.GradientBoostingClassifier(loss="deviance", 
            learning_rate=0.1,
            n_estimators=1,
            max_depth=8,subsample=1.0,
            verbose=1, warm_start=True)
gbm.fit(train, traintarget)
print "Fit initial GBM, now adding estimators"

#Now gradually add more models to the GBM and watch your memory use climb
for nest in range(2, 1000):
    print "nest "+str(nest)
    gbm.set_params(n_estimators=nest)
    gbm.fit(train, traintarget)
ðŸ˜• 1