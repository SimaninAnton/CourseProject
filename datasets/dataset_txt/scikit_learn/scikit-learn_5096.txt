Fkawala commented on Nov 28, 2012
The precision_recall_curve metric's output is occasionally meaningless, as in the following example,
Three remarks :
Meaningless outputs occurs as well using 0.13 or 0.12
if I use the lastest git version the auc function raises an exception, while it doesn't when using 0.12 version.
AssertionError: Reordering is not turned on, and The x array is not increasing:  [ 0.24195  0.24145  0.24161 ...,  1. 1.       1.     ]
The y_true and probas_ dumps are available here y_true and there probas_
Starting from the available example covering sklearn.metrics.precision_recall_curve I've wrote this piece of code which is responsible for the aforementioned picture
def plot_pr(model, X_test, y_test):

  probas_ = model.predict_proba(X_test)
  precision, recall, _thresholds = precision_recall_curve(y_test, probas_[:, 1])
  pr_auc = auc(precision, recall)

  pl.clf()
  pl.plot(precision, recall, label='P/R curve')
  pl.grid()
  pl.xlabel('Recall')
  pl.ylabel('Precision')
  pl.ylim([0.0, 1.05])
  pl.xlim([0.0, 1.0])
  pl.title('Precision-Recall: AUC=%0.2f' % pr_auc)
  pl.legend(loc="best")
  pl.savefig(open('./pr_curve_%d.png' % int(time.time()), 'a'), format="png")

  np.savetxt('precision.log', precision)
  np.savetxt('recall.log', recall)
  np.savetxt('proba.log', probas_)
Thank you for your help.
Fran√ßois Kawala.