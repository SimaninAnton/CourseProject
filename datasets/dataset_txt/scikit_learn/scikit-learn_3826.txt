amambrini commented on May 14, 2015
Let's initialize two ElasticNet models: one with default regularization parameter (alpha=1) and the other with a very small one (alpha=10^-10).
from sklearn import datasets
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import  ElasticNet

boston = datasets.load_boston()

clf  = Pipeline(
                [('poly', PolynomialFeatures(degree=2)),
                ('linear', ElasticNet(alpha= 1, positive=True, normalize=True, fit_intercept=False))]
            )

clf_low_alpha = Pipeline(
                [('poly', PolynomialFeatures(degree=2)),
                ('linear', ElasticNet(alpha= 10**-10, positive=True, normalize=True, fit_intercept=False))]
            )

clf.fit(boston.data, boston.target)
clf_low_alpha.fit(boston.data, boston.target)
If we now check the coefficients found they look very similar, even if I would expect the model with lower alpha to have less sparse coefficients
print clf.steps[1][1].coef_
[ 0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.2526706   0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.04613851  0.          0.00943962  0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.00990068  0.          0.00096327  0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.37549185  0.          0.          0.          0.          0.
  0.00291116  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.        ]

print clf_low_alpha.steps[1][1].coef_
[ 0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.40646651  0.          1.28668492  0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.08425349  0.          0.00834555  0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.0042923   0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.37904862  0.          0.          0.          0.          0.
  0.00281953  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.          0.
  0.          0.        ]
If we set positive = False instead, the behaviour is as expected (low alpha --> less sparse)
print clf.steps[1][1].coef_
[  0.00000000e+00   0.00000000e+00  -0.00000000e+00   0.00000000e+00
  -0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
  -0.00000000e+00   0.00000000e+00   4.58243351e-02  -0.00000000e+00
   4.91687963e-02  -0.00000000e+00   2.24149308e-03   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
  -2.81470431e-03  -0.00000000e+00  -1.26210743e-02   2.45852672e-04
   0.00000000e+00   5.46578959e-05   3.22868001e-03   1.20950822e-04
  -0.00000000e+00   0.00000000e+00   0.00000000e+00   4.83756478e-02
   2.74281481e-04  -4.80346476e-03  -8.11344389e-03   5.08861467e-04
  -0.00000000e+00  -1.03280118e-03  -4.02707231e-03   2.33088675e-02
  -0.00000000e+00   0.00000000e+00   0.00000000e+00   6.90941466e-03
   0.00000000e+00   0.00000000e+00   1.04100778e-03  -5.21388367e-02
  -2.42943500e-04  -2.80869383e-02  -0.00000000e+00  -0.00000000e+00
  -0.00000000e+00  -0.00000000e+00  -0.00000000e+00   0.00000000e+00
   5.14120817e-03   0.00000000e+00  -1.21280488e-03   0.00000000e+00
   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
  -0.00000000e+00  -3.62592668e-02  -0.00000000e+00  -0.00000000e+00
   0.00000000e+00   3.64972212e-01   2.46846503e-02   0.00000000e+00
  -0.00000000e+00  -1.58621964e-02   0.00000000e+00   1.24818234e-02
  -1.82623558e-02   5.16312350e-05   7.13771541e-03   5.65647979e-03
  -2.19329102e-04  -0.00000000e+00  -5.78219947e-04  -3.07318525e-03
   1.36077125e-01  -0.00000000e+00  -4.41926738e-03   0.00000000e+00
  -3.79504685e-03   0.00000000e+00  -9.26308380e-02   4.60220882e-03
   0.00000000e+00  -3.19822209e-04  -1.50184687e-02  -5.42023964e-05
   5.44324553e-03   3.56445993e-06  -1.08127402e-03  -2.67685557e-02
  -2.21209484e-03  -0.00000000e+00  -1.41600950e-05  -1.47368805e-04
   3.08194795e-02]

print clf_low_alpha.steps[1][1].coef_
[ -1.50862864e+01  -2.75941775e+00   3.24244559e-01  -3.02630532e+00
   1.87882406e+02   7.19923064e+01   8.40881854e+00   4.74483783e-01
  -1.45975270e+01  -2.52376779e-01  -1.02794498e-01   5.97632062e-01
   8.95271276e-02   2.76864954e-01   1.91662369e-03   1.50846899e-01
   1.49059269e-01   2.62781966e+00  -8.59124184e-01   1.33219338e-01
  -2.24207542e-03  -6.92195423e-02  -6.46914899e-02   9.72545053e-05
   4.94270988e-02  -2.84266273e-04   2.06435376e-02  -2.38875495e-04
  -1.73142877e-03  -8.42860648e-02  -1.00007461e+00   1.18532036e-02
   1.87363718e-04  -8.34754617e-03   1.17683778e-03   4.55322172e-04
  -3.39351100e-03   1.24472897e-04  -4.92796782e-03   1.94268393e-02
  -2.10948366e-01   2.71983576e+00   1.78733997e-01   2.91153912e-03
   8.62068419e-02  -1.03147860e-01   1.61876804e-03  -3.88375268e-02
   1.01365952e-03  -1.42838575e-02  -1.33823543e+02  -3.11752265e+01
  -5.80708985e+00   3.88945754e-02   1.27873198e+00  -3.53640588e-01
   1.57569213e-02  -7.28417688e-01   2.26555207e-02  -3.14537477e-01
  -3.85515609e+01   5.97935833e+00  -2.27752338e-01   1.59744714e+01
  -1.74384195e-01   6.15565511e-03  -7.55575649e+00  -3.58025342e-02
   1.04129649e+00   5.56508446e-01  -3.31371882e-02   3.75858214e-01
  -2.00806651e-01  -1.04823003e-02  -2.71012554e-01  -3.69172624e-03
  -1.95155661e-01   4.54246699e-05   1.30082919e-03   1.38145879e-02
  -4.93516521e-04   4.21231778e-03  -5.11191858e-04  -5.76843660e-03
   4.23433194e-01  -1.59116519e-01  -2.46035305e-03  -4.21852029e-02
   9.29104367e-04   9.26026657e-02  -1.43172067e-01   9.53435925e-03
   1.11252025e-01  -1.67697340e-03  -4.05486057e-02  -7.68553996e-05
   8.27517324e-03   8.38188347e-05  -2.98746645e-05   4.28795735e-02
  -1.35474484e-04   1.65584634e-02  -3.75860887e-05  -3.76896736e-04
   1.36671658e-02]