mshih2 commented on Mar 23, 2018
Description
Hi,
I just discovered today that the "feature_importances_" value of a feature is calculated based on the following equation:
(value of the importance of a feature) =
(sum of the importances of the feature in all trees)/(total number of trees)
However, if the "max_feature" is not equal to total number of features, it means not all features are selected for the construction of each tree. For features not selected to construct a tree, the importance of that feature in the tree will certainly be 0. I believe these 0s should be excluded from the calculation, so the new function should be like:
(value of the importance of a feature) =
(sum of the importances of the feature in all trees greater than 0)/(total number of trees with importance of the feature greater than 0)
It is because if a feature is not selected to construct a tree, then it's importance in that tree should be "unknown" but not "0".
Below is an example describing my stand point:
There are three features 'A', 'B', 'C' in the training data. The Random Forest model used to train the data has following parameters set: max_feature =2 and n_estimators =100.
For the old calculation, the importance of feature 'A' is the sum of importance of 'A' in 100 trees divided by 100. This way cause a problem that when 'A' is not selected to construct a tree, then its importance in that tree is 0. I believe if 'A' is only selected by 40 trees due to the max_feature limitation, then only the importance of A in these 40 trees should be considered in the calculation. In other words, the '0's should be excluded from calculation.
Hope this helps you improve the model, thanks!
Mei-Cheng
Steps/Code to Reproduce
This is a conceptual discussion only, so code is not provided here
Expected Results
Actual Results
Versions
0.19.1