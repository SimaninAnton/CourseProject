Contributor
ashimb9 commented on Aug 10, 2017
Description
I don't know if this generally applicable, but I often find myself requiring the log-likelihood of the entire dataset rather than a per-sample average. And the score() function of some estimators (like the Gaussian Mixture Model) return a per-sample average which of course means I need to multiply with n_samples every time I need the log-likelihood for the entire data. Even estimator methods are having to perform this multiplication -- for instance, following is the code for the AIC and BIC methods for GMM (gaussian_mixture.py):
def aic(self, X): return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()
OR
def bic(self, X): return (-2 * self.score(X) * X.shape[0] + self._n_parameters() * np.log(X.shape[0]))
I don't know if others find themselves in a similar position or not. And maybe the average score is indeed more commonly required for situations I might not frequently encounter. Or maybe there was another reason for doing this that I might not be aware of. I don't know. But if folks do indeed need the full data log likelihood more often than the per-sample average and there is no particular programming reason for doing this, then maybe we could consider returning likelihood for the entire dataset rather than the per-sample mean for this (and other?) estimators?