Member
rth commented on Mar 29, 2017 â€¢
edited
As stated in the title, with scipy <= 0.18.1 csr_matrix.toarray() fails for arrays with more than 2**31 elements (cf. parent issue scipy/scipy#7230).
This is a fairly uncommon issue because the input arrays would take >40 GB RAM in this case, however it was observed in,
http://stackoverflow.com/questions/42543125/could-not-convert-integer-scalar-error-when-using-dbscan
FreeDiscovery/FreeDiscovery#126
it could for instance affect pairwise_distance calculations (and all the dependent methods), sparse random projections, predictions with BaseSGDRegressor etc. when using sparse arrays.
It sounds like this issue would be fixed in the upcoming scipy 1.0 release via the csr_todense optimization
scipy/scipy#7081 however the question is whether anything should be done to patch this in scikit-learn for previous scipy versions?
This could be partially addressed by providing a fallback toarray() implementation inside safe_sparse_dot(..., dense_output=True) (which is where this issue was initially detected), however this would not address all the other toarray() calls in scikit-learn. Not sure what would be the best solution...
cc. @perimosocordiae