spitz-dan-l commented on Oct 11, 2014
Hi,
I wanted to bring up an issue with the parallel execution model of scikit-learn that has bitten me numerous times.
I often find myself wishing to run a sklearn task that can be parallelized with n_jobs. However, I may wish to run the task itself in parallel with even more tasks. An example of this is running a grid search on multiple different training sets at once, or with different cv generators. Or I may wish to run Validation Curves on multiple different parameters at once. This isn't currently possible because you can't kick off a joblib.Parallel() inside of a subprocess.
In some cases, it makes sense to simply break the tasks out into separate programs. However, often that breakdown doesn't make sense and it would really be best for the code if it could all be part of the same program.
The workaround I have used is to reimplement various parts of the sklearn internals in such a way that rather than create all the joblib.delayed() jobs and execute them with joblib.Parallel in the same place, I create the jobs in one place and return them, and then collect each task's delayed jobs and run them together under one Parallel() outside. I then take the results of the completed delayed jobs and send them back into their respective tasks to be aggregated and processed.
So my question is, would there be interest in adding this capability to sklearn? I think a basic pattern of providing an option to either return the completed results, or a (delayed_jobs, result_aggregator_function) tuple would go a long way. I would be interested in working on the pull request for it if there was interest in such a thing.
Thanks,
Dan Spitz
üëç 2