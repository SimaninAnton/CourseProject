pkch commented on Dec 28, 2015
Currently, MLPClassifier makes an automatic decision based on the shape of the target data. If the target is 2D with more than one column, it assumes that it is multi-label and automatically uses sigmoid/mse (even if each row contains only a single 1). If the target is 1D or 2D with one column, it assumes that it is multi-class, and automatically uses softmax / cross-entropy (same as multinomial log-loss).
While conceptually it might make sense (for multi-class it would seem we would want probabilities to sum up to 1), there are a number of papers that say that sigmoid/MSE in some cases work better for multi-class classification. If a user wanted to force output layer to be sigmoid/MSE, they could actually do it with the current MLPClassifier by encoding the categorical target as hot-one.
This seems like a rather unexpected magic (switch between a single column to hot-one changes the output layer). Would it perhaps be better to be more explicit and allow the user to specify the loss function and the output layer activation function?