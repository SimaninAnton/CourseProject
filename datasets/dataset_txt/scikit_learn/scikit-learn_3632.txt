Contributor
choldgraf commented on Aug 29, 2015
Right now LeavePLabelOut takes a set of labels as input, and goes through all iterations possible leaving p labels out. This can result in a LOT of possible permutations if you have lots of labels or if you're leaving lots of labels out on a given iteration.
It might be useful to have an n_iter parameter to stop the iteration after a certain number of goes in case the user basically just wants to do random subsampling but with more control over which data points stay with other data points.
In that case, one gotcha is that the combinations of labels are calculated as such:
    def _iter_test_masks(self):
        comb = combinations(range(self.n_unique_labels), self.p)
        for idx in comb:
            etc...
As a result, comb will successively step through the sorted labels. If you are only using a subset of combinations, then you'd probably want it to be random so that you're biasing which labels are in the training vs. test sets. So the change I'd propose is to add an n_iter flag to this CV object, add a check to make sure its a positive int in __init__, and then to update the iteration method to:
    def _iter_test_masks(self):
        comb = combinations(range(self.n_unique_labels), self.p)
        if n_iter is not None:
            comb = permutation(comb)[:n_iter]
        for idx in comb:
             etc...
Would that be a welcome PR?