Member
ogrisel commented on Sep 29, 2011
When trying to load a 1.1GB dataset in svmlight format from kaggle.com skearn.datasets.load_svmlight_file is trying to allocate several times this amount as intermediate python lists before being killed by the OS.
IMHO: This code should be refactored as a two passes algorithm that counts the number of samples (for labels and indptr which has length n_samples + 1) and the total number of non-zero values (for data and indices) so as to pre-allocate them upfront and then to a second pass to actually fill the arrays with the values.
FYI the former C++ impl that has moved to https://github.com/mblondel/svmlight-loader loads the same dataset without wasting memory.