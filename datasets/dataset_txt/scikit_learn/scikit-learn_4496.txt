danstowell commented on Feb 27, 2014
I'm happily using RandomForestClassifier for some large datasets. (I'm on sklearn 0.14.1, numpy 1.7.0, virtualenv 1.9.1, python 2.7, linux.) I'm using an invocation like:
self.classif = RandomForestClassifier(n_estimators=500, criterion='entropy', random_state=np.random.RandomState(0), verbose=0)
self.classif.fit(feats, gts)
...where the dimensionality of features is (17873, 26) and of gts is (17873, 55). This runs fine for most datasets (indeed, larger datasets) but I have one dataset for which it leads to a MemoryError() followed by a segfault:
 Exception MemoryError: MemoryError() in 'sklearn.tree._tree.Tree._resize' ignored
 Segmentation fault
MemoryErrors I can live with, but segfault is a bad sign. I'm sorry I don't have a reproducer for you (the data is large and on a remote machine, and I have a deadline to meet) - but I thought it would be better to report this rather than ignore it.