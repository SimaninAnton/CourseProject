zachmayer commented on Dec 13, 2018
Svenstrup et. al. 2017 propose an interesting way to handle hash collisions in hashing vectorizers: Use 2 different hashing functions, and concatenate their results before modeling.
They claim that the combination of multiple hash functions approximates a single hash function with much larger range (see section 4 of the paper).
I'd like to try this out with some text data I'm working with in sklearn. The idea would be to run the HashingVectorizor twice, with a different hash function each time, and then concatenate the results as an input to my model.
In theory, this would let me use a much smaller hash space, while still having few hash collisions.
Does this seem like an interesting idea? If so, what might be the best way to incorporate it into SKlearn?