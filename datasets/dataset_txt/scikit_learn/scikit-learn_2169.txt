eazhary commented on May 31, 2017 â€¢
edited
Confession: This is a usage question that should go to stackoverflow. I tried asking this question on stackoverflow but I never got an answer. So I am guessing that this feature is not implemented, that is why I am posting here, maybe we can add it to the list of new features..
.....
PROBLEM BACKGROUND
I am trying to predict the best Customer Service Agent to handle a Customer based on historical data of customer/Agent interactions and the outcome of these interactions. So I gathered a lot of features describing the customer and prepared the training data.
in the training data there are GOOD interactions where the customer was satisfied with the Agent, accordingly, I am training the model that for similar customers with the same features predict this AGENT...
Also in the training data there are BAD interactions where the Agent/Customer combination didn't produce a satisfactory result... So I want to train the model that similar customers should NOT be handled by this Agent (Negative training sample).
My Thoughts:
This is a typical multiclass classification problem, and as far as I understand in sklearn, all classifiers can handle multiclass... and I think they use OVR (onevsrest)....
If this is the case then I need a way to tell the algorithm, to include the BAD samples (Negative training samples) in the "rest" of the onevsREST...
Is there a simple way to do this through the normal sklearn classifiers... or I have to do something else