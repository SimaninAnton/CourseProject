Contributor
lucasdavid commented on Nov 30, 2015
Silva and Tenenbaum described in the paper Global versus local methods
in nonlinear dimensionality reduction an extension of ISOMAP called L-ISOMAP: the idea that n randomly selected samples could be extracted from X and used to create the low-dimensional embedding. All samples' dissimilarities would then be used to triangulate the remaining samples' position in the embedding. That way, we could drastically reduce the time/space complexity of the algorithm and still find a fair solution.
I believe scikit-learn should implement this! The paper has 702 citations, according to google scholar and this algorithm has been implemented for other languages (e.g., matlab). Furthermore, I think it would be even more simpler in scikit-learn, as Isomap is already built over KernelPCA and can easily embed new data points.
I played a little bit with the code and got some good results:
Benchmark 0: {'n_samples': 1000, 'n_components': 2, 'n_neighbors': 10, 'n_landmarks': 400}
Isomap: 0.46 sec 
LandmarkIsomap: 0.14 sec 

Benchmark 1: {'n_samples': 2000, 'n_components': 2, 'n_neighbors': 10, 'n_landmarks': 800}
Isomap: 1.9 sec 
LandmarkIsomap: 0.5 sec 

Benchmark 2: {'n_samples': 4000, 'n_components': 2, 'n_neighbors': 10, 'n_landmarks': 800}
Isomap: 8.2 sec 
LandmarkIsomap: 0.72 sec 

Benchmark 3: {'n_samples': 10000, 'n_components': 2, 'n_neighbors': 10, 'n_landmarks': 1200}
Isomap: 57 sec 
LandmarkIsomap: 2.3 sec 

Benchmark 4: {'n_samples': 30000, 'n_components': 2, 'n_neighbors': 10, 'n_landmarks': 1200}
Isomap: (failed)
LandmarkIsomap: 5.9 sec
How the reductions look like:
How is this any different from manually reducing the data set and then applying regular Isomap?
As you can see in the first reduction, it didn't manage to learn the swiss-roll of 1000 samples. I believe this happened because of the choice for the landmarks (random, as suggested by the authors). I've read some content on the internet and apparently there's a smarter way to select the landmarks based on optimization functions. This doesn't seem trivial as selecting samples at random, so I suggest we abstract users from doing this, hence improving the method's reliability.
Does that interest you guys?