12ycli commented on Dec 26, 2018 â€¢
edited
I use cosine_similarity in sklearn and found it very slow in my situation.
my data has millions of instances. Every instance has two sentences represented as 100 thousand dimensions(the length of words dictionary) sparse vector(csr). I go through every instance and use cosine_similarity to calculate their cosine similarity and found it very slow.
I saw the source code and become confused. cosine_similarity normalize x and y first, and then calculate x dot y. why not x dot y first and divide their length? It can save some divide operation.
I write my code below to cosine similarity. In my situation, v1 and v2 are row vectors.
            len_v1 = (v1 * v1.transpose())[0,0]
            len_v2 = (v2 * v2.transpose())[0,0]
            v1_dot_v2 = v1 * v2.transpose()
            rets = v1_dot_v2 / math.sqrt(len_v1 * len_v2)