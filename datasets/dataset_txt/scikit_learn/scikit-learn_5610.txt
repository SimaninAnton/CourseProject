Member
jakevdp commented on Jun 3, 2011
I've been playing with some benchmarking of nearest neighbors using algorithm='brute', algorithm='ball_tree', and also scipy.spatial.ckdtree.
The basic gist is this: BallTree beats cKDtree for just about every situation. BallTree and brute force have more complicated scalings. Currently, the 'auto' algorithm uses ball_tree for nfeatures<20, and brute force otherwise. This is a bit too simplistic, and will lead to huge inefficiencies in some cases.
There are a few things that affect the speed of neighbor searches:
number of samples N. brute-force query time grows as O[N]. Ball tree grows as O[log(N)]
dimensionality of data D. brute-force query time grows as O[D]. Ball tree query time may grown faster or slower than this, depending on data structure. Sparse or structured data will be efficiently searched by the ball tree. Dense data which fills the whole space uniformly may lead to faster than O[D] growth.
intrinsic dimensionality of the data d: this may be a linear or nonlinear embedded manifold. This makes no difference for brute force searches, but for d<D, ball tree performance vastly improves.
number of neighbors k: again, this makes very little difference for brute force, but due to the queueing and sorting during the ball tree query, k>1 can slow down the process in a way that is difficult to quantify.
All that to say, the D<20 cutoff is a bit too simplistic. It's hard to determine what the crossover point is, but I think we should assume that data is structured, i.e. d<<D, and decide which algorithm to use based on N and k, as well as D. Any thoughts on how to better quantify this? Should we run a vast suite of benchmarks against some of the sample datasets and empirically determine the turning-points? It would be nice to have a more sophisticated auto algorithm choice.