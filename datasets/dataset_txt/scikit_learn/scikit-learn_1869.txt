Member
jnothman commented on Sep 25, 2017
Users are tempted to run metric(y_true, cross_val_predict(...)). This is not equivalent to cross_val_score(...) which takes an average over cross-validation folds; nor is it a standard or appropriate measure of generalisation error.
I consider valid uses of cross_val_predict to be:
visualisation of training set predictions
model blending: you have an ensemble in which a "downstream" estimator learns from the predictions of an "upstream" estimator, so you should train the downstream not on the true labels, nor on the upstream estimator's predictions on its training data as both are too biased. cross_val_predict is not biased in the same way.
Do you agree with this summary, @GaelVaroquaux?