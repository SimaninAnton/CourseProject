HarryShomer commented on Feb 11, 2019
Description
I apologize in advance if this isn't actually an issue but a misunderstanding on my part.
Basically what I'm dealing with is the fact that changing the regularization parameter alpha for the ridge regression I'm running has very little effect on the coefficient estimates. What is therefore happening is that it is regularizing the coefficients, in my opinion, too much (you will have to take my work for it).
I'm fitting the model like this with the data already standardized.
model = Ridge(alpha=alpha)
fit_model = model.fit(features, labels,  sample_weight=sample_weights)
To give you an example of what I mean about the coefficients, here is a comparison of a few coefficients for varying values of alpha (I'd give you all but I'm dealing with fairly high dimensional data - 586,582 x 1875):
   .1        1e-25     1e-50
0  0.000075  0.000105  0.000105
1  0.000077  0.000107  0.000107
2  0.000082  0.000114  0.000114
3  0.000083  0.000116  0.000116
4  0.000107  0.000149  0.000149
5  0.000109  0.000151  0.000151
As you can see, there is only about a ~28% increase from alpha=.1 to 1e-25 and there isn't even a difference between 1e-25 and 1e-50 which seems kind of odd.
As a sanity check on my part I then fit the data using a standard linear regression to see what the coefficients would look like. I was also interested to see what would happen when I made alpha=0 for Ridge.
model = LinearRegression()
fit_model = model.fit(features, labels,  sample_weight=sample_weights)
and
model = Ridge(alpha=0)
fit_model = model.fit(features, labels,  sample_weight=sample_weights)
Oddly enough, when looking at the same coefficients as b4 we see that the two "equivalent" models aren't nearly the same:
   Linear R   alpha=0
0   0.00323  0.000105
1   0.00169  0.000107
2   0.00200  0.000114
3   0.00294  0.000116
4  -0.00302  0.000149
5  -0.00259  0.000152
Alpha=0 is just about the same as what I have above for 1e-25 and 1e-50 which strikes me as kind of weird (which leads me to wonder if the Ridge doesn't accept 0). Either way, something like alpha=1e-50 should be small enough to be similar to a regular ordinary least squares. I think?
Does anyone have any idea what could be causing the big discrepancy?
Thanks!
Versions
System:
python: 3.6.5 |Anaconda custom (x86_64)| (default, Apr 26 2018, 08:42:37) [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
executable: /anaconda/bin/python
machine: Darwin-17.5.0-x86_64-i386-64bit
BLAS:
macros: SCIPY_MKL_H=None, HAVE_CBLAS=None
lib_dirs: /anaconda/lib
cblas_libs: mkl_rt, pthread
Python deps:
pip: 18.0
setuptools: 39.1.0
sklearn: 0.20.2
numpy: 1.15.4
scipy: 1.1.0
Cython: 0.25.2
pandas: 0.23.4