Contributor
bryandeng commented on Mar 22, 2016
During my work on #6090, Travis shows a persistent failure:
Traceback (most recent call last):
  File "/home/travis/miniconda/envs/testenv/lib/python3.5/site-packages/nose/case.py", line 198, in runTest
    self.test(*self.arg)
  File "/home/travis/sklearn_build_latest/scikit-learn/sklearn/utils/testing.py", line 319, in wrapper
    return fn(*args, **kwargs)
  File "/home/travis/sklearn_build_latest/scikit-learn/sklearn/utils/estimator_checks.py", line 975, in check_classifiers_train
    assert_greater(accuracy_score(y, y_pred), 0.83)
AssertionError: 0.64666666666666661 not greater than 0.83
    """Fail immediately, with the given message."""
>>  raise self.failureException('0.64666666666666661 not greater than 0.83')
This test calculates the accuracy of predictions over the training set, which should be greater than 0.83.
It's reasonable for supervised learning, but for semi-supervised learning, in which for now -1 is used as marker for unlabeled data, predictions over the training set are training labels with -1 replaced by 1 or 0 for positive or negative samples respectively.
For example, y = [1, 0, -1, 0, -1], y_pred = [1, 0, 1, 0, 0].
Then the accuracy defined in the current way is below 0.83.
Do we need to modify accuracy_score() to take into account semi-supervised learning?