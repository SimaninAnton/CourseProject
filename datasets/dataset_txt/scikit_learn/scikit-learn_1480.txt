Contributor
dpwe commented on Mar 21, 2018
label_ranking_average_precision_score is a measure of multi-class multi-label classifier performance, defined as the average precision over all the truth labels in an eval set of samples. The value being averaged is the proportion of correct labels in a per-sample ranked list of labels (classes) that is truncated at each true label.
As presently calculated, the metric is
(1/num_samples) * (sum of per-sample average precisions)
where
per-sample average precision = 1/num_true_labels * sum_(L in true labels for this sample) (proportion of true labels in ranked label list for this sample truncated at true label L)
Note that label_ranking_average_precision_score is only interesting if your eval samples bear multiple labels, otherwise it's the same as mean reciprocal rank.
If, however, your samples have differing numbers of true labels, the contribution of each individual true label is diluted by the total number of true labels for the samples in which it occurs.
I would prefer that the average was calculated across each individual ground-truth label, meaning a single sample would contribute to the overall metric in proportion to the number of true labels it bears.
Although I can't make a strong argument to prefer the values obtained averaging-across-samples or averaging-across-true-labels, there is one interesting advantage to averaging-across-true-labels: You can calculate a per-class label ranking average precision by taking all the instances of the ground truth label c in your eval set (by definition, at most one per sample), and averaging the precisions of all those truncated ranked lists. If you then form a weighted average of the per-class average precisions weighted by the total count of that class in the ground truth, you get the averaged-by-label (unconditional) value for the whole data set.
If you normalize each sample's precision by the number of ground-truth labels in that sample (current behavior), it's not possible to get a meaningful per-class precision that you can average up in this way - each individual precision value has to be discounted by total number of labels in that sample, so the per-class average is strongly confounded by the label co-occurrence statistics for that class.
I'm wondering how hard it would be to change the definition of label ranking average precision to average over individual labels instead of samples as I'm proposing, or whether perhaps it's worth introducing another new metric?