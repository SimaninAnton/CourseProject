arita37 commented on Jul 3, 2016
Am using Random Forest with scikit learn. RF overfits the data and prediction results are bad.
The overfit does NOT depend directly on the parameters of the RF: NBtree, Depth_Tree
Overfit happens with many different set of parameters (Tested it across grid_search).
To remedy: I tweak the initial data/ down sampling some results in order to affect the fitting (Manually pre-process noise sample).
0) Pre-optimized the hyper-parameter (to reduce over-fitting)
1) Tweak the input data (down sample to reduce noise)

2) Loop on random generation of RF fits, 
3)  Get RF prediction on the  data for prediction
4)  Select the model which best fits the "cross validation data" (not the initial calibration data).
This Monte carlos approach is very consuming, Just wondering
if there is another way to do cross validation on random Forest ?
(ie NOT the hyper-parameter optimization).
Think this is more of the Loss function optimization including overfit parameters in RF.
Put in StackOverflow, but question is more technical/statistics relevance than IT/programmer,
so I put here since it may help other people.
http://stackoverflow.com/questions/38151615/specific-cross-validation-with-random-forest
PS: It would be good to have a forum/place where we can discuss/put questions about methodology
(since stackoverflow is more about coding issues....)