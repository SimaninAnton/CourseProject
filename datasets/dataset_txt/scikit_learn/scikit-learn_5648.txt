Member
mblondel commented on Feb 22, 2011
For precomputed kernels, a square matrix is not an efficient way to store the kernel matrix (since the kernel matrix is symmetric).
We should create a kernel object interface instead. Advantages:
The object can store the LRU cache
This gives a way for the user to handle kernel re-computations
Internally the gram matrix can be stored in packed format
This will be useful when we create our own kernel-based estimators (some like Ridge already support kernels, they just lack the interface)
This handle nicely the kernel computations between test instances and training instances
The object could be numpy-compatible:
kernel = GaussianKernel(X_train, sigma=0.5)
print kernel[i, j] # recompute only if not cached
print kernel.compute(X_test)
Question: shall we create our own LRU object or shall we just bind libsvm's?
Since there's plan to bind libsvm's cross-validation code, this also means that the cache will be used more efficiently for cross-validation even when kernel="precomputed".
(Sorry for opening many tickets lately: I acually intend to help close them when I get more time ;-)