Contributor
deto commented on Apr 20, 2017
I wanted to call attention to something that I believe is an error in the tSNE gradient descent implementation. Specifically with how the updates to the delta-bar-delta 'gain' are computed.
Compare these implementations:
Scikit-learn
C++
R
MATLAB
The way the gain is updated in the Python version looks to be opposite of all the others and I believe it is incorrect.
If I understand delta-bar-delta correctly, the idea is to increase the update for coordinates which are consistently heading in the same direction. However, since the gradient is subtracted from the update, gain should increase on coordinates in which update and gradient are of opposite sign.
This is the way it happens in the C++, R, and MATLAB versions. But not in the Python version.
Also, interestingly, the parameters are slightly different (0.05 and 0.95) instead of (0.2 and 0.8). I wonder if the smaller parameters were chosen for the Python implementation specifically because the gain was being done in the wrong direction and so minimizing it's affect was beneficial. I propose both fixing how the gain variable is computed as well as restoring the same parameters that all the other implementations are using here.
I was having issues with convergence and that brought me here in the first place.