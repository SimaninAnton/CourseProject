droland1 commented on Dec 29, 2014
My input matrix has approximately 1.3 million examples and 500k features with float64 data. I initialize the matrix as a sparse lil with the shape set to the exact number of features and an overestimate of the number of examples. After I've loaded my data into the matrix, I resize it to fit the actual number of examples like this:
input_matrix.rows.resize(examples_count,refcheck=False)
input_matrix.data.resize(examples_count,refcheck=False)
input_matrix._shape = (examples_count,feature_count)
and then convert it to a csr (using tocsr method). My svm LinearSVC model parameters are: C = 1.0, loss = l2 , penalty = l1, tol = 1e-4, dual = False, fit_intercept = False, intercept_scaling = 1.0. When I call the fit method on the input matrix and target vector (a numpy array) it causes a segmentation fault every time.
However, if I assign the input matrix to a slice of the entire matrix (ie input_matrix = input_matrix[ : , :] prior to passing it to fit, the fit method works. I discovered this in a process of trying smaller slices of my input_matrix, slowly increasing it to see how large it could be without causing a crash. It turned out that I could use the entire matrix as long as it was sliced before calling fit.
If I start with a smaller input matrix, fit will work without first slicing; an earlier matrix had about 200k features and maybe 250k examples and it worked fine.
I've experimented with my standard Ubuntu install of scipy/scikit learn/numpy (0.12.0, 0.14.1, 1.7.1) and the latest versions (0,16.0, 0.15.2, 1.9.1) in a virtualenv with the same results.
My machine has 8 cpus (Intel Xeon X5460) and 32gb ram running Ubuntu 13.1. The machine has plenty of free memory when calling fit. When the fit runs properly with the sliced matrix, the method takes less than 10 minutes to complete and the results seem reasonable.