arita37 commented on Jul 10, 2016 â€¢
edited
Description
We need sometimes to constraint the feature selection at each node when building a decision tree.
Reasons:
Data is mainly noise (40% noise) --> Selected feature will overfit the noise....(OOB too !)
Some features are related to physical constraints (physical hierarchy) of a system.
Search space of the feature is too large + Too many noise in data such as
random selection of feature is efficient.
Possible remedy :
  Pass as argument a hierarchy of pre-selected sub-features :
  Random Feature select will select randomly from the pre-selected list of features.
Example:
          preselected_hiearch_features =
          [  [ 5,8,9 ]  ,                          #Node 0, Hiearchy 0
             [5,8,9 0,2,3],                     #Node 1, Hiearchy 1
             [5,8,9 0,2,3,7,6,12],          #Node 2, Hiearchy 2
          ]
For the 3st first level of nodes, random feature select will select among those pre-selected list.
After, it will select among ALL the features as normal.
It will improve significantly for very noisy system where most of the FIT are overfit on noise.
Expected Results
Actual Results
Versions