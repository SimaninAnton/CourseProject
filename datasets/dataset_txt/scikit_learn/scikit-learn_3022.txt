tomtung commented on May 2, 2016 â€¢
edited
Description
The bug happens when X is sparse and initial cluster centroids are given. In this case the means of each of X's columns are computed and subtracted from init for no reason.
Steps/Code to Reproduce
import numpy as np
import scipy
from sklearn.cluster import KMeans
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data

# Get a local optimum
centers = KMeans(n_clusters=3).fit(X).cluster_centers_

# Fit starting from a local optimum shouldn't change the solution
np.testing.assert_allclose(
    centers,
    KMeans(n_clusters=3, init=centers, n_init=1).fit(X).cluster_centers_
)

# The same should be true when X is sparse
X_sparse = scipy.sparse.csr_matrix(X)
np.testing.assert_allclose(
    centers,
    KMeans(n_clusters=3, init=centers, n_init=1).fit(X_sparse).cluster_centers_
)
Expected Results
Both asserts should pass.
Actual Results
The second assert fails.
Versions
Darwin-15.4.0-x86_64-i386-64bit
Python 2.7.11 (default, Jan 22 2016, 08:29:18)
[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)]
NumPy 1.11.0
SciPy 0.17.0
Scikit-Learn 0.17.1