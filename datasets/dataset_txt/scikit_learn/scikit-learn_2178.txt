JohnNapier commented on May 24, 2017 â€¢
edited
Let us compute the oob score of a bagged classifier.
import numpy as np
import pandas as pd
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier

N = 50
randState = 5
label = 'Label'
features = ['A','B','C']

labels = np.random.randint(3, size = N) - 1
df = pd.DataFrame( labels , index=range(N), columns=[label] )
for col in features:
    df[col] = df[label] + 0.01 * np.random.rand( N )

clf = BaggingClassifier(base_estimator = KNeighborsClassifier(), n_estimators = 10, oob_score = True, random_state = randState )
clf.fit(df[features], df[label])
print clf.oob_score_
Here, clf.oob_score_=0.0.
Now, you would not expect that the OOB accuracy is a function of the class labels...
df.loc[ df[label] == -1 , label ] = 2
clf = BaggingClassifier(base_estimator = KNeighborsClassifier(), n_estimators = 10, oob_score = True, random_state = randState )
clf.fit(df[features], df[label])
print clf.oob_score_
Now, clf.oob_score_=1.0.
Clearly, OOB score should not be a function of the labels arbitrarily chosen for the classes.
sklearn.version: '0.18.1'
numpy.version: '1.11.3'