Contributor
themrmax commented on Oct 11, 2016 â€¢
edited
As raised in http://stackoverflow.com/questions/39948138/sklearn-featurehasher-parallelized/39951415 many (all?) transformers could be made parallel, would this make sense as a ParallelTransformerMixin, something like
from sklearn.externals.joblib import Parallel, delayed
import numpy as np
import scipy.sparse as sp

class ParallelTransformerMixin:
    def transform_parallel(self, X, n_jobs):
        transform_splits = Parallel(n_jobs=n_jobs, backend="threading")(
            delayed(self.transform)(X_split)
            for X_split in np.array_split(X, n_jobs))

    return sp.vstack(transform_splits)