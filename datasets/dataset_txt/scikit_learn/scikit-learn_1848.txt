antonio-fiol commented on Oct 8, 2017 â€¢
edited
Steps/Code to Reproduce
import sklearn
from sklearn.datasets import make_classification
X, Y = sklearn.datasets.make_classification(n_samples = 1000, n_features=5, n_redundant=2, 
                                            n_informative=3, n_classes=3,
                                            n_clusters_per_class=2)
from sklearn.decomposition import PCA
pca = PCA(n_components='mle')
pca.fit(X)
Actual Results
TypeError                                 Traceback (most recent call last)
<ipython-input-25-76d1451f7528> in <module>()
      5 from sklearn.decomposition import PCA
      6 pca = PCA(n_components='mle')
----> 7 pca.fit(X)
      8 
      9 X2 = pca.transform(X)

/home/bigdata/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py in fit(self, X, y)
    305             Returns the instance itself.
    306         """
--> 307         self._fit(X)
    308         return self
    309 

/home/bigdata/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py in _fit(self, X)
    358             if max(X.shape) <= 500:
    359                 svd_solver = 'full'
--> 360             elif n_components >= 1 and n_components < .8 * min(X.shape):
    361                 svd_solver = 'randomized'
    362             # This is also the case of n_components in (0,1)

TypeError: '>=' not supported between instances of 'str' and 'int'
Versions
Linux-4.10.0-19-generic-x86_64-with-debian-stretch-sid
Python 3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:09:58)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
NumPy 1.12.1
SciPy 0.19.0
Scikit-Learn 0.18.1
Notes
With n_samples < 500 it works perfectly.
With Scikit-Learn 0.17, prior to collapsing the randomized PCA into PCA, the problem did not exist. So I blame commit 328ebfa, but have not fully tested it.
Setting svd_solver='full' explicitly is a workaround.
Expected results
Setting n_components='mle' should make the SVD solver to be 'full' automatically when it is set to 'auto' (or by default).