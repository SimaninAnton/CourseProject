Member
amueller commented on May 5, 2018 â€¢
edited
I tried running LogisticRegression(solver="saga") on adult with some pretty bad results:
(csv is here)
import pandas as pd
data = pd.read_csv("data/adult.csv", index_col=0)
income = data.income
data_features = data.drop("income", axis=1)

### one hot encode data
data_one_hot = pd.get_dummies(data_features)

### Preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_one_hot, income)

scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)

### Cross-validation with default parameters
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

scores = cross_val_score(LogisticRegression(solver='saga'), X_train, y_train, cv=5)
print(scores.mean())
# 0.7908683602959505 

scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)
print(scores.mean())
# 0.8502459719171638
There are a bunch of convergence warnings, and it's much slower than liblinear.
X_train.shape == (24420, 107)
Setting max_iter=1000 yields the same result (convergence warnings and low accuracy), only much slower, max_iter=10000 yields no convergence warning, but also low accuracy - which is maybe even worse?
Is that expected? Is the dataset too small? I thought this might become a reasonable default solver at some point, but it doesn't look like it :-/