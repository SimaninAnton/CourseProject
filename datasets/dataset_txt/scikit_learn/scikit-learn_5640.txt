Contributor
amitibo commented on Mar 12, 2011
I am using the scikits.learn on very large sparse data. I have problems when using the sparse SVM with the 'poly' kernel. I attach a simple test case based on the iris example from the scikits.learn website. In this example I use 'linear' and 'poly' kernels both using the dense and sparse implementations. As the graphs show the 'linear' kernel gives similar results (sparse vs dense) but the sparse implementation of 'poly' gives wrong results.
I am using scikits.learn version 0.7.1, and I have tested it both on window 32bit and window 64bit implementations. I am using scipy version 0.8 on the win32 platform and scipy 0.9rc3 on the win64 platform.
"""
==================================================
Plot different SVM classifiers in the iris dataset
==================================================

Comparison of different linear SVM classifiers on the iris dataset. It
will plot the decision surface for four different SVM classifiers.

"""
print __doc__

import numpy as np
import pylab as pl
from scikits.learn import svm, datasets
import scipy as sp


# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2] # we only take the first two features. We could
                     # avoid this ugly slicing by using a two-dim dataset
Xs = sp.sparse.lil_matrix( X ).tocsr()

Y = iris.target

h=.02 # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
svc     = svm.SVC(kernel='linear').fit(X, Y)
rbf_svc = svm.SVC(kernel='poly').fit(X, Y)
ssvc     = svm.sparse.SVC(kernel='linear').fit(Xs, Y)
srbf_svc = svm.sparse.SVC(kernel='poly').fit(Xs, Y)

# create a mesh to plot in
x_min, x_max = X[:,0].min()-1, X[:,0].max()+1
y_min, y_max = X[:,1].min()-1, X[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['SVC with linear kernel',
          'SVC with polynomial (degree 3) kernel',
          'Sparse SVC with linear kernel',
          'Sparse SVC with polynomial (degree 3) kernel']


pl.set_cmap(pl.cm.Paired)

for i, clf in enumerate((svc, rbf_svc, ssvc, srbf_svc)):
    # Plot the decision boundary. For that, we will asign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    pl.subplot(2, 2, i+1)

    Xp = np.c_[xx.ravel(), yy.ravel()]

    if i > 1:
        Xp = sp.sparse.lil_matrix( Xp ).tocsr()

    Z = clf.predict( Xp )

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    pl.set_cmap(pl.cm.Paired)
    pl.contourf(xx, yy, Z)
    pl.axis('tight')

    # Plot also the training points
    pl.scatter(X[:,0], X[:,1], c=Y)

    pl.title(titles[i])

pl.axis('tight')
pl.show()