bensondaled commented on Jul 29, 2016
Description
When using partial_fit to update an IncrementalPCA model, each call drastically increases memory use.
Steps/Code to Reproduce
from sklearn.decomposition import IncrementalPCA
n_features = 512*512 
samples_per_batch = 500 

ipca = IncrementalPCA(n_components=100)     

# in practice, this would be iteratively read in from files
example_batch = np.random.random([samples_per_batch, n_features]) 

ipca.partial_fit(example_batch) 
# At this call, python's memory usage increases more than 3.6GB. (ideally, it would not be copied at all. but even if it must be copied once, memory should only rise ~1.05GB)
Expected Results
This increase in memory defeats the purpose of partial_fit. Even if it is necessary to copy the newly supplied data once, memory should only increase by that amount. Instead, memory blows up with increasing chunk sizes.
Actual Results
Versions