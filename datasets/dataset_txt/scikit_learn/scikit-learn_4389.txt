Contributor
choldgraf commented on May 25, 2014
note - I opened this in joblib as well because I'm not sure which repo would be better to report this in. Since this occurs with a relatively simple use case in sklearn, I'm posting this here in case it's more appropriate. Let me know if you want me to move/take one of them down.
Recently my processes have started freezing when I run models with large-ish data, and I'm not sure why. Unfortunately, I can't follow this back to a specific change that I might have made.
Basically, this works fine:
X = randn(815000, 100)
y = randn(815000, 1)
mod = Ridge()
sc = cross_val_score(mod, X, y, n_jobs=3)
while running the following code:
X = randn(815000, 300)
y = randn(815000, 1)
mod = Ridge()
sc = cross_val_score(mod, X, y, n_jobs=3)
results in the forked processes hanging.
What happens is that first a number of processes spawn off, and they churn away at the data for a while. This is top after a few seconds of running the above code:
however, after another 10 seconds or so, these processes have finished, and another set of processes are created that hang:
You can see the processes that spawned off, and that none of them are chewing up any CPU time. It remains in this state indefinitely...
I thought this might be a problem with joblib trying to memmap things, but both matrices are well over the max_nbytes default for Parallel (at least, according to X.nbytes).
Note that these matrices, and ones larger than them, have worked totally fine in the past for fitting these kinds of models. I'm not really sure what's going on...
I'm using:
sklearn version: 0.14.1
joblib version: 0.8.0a3 (though this also breaks on 7.1)
all packages linked against MKL (though I tried it after removing MKL in anaconda, and it still hangs)
Unix machine (CentOS)