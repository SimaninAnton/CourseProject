Member
rth commented on Mar 19, 2018
In a followup of #10735, @jnothman mentioned in #10818 (comment)
In terms of generating new stop lists we should consider just making it from high frequency words in Wikipedia or something for a handful of languages. The point is to handle the case where users have small datasets where max_df are inapplicable. It would be good to try find someone who's already done this and calibrated max_df for a handful of languages, perhaps.
[..]
I think we need to provide a method/function/example to learn a stop list for a given df threshold, and for a given analyser. This could be CountVectorizer.fit_stop_words(X, min_df) which would set the instance's stop_words. This could call a function calculate_stop_words(vectorizer_or_analyzer, X, min_df) which would return a set.
Perhaps that function should allow the user to augment the learnt stop words with those from a public list ..
[..]
max_df is a valuable substitute [to stop words], but not if the training data is small, and not if we're hashing (although tf.idf works there).
(just coping this to a separate issue as its likely to require another PR and discussion in a PR does tend to get overlooked once it's merged.)
cc @shaz13 @qinhanmin2014 @kmike @vene