Contributor
c56pony commented on Apr 11, 2019
Description
I propose to be able to set initial values of the hyperparameters (alpha, lambda) in BayesianRidge().fit.
I tried to fit a sinusoidal curve with polynomials by Bayesian ridge regression, but the default initial values did not work well (left figure).
So, I corrected the fit method of the BayesianRidge class so that I could set the initial value, and the regression worked well (right figure).
Steps/Code to Reproduce
The code I wrote is Here.
#!/usr/bin/env python
# coding: utf-8

import matplotlib.pyplot as plt
import numpy as np

from math import log
from scipy import linalg
from sklearn.linear_model import BayesianRidge
from sklearn.utils import check_X_y

class BayesRidge(BayesianRidge):
    def fit(self, X, y, sample_weight=None, alpha_0=None, lambda_0=None):
        """Fit the model

        Parameters
        ----------
        X : numpy array of shape [n_samples,n_features]
            Training data
        y : numpy array of shape [n_samples]
            Target values. Will be cast to X's dtype if necessary

        sample_weight : numpy array of shape [n_samples]
            Individual weights for each sample

            .. versionadded:: 0.20
               parameter *sample_weight* support to BayesianRidge.

        Returns
        -------
        self : returns an instance of self.
        """
        X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)
        X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(
            X, y, self.fit_intercept, self.normalize, self.copy_X,
            sample_weight=sample_weight)

        if sample_weight is not None:
            # Sample weight can be implemented via a simple rescaling.
            X, y = _rescale_data(X, y, sample_weight)

        self.X_offset_ = X_offset_
        self.X_scale_ = X_scale_
        n_samples, n_features = X.shape

        # Initialization of the values of the parameters
        eps = np.finfo(np.float64).eps
        # Add `eps` in the denominator to omit division by zero if `np.var(y)`
        # is zero
        if alpha_0 is None:
            alpha_ = 1. / (np.var(y) + eps)
        else:
            alpha_ = alpha_0
        if lambda_0 is None:
            lambda_ = 1.
        else:
            lambda_ = lambda_0

        verbose = self.verbose
        lambda_1 = self.lambda_1
        lambda_2 = self.lambda_2
        alpha_1 = self.alpha_1
        alpha_2 = self.alpha_2

        self.scores_ = list()
        coef_old_ = None

        XT_y = np.dot(X.T, y)
        U, S, Vh = linalg.svd(X, full_matrices=False)
        eigen_vals_ = S ** 2

        # Convergence loop of the bayesian ridge regression
        for iter_ in range(self.n_iter):

            # Compute mu and sigma
            # sigma_ = lambda_ / alpha_ * np.eye(n_features) + np.dot(X.T, X)
            # coef_ = sigma_^-1 * XT * y
            if n_samples > n_features:
                coef_ = np.dot(Vh.T,
                               Vh / (eigen_vals_ +
                                     lambda_ / alpha_)[:, np.newaxis])
                coef_ = np.dot(coef_, XT_y)
                if self.compute_score:
                    logdet_sigma_ = - np.sum(
                        np.log(lambda_ + alpha_ * eigen_vals_))
            else:
                coef_ = np.dot(X.T, np.dot(
                    U / (eigen_vals_ + lambda_ / alpha_)[None, :], U.T))
                coef_ = np.dot(coef_, y)
                if self.compute_score:
                    logdet_sigma_ = np.full(n_features, lambda_,
                                            dtype=np.array(lambda_).dtype)
                    logdet_sigma_[:n_samples] += alpha_ * eigen_vals_
                    logdet_sigma_ = - np.sum(np.log(logdet_sigma_))

            # Preserve the alpha and lambda values that were used to
            # calculate the final coefficients
            self.alpha_ = alpha_
            self.lambda_ = lambda_

            # Update alpha and lambda
            rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)
            gamma_ = (np.sum((alpha_ * eigen_vals_) /
                      (lambda_ + alpha_ * eigen_vals_)))
            lambda_ = ((gamma_ + 2 * lambda_1) /
                       (np.sum(coef_ ** 2) + 2 * lambda_2))
            alpha_ = ((n_samples - gamma_ + 2 * alpha_1) /
                      (rmse_ + 2 * alpha_2))

            # Compute the objective function
            if self.compute_score:
                s = lambda_1 * log(lambda_) - lambda_2 * lambda_
                s += alpha_1 * log(alpha_) - alpha_2 * alpha_
                s += 0.5 * (n_features * log(lambda_) +
                            n_samples * log(alpha_) -
                            alpha_ * rmse_ -
                            (lambda_ * np.sum(coef_ ** 2)) -
                            logdet_sigma_ -
                            n_samples * log(2 * np.pi))
                self.scores_.append(s)

            # Check for convergence
            if iter_ != 0 and np.sum(np.abs(coef_old_ - coef_)) < self.tol:
                if verbose:
                    print("Convergence after ", str(iter_), " iterations")
                break
            coef_old_ = np.copy(coef_)

        self.coef_ = coef_
        sigma_ = np.dot(Vh.T,
                        Vh / (eigen_vals_ + lambda_ / alpha_)[:, np.newaxis])
        self.sigma_ = (1. / alpha_) * sigma_

        self._set_intercept(X_offset_, y_offset_, X_scale_)
        return self

def main():
    def func(x):
        return np.sin(2*np.pi*x)
    size = 25
    np.random.seed(1234)
    xtrain = np.random.uniform(0.,1.,size)
    ytrain = func(xtrain)+np.random.normal(scale=0.1,size=size)
    xtest = np.linspace(0.,1.,100)

    nOrder = 3
    Xtrain = np.vander(xtrain,nOrder+1,increasing=True)
    Xtest = np.vander(xtest,nOrder+1,increasing=True)

    fig,ax = plt.subplots(1,2,figsize=(8,4))
    regs = [BayesianRidge(tol=1e-6,fit_intercept=False),
            BayesRidge(tol=1e-6,fit_intercept=False)]
    init = (1.,1.e-3)

    for i,reg in enumerate(regs):
        if i == 0:
            reg.fit(Xtrain,ytrain)
        elif i == 1:
            reg.fit(Xtrain,ytrain,alpha_0=init[0],lambda_0=init[1])

        ymean,ystd = reg.predict(Xtest,return_std=True)
        print(reg.alpha_,reg.lambda_)

        ax[i].scatter(xtrain,ytrain, s=50, alpha=0.5, label="observation")
        ax[i].plot(xtest,func(xtest), color="blue", label="sin(2$πx$)")
        ax[i].plot(xtest,ymean, color="red", label="predict_mean")
        ax[i].fill_between(xtest,ymean-ystd,ymean+ystd, color="pink", alpha=0.5, label="predict_std")
        ax[i].legend()
        if i == 0:
            ax[i].set_title("BayesianRidge")
        elif i == 1:
            ax[i].set_title("$α₀={} ,λ₀=${}".format(init[0],init[1]))

    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    main()
Expected Results
The right figure
Actual Results
The left figure
Versions
System:
python: 3.6.6 (default, Mar 8 2019, 18:24:30) [GCC 7.3.0]
executable: /home/*****/.pyenv/versions/3.6.6/bin/python
machine: Linux-4.15.0-47-generic-x86_64-with-debian-buster-sid
BLAS:
macros: NO_ATLAS_INFO=1, HAVE_CBLAS=None
lib_dirs: /usr/lib/x86_64-linux-gnu
cblas_libs: cblas
Python deps:
pip: 19.0.3
setuptools: 41.0.0
sklearn: 0.20.3
numpy: 1.16.2
scipy: 1.2.1
Cython: 0.29.6
pandas: 0.24.2