ajing commented on Aug 26, 2015
I have a dataset with 1M data points and I defined the distance function like the following.
def GetDistance(fplist, x, y):
    fp1 = fplist[int(x[0])][1]
    fp2 = fplist[int(y[0])][1]
    return 1 - getSimilarity(fp1, fp2)

def GetSmallSetAfterClustering(fp_list):
    dbs = DBSCAN(eps = 0.8, min_samples = 2, metric = partial(GetDistance, fp_list))
    dbs.fit(arange(len(fp_list)).reshape(-1, 1))
However, it already runs for more than 1 day. Should I change to a more powerful machine with more cores or use Mini Batch K-Means in this situation? Could I define the distance function in Batch K-Means also?
..OK, K-Means won't support customized distance function. I guess DBScan is the best bet here.
How to use multithread version of DBSCAN? n_jobs?