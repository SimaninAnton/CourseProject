Contributor
david-cortes commented on Aug 8, 2019 â€¢
edited
In sklearn.linear_model.logistic, function _multinomial_grad_hess returns (along with the gradient) a function hessp which gives the product of the Hessian times a vector, which is a parameter for hessp.
The outputs however always seem to be the same vector that was input, multiplied by the regularization parameter. Example:
import numpy as np
from sklearn.linear_model.logistic import _multinomial_grad_hess

n = 1000
m = 20
nc = 2
np.random.seed(1)

grad, hessp = _multinomial_grad_hess(np.random.normal(size = nc * m + nc),
                                     np.random.normal(size = (n, m)),
                                     np.random.randint(2, size = nc * n).reshape((n, nc)),
                                     1.0, np.ones(n))
hessp(np.ones(nc * m + nc))
I believe the calculation for r_yhat in the hessp definition might have something wrong as they always result in very small numbers.
I haven't looked deep into it, but I believe the line
r_yhat += (-p * r_yhat).sum(axis=1)[:, np.newaxis]
should be
r_yhat += ((1-p) * r_yhat).sum(axis=1)[:, np.newaxis]
For comparison purposes, the non-multinomial function _logistic_grad_hess would return very different numbers - e.g.
import numpy as np
from sklearn.linear_model.logistic import _logistic_grad_hess

n = 1000
m = 20
nc = 1
np.random.seed(1)

grad, hessp = _logistic_grad_hess(np.random.normal(size = nc * m + nc),
                                     np.random.normal(size = (n, m)),
                                     np.random.randint(2, size = nc * n).reshape(-1),
                                     1.0, np.ones(n))
hessp(np.ones(nc * m + nc))