williamkrivan commented on Sep 26, 2015
I am puzzled by the dependence of GaussianNB results on the overall magnitude of features values. I took a quick look at naive_bayes.py; it seems that log probabilities are utilized as they ought to be, and I didn't notice anything that struck my eye as a potential cause for numerical error, so I thought I post here before digging into the bowels of the naive_bayes.py source.
In my real-world problem, I am using NBC for multiclass-classification of time series with much larger data sets and first noticed the discrepancy there; but, here are two simplified examples to demonstrate what I think is an issue:
I am using scikit-learn 0.16.1, numpy 1.9.0, scipy 0.14.0 on openSUSE 13.2, built with pip and with python3.
In the first example, I generated the data using make_moons, and then performed several runs with the features scaled by 10^sclfac where sclfac = -7, -6, ... 7.
When I compare the results for two-class classification obtained with GaussianNB and other NBCs, such as the one from the GNU Octave NaN package, they are in perfect agreement as long as the magnitude of the features is larger than ~1E-4. For smaller features, the GaussianNB results become wacky to meaningless. [Unlike GaussianNB, LDA from scikit-learn is (as expected) insensitive to the downscaling of the features, and so are other NBC implementations I tried besides the one from the GNU Octave NaN package.]
This pair of figures show the results for predictions at the "breaking point" around 1E-4; for larger features, there is perfect agreement:
The next example shows results produced with the floatmoon.py script that you find below. The LDA and GaussianNB test results are identical for sclfac >~ -4.
> python3 floatmoon.py
LDA
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 7
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 6
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 5
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 4
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 3
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 2
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 1
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 0
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -1
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -2
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -3
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -4
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -5
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -6
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -7
GaussianNB
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 7
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 6
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 5
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 4
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 3
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 2
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 1
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= 0
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -1
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -2
x= [1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -3
x= [1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1] sclfac= -4
x= [1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1] sclfac= -5
x= [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] sclfac= -6
x= [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] sclfac= -7
This agreement of LDA and GaussianNB is generally not expected, but turns out to occur for most runs for the particular data set used here. (Change moons to circles, e.g, and chances are the results will be different.) Furthermore, the behavior - agreement above 1E-4 and disagreement below - occurs consistently when the script below is run repeatedly and different data are generated for each run.
The crucial point is that (i) LDA is scale-insensitive (as expected), (ii) unlike other NBCs, GaussianNB unexpectedly produces inconsistent results for features smaller than ~ 1E-4.
I may be missing something, or there may be an issue with the implementation of GaussianNB. Either way, I look forward to your responses.
#exec(open("floatmoon.py").read())

import numpy
from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.cross_validation import train_test_split


# Moons
from sklearn.datasets import make_moons
x,y=make_moons(noise=0.3, random_state=0)

# Circles
#from sklearn.datasets import make_circles
#x,y=make_circles(noise=0.3, random_state=0)

Xtrain, Xtest, Ytrain, Ytest = train_test_split(x, y, test_size=.3)

print("LDA")
clf=LDA()
sclfac=7
while sclfac>-8:
    Xtrain2= pow(10, sclfac)*Xtrain
    Xtest2= pow(10, sclfac)*Xtest
    clf.fit(Xtrain2, Ytrain)
    x=clf.predict(Xtest2)
    print("x=",x,"sclfac=",sclfac)
    sclfac -=1

print("GaussianNB")
clf=GaussianNB()
sclfac=7
while sclfac>-8:
    Xtrain2= pow(10, sclfac)*Xtrain
    Xtest2= pow(10, sclfac)*Xtest
    clf.fit(Xtrain2, Ytrain)
    x=clf.predict(Xtest2)
    print("x=",x,"sclfac=",sclfac)
    sclfac -=1