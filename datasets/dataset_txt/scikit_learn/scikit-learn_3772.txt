mverleg commented on Jun 15, 2015
I like the random search functionality. But I think it could be made more flexible.
The main problem I'm trying to solve is that I waste a lot of time waiting for useless results, for example because a rare bug crashes it near the end or most calculations diverged or something, and I would like more immediate feedback and control (the whole calculation can take hours/days).
I think a lot of flexibility could be added with a couple of callbacks:
Make a callback on error. The error_score parameters is nice, but now either a rare bug crashes the whole optimization or gets ignored, in which case restarting with 'raise' and hoping for the bug to reappear seems the best option to solve it (and that's really slow). A callback could be used to write a log file or determine if the exception is severe enough to terminate.
Make a callback when a model finishes fitting. I would really like to be able to execute some code for each model that has access to the input parameters, learned parameters and the score in one place. E.g. to conditionally save the learned model if the score is good, or to write all the scores to a file so as to have more output, and have output even if optimization doesn't complete (or hasn't yet).
Perhaps the least useful, there might be a callback when fitting is about to start. This would be similar to using the model's init, but that might not be desirable if it's external code. Possible uses include terminating quickly for certain combinations of parameters, or loading a model from cache instead of it's already been fit before.
If there are already ways to do all of this, then my apologies for taking your time. Possibly in that case it could be better documented?
(These would probably be beneficial for GridSearchCV as well. Possibly to a lesser extent since an incomplete grid search might not be useful).