Contributor
mfeurer commented on Oct 26, 2017
Description
When using the warm_start argument to the SGDClassifier together with a for-loop for increasing the max_iter argument, this results in a different score than when simply setting the max_iter to a high value and not using warm_start.
Steps/Code to Reproduce
import numpy as np
import sklearn.datasets
import sklearn.linear_model
import sklearn.model_selection


X, y = sklearn.datasets.load_digits(return_X_y=True)

for seed in range(100):
    print('.', end='', flush=True)
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
        X, y, random_state=seed,
    )
    classifier = sklearn.linear_model.SGDClassifier(
        warm_start=True, max_iter=1, random_state=1,
    )
    classifier.fit(X_train, y_train)
    for i in range(99):
        classifier.max_iter += 1
        classifier.fit(X_train, y_train)
    score1 = classifier.score(X_test, y_test)

    classifier = sklearn.linear_model.SGDClassifier(
        max_iter=100, random_state=1,
    )
    classifier.fit(X_train, y_train)
    score2 = classifier.score(X_test, y_test)
    if score1 != score2:
        print("\n", seed, score1, score2)
Expected Results
Only a few dots indicating that the code snippet is running.
Actual Results
Several seeds for which score1 and score2 differ.
Versions
Linux-4.10.0-37-generic-x86_64-with-debian-stretch-sid
Python 3.5.4 |Anaconda, Inc.| (default, Oct 13 2017, 11:22:58)
[GCC 7.2.0]
NumPy 1.13.3
SciPy 0.19.1
Scikit-Learn 0.20.dev0
I originally detected the issue under 0.19.1. This is a follow up to #10000.