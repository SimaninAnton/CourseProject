Contributor
dougalsutherland commented on Sep 11, 2012
When naive_bayes.BaseDiscreteNB.fit fits the class prior with
y_freq = Y.sum(axis=0)
self.class_log_prior_ = np.log(y_freq) - np.log(y_freq.sum())
if any of the classes never actually appear in the data, an element of y_freq will be zero, and this code will give a RuntimeWarning: divide by zero encountered in log and proceed merrily on its way with a -inf element in class_log_prior_.
Ideally, either there would be a real exception thrown here instead of an easy-to-miss warning, or this estimate would be smoothed so we always get a finite prior. Using Lidstone smoothing with the same self.alpha used to get feature_log_prob_, for example, would be trivial.
For example:
>>> clf = MultinomialNB(alpha=1, fit_prior=True)
>>> clf.fit([(0,) for i in range(3)], np.zeros(3))
MultinomialNB(alpha=1, fit_prior=True)
>>> clf.class_log_prior_
array([  0., -inf])