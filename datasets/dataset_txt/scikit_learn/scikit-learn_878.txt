homarb commented on Dec 6, 2018 â€¢
edited
Description
Error is returned when a list of dictionaries is passed to RandomizedSearchCV:
AttributeError: 'list' object has no attribute 'values'
Possibly related to Setting search parameters on estimators #5082
Steps/Code to Reproduce
from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

import scipy.stats as ss

rand_st = 0 # Random state

# Load data
X, y = load_digits(10, True)
# split data into train set and test set
X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=0.75, random_state=rand_st)

kfold = KFold(n_splits=5, random_state=rand_st)

pipe = Pipeline([("preprocessing", StandardScaler()), ("classifier", SVC())])

param_dist_p = [
    {
        "classifier": [SVC(random_state=rand_st)],
        "preprocessing": [StandardScaler(), None],
        "classifier__kernel": ["rbf"],
        "classifier__gamma": ss.uniform(1e-6, 1e6),
        "classifier__C": ss.uniform(1e-6, 1e6),
    },
    {
        "classifier": [SVC(random_state=rand_st)],
        "preprocessing": [StandardScaler(), None],
        "classifier__kernel": ["linear"],
        "classifier__C": ss.uniform(1e-6, 1e6),
    },
    {
        "classifier": [RandomForestClassifier(random_state=rand_st)],
        "preprocessing": [StandardScaler(), None],
        "classifier__max_depth": [3, 5, 7, None],
        "classifier__max_features": ss.randint(1, 20),
        "classifier__min_samples_split": ss.randint(2, 20),
    },
]

# run randomized search
rs_p = RandomizedSearchCV(
    pipe,
    param_distributions=param_dist_p,
    n_iter=50, #[35, 15, 25],
    cv=kfold,
    n_jobs=5,
)
rs_p.fit(X_train, y_train)

print("Val. score: %s" % rs_p.best_score_)
print("Test score: %s" % rs_p.score(X_test, y_test))
print("Best params:\n{}\n".format(rs_p.best_params_))
Proposed solution
Slightly modifiy the class ParameterSampler in /model_selection/_search.py as follows:
class ParameterSampler(object):
    """Generator on parameters sampled from given distributions.

    Non-deterministic iterable over random candidate combinations for hyper-
    parameter search. If all parameters are presented as a list,
    sampling without replacement is performed. If at least one parameter
    is given as a distribution, sampling with replacement is used.
    It is highly recommended to use continuous distributions for continuous
    parameters.

    Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not
    accept a custom RNG instance and always use the singleton RNG from
    ``numpy.random``. Hence setting ``random_state`` will not guarantee a
    deterministic iteration whenever ``scipy.stats`` distributions are used to
    define the parameter search space. Deterministic behavior is however
    guaranteed from SciPy 0.16 onwards.

    Read more in the :ref:`User Guide <search>`.

    Parameters
    ----------
    param_distributions : dict or sequence of such.
        Dictionary where the keys are parameters and values
        are distributions from which a parameter is to be sampled.
        Distributions either have to provide a ``rvs`` function
        to sample from them, or can be given as a list of values,
        where a uniform distribution is assumed.
        

    n_iter : integer or sequence of such
        Number of parameter settings that are produced.
        Or sequence of integers, one per dict of param_distributions

    random_state : int, RandomState instance or None, optional (default=None)
        Pseudo random number generator state used for random uniform sampling
        from lists of possible values instead of scipy.stats distributions.
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    Returns
    -------
    params : dict of string to any
        **Yields** dictionaries mapping each estimator parameter to
        as sampled value.

    Examples
    --------
    >>> from sklearn.model_selection import ParameterSampler
    >>> from scipy.stats.distributions import expon
    >>> import numpy as np
    >>> np.random.seed(0)
    >>> param_grid = {'a':[1, 2], 'b': expon()}
    >>> param_list = list(ParameterSampler(param_grid, n_iter=4))
    >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())
    ...                 for d in param_list]
    >>> rounded_list == [{'b': 0.89856, 'a': 1},
    ...                  {'b': 0.923223, 'a': 1},
    ...                  {'b': 1.878964, 'a': 2},
    ...                  {'b': 1.038159, 'a': 2}]
    True
    """
    def __init__(self, param_distributions, n_iter, random_state=None):
        # self.param_distributions = param_distributions
        self.n_iter = n_iter
        self.random_state = random_state

        if isinstance(param_distributions, Mapping):
            # wrap dictionary in a singleton list to support either dict
            # or list of dicts
            param_distributions = [param_distributions]

        # check if all entries are dictionaries
        for dist in param_distributions:
            if not isinstance(dist, dict):
                raise TypeError('Parameter distributions is not a '
                                'dict ({!r})'.format(dist))

        self.param_distributions = param_distributions

    def __iter__(self):
        param_distributions = self.param_distributions
        n_dists = len(self.param_distributions)

        # n_iter can be a sequence containing the number of iters 
        # settings that are sampled per par_dist.
        # If the sequence or int has less elements than number of par_dist,
        # the total sum is divided by the number of par_dists.
        if isinstance(self.n_iter, Iterable):
            n_iter = self.n_iter
            if len(self.n_iter) != n_dists:
                n_iter = [sum(n_iter)//n_dists] * n_dists
        else:
            n_iter = [self.n_iter//n_dists] * n_dists

        # If a list of dicts is passed evaluate each dict of par_dist
        for ni, dist in enumerate(param_distributions):
            # check if all distributions are given as lists
            # in this case we want to sample without replacement
            all_lists = np.all([not hasattr(v, "rvs")
                                for v in dist.values()])
            rnd = check_random_state(self.random_state)

            if all_lists:
                # look up sampled parameter settings in parameter grid
                param_grid = ParameterGrid(dist)
                grid_size = len(param_grid)
                
                if grid_size < n_iter[ni]:
                    warnings.warn(
                        'The total space of parameters %d is smaller '
                        'than n_iter=%d. Running %d iterations. For exhaustive '
                        'searches, use GridSearchCV.'
                        % (grid_size, n_iter[ni], grid_size), UserWarning)
                    n_iter[ni] = grid_size
                for i in sample_without_replacement(grid_size, n_iter[ni],
                                                    random_state=rnd):
                    yield param_grid[i]

            else:
                # Always sort the keys of a dictionary, for reproducibility
                items = sorted(dist.items())
                for _ in six.moves.range(n_iter[ni]):
                    params = dict()
                    for k, v in items:
                        if hasattr(v, "rvs"):
                            if sp_version < (0, 16):
                                params[k] = v.rvs()
                            else:
                                params[k] = v.rvs(random_state=rnd)
                        else:
                            params[k] = v[rnd.randint(len(v))]
                    yield params


    def __len__(self):
        """Number of points that will be sampled."""
        return sum(self.n_iter)
Note that a sequence of ints can also be passed to n_iter
Expected Results
Val. score: 0.9799554565701559
Test score: 0.9711111111111111
Best params:
{'classifier': SVC(C=595978.8546149381, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
  kernel='linear', max_iter=-1, probability=False, random_state=0,
  shrinking=True, tol=0.001, verbose=False), 'classifier__C': 595978.8546149381, 'classifier__kernel': 'linear', 'preprocessing': None}
ðŸ‘ 3