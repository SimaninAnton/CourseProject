denson commented on Apr 14, 2017 â€¢
edited by TomDLT
Description
I am running a GradientBoostingClassifier from Sklearn and I am getting some strange output from the verbose output. I am taking random 10% samples from my entire dataset and most seem to be fine but sometimes I get strange output and poor results.
Steps/Code to Reproduce
model = 'GBC'
start_time = time.time()
if model == 'ET':
    ic = ExtraTreesClassifier(n_estimators=forest_size, 
            criterion=criterion, n_jobs = 10,verbose = 0,max_features=max_features)
elif model == 'RDF':
    ic = RandomForestClassifier(n_estimators=forest_size, 
            criterion=criterion, n_jobs = 10,verbose = 0,max_features=max_features)     
elif model == 'SGD':
    ic = SGDClassifier(shuffle=True,loss='log', verbose = 4, n_iter = 10000)

elif model == 'GBC':
    # learning_rates = [1.0, 0.1, 0.01]
    
    roll = np.random.randint(len(learning_rates),size = 1)[0]
    learning_rate = learning_rates[roll]
    this_n_boosts = n_boosts[roll]

    # max_depths = [3,4,5]
    
    roll = np.random.randint(len(max_depths),size = 1)[0]
    this_max_depth = max_depths[roll]

    
    roll = np.random.randint(len(max_features_list),size = 1)[0]
    max_features = max_features_list[roll]      

     
        
    ic = GradientBoostingClassifier(n_estimators=this_n_boosts, 
                        learning_rate=learning_rate, 
                        max_depth = this_max_depth,
                        max_features = max_features,
                        verbose = 1)
    print ic
    
    gbc_params = ic.get_params(deep=True)
    
    # Make a dataframe of the parameters


    if sample_paste_id == 0:
        
    
        keys = gbc_params.keys()
        values = gbc_params.values()
        
        header = ('values bite %i' % sample_paste_id)
        params_df = pd.DataFrame(data=values,index=keys,columns = [header])
        
    else:
        header = ('values bite %i' % sample_paste_id)
        values = gbc_params.values()
        params_df[header] = values
        
    params_output_path = windowed_bites_summary_path + 'bite_model_parameters.csv'
        
    params_df.to_csv(params_output_path,
                     index_label = 'parameter')
    
        
    
#fit and transform the training data
ic = ic.fit(X_train, y_train)

print('predicting')

y_pred = ic.predict(X_test)

probas = ic.predict_proba(X_test)

this_file_MCC = matthews_corrcoef(y_test,y_pred)
print('this_file_MCC = %.4f' % this_file_MCC)
Expected Results
This is a run that looks fine:
Training the classifier
n features = 168
GradientBoostingClassifier(criterion='friedman_mse', init=None,
learning_rate=0.01, loss='deviance', max_depth=4,
max_features=None, max_leaf_nodes=None,
min_impurity_split=1e-07, min_samples_leaf=1,
min_samples_split=2, min_weight_fraction_leaf=0.0,
n_estimators=2000, presort='auto', random_state=None,
subsample=1.0, verbose=1, warm_start=False)
Iter Train Loss Remaining Time
1 0.6427 40.74m
2 0.6373 40.51m
3 0.6322 40.34m
4 0.6275 40.33m
5 0.6230 40.31m
6 0.6187 40.18m
7 0.6146 40.34m
8 0.6108 40.42m
9 0.6071 40.43m
10 0.6035 40.28m
20 0.5743 40.12m
30 0.5531 39.74m
40 0.5367 39.49m
50 0.5237 39.13m
60 0.5130 38.78m
70 0.5041 38.47m
80 0.4963 38.34m
90 0.4898 38.22m
100 0.4839 38.14m
200 0.4510 37.07m
300 0.4357 35.49m
400 0.4270 33.87m
500 0.4212 31.77m
600 0.4158 29.82m
700 0.4108 27.74m
800 0.4065 25.69m
900 0.4025 23.55m
1000 0.3987 21.39m
2000 0.3697 0.00s
predicting
this_file_MCC = 0.5777
Actual Results
Training the classifier
n features = 168
GradientBoostingClassifier(criterion='friedman_mse', init=None,
learning_rate=1.0, loss='deviance', max_depth=5,
max_features='sqrt', max_leaf_nodes=None,
min_impurity_split=1e-07, min_samples_leaf=1,
min_samples_split=2, min_weight_fraction_leaf=0.0,
n_estimators=500, presort='auto', random_state=None,
subsample=1.0, verbose=1, warm_start=False)
Iter Train Loss Remaining Time
1 0.5542 1.07m
2 0.5299 1.18m
3 0.5016 1.14m
4 0.4934 1.16m
5 0.4864 1.19m
6 0.4756 1.21m
7 0.4699 1.24m
8 0.4656 1.26m
9 0.4619 1.24m
10 0.4572 1.26m
20 0.4244 1.27m
30 0.4063 1.24m
40 0.3856 1.20m
50 0.3711 1.18m
60 0.3578 1.13m
70 0.3407 1.10m
80 0.3264 1.09m
90 0.3155 1.06m
100 0.3436 1.04m
200 0.3516 46.55s
300 1605.5140 29.64s
400 52215150662014.0469 13.70s
500 585408988869401440279216573629431147797247696359586211550088082222979417986203510562624281874357206861232303015821113689812886779519405981626661580487933040706291550387961400555272759265345847455837036753780625546140668331728366820653710052494883825953955918423887242778169872049367771382892462080.0000 0.00s
predicting
this_file_MCC = 0.0398
ensemble MCC = 0.5117
Versions
Python 2.7.13 |Anaconda custom (x86_64)| (default, Dec 20 2016, 23:05:08)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
import platform; print(platform.platform())
import sys; print("Python", sys.version)
Darwin-14.5.0-x86_64-i386-64bit
('Python', '2.7.13 |Anaconda custom (x86_64)| (default, Dec 20 2016, 23:05:08) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]')
import numpy; print("NumPy", numpy.version)
('NumPy', '1.11.3')
import scipy; print("SciPy", scipy.version)
import sklearn; print("Scikit-Learn", sklearn.version)
('SciPy', '0.19.0')
('Scikit-Learn', '0.18.1')