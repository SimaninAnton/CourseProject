Contributor
f0k commented on Mar 3, 2011
The k-means center initialization in https://github.com/scikit-learn/scikit-learn/blob/master/scikits/learn/cluster/k_means_.py, function k_init(), is not k-means++. The paper authors' implementation (http://www.stanford.edu/~darthur/kMeansppTest.zip, Utils.cpp, chooseSmartCenters()) does the following:
One center is chosen randomly.
Now repeat numCenters-1 times:
Repeat numLocalTries times:
Add a point x with probability proportional to the distance squared from x to the closest existing center
Add the point chosen above that results in the smallest potential.
scikit-learn's implementation (taken from pybrain, which in turn took it from Yong Sun's blog at http://blogs.sun.com/yongsun/entry/k_means_and_k_means) does this instead:
One center is chosen randomly.
Now repeat numCenters-1 times:
Repeat numSamples times:
Add a point x that has not been tried yet
Add the point chosen above that results in the smallest potential.
The authors' implementation samples numLocalTries points with D^2 weighting and chooses the best among those (repeated for each of the k-1 centers to find). For all the results in their paper, the authors used numLocalTries==1. (Only in their "Conclusion and future work" section they state that "experiments showed that k-means++ generally performed better if it selected several new centers during each iteration, and then greedily chose the one that decreased \phi as much as possible", and in their code you can see they tried numLocalTries==2+log(k).)
scikit-learn completely omits the sampling step (authors' Utils.cpp, lines 299-305) and instead greedily chooses the center that minimizes the potential. While this is not necessarily bad, it is not k-means++.
The easy way to fix this would be changing the documentation to not refer to k-means++ any longer (and find out how this greedy scheme is called in the literature; I assume somebody described it already), the better way would be fixing the implementation. I will do the latter (unless I decide I don't need it) and post back here; until then just take this as a warning.