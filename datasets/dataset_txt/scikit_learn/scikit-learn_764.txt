hsafer commented on Jan 30, 2019
Description
In the documentation of precision_recall_fscore_support, the explanation of the micro value of average is misleading.
The documentation says "Calculate metrics globally by counting the total true positives, false negatives and false positives." Since these three numbers are only really meaningful in binary classification, the sentence implies that true negatives are not used to calculate the micro values. This is incorrect, however; the micro values are accuracy over all classes. For binary classification, the true negatives are used.
The source of the problem is, I think, that the code uses "tp" to represent all correct classifications. For binary classification, it counts both true positives and true negatives. This naming choice (a poor one, in my opinion) morphed into the misleading documentation. I suggest that the documentation say either "Calculate metrics globally by counting the total correct and incorrect predictions." or more simply "Calculate overall prediction accuracy."
Versions
0.20.2