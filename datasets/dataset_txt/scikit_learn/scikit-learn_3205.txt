Member
amueller commented on Feb 8, 2016
I often get "why are categorical variables so hard in scikit-learn" questions. Below is a recent one and my answer. Probably not the best formulation, but maybe a starting point. I think we should add a version of this to the FAQ. Possibly with a note about missing values, as it goes into a similar can of worms.
As you know, both SAS and R handle categorical variables well. SAS has the CLASS statement and R has the Factor option.
Are there any plans to add that feature into scikit learn?
This is not only a scikit-learn problem, but more of a python data science ecosystem problem. scikit-learn builds on numpy arrays. numpy arrays have homogeneous types, so doesn't
support columns of different types. It also doesn't really have a categorical dtype, only ints. It also doesn't handle missing values (a very related problem).
The standard library to handle categorical variables at the moment is pandas, which has its whole own type system and is not great with numpy compatibility.
pandas also doesn't have a C-level API, which means that it is not possible for scikit-learn to access the categorical variables stored in pandas dataframes in a manner that is fast enough.
Also, scikit-learn doesn't really want to commit to using dataframes as their main data structure, as numpy arrays are much more efficient and convenient for most algorithms.
The current "solution" is to put all the feature encoding in a preprocessing step. Unfortunately, currently that makes it hard to track the origin of features.
I think a forthcoming solution might be to propagate information about feature / column names in pipelines.
Another possible solution would be to establish labeled columns in numpy arrays, something that might actually be added to numpy.