valignatev commented on Oct 28, 2016
Description
RandomForestClassifier.fit uses different amount of RAM on different machines with absolutely same data sets.
Steps/Code to Reproduce
from sklearn.ensemble import RandomForestClassifier

y_train = data_train['train_column']
x_train = data_train.drop('train_column', axis=1)

# Difference in memory consuming starts here
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf = clf.fit(x_train, y_train)
preds = clf.predict(data_test)
Actual difference happens inside fit method in this strings:
# Parallel loop: we use the threading backend as the Cython code
# for fitting the trees is internally releasing the Python GIL
# making threading always more efficient than multiprocessing in
# that case.
trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
                 backend="threading")(
    delayed(_parallel_build_trees)(
        t, self, X, y, sample_weight, i, len(trees),
        verbose=self.verbose, class_weight=self.class_weight)
    for i, t in enumerate(trees))
Expected Results
On my local machine operation consumes only 2.5GB of RAM
Actual Results
On my server operation comsumes more than 7GB (After that I usually see MemoryError or Killed message)
Versions
Configuration of my local machine:
Darwin-16.1.0-x86_64-i386-64bit
Python 3.5.2 (default, Oct 11 2016, 05:05:28)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)]
NumPy 1.11.2
SciPy 0.18.1
Scikit-Learn 0.18
Of my server:
Linux-3.13.0-57-generic-x86_64-with-Ubuntu-16.04-xenial
Python 3.5.1 (default, Dec 18 2015, 00:00:00)
[GCC 4.8.4]
NumPy 1.11.2
SciPy 0.18.1
Scikit-Learn 0.18
P.S. I also asked question on SO: http://stackoverflow.com/q/40293169/3606603