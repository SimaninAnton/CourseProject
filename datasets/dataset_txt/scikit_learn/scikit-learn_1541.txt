Contributor
f0k commented on Feb 23, 2018 â€¢
edited
By pure luck, I found that the scores I computed with my version of sklearn.metrics.average_precision_score() were about 5 percent points lower than the scores computed with the latest version. That's problematic since I was comparing my scores with a remote collaborator to figure out who's models are better. I would have expected this metric to be stable.
By bisection, I found that this difference was introduced in 494a240. I think this strongly deserves a :versionchanged note in http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html, I would have noticed it there. I'm fine with sending a PR, but wanted to check back if you agree on that.
(Pinging @GaelVaroquaux, who committed this change.)