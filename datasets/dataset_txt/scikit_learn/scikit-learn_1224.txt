KOLANICH commented on Jul 18, 2018 ‚Ä¢
edited
The interface to built-in datasets is terrible.
1
There is no registry of all datasets available.
I have to use the code like this one:
import lazy_object_proxy
import sklearn.datasets

import re
dsLoadFuncNameRx=re.compile("^(load|fetch)_(.+)")
datasets={}
for fName in dir(sklearn.datasets):
    m=dsLoadFuncNameRx.match(fName)
    if m:
        datasets[m.group(2)]=lazy_object_proxy.Proxy(getattr(sklearn.datasets, fName))
datasets["boston"]
2 There is no name for target feature available
3 there is no machine-readable description for columns available
4 there is no data about whether the file is to be downloaded from the net and about the size of the data available
5 there is no hints about datasets usage and best CV accuracies on this ds available.
6 foreign packages cannot plug their datasets here
So I propose
1 store all the needed metadata in machine-readable form. For columns store column class, one of "categorical", "numerical", "binary" and "stop". "Stop" means that the column is not suitable for machine-learning immediately.
For datasets store name of target column, original crossvalidation accuracy, best known crossvalidation accuracy (not less than 10 folds), count of folds in CV, purpose of dataset like classification, regression, clustering, etc, whether the dataset is to be downloaded from the internet, download size, ondisk size.
2 use setuptools entry points mechanism to allow external packages put their datasets into the registry.
üëç 3