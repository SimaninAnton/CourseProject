annapaux commented on Sep 22, 2019
I find it confusing that the inputs for the roc_auc_score metric are called 'y_true' and 'y_score'. 'y_true' for the true label and 'y_pred' for the prediction would be more intuitive and more consistent with other metrics (e.g. accuracy score)
Current definition:
sklearn.metrics.roc_auc_score(y_true, y_score, average=’macro’, sample_weight=None, max_fpr=None)
y_score : array, shape = [n_samples] or [n_samples, n_classes]
Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers). For binary y_true, y_score is supposed to be the score of the class with greater label.
Source:
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html