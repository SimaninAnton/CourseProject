NagabhushanS commented on Dec 15, 2017 â€¢
edited
I noticed that k-fold cross-validation with lightgbm models takes clearly very different times to train each cv model. Following is an example of the output of cross_val_score:
[CV] ................................................................
[CV] ........................ , score=0.933448908246891, total= 5.9s
[Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 5.9s remaining: 0.0s
[CV] ................................................................
[CV] ....................... , score=0.9328614820333544, total= 3.6min
[Parallel(n_jobs=1)]: Done 2 out of 2 | elapsed: 3.7min remaining: 0.0s
[CV] ................................................................
[CV] ....................... , score=0.9330334116568285, total= 8.3s
[Parallel(n_jobs=1)]: Done 3 out of 3 | elapsed: 3.9min remaining: 0.0s
[CV] ................................................................
[CV] ....................... , score=0.9335195930940612, total= 3.3min
[Parallel(n_jobs=1)]: Done 4 out of 4 | elapsed: 7.2min remaining: 0.0s
[CV] ................................................................
[CV] ....................... , score=0.9324020345296941, total= 6.0s
[Parallel(n_jobs=1)]: Done 5 out of 5 | elapsed: 7.3min remaining: 0.0s
[Parallel(n_jobs=1)]: Done 5 out of 5 | elapsed: 7.3min finished
Just notice the difference between the training + predicting times for each of the folds. It is considerable difference. This difference is not justifiable since the n_estimators is same for all folds. The same happens when I use GridSearchCV or RandomizedSearchCV. What could be the reason for this?