one-two-one-two commented on Sep 10, 2019
Description
Running OneHotEncoder in loop of millions/day. Process Memory slowly builds up. ~40k / 1000 calls.
Steps/Code to Reproduce
Example: For input data, see a.txt file below
import os, psutil
import pandas as pd
import numpy as np
import sklearn.preprocessing as prep

def encoder(df):
    m = df.values.reshape(-1, 1)
    enc = prep.OneHotEncoder(sparse=True)
    enc.fit(m) # leaky
    design_matrix = enc.transform(m).toarray() # leaky

df = pd.read_csv('./a.txt', index_col=0)

nCycles = 1000
mem = np.tile(0.0, nCycles)
for i in range(1,nCycles):
    encoder(df)
    # get process memory (and store it in np.array mem)
    process = psutil.Process(os.getpid())
    mem[i] = process.memory_info()[0] / float(2 ** 20)
Expected Results
I would expect memory usage to be flat over time
Actual Results
Memory usage in M-Byte:

When fit and transform are commented the line actually becomes flat
Versions
System:
python: 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)]
executable: C:...\Scripts\python.exe
machine: Windows-7-6.1.7601-SP1
Python deps:
pip: 19.1.1
setuptools: 41.0.1
sklearn: 0.21.3
numpy: 1.16.2
scipy: 1.2.1
Cython: None
pandas: 0.24.2
a.txt