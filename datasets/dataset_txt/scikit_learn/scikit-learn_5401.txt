Contributor
conradlee commented on Apr 12, 2012
When I do recursive feature selection using stochastic gradient descent (SGDRegressor), I'm getting various errors. Sometimes I get a segmentation fault with no traceback, sometimes I get thrown out of python with a strange malloc error:
*** glibc detected *** /usr/bin/python: malloc(): smallbin double linked list corrupted: 0x0000000003629150 ***
and other times I get a python exception raised by linear_model/sgd_fast.so:
ValueError: floating-point under-/overflow occured.
Here's what you can do to recreate the problem. Download my example data here and extract it. Now start up python in the same folder and run
import numpy
from sklearn.linear_model import SGDRegressor
from sklearn.feature_selection import RFECV

X = numpy.load("problematic_X2.npy")
y = numpy.load("problem_y.npy").astype("f8")

selector = RFECV(SGDRegressor(alpha=5, warm_start=True), step=3, cv=4)
selector.fit(X, y)
As an aside: will using the warm_start=True option of SGDRegressor in the recursive feature elimination speed things up? Is that a good idea? It seems reasonable to me, but it might be a bad idea if the number of coefficients changes from run to run, and the selector_ array isn't used to update them between runs. (The same errors come up whether or not I use the warm_start option).