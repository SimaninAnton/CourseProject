Member
pprett commented on Sep 21, 2013
Our new tree engine (release 0.14) caused a performance regression in GBRT for small - moderate deep trees and large number of samples (>100K). The regression is even worse if you use large max_features (ie. the default) because the new tree engine is optimized for random split point selection.
If you care about performance on large sample size and moderately deep trees I recommend using release 0.13.1 for the time being -- its supposed to be 5x faster than 0.14 in this regime.
I apologize for the inconvenience -- we did test for performance regression in GBRT before switching to the new engine but underestimated the regression for large sample sizes.
The purpose of this issue is to collect some findings from GBRT users and that they care about (typical hyper-parameters settings, typical n_samples & n_features regimes, ...).