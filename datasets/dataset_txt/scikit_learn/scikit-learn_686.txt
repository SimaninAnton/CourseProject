Contributor
jeromedockes commented on Feb 28, 2019
The performance and memory usage of RidgeCV could be slightly improved
X is copied several times:
scikit-learn/sklearn/linear_model/ridge.py
Line 1017 in 7389dba
 X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64, 
scikit-learn/sklearn/linear_model/ridge.py
Line 1023 in 7389dba
 X, y, X_offset, y_offset, X_scale = LinearModel._preprocess_data( 
scikit-learn/sklearn/linear_model/ridge.py
Line 967 in 7389dba
 X = np.hstack((X, np.ones((X.shape[0], 1)))) 
the inputs are always cast to float64, would it make sense to keep their
original type?
scikit-learn/sklearn/linear_model/ridge.py
Line 1017 in 7389dba
 X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64, 
the cv scores are always stored in float64, would it make sense to store them
in float32 if X and Y were provided in float32? Also, a matrix of size (n
samples x n targets x n alphas) is always created, even when store_cv_values
is False. This can be a problem when there are many targets (but I guess this
may not be a very common scenario)
scikit-learn/sklearn/linear_model/ridge.py
Line 1060 in 7389dba
 cv_values = np.zeros((n_samples * n_y, len(self.alphas))) 