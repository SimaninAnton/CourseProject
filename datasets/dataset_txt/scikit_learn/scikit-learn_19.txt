Tizpaz commented 18 days ago ‚Ä¢
edited
Hello,
I want to bring your attention to the implementations of the batch generator in util module:
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/__init__.py#L719
In particular, batch_size parameter is required to be an integer. However, it might not be properly checked in the calling the library and be called with float values such as 0.00001 which leads to unexpected expensive iterations.
In the neural network module, the developers checked the parameters appropriately:
scikit-learn/sklearn/neural_network/_multilayer_perceptron.py
Line 345 in b194674
 # lbfgs does not support mini-batches 
However, this is not the case in the Kmeans module:
scikit-learn/sklearn/cluster/_kmeans.py
Line 1666 in b194674
 slices = gen_batches(X.shape[0], self.batch_size) 
I suggest checking the value of batch_size parameter directly in sklearn/utils/init.py module and avoid the possibility of calling it with small float values.
Please let me know if you have any questions,
Saeid
üëç 1