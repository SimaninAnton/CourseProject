shoeffner commented on Oct 5, 2016
Description
When doing PCA manually I would create the eigenvectors of the covariance matrix either using eig/eigh/svd etc. When doing it with scikit learn, I get inconsistent results.
Please refer to stackoverflow for details if the description below is not enough. You can find a full working example of the jupyter notebook here (gist) (nbviewer).
Steps/Code to Reproduce
def pca_eig(data):
    """Uses numpy.linalg.eig to calculate the PCA."""
    data = data.T @ data
    val, vec = np.linalg.eig(data)
    return val, vec
and
def pca_svd(data):
    """Uses numpy.linalg.svd to calculate the PCA."""
    u, s, v = np.linalg.svd(data)
    return s ** 2, v.T
lead to the same results, while
def pca_PCA(data):
   """Uses sklearn.decomposition.PCA to calculate the PCA."""
    pca = decomp.PCA().fit(data)
    return pca.explained_variance_, pca.components_
lead to the inconsistent result seen in the image below.
Expected Results
Actual Results
Versions
Darwin-15.6.0-x86_64-i386-64bit
Python 3.6.0b1 (default, Sep 29 2016, 01:14:08) 
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)]
NumPy 1.11.1
SciPy 0.18.1
Scikit-Learn 0.18
But also reproducible on Windows 8.1, 10, some ArchLinux derivatives and Ubuntu 14.04 with Python 3.5, because it's an issue with the code.
Bug source
If this really is a bug and not me understand the math wrong (see StO), then a fix will involve correcting this code (decomposition/pca.py:378ff):
U, S, V = linalg.svd(X, full_matrices=False)
# flip eigenvectors' sign to enforce deterministic output
U, V = svd_flip(U, V)

components_ = V
and similar code in the IncrementalPCA and possibly other PCA derivatives as well.