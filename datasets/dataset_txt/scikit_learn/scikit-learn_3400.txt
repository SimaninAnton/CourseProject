Aunsiels commented on Nov 13, 2015
Hi,
I found a problem while using VGGMM. Here is the code that does not work :
        clf = mixture.VBGMM(n_components=2)
        clf.fit(X_train, y_train)
        # create a k-fold cross validation iterator of k=5 folds
        cv = KFold(X_train.shape[0], 5, shuffle=True, random_state=33)
        #scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')
        scores = cross_val_score(clf,np.concatenate([X_train,sleep_test], axis=0), np.concatenate([y_train,y_unknown], axis=0), cv=cv, scoring='accuracy')
And I have got the error :
Traceback (most recent call last):
File "sleep.py", line 386, in
plot_alpha(X_train, y_train, sleep_test, y_unknown)
File "sleep.py", line 375, in plot_alpha
scores = cross_val_score(clf,np.concatenate([X_train,sleep_test], axis=0), np.concatenate([y_train,y_unknown], axis=0), cv=cv, scoring='accuracy')
File "/usr/lib/python3.5/site-packages/sklearn/cross_validation.py", line 1433, in cross_val_score
for train, test in cv)
File "/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py", line 804, in call
while self.dispatch_one_batch(iterator):
File "/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py", line 657, in dispatch_one_batch
tasks = BatchedCalls(itertools.islice(iterator, batch_size))
File "/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py", line 68, in init
self.items = list(iterator_slice)
File "/usr/lib/python3.5/site-packages/sklearn/cross_validation.py", line 1433, in
for train, test in cv)
File "/usr/lib/python3.5/site-packages/sklearn/base.py", line 105, in clone
(estimator, name))
RuntimeError: Cannot clone object VBGMM(alpha=0.5, covariance_type='diag', init_params='wmc', min_covar=None,
n_components=2, n_iter=10, params='wmc', random_state=None, thresh=None,
tol=0.001, verbose=0), as the constructor does not seem to set parameter alpha
The error seems to come from the dpgmm.py file, in the constructor of VBGMM. More exactly at line 701, in the way alpha is initialized :
self.alpha = float(alpha) / n_components
I am not an expert in python, but by removing the " / n_components", it seems to work again, but it does not have the same behavior.
I hope I am not doing something wrong.