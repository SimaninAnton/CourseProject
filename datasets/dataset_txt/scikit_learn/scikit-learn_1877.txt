maka89 commented on Sep 20, 2017 â€¢
edited
Encapsulating feed-forward neural network layers in classes with a forward() and backward() function, allows for more flexible implementations of neural networks(more like keras) and a cleaner constructor for the feed forward neural network class (MLPRegressor etc.). Optimizers can also be encapsulated in the same way, like I have below, but the layers are my main concern.
Kind of a big one, but think this could be useful. Think this would be a good balance between simplicity and flexibility. Also, it makes it natural to add new features (layers or optimizers) without having to make changes to the NN class itself.
I have a working implementation here that could be used, but needs some attention to meet sklearn specs. If people are interested, I could make a PR from this.
Here is a quick example of what this might look like:
#Can be used somewhat like current implementation:
model=NNClassifier([16,16],batch_size=32, maxiter=1)

#or more keras-like:
hidden_layers=[]
hidden_layers.append(Conv2D(4,kernel_size=(3,3),activation='relu'))
hidden_layers.append(MaxPool2D((2,2)))
hidden_layers.append(Conv2D(4,kernel_size=(3,3),activation='relu'))
hidden_layers.append(MaxPool2D((2,2)))
hidden_layers.append(Flatten())
hidden_layers.append(Dense(512,activation='relu'))
hidden_layers.append(Dropout(0.9)) 

model=NNClassifier(hidden_layers,opt=Adam(1e-3,beta_1=0.9,beta_2=0.999),batch_size=64,maxiter=50)
model.fit(x_train,y_train)
#Calculate score.
print("acc: " + str(model.score(x_test,y_test)))
What do people think of this approach?