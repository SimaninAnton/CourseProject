ktrnka commented on Nov 13, 2015
I've run several tests with 0.17 and found that GradientBoostingClassifier takes 40-50% longer to run.
50k x 61 dataset:
0.16.1: 7.6 min
0.17: 11.0 min, 11.6 min
200k x 61 dataset:
0.16.1: 43.9 min
0.17: 62.3 min
I tried adding feature scaling but that doesn't help. I tried disabling presort but it runs slower (good sign at least).
Another strange part is that my accuracy improved slightly with gradient boosting in 0.17. Could just be randomness and small sample but thought I should note it just in case. On the 200k test I got 67.66% in 0.16.1 and 67.75% in 0.17. On the 50k tests I got 66.08% in 0.16.1 and 66.17% and 66.34% in 0.17. The latter surprises me; I've seen variation due to randomness but usually under 0.05 not 0.17.
I also tested RandomForestClassifier but found that it was slightly faster in 0.17.
I've prepared a fork of my repo if you'd like to take a look. There's a dropbox link to the data in the readme. The main experimental script is src/exploration/train_test and gradient boosting is in the function gradient_boosting_exp. (Great apologies if you have to read my code... I've been working on this solo for a while. If you want code explanations let me know)
I'm running on a Mid 2014 MacBook Pro 2.6ghz i5 8gb ram, integrated graphics