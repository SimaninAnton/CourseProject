Contributor
kingjr commented on May 5, 2015
Dear all,
I'm trying to combine some linear steps of a fitted pipeline to minimize memory storage. Typically, I would like to rescale the coef of an SVC with the inverse_transform of a StandardScaler:
pipeline = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])
pipeline.fit(X, y)

scaler = pipeline.steps[0][1]
svc = pipeline.steps[1][1]
SV = svc.__getattribute__('support_vectors_')
for i, sv in enumerate(SV):
    SV[i] = scaler.inverse_transform(sv)
However, this produces some weird results. Typically the predictions of the classic pipeline (scaler then svc) differs from the combined pipeline (rescaled svc).
Here is a full example
import numpy as np
from matplotlib import pyplot as plt
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# setup dataset
snr = .1
X = np.random.randn(100, 10)
y = np.random.randint(0, 2, 100)
X[y == 1, :] += snr
# unwhiten data
X += 500
X *= 10

# set classic classifier
pipeline = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])
pipeline.fit(X, y)
y_pred = pipeline.decision_function(X)

# rescale SVC coef
scaler = pipeline.steps[0][1]
svc = pipeline.steps[1][1]
SV = svc.__getattribute__('support_vectors_')
# combine the scaler and support vector steps
for i, sv in enumerate(SV):
    SV[i] = scaler.inverse_transform(sv)
# test predictions
y_pred_scaled = svc.decision_function(X)

# plot both predictions
fig, ax = plt.subplots(1, 2)
ax[0].scatter(y_pred, y)
ax[0].scatter(y_pred_scaled, y + .1, color='r')
ax[0].set_ylim([-1, 3])
ax[0].set_xlabel('distance from hyperplan')
ax[0].set_ylabel('category')
ax[0].legend(['normal pipeline', 'combined pipeline'])
# plot correlation between classic and rescaled clf
ax[1].scatter(y_pred, y_pred_scaled, color='k')
ax[1].set_xlabel('normal pipeline distance')
ax[1].set_ylabel('combined pipeline distance')
plt.show()
I think the issue is due to a precision error.
Has anyone got any idea on how to circumvent this?
Thanks!