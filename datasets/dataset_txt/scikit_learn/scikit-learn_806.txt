Engineero commented on Jan 8, 2019
Description
metrics.adjusted_rand_score seems to give inconsistent results. The example given below is extreme, wherein two almost identical inputs return an ARI of 0.0.
Steps/Code to Reproduce
Example:
from sklearn import metrics as m

labels_true = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
labels_pred = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1]  # one digit off from labels_true

m.adjusted_rand_score(labels_true,labels_pred)
# 0.0
If you change the single 0 in labels_pred to a 1, the result is, as expected, 1.0.
Expected Results
One would expect the ARI in the case shown above to be very close to 1.0 for two almost-identical inputs.
Actual Results
The actual result for the example given is 0.0, which seems to indicate unexpected behavior in the algorithm.
Versions
Linux-2.6.32-696.23.1.el6.x86_64-x86_64-with-redhat-6.9-Santiago
Python 3.6.2 (default, Nov 4 2017, 17:40:18)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-17)]
NumPy 1.14.3
SciPy 1.1.0
Scikit-Learn 0.19.1