Member
jmschrei commented on Jul 9, 2015
On small sample sizes, GradientBoostingClassifier fails randomly, predicting entirely 0's or 1's. I have a benchmark script here (https://gist.github.com/jmschrei/d12e5d319997e00b8c03#file-gbfc_stability-py) which generates Gaussian blob datasets, and I have set the seed to an example where the classifier fails.
The sample script currently uses a single estimator with a max depth of 1, and compares this to the performance of a single decision tree. This dataset is linearly separable by a single decision stump. When I add more classifiers to the GBM, the probability of it predicting entirely 0's or 1's goes down, but still exists. The same for if I increase depth. The dataset is currently 100 points, split evenly into training and testing points, and by 1000 points it seems that this issue disappears.
I believe this is likely an overflow or underflow error somewhere in the implementation.