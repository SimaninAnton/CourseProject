randyphoa commented on Jul 31, 2016
Description
https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neural_network/multilayer_perceptron.py#L238-L243
When using an output activation besides the identity function, the gradients of the last layer of backpropagation does not include the partial derivative of the output activation function.
Current code
diff = y - activations[-1]
deltas[last] = -diff
# Compute gradient for the last layer
coef_grads, intercept_grads = self._compute_loss_grad( last, n_samples, activations, deltas, coef_grads, intercept_grads)
Possible fix
diff = y - activations[-1]
deltas[last] = -diff
#possible fix
if out_activation_ != "identity":
deltas[last] *= DERIVATIVES[self.out_activation_](activations[-1])
# Compute gradient for the last layer
coef_grads, intercept_grads = self._compute_loss_grad( last, n_samples, activations, deltas, coef_grads, intercept_grads)