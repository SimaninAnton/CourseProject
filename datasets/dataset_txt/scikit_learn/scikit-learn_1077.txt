ashishanand7 commented on Sep 18, 2018 â€¢
edited by TomDLT
Python 3.6.4
sklearn 0.19.1
CPU - Core i5 7200U (4 cores , n_jobs=-1 runs fine generally )
Using OnevsRestClassfier with n_jobs > 1 raises the issue . Fine if we keep n_jobs=1 .
code to reproduce:
classifier = OneVsRestClassifier(LogisticRegression(max_iter=10,penalty='l1'), n_jobs=-1)
classifier.fit(x_train_multilabel, y_train)
Output:
_``RemoteTraceback`

                           Traceback (most recent call last)
RemoteTraceback: 
"""
Traceback (most recent call last):
  File "c:\python36\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py", line 350, in __call__
    return self.func(*args, **kwargs)
  File "c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "c:\python36\lib\site-packages\sklearn\multiclass.py", line 80, in _fit_binary
    estimator.fit(X, y)
  File "c:\python36\lib\site-packages\sklearn\linear_model\logistic.py", line 1216, in fit
    order="C")
  File "c:\python36\lib\site-packages\sklearn\utils\validation.py", line 573, in check_X_y
    ensure_min_features, warn_on_dtype, estimator)
  File "c:\python36\lib\site-packages\sklearn\utils\validation.py", line 431, in check_array
    force_all_finite)
  File "c:\python36\lib\site-packages\sklearn\utils\validation.py", line 296, in _ensure_sparse_format
    spmatrix = spmatrix.astype(dtype)
  File "c:\python36\lib\site-packages\scipy\sparse\data.py", line 71, in astype
    self._deduped_data().astype(dtype, casting=casting, copy=copy),
  File "c:\python36\lib\site-packages\scipy\sparse\data.py", line 34, in _deduped_data
    self.sum_duplicates()
  File "c:\python36\lib\site-packages\scipy\sparse\compressed.py", line 1009, in sum_duplicates
    self.sort_indices()
  File "c:\python36\lib\site-packages\scipy\sparse\compressed.py", line 1055, in sort_indices
    self.indices, self.data)
ValueError: UPDATEIFCOPY base is read-only

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\python36\lib\multiprocessing\pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "c:\python36\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py", line 359, in __call__
    raise TransportableException(text, e_type)
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
ValueError                                         Tue Sep 18 10:02:29 2018
PID: 6736                              Python 3.6.4: c:\python36\python.exe
...........................................................................
c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_binary>, (LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, array([0, 0, 0, ..., 1, 0, 0])), {'classes': ['not 0', 0]})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_binary>
        args = (LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, array([0, 0, 0, ..., 1, 0, 0]))
        kwargs = {'classes': ['not 0', 0]}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
c:\python36\lib\site-packages\sklearn\multiclass.py in _fit_binary(estimator=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), classes=['not 0', 0])
     75             warnings.warn("Label %s is present in all training examples." %
     76                           str(classes[c]))
     77         estimator = _ConstantPredictor().fit(X, unique_y)
     78     else:
     79         estimator = clone(estimator)
---> 80         estimator.fit(X, y)
        estimator.fit = <bound method LogisticRegression.fit of Logistic...l=0.0001,
          verbose=0, warm_start=False)>
        X = <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>
        y = array([0, 0, 0, ..., 1, 0, 0])
     81     return estimator
     82 
     83 
     84 def _partial_fit_binary(estimator, X, y):

...........................................................................
c:\python36\lib\site-packages\sklearn\linear_model\logistic.py in fit(self=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), sample_weight=None)
   1211             _dtype = [np.float64, np.float32]
   1212         else:
   1213             _dtype = np.float64
   1214 
   1215         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,
-> 1216                          order="C")
   1217         check_classification_targets(y)
   1218         self.classes_ = np.unique(y)
   1219         n_samples, n_features = X.shape
   1220 

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in check_X_y(X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), accept_sparse='csr', dtype=<class 'numpy.float64'>, order='C', copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=False, estimator=None)
    568     y_converted : object
    569         The converted and validated y.
    570     """
    571     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
    572                     ensure_2d, allow_nd, ensure_min_samples,
--> 573                     ensure_min_features, warn_on_dtype, estimator)
        ensure_min_features = 1
        warn_on_dtype = False
        estimator = None
    574     if multi_output:
    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
    576                         dtype=None)
    577     else:

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in check_array(array=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, accept_sparse='csr', dtype=<class 'numpy.float64'>, order='C', copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)
    426         estimator_name = "Estimator"
    427     context = " by %s" % estimator_name if estimator is not None else ""
    428 
    429     if sp.issparse(array):
    430         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,
--> 431                                       force_all_finite)
        force_all_finite = True
    432     else:
    433         array = np.array(array, dtype=dtype, order=order, copy=copy)
    434 
    435         if ensure_2d:

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in _ensure_sparse_format(spmatrix=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, accept_sparse=['csr'], dtype=<class 'numpy.float64'>, copy=False, force_all_finite=True)
    291                          "boolean or list of strings. You provided "
    292                          "'accept_sparse={}'.".format(accept_sparse))
    293 
    294     if dtype != spmatrix.dtype:
    295         # convert dtype
--> 296         spmatrix = spmatrix.astype(dtype)
        spmatrix = <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>
        spmatrix.astype = <bound method _data_matrix.astype of <80117x1200...stored elements in Compressed Sparse Row format>>
        dtype = <class 'numpy.float64'>
    297     elif copy and not changed_format:
    298         # force copy
    299         spmatrix = spmatrix.copy()
    300 

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\data.py in astype(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, dtype=dtype('float64'), casting='unsafe', copy=True)
     66 
     67     def astype(self, dtype, casting='unsafe', copy=True):
     68         dtype = np.dtype(dtype)
     69         if self.dtype != dtype:
     70             return self._with_data(
---> 71                 self._deduped_data().astype(dtype, casting=casting, copy=copy),
        self._deduped_data.astype = undefined
        dtype = dtype('float64')
        casting = 'unsafe'
        copy = True
     72                 copy=copy)
     73         elif copy:
     74             return self.copy()
     75         else:

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\data.py in _deduped_data(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
     29         self.data.dtype = newtype
     30     dtype = property(fget=_get_dtype, fset=_set_dtype)
     31 
     32     def _deduped_data(self):
     33         if hasattr(self, 'sum_duplicates'):
---> 34             self.sum_duplicates()
        self.sum_duplicates = <bound method _cs_matrix.sum_duplicates of <8011...stored elements in Compressed Sparse Row format>>
     35         return self.data
     36 
     37     def __abs__(self):
     38         return self._with_data(abs(self._deduped_data()))

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\compressed.py in sum_duplicates(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
   1004 
   1005         The is an *in place* operation
   1006         """
   1007         if self.has_canonical_format:
   1008             return
-> 1009         self.sort_indices()
        self.sort_indices = <bound method _cs_matrix.sort_indices of <80117x...stored elements in Compressed Sparse Row format>>
   1010 
   1011         M, N = self._swap(self.shape)
   1012         _sparsetools.csr_sum_duplicates(M, N, self.indptr, self.indices,
   1013                                         self.data)

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\compressed.py in sort_indices(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
   1050         """Sort the indices of this matrix *in place*
   1051         """
   1052 
   1053         if not self.has_sorted_indices:
   1054             _sparsetools.csr_sort_indices(len(self.indptr) - 1, self.indptr,
-> 1055                                           self.indices, self.data)
        self.indices = memmap([ 96128,  15218, 113979, ...,  66087, 113893,  15114])
        self.data = memmap([1, 3, 1, ..., 5, 1, 2], dtype=int64)
   1056             self.has_sorted_indices = True
   1057 
   1058     def prune(self):
   1059         """Remove empty space after all non-zero elements.

ValueError: UPDATEIFCOPY base is read-only
___________________________________________________________________________
"""

The above exception was the direct cause of the following exception:

TransportableException                    Traceback (most recent call last)
c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in retrieve(self)
    698                 if getattr(self._backend, 'supports_timeout', False):
--> 699                     self._output.extend(job.get(timeout=self.timeout))
    700                 else:

c:\python36\lib\multiprocessing\pool.py in get(self, timeout)
    643         else:
--> 644             raise self._value
    645 

TransportableException: TransportableException
___________________________________________________________________________
ValueError                                         Tue Sep 18 10:02:29 2018
PID: 6736                              Python 3.6.4: c:\python36\python.exe
...........................................................................
c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_binary>, (LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, array([0, 0, 0, ..., 1, 0, 0])), {'classes': ['not 0', 0]})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_binary>
        args = (LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, array([0, 0, 0, ..., 1, 0, 0]))
        kwargs = {'classes': ['not 0', 0]}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
c:\python36\lib\site-packages\sklearn\multiclass.py in _fit_binary(estimator=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), classes=['not 0', 0])
     75             warnings.warn("Label %s is present in all training examples." %
     76                           str(classes[c]))
     77         estimator = _ConstantPredictor().fit(X, unique_y)
     78     else:
     79         estimator = clone(estimator)
---> 80         estimator.fit(X, y)
        estimator.fit = <bound method LogisticRegression.fit of Logistic...l=0.0001,
          verbose=0, warm_start=False)>
        X = <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>
        y = array([0, 0, 0, ..., 1, 0, 0])
     81     return estimator
     82 
     83 
     84 def _partial_fit_binary(estimator, X, y):

...........................................................................
c:\python36\lib\site-packages\sklearn\linear_model\logistic.py in fit(self=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), sample_weight=None)
   1211             _dtype = [np.float64, np.float32]
   1212         else:
   1213             _dtype = np.float64
   1214 
   1215         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,
-> 1216                          order="C")
   1217         check_classification_targets(y)
   1218         self.classes_ = np.unique(y)
   1219         n_samples, n_features = X.shape
   1220 

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in check_X_y(X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), accept_sparse='csr', dtype=<class 'numpy.float64'>, order='C', copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=False, estimator=None)
    568     y_converted : object
    569         The converted and validated y.
    570     """
    571     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
    572                     ensure_2d, allow_nd, ensure_min_samples,
--> 573                     ensure_min_features, warn_on_dtype, estimator)
        ensure_min_features = 1
        warn_on_dtype = False
        estimator = None
    574     if multi_output:
    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
    576                         dtype=None)
    577     else:

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in check_array(array=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, accept_sparse='csr', dtype=<class 'numpy.float64'>, order='C', copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)
    426         estimator_name = "Estimator"
    427     context = " by %s" % estimator_name if estimator is not None else ""
    428 
    429     if sp.issparse(array):
    430         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,
--> 431                                       force_all_finite)
        force_all_finite = True
    432     else:
    433         array = np.array(array, dtype=dtype, order=order, copy=copy)
    434 
    435         if ensure_2d:

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in _ensure_sparse_format(spmatrix=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, accept_sparse=['csr'], dtype=<class 'numpy.float64'>, copy=False, force_all_finite=True)
    291                          "boolean or list of strings. You provided "
    292                          "'accept_sparse={}'.".format(accept_sparse))
    293 
    294     if dtype != spmatrix.dtype:
    295         # convert dtype
--> 296         spmatrix = spmatrix.astype(dtype)
        spmatrix = <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>
        spmatrix.astype = <bound method _data_matrix.astype of <80117x1200...stored elements in Compressed Sparse Row format>>
        dtype = <class 'numpy.float64'>
    297     elif copy and not changed_format:
    298         # force copy
    299         spmatrix = spmatrix.copy()
    300 

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\data.py in astype(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, dtype=dtype('float64'), casting='unsafe', copy=True)
     66 
     67     def astype(self, dtype, casting='unsafe', copy=True):
     68         dtype = np.dtype(dtype)
     69         if self.dtype != dtype:
     70             return self._with_data(
---> 71                 self._deduped_data().astype(dtype, casting=casting, copy=copy),
        self._deduped_data.astype = undefined
        dtype = dtype('float64')
        casting = 'unsafe'
        copy = True
     72                 copy=copy)
     73         elif copy:
     74             return self.copy()
     75         else:

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\data.py in _deduped_data(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
     29         self.data.dtype = newtype
     30     dtype = property(fget=_get_dtype, fset=_set_dtype)
     31 
     32     def _deduped_data(self):
     33         if hasattr(self, 'sum_duplicates'):
---> 34             self.sum_duplicates()
        self.sum_duplicates = <bound method _cs_matrix.sum_duplicates of <8011...stored elements in Compressed Sparse Row format>>
     35         return self.data
     36 
     37     def __abs__(self):
     38         return self._with_data(abs(self._deduped_data()))

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\compressed.py in sum_duplicates(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
   1004 
   1005         The is an *in place* operation
   1006         """
   1007         if self.has_canonical_format:
   1008             return
-> 1009         self.sort_indices()
        self.sort_indices = <bound method _cs_matrix.sort_indices of <80117x...stored elements in Compressed Sparse Row format>>
   1010 
   1011         M, N = self._swap(self.shape)
   1012         _sparsetools.csr_sum_duplicates(M, N, self.indptr, self.indices,
   1013                                         self.data)

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\compressed.py in sort_indices(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
   1050         """Sort the indices of this matrix *in place*
   1051         """
   1052 
   1053         if not self.has_sorted_indices:
   1054             _sparsetools.csr_sort_indices(len(self.indptr) - 1, self.indptr,
-> 1055                                           self.indices, self.data)
        self.indices = memmap([ 96128,  15218, 113979, ...,  66087, 113893,  15114])
        self.data = memmap([1, 3, 1, ..., 5, 1, 2], dtype=int64)
   1056             self.has_sorted_indices = True
   1057 
   1058     def prune(self):
   1059         """Remove empty space after all non-zero elements.

ValueError: UPDATEIFCOPY base is read-only
___________________________________________________________________________

During handling of the above exception, another exception occurred:

JoblibValueError                          Traceback (most recent call last)
<ipython-input-15-29ca890d9ade> in <module>()
      1 start = datetime.now()
      2 classifier_2 = OneVsRestClassifier(LogisticRegression(max_iter=10,penalty='l1'), n_jobs=-1)
----> 3 classifier_2.fit(x_train_multilabel, y_train)
      4 predictions_2 = classifier_2.predict(x_test_multilabel)
      5 print("Accuracy :",metrics.accuracy_score(y_test, predictions_2))

c:\python36\lib\site-packages\sklearn\multiclass.py in fit(self, X, y)
    213                 "not %s" % self.label_binarizer_.classes_[i],
    214                 self.label_binarizer_.classes_[i]])
--> 215             for i, column in enumerate(columns))
    216 
    217         return self

c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in __call__(self, iterable)
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time

c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in retrieve(self)
    738                     exception = exception_type(report)
    739 
--> 740                     raise exception
    741 
    742     def __call__(self, iterable):

JoblibValueError: JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
c:\python36\lib\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)
    188         sys.exit(msg)
    189     main_globals = sys.modules["__main__"].__dict__
    190     if alter_argv:
    191         sys.argv[0] = mod_spec.origin
    192     return _run_code(code, main_globals, None,
--> 193                      "__main__", mod_spec)
        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...on36\\lib\\site-packages\\ipykernel_launcher.py')
    194 
    195 def run_module(mod_name, init_globals=None,
    196                run_name=None, alter_sys=False):
    197     """Execute a module's code without importing it

...........................................................................
c:\python36\lib\runpy.py in _run_code(code=<code object <module> at 0x000002DEF864E390, fil...lib\site-packages\ipykernel_launcher.py", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'c:\python36\lib\site-packages\__pycache__\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\n\nTh...orts until\nafter removing the cwd from sys.path.\n', '__file__': r'c:\python36\lib\site-packages\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...on36\\lib\\site-packages\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'c:\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...on36\\lib\\site-packages\\ipykernel_launcher.py'), pkg_name='', script_name=None)
     80                        __cached__ = cached,
     81                        __doc__ = None,
     82                        __loader__ = loader,
     83                        __package__ = pkg_name,
     84                        __spec__ = mod_spec)
---> 85     exec(code, run_globals)
        code = <code object <module> at 0x000002DEF864E390, fil...lib\site-packages\ipykernel_launcher.py", line 5>
        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'c:\python36\lib\site-packages\__pycache__\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\n\nTh...orts until\nafter removing the cwd from sys.path.\n', '__file__': r'c:\python36\lib\site-packages\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...on36\\lib\\site-packages\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'c:\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py'>, ...}
     86     return run_globals
     87 
     88 def _run_module_code(code, init_globals=None,
     89                     mod_name=None, mod_spec=None,

...........................................................................
c:\python36\lib\site-packages\ipykernel_launcher.py in <module>()
     11     # This is added back by InteractiveShellApp.init_path()
     12     if sys.path[0] == '':
     13         del sys.path[0]
     14 
     15     from ipykernel import kernelapp as app
---> 16     app.launch_new_instance()

...........................................................................
c:\python36\lib\site-packages\traitlets\config\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})
    653 
    654         If a global instance already exists, this reinitializes and starts it
    655         """
    656         app = cls.instance(**kwargs)
    657         app.initialize(argv)
--> 658         app.start()
        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>
    659 
    660 #-----------------------------------------------------------------------------
    661 # utility functions, for convenience
    662 #-----------------------------------------------------------------------------

...........................................................................
c:\python36\lib\site-packages\ipykernel\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)
    473         if self.poller is not None:
    474             self.poller.start()
    475         self.kernel.start()
    476         self.io_loop = ioloop.IOLoop.current()
    477         try:
--> 478             self.io_loop.start()
        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>
    479         except KeyboardInterrupt:
    480             pass
    481 
    482 launch_new_instance = IPKernelApp.launch_instance

...........................................................................
c:\python36\lib\site-packages\zmq\eventloop\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)
    172             )
    173         return loop
    174     
    175     def start(self):
    176         try:
--> 177             super(ZMQIOLoop, self).start()
        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>
    178         except ZMQError as e:
    179             if e.errno == ETERM:
    180                 # quietly return on ETERM
    181                 pass

...........................................................................
c:\python36\lib\site-packages\tornado\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)
    883                 self._events.update(event_pairs)
    884                 while self._events:
    885                     fd, events = self._events.popitem()
    886                     try:
    887                         fd_obj, handler_func = self._handlers[fd]
--> 888                         handler_func(fd_obj, events)
        handler_func = <function wrap.<locals>.null_wrapper>
        fd_obj = <zmq.sugar.socket.Socket object>
        events = 1
    889                     except (OSError, IOError) as e:
    890                         if errno_from_exception(e) == errno.EPIPE:
    891                             # Happens when the client closes the connection
    892                             pass

...........................................................................
c:\python36\lib\site-packages\tornado\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})
    272         # Fast path when there are no active contexts.
    273         def null_wrapper(*args, **kwargs):
    274             try:
    275                 current_state = _state.contexts
    276                 _state.contexts = cap_contexts[0]
--> 277                 return fn(*args, **kwargs)
        args = (<zmq.sugar.socket.Socket object>, 1)
        kwargs = {}
    278             finally:
    279                 _state.contexts = current_state
    280         null_wrapper._wrapped = True
    281         return null_wrapper

...........................................................................
c:\python36\lib\site-packages\zmq\eventloop\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)
    435             # dispatch events:
    436             if events & IOLoop.ERROR:
    437                 gen_log.error("got POLLERR event on ZMQStream, which doesn't make sense")
    438                 return
    439             if events & IOLoop.READ:
--> 440                 self._handle_recv()
        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>
    441                 if not self.socket:
    442                     return
    443             if events & IOLoop.WRITE:
    444                 self._handle_send()

...........................................................................
c:\python36\lib\site-packages\zmq\eventloop\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)
    467                 gen_log.error("RECV Error: %s"%zmq.strerror(e.errno))
    468         else:
    469             if self._recv_callback:
    470                 callback = self._recv_callback
    471                 # self._recv_callback = None
--> 472                 self._run_callback(callback, msg)
        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>
        callback = <function wrap.<locals>.null_wrapper>
        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]
    473                 
    474         # self.update_state()
    475         
    476 

...........................................................................
c:\python36\lib\site-packages\zmq\eventloop\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})
    409         close our socket."""
    410         try:
    411             # Use a NullContext to ensure that all StackContexts are run
    412             # inside our blanket exception handler rather than outside.
    413             with stack_context.NullContext():
--> 414                 callback(*args, **kwargs)
        callback = <function wrap.<locals>.null_wrapper>
        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)
        kwargs = {}
    415         except:
    416             gen_log.error("Uncaught exception, closing connection.",
    417                           exc_info=True)
    418             # Close the socket on an uncaught exception from a user callback

...........................................................................
c:\python36\lib\site-packages\tornado\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})
    272         # Fast path when there are no active contexts.
    273         def null_wrapper(*args, **kwargs):
    274             try:
    275                 current_state = _state.contexts
    276                 _state.contexts = cap_contexts[0]
--> 277                 return fn(*args, **kwargs)
        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)
        kwargs = {}
    278             finally:
    279                 _state.contexts = current_state
    280         null_wrapper._wrapped = True
    281         return null_wrapper

...........................................................................
c:\python36\lib\site-packages\ipykernel\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])
    276         if self.control_stream:
    277             self.control_stream.on_recv(self.dispatch_control, copy=False)
    278 
    279         def make_dispatcher(stream):
    280             def dispatcher(msg):
--> 281                 return self.dispatch_shell(stream, msg)
        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]
    282             return dispatcher
    283 
    284         for s in self.shell_streams:
    285             s.on_recv(make_dispatcher(s), copy=False)

...........................................................................
c:\python36\lib\site-packages\ipykernel\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 18, 4, 32, 27, 665742, tzinfo=tzutc()), 'msg_id': '93ABCAF5F2F141D6A925520C6DAE65B2', 'msg_type': 'execute_request', 'session': 'F1F7640B6828436D8D05CDBE08A7F6B7', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '93ABCAF5F2F141D6A925520C6DAE65B2', 'msg_type': 'execute_request', 'parent_header': {}})
    227             self.log.warn("Unknown message type: %r", msg_type)
    228         else:
    229             self.log.debug("%s: %s", msg_type, msg)
    230             self.pre_handler_hook()
    231             try:
--> 232                 handler(stream, idents, msg)
        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>
        stream = <zmq.eventloop.zmqstream.ZMQStream object>
        idents = [b'F1F7640B6828436D8D05CDBE08A7F6B7']
        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 18, 4, 32, 27, 665742, tzinfo=tzutc()), 'msg_id': '93ABCAF5F2F141D6A925520C6DAE65B2', 'msg_type': 'execute_request', 'session': 'F1F7640B6828436D8D05CDBE08A7F6B7', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '93ABCAF5F2F141D6A925520C6DAE65B2', 'msg_type': 'execute_request', 'parent_header': {}}
    233             except Exception:
    234                 self.log.error("Exception in message handler:", exc_info=True)
    235             finally:
    236                 self.post_handler_hook()

...........................................................................
c:\python36\lib\site-packages\ipykernel\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'F1F7640B6828436D8D05CDBE08A7F6B7'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 9, 18, 4, 32, 27, 665742, tzinfo=tzutc()), 'msg_id': '93ABCAF5F2F141D6A925520C6DAE65B2', 'msg_type': 'execute_request', 'session': 'F1F7640B6828436D8D05CDBE08A7F6B7', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '93ABCAF5F2F141D6A925520C6DAE65B2', 'msg_type': 'execute_request', 'parent_header': {}})
    392         if not silent:
    393             self.execution_count += 1
    394             self._publish_execute_input(code, parent, self.execution_count)
    395 
    396         reply_content = self.do_execute(code, silent, store_history,
--> 397                                         user_expressions, allow_stdin)
        user_expressions = {}
        allow_stdin = True
    398 
    399         # Flush output before sending the reply.
    400         sys.stdout.flush()
    401         sys.stderr.flush()

...........................................................................
c:\python36\lib\site-packages\ipykernel\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)
    203 
    204         self._forward_input(allow_stdin)
    205 
    206         reply_content = {}
    207         try:
--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)
        res = undefined
        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>
        code = 'start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)'
        store_history = True
        silent = False
    209         finally:
    210             self._restore_input()
    211 
    212         if res.error_before_exec is not None:

...........................................................................
c:\python36\lib\site-packages\ipykernel\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)',), **kwargs={'silent': False, 'store_history': True})
    528             )
    529         self.payload_manager.write_payload(payload)
    530 
    531     def run_cell(self, *args, **kwargs):
    532         self._last_traceback = None
--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>
        args = ('start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)',)
        kwargs = {'silent': False, 'store_history': True}
    534 
    535     def _showtraceback(self, etype, evalue, stb):
    536         # try to preserve ordering of tracebacks and print statements
    537         sys.stdout.flush()

...........................................................................
c:\python36\lib\site-packages\IPython\core\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)', store_history=True, silent=False, shell_futures=True)
   2723                 self.displayhook.exec_result = result
   2724 
   2725                 # Execute the user code
   2726                 interactivity = "none" if silent else self.ast_node_interactivity
   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,
-> 2728                    interactivity=interactivity, compiler=compiler, result=result)
        interactivity = 'last_expr'
        compiler = <IPython.core.compilerop.CachingCompiler object>
   2729                 
   2730                 self.last_execution_succeeded = not has_raised
   2731                 self.last_execution_result = result
   2732 

...........................................................................
c:\python36\lib\site-packages\IPython\core\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-15-29ca890d9ade>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 2dee32fe3c8, executio..._before_exec=None error_in_exec=None result=None>)
   2845 
   2846         try:
   2847             for i, node in enumerate(to_run_exec):
   2848                 mod = ast.Module([node])
   2849                 code = compiler(mod, cell_name, "exec")
-> 2850                 if self.run_code(code, result):
        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>
        code = <code object <module> at 0x000002DEE2EEFF60, file "<ipython-input-15-29ca890d9ade>", line 3>
        result = <ExecutionResult object at 2dee32fe3c8, executio..._before_exec=None error_in_exec=None result=None>
   2851                     return True
   2852 
   2853             for i, node in enumerate(to_run_interactive):
   2854                 mod = ast.Interactive([node])

...........................................................................
c:\python36\lib\site-packages\IPython\core\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000002DEE2EEFF60, file "<ipython-input-15-29ca890d9ade>", line 3>, result=<ExecutionResult object at 2dee32fe3c8, executio..._before_exec=None error_in_exec=None result=None>)
   2905         outflag = True  # happens in more places, so it's easier as default
   2906         try:
   2907             try:
   2908                 self.hooks.pre_run_code_hook()
   2909                 #rprint('Running code', repr(code_obj)) # dbg
-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)
        code_obj = <code object <module> at 0x000002DEE2EEFF60, file "<ipython-input-15-29ca890d9ade>", line 3>
        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import warnings\nwarnings.filterwarnings("ignore"... LogisticRegression\nfrom datetime import datetime', '#http://www.sqlitetutorial.net/sqlite-python/cre..._database_table("Processed.db", sql_create_table)', '#Taking 0.5 Million entries to a dataframe.\nwrit...cessed""", conn_r)\nconn_r.commit()\nconn_r.close()', 'sampled_data=[]\nfor i in range(0,preprocessed_da...[i])\npreprocessed_data=pd.DataFrame(sampled_data)', 'print("number of data points in sample :", prepr...ber of dimensions :", preprocessed_data.shape[1])', "vectorizer = CountVectorizer(tokenizer = lambda ...ctorizer.fit_transform(preprocessed_data['tags'])", 'def tags_to_choose(n):\n    t = multilabel_y.sum(...n.sum(axis=1)\n    return (np.count_nonzero(x==0))', 'questions_explained = []\ntotal_tags=multilabel_y...l_qs-questions_explained_fn(i))/total_qs)*100,3))', '# we will be taking 500 tags\nmultilabel_yx = tag... questions_explained_fn(500),"out of ", total_qs)', 'train_datasize = int(preprocessed_data.shape[0]*...l_yx[train_datasize:preprocessed_data.shape[0],:]', 'print("Number of data points in train data :", y...ber of data points in test data :", y_test.shape)', 'start = datetime.now()\nvectorizer = CountVectori...aken to run this cell :", datetime.now() - start)', 'print("Dimensions of train data X:",x_train_mult...ta X:",x_test_multilabel.shape,"Y:",y_test.shape)', 'start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)', 'start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)'], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'NamespaceMagics': <class 'IPython.core.magics.namespace.NamespaceMagics'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {}, 'SGDClassifier': <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'WordCloud': <class 'wordcloud.wordcloud.WordCloud'>, ...}
        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import warnings\nwarnings.filterwarnings("ignore"... LogisticRegression\nfrom datetime import datetime', '#http://www.sqlitetutorial.net/sqlite-python/cre..._database_table("Processed.db", sql_create_table)', '#Taking 0.5 Million entries to a dataframe.\nwrit...cessed""", conn_r)\nconn_r.commit()\nconn_r.close()', 'sampled_data=[]\nfor i in range(0,preprocessed_da...[i])\npreprocessed_data=pd.DataFrame(sampled_data)', 'print("number of data points in sample :", prepr...ber of dimensions :", preprocessed_data.shape[1])', "vectorizer = CountVectorizer(tokenizer = lambda ...ctorizer.fit_transform(preprocessed_data['tags'])", 'def tags_to_choose(n):\n    t = multilabel_y.sum(...n.sum(axis=1)\n    return (np.count_nonzero(x==0))', 'questions_explained = []\ntotal_tags=multilabel_y...l_qs-questions_explained_fn(i))/total_qs)*100,3))', '# we will be taking 500 tags\nmultilabel_yx = tag... questions_explained_fn(500),"out of ", total_qs)', 'train_datasize = int(preprocessed_data.shape[0]*...l_yx[train_datasize:preprocessed_data.shape[0],:]', 'print("Number of data points in train data :", y...ber of data points in test data :", y_test.shape)', 'start = datetime.now()\nvectorizer = CountVectori...aken to run this cell :", datetime.now() - start)', 'print("Dimensions of train data X:",x_train_mult...ta X:",x_test_multilabel.shape,"Y:",y_test.shape)', 'start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)', 'start = datetime.now()\nclassifier_2 = OneVsRestC...aken to run this cell :", datetime.now() - start)'], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'NamespaceMagics': <class 'IPython.core.magics.namespace.NamespaceMagics'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {}, 'SGDClassifier': <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'WordCloud': <class 'wordcloud.wordcloud.WordCloud'>, ...}
   2911             finally:
   2912                 # Reset our crash handler in place
   2913                 sys.excepthook = old_excepthook
   2914         except SystemExit as e:

...........................................................................
D:\Downloads\AAIC\Module 44\<ipython-input-15-29ca890d9ade> in <module>()
      1 start = datetime.now()
      2 classifier_2 = OneVsRestClassifier(LogisticRegression(max_iter=10,penalty='l1'), n_jobs=-1)
----> 3 classifier_2.fit(x_train_multilabel, y_train)
      4 predictions_2 = classifier_2.predict(x_test_multilabel)
      5 print("Accuracy :",metrics.accuracy_score(y_test, predictions_2))
      6 print("Hamming loss ",metrics.hamming_loss(y_test,predictions_2))
      7 
      8 
      9 precision = precision_score(y_test, predictions_2, average='micro')
     10 recall = recall_score(y_test, predictions_2, average='micro')

...........................................................................
c:\python36\lib\site-packages\sklearn\multiclass.py in fit(self=OneVsRestClassifier(estimator=LogisticRegression...erbose=0, warm_start=False),
          n_jobs=-1), X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=<80117x500 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>)
    210         # of spawning threads.  See joblib issue #112.
    211         self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary)(
    212             self.estimator, X, column, classes=[
    213                 "not %s" % self.label_binarizer_.classes_[i],
    214                 self.label_binarizer_.classes_[i]])
--> 215             for i, column in enumerate(columns))
        columns = <generator object OneVsRestClassifier.fit.<locals>.<genexpr>>
    216 
    217         return self
    218 
    219     @if_delegate_has_method('estimator')

...........................................................................
c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object OneVsRestClassifier.fit.<locals>.<genexpr>>)
    784             if pre_dispatch == "all" or n_jobs == 1:
    785                 # The iterable was consumed all at once by the above for loop.
    786                 # No need to wait for async callbacks to trigger to
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time
    792             self._print('Done %3i out of %3i | elapsed: %s finished',
    793                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Tue Sep 18 10:02:29 2018
PID: 6736                              Python 3.6.4: c:\python36\python.exe
...........................................................................
c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_binary>, (LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, array([0, 0, 0, ..., 1, 0, 0])), {'classes': ['not 0', 0]})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
c:\python36\lib\site-packages\sklearn\externals\joblib\parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_binary>
        args = (LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, array([0, 0, 0, ..., 1, 0, 0]))
        kwargs = {'classes': ['not 0', 0]}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
c:\python36\lib\site-packages\sklearn\multiclass.py in _fit_binary(estimator=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), classes=['not 0', 0])
     75             warnings.warn("Label %s is present in all training examples." %
     76                           str(classes[c]))
     77         estimator = _ConstantPredictor().fit(X, unique_y)
     78     else:
     79         estimator = clone(estimator)
---> 80         estimator.fit(X, y)
        estimator.fit = <bound method LogisticRegression.fit of Logistic...l=0.0001,
          verbose=0, warm_start=False)>
        X = <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>
        y = array([0, 0, 0, ..., 1, 0, 0])
     81     return estimator
     82 
     83 
     84 def _partial_fit_binary(estimator, X, y):

...........................................................................
c:\python36\lib\site-packages\sklearn\linear_model\logistic.py in fit(self=LogisticRegression(C=1.0, class_weight=None, dua...ol=0.0001,
          verbose=0, warm_start=False), X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), sample_weight=None)
   1211             _dtype = [np.float64, np.float32]
   1212         else:
   1213             _dtype = np.float64
   1214 
   1215         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,
-> 1216                          order="C")
   1217         check_classification_targets(y)
   1218         self.classes_ = np.unique(y)
   1219         n_samples, n_features = X.shape
   1220 

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in check_X_y(X=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 0, 0]), accept_sparse='csr', dtype=<class 'numpy.float64'>, order='C', copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, multi_output=False, ensure_min_samples=1, ensure_min_features=1, y_numeric=False, warn_on_dtype=False, estimator=None)
    568     y_converted : object
    569         The converted and validated y.
    570     """
    571     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
    572                     ensure_2d, allow_nd, ensure_min_samples,
--> 573                     ensure_min_features, warn_on_dtype, estimator)
        ensure_min_features = 1
        warn_on_dtype = False
        estimator = None
    574     if multi_output:
    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
    576                         dtype=None)
    577     else:

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in check_array(array=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, accept_sparse='csr', dtype=<class 'numpy.float64'>, order='C', copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)
    426         estimator_name = "Estimator"
    427     context = " by %s" % estimator_name if estimator is not None else ""
    428 
    429     if sp.issparse(array):
    430         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,
--> 431                                       force_all_finite)
        force_all_finite = True
    432     else:
    433         array = np.array(array, dtype=dtype, order=order, copy=copy)
    434 
    435         if ensure_2d:

...........................................................................
c:\python36\lib\site-packages\sklearn\utils\validation.py in _ensure_sparse_format(spmatrix=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, accept_sparse=['csr'], dtype=<class 'numpy.float64'>, copy=False, force_all_finite=True)
    291                          "boolean or list of strings. You provided "
    292                          "'accept_sparse={}'.".format(accept_sparse))
    293 
    294     if dtype != spmatrix.dtype:
    295         # convert dtype
--> 296         spmatrix = spmatrix.astype(dtype)
        spmatrix = <80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>
        spmatrix.astype = <bound method _data_matrix.astype of <80117x1200...stored elements in Compressed Sparse Row format>>
        dtype = <class 'numpy.float64'>
    297     elif copy and not changed_format:
    298         # force copy
    299         spmatrix = spmatrix.copy()
    300 

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\data.py in astype(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>, dtype=dtype('float64'), casting='unsafe', copy=True)
     66 
     67     def astype(self, dtype, casting='unsafe', copy=True):
     68         dtype = np.dtype(dtype)
     69         if self.dtype != dtype:
     70             return self._with_data(
---> 71                 self._deduped_data().astype(dtype, casting=casting, copy=copy),
        self._deduped_data.astype = undefined
        dtype = dtype('float64')
        casting = 'unsafe'
        copy = True
     72                 copy=copy)
     73         elif copy:
     74             return self.copy()
     75         else:

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\data.py in _deduped_data(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
     29         self.data.dtype = newtype
     30     dtype = property(fget=_get_dtype, fset=_set_dtype)
     31 
     32     def _deduped_data(self):
     33         if hasattr(self, 'sum_duplicates'):
---> 34             self.sum_duplicates()
        self.sum_duplicates = <bound method _cs_matrix.sum_duplicates of <8011...stored elements in Compressed Sparse Row format>>
     35         return self.data
     36 
     37     def __abs__(self):
     38         return self._with_data(abs(self._deduped_data()))

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\compressed.py in sum_duplicates(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
   1004 
   1005         The is an *in place* operation
   1006         """
   1007         if self.has_canonical_format:
   1008             return
-> 1009         self.sort_indices()
        self.sort_indices = <bound method _cs_matrix.sort_indices of <80117x...stored elements in Compressed Sparse Row format>>
   1010 
   1011         M, N = self._swap(self.shape)
   1012         _sparsetools.csr_sum_duplicates(M, N, self.indptr, self.indices,
   1013                                         self.data)

...........................................................................
c:\python36\lib\site-packages\scipy\sparse\compressed.py in sort_indices(self=<80117x120000 sparse matrix of type '<class 'num... stored elements in Compressed Sparse Row format>)
   1050         """Sort the indices of this matrix *in place*
   1051         """
   1052 
   1053         if not self.has_sorted_indices:
   1054             _sparsetools.csr_sort_indices(len(self.indptr) - 1, self.indptr,
-> 1055                                           self.indices, self.data)
        self.indices = memmap([ 96128,  15218, 113979, ...,  66087, 113893,  15114])
        self.data = memmap([1, 3, 1, ..., 5, 1, 2], dtype=int64)
   1056             self.has_sorted_indices = True
   1057 
   1058     def prune(self):
   1059         """Remove empty space after all non-zero elements.

ValueError: UPDATEIFCOPY base is _read-only``_