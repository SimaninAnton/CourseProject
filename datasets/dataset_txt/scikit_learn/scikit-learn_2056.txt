pancodia commented on Jul 12, 2017
Description
I wrote a script to fit Ridge with different values of alpha, then plot the shrinkage path of the coefficients. The plot looks as expected when the code is run in Python 3.6. However, when I run the same code in Python 3.5, the plot looks very bizarre.
Steps/Code to Reproduce
https://gist.github.com/pancha0/593308bab1e1202f7213f3c0b1e3976d
Expected Results
The two plots should look the same.
Actual Results
They are different. The plot from Python 3.6 shows as expected that the coefficients becomes smaller when alpha becomes larger. However, the plot from Python 3.5 shows the coefficient paths which seems random.
Versions
Python 3.5
Darwin-16.6.0-x86_64-i386-64bit
Python 3.5.3 |Anaconda custom (x86_64)| (default, Mar 6 2017, 12:15:08)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
NumPy 1.12.1
SciPy 0.19.0
Scikit-Learn 0.18.2
Python 3.6
Darwin-16.6.0-x86_64-i386-64bit
Python 3.6.1 |Anaconda custom (x86_64)| (default, May 11 2017, 13:04:09)
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
NumPy 1.12.1
SciPy 0.19.0
Scikit-Learn 0.18.2