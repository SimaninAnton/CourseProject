RDMelamed commented on Aug 27, 2018 â€¢
edited
Description
I am doing a logistic regression with imbalanced classes (85% of examples are positive). When a feature is included that is all "1" (the first column of X below), so it has no predictive value of the label, it sometimes gets a strong positive coefficient. I assume this has to do with this feature replacing the intercept somehow (I have not changed the intercept settings). Bizarrely, I find this DOES happen with LogisticRegression, but not with SGDClassifier, but it happens with SGDClassifier if I convert the input samples to a sparse.csr_matrix. I'm not totally sure if this is a bug or if there is some subtle usage of sklearn that I'm getting wrong.
Steps/Code to Reproduce
Example:
from sklearn.linear_model import SGDClassifier
from scipy import sparse
import numpy as np
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(penalty="l2",C=1)
sgdc = SGDClassifier(loss="log", penalty="elasticnet",alpha=.001,l1_ratio=.3, max_iter=3)

observations = 380000
ones = 324000
labels = np.hstack((np.ones(ones), np.zeros(observations - ones)))
## 4 columns -- first column is all 1's others are random floats
X = np.hstack((np.ones((observations,1)),   
               np.random.rand(observations,3))) 

lr.fit(X, labels)
lr.coef_ ## array([[ 0.87819687, -0.00415632,  0.01106184, -0.00892921]])
lr.intercept_ ## array([ 0.88786446])

sgdc.fit(sparse.csr_matrix(X), labels)
sgdc.coef_  ## array([[ 1.55860232,  0.02463718,  0.03860308,  0.        ]])
sgdc.intercept_   ## array([ 0.14245333])


sgdc.fit(X, labels)
sgdc.coef_   ## array([[ 0.,  0.,  0.,  0.]])
sgdc.intercept_   ## array([ 1.75078929])
Versions
Linux-3.10.0-693.11.6.el7.x86_64-x86_64-with-redhat-7.4-Nitrogen
Python 3.6.2 |Anaconda custom (64-bit)| (default, Jul 20 2017, 13:51:32)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
NumPy 1.13.3
SciPy 1.0.0
Scikit-Learn 0.19.1