daniel-vainsencher commented on Jul 23, 2013
See the demo at [1]: while the size of the data points (representing weight assigned) are much bigger for one plot than the other, the boundary does not seem to prefer one over the other. To really see this conclusively, add a colorbar to the demo (should really be there anyway). Or run the script below: multiplying the points from the "preferred" class by another factor of 2 does not change the decision_function at all.
Note that from the existance of [2], I would guess that sample weighting in libsvm does not seem to be a completely stable feature. Maybe sklearn should be including that version of weighting? In any case, sklearn should clarify how instance weights are treated. Should they sum to 1 (be a distribution)? should they sum to C? is C then ignored? I haven't managed to get clear answers to these questions from the libsvm documentation either, btw.
Daniel Vainsencher
[1] http://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html
[2] http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances
Script that demonstrates problem:
print doc
import numpy as np
import pylab as pl
from sklearn import svm
we create 20 points
np.random.seed(0)
X = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]
Y = [1] * 10 + [-1] * 10
sample_weight = 100 * np.abs(np.random.randn(20))
sample_weight[:10] *= 10.
sample_weight2 = sample_weight
sample_weight2[:10] *= 2.
fit the weighted models
clf = svm.SVC()
clf.fit(X, Y, sample_weight=sample_weight)
clf2 = svm.SVC()
clf2.fit(X, Y, sample_weight=sample_weight2)
print "the following vectors should be significantly different:"
print clf.decision_function(X)
print clf2.decision_function(X)