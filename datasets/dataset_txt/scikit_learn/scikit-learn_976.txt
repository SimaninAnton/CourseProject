cdluminate commented on Oct 24, 2018
____________ test_logistic_regression_multi_class_auto[lbfgs-est1] _____________

est = LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,
           fi...=0,
           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)
solver = 'lbfgs'

    @pytest.mark.parametrize('est', [LogisticRegression(random_state=0),
                                     LogisticRegressionCV(random_state=0, cv=3),
                                     ])
    @pytest.mark.parametrize('solver', ['liblinear', 'lbfgs', 'newton-cg', 'sag',
                                        'saga'])
    def test_logistic_regression_multi_class_auto(est, solver):
        # check multi_class='auto' => multi_class='ovr' iff binary y or liblinear
    
        def fit(X, y, **kw):
            return clone(est).set_params(**kw).fit(X, y)
    
        X = iris.data[::10]
        X2 = iris.data[1::10]
        y_multi = iris.target[::10]
        y_bin = y_multi == 0
        est_auto_bin = fit(X, y_bin, multi_class='auto', solver=solver)
        est_ovr_bin = fit(X, y_bin, multi_class='ovr', solver=solver)
        assert np.allclose(est_auto_bin.coef_, est_ovr_bin.coef_)
        assert np.allclose(est_auto_bin.predict_proba(X2),
                           est_ovr_bin.predict_proba(X2))
    
        est_auto_multi = fit(X, y_multi, multi_class='auto', solver=solver)
        if solver == 'liblinear':
            est_ovr_multi = fit(X, y_multi, multi_class='ovr', solver=solver)
            assert np.allclose(est_auto_multi.coef_, est_ovr_multi.coef_)
            assert np.allclose(est_auto_multi.predict_proba(X2),
                               est_ovr_multi.predict_proba(X2))
        else:
            est_multi_multi = fit(X, y_multi, multi_class='multinomial',
                                  solver=solver)
            if sys.platform == 'darwin' and solver == 'lbfgs':
                pytest.xfail('Issue #11924: LogisticRegressionCV(solver="lbfgs", '
                             'multi_class="multinomial") is nondterministic on '
                             'MacOS.')  # pragma: no cover
>           assert np.allclose(est_auto_multi.coef_, est_multi_multi.coef_)
E           AssertionError: assert False
E            +  where False = <function allclose at 0x3fff90b8a050>(array([[-1.9311303 ,  2.82716291, -9.14686378, -4.03266391],\n       [-2.863746...15, -7.47563299],\n       [ 4.79487692,  0.07942529, 10.62611993, 11.5082969 ]]), array([[-1.80929916,  2.83798098, -9.17700613, -4.04065419],\n       [-2.952913...61, -7.52719256],\n       [ 4.76221304,  0.11442281, 10.58513974, 11.56784676]]))
E            +    where <function allclose at 0x3fff90b8a050> = np.allclose
E            +    and   array([[-1.9311303 ,  2.82716291, -9.14686378, -4.03266391],\n       [-2.863746...15, -7.47563299],\n       [ 4.79487692,  0.07942529, 10.62611993, 11.5082969 ]]) = LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,\n           fi...=0,\n           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0).coef_
E            +    and   array([[-1.80929916,  2.83798098, -9.17700613, -4.04065419],\n       [-2.952913...61, -7.52719256],\n       [ 4.76221304,  0.11442281, 10.58513974, 11.56784676]]) = LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,\n           fi...=0, refit=True, scoring=None, solver='lbfgs',\n           tol=0.0001, verbose=0).coef_

sklearn/linear_model/tests/test_logistic.py:1436: AssertionError
 test_non_meta_estimators[LocallyLinearEmbedding-LocallyLinearEmbedding-check_pipeline_consistency] 

name = 'LocallyLinearEmbedding'
Estimator = <class 'sklearn.manifold.locally_linear.LocallyLinearEmbedding'>
check = <function check_pipeline_consistency at 0x3fff89bda938>

    @pytest.mark.parametrize(
            "name, Estimator, check",
            _generate_checks_per_estimator(_yield_all_checks,
                                           _tested_non_meta_estimators()),
            ids=_rename_partial
    )
    def test_non_meta_estimators(name, Estimator, check):
        # Common tests for non-meta estimators
        with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,
                                       UserWarning, FutureWarning)):
            estimator = Estimator()
            set_checking_parameters(estimator)
>           check(name, estimator)

sklearn/tests/test_common.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
sklearn/utils/testing.py:350: in wrapper
    return fn(*args, **kwargs)
sklearn/utils/estimator_checks.py:1047: in check_pipeline_consistency
    assert_allclose_dense_sparse(result, result_pipe)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[-4.09281340e-02,  2.82586226e-01],
       [-2.54934412e-01,  1.5124485...[-4.09281341e-02, -1.78058052e-01],
       [-2.54934412e-01,  2.87156151e-11]])
y = array([[ 2.34354668e-01,  2.82586226e-01],
       [ 1.08372304e-01,  1.0799944...[ 2.34354668e-01, -1.78058052e-01],
       [ 1.08372304e-01,  1.47967914e-11]])
rtol = 1e-07, atol = 1e-09, err_msg = ''

    def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=''):
        """Assert allclose for sparse and dense data.
    
        Both x and y need to be either sparse or dense, they
        can't be mixed.
    
        Parameters
        ----------
        x : array-like or sparse matrix
            First array to compare.
    
        y : array-like or sparse matrix
            Second array to compare.
    
        rtol : float, optional
            relative tolerance; see numpy.allclose
    
        atol : float, optional
            absolute tolerance; see numpy.allclose. Note that the default here is
            more tolerant than the default for numpy.testing.assert_allclose, where
            atol=0.
    
        err_msg : string, default=''
            Error message to raise.
        """
        if sp.sparse.issparse(x) and sp.sparse.issparse(y):
            x = x.tocsr()
            y = y.tocsr()
            x.sum_duplicates()
            y.sum_duplicates()
            assert_array_equal(x.indices, y.indices, err_msg=err_msg)
            assert_array_equal(x.indptr, y.indptr, err_msg=err_msg)
            assert_allclose(x.data, y.data, rtol=rtol, atol=atol, err_msg=err_msg)
        elif not sp.sparse.issparse(x) and not sp.sparse.issparse(y):
            # both dense
>           assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)
E           AssertionError: 
E           Not equal to tolerance rtol=1e-07, atol=1e-09
E           
E           (mismatch 50.0%)
E            x: array([[-4.092813e-02,  2.825862e-01],
E                  [-2.549344e-01,  1.512448e-11],
E                  [-2.549344e-01,  2.129832e-11],...
E            y: array([[ 2.343547e-01,  2.825862e-01],
E                  [ 1.083723e-01,  1.079994e-11],
E                  [ 1.083723e-01,  1.725964e-11],...

sklearn/utils/testing.py:464: AssertionError