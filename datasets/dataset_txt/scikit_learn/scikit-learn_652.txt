morawi commented on Mar 8, 2019 â€¢
edited
If my code is correct, accuracy_score is probably giving incorrect results in the multilabel case with binary label indicators. Without further ado, I've made a simple reproducible code, here it is, copy, paste, then run it:
"""
Created on Wed Feb 27 18:36:33 2019

@author: malrawi
"""

import numpy as np
from sklearn.metrics.pairwise import euclidean_distances as dist
from scipy.spatial import distance 
from collections import Counter
from sklearn.metrics import accuracy_score


import platform; print(platform.platform())
import sys; print("Python", sys.version)
import numpy; print("NumPy", numpy.__version__)
import scipy; print("SciPy", scipy.__version__)
import sklearn; print("Scikit-Learn", sklearn.__version__)


pred_vector = np.array([
              [1,0,1], 
              [1,1,0],
              [1,0,0],
              [0,0,1],
              [1,1,1], 
              [1,1,1],
              [1,1,1]
                   ])
gt_vector = np.array([
              [1,0,1],  #gt = ground_truth
              [0,0,1],
              [1,0,0],
              [1,0,1],
              [1,1,1], 
              [1,1,1],
              [1,1,1] ])

gt_labels = ["A", "B", "C", "A", "A", "A", "A"  ]

# dist_mat = distance.cdist(gt_vector, pred_vector)
dist_mat = dist(gt_vector, pred_vector)
dist_indices = np.argmin(dist_mat, axis=1)

cnt = Counter()
predicted_labels = []       

j = 0; 
for i in dist_indices:  
    predicted_labels.append(gt_labels[i])    
    cnt[gt_labels[i]] += 1*(predicted_labels[j] == gt_labels[j])    
    j += 1
# print(dist_mat)
# print(dist_indices)
print('correctly identified labels:', cnt)
print('but, accuracy_score gives', accuracy_score(gt_vector, pred_vector, normalize=True))
print('the number correct labels should be', sum( cnt.values()), ', and thus, the accuracy should be', sum( cnt.values())/len(gt_labels))
print('obviously, accuracy_score is counting the number of non-zero elements in the min distance value, that is', sum(dist_indices!=0)/len(gt_labels))
Here's the output that demonstrates the situation:
Linux-4.4.0-142-generic-x86_64-with-debian-stretch-sid
Python 3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) 
[GCC 7.3.0]
NumPy 1.14.3
SciPy 1.0.0
Scikit-Learn 0.19.1

correctly identified labels: Counter({'A': 5, 'C': 1})
but, accuracy_score gives 0.7142857142857143
the number correct labels should be 6 , and thus, the accuracy should be 0.8571428571428571
obviously, accuracy_score is counting the number of non-zero elements in the min distance value, that is 0.7142857142857143