bclayman commented on Aug 23, 2018 â€¢
edited
Description
RandomForestClassifier Takes Up Enormous Amount of Space; How Can I Minimize it Solely for Prediction Purposes?
For reference, this is a multi-classification model with 70 classes.
Steps/Code to Reproduce
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=12, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=10,
            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
rf_model.fit(X, y)
pickle.dump(rf_model, open( "rf_model.pkl", "wb"))
This file is roughly 4GB. Some back of the envelope calculations:
Assuming each tree is full, there are (2^13) - 1 = 8,191 nodes per tree. We have 1,000 estimators, so that's 8,191,000 nodes. 4GB = 4 * 10^9. So bytes per node = 4 * 10^9 / 8,191,000 = ~488 bytes per node.
I'm only using this model for classification of new samples. So, in theory, I should only need to know which feature a particular node is splitting on and what the threshold is for going left vs. right. Is there a way I can strip out all information contained in the RandomForestClassifier to minimize its size and still predict on new samples?
As it is, over 400 bytes per node seems like an absolute ton. I appreciate there's additional overhead besides the nodes that's still necessary to predict on new samples, but hopefully a lot isn't needed too.
Versions
Python==2.7.14
numpy==1.14.3
scipy==1.1.0
Scikit-learn==0.19.2