Member
mblondel commented on Jul 11, 2014
It would be nice to have the warm_start option, as in gradient boosting.
clf = None
scores = []
for n_estimators in (5, 10, 15):
    if clf is None:
        clf = RandomForestClassifier(n_estimators=n_estimators, warm_start=True)
    else:
        clf.n_estimators = n_estimators
    clf.fit(X, y)
    scores.append(clf.score(X, y))
This will of course require to store the random state.