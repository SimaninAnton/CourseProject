Member
ogrisel commented on Jun 28, 2013
Charset decoding is often not understood well by users so we need to improve the documentation to help them find out which charset to use for decoding their input.
explain that you need to find out which charset is your text data encoded with and that ignoring errors will not solve the issue as data will be thrown away arbitrarily potentially harming the performance of the machine learning algorithms downstream
explain that nowadays many software use utf-8 as the default encoding so the user should check that decoding print(open('file.txt', 'rb').decode('utf-8'))
for people with mixed input charsets, demo a sample script that uses chardet can help find the most likely charset based on a heuristic detection (that can fail)
also when the input data is not consistently encoded, explain the benefit of using a single byte encoding (for instance 'latin-1') vs variable byte encoding such as utf-8: the decoded text will look wrong to humans reading the feature names but a text classification algorithm does not really care.