altieresdelsent commented on Jan 23, 2019
I am trying to use LinearSVM and SDGClassifier and all of them requires a dense matrix, my input are n-grams from small text with many classes ( more than (1 thousand), my data has 18 millions features ( n-grams) each sample has on average 13 features, my X is very, very sparse. when I use SDGClassifier to fit it uses 25 GB of RAM, when I call sparsify it goes to 50mb, when I try LinearSVM it uses more than 128 GB and it crashes, I have debugged and saved the X and y, and only loaded them using sparse matrix and I am sure the memory only goes up when training, I am pretty sure its because both use dense coefficients matrixes.
A lot of classes support sparse matrix as inputs, on the coef_ after training, and on the X when predicting, but when fitting the coef_ has to be dense, you can read the following in a lot of sparsify methods: "After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify."
I wonder why is that, is that because most problems will converge fast using dense coef_ and the dense will make less data copys? Or is there a more fundamental problem that requires coef_ to be always dense when fitting?
I am also wondering if that is a problem that only happens with LinearSVM and SDGClassifier or is it common in a lot of other classifiers. If its just a matter of implementing, I or anyone interested could implement on the fundamental core functions, and we could just put a parameter on all those models that support the method sparsify saying " If parameter blablabla = True, then the fitting will be slower for normal operations but for very sparse matrices will use substantially less memory"
If the problem is not fundamentally mathematical, can someone point me out a decent documentation in how to calculate the coef_ for SVM? , I would be interested in code a function or class that accepts coef_ sparse when training, I just don`t want to try to do something that would be fundamentally impossible.