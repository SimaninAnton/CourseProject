tkrabel commented on Oct 28, 2018
In my master's thesis, I was experimenting with a new feature in Friedman's Multiple Additive Regression Trees algorithm, namely randomizing base learners' depths. I figured out that this is actually a quite nice add-on and I called the resulting algorithm Random Boost (just so that it has a neat name).
My master's thesis is not publicly accessible since I was mainly working on a real business case, but parts of my thesis contain a theoretical explanation of Random Boost and a simulation study, so I pasted them together and made them accessible for you below for more details. I kept parts of the Intro and History section in this paper, but they may read a bit clumsy now, as I had to delete references to my business case. The relevant sections are: Random Boosting and Simulation Study.
As you can see in my simulation study, Random Boost is up to 30% faster than the normal MART algorithm while having comparable or sometimes even better prediction performance. The randomization can be regarded as a regularization method.
At the time of writing the thesis, I was implementing Random Boost in R, but I have now shifted to Python, as I think it is much easier to embed that feature into the existing sklearn framework.
On my GitHub site, I have already implemented a version of it using sklearn, where Random Boost is basically realized through an additional argument to sklearn.ensemble.GradientBoostingRegressor/Classifier (and of course also to their super-class sklearn.ensemble.BaseGradientBoosting). I added a boolean argument random_depth which turns on "random mode". With random_depth = True, max_depth is used as an upper bound of a discrete uniform distribution with lower bound 1. At each iteration of Gradient Boosting, a random number between 1 and max_depth is consequently drawn which marks the maximum depth of the respective tree being fit. I also created two wrapper classes (which are not necessary, but I like the name of the algorithm :)).
Based on my study and my experience with the feature, I suggest incorporating Random Boost into the sklearn package via the suggested random_depth argument. What do you think about it? I already forked the scikit-learn repo and implemented the add-on (not pushed yet). Do you think I shall simply start a pull request? What's the process for contributing something totally new to the package? I read the contribution guide, yet I am not sure whether it applies to new features. I thought, a new feature may have to be discussed first somehow (newbie alert!).
I would appreciate any remarks on the issue and wanted to say a big thanks to the sklearn contributors/maintainers for that great package.
Cheers,
Tobias
krabel_2018_random_boost.pdf