msaber commented on Apr 17, 2018 â€¢
edited
Hi,
From the equation written in the documentation, the length of grid_scores_ is equal to ceil((n_features - 1) / step) + 1, where step is the number of features removed at each iteration.
When n_features and step are not divisible the size makes sense. For example:
n_features = 5, step = 2 then len(grid_scores_) = 3
and grid_scores_ elements correspond to when 1,3,5 features are left respectively.
When n_features and step are divisible the size does makes sense. For example:
n_features = 6, step = 2 then len(grid_scores_) = 6
and grid_scores_ elements correspond to when 0,2,4,6 features are left respectively. However, it does not make sense to evaluate the score when 0 features are left.
I think the correct size would be ceil(n_features / step) instead. Is there any reason behind this decision, or I cannot understand it correctly?
Here is more info about my platform
Linux-4.10.0-37-generic-x86_64-with-debian-stretch-sid
('Python', '2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]')
('NumPy', '1.13.1')
('SciPy', '0.19.1')
('Scikit-Learn', '0.19.0')