Member
jnothman commented on Feb 27, 2018
The scoring parameter allows users to specify a scoring method by name. Currently a list of names is available by getting it wrong:
>>> from sklearn.metrics import get_scorer
>>> get_scorer('rubbish')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/joel/repos/scikit-learn/sklearn/metrics/scorer.py", line 239, in get_scorer
    % (scoring, sorted(scorers)))
ValueError: 'rubbish' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']
I think this error message, and maintaining it, is getting a bit absurd. Instead we should have a function sklearn.metrics.list_scorers implemented in sklearn/metrics/scorer.py and the error message should say "Use sklearn.metrics.list_scorers to get valid strings.". Perhaps we would eventually have list_scorers allow users to filter scorers by task type (binary classification, multiclass classification, multilabel classification, regression, etc.), or even to provide metadata about each scorer (a description, for instance), but initially we should just be able to list them.
üëç 2