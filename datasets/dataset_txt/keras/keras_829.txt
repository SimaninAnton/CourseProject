li2109 commented on 8 Aug 2018
For SGD optimizer, get_updates(self, params, loss) are implemented as such way:
w = current weights of your model
assigning a learning rate lr to some number
g = taking gradients of your weights w with respect to loss.
and do update operation like w - lr*g
But, what if i want different gradients?
say, i defined a function z = x/y, how do i get gradients of z with respect to the same loss function as previous? and doing then doing the update