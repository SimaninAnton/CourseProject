OverLordGoldDragon commented on 12 May 2019 â€¢
edited
Training a GRU autoencoder on EEG data, I discover the model predicts lot more poorly than train loss (mse) suggests; evaluate, test_on_batch, predict with hand-coded loss, and fit all agree the loss to be x10 the train values; some results below.
Exploring various predictions, it often seems that the network performs well except for a bias (offset). Aside this, I'm without a clue to the cause underlying the discrepancy; disabling dropout & batch norm doesn't help.
Any clues? Help is appreciated.
Additional details:
CuDNNGRU stateful implementation, TensorFlow backend
Layers: (300,80,150) -- (encoder, latent, decoder) -- (selu, tanh, tanh)
Output: TimeDistributed(Dense(units=input_dim, activation='linear'))
return_sequences=True for all layers
GaussianNoise + Dropout at (=after) input, AlphaDropout at encoder, Dropout at latent
BatchNormalization between encoder and latent, latent and decoder
batch_size=25, timesteps=400, input_dim=16 - 25 separate, 10-min sequences fed 400 timesteps
(=1 sec) at a time (as 10*60=600 'windows' in parallel, non-shuffled)
reset_states() applied before testing on new x25 10-min sequences
train_on_batch for training, but results don't differ from fit
Keras 2.2.4, Python 3.6, Spyder 3.3.4 via Anaconda
Results
model.fit(x,x,validation_data=(x,x))

25/25 [==============================] - 1s 42ms/step - loss: 0.1780 - val_loss: 2.9175
25/25 [==============================] - 1s 39ms/step - loss: 0.1794 - val_loss: 2.9380