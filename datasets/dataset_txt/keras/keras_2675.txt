Huxwell commented on 12 Apr 2017 â€¢
edited
I am using keras implementation of wrn network from here: https://github.com/asmith26/wide_resnets_keras but as far as i know, the problem appears for every transfer learning attempt.
model = wrn.create_wide_residual_network(init_shape, nb_classes=10, N=4, k=8, dropout=0.0)
model.compile(loss='categorical_crossentropy', optimizer="adadelta", metrics=['accuracy'])
model.summary()
model.layers.pop()
last = model.output
preds = Dense(nb_classes=47, activation='softmax', name="some_generic_name")(last)
model = Model(model.input, preds)
model.compile(loss='categorical_crossentropy', optimizer="adadelta", metrics=['accuracy'])
model.summary()
So, the end of first summary is:
flatten_1 (Flatten)              (None, 512)           0                                            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 10)            5130                                         
====================================================================================================
Than after removing classification head everything is OK:
flatten_1 (Flatten)              (None, 512)           0                                            
====================================================================================================
but after calling it once the new classification head is added, the old one reappears.
flatten_1 (Flatten)              (None, 512)           0                                            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 10)            5130                                         
____________________________________________________________________________________________________
some_generic_name (Dense)        (None, 47)            517                                          
====================================================================================================
The #2371 custom layer.pop doesn't solve this issue.
1