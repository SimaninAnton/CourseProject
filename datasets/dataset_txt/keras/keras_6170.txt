dpappas commented on 4 Jan 2016
Hello everyone and happy new year
I am trying to create an LSTM Autoencoder as shown on the image bellow.
The encoder consumes the input "the cat sat",
and creates a vector depicted as the big red arrow.
The decoder takes this vector and tries to reconstruct the sequence
given the position in the sentence.
I would like to save this vector (big red arrow) to use it on another model.
The code i have wrote so far is the following:
from keras.layers import containers
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, AutoEncoder
import numpy as np
from keras.layers.recurrent import LSTM

train_x = [
    [[ 1 ,3 ],[ 1 ,3 ]],
    [[ 2 ,4 ],[ 2 ,4 ]],
    [[ 3 ,5 ],[ 3 ,5 ]]
]
train_x = np.array(train_x)

encoder = containers.Sequential([ LSTM(output_dim=5, input_dim = 2, activation='tanh' , return_sequences=True) ])
decoder = containers.Sequential([ LSTM(output_dim=2, input_dim = 5, activation='tanh', return_sequences=True) ])
autoencoder = Sequential()
autoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False))
autoencoder.compile(loss='mean_squared_error', optimizer='sgd')
autoencoder.fit(train_x,train_x, nb_epoch=10)
It is not clear to me if the code above does what i ask for.
If i do not use return_sequences=True it yields an error.
Should i use a graph model to do exactly what i ask for?
Thank you in advance for your help.