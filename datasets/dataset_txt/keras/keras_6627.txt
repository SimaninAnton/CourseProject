aseveryn commented on 26 Sep 2015
I'm experimenting with LSTMs for sequence labeling where I have several inputs: words, capitalization features, etc. Hence, I'm using a Graph() model. However, unlike for a Sequence() model it's not possible to supply 'sample_weight' param to the .fit() function. This is a big killer, since I need to be able to mask the loss related to part of the input (I'm feeding the input first in the reversed order and then in the original order -- this is important to get the hidden layer in a good state before making predictions, but the predictions for the first part of the input should be discarded). Hence, supplying masked_zero=True to the Embedding layer doesn't work here. This can be only done with a sample_weight inputs. I was wondering if there is a solution to this.
Thanks!