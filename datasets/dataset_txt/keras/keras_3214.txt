kvrd18 commented on 14 Feb 2017 â€¢
edited
Hi,
I've been following this Keras tutorial for training a model on a Multi-GPU setting. This works fine and the ops are executed on the GPU as long as the models are defined using a sequential container. But when I define the same model using graphical operations as shown here in this Gist, the ops are being executed on the CPU.
To run the Gist and reproduce the bug, download the script and execute python kerasSetGPU.py on the terminal.
When I ran some benchmarks for a fully convolutional network with an input batch size of 4x512x512x1 with around 12 convolutional layers, I found that the sequential model took around 0.7s per batch on a TITANx card and the graphical model took around 2.2s per batch.
What's going wrong here? Why are the ops defined by the graphical model being executed on the CPU even if I call it under with tf.device('/gpu:0'):?