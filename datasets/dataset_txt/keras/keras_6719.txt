ghost commented on 25 Aug 2015
Hello All,
I wrote a new layer to do convolution calculation for re-formed input of the previous layer.
My related code is:
 def get_output(self, train):
        nb_col = 6
        aaQua=[57.0519,71.0788,87.0782,97.1167,99.1326,101.1051,103.1388,113.1594,113.1594,114.1038,115.0886,128.1307,128.1741,129.1155,131.1926,137.1411,147.1766,156.1875,163.176,186.2132]
        aaQuaInt = np.zeros(20)
        for i in range(20):
           aaQuaInt[i]=int(aaQua[i]*10)
                                   #               50, 4, 1, 995
        X = self.get_input(train)  # 4D tensor: nb_samples, feature_map, 1, nb_col
        layer20aaLen=X.shape[3]-nb_col-aaQuaInt[0]
        border_mode = self.border_mode
        X3 = np.zeros(X.shape[0], X.shape[1], 2, X.shape[3],dtype=theano.config.floatX)
        conv_out = np.zeros((X.shape[0], 20, X.shape[2],layer20aaLen),dtype=theano.config.floatX)
        for i in range(20):
            length1=int(X.shape[3]-nb_col-aaQuaInt[i])
            length2=int(nb_col+aaQuaInt[i])
            X1= X[:,:,0,0:int(length1)]
            X2= X[:,:,0,length2:]
            for inum in range(X.shape[0]):
                for jnum in range(X.shape[1]):
                    for knum in range(length1):
                        X3[inum,jnum,0,knum]=X1[inum,jnum,0,knum]
                        X3[inum,jnum,1,knum]=X2[inum,jnum,0,knum]
          # the above is only re-format the input of layer2
          # the output is each filter's
            current_conv_out= theano.tensor.nnet.conv.conv2d(X3, self.W[i,:,:,:],border_mode=border_mode, subsample=self.subsample)

            for ii in range(X.shape[0]):  # X.shape[0] is the nb_samples
                for jj in range(current_conv_out.shape[3]):
                    conv_out[ii,i,0,jj]=current_conv_out[ii,0,0,jj]


        return self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))
The error information is:
*** ValueError: Output dimensions are not valid 0x990
Apply node that caused the error: ConvOp{('imshp', (None, None, None)),('kshp', (None, None)),('nkern', None),('bsize', None),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', None),('unroll_kern', None),('unroll_patch', True),('imshp_logical', (None, None, None)),('kshp_logical', (None, None)),('kshp_logical_top_aligned', True)}(Elemwise{mul,no_inplace}.0, <TensorType(float64, 4D)>)
Inputs types: [TensorType(float64, 4D), TensorType(float64, 4D)]
Inputs shapes: [(50L, 4L, 1L, 995L), (20L, 4L, 2L, 6L)]
Inputs strides: [(31840L, 7960L, 7960L, 8L), (384L, 96L, 48L, 8L)]
Inputs values: ['not shown', 'not shown']
I just re-format the input of the previous layer and do convolution operation for the re-formated input. I double-checked my dimensions. I cannot fix the error.
Any one has any ideas?
My complete code is as follows:
class TwentyConvs(Layer):

    def __init__(self, nb_filter, stack_size, nb_row, nb_col,
        init='glorot_uniform', activation='linear', weights=None,
        border_mode='valid', subsample=(1, 1),
        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):

        if border_mode not in {'valid', 'full', 'same'}:
            raise Exception('Invalid border mode for Convolution2D:', border_mode)

        super(TwentyConvs, self).__init__()
        self.init = initializations.get(init)
        self.activation = activations.get(activation)
        self.subsample = subsample
        self.border_mode = border_mode
        self.nb_filter = nb_filter
        self.stack_size = stack_size

        self.nb_row = nb_row
        self.nb_col = nb_col

        self.input = T.tensor4()
        self.W_shape = (nb_filter, stack_size, nb_row, nb_col)
        self.W = self.init(self.W_shape)
        self.b = shared_zeros((nb_filter,))

        self.params = [self.W, self.b]

        self.regularizers = []

        self.W_regularizer = regularizers.get(W_regularizer)
        if self.W_regularizer:
            self.W_regularizer.set_param(self.W)
            self.regularizers.append(self.W_regularizer)

        self.b_regularizer = regularizers.get(b_regularizer)
        if self.b_regularizer:
            self.b_regularizer.set_param(self.b)
            self.regularizers.append(self.b_regularizer)

        self.activity_regularizer = regularizers.get(activity_regularizer)
        if self.activity_regularizer:
            self.activity_regularizer.set_layer(self)
            self.regularizers.append(self.activity_regularizer)

        self.W_constraint = constraints.get(W_constraint)
        self.b_constraint = constraints.get(b_constraint)
        self.constraints = [self.W_constraint, self.b_constraint]

        if weights is not None:
            self.set_weights(weights)

    def get_output(self, train):
        nb_col = 6
        aaQua=[57.0519,71.0788,87.0782,97.1167,99.1326,101.1051,103.1388,113.1594,113.1594,114.1038,115.0886,128.1307,128.1741,129.1155,131.1926,137.1411,147.1766,156.1875,163.176,186.2132]
        aaQuaInt = np.zeros(20)
        for i in range(20):
           aaQuaInt[i]=int(aaQua[i]*10)
                                   #               50, 4, 1, 995
        X = self.get_input(train)  # 4D tensor: nb_samples, feature_map, 1, nb_col
        layer20aaLen=X.shape[3]-nb_col-aaQuaInt[0]
        border_mode = self.border_mode
        X3 = np.zeros(X.shape[0], X.shape[1], 2, X.shape[3],dtype=theano.config.floatX)
        conv_out = np.zeros((X.shape[0], 20, X.shape[2],layer20aaLen),dtype=theano.config.floatX)
        for i in range(20):
            length1=int(X.shape[3]-nb_col-aaQuaInt[i])
            length2=int(nb_col+aaQuaInt[i])
            X1= X[:,:,0,0:int(length1)]
            X2= X[:,:,0,length2:]
            for inum in range(X.shape[0]):
                for jnum in range(X.shape[1]):
                    for knum in range(length1):
                        X3[inum,jnum,0,knum]=X1[inum,jnum,0,knum]
                        X3[inum,jnum,1,knum]=X2[inum,jnum,0,knum]
            print ("XXXXXXX3Shape2",X3.shape[2])
            print ("XXXXXXX3NDIM",X3.ndim)
          # the above is only re-format the input of layer2
          # the output is each filter's
            current_conv_out= theano.tensor.nnet.conv.conv2d(X3, self.W[i,:,:,:],border_mode=border_mode, subsample=self.subsample)

            for ii in range(X.shape[0]):  # X.shape[0] is the nb_samples
                for jj in range(current_conv_out.shape[3]):
                    conv_out[ii,i,0,jj]=current_conv_out[ii,0,0,jj]


        return self.activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))

    def get_config(self):
        return {"name": self.__class__.__name__,
                "nb_filter": self.nb_filter,
                "stack_size": self.stack_size,
                "nb_row": self.nb_row,
                "nb_col": self.nb_col,
                "init": self.init.__name__,
                "activation": self.activation.__name__,
                "border_mode": self.border_mode,
                "subsample": self.subsample,
                "W_regularizer": self.W_regularizer.get_config() if self.W_regularizer else None,
                "b_regularizer": self.b_regularizer.get_config() if self.b_regularizer else None,
                "activity_regularizer": self.activity_regularizer.get_config() if self.activity_regularizer else None,
                "W_constraint": self.W_constraint.get_config() if self.W_constraint else None,
                "b_constraint": self.b_constraint.get_config() if self.b_constraint else None}