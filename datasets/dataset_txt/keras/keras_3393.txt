tobyfrancis commented on 24 Jan 2017
For research purposes, the ability to define custom functions on the gradient as a part of the optimizer would be very useful. It would be in line with the implementations of clipnorm/clipvalue. This would allow different functions other than simply clipping to be experimented with. An example of the utility of this functionality is demonstrated by the creation of the RevReLU layer in https://arxiv.org/pdf/1612.02766.pdf, which uses the ReLU function on the gradients to perform weakly supervised semantic segmentation.