pkch commented on 1 Dec 2015
Is there any reason not to provide to the callback functions the training loss (and accuracy if accuracy monitoring is enabled) at the end of each epoch? This is only provided in the log at the end of each batch.
This means (among other things) that EarlyStopping callback can't use 'loss' as the monitor; I understand why 'val_loss' is usually better, but there might be reasons to minimize the training loss.
Somewhat related to this, it may be worth documenting whether the 'loss' reported at the end of each batch is actually the loss calculated for the entire training dataset or just for that batch.
1