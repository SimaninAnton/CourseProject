dambuck commented on 16 Jan 2019
Hi,
I am trying to randomly change the sign of an element of a weight matrix in a specific (diagonal weight matrix) layer. I do this with the following constraint:
class DiagonalWeight(Constraint):
"""Constrains the weights to be diagonal.
"""

def __call__(self, w):
    N = K.int_shape(w)[-1]
    
    
    op=np.eye(N)
    pos=np.random.randint(3)
    op[pos,pos]*=-3.
    op= tf.convert_to_tensor(op,dtype=tf.float32)
    w*=op
    return w
I randomly multiplied by 3 to check if it is working or not and noticed that the entry to be changed seems to be fixed, because the matrix I printed out through a Lambdacallback at the end of every epoch shows only an increase of one entry throughout the training.
Why is that?
The constraint is called at every optimization step, so at the end of an batch. Is that correct?
Thanks