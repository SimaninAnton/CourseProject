Contributor
crowsonkb commented on 25 Aug 2016 â€¢
edited
According to the PReLU paper (https://arxiv.org/pdf/1502.01852v1.pdf section 2.1), "PReLU introduces a very small number of extra parameters. The number of extra parameters is equal to the total number of channels...". But in Keras PReLU (as well as the other parametric activations) is feature-wise, not channel-wise. This introduces a much larger number of additional parameters, when it should be adding half as many as an adjacent batch normalization layer.
Please make sure that the boxes below are checked before you submit your issue. Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
3