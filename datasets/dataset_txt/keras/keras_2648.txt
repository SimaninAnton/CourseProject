amaall commented on 15 Apr 2017 â€¢
edited
Hi, I have been struggling to address this problem, even though, similar problems were solved
. But I've been trying to handle this, with less success. I will appreciate any idea to my peculiar problem.Thank you.
x= Dense(n_neurons,input_dim=n_feat, W_regularizer=l2(0.001),activation='relu')(input1)
y = Dense(n_neurons,input_dim=n_feat,activation='relu')(input2)
#concat_x = K.concat([x,input2],1)# + 0.01* tf.nn.l2_loss(W0)
#x2 = Dense(n_neurons)(input2)
#my_lamb = Lambda(my_input,output_shape=my_output)(x2)
merg_x = merge([x, y],mode='concat',concat_axis=-1)
pred_out = Dense(1,activation='relu')(merg_x)
f_ =flip_gradient(merg_x, 1)
x = Dense(n_feat,activation='relu')(f_)
x = Dense(n_feat,activation='relu')(x)
dom_out = Dense(2,activation='softmax')(x)
print(K.shape(pred_out))
#mse2 = Dense(1,activation='relu')(mse)
#total_loss = add(mse , dom_out) # do i need to reduce or take it directly
#model= Model(inputs=[input1, input2], outputs=[mse,dom_out])

model = Model(input=[input1, input2], output=[pred_out, dom_out])
TypeError: Output tensors to a Model must be Keras tensors. Found: Tensor("Softmax:0", shape=(?, 2), dtype=float32)