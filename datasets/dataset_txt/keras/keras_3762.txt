twangnh commented on 5 Dec 2016 â€¢
edited
Hey! Guys, recently I'm working on a project for training LSTM with variable-length sequences, the length of the sequences is 4-12, I'm using one-hot representation, following the example here: https://github.com/erikbern/rnn-lang-model/blob/master/train_lstm.py , but the problem is that I have to train the network with variable-length sequences,
First, I tried to use making layer, but I don't know if my code is correct:(I pasted major part of the code)
training data and validation data: where mylist and mylist_test is list of sequences, chars is total vocabulary of the letters of sequences, this is to get one-hot representation of the training data and validation data
X = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)
y = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)

for i, sentence in enumerate(mylist):
    for t in range(len(sentence)-Data_end):
        X[i, t, char_indices[sentence[t]]] = 1
        y[i, t, char_indices[sentence[t+1]]] = 1
X_val = np.zeros((len(mylist_test), MAXLEN, len(chars)), dtype=np.bool)
y_val = np.zeros((len(mylist_test), MAXLEN, len(chars)), dtype=np.bool)

for i, sentence in enumerate(mylist_test):
    for t in range(len(sentence)-Data_end):
        X_val[i, t, char_indices[sentence[t]]] = 1
        y_val[i, t, char_indices[sentence[t+1]]] = 1
network:
model = Sequential()
model.add(Masking(mask_value=0., input_shape=(None, len(chars))))
model.add(LSTM(2000, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(2000, return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributed(Dense(len(chars))))
model.add(Activation('softmax'))
then is training:
model.fit(X, y, callbacks=[early_stopping],validation_data=(X_val, y_val),batch_size=32, nb_epoch=1)
second, I tried to train the network with one pair of data per-time, which is batch_size = 1 method and remove the masking layer, to test if my use of masking layer is correct
third, I tried to use the 'pad with neutral data ' method following https://github.com/fchollet/keras/issues/40, I paded the sequences with end symbol /n
I have not tried group method yet, the performance of my above try are(evaluated with validation loss):
batch_size =1 method >masking >padding with neutral data
my question is : am I right on using the masking method? since its performance is much lower than the batch_size =1 method, I think there is some problem with my using of masking layer, can anyone help me ? thanks in advance!