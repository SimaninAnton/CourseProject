mathDR commented on 14 Apr 2016
Hi. I extended the keras example file mnist_cnn.py to incorporate the incremental learning done in T. Xiao "Error-Driven Incremental Learning in Deep Convolutional Neural Network for Large-Scale Image Classification" : http://research.microsoft.com/apps/pubs/default.aspx?id=238746
The model is trained as follows: First, 3 classes are trained (pruning the image samples from mnist.load_data() accordingly), call this L0. Then, a new network with 10 nodes in the softmax layer is formed, call it L0'. The weights of L0' are initialized copying the weights from L0 over. The remaining 7 classes of the softmax layer are initialized randomly.
Then the L0' network is trained on the remaining samples (those excluded in the training/test of L0).
For both training sessions, high accuracies are achieved (99.7% and 99.3%, respectively). When putting all test samples through the trained L0' network though, the accuracy falls to 68.5%.
I looked at the weights in the softmax layer, and to my surprise, the weights connecting the original 3 nodes changed when training the L0'. I would think that these would be fixed (as there are no training classes corresponding to their output node).
Can anyone comment on this phenomenon? I attached the gist replicating the code and output accuracies (also, both models are saved and the respective weights can be polled to see they are indeed different). https://gist.github.com/mathDR/3a2a081e4f3089920fd8aecefecbe280
Thanks