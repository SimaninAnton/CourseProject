zhunzhong07 commented on 21 Mar 2017
When I using the ImageDataGenerator flow function to train the model (train_on_batch), the memory will leak. My keras version is 2.0.1, but the 1.2.0 version does not enconter this problem. Code:
def extract_imagelist(target_path):

     with open(target_path, 'r') as f:
            images, labels = [], []
            for l in f.readlines():
                l = l.strip('\n').split()
                images.append(l[0])
                labels.append(int(l[1]))
     image_len = len(images)
     all_images = np.empty([image_len, 224, 224, 3])
     all_labels = []
     for i in range(image_len):
           img = image.load_img(images[i], target_size=(224, 224))
           img = image.img_to_array(img)
           img = np.expand_dims(img, axis=0)
           img = preprocess_input(img)        
           lab = labels[i]        
           all_images[i, :, :, :] = img
           all_labels.append (lab)
     all_labels = to_categorical(all_labels)
     return all_images, all_labels

files_list = "train.txt"

X, Y = extract_imagelist(files_list)

datagen = ImageDataGenerator(horizontal_flip=True)

net.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy')


for e in range(10):  
      batches = 0
      for X_batch, Y_batch in datagen.flow(X, Y, batch_size=16):
            loss = net.train_on_batch(X_batch, Y_batch)
            if batches % 20 == 0:
         print ('Epoch', e, 'iteration:', batches, 'loss:', loss)
            batches += 1
            if batches >= len(X) / 16:
                break'