LCorleone commented on 31 Oct 2018
My custom layer is defined as this.
class Dense_with_AMsoftmax_loss(Layer):

    def __init__(self, output_dim, m, scale, **kwargs):
        self.output_dim = output_dim
        self.m = m
        self.scale = scale
        super(Dense_with_AMsoftmax_loss, self).__init__(**kwargs)

    def build(self, input_shape):
        self.kernel = self.add_weight(name='kernel',
                                      shape=(input_shape[1], self.output_dim),
                                      initializer='glorot_normal',
                                      trainable=True)

    def call(self, inputs):
        self.inputs = inputs
        # self.xw = K.dot(inputs, self.kernel)
        # self.w_norm = K.tf.norm(self.kernel, axis=0) + 1e-8
        # self.x_norm = K.tf.norm(inputs, axis=1) + 1e-8
        # self.x_norm = K.expand_dims(self.x_norm, 1)
        # self.logits = self.xw / self.w_norm / self.x_norm
        self.w_norm = K.tf.nn.l2_normalize(self.kernel, 0, 1e-10)
        self.x_norm = K.tf.nn.l2_normalize(self.inputs, 1, 1e-10)
        self.logits = K.tf.matmul(self.x_norm, self.w_norm)

        return self.logits

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)

    def loss(self, y_true, y_pred):
        y_true = K.expand_dims(y_true[:, 0], 1)
        y_true = K.cast(y_true, 'int32')
        batch_idxs = K.arange(0, K.shape(y_true)[0])
        batch_idxs = K.expand_dims(batch_idxs, 1)
        ordinal_y = K.concatenate([batch_idxs, y_true], 1)
        sel_logits = K.tf.gather_nd(self.logits, ordinal_y)
        comb_logits_diff = K.tf.add(self.logits, K.tf.scatter_nd(ordinal_y, sel_logits - self.m - sel_logits, K.tf.to_int32(K.tf.shape(self.logits))))
        return K.sparse_categorical_crossentropy(y_true, self.scale * comb_logits_diff, from_logits=True)

    def get_config(self):
        config = {'output_dim': self.output_dim,
                  'm': self.m,
                  'scale': self.scale}
        base_config = super(Dense_with_AMsoftmax_loss, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
when training
        AM_softmax_logits = Dense_with_AMsoftmax_loss(num_classes, m=0.2, scale=24)
        out = AM_softmax_logits(feature_embedding)
        model = Model(inputs=nn_inputs, outputs=out)
        model.compile(loss=AM_softmax_logits.loss, optimizer='Adam', metrics=['sparse_categorical_crossentropy'])
when I load_model
AM_softmax_logits = Dense_with_AMsoftmax_loss(10, m=0.2, scale=24)
model = load_model(model_path, custom_objects={'Dense_with_AMsoftmax_loss': AM_softmax_logits, 'loss': AM_softmax_logits.loss})
some errors occur
Traceback (most recent call last):
File "model_analysis.py", line 28, in
model = load_model(model_path, custom_objects={'Dense_with_AMsoftmax_loss': AM_softmax_logits, 'loss': AM_softmax_logits.loss})
File "/home/lxy/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py", line 290, in load_model
sample_weight_mode=sample_weight_mode)
File "/home/lxy/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 332, in compile
sample_weight, mask)
File "/home/lxy/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py", line 403, in weighted
score_array = fn(y_true, y_pred)
File "./utils/utils_func.py", line 253, in loss
sel_logits = K.tf.gather_nd(self.logits, ordinal_y)
AttributeError: 'Dense_with_AMsoftmax_loss' object has no attribute 'logits'
The error is clear, but i do not know how to fix it. it seems that keras can not define loss in custom layer with self.x? But i want to use the layer weights and i think my implementation is straightforward.
is there any ideas for fixing this?
thank you!