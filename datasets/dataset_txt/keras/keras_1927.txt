MehulAjax21 commented on 14 Jul 2017
I am using a GRU layer in my network. I have divided my data into (None,5625,9).
The batch size is 5, hence the shape of input is (5,5625,9). The GRU has 64 units in output. Now I wanted to know how is the GRU processing the whole data of (5,5625,9) and producing features of dimension (5,64).
Does it takes all the timesteps into account and then retain the final 64 values? Or is it something else.
It was not clear from Keras code how this is done. Also, I wanted to know where the BPTT is really occurring?
What I think is that it takes first 64 time steps, each of size 9 and generates features, which are used with next 64 and the loop continues till all the time steps are exhausted. Then the last 64 features for all the 5 samples are retained and passed further down the model. Once it reaches the loss, it is back propagated and when it reaches the GRU, gradients are updated for all 5625 timesteps and propagated back.
The essence of this question is to know whether all the timesteps for all the samples are used in one forward pass of network or not? Also if this is the case, how can is restrict my network to take only 100 timesteps or so, if I have 1M samples.