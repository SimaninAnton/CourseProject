ryan-lowe commented on 5 Apr 2017
I originally asked this on StackOverflow, but I think it's also relevant here, since it involves potentially incorrect documentation (vs. what is actually executed in the code) and a basic feature that seems difficult to implement.
I have two layers with output shapes (None, 2, 50, 5, 3) and (None, 2, 50, 3, 1), and I want to take the dot product of the '3' dimension, and have that broadcasted over the (None, 2, 50) dimensions -- i.e., I want an output of (None, 2, 50, 5, 1). My use case is very simple: I am computing a matrix (5, 3) and a vector (3, 1) at each timestep of a sequence, and I want to take their dot product at each timestep.
Doing this seems to work with K.batch_dot, but not with the Dot layer (even though this is implemented using K.batch_dot).
The documentation for the Dot layer states: "E.g. if applied to two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i]." This seems inconsistent with the behaviour I'm observing -- e.g. if I take Dot(2)([a, b]), with a, b = Input((10, 20)), I get a shape (None, 10, 10).
Here is an example showing what I am coming across:
import keras
import keras.backend as K
from keras.layers import Dot, Input

v1 = K.variable(value=np.random.rand(2, 50, 5, 3))
v2 = K.variable(value=np.random.rand(2, 50, 3, 1))
K.batch_dot(v1, v2)  # this works as desired, gives output shape: (2, 50, 5, 1)

x1 = Input((2, 50, 3, 5)) # shape: (None, 2, 50, 3, 5)
x2 = Input((2, 50, 3, 1)) # shape: (None, 2, 50, 3, 1)
Dot(3)([x1, x2]) # output shape is (None, 2, 50, 5, 2, 50, 1)
I've tried other things with no success, e.g. wrapping K.batch_dot in a Lambda layer (which can only take a single input - shouldn't there be an equivalent general-purpose layer taking multiple inputs?) or wrapping the Dot layer in a TimeDistributed Layer (which seems to not work since TimeDistributed can't handle a list as input).