ghost commented on 20 Jan 2017 â€¢
edited by ghost
I am trying to build a system that works similarly to the image captioning example in Keras. However, given a static image, I want to predict a sequence of images. My idea was to predict one image at a time based on my static input and the previous element of the sequence. Now I'm wondering if anyone can help me to define a network. The code is based on examples of image captioning and convolutional LSTMs but I wouldn't know how to merge the two input streams into a common one. Thanks in advance!
max_sequence = 10

inp1 = Input(shape=(128, 128, 1))
    
x = Convolution2D(16, 3, 3, border_mode='same', activation='relu')(inp1)
x = Convolution2D(32, 3, 3, border_mode='same', activation='relu')(x)
x = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(x)
   
x = Flatten()(x)
x = RepeatVector(max_sequence)(x)
out1 = Reshape((max_sequence, 128, 128, 64))(x)

inp2 = Input(shape=(max_sequence, 128, 128, 1))
    
out2 = ConvLSTM2D(nb_filter=64, nb_row=3, nb_col=3, border_mode='same',
               return_sequences=True)(inp2)
    
x = Merge([out1, out2], mode='concat') <-- ?
  
x = ConvLSTM2D(nb_filter=64, nb_row=3, nb_col=3, border_mode='same',
               return_sequences=False)(x)
  
x = Convolution3D(nb_filter=1, kernel_dim1=1, kernel_dim2=3, kernel_dim3=3,
                  activation='sigmoid', border_mode='same')(x)
    
model = Model(input=[inp1, inp2], output=x)