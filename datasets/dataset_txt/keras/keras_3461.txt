pchowdhry commented on 16 Jan 2017
Hi I'm trying to build a simple LSTM for a sentence similarity task. I'm using Glove word embeddings going into the LSTMs, and padding my sentences to 50. I then go into a merge layer on axis 1 using 'cos'. Whenever I fit and compile I get a nan for loss, immediately. I've tried to keep it simple for easy debugging. Any help would be appreciated. Model summary and code below (left out data preprocessing)
total training data = 1184
X1.shape = (1184, 50)
X2.shape = (1184, 50)
Y.shape = (1184, 1)
Layer (type) Output Shape Param # Connected to
embedding_1 (Embedding) (None, 50, 100) 310200 embedding_input_1[0][0]
lstm_1 (LSTM) (None, 128) 117248 embedding_1[0][0]
embedding_2 (Embedding) (None, 50, 100) 310200 embedding_input_2[0][0]
lstm_2 (LSTM) (None, 128) 117248 embedding_2[0][0]
activation_1 (Activation) (None, 1) 0 merge_1[0][0]
Total params: 854,896
Trainable params: 234,496
Non-trainable params: 620,400
embedding_layer_a = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=50,
                            trainable=False)

embedding_layer_b = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=50,
                            trainable=False)

s1rnn = Sequential()
s1rnn.add(embedding_layer_a)
s1rnn.add(LSTM(128, input_shape=(100, 1), dropout_W=0.2, dropout_U=0.2))


s2rnn = Sequential()
s2rnn.add(embedding_layer_b)
s2rnn.add(LSTM(128, input_shape=(100, 1), dropout_W=0.2, dropout_U=0.2))


model = Sequential()
merged = Merge([s1rnn, s2rnn], mode='cos', dot_axes=1)
model.add(merged)
model.add(Activation('relu'))

model.summary()

model.compile(optimizer='rmsprop', loss='cosine_proximity', metrics=['accuracy'])

model.fit([X1, X2], Y, batch_size=32, nb_epoch=500, validation_split=0.10)