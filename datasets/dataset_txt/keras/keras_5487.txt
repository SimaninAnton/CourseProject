Contributor
braingineer commented on 14 Apr 2016
I've run into some cranky issues regarding the re-use of an Embedding layer.
Specifically, I have two inputs---a word-tree with a parent and many children. I want to separately embed the parent apart from the children (I'm doing soft attention over the children).
However, reusing the same Embedding layer makes that difficult because it injects a None into the first dimension of _keras_shape (to make it (batch, None, embedding_size)).
In my local copy, I've dereferenced input_length everywhere and use input_shape[1] in get_output_shape_for. aka:
    def get_output_shape_for(self, input_shape):
        return (input_shape[0], input_shape[1], self.output_dim)
Is there a better way or should the input_shape be handled by Input instead?