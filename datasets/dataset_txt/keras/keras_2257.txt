mehrdadscomputer commented on 31 May 2017
As I see more LSTM implementation on keras, it seems that the length of input sequences should be equal. for example in addition_rnn, we use padding to make length of sequences equal.
But as you now LSTM is able to learn sequences with different lengths. So why we use padding?
I asked this question generally (not about keras) which people says there should not be such equalization.
https://stats.stackexchange.com/questions/282319/how-to-feed-time-series-with-different-length-into-lstm
I appreciate if anyone explain this issue.