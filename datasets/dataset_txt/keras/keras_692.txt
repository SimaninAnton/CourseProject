Wazaki-Ou commented on 18 Oct 2018 â€¢
edited
I have been struggling with this issue for a while and I'm starting to think that it might be more than just an implementation problem.
After fine tuning a Keras VGG 16 on my dataset and getting good accuracy on both Training and Validation sets, I try to evaluate it on my Testing set. However, the two methods I use give me very contradicting results. While Keras model.evaluate() shows a good accuracy, the confusion matrix shows a really bad one.
This is the script I am using:
from sklearn.metrics import classification_report, confusion_matrix
import keras
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator
import numpy as np
from keras.models import load_model


image_size = 224
test_dir = 'D:/pytorch-pwc-master/DataSet_Prince - flow/test'
test_batchsize = 9


test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=(image_size, image_size),
        batch_size=test_batchsize,
        class_mode='categorical',
        shuffle=True)

FLOW1_model = load_model('VGG_FLOW1.h5')

#Confusion Matrix and Classification Report
Y_pred = FLOW1_model.predict_generator(test_generator, test_generator.samples // test_generator.batch_size)
y_pred = np.argmax(Y_pred, axis=1)
print('Confusion Matrix')
print(confusion_matrix(test_generator.classes, y_pred))
print('Classification Report')
target_names = ['Bark', 'Jump', 'Stand','Walk']
print(classification_report(test_generator.classes, y_pred, target_names=target_names))


#Evaluating using Keras model_evaluate:
x, y = zip(*(test_generator[i] for i in range(len(test_generator))))
x_test, y_test = np.vstack(x), np.vstack(y)
loss, acc = FLOW1_model.evaluate(x_test, y_test, batch_size=64)

print("Accuracy: " ,acc)
print("Loss: ", loss)
And these are the results I get:
Confusion Matrix and Report:
Confusion Matrix
[[66 53 70 81]
 [64 70 61 75]
 [67 71 64 68]
 [64 65 57 84]]
Classification Report
             precision    recall  f1-score   support

       Bark       0.25      0.24      0.25       270
       Jump       0.27      0.26      0.26       270
      Stand       0.25      0.24      0.25       270
       Walk       0.27      0.31      0.29       270

avg / total       0.26      0.26      0.26      1080
And then the evaluate:
Accuracy:  0.8601851860682169
Loss:  0.40207018432793795
What can be the cause of this issue ? I tried training again and again but still similar results. Even when I try to predict my training dataset and use the confusion matrix/report it gives me similar weird results. As if the prediction is evenly split among the 4 classes.
My environment: Anaconda 3. Python 3.6. Windows 10 64bits, Keras 2.1.1 Tensorflow-gpu 1.4.0
Any help/advice will be very appreciated. I am starting to believe there is a bug somewhere...