oxydron commented on 7 Apr 2018 â€¢
edited
I've written a genetic algorithm to evolve Keras models, but every time a new model is created it spends more time to train than before, as if keras/tensorflow keeps something in memory for processing, summing up and slowing down all training. I generate many models (one at a time) and train every one of them for 15 epochs on Cohn-Kanade dataset.
The average training time for the first batch of models is about 6 seconds.
The average training time for the fourth batch of models is about 54 seconds.
Observe that the same model used less time in the first epoch than the 4th.
Tried this but didn't worked:
  model = None
  gc.collect()
Gist with some results:
https://gist.github.com/oxydron/1ba27a6c543143a749197a28a6a03592
Note that t: <time> represents the time spent to do a 15 epoch trainning.
Last observation: as evolution goes on, the model complexity not necessarily increases, as someone can use it as an argument for the increasing time spent on training.
Keras version: 2.1.5
Tensorflow version: 1.7
Edit 1:
Related to #3579