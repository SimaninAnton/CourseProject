Contributor
arodiss commented on 21 Jun 2017
Some ReLU neurons never fire during entire training epoch due to their unlucky weights. Since they are flat on negative input, their gradient is zero. Hence, their weights are not updated. Hence, they are not trained and are essentially useless.
Additionally, too high learning rate can eventually push neuron's weights into the never-active zone, knocking it out from training.
This problem (often referenced as "dead ReLU") is very common but somewhat tricky to debug because internal state of NN is obscure.
I think it would be nice to have callback that will check for dead ReLU after each training epoch and warn user so he can change initialization scheme, reduce learning rate etc.
I have very crude implementation of the feature here: #7056
If community and maintainers agree this makes sense I will finalize this PR
7