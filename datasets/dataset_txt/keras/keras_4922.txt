dongzhuoyao commented on 22 Jun 2016
when we use fit_generator,some typical code gists are as follows:
`(train_x, train_y), (test_x, test_y) = loadSet()
train_x = train_x.reshape(train_x.shape[0], 3, img_width, img_height)
test_x = test_x.reshape(test_x.shape[0], 3, img_width, img_height)
train_x = train_x.astype('float32')
test_x = test_x.astype('float32')
train_x /= 255  ## normalizing
test_x /= 255
print('train_x shape:', train_x.shape)
print(train_x.shape[0], 'train samples')
print(test_x.shape[0], 'test samples')

# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(train_y, nb_classes)
Y_test = np_utils.to_categorical(test_y, nb_classes)


if not data_augmentation:
    print('Not using data augmentation.')
    model.fit(train_x, Y_train,
              batch_size=batch_size,
              nb_epoch=nb_epoch,
              validation_data=(test_x, Y_test),
              shuffle=True)
else:
    print('Using real-time data augmentation.')

    # this will do preprocessing and realtime data augmentation
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images

    # compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied)
    datagen.fit(train_x)


    # fit the model on the batches generated by datagen.flow()
    model.fit_generator(datagen.flow(train_x, Y_train,batch_size=batch_size),
                        samples_per_epoch=train_x.shape[0],
                        nb_epoch=nb_epoch,
                        validation_data=(test_x, Y_test),callbacks=callback_list)`
since keras's document says that when the data cannot fit in memory,we can use the fit_generator method.however,i see some doubtful points about "fit_generator",bacause when usually do the following method:
(train_x, train_y), (test_x, test_y) = loadSet()
which load the data into memory first
since we had loaded the data into memory,why we say"if the data cannot fit in memory,you can use fit_generator"???