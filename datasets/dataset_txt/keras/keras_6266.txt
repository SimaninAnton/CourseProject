around1991 commented on 14 Dec 2015
Currently, when we call the add method on Sequential (and the add_node method on Graph) models, set_previous is called, which also calls build. Most of the time, this results in any existing weights on the layer being re-initialised. Thus, if we want to train a layer-by-layer autoencoder (for example), we need to store the existing weights of each encoder layer before adding them to our model, and then resetting the weights again after adding.
It seems like adding a overwrite_weights argument to Sequential.add and Graph.add_node methods, which feeds into set_previous on both containers and layers, basically telling Keras not to call Layer.build, would simplify this considerably. This would make incorporating pre-trained layers into a model much easier, and wouldn't impact any existing APIs.
Thoughts?