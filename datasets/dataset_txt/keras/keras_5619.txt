unrealwill commented on 30 Mar 2016
In the title ^^
I'm not sure whether this is a bug or an intended behaviour, but it looks strange because it would mean that when the example are streamed one at a time they wouldn't get normalized at all, during training.
I would recommend outputting something like (1/momentum) / ((1/momentum)+minibatchsize) * runningMean + (minibatchsize)/((1/momentum)+minibatchsize) * meanofminibatch
Below is the problematic code in get_output of BatchNormalization
if train:
m = K.mean(X, axis=reduction_axes)
brodcast_m = K.reshape(m, broadcast_shape)
std = K.mean(K.square(X - brodcast_m) + self.epsilon, axis=reduction_axes)
std = K.sqrt(std)
brodcast_std = K.reshape(std, broadcast_shape)
mean_update = self.momentum * self.running_mean + (1-self.momentum) * m
std_update = self.momentum * self.running_std + (1-self.momentum) * std
self.updates = [(self.running_mean, mean_update),
(self.running_std, std_update)]
X_normed = (X - brodcast_m) / (brodcast_std + self.epsilon)