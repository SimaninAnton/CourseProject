ipoletaev commented on 24 May 2016 â€¢
edited
Good day for everyone! I have noticed interesting thing: when you try to make embedding layer in the "Sequential" or in the "Graph" model - they have different input shapes...
For example:
emb_model = Sequential()
emb_model.add(Embedding(vocab_size, embedding_size, input_length=maxlen))
...
So, the input shape of emb_model is (None,vocab_size) and the output shape is (None,maxlen,embedding_size) as expected. Even though a very strange input size, with the input data shape - (None,maxlen) everything works correctly.
Let's try make the same "Graph" model:
...
model  = Graph()
model.add_input(name='word_emb_input',input_shape=(maxlen,),dtype=int)
model.add_node(emb_model,name="emb_model",input="word_emb_input")
Now, if we look at the size of the input - instead of (None,vocab_size) shape, like in the "Sequential" model, we got (None,maxlen) shape.
Is it normal? Thanks for answering.