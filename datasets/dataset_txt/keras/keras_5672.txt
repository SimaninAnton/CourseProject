fluency03 commented on 25 Mar 2016
When I was running some examples, I have tested them by switching between CPU and GPU on my PC.
Obviously, when using GPU, the training time is less than using CPU.
However, the timing of building the model seems takes longer time when enabling GPU than purely using CPU.
I am assuming that, when enabling GPU, even the model building is done on GPU, which may have less parallel requirements and CPU would be faster on building it.
So I am wondering, is there any way (or api or code) that I could use CPU to build the model and switch to GPU to train the model?