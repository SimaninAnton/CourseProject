jerryli1981 commented on 18 Nov 2015
Hi all,
I try to put l2 on LSTM, however, I got the error. " WeightRegularizer' object has no attribute 'p'"
    model = Sequential()
    model.add(Embedding(input_dim=n_symbols, output_dim=args.wvecDim, 
        mask_zero=True, weights=[wordEmbeddings.T],input_length=maxlen))
    model.add(LSTM(output_dim=args.wvecDim, return_sequences=False, 
        input_shape=(maxlen, args.wvecDim)))
    model.layers[-1].regularizers = [l2(1e-4)]
Traceback (most recent call last):
  File "main_keras.py", line 189, in <module>
    train_fn, val_fn, predict_proba= build_network(args, wordEmbeddings)
  File "main_keras.py", line 145, in build_network
    model.compile(loss='categorical_crossentropy', optimizer=optimizer)
  File "/Users/peng/Develops/keras/keras/models.py", line 390, in compile
    train_loss = r(train_loss)
  File "/Users/peng/Develops/keras/keras/regularizers.py", line 28, in __call__
    loss += T.sum(abs(self.p)) * self.l1
AttributeError: 'WeightRegularizer' object has no attribute 'p'
Is it cause by below line
model.layers[-1].regularizers = [l2(1e-4)]