FiammettaC commented on 27 May 2018 â€¢
edited
Hi, I have a network in Keras and my last layer is a fully connected layer with activation = softmax. I extracted the weights from the softmax and multiplied them by 0 and 1 (I am trying to restrict the predictions).
This is my code:
from keras import backend as K

def get_activations(model, layer, X_batch):
    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])
    activations = get_activations([X_batch,0])
    return activations

model = Sequential()
model.add(Embedding(vocab_size, embedding_size, input_length=55, weights=[pretrained_weights]))
model.add(Bidirectional(LSTM(units=embedding_size)))
print(model.add(Dense(vocab_size, activation='softmax'))) 

softmax_weights = np.array(get_activations(model, 3, X_train)[0], dtype=np.float32)
modified_softmax_weights = np.multiply(softmax_weights, weights_array) 
#weights array is a binary array containing 0 and 1, according to the index of the element I would like to set to 0)
Now my question is: how can I pass the modified softmax weights matrix to the model? Is there a way compile and fit a Keras model with updated weights?
I know about the function set_weights(), but my problem is that set_weights() expects input of shape (hidden_layers, vocab_size), while my new weights have size (len(X_train), vocab_size). Does anybody know how to do this?