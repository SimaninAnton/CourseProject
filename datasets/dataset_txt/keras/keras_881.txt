skeydan commented on 18 Jul 2018
[ X] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps
[ X] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
[ X] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
[ X] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
As demonstrated by @fchollet in the DL with Python notebook "Visualizing what convnets learn", we can get the gradients of the output w.r.t. an arbitrary conv layer in the pretrained model (minimal example below).
However, when we attach a custom head to the model and try to get the gradients w.r.t. a layer of the VGG model included, the gradients are None (minimal example below).
Is this a bug, or expected behavior - and if so, is there an alternative way to get the gradients in this case?
Many thanks in advance!
Works:
from keras.applications.vgg16 import VGG16
from keras import backend as K

model = VGG16(weights='imagenet')
last_conv_layer = model.get_layer('block5_conv3')
grads = K.gradients(model.output, last_conv_layer.output)[0]
grads
Gradients are None:
from keras.applications.vgg16 import VGG16
from keras import backend as K
from keras import models
from keras import layers


conv_base = VGG16(weights='imagenet',
                  include_top=False,
                  input_shape=(224, 224, 3))
                  
model = models.Sequential()
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

conv_layer = model.layers[0].get_layer("block5_conv3")
conv_layer 
grads = K.gradients(model.output, conv_layer.output)[0]
print(grads)