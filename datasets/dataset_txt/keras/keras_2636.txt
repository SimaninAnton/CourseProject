kahrabian commented on 17 Apr 2017 â€¢
edited
I'm trying to build a sequence to sequence autoencoder with variable length sequences of floating points.
I was wondering which of these implementations are correct:
seq_autoenc = Sequential()
seq_autoenc.add(Masking(mask_value=mask_value, input_shape=(None, inp_dim)))
seq_autoenc.add(LSTM(enc_len, return_sequences=True, implementation=implementation, name='encoder'))
seq_autoenc.add(LSTM(inp_dim, return_sequences=True, implementation=implementation, name='decoder'))
or
seq_autoenc.add(Masking(mask_value=mask_value, input_shape=(inp_max_len, inp_dim)))
seq_autoenc.add(cell(enc_len, implementation=implementation, name='encoder'))
seq_autoenc.add(RepeatVector(inp_max_len, name='repeater'))
seq_autoenc.add(cell(inp_dim, return_sequences=True, implementation=implementation, name='decoder'))
The problem with the last code is that the maximum length of sequences is now known and it will truncate some of sequences in the testing phase.
Another problem that I'm facing is that the range of mask_value is not available, is it possible to use np.nan or np.inf as the masking value?