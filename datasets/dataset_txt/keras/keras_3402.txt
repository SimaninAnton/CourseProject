zhengsx commented on 22 Jan 2017
Hi all,
I post a question about keras' optimizer in stackoverflow. Can anybody help me to answer that?
The function get_update() in Adagrad seems one step update. However should the accumulators be stored the history information? Why it has been initialized to zeros at each step? How it can be an accumulator through the whole training process?
What does this line do?
self.weights = accumulators
It seems self.weights is never been called anymore.
Thanks.