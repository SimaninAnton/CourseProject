Contributor
jmhessel commented on 12 Feb 2016
I am attempting to implement Deep Averaging Networks in keras. The model applies dropout to words in a sequence prior to embedding them. So, the thing I want to do is basically (assuming that I have padded sequences of length maxlen, and I have max_features tokens).
model = Sequential()
model.add(Dropout(.3, input_shape = [maxlen]))
model.add(Embedding(max_features, embedding_dim))
model.add(LSTM(128))
model.compile(loss='binary_crossentropy', optimizer='adagrad', class_mode="binary")
This relates to another issue I submitted tonight (#1698) but for the purposes of this issue -- it seems that dropout kills the type of the input (I think it makes it a float). Is it worth supporting dropout over ints? I could see an argument that this is such a weird case that the API shouldn't be broken for this, but it would be nice to index into Embedding layers based on dropped out ints. I don't really see why Dropout should change the typing.