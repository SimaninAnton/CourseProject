bernardohenz commented on 2 Aug 2016
I am trying to separate my full model into two parts (similar to autoencoder having encoder/decoder), but I keep getting problems when doing so. I wrote a simple case to reproduce my problem, first I started by trying doing so:
from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D,merge
from keras.models import Model,Sequential
from PIL import Image
import numpy as np
from keras.layers.normalization import BatchNormalization

main_input= Input(shape=(3, 64, 64))

#first 'model'
conv1 = Convolution2D(64, 3, 3, border_mode='same')(main_input)
conv1 = BatchNormalization(mode=0, axis=1)(conv1)
conv1 = Convolution2D(128, 3, 3, border_mode='same')(conv1)
conv1 = BatchNormalization(mode=0, axis=1)(conv1)
conv1 = Convolution2D(256, 3, 3, border_mode='same')(conv1)

#second 'model'
conv2 = Convolution2D(256, 3, 3, border_mode='same')(conv1)
conv2 = BatchNormalization(mode=0, axis=1)(conv2)
conv2 = Convolution2D(128, 3, 3, border_mode='same')(conv2)
conv2 = BatchNormalization(mode=0, axis=1)(conv2)
conv2 = Convolution2D(64, 3, 3, border_mode='same')(conv2)
conv2 = BatchNormalization(mode=0, axis=1)(conv2)
conv2 = Convolution2D(3, 3, 3, border_mode='same')(conv2)

first_model = Model(input=main_input,output=conv1)
second_model = Model(input=conv1,output=conv2)
full_model = Model(input=main_input,output=conv2)
But when trying to create de second_model I get the error 'Exception: Graph disconnected: cannot obtain value for tensor input_1 at layer "input_1". The following previous layers were accessed without issue: []'
Then, I tried to get layer by layer of the full model:
full_model = Model(input=main_input,output=conv2)

second_input= Input(shape=(256, 64, 64))
current_node = second_input
for i in range(len(first_model.layers),len(full_model.layers)):
    current_layer = full_model.layers[i]
    current_node = current_layer(current_node)

second_model = Model(input=second_input,output=current_node)
However I got the error 'Exception: You are attempting to share a same BatchNormalization layer across different data flows. This is not possible. You should use mode=2 in BatchNormalization, which has a similar behavior but is shareable (see docs for a description of the behavior).'
Could someone help me?
Obs: I am building my architecture using Graph blocks, so I prefer if the solution can be done in a similar way
Please make sure that the boxes below are checked before you submit your issue. Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
1
1