sergeyf commented on 24 Jul 2015
Howdy,
I was looking at this blog post http://sifter.org/~simon/journal/20150420.html in hopes of adding SMORMS3 to Keras, and discovered a small discrepancy between the way RMSProp is implemented in Keras and in published literature. Not sure if it's important, so posting here for the relevant eyeballs to look at.
In line 85 of optimizers.py, we have:
new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)
However, if we were to heed this recent paper (http://arxiv.org/pdf/1502.04390v1.pdf, equation after equation (2)), the code should be:
new_p = p - self.lr * g / ( T.sqrt(new_a) + self.epsilon )
In other words, the self.epsilon should not be inside the square root.
This tutorial: https://github.com/Newmu/Theano-Tutorials/blob/master/4_modern_net.py also has the self.epsilon inside the square root, so not sure what the "standard" way is. The behavior would certainly be different...
Thoughts from someone who knows more about this than me?
PS, the RMSProp in this work by Alex Graves looks entirely different: http://arxiv.org/pdf/1308.0850v5.pdf