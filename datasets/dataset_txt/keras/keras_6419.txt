RaffEdwardBAH commented on 19 Nov 2015
I'm attempting to replicate the results in Generating Sequences With Recurrent Neural Networks, by Alex Graves for a generative model on Wikipedia. The generative example here is built by eating a sequence of n characters and then attempting to predict the _n+1_th character.
However, the paper by Graves (as I understand, please correct me if wrong) eats characters (with a zero vector as the first input) and outputs the same characters being eaten. ie: "Hello World" would be the input characters and "Hello World" would be the outputs, where "" would indicate the zero vector. Another note in the paper is that, to learn the long term structure, they eat 10k characters (100 byte sequences with internal state only reset every 100 sequences, and sequences are kept in order) but only push the gradient back to 100 steps.
So I started to set up a similar model as
model = Sequential()
model.add(LSTM(2048, return_sequences=True, input_shape=(max_len, tokens), truncate_gradient=100))
model.add(Dropout(0.5))
model.add(LSTM(2048, return_sequences=True, truncate_gradient=100))
model.add(Dropout(0.5))
model.add(LSTM(2048, return_sequences=False, truncate_gradient=100))
model.add(Dropout(0.5))
model.add(Dense(tokens))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1))
However I'm not sure this is correct, and I'm not sure what shape the target variable y should be. I think the training matrix would be shaped as X = np.zeros((train_sequences, max_len+1, tokens), dtype=np.bool), where the +1 would be for the zero vector start.
I have all characters read into a string and my initialization looks like
row= 0
for i in range(0, len(train_bytes), max_len):
    for t, byte in enumerate(train_bytes[i:i+max_len]):
        X[row, t+1, ord(byte)] = 1
    row += 1
Can anyone help me finish this / set me in the right direction? I'm not seeing how to indicate to keras that if I made y 3 dimensional how the targets line up with the inputs. ie: the fist "l" should be predicted after receiving "*He" and the gradient pushed back from there, and then the second "l" after "*Hel", and so on.
I saw issue #395 , but it is set up somewhat different - and the issue quite cluttered at this point.