Golbstein commented on 24 Sep 2017 â€¢
edited
Hello guys, I've created and trained a binary classification VGG16 style model which is reaching over 98% accuracy on a validation set.
Then I decided to change the dense layers of my model with conv layers + global average pooling in order to be independent of the image size.
Unfortunately the new model is stuck at 50% accuracy ...
Why? How to interpret this? I'm afraid that the dense layers over-fitted my original model and I don't really have a tool that classify my images... (My dataset contains over 25K images per category)
My original model final layers:
model.add(BatchNormalization(axis=1))
model.add(MaxPooling2D())
model.add(Flatten())
model.add(Dense(1000, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(1000, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))
My new FCN model final layers:
BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),
MaxPooling2D(),
Convolution2D(1000,(3,3), activation='relu', padding='same'),
BatchNormalization(axis=1),
Convolution2D(1000,(3,3), activation='relu', padding='same'),
BatchNormalization(axis=1),
Convolution2D(2,(3,3), padding='same'),
GlobalAveragePooling2D(),
Activation('softmax')