liweitj47 commented on 25 Mar 2016
I am currently trying to implement various rnn structures, I followed the code in the examples,"imdb_bidirectional_lstm.py", however, I want to do sequence labeling, which not only needs the last step, but also needs the complete sequence.
So here's the issue, I want the forward lstm and the backward lstm to be strictly aligned in the same order of the input sequence. But if I only set the go_backwards=True, it will return a backward sequence, which is not the same order as the input. Since the code in seya and other repos haven't updated for a long time, and seem difficult to apply, I added a Lambda Layer upon the backward lstm. Will it work as I expected? Thanks!
106 model.add_node(LSTM(200,return_sequences=True),name='lstm',input='dropou t')
107 model.add_node(LSTM(200,return_sequences=True,go_backwards=True),name='l stm2',input='dropout')
108 model.add_node(Lambda(lambda x:x[:,::-1,:]),name='reverse',input='lstm2' )
109 model.add_node(Dropout(0.5),name='dropout2',inputs=['lstm','reverse'],me rge_mode='concat')
110 model.add_node(TimeDistributedDense(nb_label),name='tdd',input='dropout2 ')