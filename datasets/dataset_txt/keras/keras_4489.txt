Contributor
tzachar commented on 30 Aug 2016
The following code triggers a bug:
import numpy as np                                                                                      
from keras.layers import merge, Input, Dense, TimeDistributed, Lambda                                   
from keras.models import Model                                                                          
from keras import backend as K                                                                          
from keras.constraints import maxnorm                                                                   

# first, define the shared model                                                                          
input = Input(shape=(100,))                                                                             
out = Dense(10, W_constraint=maxnorm(10), name='shared_dense')(input)                                   
shared_model = Model(input, out, name='shared_model')                                                   

# then define the inputs                                                                                
a = Input(shape=(100,))                                                                                 
b = Input(shape=(5, 100,))                                                                              
out_a = shared_model(a)                                                                                 
out_b = TimeDistributed(shared_model)(b)                                                                
out_b = Lambda(                                                                                         
    function=lambda x: K.sum(x, axis=1),                                                                
    output_shape=lambda shape: (shape[0],) + shape[2:])(out_b)                                          
concatenated = merge([out_a, out_b], mode='concat')                                                     
out = Dense(1, activation='sigmoid')(concatenated)                                                      

classification_model = Model([a, b], out)                                                               
classification_model.compile(optimizer='sgd', loss='binary_crossentropy')                               

classification_model.train_on_batch(                                                                    
    [np.ones((20, 100)), np.ones((20, 5, 100))],                                                        
    [np.ones(20)]                                                                                       
)                
The bug is triggered by the constraint on the shared_dense layer (removing it results in working code), and stems from container:constraints method.
The max_norm constraint is added both via the shared_model and the shared_dense.
This also results in regularizations appearing more than once, but silently, as there is no check as in constraints. This bug is only triggered when sharing a model via the TimeDistributed wrapper.
Any ideas on how to solve this?