redst4r commented on 4 Jan 2017 â€¢
edited
Hi,
sometimes its useful to disable the centering and scaling parameters (beta/gamma) of the BatchNormalization, see e.g. this paper, page 6 (e.g. if the bn is followed by a Relu; then beta/gamma can be absorbed into the next layers weights).
In keras.layers.BatchNormalization there's currently no way to disable training of beta/gamma.
Tensorflow already has some convenient way to disable it in tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon) by setting beta=None and/or gamma=None. This is already used in tf.slim via flags center and scale
We could add this fairly easily to the tensorflow backend, putting two booleans center, scale to the BatchNormalization constructor, setting e.g. self.beta=None in .build() if self.center=False; plus a few changes in .call() due to the broadcasting, which will fail on None
However I dont know how it would work in the theano-backend. batchnorm looks kind of more complicated there.
Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on StackOverflow or join the Keras Slack channel and ask there instead of filing a GitHub issue.
Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).