Contributor
erg commented on 13 Jul 2017 â€¢
edited
It looks like the skipgrams function is generating too many skip-grams, out-of-order skip-grams, and skip-grams from what would usually be considered outside of the window. This generates a much larger set of training data than is otherwise required.
Current behavior:
In [1]: from keras.preprocessing.sequence import skipgrams
Using TensorFlow backend.

In [2]: sequence = ['ab','cd','ef','gh','ij','kl','mn']

In [3]: skipgrams(sequence, 7, window_size=2, negative_samples=0, shuffle=False)
Out[3]: 
([['ab', 'cd'],
  ['ab', 'ef'],
  ['cd', 'ab'], # backwards, repeated from above
  ['cd', 'ef'],
  ['cd', 'gh'],
  ['ef', 'ab'],
  ['ef', 'cd'],
  ['ef', 'gh'],
  ['ef', 'ij'],
  ['gh', 'cd'],
  ['gh', 'ef'],
  ['gh', 'ij'],
  ['gh', 'kl'],
  ['ij', 'ef'],
  ['ij', 'gh'],
  ['ij', 'kl'],
  ['ij', 'mn'],
  ['kl', 'gh'],
  ['kl', 'ij'],
  ['kl', 'mn'],
  ['mn', 'ij'],
  ['mn', 'kl']],
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

In [4]: skipgrams(sequence, 7, window_size=3, negative_samples=0, shuffle=False)
Out[4]: 
([['ab', 'cd'],
  ['ab', 'ef'],
  ['ab', 'gh'],
  ['cd', 'ab'],
  ['cd', 'ef'],
  ['cd', 'gh'],
  ['cd', 'ij'],
  ['ef', 'ab'],
  ['ef', 'cd'],
  ['ef', 'gh'],
  ['ef', 'ij'],
  ['ef', 'kl'],
  ['gh', 'ab'], # backwards, apparent window_size 4, repeated from above
  ['gh', 'cd'],
  ['gh', 'ef'],
  ['gh', 'ij'],
  ['gh', 'kl'],
  ['gh', 'mn'],
  ['ij', 'cd'],
  ['ij', 'ef'],
  ['ij', 'gh'],
  ['ij', 'kl'],
  ['ij', 'mn'],
  ['kl', 'ef'],
  ['kl', 'gh'],
  ['kl', 'ij'],
  ['kl', 'mn'],
  ['mn', 'gh'],
  ['mn', 'ij'],
  ['mn', 'kl']],
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
Patched:
In [1]: from keras.preprocessing.sequence import skipgrams
Using TensorFlow backend.

In [2]: sequence = ['ab','cd','ef','gh','ij','kl','mn']

In [3]: skipgrams(sequence, 7, window_size=2, negative_samples=0, shuffle=False)
Out[3]: 
([['ab', 'cd'],
  ['cd', 'ef'],
  ['ef', 'gh'],
  ['gh', 'ij'],
  ['ij', 'kl'],
  ['kl', 'mn']],
 [1, 1, 1, 1, 1, 1])

In [4]: skipgrams(sequence, 7, window_size=3, negative_samples=0, shuffle=False)
Out[4]: 
([['ab', 'cd'],
  ['ab', 'ef'],
  ['cd', 'ef'],
  ['cd', 'gh'],
  ['ef', 'gh'],
  ['ef', 'ij'],
  ['gh', 'ij'],
  ['gh', 'kl'],
  ['ij', 'kl'],
  ['ij', 'mn'],
  ['kl', 'mn']],
 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
Patch:
diff --git a/keras/preprocessing/sequence.py b/keras/preprocessing/sequence.py
index 6004a9e0..f5688d1e 100644
--- a/keras/preprocessing/sequence.py
+++ b/keras/preprocessing/sequence.py
@@ -158,9 +158,8 @@ def skipgrams(sequence, vocabulary_size,
             if sampling_table[wi] < random.random():
                 continue
 
-        window_start = max(0, i - window_size)
-        window_end = min(len(sequence), i + window_size + 1)
-        for j in range(window_start, window_end):
+        window_end = min(len(sequence), i + window_size)
+        for j in range(i, window_end):
             if j != i:
                 wj = sequence[j]
                 if not wj: