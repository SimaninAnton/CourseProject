Contributor
carlthome commented on 20 Mar 2016 â€¢
edited
I'm using fit_generator(...) because my datasets are too large to fit in memory, but I don't know how to set samples_per_epoch. I don't know beforehand how many samples I'll actually end up with.
The dataset is procedurally generated and cannot fit on disk. I'd like to avoid running the batch generator one full iteration before training just to count the samples, because that takes a long time.
Could fit_generator(...) provide a way for the generator to say its done one full iteration? For example by letting it the generator call a model.stop_epoch() function that ends the training loop (e.g. flipping a boolean in the while condition). This would then make epoch_per_samples optional.
1