ijmbarr commented on 5 Sep 2016
I'm trying to understand how dropouts are implemented within Keras.
When are units to block in the dropouts layer set? Can they be set to specific configurations? Are they reset everytime I call train_on_batch?, or is there a way to get the same dropout configuration to persist over multiple calls, or is a different one created each time?
Thanks