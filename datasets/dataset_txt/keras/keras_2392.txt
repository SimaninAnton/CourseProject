kuatroka commented on 13 May 2017
Hi,
I'm running the code that prints the model.predict, model_proba and predict_classes. After it gets printed I see that model.predict() and model.predict_proba() do not give the same probabilities and actual classes predicted do not correspond to the probabilities. See the sample of the code and the printout below:
Code:
p = pd.read_csv("test.csv", header=None)
p = np.reshape(p.values, (50, seq_length))
for i in range(len(p)):
    p[i] = scaler.fit_transform(p[i])

p = np.reshape(p, (50, seq_length, 1))


model.predict(p, batch_size=50)
model.predict_classes(p, batch_size=50)
model.predict_on_batch(p)
model.predict_proba(p, batch_size=50)
for i in zip(model.predict_proba(p, batch_size=50), model.predict_classes(p, batch_size=50), model.predict(p, batch_size=50)):
    print("model.predict_proba", "--", i[0], "model.predict","--", i[2], "predict_clases", "--", i[1])
Sample printout with different values for model_proba() and model.predict()
model.predict_proba -- [ 0.18768159 0.81231844] model.predict -- [ 0.18982948 0.81017047] predict_clases -- 1
Sample of incorrect class predicted as per corresponding probability:
model.predict_proba -- [ 0.09940339 0.90059662] model.predict -- [ 0.09940339 0.90059662] predict_classes -- 0
In the last example, the 0.9 value clearly corresponds to the second class (index = 1), but the prediction is 0
In general, I have multiple cases like this in my validation batch of 50.
Thanks