25suman08 commented on 17 Mar 2018
import re
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import csv
import codecs
import numpy as np
from nltk.corpus import stopwords
from gensim.models.keyedvectors import KeyedVectors
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
#from keras.utils.np_utils import accuracy
BASE_DIR = ''
TRAIN_DATA_FILE = BASE_DIR + 'dvd_data.csv'
TEST_DATA_FILE = BASE_DIR + 'books.csv'
MAX_SEQUENCE_LENGTH = 50
EMBEDDING_DIM = 300
act = 'relu'
def text_to_wordlist(text, remove_stopwords=False, stem_words=False):
# Clean the text, with the option to remove stopwords and to stem words.
# Convert words to lower case and split them
text = text.lower().split()
text = " ".join(text)
# Clean the text
text = re.sub(r"[^A-Za-z0-9^,!./'+-=]", " ", text)
text = re.sub(r"what's", "what is ", text)
text = re.sub(r"'s", " ", text)
text = re.sub(r"'ve", " have ", text)
text = re.sub(r"can't", "cannot ", text)
text = re.sub(r"n't", " not ", text)
text = re.sub(r"i'm", "i am ", text)
text = re.sub(r"'ll", " will ", text)
text = re.sub(r",", " ", text)
text = re.sub(r".", " ", text)
text = re.sub(r"!", " ! ", text)
text = re.sub(r"/", " ", text)
text = re.sub(r"^", " ^ ", text)
text = re.sub(r"+", " + ", text)
text = re.sub(r"-", " - ", text)
text = re.sub(r"=", " = ", text)
text = re.sub(r"'", " ", text)
text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
text = re.sub(r":", " : ", text)
text = re.sub(r" e g ", " eg ", text)
text = re.sub(r" b g ", " bg ", text)
text = re.sub(r" u s ", " american ", text)
text = re.sub(r"\0s", "0", text)
text = re.sub(r" 9 11 ", "911", text)
text = re.sub(r"e - mail", "email", text)
text = re.sub(r"j k", "jk", text)
text = re.sub(r"\s{2,}", " ", text)
# Optionally, shorten words to their stems
if remove_stopwords:
text = text.split()
text = [word for word in text if word not in stopwords.words('english')]
text = " ".join(text)
# Return a list of words
return (text)
texts_1 = []
labels = []
with codecs.open(TRAIN_DATA_FILE, encoding='latin1') as f:
reader = csv.reader(f, delimiter=',')
header = next(reader)
for values in reader:
texts_1.append(text_to_wordlist(values[0]))
labels.append(float(values[1]))
#print('Found %s texts in train.csv' % len(texts_1))
test_texts_1 = []
test_ids = []
with codecs.open(TEST_DATA_FILE, encoding='latin1') as f:
reader = csv.reader(f, delimiter=',')
header = next(reader)
for values in reader:
test_texts_1.append(text_to_wordlist(values[0]))
test_ids.append(float(values[1]))
#print(texts_1)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts_1+test_texts_1)
sequences_1 = tokenizer.texts_to_sequences(texts_1)
test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)
word_index = tokenizer.word_index
#print(word_index)
print('Found %s unique tokens' % len(word_index))
data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)
labels = (np.asarray(labels))
print('Shape of data tensor:', data_1.shape)
print('Shape of label tensor:', labels.shape)
test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)
test_ids = (np.asarray(test_ids))
print('Shape of data tensor:', test_data_1.shape)
print('Shape of id tensor:', test_ids.shape)
word2vec = KeyedVectors.load_word2vec_format('dvd_vec.txt', binary=False)
#print(word_vectors)
vocabulary_size=len(word_index)+1
#print(vocabulary_size)
embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))
#print(embedding_matrix.shape)
for word, i in word_index.items():
if word in word2vec.wv.vocab:
embedding_matrix[i] = word2vec.word_vec(word)
#print(word)
print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))
from keras.layers import Embedding
from keras.layers import GlobalAveragePooling1D, Convolution1D
from keras.optimizers import SGD,Adadelta
from keras.layers import Dropout
from keras.callbacks import ModelCheckpoint
sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences_1 = Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix],
input_length=MAX_SEQUENCE_LENGTH, trainable=True)(sequence_1_input)
conv_q= Convolution1D(100, 2, border_mode='same', activation='relu')(embedded_sequences_1)
conv_q = Dropout(0.25)(conv_q)
gl = GlobalAveragePooling1D()(conv_q )
merged = Dense(200, activation=act)(gl)
merged = Dropout(0.2)(merged)
merged = Dense(200, activation=act)(merged)
merged = Dropout(0.2)(merged)
preds = Dense(1, activation='sigmoid', kernel_initializer='normal')(merged)
model = Model(inputs=[sequence_1_input], outputs=preds)
model.compile(loss= 'binary_crossentropy', optimizer='adadelta', metrics=["accuracy"] )
model.summary()
bst_model_path = '2.h5'
model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)
hist=model.fit(data_1, labels,
validation_split=0.20, epochs=100, batch_size=64,
shuffle=True, verbose=2 , callbacks=[model_checkpoint])
model.load_weights(bst_model_path)
bst_val_score = max(hist.history['val_acc'])
acc = model.evaluate(test_data_1, test_ids, batch_size=64)
print("Accuracy: %.2f%%" % (acc[1]*100))
Thank you!
is this code correct or i need to modify ..