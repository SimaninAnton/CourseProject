maxleverage commented on 15 May 2017
Hi,
I've been attempting to implement the EntropySGD in this paper (https://arxiv.org/pdf/1611.01838.pdf). However the inner loop of the optimizer requires L gradient updates to estimate the exponential decayed mus which are used to update the parameters on one batch of data. It looks like 'get_updates' is called from training.py. My question is:
Is there a way to update the network params, compute a new loss and get new gradients on the same batch of data in the optimizer class to run the L iterations for the Langevin dynamics?
If not, can this be implemented in other ways or can this be added as a future feature?
Thanks