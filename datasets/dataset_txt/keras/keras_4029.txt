P-Takenaka commented on 1 Nov 2016 â€¢
edited
I noticed that when using the "tf" dimension ordering, the training loss and accuracy do not improve at all. For example, using the keras cifar 10 example if you add set_image_dim_ordering("tf") at the top, the accuracy stays at 0.1. The training data however has the correct ordering of (32, 32, 3).
I tried it with Keras 1.1.1, Tensorflow 0.11.0rc1, Cuda 8.0 and Keras 1.1.1, Tensorflow 0.10.0, Cuda 7.5
Edit:
I found out that it is caused by the optimizer. For example, SGD and Adagrad result in no accuracy improvement, but when using adadelta it works just fine with "tf" ordering.
However, it is still only limited to the cifar example, in the mnist example "tf" ordering works just fine with any optimizer.
Edit2:
I noticed that sometimes the accuracy actually improves with "tf" ordering. It seems to be random behaviour, which may result from the weight initializations (I use the default "glorot_uniform"). In around 1 of 5 runs the accuracy improves beyond 0.1 in the first epoch. With for example "he_normal" it works every time.