C0NTRIBUTE commented on 9 Oct 2018 â€¢
edited
Using the MNIST example model, when multi_gpu_model is set to 2 GPU, the speed is up to 50% LOWER than with a single GPU on the same system. Nothing else changes. I'm using the example model as-is with the exception that there is just one Dense layer and no Dropout. I'm timing just model.fit().
The exact code to try and reproduce.
Increasing the epochs to 100, the difference is now 100 seconds with multi_gpu_support on vs. 63 seconds without.
I've confirmed that both GPU are activated with:
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
...which produces in my console:
2018-10-09 17:18:47.610895: I tensorflow/core/common_runtime/direct_session.cc:291] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: GeForce GTX 1070 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1
I'm on Ubuntu 18.04, CUDA 9, cudNN 7, Keras 2.2.4, and Tensorflow 1.11.0.
My system has two GPUs.