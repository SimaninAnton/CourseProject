hobson commented on 22 Sep 2016 •
edited
PR #3851
This paper explains the advantages of a soft exponential activation function including improved training, simplified neural nets, and improved accuracy for models that rely on multiplicative transforms of the input, such as dot product, cross product, and polynomial transforms.
Draft implementation:
def softexp(x, alpha=0., max_value=None):
    """Soft Exponential activation function by Godfrey and Gashler

    See: https://arxiv.org/pdf/1602.01321.pdf

    α == 0:  f(α, x) = x
    α  > 0:  f(α, x) = (exp(αx)-1) / α + α
    α  < 0:  f(α, x) = -ln(1-α(x + α)) / α
    """
    if alpha == 0:
        return x
    elif alpha > 0:
        return alpha + (K.exp(alpha * x) - 1.) / alpha
    else:
        return - K.log(1 - alpha * (x + alpha)) / alpha
I'm testing it for correctness (match this plot):
Here's the fork and branch: https://github.com/totalgood/keras/tree/softexp