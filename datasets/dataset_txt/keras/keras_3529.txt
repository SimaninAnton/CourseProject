fyquah95 commented on 7 Jan 2017
I am currently working on text synthesis. I want to model an arbitrarily long sequence, and would want to have the ability to recursively apply the output. Ie: feed an input to the stateful LSTM, get an output, do some post-processing and feed that processed output to the stateful LSTM, and repeat.
I am currently designing my model as such:
model = models.Sequential()
model.add(layers.LSTM(256, batch_input_shape=(1, 1, TOKENS_COUNT),
return_sequences=True, stateful=True))
I have to set the first argument of batch_input_shape to 1 as I need to feed a single input at during text synthesis. However, this causes problem during training, where I wish to have a single gradient update at every 250 steps, rather than every single step. More specifically, I wish to call
# TOKENS_COUNT = 84
# x.shape = (250, 1, TOKENS_COUNT)
# y.shape = (250, 1, TOKENS_COUNT)
model.fit(x, y,  nb_epoch=1, verbose=1, batch_size=250, shuffle=False)
but it wouldn't work. model.fit in this case would not accept a batch_size of 250 and yield the following error message:
ValueError: Cannot feed value of shape (250, 1, 84) for Tensor 'lstm_input_1:0', which has shape '(1, 1, 84)'
I tried train_with_batch to no avail too. Am I designing the net wrongly or is this simply a limitation of the LSTM class?