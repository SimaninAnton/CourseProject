tadpole commented on 10 Jul 2017
Hi, i am trying to implement a LSTM. The input and output are all followed by the same embedding layer. The goal is to reconstruct the output embeddings' MSE. For the following code, the loss is defined as the MSE of e_y and y_train's embedding:
x_train = np.array([[1,2,3,4]])
y_train = np.array([5])
x_input = Input(shape=(4, ), dtype='int32')
embedding_layer = Embedding(4+1, 5, mask_zero=True)
e_x = embedding_layer(x_input)
e_y = LSTM(5)(e_x)
m = Model(x_input, e_y)
m.compile(loss='mse', optimizer='adam')
I use m.train(x_train, y_train), the loss is ||e_y-y_train||^2. but the expected loss is ||e_y-embedding_layer(y_train)||^2. How to do that?