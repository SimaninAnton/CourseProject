twangnh commented on 26 May 2016
I'm new using keras, I want to get the learning rate during training LSTM with sgd optimizer, I have set the decay parameter, it seems it works, but when I use model.optimizer.lr.get_value() to read the learning rate, it didn't change at all, my setting is as follows:
lr_init=0.12; decay_init=1e-2; batch_size_init=30
momentum_init=0.9; np_epoch_init=1
sgd = SGD(lr=lr_init, decay=decay_init, momentum=momentum_init, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd)
early_stopping = EarlyStopping(patience=2,verbose=1)
lr_record=[]
for iteration in range(1, 60):
    print('Iteration', iteration)
    hist=model.fit(X, y, callbacks=[early_stopping],validation_data=(X_val, y_val),batch_size=batch_size_init, nb_epoch=np_epoch_init)
    lr_temp=model.optimizer.lr.get_value()
    lr_record.append(lr_temp)
after ran it, the learning rate recorded in lr_recorddid not change a bit at all, I'm wondering if someone could help me, thanks in advance!
1