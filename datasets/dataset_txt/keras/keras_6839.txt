Contributor
iskandr commented on 20 Jul 2015
I have an idea for adding bidirectional RNNs to Keras and I'm curious what the Keras devs think of it.
Add a Reverse layer which simply slices its input tensor along the timestep dimension (e.g. X_input[:, ::-1]. This would preserve masking and slice masks in reverse as well.
Add a Bidirectional class which takes an RNN class as a parameter and internally constructs the forward and backward instances, along with their merge. The backward instance can be Reverse(RNN(Reverse(x))).
How does that sound?