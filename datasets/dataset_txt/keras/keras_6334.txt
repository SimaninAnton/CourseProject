Contributor
jfsantos commented on 3 Dec 2015
I have a network that looks like this (it's kind of a next character prediction model, but using masks, and it predicts the whole sequence of "future characters" at once):
model = Sequential()
model.add(Embedding(vocab_size, n_hidden, mask_zero=True))
model.add(Dropout(0.5))
model.add(LSTM(n_hidden, return_sequences=True))
model.add(Dropout(0.5))
model.add(LSTM(n_hidden, return_sequences=True))
model.add(Dropout(0.5))
model.add(TimeDistributedDense(vocab_size))
model.add(Activation('softplus'))
The model is compiled with categorical crossentropy as the loss function. However, when I try to train it, I get a shape mismatch error from Theano which looks like this:
ValueError: Input dimension mis-match. (input[0].shape[1] = 64, input[1].shape[1] = 250)
Apply node that caused the error: Elemwise{mul,no_inplace}(InplaceDimShuffle{x,0}.0, 
Elemwise{Composite{(i0 - EQ(i1, i2))}}.0)
Toposort index: 452
Inputs types: [TensorType(float64, row), TensorType(int8, matrix)]
Inputs shapes: [(1, 64), (64, 250)]
Inputs strides: [(512, 8), (250, 1)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[Sum{axis=[1], acc_dtype=float64}(Elemwise{mul,no_inplace}.0)]]
(here, 64 is the batch size, 250 is the sequence length, and n_hidden = 512)
The same model works properly if I disable masking in the Embedding layer. I tried investigating the Embedding layer implementation for clues but I'm afraid I don't understand how masking works that well on Keras.