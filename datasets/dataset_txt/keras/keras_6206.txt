napsternxg commented on 25 Dec 2015
Hi @fchollet ,
I wrote a simple LSTM based model for doing Boundary Detection in text data using the following code. This model was working fine with version 0.2.0 of Keras but after I updated Keras and Theano to the latest version using the git repo of each module, I have started getting the following errors (I have added the THEANO VERBOSE output as well):
$ THEANO_FLAGS="optimizer=fast_compile,exception_verbosity=high" python model.py 
EntityExtractor_Model INFO 2015-12-24 18:46:53,845:Started Logger
Couldn't import dot_parser, loading of dot files will not be possible.
EntityExtractor_Model INFO 2015-12-24 18:46:54,253:Using Keras version 0.3.0
EntityExtractor_Model INFO 2015-12-24 18:46:54,253:Using Theano version 0.7.0.dev-54186290a97186b9c6b76317e007844529a352f4
Using Theano backend.
Reloaded Ext.
EntityExtractor INFO 2015-12-24 18:46:54,356:Started Logger
EntityExtractor INFO 2015-12-24 18:46:54,370:Initializing WordToken word_dict with 10000 items
EntityExtractor_Model INFO 2015-12-24 18:46:54,370:Parameters: vocab_size = 10004, embedding_size = 128, maxlen = 100, boundary_size = 6, category_size = 96, embedding_size = 128, hidden_layer_size = 100
EntityExtractor_Model INFO 2015-12-24 18:46:54,370:Parameters: vocab_size = 10004, embedding_size = 128, maxlen = 100, boundary_size = 6, category_size = 96, embedding_size = 128, hidden_layer_size = 100
EntityExtractor_Model INFO 2015-12-24 18:46:54,370:Building Model
EntityExtractor_Model INFO 2015-12-24 18:46:54,370:Init Model with vocab_size = 10004, embedding_size = 128, maxlen = 100
EntityExtractor_Model INFO 2015-12-24 18:46:54,398:Added Embedding Layer
EntityExtractor_Model INFO 2015-12-24 18:46:54,401:Added Dropout Layer
EntityExtractor_Model INFO 2015-12-24 18:46:54,431:Added LSTM Layer
EntityExtractor_Model INFO 2015-12-24 18:46:54,432:Added Dropout Layer
EntityExtractor_Model INFO 2015-12-24 18:46:54,435:Added LSTM Layer
EntityExtractor_Model INFO 2015-12-24 18:46:54,436:Created model with following config:
{
    "layers": [
        {
            "name": "Embedding", 
            "output_dim": 128, 
            "W_constraint": null, 
            "input_shape": [
                10004
            ], 
            "cache_enabled": true, 
            "init": "uniform", 
            "input_dim": 10004, 
            "mask_zero": true, 
            "W_regularizer": null, 
            "activity_regularizer": null, 
            "input_length": 100
        }, 
        {
            "cache_enabled": true, 
            "name": "Dropout", 
            "p": 0.5
        }, 
        {
            "name": "LSTM", 
            "inner_activation": "hard_sigmoid", 
            "go_backwards": false, 
            "output_dim": 100, 
            "stateful": false, 
            "cache_enabled": true, 
            "init": "glorot_uniform", 
            "inner_init": "orthogonal", 
            "input_dim": 128, 
            "return_sequences": true, 
            "activation": "sigmoid", 
            "forget_bias_init": "one", 
            "input_length": null
        }, 
        {
            "cache_enabled": true, 
            "name": "Dropout", 
            "p": 0.5
        }, 
        {
            "name": "LSTM", 
            "inner_activation": "hard_sigmoid", 
            "go_backwards": false, 
            "output_dim": 6, 
            "stateful": false, 
            "cache_enabled": true, 
            "init": "glorot_uniform", 
            "inner_init": "orthogonal", 
            "input_dim": 100, 
            "return_sequences": true, 
            "activation": "softmax", 
            "forget_bias_init": "one", 
            "input_length": null
        }
    ], 
    "name": "Sequential"
}
EntityExtractor_Model INFO 2015-12-24 18:46:54,436:Compiling model with optimizer adam
EntityExtractor_Model INFO 2015-12-24 18:47:53,062:Model compiled in 58.6263 seconds.
EntityExtractor_Model INFO 2015-12-24 18:48:44,030:Loaded data shapes:
X_train: (39459, 100), Y_train: (39459, 100, 6)
X_test: (9452, 100), Y_test: (9452, 100, 6)
EntityExtractor_Model INFO 2015-12-24 18:48:44,030:Starting Epochs 0 to 1
Train on 39459 samples, validate on 9452 samples
Epoch 1/1
Traceback (most recent call last):
  File "model.py", line 136, in <module>
    model.fit(X_train,Y_train, validation_data=(X_test, Y_test), nb_epoch=save_every, verbose=2)
  File "/usr/local/lib/python2.7/dist-packages/keras/models.py", line 581, in fit
    shuffle=shuffle, metrics=metrics)
  File "/usr/local/lib/python2.7/dist-packages/keras/models.py", line 239, in _fit
    outs = f(ins_batch)
  File "/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py", line 365, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/usr/local/lib/python2.7/dist-packages/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 859, in __call__
    outputs = self.fn()
ValueError: Input dimension mis-match. (input[0].shape[1] = 128, input[1].shape[1] = 100)
Apply node that caused the error: Elemwise{mul,no_inplace}(DimShuffle{x,0}.0, Elemwise{mul,no_inplace}.0)
Toposort index: 330
Inputs types: [TensorType(float32, row), TensorType(int32, matrix)]
Inputs shapes: [(1, 128), (128, 100)]
Inputs strides: [(512, 4), (400, 4)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[Elemwise{true_div,no_inplace}(Elemwise{mul,no_inplace}.0, DimShuffle{x,x}.0)]]

Backtrace when the node is created:
  File "/usr/local/lib/python2.7/dist-packages/keras/models.py", line 85, in weighted
    score_array *= mask

Debugprint of the apply node: 
Elemwise{mul,no_inplace} [id A] <TensorType(float64, matrix)> ''   
 |DimShuffle{x,0} [id B] <TensorType(float32, row)> ''   
 | |Elemwise{true_div,no_inplace} [id C] <TensorType(float32, vector)> 'mean'   
 |Elemwise{mul,no_inplace} [id D] <TensorType(int32, matrix)> ''   
   |Elemwise{second,no_inplace} [id E] <TensorType(int32, matrix)> ''   
   | |<TensorType(int32, matrix)> [id F] <TensorType(int32, matrix)>
   | |TensorConstant{(1, 1) of 1} [id G] <TensorType(int32, (True, True))>
   |Elemwise{sub,no_inplace} [id H] <TensorType(int8, matrix)> ''   
     |TensorConstant{(1, 1) of 1} [id I] <TensorType(int8, (True, True))>
     |Elemwise{eq,no_inplace} [id J] <TensorType(int8, matrix)> ''   
       |<TensorType(int32, matrix)> [id F] <TensorType(int32, matrix)>
       |TensorConstant{(1, 1) of 0} [id K] <TensorType(int8, (True, True))>

Storage map footprint:
 - AdvancedSubtensor1.0, Shape: (12800, 128), ElemSize: 4 Byte(s), TotalSize: 6553600 Byte(s)
 - Elemwise{Cast{float32}}.0, Shape: (128, 100, 128), ElemSize: 4 Byte(s), TotalSize: 6553600 Byte(s)
 - DimShuffle{1,0,2}.0, Shape: (100, 128, 128), ElemSize: 4 Byte(s), TotalSize: 6553600 Byte(s)
 - Subtensor{int64::}.0, Shape: (100, 128, 128), ElemSize: 4 Byte(s), TotalSize: 6553600 Byte(s)
 - Subtensor{:int64:}.0, Shape: (100, 128, 128), ElemSize: 4 Byte(s), TotalSize: 6553600 Byte(s)
 - for{cpu,scan_fn}.0, Shape: (101, 128, 100), ElemSize: 4 Byte(s), TotalSize: 5171200 Byte(s)
 - for{cpu,scan_fn}.1, Shape: (101, 128, 100), ElemSize: 4 Byte(s), TotalSize: 5171200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (10004, 128), ElemSize: 4 Byte(s), TotalSize: 5122048 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (10004, 128), ElemSize: 4 Byte(s), TotalSize: 5122048 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (10004, 128), ElemSize: 4 Byte(s), TotalSize: 5122048 Byte(s)
 - Elemwise{Cast{float32}}.0, Shape: (128, 100, 100), ElemSize: 4 Byte(s), TotalSize: 5120000 Byte(s)
 - DimShuffle{1,0,2}.0, Shape: (100, 128, 100), ElemSize: 4 Byte(s), TotalSize: 5120000 Byte(s)
 - Subtensor{:int64:}.0, Shape: (100, 128, 100), ElemSize: 4 Byte(s), TotalSize: 5120000 Byte(s)
 - Subtensor{int64::}.0, Shape: (100, 128, 100), ElemSize: 4 Byte(s), TotalSize: 5120000 Byte(s)
 - for{cpu,scan_fn}.2, Shape: (100, 128, 100), ElemSize: 4 Byte(s), TotalSize: 5120000 Byte(s)
 - mrg_uniform{TensorType(float32, 3D),no_inplace}.0, Shape: (15360, 6), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
 - <TensorType(int32, matrix)>, Shared Input, Shape: (15360, 6), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
 - mrg_uniform{TensorType(float32, 3D),no_inplace}.0, Shape: (15360, 6), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
 - <TensorType(int32, matrix)>, Shared Input, Shape: (15360, 6), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)
 - for{cpu,scan_fn}.0, Shape: (101, 128, 6), ElemSize: 4 Byte(s), TotalSize: 310272 Byte(s)
 - for{cpu,scan_fn}.1, Shape: (101, 128, 6), ElemSize: 4 Byte(s), TotalSize: 310272 Byte(s)
 - Elemwise{mul,no_inplace}.0, Shape: (128, 100, 6), ElemSize: 4 Byte(s), TotalSize: 307200 Byte(s)
 - for{cpu,scan_fn}.2, Shape: (100, 128, 6), ElemSize: 4 Byte(s), TotalSize: 307200 Byte(s)
 - <TensorType(float32, 3D)>, Input, Shape: (128, 100, 6), ElemSize: 4 Byte(s), TotalSize: 307200 Byte(s)
 - Elemwise{true_div,no_inplace}.0, Shape: (128, 100, 6), ElemSize: 4 Byte(s), TotalSize: 307200 Byte(s)
 - DimShuffle{1,0,2}.0, Shape: (128, 100, 6), ElemSize: 4 Byte(s), TotalSize: 307200 Byte(s)
 - Elemwise{clip,no_inplace}.0, Shape: (128, 100, 6), ElemSize: 4 Byte(s), TotalSize: 307200 Byte(s)
 - DimShuffle{0,1,x}.0, Shape: (128, 100, 1), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(int32, matrix)>, Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - Elemwise{neg,no_inplace}.0, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - Elemwise{mul,no_inplace}.0, Shape: (128, 100), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - Elemwise{Cast{float32}}.0, Shape: (128, 100, 1), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - Reshape{1}.0, Shape: (12800,), ElemSize: 4 Byte(s), TotalSize: 51200 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 100), ElemSize: 4 Byte(s), TotalSize: 40000 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (100, 6), ElemSize: 4 Byte(s), TotalSize: 2400 Byte(s)
 - DimShuffle{x,0}.0, Shape: (1, 128), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)
 - <TensorType(float32, vector)>, Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (100,), ElemSize: 4 Byte(s), TotalSize: 400 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, matrix)>, Shared Input, Shape: (6, 6), ElemSize: 4 Byte(s), TotalSize: 144 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - <TensorType(float32, vector)>, Shared Input, Shape: (6,), ElemSize: 4 Byte(s), TotalSize: 24 Byte(s)
 - Subtensor{int64}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - ScalarFromTensor.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - Constant{2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - Subtensor{int64}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - Constant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - ScalarFromTensor.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - Constant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - TensorConstant{1.0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - Constant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)
 - TensorConstant{(1,) of -1}, Shape: (1,), ElemSize: 8 Byte(s), TotalSize: 8 Byte(s)
 - TensorConstant{(1,) of 0.0}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{(1,) of inf}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{(1, 1) of 1}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{(1, 1) of inf}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - <TensorType(float32, scalar)>, Shared Input, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
 - <TensorType(float32, scalar)>, Shared Input, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
 - <TensorType(float32, scalar)>, Shared Input, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
 - <TensorType(float32, scalar)>, Shared Input, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
 - TensorConstant{(1, 1, 1) of 1e-07}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{inf}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
 - TensorConstant{(1, 1) of 1e-08}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{(1,) of 1e-08}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - DimShuffle{x}.0, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{(1, 1, 1) of 0.0}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
 - TensorConstant{(1, 1, 1) of 0.5}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{(1, 1) of 0.0}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{(1, 1, 1) of 1.0}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{(1, 1, 1) of 1.0}, Shape: (1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)
 - TensorConstant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)
 - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)
 - TensorConstant{(1, 1) of 1}, Shape: (1, 1), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
 - TensorConstant{1}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)
 - TensorConstant{(1, 1) of 0}, Shape: (1, 1), ElemSize: 1 Byte(s), TotalSize: 1 Byte(s)
 TotalSize: 66108452.0 Byte(s) 0.062 GB
 TotalSize inputs: 17646080.0 Byte(s) 0.016 GB
The code of model.py is as follows:
$ cat model.py 
# coding: utf-8
import logging
logger = logging.getLogger("EntityExtractor_Model")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
formatter = logging.Formatter('%(name)s %(levelname)s %(asctime)s:%(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)
logger.info("Started Logger")

import theano, keras
logger.info("Using Keras version %s" % keras.__version__)
logger.info("Using Theano version %s" % theano.__version__)
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, TimeDistributedDense, Flatten, Merge
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM, GRU
from keras.preprocessing.sequence import pad_sequences

import numpy as np
import glob
import json
import time

import preprocess as pp

CONFIG = json.load(open("config.json"))

BASE_DATA_DIR = CONFIG["BASE_DATA_DIR"]
DATA_DIR = "%s/%s" % (BASE_DATA_DIR, CONFIG["DATA_DIR"])
vocab_file = "%s/%s" % (BASE_DATA_DIR, CONFIG["vocab_file"])
boundary_file = "%s/%s" % (BASE_DATA_DIR, CONFIG["boundary_file"])
category_file = "%s/%s" % (BASE_DATA_DIR, CONFIG["category_file"])
BASE_OUT_DIR = CONFIG["BASE_OUT_DIR"]
SAVE_MODEL_DIR = "%s/%s" % (BASE_OUT_DIR, CONFIG["SAVE_MODEL_DIR"])
MODEL_PREFIX = CONFIG.get("MODEL_PREFIX", "model")
maxlen = CONFIG["maxlen"]
num_hidden_layers = CONFIG["num_hidden_layers"]
embedding_size = CONFIG["embedding_size"]
hidden_layer_size = CONFIG["hidden_layer_size"]
RNN_LAYER_TYPE = CONFIG.get("RNN_LAYER_TYPE", "LSTM")
optimizer = CONFIG["optimizer"]
n_epochs = CONFIG["n_epochs"]
save_every = CONFIG["save_every"]

RNN_CLASS = LSTM
if RNN_LAYER_TYPE == "GRU":
    RNN_CLASS = GRU


index_word, word_dict = pp.load_vocab(vocab_file)
pp.WordToken.set_vocab(word_dict = word_dict)
index_boundary, boundary_dict = pp.load_vocab(boundary_file)
index_category, category_dict = pp.load_vocab(category_file)
vocab_size = len(index_word) + pp.WordToken.VOCAB + 1 # Add offset of VOCAB and then extra token for padding
boundary_size = len(index_boundary) + 1 # Add extra token for padding 
category_size = len(index_category) + 1 # Add extra token for padding

logger.info("Parameters: vocab_size = %s, embedding_size = %s, maxlen = %s, boundary_size = %s, category_size = %s, embedding_size = %s, hidden_layer_size = %s" %\
                (vocab_size, embedding_size, maxlen, boundary_size, category_size, embedding_size, hidden_layer_size))

def vectorize_data(filenames, maxlen=maxlen, output_label_size=boundary_size, output_label_dict=boundary_dict):
    X = []
    Y = []
    for i, filename in enumerate(filenames):
        for docid, doc in pp.get_documents(filename):
            for seq in pp.get_sequences(doc):
                x = []
                y = []
                for token in seq:
                    x.append(1 + token.word_index) # Add 1 to include token for padding
                    #y_vec = np.zeros(boundary_size)
                    y_idx = 1 + output_label_dict.get(token.b_label, -1) # Add 1 to include token for padding
                    #y_vec[y_idx] = 1
                    y.append(y_idx) # Add 1 to include token for padding
                X.append(x)
                Y.append(y)
    X = pad_sequences(X, maxlen=maxlen)
    Y = pad_sequences(Y, maxlen=maxlen)
    Y_vec = []
    # Now y should be converted to one hot vector for each index value
    for i in xrange(len(Y)):
        Y_vec.append([])
        for j in xrange(maxlen):
            #Y_vec[-1].append([])
            y_vec = np.zeros(output_label_size)
            y_vec[Y[i][j]] = 1
            Y_vec[-1].append(y_vec)
    X = np.array(X)
    Y = np.array(Y_vec)
    return X, Y

def gen_model():
    logger.info("Parameters: vocab_size = %s, embedding_size = %s, maxlen = %s, boundary_size = %s, category_size = %s, embedding_size = %s, hidden_layer_size = %s" %\
            (vocab_size, embedding_size, maxlen, boundary_size, category_size, embedding_size, hidden_layer_size))
    logger.info("Building Model")
    model = Sequential()
    logger.info("Init Model with vocab_size = %s, embedding_size = %s, maxlen = %s" % (vocab_size, embedding_size, maxlen))
    model.add(Embedding(vocab_size, embedding_size, input_length=maxlen, mask_zero=True))
    logger.info("Added Embedding Layer")
    model.add(Dropout(0.5))
    logger.info("Added Dropout Layer")
    for i in xrange(num_hidden_layers):
        model.add(RNN_CLASS(output_dim=hidden_layer_size, activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True))
        logger.info("Added %s Layer" % RNN_LAYER_TYPE)
        model.add(Dropout(0.5))
        logger.info("Added Dropout Layer")
    model.add(RNN_CLASS(output_dim=boundary_size, activation='softmax', inner_activation='hard_sigmoid', return_sequences=True))
    logger.info("Added %s Layer" % RNN_LAYER_TYPE)
    logger.info("Created model with following config:\n%s" % json.dumps(model.get_config(), indent=4))
    logger.info("Compiling model with optimizer %s" % optimizer)
    start_time = time.time()
    model.compile(loss='categorical_crossentropy', optimizer=optimizer)
    total_time = time.time() - start_time
    logger.info("Model compiled in %.4f seconds." % total_time)
    return model


model = gen_model()

# Read the data
CV_filenames = [glob.glob("%s/%s/*.xml" % (DATA_DIR, i)) for i in range(1,6)]

train_files = reduce(lambda x, y: x + y, CV_filenames[0:4])
test_files = reduce(lambda x, y: x + y, CV_filenames[4:])

X_train, Y_train = vectorize_data(train_files)
X_test, Y_test = vectorize_data(test_files)

logger.info("Loaded data shapes:\nX_train: %s, Y_train: %s\nX_test: %s, Y_test: %s" % (X_train.shape, Y_train.shape, X_test.shape, Y_test.shape))

for epoch in range(0, n_epochs, save_every):
    logger.info("Starting Epochs %s to %s" % (epoch, epoch + save_every))
    start_time = time.time()
    model.fit(X_train,Y_train, validation_data=(X_test, Y_test), nb_epoch=save_every, verbose=2)
    total_time = time.time() - start_time
    logger.info("Finished training %.3f epochs in %s seconds with %.5f seconds/epoch" % (save_every, total_time, total_time * 1.0/ save_every))
    model.save_weights("%s/%s_%s.h5" % (SAVE_MODEL_DIR, MODEL_PREFIX, epoch))