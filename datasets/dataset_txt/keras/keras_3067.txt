KiranBaktha commented on 6 Mar 2017
Hi Guys,
I need to feed custom weights to my embedding layer, so I do it using this line of code:
model.add(Embedding(47747, embedding_vecor_length, input_length=500, weights = [weights],dropout=0.2))
However, I need to prevent the embedding layer from changing these weights, so when I add trainable = False to the code:
Embedding(47747, embedding_vecor_length, input_length=500, weights = [weights],dropout=0.2, trainable = False)
I get the following error:
You called set_weights(weights) on layer "embedding_1" with a weight list of length 1, but the layer was expecting 0 weights. Provided weights: [array([[-2.07139611, 0.38602829, -0.56659412, .....
Kindly help me correct the issue. Thanks in advance.