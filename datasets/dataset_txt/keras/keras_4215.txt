tivaro commented on 8 Oct 2016 â€¢
edited
In my complex model (build using the functional API), I have a dense layer, that I want to repeat over the middle dimension of a 3D input. The TimeDistributed layer is perfect for this task (funny thing is that in my case it has nothing to do with time ;), but still).
I want to use dropout, but the dropout will also have to be distributed over the middle dimension. I think the correct behaviour would be that the same datapoints are 'dropped out' at each 'timestep'. (I think it wouldn't make sense to have the same weight at each timestep, but random dropout for that weight).
I came up with the following implementation:
data = np.random.rand(2, 5, 3)

input_layer = Input(name='atom_inputs', shape=(5, 3))
output1 = TimeDistributed(Dropout(0.2))(input_layer)
output2 = TimeDistributed(Dense(2))(output1)
model = models.Model(input=[input_layer], output=[output2])
Does dropout here actually behave the way I intended?
I tried to investigate this with the above toy example using theano functions (as in here).
Unfortunately, I cannot get the output at training time, because TimeDistributed doesn't have a get_input method. I want to make sure to get the output at training time, so I do not know if there are other functions that I could use for that.