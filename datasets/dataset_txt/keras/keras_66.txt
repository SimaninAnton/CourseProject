tonmoyborah commented on 13 Sep 2019
Hi, I am not sure if this is a bug/ feature, more of a general issue. I recently came to know of a feature in pytorch Autograd where we can pass no_grad as described here. I read somewhere that pytorch calculates gradients even during inference and ends up consuming more than 1GB GPU RAM due to it. This feature allows users to stop this and save space (I don't use pytorch so not 100pc sure). I am looking around Keras to find if Keras does this too? And is there anything we can do to stop this? (Yet to find something). Any pointers would be appreciated.
Thanks!