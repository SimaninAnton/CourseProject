planaria158 commented on 29 May 2016
Hi All,
I've been experimenting with autoencoders for reconstructing input images. Had good success with MNIST digits. Now wanting to try CIFAR10 images.
At the end of this message is the network I've coded up.
In this network below, I've experimented with:
number of filters (64, 32, 16, 8)
weight initialization for the Conv layers.
activation functions in the decoding layers.
overall depth of the network.
Sometimes I get nan's for loss. More recently, this example below returns negative values for loss and has a great deal of trouble converging to anything (accuracies are only about 0.003! after 100 epochs and loss is large negative value).
Some other details:
Training set is only 100 images
Running on my Macbook pro (El Capitan)
Theano backend
Using cpu (not gpu)
Keras 1.0.3
Theano 0.8.2
.theanorc: [global] floatX=float64 (I've tried both float32 and float64) device=cpu optimizer=fast_run (fast_compile tends to create NANs) mode=FAST_RUN exception_verbosity=high
Here is my fitting and the first 2 epochs:
autoencoder.fit(x_train, x_train, nb_epoch=2,  batch_size=20, 
          shuffle=True, verbose=1, validation_data=(x_test, x_test))

Train on 100 samples, validate on 20 samples
Epoch 1/2
100/100 - 0s - loss: -10.1022 - acc: 0.0515 - val_loss: -49.2346 - val_acc: 0.0464
Epoch 2/2
100/100- 0s - loss: -67.9034 - acc: 0.0470 - val_loss: -111.7369 - val_acc: 0.0464
It basically moves very little from what you see above (even after a few hundred epochs). Also, the negative loss values (based on log loss) trouble me since I believe the log loss value should always be > 0.
Visualizing what the network predicts shows blank images.
Here is the complete iPython notebook of my network:
CIFAR10.ipynb.zip