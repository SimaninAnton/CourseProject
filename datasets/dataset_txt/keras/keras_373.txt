AlexBarbera commented on 21 Feb 2019
This proposal means to add the capability to dynamically change the batch size while training a model. As this paper presented in the ICLR 2018 conference shows, many tasks can benefit from this kind of methodology.
It proposes that as training goes on, a method that speeds up training without losing accuracy is to periodically increase the batch size of the training set. It proves that not only it reaches loss values almost identical to training without it, but the increase of batch size speeds up the training significantly.
This is a really useful feature both to people that are starting to learn about machine learning and want to experiment on diverse datasets to more professional applications with uneven datasets.