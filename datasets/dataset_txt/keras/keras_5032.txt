stifflerhe commented on 4 Jun 2016
Hey guys!
Based on this paper, I implemented a attention GRU layer, which is based on the current GRU layer, and this is the gist. It seems to be able to run in this example,however, the compile time of the test code is extremely long and when I use this layer in a more complex model( say a encoder -decoder model), it seems to take forever to compile.
Any idea how to solve this ? Am I doing something wrong when I implement this layer ï¼Ÿ
@fchollet
By the way, the arg hidden_input of the init function is the hidden output of the RNN encoder, because I don't know how to define a multiple input for the GRU layer , I just code like this, could this be the problem?
THANKS !!!!!!!!!!!!