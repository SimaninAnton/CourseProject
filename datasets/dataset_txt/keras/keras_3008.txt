mpgussert commented on 13 Mar 2017 â€¢
edited
Greetings all!
I'm having a bit of trouble implementing a custom loss function for a naive actor critic (no entropy term). My critic model works fine, it's my actor model that's giving me trouble. The loss is always NaN and I don't see why.
my model returns a vector, 1 row and 2 columns, representing probabilities. For example, as a numpy array it might look like...
policy = numpy.array([0.71720082, 0.28279918]).reshape(1,2)
the loss I want to minimize is the negative of the inner product of the log of that vector and something called the baseline. the baseline is a row vector with zeros everywhere except for the one location (representing the action taken by the agent). as a numpy array it might look like
baseline = numpy.array([ 0.0 , 1.17142606]).reshape(1,2)
in numpy my loss is calculated as
loss = -np.sum(np.log(policy)*baseline)
and I get 1.47953249009.
I implemented my loss function for a keras model in the following way
def policyLoss(self, y_pred, y_true):
    loss = K.dot(K.log(y_pred),K.transpose(y_true))
    return -loss
I compile the model with this loss (no errors), and when I train I train on a batch of exactly 1 every time
ActorLoss = actor.train_on_batch(state.reshape(batch_shape), baseline)
the shape of the state and the target are correct (I already checked), but ActorLoss always returns NaN.
can you see why? I can't...
the example policy above is the actual policy prediction for state and the baseline is same. that is, the numbers given here are the actual values returned by my code that actually produces nan. y_true = [ 0.0 , 1.17142606] and y_pred = [0.71720082, 0.28279918], and I get NaN from PolicyLoss when it is called from within train_on_batch.
Halp?
Thanks!