lomizandtyd commented on 18 Mar 2016
Please make sure that the boxes below are checked before you submit your issue. Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
[ 1 ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
I was using Keras 0.1.2 after Sep. 2015, for some personal code I added.
I'm planning to move into the up-to-date version..
I'm new to github..
So I run the imdb - lstm example.
the result (remove dropout in the imdb_lstm.py on Keras 0.3.2)
`Epoch 13/15
20000/20000 [==============================] - 75s - loss: 0.0153 - acc: 0.9953 - val_loss: 0.9700 - val_acc: 0.8210
Epoch 14/15
20000/20000 [==============================] - 75s - loss: 0.0075 - acc: 0.9978 - val_loss: 1.2623 - val_acc: 0.8158
Epoch 15/15
20000/20000 [==============================] - 75s - loss: 0.0150 - acc: 0.9947 - val_loss: 0.8372 - val_acc: 0.8150
5000/5000 [==============================] - 6s
Test score: 0.837204417372
Test accuracy: 0.815
`
the result (remove dropout in the imdb_lstm.py on Keras 0.1.2)
`
Epoch 12
20000/20000 [==============================] - 43s
loss: 0.0205 - acc: 0.9931 - val_loss: 0.9505 - val_acc: 0.8154
Epoch 13
20000/20000 [==============================] - 43s
loss: 0.0115 - acc: 0.9966 - val_loss: 0.9474 - val_acc: 0.8224
Epoch 14
20000/20000 [==============================] - 43s
loss: 0.0050 - acc: 0.9987 - val_loss: 1.1320 - val_acc: 0.8250
5000/5000 [==============================] - 3s
Test score: 1.13196573572
Test accuracy: 0.825
`
the average time of 0.3.2 is 75s,
while 0.1.2 is 43s.
Here is the model code in 0.1.2.
I changed the Embedding Layer, and
`print('Build model...')
model = Sequential()
model.add(Embedding(max_features, 128)) # remove input_length
model.add(LSTM(128, 128))
model.add(Dropout(0.5))
model.add(Dense(128, 1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')`
And here is the code in 0.3.2
`print('Build model...')
model = Sequential()
model.add(Embedding(max_features, 128, input_length=maxlen, dropout=0.0))
model.add(LSTM(128, dropout_W=0.0, dropout_U=0.0))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam')
`
did I get wrong in the Embedding Layer? Or something else?
I use Theano -0.8.0 rc 1