IFeelBloated commented on 6 Nov 2018
I have figured out the cause of the previous issue I posted here

well I guess it's more of a tensorflow problem in general but it also has something to do with certain keras functions.
so for a large network with thousands or even more layers (the network might not be that large mathematically since each layer could contain just a small number of parameters, but u get the idea), the overhead of basically anything messing with some graph stuff becomes unacceptable, things like model compilation, XLA optimization, gradient checkpoint, and also the keras "save_weights" functions are of ABYSMAL performance and could take literally DAYS!! even longer than the actual training process on GPU, I noticed that every time something like the aforementioned stuff was being executed, the python script just got stuck, one CPU core skyrocketed to 100% usage and the rest 39 cores were just almost 0% usage, then it would take hours and hours before that thing was done and then we could finally move on to the actual training.
It's honestly just frustrating and unacceptable, any idea to solve this?