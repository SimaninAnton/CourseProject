allentran commented on 22 Sep 2015
I think there is a bug in how batch normalization is implemented.
Let me focus on the mode=0 case.
The gamma and beta parameters are feature-specific (the same across batches or other dimensions like time). They are initiated with shared parameters with the same shape as inputs.
self.gamma = self.init((self.input_shape))
Note that with mini batches, this is too many parameters (you need as many gamma parameters as features, not features x mini batches x other dimensions). And then they are directly applied to the normed inputs
out = self.gamma * X_normed + self.beta
In theory, if you are randomizing the order of mini batches at each iteration, you might learn the same set of parameters across the redundant dimensions but this is probably something you don't want to leave to chance. Lasagne does a nice trick at the final application of the gamma and beta to the normed_inputs by using Theano's add_broadcast(). See https://gist.github.com/f0k/f1a6bd3c8585c400c190#file-batch_norm-py-L92