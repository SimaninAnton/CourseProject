Contributor
mjdietzx commented on 8 Mar 2017
Easy to reproduce this bug when training GANs but probably occurs in other use cases as well. When BatchNormalization is used in the generator of a GAN, combined.train_on_batch fails. It's really weird but for some reason combined.train_on_batch does not calculate the loss correctly when batch norm is used in the generator. I tested the loss by hand doing something like:
loss = combined.train_on_batch(x, y)
combined_pred = combined.predict(x)
loss_check = K.eval(custom_loss(y, combined_pred))
>>> loss != loss_check
You can reproduce this in wGAN branch here: https://github.com/wayaai/GAN-Sandbox and uncomment the BatchNorm layer in the generator.
1