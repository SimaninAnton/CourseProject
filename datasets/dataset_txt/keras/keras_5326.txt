huula commented on 28 Apr 2016
I'm training a model using stateful LSTM on tensorflow backend, and see some suspicious memory leaks too.
I trained my model using 10,000 batch size at first, after around 20 iterations, OOM exception.
Then I lowered the batch size to 2,000, after around 30 iterations, OOM again.
My GPU memory always shows 95% full when I'm not fitting the model.
Any quick way to work around this?