inspiralpatterns commented on 13 May 2017
I have a structure made of several of such convolutional layers:
# encoding structure
x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.01),
activity_regularizer=SparseReg(beta=5e-1,p=1e-2),name='lay1')(input_img)
x = LeakyReLU(alpha=1e-1)(x)
and when I inspect the loss values I see all the time
loss: nan
When using activation='relu' inside the layer, for instance, I get normal values.
Does anyone have any similar experience when using an advanced activation layer?