hr0nix commented on 24 Nov 2015
Hi! First of all, thanks for building this great framework! I really like how simple to understand and yet powerful it is!
Now to my issues. I've been trying to implement a conventional lstm-based language model that tries to predict the next character in a sequence (essentially takes a sequence padded with a space on the left as an input and attempts to output the same sequence). The structure of my model is as follows:
    model = keras.models.Sequential()
    model.add(keras.layers.embeddings.Embedding(input_dim=word_count, output_dim=embedding_dim,
              input_length=max_text_len, mask_zero=True))
    model.add(keras.layers.recurrent.LSTM(output_dim=lstm_dim, return_sequences=True,
              truncate_gradient=bptt_steps))
    model.add(keras.layers.core.Dropout(0.2))
    model.add(keras.layers.core.TimeDistributedDense(output_dim=word_count))
    model.add(keras.layers.core.Activation('softmax'))
If I run learning for this model on a GPU, the training speed is really disappointing compared to a very similar model that outputs a class instead of a sequence (the difference is 10-fold). Now, I don't know much about the internals of theano, but from the profile it seems to me that some of the difference in the running time comes from heavy operations (Elemwise, AdvancedIncSubtensor) being run on CPU instead of GPU. Can you please advice on whether my suspicions are correct and, if so, how can the model be adjusted to improve training performance?
Thanks in advance!