holderm commented on 30 Sep 2015
I use embedded reber grammar for experimenting around with the LSTM layer.
Here is my model:
model = Sequential()
model.add(LSTM(7, 10, activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributedDense(10, 7))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop')

model.fit(Xtrain, ytrain, nb_epoch=250, batch_size=1,show_accuracy=True, verbose=2)
score = model.evaluate(Xtest, ytest, batch_size=1,show_accuracy=True, verbose=1)
My data features are one-hot decoded and of following shape: (samples,22,7)
I have 1000 samples for training and 250 for testing and the sequence length is limited to 22 while having 7 features (1 for each letter).
During training (model.fit( ... ) ) the accuracy does not go higher than ca. 0.4 while the loss goes down as expected
Epoch 250/250
2s - loss: 0.0339 - acc: 0.4344
Finally I run
print('Test score:', score[0])
print('Test accuracy:', score[1])
and observe
('Test score:', 0.011034337697834417)
('Test accuracy:', 0.41848484848484852)
So is the accuracy not going any higher?