ghost commented on 18 Nov 2016 â€¢
edited by ghost
Hello!
I'm fine tuning a vgg16 model using my custom dataset and i'm getting different values for val_loss and val_acc during training vs after, is this normal behavious or can anyone explain why this is happening?
My code:
train_generator = train_datagen.flow_from_directory(
   train_data_dir,
   target_size=(img_height, img_width),
   batch_size=batch_size,
   class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
   validation_data_dir,
   target_size=(img_height, img_width),
   batch_size=batch_size,
   class_mode='binary')

model_checkpoint = ModelCheckpoint(fine_tuned_weights_path, save_best_only=True, save_weights_only=True, monitor='val_loss')

# fine-tune the model
history = model.fit_generator(
     train_generator,
     samples_per_epoch=nb_train_samples,
     nb_epoch=nb_epoch,
     validation_data=validation_generator,
     nb_val_samples=nb_validation_samples,
     callbacks=[model_checkpoint])

utils.save_graphs(history, loss_graph_path, acc_graph_path)

result = model.evaluate_generator(validation_generator, nb_validation_samples)
print result
Last epoch is:
Epoch 30/30
7929/7929 [==============================] - 1049s - loss: 0.1352 - acc: 0.9897 - val_loss: 0.0774 - val_acc: 0.9823
Result from evaluate generator is:
[0.063121340091613118, 0.98436712052445785]
I also get different results when using the saved weights.
Epoch which weights was saved (best val_loss):
Epoch 22/30
7929/7929 [==============================] - 1055s - loss: 0.1521 - acc: 0.9861 - val_loss: 0.0608 - val_acc: 0.9813
When i evaluate the same model with those weights on the same validation data i get:
[0.078354276560274327, 0.98033282904689867]