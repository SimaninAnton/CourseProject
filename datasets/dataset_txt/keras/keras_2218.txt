artyte commented on 5 Jun 2017
I've been trying to code a grammar corrector program using an encoder-decoder model. This is my model:
model = Sequential()

 # produces the 3d tensors to feed as input into the encoder
model.add(Embedding(input_dim=weights.shape[0],output_dim=output_dim[0],weights=[weights],input_length=200,trainable=False))

# the encoder
model.add(LSTM(output_dim[1])) 

# the summary context vector, repeated to form 3d tensor to feed as input into the decoder
model.add(RepeatVector(repeat)) 

# the decoder
model.add(GRU(output_dim[2], return_sequences=True)) 

#just using some dense layers on each timestep's output
model.add(TimeDistributed(Dense(output_dim[3], activation = 'sigmoid'))) 

model.compile(optimizer=grad_desc, loss=error_calc, metrics=['accuracy'])
model.fit(X_train, y_train, nb_epoch=epochs, batch_size=batch)
return model
These are my hyperparameters:
output_dim = [200, 20, 10, 1]
repeat = 200
max_len = 200
epochs = 5
batch = 32
grad_desc = 'adam'
error_calc = 'binary_crossentropy'
Here is the error:
> Layer (type)                 Output Shape              Param #   
> =================================================================
> embedding_1 (Embedding)      (None, 200, 200)          80000200  
> _________________________________________________________________
> lstm_1 (LSTM)                (None, 20)                17680     
> _________________________________________________________________
> repeat_vector_1 (RepeatVecto (None, 200, 20)           0         
> _________________________________________________________________
> gru_1 (GRU)                  (None, 200, 10)           930       
> =================================================================
> Total params: 80,018,810
> Trainable params: 18,610
> Non-trainable params: 80,000,200
> _________________________________________________________________
> None
> /usr/local/lib/python2.7/dist-packages/keras/models.py:834: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
>   warnings.warn('The `nb_epoch` argument in `fit` '
> Traceback (most recent call last):
>   File "encoder_decoder.py", line 110, in <module>
>     save_model(train_model(weights, X_train, y_train))
>   File "encoder_decoder.py", line 79, in train_model
>     model.fit(X_train, y_train, nb_epoch=epochs, batch_size=batch)
>   File "/usr/local/lib/python2.7/dist-packages/keras/models.py", line 853, in fit
>     initial_epoch=initial_epoch)
>   File "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py", line 1406, in fit
>     batch_size=batch_size)
>   File "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py", line 1300, in _standardize_user_data
>     exception_prefix='model target')
>   File "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py", line 121, in _standardize_input_data
>     str(array.shape))
> ValueError: Error when checking model target: expected time_distributed_1 to have 3 dimensions, but got array with shape (9508, 200)
> 
I am aware that there were similar issues here and here. However, since their first layer (LSTM/GRU) required 3d tensors, their problems were solved by manually reshaping the 2d tensors into 3d tensors. My model requires 2d tensors since it is using an embedding layer as the first layer. The following are what I've tried:
Rehsaping my input into 3d tensors, and I got an error saying embedding layer only accepts 2d.
Changed binary_crossentropy to categorical_crossentropy or sparse_categorical_crossentropy as the loss function, none of them solved the issue.
Removed TimeDistributed layer and the error message changed from time_distributed_1 to gru_1.
I am in a lost of ideas on how to solve this issue. Is there anything that I can do to solve it? Here is my code for reference. Any help is appreciated!