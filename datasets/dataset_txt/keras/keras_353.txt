tmac1997 commented on 7 Mar 2019 â€¢
edited
I pulled latest example code and train cifar_resnet.py (ResNet56V1) with default parameters but just change optimizer from Adam to SGD with lr = 0.1 and momentum = 0.9. However, this simple modification leads to not learning just like below:
I've been monitoring the training process and after first several batches, the training loss suddenly explode. Then the total training ruined.
This is quite odd because many different papers claim that they can get better results using SGDM instead of Adam. And when training cifar10 with ResNet in Pytorch, SGDM works well. Could anyone help solve this problem? I've been implementing experiments in Keras and I don't know whether it's my problem or Keras's.