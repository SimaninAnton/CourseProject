Contributor
cbensimon commented on 21 Apr 2017
Hi,
I was wondering why at line 1039 in layers/recurrent.py, the dropout mask of the LSTM seems to be added only if implementation is 0
if self.implementation == 0 and 0 < self.dropout < 1:
  ...
else: 
  constants.append([K.cast_to_floatx(1.) for _ in range(4)])
After checking, the value of dp_mask[0]at line 1076 is a numpy array whose value is 1.0,
while rec_dp_mask[0]at line 1077 is a tensor
Is it what is expected ?
Thank you,
Charles