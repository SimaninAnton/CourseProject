vgoklani commented on 20 Nov 2016 â€¢
edited
Apologies for asking this question here, but I'm not sure where else to post...
looking at an example like this:
https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py
and specifically this:
model = Sequential()

# we start off with an efficient embedding layer which maps
# our vocab indices into embedding_dims dimensions
model.add(Embedding(max_features,
                    embedding_dims,
                    input_length=maxlen,
                    dropout=0.2))

# we add a Convolution1D, which will learn nb_filter
# word group filters of size filter_length:
model.add(Convolution1D(nb_filter=nb_filter,
                        filter_length=filter_length,
                        border_mode='valid',
                        activation='relu',
                        subsample_length=1))
How is the data from the word embedding getting passed to the Convolution1D layer? I know what a word embedding is, but it's not clear to me how the output is being passed to (or even used by) the next layer? Also, since this model is building the embedding in batches, would it make more sense to take one full pass through the data and build the embedding first, and then use those weights in this loop (with trainable=False), as certain connected words may appear in a later batch?
Thanks!