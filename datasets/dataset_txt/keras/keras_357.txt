lminer commented on 5 Mar 2019
I have a multi-GPU machine and am trying to train different models on different GPUs. I'm trying to set the GPU programmatically rather than use env vars, but am running into issues. Here's my code:
def choose_gpu(gpu_id):
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    with K.tf.device(f'/gpu:{gpu_id}'):
        config = tf.ConfigProto(intra_op_parallelism_threads=10,
                                inter_op_parallelism_threads=10,
                                allow_soft_placement=True,
                                device_count={'CPU': 1, 'GPU': 1})
        session = tf.Session(config=config)
        K.set_session(session)
However, when I try to run the code in two separate jupyter notebooks, using ids 0 and 1, I get the following error:
2019-03-04 10:36:27.210489: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11523260416
2019-03-04 10:36:27.210803: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56311c4bccf0 executing computations on platform CUDA. Devices:
2019-03-04 10:36:27.211230: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value
instead of handling error Invalid argument: device CUDA:0 not supported by XLA service
It works fine if I set CUDA_VISIBLE_DEVICES at the top of the script, but I'd rather not have to manage this. Any sense of what I may be doing wrong? I'm using
keras 2.2.4
tensorflow 1.13.1
ubuntu 18.04
jupyter 5.2.4
jupyter lab 0.35.4