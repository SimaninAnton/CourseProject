fluency03 commented on 12 Apr 2016
I am wondering how the accuracy in training and validation is calculated in sequence learning using RNN? Let's say I have two cases just examples for showing my problems) as following:
(1).
X_train       |    y_train 
ABCDEFGHIJ    |       K 
BCDEFGHIJK    |       L
CDEFGHIJKL    |       M
DEFGHIJKLM    |       N
EFGHIJKLMN    |       O
......
(2).
X_train       |    y_train 
ABCDEFGHIJ    |    BCDEFGHIJK
BCDEFGHIJK    |    CDEFGHIJKL
CDEFGHIJKL    |    DEFGHIJKLM
DEFGHIJKLM    |    EFGHIJKLMN
EFGHIJKLMN    |    FGHIJKLMNO
......
And this is how I understand the accuracy. In general, accuracy=#correct_output_y / #total_y
So, in the first case, as long as the outputted y is equal to the targeted y, then it is correct, where there is only one char to be considered.
However, in the second case, only when all chars in one outputted y are equal to all chars in targeted y, the output will be considered as correct one.
Find the accuracy calculation here:
def categorical_accuracy(y_true, y_pred):
    return K.mean(K.equal(K.argmax(y_true, axis=-1),
                  K.argmax(y_pred, axis=-1)))
But when calculating the loss in the second case, even though most of the chars in one outputted y is correct, which leads to a low loss value, the output y is still considered as wrong when calculating the accuracy.
Am I correct?