LenaMartens commented on 10 May 2017 â€¢
edited
I've been wanting to implement the structured neural network described in this paper. It uses early updates and global normalisation when training the model. SyntaxNet is the implementation of the model described in the paper.
So the model being trained is a simple feed forward network, and would be used to predict a sequence of actions. There are 4 possible actions at any point, so the output layer of the model has a size of 4. The sequence can be predicted through beam search.
Now, as described in the paper, at training time early updates are performed. This means that while decoding the gold sequence, it keeps track of the beam and the location of the golden sequence in the beam. When the golden sequence falls out of the beam, the model is updated with the sequence up until that point. Afterwards it moves on to the next sequence.
On a high level the objective function looks like this (with global normalisation):
-(sum of predictions of actions in golden sequence) + (sum of the sums of predictions of actions in all sequences in the beam)
I've been trying different things to implement this kind of training in Keras, but I feel like I am stuck. Writing my own loss function is not really suitable. I tried to solve it by having a data generator that computes the sum of the golden sequence predictions, the beam and the sum of all predictions in the beam. I then encode these results in the y_true of the data pair, and in my own loss function I use those results. This does not work because Keras computes the gradient of the function with respect to the y_pred, which I do not use in my loss function.
Should I maybe write my own optimizer and construct the gradients myself? Or is this case too complex to be implemented within Keras?
Thank you!