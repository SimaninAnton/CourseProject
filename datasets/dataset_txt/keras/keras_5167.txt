hr0nix commented on 16 May 2016
Hello!
Currently training with sparse_categorical_crossentropy works as follows: it takes a sparse representation of the target, converts it into dense one-hot representation on CPU, copies the resulting dense matrix on GPU and then calls categorical_crossentropy with dense inputs. When training a generative text model with large output vocabulary size (100k+), the time needed to copy a target matrix to GPU can be quite significant (up to 20% in some of my experiments). Given that categorical_crossentropy in theano can take sparse inputs, can we get rid of this inefficiency?