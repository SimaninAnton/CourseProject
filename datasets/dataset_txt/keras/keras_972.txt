Hhhhhhhhhhao commented on 12 Jun 2018 â€¢
edited
I'm currently considering implement SAGAN in keras. Here is my code:
def Attention(X, channels):
    def hw_flatten(x):
        return np.reshape(x, (x.shape[0], -1, x.shape[-1]))

    f = Conv2D(channels//8, kernel_size=1, strides=1, padding='same')(X)  # [bs, h, w, c']
    g = Conv2D(channels//8, kernel_size=1, strides=1, padding='same')(X)  # [bs, h, w, c']
    h = Conv2D(channels, kernel_size=1, strides=1, padding='same')(X)  # [bs, h, w, c]

    # N = h * w
    flatten_g = hw_flatten(g)
    flatten_f = hw_flatten(f)
    s = np.matmul(flatten_g, flatten_f.reshape((flatten_f.shape[0], flatten_f.shape[-1], -1)))  # [bs, N, N]

    beta = softmax(s, axis=-1)  # attention map

    flatten_h = hw_flatten(h)   # [bs, N, C]
    o = np.matmul(beta, flatten_h)  # [bs, N, C]
    gamma = 0

    o = np.reshape(o, X.shape)  # [bs, h, w, C]
    y = gamma * o + X

    return y
Here is the idea of the authors:
But the gamma in my code should be a trainable scalar as described in the original paper: Self-Attention GAN
I don't know how to add a trainable scalar in a block as I implemented. How to initialize such a trainable scalar in keras?