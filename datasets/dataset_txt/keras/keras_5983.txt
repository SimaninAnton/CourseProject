raindeer commented on 8 Feb 2016
Im trying to figure out if it is possible to define a model that combines Convolutional layers with Recurrent layers without specifying the width of the input image/length of the sequence.
For a model that only contains Convolution2D layers we can set the input shape to (1, None, None) to allow it to take images of any height and width. And for an RNN layer we can set the number of timesteps to None (e.g., input shape=(None, 10)).
What I would like to do is to be able to do something like this:
num_classes = 5
input_height = 16
cnn_out_height = input_height
cnn_out_channels = 8

cnn_input_shape = (1, input_height, None)
rnn_input_shape = (None, cnn_out_height * cnn_out_channels)

model = Sequential()
model.add(Convolution2D(cnn_out_channels, 3, 3,
                        border_mode='same',
                        input_shape=cnn_input_shape))

model.add(Permute((3, 2, 1)))
model.add(Reshape(rnn_input_shape))  # does not work

model.add(SimpleRNN(1, return_sequences=True, input_shape=rnn_input_shape))
model.add(TimeDistributedDense(num_labels))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='sgd')
Here the problem is the Reshape layer, that is needed to go from the 4D output tensor of the Convolution2D to the 3D input of the RNN. I basically just want to merge to last two dimensions of the permuted Convolution2D output without caring for the shape of the other two dimensions (batch size and width). Anyone has an idea how to do this?
1