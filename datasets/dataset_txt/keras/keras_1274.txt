stoney95 commented on 30 Jan 2018
Hi,
I've a neural network that looks something like this.
input_layer_1            input_layer_2
      |                        |
      |                        |
  some_stuff           some_other_stuff
      |                       /|
      |     _________________/ |
      |    /                   |
   multiply                    |
      |                        |
      |                        |
   output_1                 output_2
Is there any possibility to cut the connection between some_other_stuff and multiplyduring back-propagation? I was thinking of dropout but that is also applied during forward-propagation
So during back-propagation it should be like two networks:
 input_layer_1            input_layer_2
      |                        |
      |                        |
  some_stuff           some_other_stuff
      |                        |
      |                        |
      |                        |
   multiply                    |
      |                        |
      |                        |
   output_1                 output_2
Output_1 errors only influence weight adjustment in the left part of the network and Output_2 errors only on the right part.
I'm using keras 2.1.3 with tensorflow 1.5, are the any layers or functions that can achieve this?
Thanks.