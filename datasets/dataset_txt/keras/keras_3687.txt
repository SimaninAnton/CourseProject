vyraun commented on 13 Dec 2016 â€¢
edited
Hi
I am trying to extend the babi_memnn example to accomodate multiple hops of a memory network.
It seems like just stacking layers would work but I am getting an error of dimension incompatibility.
Exception: Dimension incompatibility using dot mode: 68 != 64. Layer shapes: (None, 4, 68), (None, 4, 64)
while doing the operation:
match1.add(Merge([response0, question_encoder], mode='dot',dot_axes=[2, 2]))
But response0 is derived from layer of the same dimension as question_encoder. I am missing some detail here, but element-wise operations can't change dimensions, so why is this happening?
Dimensions of embedding layers are not consistent.
Here is the gist. Thanks.