e181978 commented on 8 Mar 2018
Hello,
I used following custom loss function.
def w_categorical_crossentropy(y_true, y_pred, weights):
  nb_cl = len(weights)
  final_mask = K.zeros_like(y_pred[:, 0])
  y_pred_max = K.max(y_pred, axis=1)
  y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))
  y_pred_max_mat = K.cast(K.equal(y_pred, y_pred_max), K.floatx())
  for c_p, c_t in product(range(nb_cl), range(nb_cl)):
      final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])
  return K.categorical_crossentropy(y_pred, y_true) * final_mask
I need to change weight parameters after every epoch.
ncce = functools.partial(w_categorical_crossentropy, weights=w_array)
model.compile(loss=ncce, optimizer=sgd, metrics=['accuracy'])
However, when i call to above codes, optimizers state is lost, therefore training doesn't continue from previous epoch.
Are there any way for updating loss function without effecting training?