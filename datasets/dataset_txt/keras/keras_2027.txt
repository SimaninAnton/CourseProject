mycrazycracy commented on 30 Jun 2017
Hi there, I'm using Keras to train a NN and use the intermediate layer's output the feature. The thing I want to do is to replace the sigmoid activation of the last layer to linear. Because linear activation performs better according to our preliminary experiment.
My code is:
model = keras.models.load_model(xxx)
bn_layer = model.get_layer(index=target_layer)
bn_layer.activation = keras.activations.get('linear')
bn_model = Model(inputs=model.inputs, outputs=bn_layer.output)
But this code seems cannot change the activation function of the last layer. Anyone could help?