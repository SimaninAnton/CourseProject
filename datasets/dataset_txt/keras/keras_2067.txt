TGlas commented on 25 Jun 2017 â€¢
edited
I get an error in fit when combining LSTM, TimeDistributed, and concatenate for time series prediction, but only when using only a single LSTM cell. Minimal example:
import numpy as np
from keras.models import Model
from keras.layers import Input, Dense, LSTM, TimeDistributed, concatenate

# dummy time series prediction data:
# batch size 32, 100 time steps, 3 inputs/outputs
x = np.random.randn(32, 100, 3)

# number of LSTM cells
N = 1   # it works with N > 1

# create a network for predicting the next time step
input = Input(shape=(99, 3))
lstm = LSTM(N, return_sequences=True)(input)
output = TimeDistributed(Dense(3, activation='softmax'))(concatenate([input, lstm]))
model = Model(inputs=input, outputs=output)
model.compile(loss='mse', optimizer='sgd')

# the following command fails:
model.fit(x[:, :-1, :], x[:, 1:, :], batch_size=1, epochs=1)
I get the following error report:
Using Theano backend.
Traceback (most recent call last):
  File "minimal-example.py", line 21, in <module>
    model.fit(x[:, :-1, :], x[:, 1:, :], batch_size=1, epochs=1)
  File "/anaconda/lib/python3.6/site-packages/keras/engine/training.py", line 1458, in fit
    self._make_train_function()
  File "/anaconda/lib/python3.6/site-packages/keras/engine/training.py", line 1002, in _make_train_function
    self.total_loss)
  File "/anaconda/lib/python3.6/site-packages/keras/optimizers.py", line 128, in get_updates
    grads = self.get_gradients(loss, params)
  File "/anaconda/lib/python3.6/site-packages/keras/optimizers.py", line 47, in get_gradients
    grads = K.gradients(loss, params)
  File "/anaconda/lib/python3.6/site-packages/keras/backend/theano_backend.py", line 1136, in gradients
    return T.grad(loss, variables)
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 555, in grad
    grad_dict, wrt, cost_name)
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1317, in _populate_grad_dict
    rval = [access_grad_cache(elem) for elem in wrt]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1317, in <listcomp>
    rval = [access_grad_cache(elem) for elem in wrt]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 967, in <listcomp>
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/anaconda/lib/python3.6/site-packages/theano/gradient.py", line 1108, in access_term_cache
    new_output_grads)
  File "/anaconda/lib/python3.6/site-packages/theano/scan_module/scan_op.py", line 2555, in L_op
    outputs = local_op(*outer_inputs)
  File "/anaconda/lib/python3.6/site-packages/theano/gof/op.py", line 615, in __call__
    node = self.make_node(*inputs, **kwargs)
  File "/anaconda/lib/python3.6/site-packages/theano/scan_module/scan_op.py", line 463, in make_node
    check_broadcast(outer_seq, inner_seq)
  File "/anaconda/lib/python3.6/site-packages/theano/scan_module/scan_op.py", line 434, in check_broadcast
    raise TypeError(msg % (v1.type, v2.type, a1, b1, b2, a2))
TypeError: The broadcast pattern of the output of scan (TensorType(float32, 3D)) is inconsistent with the one provided in `output_info` (TensorType(float32, col)). The output on axis 1 is `False`, but it is `True` on axis 2 in `output_info`. This can happen if one of the dimension is fixed to 1 in the input, while it is still variable in the output, or vice-verca. You have to make them consistent, e.g. using theano.tensor.{patternbroadcast,unbroadcast,addbroadcast}.
Interestingly, using SimpleRNN or GRU instead of LSTM fixes the problem, and so does setting N = 2 or higher. To me, this is a clear indication of a bug, but I don't know whom to blame, keras or theano :) I am using MacOS 10.12.5 (Sierra) and the Anaconda bundle for Python 3.6. Version numbers:
Python 3.6.0 |Anaconda 4.3.1 (x86_64)| (default, Dec 23 2016, 13:19:00)
>>> print(theano.__version__)
0.9.0
>>> print(keras.__version__)
2.0.3
4
2