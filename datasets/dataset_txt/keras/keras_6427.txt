chenych11 commented on 16 Nov 2015
A very import feature needed for word embedding is that word embeddings are updated partially depending on the input indexes, especially when the vocabulary is very large. The current code seems that it updates embeddings of all words in vocabulary for each mini-batch input. We can calculate the gradients w.r.t a subtensor instead of the whole tensor to get this feature, as the official document shows: http://deeplearning.net/software/theano/tutorial/faq_tutorial.html.
There are other mertials also show that updating with subtensor speed up weight update. For example: Theano/Theano#3342
However, it seems that there is no easy way to fullfil this. The problem is that layer.params must be a list of shared variables, (unfortunately, subtensors are not shared variables), otherwise the optimizer would raise an exception. Some tricks can be done to implement this feature, but they introudce dependencies between Layers and Optimizers. What a pity! Any ideas?
5