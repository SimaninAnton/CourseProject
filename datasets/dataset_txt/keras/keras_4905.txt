KarenUllrich commented on 25 Jun 2016 â€¢
edited
Hallo community,
I want to train a regularizer that has trainable weights itself.
My problem is that the regularizers do not support training additional weights and the layers do not support collecting all the weights present in the model. How should I approach this best? Is it possible at all given the current framework?
This is my approach so far: I build the regular model, append a special layer to which the input is the model weights, consequently compile a new model as in my mini example:
InputLayer = Input(shape=(1, img_rows, img_cols,), name='input')
Layers = Convolution2D(nb_filters, nb_conv, nb_conv, border_mode='valid')(InputLayer)
....
Layers = Activation('softmax', name = 'out_loss')(Layers)

intermediate_model = Model(input = [InputLayer], output = [Layers])

RegularizationLayer = SpecialLayer( name = 'out_regularizer')(extract_weights(intermediate_model))

model = Model(input = [InputLayer], output = [Layers, RegularizationLayer])
where I created a function that extracts weights from the model
def extract_weights(model):
    trainable_weights = []
    for layer in model.layers:
        trainable_weights += collect_trainable_weights(layer)
    flattened_weights = []
    for weight in trainable_weights:
        flattened_weights.append(weight.flatten())
    out = K.concatenate(flattened_weights)
    # reshape for batch 
    return  K.reshape(out, shape = (1,out.shape[0], ))`
It works but its rather ugly.
Grateful for good ideas/clarification
K