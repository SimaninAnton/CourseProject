mininaNik commented on 28 Apr 2016 â€¢
edited
I have 2 questions:
I discussed about sequence to sequence learning where the length of input and output are different in problem #2403 and I concluded that I have to use encoder and decoder architecture. I have a fundamental question, what is the reason that keras doesn't support this case and encoder and decoder is needed. Is it a fundamental problem of recurrent neural networks (if it is, can anyone explain this to me?), or is it the Keras problem that doesn't support this case?
2.When I have the following model:
      model = Sequential()
      hidden_neurons = 50 
      model.add(LSTM(hidden_neurons,
             batch_input_shape=(batch_size, n_prev, 1),
             forget_bias_init='one',
             return_sequences=True,
             stateful=True))
     model.add(LSTM(hidden_neurons,
             batch_input_shape=(batch_size, n_prev, 1),
             forget_bias_init='one',
             return_sequences=False,
             stateful=True))

    model.add(Dense(n_nxt))
    model.compile(loss='mse', optimizer='rmsprop')
consider batch_size =1, then which of the following statement is true:
A. All n_prev inputs are fed to all hidden_neurons in the first LSTM layer?
B. At each time only one of in_prev inputs are fed to the first LSTM layer and it will propagate to all other hidden_neurons in this layer?