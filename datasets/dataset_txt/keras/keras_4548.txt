armancohan commented on 23 Aug 2016
I was looking at examples of text classification using Convnets (e.g. imdb_cnn and pretrained_word_embeddings.py). I saw that 1D convolution is used. However, this does not make intuitive sense because we are not considering any local neighboring words in this way. For example if dimension of word embedding is 100 and convolution length is 5, we are only convolving over the emedding dimensions (e.g. [1,...,5], [2,...,6], ...) and not the adjacent words. By contrast, Conv2D can also consider local patterns of neighboring words. So my question is why Conv1D is used in the examples?
6