w1d2s commented on 22 Jul 2016
Hi everyone, I use this theano wrapper for warp-ctc as Keras objective function with a 1 layer lstm RNN, but got following error. My code is attached in the end. Did I make mistakes using this lib? I'm sure it was installed correctly since 2 tests were all passed. I dont know where the problem is and hope someone can help me out. Thanks a lot:D
Epoch 1/5
Traceback (most recent call last):
  File "./lstm_ctc.py", line 66, in <module>
    model.fit(data, label, nb_epoch = 5, batch_size = batch_size)
  File "/home/speech/TRAINDATA1/wudan/iris75dev/lib/python2.7/site-packages/Keras-1.0.5-py2.7.egg/keras/models.py", line 415, in fit
    sample_weight=sample_weight)
  File "/home/speech/TRAINDATA1/wudan/iris75dev/lib/python2.7/site-packages/Keras-1.0.5-py2.7.egg/keras/engine/training.py", line 1151, in fit
    callback_metrics=callback_metrics)
  File "/home/speech/TRAINDATA1/wudan/iris75dev/lib/python2.7/site-packages/Keras-1.0.5-py2.7.egg/keras/engine/training.py", line 870, in _fit_loop
    outs = f(ins_batch)
  File "/home/speech/TRAINDATA1/wudan/iris75dev/lib/python2.7/site-packages/Keras-1.0.5-py2.7.egg/keras/backend/theano_backend.py", line 533, in __call__
    return self.function(*inputs)
  File "/home/speech/TRAINDATA1/wudan/iris75dev/lib/python2.7/site-packages/Theano-0.9.0.dev1-py2.7.egg/theano/compile/function_module.py", line 908, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/home/speech/TRAINDATA1/wudan/iris75dev/lib/python2.7/site-packages/Theano-0.9.0.dev1-py2.7.egg/theano/gof/link.py", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/home/speech/TRAINDATA1/wudan/iris75dev/lib/python2.7/site-packages/Theano-0.9.0.dev1-py2.7.egg/theano/compile/function_module.py", line 895, in __call__
    self.fn() if output_subset is None else\
TypeError: expected a CudaNdarray, not None
Apply node that caused the error: GpuDimShuffle{1,0,2}(gpu_ctc_grad)
Toposort index: 171
Inputs types: [CudaNdarrayType(float32, 3D)]
Inputs shapes: ['No shapes']
Inputs strides: ['No strides']
Inputs values: [None]
Outputs clients: [[GpuElemwise{Composite{((-(i0 * i1)) / i2)},no_inplace}(GpuDimShuffle{1,0,2}.0, GpuElemwise{Composite{exp((i0 - i1))}}[(0, 0)].0, GpuElemwise{sqr,no_inplace}.0), GpuElemwise{Composite{(((i0 / i1) + i2) * i3)}}[(0, 0)](GpuDimShuffle{1,0,2}.0, GpuDimShuffle{0,1,x}.0, GpuDimShuffle{0,1,x}.0, GpuElemwise{Composite{exp((i0 - i1))}}[(0, 0)].0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
My code
#!/usr/bin/env python

from __future__ import print_function
import numpy as np
import keras.backend as K
np.random.seed(1337)

from keras.preprocessing import sequence
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Masking
from keras.layers.wrappers import TimeDistributed
from keras.layers.recurrent import LSTM, GRU, SimpleRNN
from keras.layers.normalization import BatchNormalization
from keras.optimizers import SGD, Adam, RMSprop, Adadelta
from keras.callbacks import LearningRateScheduler

gpuid = 1

from theano.sandbox import cuda
cuda.use('gpu{}'.format(gpuid))

# wrapper of ctc_cost
def ctc_cost(y_true, y_pred):
    '''
    CTC cost:
    a theano wrapper for warp-ctc
    Arguments:
        y_true : label
        y_pred : acts
    '''
    from theano_ctc import ctc_cost as warp_ctc_cost

    # convert (batch size, timestep, target) to (timestep, batch size, target)
    acts = K.permute_dimensions(y_pred, (1, 0, 2))
    labels = K.cast(K.squeeze(y_true, axis=2), 'int32')
    return warp_ctc_cost(acts, labels)

batch_size = 16
frame_len = 80
nb_feat = 120
nb_class = 12250
inner_dim = 512
nb_cell = 1024
print("Building model...")
model = Sequential()
model.add(LSTM(inner_dim, input_shape = (frame_len, nb_feat), return_sequences = True))
model.add(BatchNormalization())
model.add(TimeDistributed(Dense(nb_class)))
model.add(Activation('softmax'))

model.summary()

# Compiling
opt = SGD(lr = 1e-3, momentum = 0.9, nesterov = True)
model.compile(optimizer = opt, loss = ctc_cost, metrics = ['accuracy'],
              sample_weight_mode = None)

# Generate dummy data
data = np.random.uniform(low = -5, high = 5, size = (batch_size, frame_len, nb_feat))
label = np.random.randint(nb_class, size = (batch_size, frame_len, 1))

# Training
model.fit(data, label, nb_epoch = 5, batch_size = batch_size)
1