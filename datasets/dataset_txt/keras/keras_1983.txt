Contributor
5ke commented on 6 Jul 2017
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
I've been training models with an older setup that I'd still like to be able to use after updating my python packages. However, some of the models can be used without errors, but suddenly produce completely different output.
See the following example (models saved here:https://github.com/5ke/FileDumpForIssues ):
import numpy as np
import keras
from keras.models import model_from_json
from keras.optimizers import Adam
model = model_from_json(open('model_withBN_arch.json').read())
model.load_weights('model_withBN_weights.hdf5')
np.random.seed(42)
X = np.random.rand(1,2709)*5
Y = model.predict(X, batch_size = 2048, verbose = 1)
If I run this with my older setup, namely Keras 1.0.5 and Theano 0.8.2, I get sensible output with values between 0 and 5. I also get the exact same output when upgrading Theano to 0.9.0, while keeping Keras at 1.0.5.
However, if I run this script with Keras 2.0.5, I either get an error when running Theano 0.9.0 ('ValueError: epsilon must be at least 1e-5', which is a BatchNormalization issue reported here: #5680), or, when running Theano 0.8.2, I get no errors, but a output with huge negative values (note the e7 on the y-axis):
So far I've only seen this unexpected behaviour with models that have BatchNormalization layers and with all Keras versions newer than 1.1.1. So something happened with the BatchNormalization layers somewhere between Keras 1.0.5 and 1.1.1, which now leaves me with a pile of trained models that I can't use on an updated system.