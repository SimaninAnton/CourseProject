kuchenrolle commented on 6 Aug 2017 â€¢
edited
I'm trying to fit a very simple network one data point at a time, so with a batch size of 1. The code is below, the network is a single dense layer, inputs and outputs are categorical (they come as sequences of indices and are then translated to n_hot vectors) and very sparse (3 out of 10000) and the loss function is the summed squared loss.
It is unbearably slow (1s per update on CPU, it is supposed to run over a million examples easily) and I'm wondering whether I'm missing something. I feel like keras might not be able to figure out that for each batch, the only weights that can change are those from the three present inputs to all outputs, but not from the inputs that are 0. And that maybe there is a way to make that explicit and reduce the number of calculations made.
class OnlineLearner:
def __init__(self, input_data):
    self.batches = self.batch_generator(input_data)

    # structure
    self.network = Sequential()
    self.network.add(Dense(10000, input_dim = 10000, kernel_initializer = "zero", activation = "linear", use_bias = False))

    # compile
    self.network.compile(loss = self.summed_squared_error, optimizer = SGD(lr = 0.01))


# input data is of the form ([input_indices], [output_indices])
# e.g. ([1,3499, 1249], [553, 33, 9621])
def batch_generator(self, input_data):
    for cues, outcomes in input_data:
        cues = to_categorical(cues, num_classes = 10000)
        cues = np.expand_dims(sum(cues), 0)
        outcomes = to_categorical(outcomes, num_classes = 10000)
        outcomes = np.expand_dims(sum(outcomes), 0)
        yield (cues, outcomes)


def learn(self, num_events):
    for i in range(num_events):
        cues, outcomes = next(self.batches)
        self.network.train_on_batch(cues, outcomes)


@staticmethod
def summed_squared_error(y_true, y_pred):
    return K.sum(K.square(y_pred - y_true))/2