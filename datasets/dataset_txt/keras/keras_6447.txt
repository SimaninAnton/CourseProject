rmalouf commented on 12 Nov 2015
I've run into a problem that I'm not sure where to even begin debugging. I'm getting wildly different results when I run the same program on different hardware. It has something to do with the Intel MKL BLAS -- when I use a version of numpy linked with MKL, the results are terrible. To take one example, when I run a particular network using MKL BLAS and device=cpu, the loss starts at around 10 and drifts a but up and down but never consistently decreases . With MKL BLAS and device=gpu, the loss drops quick to 5 and then hovers around that. Using numpy linked with OpenBLAS, the loss steadily decreases from 10 to (after 1500 epochs) about 0.003.
Has anyone reported something like this before? It's not a critical bug for me, since the solution is obvious: just use OpenBLAS. But the community ought to be concerned about this. As I say, I don't know where to begin to isolate this... whether it's a problem with MKL, numpy, theano, keras, the compilers, or some combination of all of the above.