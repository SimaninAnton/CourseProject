felipefariax commented on 12 Nov 2015
Hello!
I was trying to use the BatchNormalization but I don't know if I am using it correctly...
The loss keeps decreasing together with the AUC, but the AUC should be increasing, right?
Below I've pasted the log from my trainings...
Is my model using wrong the BatchNormalization Layers?
{ 'class_mode': 'categorical',
'layers': [ { 'dims': (1, 64, 32),
'input_shape': (64, 32),
'name': 'Reshape'},
{ 'W_constraint': None,
'W_regularizer': None,
'activation': 'linear',
'activity_regularizer': None,
'b_constraint': None,
'b_regularizer': None,
'border_mode': 'valid',
'init': 'glorot_uniform',
'name': 'Convolution2D',
'nb_col': 3,
'nb_filter': 64,
'nb_row': 1,
'subsample': (1, 1)},
{ 'ignore_border': True,
'name': 'MaxPooling2D',
'pool_size': (1, 2),
'stride': (1, 2)},
{ 'epsilon': 1e-06,
'mode': 0,
'momentum': 0.9,
'name': 'BatchNormalization'},
{ 'name': 'Flatten'},
{ 'W_constraint': None,
'W_regularizer': None,
'activation': 'relu',
'activity_regularizer': None,
'b_constraint': None,
'b_regularizer': None,
'init': 'glorot_uniform',
'input_dim': None,
'name': 'Dense',
'output_dim': 128},
{ 'epsilon': 1e-06,
'mode': 0,
'momentum': 0.9,
'name': 'BatchNormalization'},
{ 'W_constraint': None,
'W_regularizer': None,
'activation': 'sigmoid',
'activity_regularizer': None,
'b_constraint': None,
'b_regularizer': None,
'init': 'glorot_uniform',
'input_dim': None,
'name': 'Dense',
'output_dim': 6}],
'loss': 'categorical_crossentropy',
'name': 'Sequential',
'optimizer': { 'epsilon': 1e-10, 'lr': 1.0, 'name': 'Adadelta', 'rho': 0.9},
'theano_mode': None}
<class 'keras.layers.core.Reshape'>: (None, 64, 32) -> (None, 1, 64, 32)
<class 'keras.layers.convolutional.Convolution2D'>: (None, 1, 64, 32) -> (None, 64, 64, 30)
<class 'keras.layers.convolutional.MaxPooling2D'>: (None, 64, 64, 30) -> (None, 64, 64, 15)
<class 'keras.layers.normalization.BatchNormalization'>: (None, 64, 64, 15) -> (None, 64, 64, 15)
<class 'keras.layers.core.Flatten'>: (None, 64, 64, 15) -> (None, 61440)
<class 'keras.layers.core.Dense'>: (None, 61440) -> (None, 128)
<class 'keras.layers.normalization.BatchNormalization'>: (None, 128) -> (None, 128)
<class 'keras.layers.core.Dense'>: (None, 128) -> (None, 6)
Model compiled in 11.9373350143
Base: files/15-11-10_02-46-18/s01/s01
Train: series: [6] X: (119561, 32) batches: 927, samples: 118538
Valid: series: [7] X: (117333, 32) batches: 909, samples: 116310
Test: series: [7] X: (117333, 32) batches: 909, samples: 116310
train set: shape=(119561, 32), min=-13.4839820862, max=67.4118881226, mean=-6.47669153864e-08, std=1.00000023842, inds_shape=(118538,)
valid set: shape=(117333, 32), min=-6.99578380585, max=10.3131809235, mean=0.000355668569682, std=0.839206457138, inds_shape=(116310,)
test set: shape=(117333, 32), min=-6.99578380585, max=10.3131809235, mean=0.000355668569682, std=0.839206457138, inds_shape=(116310,)
Epoch 1/100
118538/118538 [==============================] - 244s - loss: 0.3684 - val_loss: 0.3488
Epoch 00000: val_loss improved from inf to 0.34877, saving model to files/15-11-10_02-46-18/s01/s01_r00.h5
Subject 1 - Simulation 1/20: Epoch 1/100 done in 244.144258976
Losses: train=0.368380062447 valid=0.348767124248
AUC: mean=0.813537445497 classes=[0.83259462754618974, 0.8078269922122252, 0.81799431564730918, 0.77309828869952801, 0.82597633517933744, 0.82373411369763261]
Epoch 2/100
118538/118538 [==============================] - 245s - loss: 0.3037 - val_loss: 0.3175
Epoch 00001: val_loss improved from 0.34877 to 0.31749, saving model to files/15-11-10_02-46-18/s01/s01_r00.h5
Subject 1 - Simulation 1/20: Epoch 2/100 done in 245.650045156
Losses: train=0.303749409789 valid=0.317486396275
AUC: mean=0.808930490834 classes=[0.81868106532245122, 0.79346636640448831, 0.80015476284929954, 0.7701480541141914, 0.83903282431577064, 0.83209987199627622]
Epoch 3/100
118538/118538 [==============================] - 244s - loss: 0.2722 - val_loss: 0.2983
Epoch 00002: val_loss improved from 0.31749 to 0.29834, saving model to files/15-11-10_02-46-18/s01/s01_r00.h5
Subject 1 - Simulation 1/20: Epoch 3/100 done in 244.053970098
Losses: train=0.272202405365 valid=0.298341377159
AUC: mean=0.790205315504 classes=[0.80706063603392975, 0.75129021318085731, 0.75674956318288489, 0.75250099617223021, 0.84026055722171977, 0.83336992723534875]
Epoch 4/100
118538/118538 [==============================] - 244s - loss: 0.2471 - val_loss: 0.2789
Epoch 00003: val_loss improved from 0.29834 to 0.27886, saving model to files/15-11-10_02-46-18/s01/s01_r00.h5
Subject 1 - Simulation 1/20: Epoch 4/100 done in 244.213304043
Losses: train=0.247141240896 valid=0.278859412135
AUC: mean=0.762533747612 classes=[0.79358088654039083, 0.69267690343829291, 0.70756395866502342, 0.72910775409885198, 0.82863391640263695, 0.82363906652491048]
Epoch 5/100
118538/118538 [==============================] - 243s - loss: 0.2266 - val_loss: 0.2703
Epoch 00004: val_loss improved from 0.27886 to 0.27028, saving model to files/15-11-10_02-46-18/s01/s01_r00.h5
Subject 1 - Simulation 1/20: Epoch 5/100 done in 243.933107138
Losses: train=0.226602923641 valid=0.270280247174
AUC: mean=0.74088472054 classes=[0.77883921780203846, 0.64591904734198324, 0.67454675574033229, 0.71675802359429519, 0.81682663429547708, 0.81241864446524947]
Epoch 6/100
118538/118538 [==============================] - 243s - loss: 0.2085 - val_loss: 0.2537
Epoch 00005: val_loss improved from 0.27028 to 0.25368, saving model to files/15-11-10_02-46-18/s01/s01_r00.h5
Subject 1 - Simulation 1/20: Epoch 6/100 done in 243.561411858
Losses: train=0.208544923949 valid=0.253678955325
AUC: mean=0.712563436042 classes=[0.76428585911479952, 0.5989489201669338, 0.63757990182854907, 0.69582932484206705, 0.7946320492408816, 0.78410456105830517]
Epoch 7/100
118538/118538 [==============================] - 243s - loss: 0.1935 - val_loss: 0.2495
Epoch 00006: val_loss improved from 0.25368 to 0.24949, saving model to files/15-11-10_02-46-18/s01/s01_r00.h5
Subject 1 - Simulation 1/20: Epoch 7/100 done in 243.918431997
Losses: train=0.193528926922 valid=0.249486398313
AUC: mean=0.684804960203 classes=[0.75370021827632228, 0.55020451151416427, 0.59394265309756666, 0.67415250074492528, 0.77680955038251243, 0.76002032720290702]
Epoch 8/100
118538/118538 [==============================] - 244s - loss: 0.1809 - val_loss: 0.2463
Epoch 00007: val_loss improved from 0.24949 to 0.24627, saving model to files/15-11-10_02-46-18/s01/s01_r00.h5
Subject 1 - Simulation 1/20: Epoch 8/100 done in 244.569164991
Losses: train=0.180861973762 valid=0.246267953367
AUC: mean=0.664114248466 classes=[0.74229034806081418, 0.51641853603234289, 0.56320211893767491, 0.67571826398035151, 0.75300729674119438, 0.73404892704316693]