Contributor
NickShahML commented on 2 Nov 2015
Hey guys, its a long story: , but basically I want to try to do a softmax over 100k words. https://redd.it/3qyn0m for more info.
Regularly, you use a sparse, boolean matrix to do softmaxes (such as the lstm text gen example)
The problem with 100,000 words is that it makes the matrix huge so that it is almost impossible to store in memory (ram). If you do batch training (loading parts of the matrix) it gets incredibly slow because you are constantly loading and reloading matrices every 15 secs.
So the question is: Can you do a categorical softmax with the output numpy array that has integers instead of 1's and 0's? To be clear, I'm only talking about y_train. For x_train you can do this with the embedding layer. I'm wondering if there's a way to do with it the y_train?
Instead of:
[0, 1, 0, 0]
[0, 0, 0, 1]
[0, 0, 1, 0]
You would do:
[2]
[4]
[3]
And perform a categorical softmax. Thanks alot guys!