shastakr commented on 26 Aug 2016
I am already aware of some discussions on how to use Keras for large dataset like Imagenet such as this, this ... but it doesn't help my situation.
By ImageDataGenerator, I takes 0.3 sec for batch of 32 samples. But my total training sample size is about 700,000 so one epoch takes almost 2 hours! OTL.. I think this time is so long, It is impossible for training deep network such as ResNet.
In order to solve this problem, I found fit_generator and it has nb_worker options. So I understand nb_worker option as "I use nb_worker=23 then 23 process run for preparing batches and other core is training model with GPU". So I use this option but there are no difference time for one epoch with nb_worker=1 and nb_worker=23..
So I have two Questions
Why there are no improvement by nb_worker option? (by ps aux | grep 'python run.py' I checked that there are many process by nb_worker....)
And are there any tips for large scale Image training by Keras? I heard that in tensorflow, there are tf.train.batch methods and it really helps large scale training by parallel data preprocessing(but i do not test it yet.)
6