turambar commented on 14 Jul 2015
Hi Francois, et al., I'm a PhD student in CS/machine learning at USC. We started our DL odyssey using Theano directly and just discovered keras, which has thus far been great, especially for rapidly trying a lot of different models. Thanks for the great work!
I did find what I believe to be a bug in the Autoencoder class, in its handling of tied weights. I noticed after training an autoencoder with tied weights set to true, the actual encoder and decoder weights were NOT identical (which means they WEREN'T actually tied during training). This is because the only place the tie_weights option is used is inside of get_output -- i.e., it just returns the encoder weights as the decoder weights.
Unless I am mistaken, what you want is for the decoder's Theano symbolic variable parameter (e.g., W for a Dense layer) to be the transpose of the encoder's parameter. For example, see the dA class in the Theano tutorial.
I can fix this, but I have two questions:
Do you have a preferred procedure for accepting fixes? I.e., just fork keras, make the changes, send a pull request?
I'm torn about the best way to make the actual fix. I created a new Layer called TiedDense that accepts a Dense "layer" in its constructor and does the "right thing" (sets the W parameter to be equal to layer.W.T, makes an untied bias, only includes the bias in the params list, etc.). For now, I've just been sticking that in a Sequential model and eschewing the Autoencoder class altogether.
Another option would be a to make a class method that takes a (generic) layer and creates a decoder version of that layer with tied weights. With that, one could perhaps build the decoder automatically within the Autoencoder constructor.
Thoughts?
P.S. I've also implemented some Noise layers for use with denoising autoencoders, if those are of interest.