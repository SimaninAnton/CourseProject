X-Wei commented on 5 Dec 2016
Hi all,
I want to implement a very simple lambda layer, for a input vector, I want to scale them by a weight W and then connect to the softmax activation function.
I implemented the functionality this way, the weight W is declared as a K.variable:
import keras.backend as K
from keras.models import Sequential, Model
from keras.layers import Dense, Activation, InputLayer, Flatten, Input, Merge, merge, Reshape
from keras.layers.core import Lambda
N = 11
my_input = Input(shape=(N,), name='input')
def scale(x): 
    w = K.variable(1.0, name='w_g')
    return K.mul(x,w)
def scale_output_shape(input_shape):
    return input_shape

scaled = Lambda(scale, scale_output_shape, name='softmax_scale')(my_input)
my_out = Activation('softmax', name='softmax')(scaled)
gating = Model(input=my_input, output=my_out, name='gating')
This definition works, but I wonder whether the parameter W will be updated at all, as in the model's summary, it's written there's no parameter in this model:
>>> gating.summary()
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input (InputLayer)               (None, 11)            0                                            
____________________________________________________________________________________________________
softmax_scale (Lambda)           (None, 11)            0           input[0][0]                      
____________________________________________________________________________________________________
softmax (Activation)             (None, 11)            0           softmax_scale[0][0]              
====================================================================================================
Total params: 0
____________________________________________________________________________________________________
Or do you have any better way to implement this "scaled softmax" computation ?
Thanks,
3