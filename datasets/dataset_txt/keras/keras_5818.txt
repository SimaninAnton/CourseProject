viksit commented on 6 Mar 2016
I haven't been able to solve a similar problem to #1739. I'm trying to feed an embedding layer into a stateful LSTM, but I end up running into an error like the following,
# input shape: (291, 200, 64)
Model:
model = Sequential()
model.add(Embedding(max_features,
                            embedding_size, # output dim
                            batch_input_shape =(batch_size, maxlen, 64), # 25, 200, 64
                            input_length=maxlen)) # sequence length
model.add(LSTM(50,
                       return_sequences=True,
                       stateful=True))
model.add(LSTM(50,
                       return_sequences=False,
                       stateful=True))
model.add(Dense(1))
model.compile(loss='mse', optimizer='rmsprop')
Error:
Exception: Invalid input shape - Layer expects input ndim=2, was provided with input shape (25, 200, 64)
@fchollet - In your previous comment on the other issue, you suggested putting in the batch_input_shape into the first layer. Is this how you were suggesting?
Thanks.