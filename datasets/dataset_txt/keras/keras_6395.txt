Contributor
dbonadiman commented on 24 Nov 2015
Guys i have a question on the mask behaviour, i don't understand how masks works in Keras.
Now let's assume i have a masked Input (embedding layer with mask zeros) If i want to concatenate over the embedding dimension that layer with another embedding layer (to represent another feature of the word (pos tag?, capitalisation?, ner tag?) with the same mask all the mask information are lost? So if i apply a recurrent layer on top of that it will be unmasked?
Another things is about the Time distributed merge, if i have a mask on my embedding layer i cannot compute the average of the embeddings to lazy represent my input sentence? For two reason actually TimeDistributedMerge do not support masked inputs, and if it can accept mask the mean is computed over all the tensors without taking in consideration the possibility of having variable length inputs. I think that this behaviour is problematic even if we are applying this layer over a recurrent layer.
From my honest opinion the mask should be calculated over the input sequence and broadcasted through the upper layers that have a time dimension that may be specified in the Layer constructor, in that way it is possible to keep track of the masks when needed. Most of the time, masks are lost during model composition without any notification. Another advantage of making clear what the time dimension is about debugging and error checking.
What do you think?
Thanks,
Daniele