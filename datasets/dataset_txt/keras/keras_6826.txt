Contributor
pranv commented on 26 Jul 2015
Hey,
I've been thinking about recurrent networks and how to implement them. I've looked at keras's implementation and some other libraries. What I found (or understood) was that most offer a standard step function - be it LSTM or GRU or something else. One can usually vary the number of inputs and outputs and that's about it.
So I started thinking about how to make arbitrary amount and type of processing per time step, which led me to think about a Recurrent model and the corresponding Recurrent container. The basic idea is simple:
Initialize a Recurrent model object, just like others
Add all required layers
When run, it uses theano.scan over all time steps, except the step function being defined by added layers.
LSTM, GRU and other gating mechanisms would have to be reduced to activation layers (which they are). I think this would also help construction of things like Highway Networks.
I dont have a ton of experience in RNNs. So does this make any sense? Would it be useful? My understanding is that it'd really help support the wide variety of RNN based neural networks cropping up almost everyday. RNNs in 2015 are Conv Nets in 2013.