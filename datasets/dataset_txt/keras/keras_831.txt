NoaKel commented on 7 Aug 2018
I added a metric of precision I wrote to the model. The callback monitors this metric on the validation set and saves the best one according to it (val_precision).
BUT for some reason it only saves the first model (as improved from "-inf").
Here is an example where the val_precision improved but the callback did not save the model.
12822/12822 [==============================] - 533s 42ms/step - loss: 2.6052 - acc: 0.9829 - precision: 0.9850 - val_loss: 3.9388 - val_acc: 0.9579 - val_precision: 0.9551
Epoch 00002: val_precision did not improve from 0.88772
the metric the model is compiled with:
def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true[:] * y_pred[:], 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred[:], 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision
the callback:
 best_model = ModelCheckpoint('checkpoints/best_model.h5', monitor='val_precision', verbose=1,
                                   save_best_only=True, mode='auto', period=1)