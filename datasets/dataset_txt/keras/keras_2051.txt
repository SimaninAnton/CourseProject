Contributor
5ke commented on 28 Jun 2017
I've just updated my system to Keras 2.0.5, Theano 0.9.0, and python 3.5 in a conda virtual environment. I've kept cuDNN 7.5 version 5005, which I have earlier used together with BatchNormalization without problems.
Now, when using a batch_size larger than 1024 together with BatchNormalization, an only slightly adjusted mnist_mlp script (from Keras examples, adjusted to include BatchNormalization, see mnist_mlp_bn.py.txt) gives the error:
Error during batchnorm: CUDNN_STATUS_NOT_SUPPORTED.
Reducing batch_size, or removing BatchNormalization layers fixes the problem.
It looks like pytorch had the same issue, which was fixed: pytorch/pytorch#1004. I'm not sure how they fixed it, but they refer to the 'cuDNN version we're shipping with the binaries' as if updating cuDNN may solve the problem. However, then I don't see how this issue showed up by updating Keras&Theano... (I'm hesitant to update cuDNN, because I'm running Ubuntu 15.10, which doesn't seem to be supported).
Full Traceback:
Traceback (most recent call last):
File "mnist_mlp_bn.py", line 54, in
validation_data=(x_test, y_test))
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/site-packages/keras/models.py", line 876, in fit
initial_epoch=initial_epoch)
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/site-packages/keras/engine/training.py", line 1419, in fit
initial_epoch=initial_epoch)
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/site-packages/keras/engine/training.py", line 1068, in _fit_loop
outs = f(ins_batch)
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/site-packages/keras/backend/theano_backend.py", line 1197, in call
return self.function(*inputs)
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/site-packages/theano/compile/function_module.py", line 898, in call
storage_map=getattr(self.fn, 'storage_map', None))
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/site-packages/theano/gof/link.py", line 325, in raise_with_op
reraise(exc_type, exc_value, exc_trace)
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/site-packages/six.py", line 685, in reraise
raise value.with_traceback(tb)
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/site-packages/theano/compile/function_module.py", line 884, in call
self.fn() if output_subset is None else
RuntimeError: Error during batchnorm: CUDNN_STATUS_NOT_SUPPORTED
Apply node that caused the error: GpuDnnBatchNormInference{mode='per-activation', inplace=False}(GpuContiguous.0, GpuContiguous.0, GpuContiguous.0, GpuContiguous.0, GpuContiguous.0, Constant{0.0010000000475})
Toposort index: 63
Inputs types: [GpuArrayType(float32, (False, False, True, True)), GpuArrayType(float32, (True, False, True, True)), GpuArrayType(float32, (True, False, True, True)), GpuArrayType(float32, (True, False, True, True)), GpuArrayType(float32, (True, False, True, True)), Scalar(float64)]
Inputs shapes: [(1025, 512, 1, 1), (1, 512, 1, 1), (1, 512, 1, 1), (1, 512, 1, 1), (1, 512, 1, 1), ()]
Inputs strides: [(2048, 4, 4, 4), (4, 4, 2048, 2048), (4, 4, 2048, 2048), (4, 4, 2048, 2048), (4, 4, 2048, 2048), ()]
Inputs values: ['not shown', 'not shown', 'not shown', 'not shown', 'not shown', 0.0010000000474974513]
Outputs clients: [[GpuReshape{2}(GpuDnnBatchNormInference{mode='per-activation', inplace=False}.0, MakeVector{dtype='int64'}.0), Shape(GpuDnnBatchNormInference{mode='per-activation', inplace=False}.0)]]
HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
Exception ignored in: <function WeakValueDictionary.init..remove at 0x7f06c045c598>
Traceback (most recent call last):
File "/opt/anaconda/anaconda3/envs/myenv_1/lib/python3.5/weakref.py", line 117, in remove
TypeError: 'NoneType' object is not callable