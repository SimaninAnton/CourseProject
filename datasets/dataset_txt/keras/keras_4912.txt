yataozhong commented on 24 Jun 2016
TimeDistributed works fine if there is only one input as is in this exampe at the bottom of the page. But when there are multiple inputs, TimeDistributed seems not working.
Say, if my model has 3 inputs,
seq_inputs= [Input(shape=(TIME_STEPS, FEATURE_LENGTH)) for i in range(3)] outputs=TimeDistributed(model)(seq_inputs)
the reported error is: TypeError: can only concatenate tuple (not "list") to tuple
So I changed the last to outputs=TimeDistributed(model)(*seq_inputs), but there is still an error saying that TypeError: call() takes at most 3 arguments (4 given)
################# below is my code
from keras.models import Sequential, Model, Graph
from keras.layers import Input, Convolution2D, MaxPooling2D, LSTM, Dense, BatchNormalization, ZeroPadding2D, Flatten, merge, Masking, Dropout, TimeDistributed, Reshape, Lambda, Embedding
from keras import backend
NUM_INPUTS=3
TIME_STEPS=20
model = Sequential()
model.add(Dense(32, input_dim=784))
inputs = [Input(shape=(32,)) for i in range(NUM_INPUTS)]
temps=[model(x) for x in inputs]
merged=merge(temps, mode='concat')
merged_model=Model(input=inputs, output=merged)
merged_model(inputs)
pdb.set_trace()
seq_inputs = (Input(shape=(TIME_STEPS, 32)) for i in range(NUM_INPUTS))
outputs=TimeDistributed(merged_model)(*seq_inputs)
4