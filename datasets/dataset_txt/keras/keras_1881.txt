julie-jiang commented on 22 Jul 2017
I'm trying to create a lambda layer that will perform some deterministic masking (I'm not talking about the Keras Masking layer) before pumping out the final output. This is what I have so far:
def binary_mask(x):
    # Mask is half the size of x. 
    # 1 if pred1 > pred2 element-wise, 0 otherwise.

    pred1 = x(:, :units)
    pred2 = x(: units:)
    mask = K.greater(pred1, pred2)
    mask = K.cast(mask, ,K.floatx())
    return mask

def mask_output_shape(input_shape):
    return (input_shape[0], units)
And this is how I create the layer with the functional API:
outputs = Lambda(binary_mask, output_shape=mask_output_shape)(inputs) 
I'm getting a very weird error coming from my optimizer. It appears that my gradient is None.
However, if I replace my current code in binary_mask with any of the example Lambda layer implementations, it works fine, e.g. return x. What am I doing wrong here?