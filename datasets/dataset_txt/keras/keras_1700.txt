32nguyen commented on 31 Aug 2017 â€¢
edited
I do have the issue with val_loss and loss using fit_generator():
Here is my code:
it_train = data_generator(params.path_train, batch_size=params.batch_size)
it_val = data_generator(params.path_valid, batch_size=params.batch_size)
checkpoint = ModelCheckpoint('3DUnetWeights.h5', monitor='val_loss', verbose=1, save_best_only=True)
unet_hist = models.fit_generator(it_train, steps_per_epoch=params.step_per_epoch_train, epochs=params.epochs, verbose=1,
validation_data=it_val, validation_steps=params.steps_batch_valid,
callbacks=[checkpoint])
step_per_epoch_train = 50
steps_batch_valid = 10
Then I have the log:
50/50 [==============================] - 499s - loss: 7.8276 - acc: 0.8021 - val_loss: 0.8207 - val_acc: 0.8056
Epoch 11/100
49/50 [============================>.] - ETA: 8s - loss: 5.2064 - acc: 0.8076 Epoch 00010: val_loss improved from 0.67726 to 0.55864, saving model to Weights.h5
50/50 [==============================] - 501s - loss: 5.2130 - acc: 0.8081 - val_loss: 0.5586 - val_acc: 0.8054
Epoch 12/100
49/50 [============================>.] - ETA: 8s - loss: 4.4972 - acc: 0.8041 Epoch 00011: val_loss did not improve
50/50 [==============================] - 491s - loss: 4.4658 - acc: 0.8044 - val_loss: 0.6038 - val_acc: 0.8132
So I am not sure if the loss in training and validation is the sum of batches' running or the average. As I see, the loss of training and validation is convergent but it does not make sense when the accuracy is nearly the same but huge difference in loss.