Contributor
wxs commented on 18 Dec 2015
OK so as discussed over in #1206 I think the masking needs to be revisited (I am doing this now).
As part of this investigation I've found an issue with the current masking approach. So even if my preferred solution (masks being passed around) is not accepted, we'll need to solve this.
The issue is, that an all-zero masked out timestep in an RNN still affects the next hidden state, via biases. Look at the SimpleRNN's step function for instance:
    def step(self, x, states):
        # states only contains the previous output.
        assert len(states) == 1
        prev_output = states[0]
        h = K.dot(x, self.W) + self.b
        output = self.activation(h + K.dot(prev_output, self.U))
        return output, [output]
Let's say we're at the first timestep, and it's masked.The input is all 0, so h becomes activation(b).
Now we reach the second timestep, still masked. The input is is all 0. however prev_output is not all 0 anymore, but activation(b) from the previous timestep, so the new output becomes activation(b + K.dot(activation(b), self.U)), etc.
By the time we reach the first unmasked timestep we'll have a garbage prev_output generated by all the previous masked timesteps. What's worse is that this garbage input is a function of how much padding there was, so you will get different results if you later feed inputs with different amounts of padding into the RNN.
Or have I grossly misinterpreted something here?
(this is relevant to #1282 as well)