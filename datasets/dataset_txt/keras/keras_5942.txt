fgadaleta commented on 15 Feb 2016
I am training an LSTM with this language model:
every 20 words, I predict the next one (the 21st).
Therefore I give batches of matrices (20, vocabulary_size) as input x and
a vector of (vocsize) as input y.
The model in keras is this one:
# build the model: 2 stacked LSTM
print('Build model...')
model = Sequential()
model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, vocsize)))
model.add(Dropout(0.2))
model.add(LSTM(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(vocsize))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(X, y, batch_size=128, nb_epoch=100)
How can I add and embedding layer (vocsize, emb_dims) to such a model?
Does keras support this?