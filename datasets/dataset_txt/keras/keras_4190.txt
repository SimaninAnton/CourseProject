mossaab0 commented on 12 Oct 2016
I have 9 models with identical architecture and embedding weights But they differ in the weights of the upper layers. The models are already trained, and I need to use them only for predicting some binary labels through majority voting.
My issue is that these 9 models take a good chunk of memory. Is there a way to keep one copy of the embedding layer (e.g., from model 1), and point the other models to it?