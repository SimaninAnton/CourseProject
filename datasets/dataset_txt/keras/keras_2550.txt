mohammadzzamani commented on 26 Apr 2017
Hi,
I am getting 'loss: nan' when I increase the size of train data from 1000 to 2000 or when I increase the LOOK_BACK from 5 to 10.
Would someone please help me to find out what the issue is?
I have tried different models like, 'adam' optimizer, adding one LSTM layer, adding one Dense layer. But nothing helped so far.
I have also tried normalized train data, and still got nan.
LOOK_BACK = 5
batch_size = 10
model = Sequential()
model.add(LSTM(4, batch_input_shape=(batch_size, LOOK_BACK, 1), stateful=True))
model.add(BatchNormalization())
model.add(layers.core.Dropout(0.2))
model.add(Dense(1))
sgd = optimizers.SGD(lr=0.005, clipnorm=0.1)
model.compile(loss='mean_squared_error', optimizer=sgd)
model.fit(trainX, trainY, nb_epoch=10, batch_size=batch_size, verbose=1, shuffle=True)