Contributor
jfsantos commented on 15 Mar 2016
I have a model using LSTMs where I implemented backpropagation through time by using stateful LSTM layers and calling model.reset_states() every time I finish feeding the model a sequence of batches that corresponds to a whole, larger sequence. Each of my original sequences was 450 timesteps long and I was dividing them into 9 segments with 50 timesteps each, so I was calling model.reset_states() after every 9 batches. I noticed a considerable delay when using the Tensorflow backend every time I needed to reset the LSTM inner states (i.e., during training, there was a small delay of approximately 1 second after every time I called model.reset_states(), which does not happen with the Theano backend). I guess this is probably related to unrolling vs. scan.
Am I doing something wrong or is this expected?