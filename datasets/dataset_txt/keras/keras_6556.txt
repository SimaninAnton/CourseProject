Contributor
dbonadiman commented on 14 Oct 2015
I just experienced a problem concatenating the output of two embedding layer
model = Graph()
model.add_input(name='input1', input_shape=(1,), dtype=int)
model.add_input(name='input2', input_shape=(1,), dtype=int)
model.add_node(Embedding(1000, 50, input_length=5), name='we', input='input1')
model.add_node(Embedding(500, 25, input_length=5), name='f1e', input='input2')
model.add_node(Flatten(), name='flat', inputs=['we', 'f1e'])
model.add_node(Dense(256, activation='tanh'), name='hid', input='flat')
model.add_node(Dense(len(labels), activation='softmax'), name='class', input='hid')
model.add_output('output', input='class')
printing the output shape of each node i obtain:
(None, 5,  50)
(None, 5, 25)
(None, 375)
(None, 256)
with input_shape for the flatten layer as
(None, 5, 75)
so i was pretty sure all was ok.
But i obtained a dimension mismatch error due to the merging layer.
All was fixed by explicitly force the concat_axis=2 (the output_shapes as well as the input shapes results the same but it works at run time).
I was pretty sure that in the embedding case:
concat_axis=-1=2
Am i missing something or it should be considered as bug?