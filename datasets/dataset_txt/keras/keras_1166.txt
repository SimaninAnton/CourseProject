LeZhengThu commented on 14 Mar 2018
Hi, I'm trying to implementing the attention mechanism in my project. However, my sequences have varying lengths and Iâ€™m using bucketing to solve the issue. Therefore, I define the LSTM input shape as (None, None, features). Currently, it seems that every attention implementation using Keras requires a fixed number of timesteps that declared in the input shape. And theoretically, attention should work well with these varying lengths since it's just a softmax regardless of the input length. Is there any way to make a "dynamic_attention" just like the RNN layer, which can accept (None, None, features) as the input shape. Thanks.