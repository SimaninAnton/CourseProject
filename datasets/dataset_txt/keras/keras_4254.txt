fish128 commented on 3 Oct 2016
I am using RNN for sequence classification on a dataset which has value range in [0, 10000]. I trained a RNN on this dataset and got an accuracy around 92%.
Then I normalize the data by dividing 100 so that the value of my data is in the range [0, 100], and train the RNN again. However, the performance (accuracy) dropped by 2% to 90%.
Does anyone know any explanation for this phenomena?
Thanks!