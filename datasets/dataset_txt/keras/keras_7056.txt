Contributor
the-moliver commented on 22 Apr 2015
Hello,
Great job with keras! I wanted to see what you thought about this before I began hacking on it since it would involve some breaking changes.
It seems to me that the regularizers, i.e. maxnorm, L1 and L2, would be more flexible if they were incorporated into the layer definitions, so that different regularization and/or constraints could be applied at each layer if desired. The reason I bring this up is that I wanted to add a non-negativity constraint at a particular layer but there didn't seem to be a straight-forward way to do so.
Let me know any thoughts.
Best,
Mike