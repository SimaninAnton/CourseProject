crthitmo commented on 29 Nov 2016 â€¢
edited
I have built and trained an LSTM classifier which has a single layer of LSTM cells to go from an input of two sequences (time series data) to the classification output.
One feature I would like to implement is to visualize the classification as a function of time, i.e. to get an idea of how many datapoints the classifier needs to be accurate. Is there a way to return the classifier output as a single sample is fed into the network?
If that works, I would want to take it a step further and visualize the individual cell activations as a function of input, e.g. as shown in Andrej Karpathy's blog. Is it possible to collect these for a single sequence or a batch?