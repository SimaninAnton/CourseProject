dc-ai commented on 14 Jan 2017 â€¢
edited
I created a MT-DNN that uses one set of image data to predict 3 properties (and there are 3 classes in each property)
Sample code below:
task_input = Input(shape=(1, 100, 100))
mtdnn = Convolution2D(100, 5, 5, subsample=(2, 2), border_mode="same", init=init)(task_input)
mtdnn = Dropout(0.33)(mtdnn)
mtdnn = Convolution2D(100, 2, 2, subsample=(1, 1), border_mode="same", init=init)(mtdnn)
mtdnn = Flatten()(mtdnn)
mtdnn = Dense(128)(mtdnn)
task_output = Dropout(0.33)(mtdnn)
mtdnn_model = Model(task_input, task_output)

task_all = Input(shape=(1, 100, 100))

out_a = mtdnn_model(task_all)
out_b = mtdnn_model(task_all)
out_c = mtdnn_model(task_all)

predict_a = Dense(3, activation="softmax")(out_a)
predict_b = Dense(3, activation="softmax")(out_b)
predict_c = Dense(3, activation="softmax")(out_c)

model = Model(input=task_all, output=[predict_a, predict_b, predict_c])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
I can run this model fine with:
model.fit(X_train, [y_train_1, y_train_2, y_train_3], nb_epoch=10, batch_size=128, verbose=1)
But when I run with image augmentation:
model.fit_generator(datagen.flow(X_train, [y_train_1, y_train_2, y_train_3], batch_size=128), nb_epoch=10, samples_per_epoch=X_train.shape[0], verbose=1)
I get an error that the dimensions of X_train and y_train_N do not match:
ValueError: X (images tensor) and y (labels) should have the same length. Found: X.shape = (7116, 1, 100, 100), y.shape = (3, 7116, 3)
Is there any workaround to this? Considering that the image augmentation only applies to X, the number of labels y should not need to match? Thanks for the help.