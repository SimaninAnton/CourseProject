lhk commented on 9 Jun 2017
After training for some time, my loss becomes nan.
I don't know why. I'm creating training samples on the fly in a non-deterministic way and it is rather hard to reproduce the problem.
I would like to look at the batch that has provoked the nan loss.
The proper way to do this seems to be a callback, but the logs only provide loss, batch number and batch size.
Is there also a way to get the input to the network and the loss function ?