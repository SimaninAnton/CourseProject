Contributor
tboquet commented on 27 Aug 2015
Hello all,
I have tried to look in Keras models.py but I'm not experienced enough to answer this question.
I'm not sure if it's the expected behaviour and if I understood well how to use validation_data but I get different results for loss and val_loss using the same data.
Train on 100000 samples, validate on 100000 samples
Epoch 0
100000/100000 [==============================] - 144s - loss: 0.8586 - val_loss: 0.8122
Epoch 1
100000/100000 [==============================] - 149s - loss: 0.8168 - val_loss: 0.8112
Epoch 2
100000/100000 [==============================] - 149s - loss: 0.8095 - val_loss: 0.8107
Epoch 3
100000/100000 [==============================] - 149s - loss: 0.8032 - val_loss: 0.8102
Epoch 4
100000/100000 [==============================] - 151s - loss: 0.8001 - val_loss: 0.8069
Epoch 5
100000/100000 [==============================] - 150s - loss: 0.7941 - val_loss: 0.8022
Epoch 6
100000/100000 [==============================] - 144s - loss: 0.7898 - val_loss: 0.7992
Epoch 7
100000/100000 [==============================] - 145s - loss: 0.7872 - val_loss: 0.8005
Here is my code:
X_1 = pd.read_pickle("X_1.pkl")
X_2 = pd.read_pickle("X_2.pkl")
y_train = pd.read_pickle("y_train.pkl")

left = Sequential()
left.add(Embedding(len(X_1), 64))
left.add(LSTM(64, 64,
              forget_bias_init='one'))
left.add(Dropout(0.1))
left.add(Dense(64, 64))

right = Sequential()
right.add(Masking(mask_value=0.))
right.add(LSTM(X_train.shape[2], 64, 
               forget_bias_init='one',
               return_sequences=False))
right.add(Dropout(0.1))
right.add(Dense(64,64))

model = Sequential()
model.add(Merge([left, right], mode='sum'))

model.add(Dense(64, 1, W_regularizer=l1l2(0.01)))
model.add(ParametricSoftplus(1))

model.compile(loss="poisson_loss", optimizer='rmsprop')

components = model.fit([X_1, X_2], y_train, batch_size=128,
                       nb_epoch=20, validation_data=([X_1, X_2], y_train),
                       shuffle=False)
Thank you!
5