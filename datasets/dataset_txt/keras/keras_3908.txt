hassaku commented on 17 Nov 2016
I try to implement the following model called LSDNN.
CONVOLUTIONAL, LONG SHORT-TERM MEMORY, FULLY CONNECTED DEEP NEURAL NETWORKS
http://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/43455.pdf
Input -> Conv(+Pool) -> Dense (to reduce dimension) -> LSTM -> Dense -> Output
model = Sequential()
model.add(Convolution1D(..., input_shape=(timesteps, data_dim)))
model.add(MaxPooling1D(...))
model.add(Dense(...))  # to reduce dimension
model.add(LSTM(...))
model.add(Dense(...))
model.add(Dense(nb_classes, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

model.fit(...)  # Exception: Input 0 is incompatible with layer dense_1: expected ndim=2, found ndim=3
If the first Dense layer is removed, exception not occurs.
But dimension isn't reduced.
How to use Dense layer keeping input shape for LSTM?
Thank you for any help someone can provide.