LCorleone commented on 19 Nov 2018
hi all, this is my own data generator.
def img_generator(img_path_list, label, image_shape=(112, 112, 3), batchsize=32, if_shuffle=True):
    if if_shuffle:
        img_path_list, label = shuffle(img_path_list, label)

    print('total img is {}, and from total subject {}'.format(len(label), len(set(label))))
    # iter
    batch_x = np.zeros((batchsize,) + image_shape, dtype=K.floatx())
    batch_y = np.zeros((batchsize,), dtype=np.int32)
    count = 0
    while 1:
        del batch_x, batch_y
        batch_x = np.zeros((batchsize,) + image_shape, dtype=K.floatx())
        batch_y = np.zeros((batchsize,), dtype=np.int32)
        i_index = 0
        for index in range(count * batchsize, (count + 1) * batchsize):
            img = load_img(img_path_list[index],
                           grayscale=False,
                           target_size=(image_shape[0], image_shape[1]))
            x = img_to_array(img)
            batch_x[i_index] = x
            batch_y[i_index] = label[index]
            i_index += 1

        if count < math.floor(len(img_path_list) / float(batchsize)) - 1:
            count += 1
        else:
            if if_shuffle:
                img_path_list, label = shuffle(img_path_list, label)
            count = 0
        # print('here')
        # print('size of batch x: {}'.format(sys.getsizeof(batch_x)))
        yield (batch_x, batch_y)
And I train my model like this: Batch size is 4ï¼Œvery small.
path_list, label = gen_img_path_label(train_dir)
train_generator = img_generator(path_list, label, batchsize=batch_size, image_shape=(112, 112, 3))
num_train = len(label)
model.fit_generator(train_generator, epochs=num_epochs, steps_per_epoch=num_train // batch_size, callbacks=[checkpoint], max_queue_size=1)
it works fine when my image dataset has 10572 labels of 490000 images. it consumes 3.2G ram.
however, when I use a large dataset, has 73486 labels of 3740000 images. it consumes 28G ram totally!
and it gets stuck.

my question is:
the code is the same, the only difference is the parameters in last dense layer gets much bigger because the label increases much. But, that doesn't matter. the generator only generates batch images, why it consumes so much ram for that I only change the dataset.
is there any ideas?