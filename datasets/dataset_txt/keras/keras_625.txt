faruknane commented on 18 Nov 2018 â€¢
edited
I have the model which gets 3 inputs and gives 1 output using CNTK background. My output data consists of 2 numbers which can be between 0 and 15, so it could be [0,0] [12,8] [3,1] [15,15] etc so I expect two numbers between 0 and 15.
I use MSE as loss func and adam optimizer. when I try to train my model there are two things I ve noticed going on weirdly,
the loss coming from MSE is about 50 at beginning, and when I predict something using my model, according to me the loss of that prediction was about 250 - 1000, I think it s so weird.
after I train model the numbers are so different from what I want to get and train for, like I want numbers between 0 and 15, but the model gives me 1000-1500 as two numbers, why are they so big? and why my loss going to 0 and still it gives wrong numbers
input1 = Input(shape=(256,256,3))

model = Convolution2D(16, kernel_size=(3, 3), activation='relu')(input1)
model = MaxPooling2D(pool_size=(2,2))(model)
model = Dropout(0.2)(model)
model = Convolution2D(32, kernel_size=(3, 3), activation='relu')(model)
model = MaxPooling2D(pool_size=(2,2))(model)
model = Dropout(0.2)(model)
model = Convolution2D(64, kernel_size=(3, 3), activation='relu')(model)
model = MaxPooling2D(pool_size=(2,2))(model)
model = Dropout(0.2)(model)
model = Convolution2D(128, kernel_size=(3, 3), activation='relu')(model)
model = MaxPooling2D(pool_size=(2,2))(model)
model = Dropout(0.2)(model)
model = Convolution2D(156, kernel_size=(3, 3), activation='relu')(model)
model = MaxPooling2D(pool_size=(2,2))(model)
model = Dropout(0.2)(model)
model = Convolution2D(200, kernel_size=(3, 3), activation='relu')(model)
model = MaxPooling2D(pool_size=(2,2))(model)
model = Flatten()(model)

input3 = Input(shape=(256,256,1))

model3 = Convolution2D(16, kernel_size=(3, 3), activation='relu')(input3)
model3 = MaxPooling2D(pool_size=(2,2))(model3)
model3 = Dropout(0.2)(model3)
model3 = Convolution2D(32, kernel_size=(3, 3), activation='relu')(model3)
model3 = MaxPooling2D(pool_size=(2,2))(model3)
model3 = Dropout(0.2)(model3)
model3 = Convolution2D(64, kernel_size=(3, 3), activation='relu')(model3)
model3 = MaxPooling2D(pool_size=(2,2))(model3)
model3 = Dropout(0.2)(model3)
model3 = Convolution2D(128, kernel_size=(3, 3), activation='relu')(model3)
model3 = MaxPooling2D(pool_size=(2,2))(model3)
model3 = Dropout(0.2)(model3)
model3 = Convolution2D(256, kernel_size=(3, 3), activation='relu')(model3)
model3 = MaxPooling2D(pool_size=(2,2))(model3)
model3 = Dropout(0.2)(model3)
model3 = Convolution2D(400, kernel_size=(3, 3), activation='relu')(model3)
model3 = MaxPooling2D(pool_size=(2,2))(model3)
model3 = Flatten()(model3)


input2 = Input(shape=(3,))
model2 = Dense(20, activation='relu')(input2)
model2 = Dense(100, activation='relu')(model2)
model2 = Dense(200, activation='relu')(model2)
model2 = Dense(300, activation='relu')(model2)
model = concatenate([model, model3, model2])

model = Dropout(0.2)(model)
model = Dense(500, activation='relu')(model)
model = Dropout(0.2)(model)
model = Dense(200, activation='relu')(model)
model = Dropout(0.2)(model)
model = Dense(50, activation='relu')(model)
model = Dropout(0.2)(model)
output = Dense(2, activation='linear')(model)

model = Model(inputs=(input1, input2, input3), outputs=output)
 # 8. Compile model
model.compile(loss='mean_squared_error',
              optimizer= keras.optimizers.adam(lr=0.0005, decay=0), shuffle=True, metrics = ['MSE'])

history = model.fit([x_train, x2_train, x3_train] , y_train, 
          batch_size=40, epochs=10, verbose=1)

plt.plot(history.history['acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
Is there some type of bug or am I doing things wrongly?