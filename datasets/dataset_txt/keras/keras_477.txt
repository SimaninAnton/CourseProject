sailormoon2016 commented on 8 Jan 2019 â€¢
edited
The entire code of the model is here:
https://github.com/sailormoon2016/ParticleVideoClassification/blob/master/MLpaper/problem/FiveClasses2Bin.ipynb
You can reproduce this problem with all the files in this folder:
https://github.com/sailormoon2016/ParticleVideoClassification/tree/master/MLpaper/problem
Background of this task and model(I am sorry this is a long story to explain my logic of designing this structure, please jump to the question if you are more comfortable to just read my code):
I am a molecular biology student. I have 5 classes of bacteria trajectory data. The trajectory data is 60 time steps with 2 features each step, but I know these 2 features are not closely related (one is change of speed, another is change of angular speed), so I customized my own model based on this example structure on Keras(search 'shared layers' on Keras functional API guide page (https://keras.io/getting-started/functional-api-guide/). You basically process 2 sequential features separately then concatenate the results together into a shared layer. This works well for this 5-class classification.
But, as biology experimentalists, we are more interested in interpretation rather than just predication. In this 5 classes, there is 1 class is wild-type which means "genetically natural one", the other 4 classes are genetically mutant. What researchers are more concerned with is just to compare each mutant with the wild type (instead of comparing all 5 classes together). Therefore, I did binary classification between each mutant and the wild type again with same architecture just different number of classes and therefore the number of nodes of last layer is different.
To my surprise, the accuracy of binary classification is lower than the 5-class tasks. I searched a bit and found out it does happen that multi-classes classification accuracy is higher than binary classification with same data. This may due to more features from more classes and the model is forced to pay more attention to the details of the features.
So I am wondering, since the trained model for 5-classes already worked well (90-93%, genetically mutant doesn't mean necessarily to behave differently, that is, their trajectory data can be very similar, the ground truth of 5 classes refers to their genetic difference, we don't actually know the ground truth on behavioral level), so I decided to add one more layer on the top of the pretrained 5-classes model so that the model can be turned into a binary task.
The problem
Then the problem happened.....The loss never changes, so I printed the weights and biases of each epoch in the newly added layer which is just the last layer, it just never changes...I read all related topic on github issues(e.g. #255) and stachexchange etc, so far I have tried the following solutions, but none of them works:
normalize the input and each layer
make sure the data both input and output are shuffled as expected.
different optimizer with default parameters' values.
sgd with different scales of learning rates
different initializer.
My code for the model:
x_train_a, x_train_b, y_train, x_val_a, x_val_b, y_val = GetBinData('A4')
n_classes = 2
Initializer=keras.initializers.glorot_normal(seed=None)
optimizer = SGD(lr=0.00001)
json_file = open('model_5classes_dense64.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
base_model = model_from_json(loaded_model_json)
base_model.load_weights("471-0.907-5classes-dense64.hdf5")
x = base_model.output
x = Dropout(0.2)(x)
x = BatchNormalization()(x)
#x = Dense(2, activation='relu', kernel_initializer = Initializer, name='dense_-2')(x)
#x = Dropout(0.2)(x)
#x = BatchNormalization()(x)
predictions = Dense(1, activation = 'softmax', kernel_initializer = Initializer, name='dense_-1')(x)
head_model = Model(input = base_model.input, output = predictions)
for layer in base_model.layers:
layer.trainable = False
head_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
head_model.summary()
##################################### Helpers in callbacks
tb = TensorBoard(log_dir=os.path.join('tensorboard', 'logs'))
early_stopper = EarlyStopping(patience=10)
csv_logger = CSVLogger(os.path.join('logs', str(time.time()) + '.log'))
checkpointer = ModelCheckpoint(filepath=os.path.join('checkpoints','{epoch:03d}-{val_acc:.3f}.hdf5'),
verbose=1,save_best_only=True)
history = History()
callback = Callback()
print_weights1 = LambdaCallback(on_epoch_end=lambda batch, logs: print('layer-1', head_model.layers[-1].get_weights()))
Some more notes:
it doesn't matter if I have the one more dense layer before the softmax dense layer, the weights won't change, so I just comment it out.
the number of epoch in model.fit was 2000, since the weights never change, I set it to 10 to make it faster.
The weights of layers in base_model are frozen, but, I have tried, even if they are not frozen, the weights of newly added layer still won't update.
To reproduce this results, you may need to use or read the files:
data files in ./data/nyp/bin_order/A1_808_bin.npy and ./data/nyp/bin_order/A1_808_bin_y.npy
Steps in function TwoStreamLSTM I used for 5-classes model or base model or pretrained model.
the pretrained model(model_5classes_dense64.json), weights(471-0.907-5classes-dense64.hdf5) or model with weights(model_5classes_dense64.h5) files.
again all files you can find here:
https://github.com/sailormoon2016/ParticleVideoClassification/tree/master/MLpaper/problem
The structure of base_model(5-classes):
Layer (type) Output Shape Param # Connected to
input_3 (InputLayer) (None, 60, 1) 0
input_4 (InputLayer) (None, 60, 1) 0
lstm_5 (LSTM) (None, 60, 32) 4352 input_3[0][0]
lstm_7 (LSTM) (None, 60, 32) 4352 input_4[0][0]
dropout_5 (Dropout) (None, 60, 32) 0 lstm_5[0][0]
dropout_7 (Dropout) (None, 60, 32) 0 lstm_7[0][0]
lstm_6 (LSTM) (None, 32) 8320 dropout_5[0][0]
lstm_8 (LSTM) (None, 32) 8320 dropout_7[0][0]
dropout_6 (Dropout) (None, 32) 0 lstm_6[0][0]
dropout_8 (Dropout) (None, 32) 0 lstm_8[0][0]
concatenate_2 (Concatenate) (None, 64) 0 dropout_6[0][0]
dropout_8[0][0]
dense_3 (Dense) (None, 32) 2080 concatenate_2[0][0]
dense_4 (Dense) (None, 5) 165 dense_3[0][0]
Total params: 27,589
Trainable params: 27,589
Non-trainable params: 0
The structure of head_model(binary):
Layer (type) Output Shape Param Connected to
input_11 (InputLayer) (None, 60, 1) 0
input_12 (InputLayer) (None, 60, 1) 0
lstm_21 (LSTM) (None, 60, 32) 4352 input_11[0][0]
lstm_23 (LSTM) (None, 60, 32) 4352 input_12[0][0]
dropout_21 (Dropout) (None, 60, 32) 0 lstm_21[0][0]
dropout_23 (Dropout) (None, 60, 32) 0 lstm_23[0][0]
lstm_22 (LSTM) (None, 32) 8320 dropout_21[0][0]
lstm_24 (LSTM) (None, 32) 8320 dropout_23[0][0]
dropout_22 (Dropout) (None, 32) 0 lstm_22[0][0]
dropout_24 (Dropout) (None, 32) 0 lstm_24[0][0]
concatenate_6 (Concatenate) (None, 64) 0 dropout_22[0][0]
dropout_24[0][0]
dense_11 (Dense) (None, 64) 4160 concatenate_6[0][0]
dense_12 (Dense) (None, 5) 325 dense_11[0][0]
dropout_5 (Dropout) (None, 5) 0 dense_12[0][0]
batch_normalization_5 (BatchNor (None, 5) 20 dropout_5[0][0]
dense_-2 (Dense) (None, 2) 12 batch_normalization_5[0][0]
dropout_6 (Dropout) (None, 2) 0 dense_-2[0][0]
batch_normalization_6 (BatchNor (None, 2) 8 dropout_6[0][0]
dense_-1 (Dense) (None, 1) 3 batch_normalization_6[0][0]
Total params: 29,872
Trainable params: 29
Non-trainable params: 29,843