Youmna-H commented on 25 Feb 2017 â€¢
edited
I am trying to implement an LSTM that applies a recurrent operation to words in the same sentence independently, then to the sentences in the whole document.
My input shape is accordingly:
sequence = Input(shape=(max_num_sents_in_doc, max_num_words_in_sent, ), dtype='int32')
The first problem I faced is creating the Embedding layer. It takes an input of 2D shape (batch, input_length), while my input here is 3D (batch,max_num_sents_in_doc, max_num_words_in_sent, )
I tried flattening the input before the embedding layer then reshaping it after the embeddings are retrieved, but failed. I also tried using TimeDistributed around my Embedding layer but the masking wasn't passed properly to the next layers.
Ideally what I want to do is something like:
sequence = Input(shape=(max_num_sents_in_doc, max_num_words_in_sent, ), dtype='int32')
embedded = Embedding(vocab_size, emb_dim, mask_zero=True)(sequence)
lstm1 = TimeDistributed(LSTM(lstm_dim, return_sequences=False))(embedded)
dropout1 = Dropout(dropout_prob)(lstm1)
lstm2 = LSTM(lstm_dim, return_sequences=False)(dropout1)
dropout2 = Dropout(dropout_prob)(lstm2)
Thanks