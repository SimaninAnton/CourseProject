gaw89 commented on 31 Mar 2017 â€¢
edited
First off, I recognize that this may not be an actual "issue" and that it may stem from some misunderstanding that I personally have about Keras. If that is the case, I am happy to move this question to SO.
I am trying to implement neural network-based clustering as per this paper:
http://arxiv.org/pdf/1511.06321v5.pdf
I believe I should be able to do this with a Siamese network architecture simply by providing a custom loss function to the Keras model similar to that of a standard Siamese network with contrastive loss. I have implemented a custom loss function using a Lambda layer that should (I think) perform the calculations indicated in the paper however, I am not getting the output I expect.
I have spent a couple weeks trying to sort this out. I have tried both Theano and Tensorflow backends, I have tried multiple datasets, I have tried numerous network architectures, I have also tried following the example here. Right now, I am running it one training sample at a time to see if I can sort it out, and for some reason, I get different cost output from the model's calculation of the loss vs. my own manual calculation.
I am currently running it on Tensoflow 1.0.1 and Keras 2.0.2 on Windows 7 Eneterprise. In this gist:
https://gist.github.com/gaw89/8e7db31ab180609b839d9515a3462563
my expected cost output for 1 training example (by manual calculation) is ~.9694, but what I am seeing is 19.9736. Is this a Keras issue or am I just stupid (<- entirely possible)?