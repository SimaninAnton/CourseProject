JJN123 commented on 24 Jun 2017 â€¢
edited
I compute an AUC ROC score every epoch as follows:
class Histories(keras.callbacks.Callback):
 
def __init__(self, validation_generator = None):
  super(Histories, self).__init__()
  self.validation_generator = validation_generator

 def on_train_begin(self, logs={}):
  self.aucs = []
  self.losses = []

 def on_epoch_end(self, epoch, logs={}):
  self.losses.append(logs.get('loss'))
  valid_steps = np.ceil(self.validation_generator.samples/self.validation_generator.batch_size)
  true_classes = self.validation_generator.classes
  predictions = self.model.predict_generator(generator = self.validation_generator, steps = valid_steps)

  self.aucs.append(roc_auc_score(y_true = true_classes, y_score = np.ravel(predictions)))
  print(self.aucs)
  return
The ROC AUC score's end up fluctuating around 0.5, for example:
[0.50508310249307475, 0.4822398100514444, 0.49138405223585269, 0.49789176889592401, 0.5143925603482391, 0.49754946576968734, 0.50938959240205783, 0.50715967550455088]
For every model and data set combination I have tried running, including those which achieve high validation accuracy and low validation loss, and with balanced classes, I get these poor ROC AUC results. Without looking at more of my code, is there anything wrong with calculating an ROC AUC score as I have above? Training basically follows this tutorial: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
Note: roc_auc_score function is imported from sklearn