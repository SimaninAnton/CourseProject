alzhusan commented on 6 Aug 2015
I found the implementation of the BatchNormalization is wrong. If the mode is 0, "featurewise normalization" is used and two attributes which named running_mean and running_std are used to store the mean and STD of the training data. But both of these attributes are implemented as a Theano expression in the get_output function. In the test stage, they refer to mean and STD of the input data, not the mean and STD of the training data as we expected.
To fix it, the running_mean and running_std should be implemented as a Theano shared value as model's parameters, and should be updated when training. And I noted that the updating mechanism of them is different from the conventional parameters which are updated by their gradient. Therefore, we need to define individual update methods in some especial Layer, and the update list should be integrated into the conventional one in the training function of the Model.