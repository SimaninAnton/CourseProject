Palang2014 commented on 15 Feb 2016
Hi all,
First of all thanks for this wonderful framework and great work you guys have done.
I have been trying to use Keras to build a simple LSTM language model with a vocabulary of size 50,000. Considering one-hot representation of words, this translates into 50,000 classes.
To avoid padding, I have been dividing my training data (~80 million sentences) into chunks where each chunk includes sentences of the same length. Model compiles well and training loss is decreasing but I have two main problems:
Using " np_utils.to_categorical () " function results in huge memory (and computation?) waste. I tried "K.mean(K.categorical_crossentropy(y_pred, K.cast(y_true.flatten(), dtype='int32')), axis=-1)" to avoid it but got a rank mismatch error from Theano. Is there any work around for this?
I guess when the number of classes is huge the running time becomes very high. Is there any solution for this?
E.g., in my language model experiment, with just one LSTM layer and a Softmax layer on the top, 512 cells for LSTM, maximum sentence length of 15, vocabulary size 50,000 and mini-batch size = 1024 sentences, it takes 4 sec / mini-batch on GPU, which is too long.
It also consumes up to 7 GB of GPU memory which I believe is because of " np_utils.to_categorical () " function.
Thanks!
Hamid