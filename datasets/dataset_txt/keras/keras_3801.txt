phdowling commented on 30 Nov 2016
I am trying to implement the ABCNN introduced here, which entails evaluating a word-by-word similarity matrix of two input sentences. If one sentence has 5 words, and the other 8, this matrix should be 5 x 8. Applying a merge layer with mode="dot" is the right functional interface, i.e. it leads to the correct dimensions, but it's incomplete in that I actually need a similarity measure. I figured I should be able to apply the same layer with "cos" mode, but it leads to an error:
  File "/Users/dowling/Development/keras-abcnn/abcnn.py", line 49, in MatchScore
    dot = merge([l, r], mode="cos")
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/topology.py", line 1528, in merge
    name=name)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/topology.py", line 1186, in __init__
    node_indices, tensor_indices)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/topology.py", line 1221, in _arguments_validation
    'Layer shapes: %s' % input_shapes)
Exception: Only layers of same output shape can be merged using cos mode. Layer shapes: [(None, 14, 64), (None, 10, 64)]
This is the main point of this issue - I assumed that cos and dot should be interchangable and handle the same input dims, but that seems not to be the case. Is this desired behavior?
The second part of this issue is more a programming problem, so I'm not sure if here is the right place to ask, but perhaps this is relevant to Keras somehow:
Because the "cos" merge did not work as expected, I tried implementing cosine similarity myself (as well as the "euclidean" similarity introduced in the paper (sim(a, b) = 1 / (1 + euclidean_distance(a, b)))).
This seems to work when I test the functions standalone, but included in the my model architecture they lead to NaN values (weights and predictions) once backprop is applied. For a runnable example, see: https://github.com/phdowling/abcnn-keras/blob/master/abcnn.py. Note that with the abcnn_1 flag set to False (attention only in upper layers), the NaN values only occur on some training runs. I never got NaN values using only a dot merge, even though my own merges are quite similar in their implementation.
Any help is appreciated!