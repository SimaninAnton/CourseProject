ankeshanand commented on 3 Apr 2016
Hi,
I am modelling a simple text classification task with 3 target labels. I am using word2vec embeddings as input to the LSTM layer.
sequence_size is the size of each sequence. dimsize is the word2vec dimension.
X_train is a 3D tensor of the shape (n_samples, sequence_size, dimsize)
y_train is a simple vector of labels like [0,2,1,0,1,0,...]
model = Sequential()
model.add(LSTM(400, input_shape=(sequence_size, dimsize)))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam')

batch_size = 32
model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,
          validation_data=(X_test, y_test), show_accuracy=True)
score, acc = model.evaluate(X_test, y_test,
                            batch_size=batch_size,
                            show_accuracy=True)
But while training this model, I always get accuracy scores of 1 like this:
Epoch 1/3 31168/47226 [==================>...........] - ETA: 2142s - loss: -8.0915 - acc: 1.0000
What could be the issue here?
PS: I am using Keras 0.3.2