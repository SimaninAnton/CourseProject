parmsib commented on 10 May 2017
Hi!
Long story short: I feel like the "stateful" flag of recurrent layers should be an argument of fit, predict etc,
rather than an attribute of the layer.
I've been fiddling about with LSTMs for the past few weeks, and have run into something
I feel is a bit weird/confusing, the (seemingly notorious) stateful flag. If I understand correctly,
it decides whether the state in each LSTM neuron is reset between batches during training
and prediction. If they are not to be reset, every nth sequence in a batch is initialized with the states
obtained after running the nth sequence of the previous batch.
I have a scenario in which I've created a non-stateful LSTM network, and trained it on sequences
of length (window size) 100. Now I'd like to test the network's behavior on an a lot longer sequence. To achieve this without ridiculous memory requirements, using a stateful behavior seems like a good idea. Now the problem is that my network isn't stateful, since it was
not trained as such. In order to to predict in a stateful manner, I need to either create a new model,
with the stateful parameter set to True, load the existing model's weights, and then run the prediction;
or I could iterate through each layer of the existing model and set its statefulness to True. A similar issue
on the topic can be found here #5714
This seems like a weird workaround for the simple goal of having a prediction being done without resetting the states between sequences, after training it in a non-stateful manner.
The stateful flag seems to only alter the behavior of a model's fit and predict, etc, functions. It does not seem like an attribute of the layer to me intuitively.
Is there something I'm misunderstanding? I'm genuinely curious and would be very grateful
if someone could clear this up for me or have a better solution to my problem.
Thanks
1