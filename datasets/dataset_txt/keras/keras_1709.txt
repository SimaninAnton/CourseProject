HaziqRazali commented on 26 Aug 2017
Hello, I am following the mnist tutorial but am unable to get ~99% accuracy even after 12 epochs. I am running the latest version of keras (2.0.7) with tensorflow backend (1.3.0, but the command tf.version gives me an error so i cant verify) on PyCharm via docker and was hoping some of the experts have any ideas on this issue that I am facing. I have the mnist script below and the output I get after 9 epochs.
https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py
'''Trains a simple convnet on the MNIST dataset.
Gets to 99.25% test accuracy after 12 epochs
(there is still a lot of margin for parameter tuning).
16 seconds per epoch on a GRID K520 GPU.
'''

from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
print('keras: ', keras.__version__)

batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=2,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
a39a35636169:python -u /opt/project/mnist_cnn.py
Using TensorFlow backend.
keras:  2.0.7
Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz
   16384/11490434 [..............................] - ETA: 0s
   24576/11490434 [..............................] - ETA: 114s
   40960/11490434 [..............................] - ETA: 137s
   73728/11490434 [..............................] - ETA: 115s
  106496/11490434 [..............................] - ETA: 106s
  180224/11490434 [..............................] - ETA: 78s 
  229376/11490434 [..............................] - ETA: 64s
  245760/11490434 [..............................] - ETA: 68s
  303104/11490434 [..............................] - ETA: 60s
  319488/11490434 [..............................] - ETA: 60s
  385024/11490434 [>.............................] - ETA: 51s
  401408/11490434 [>.............................] - ETA: 54s
  524288/11490434 [>.............................] - ETA: 42s
  540672/11490434 [>.............................] - ETA: 44s
  720896/11490434 [>.............................] - ETA: 34s
  753664/11490434 [>.............................] - ETA: 35s
  835584/11490434 [=>............................] - ETA: 32s
 1032192/11490434 [=>............................] - ETA: 26s
 1064960/11490434 [=>............................] - ETA: 26s
 1204224/11490434 [==>...........................] - ETA: 23s
 1466368/11490434 [==>...........................] - ETA: 19s
 1499136/11490434 [==>...........................] - ETA: 19s
 1515520/11490434 [==>...........................] - ETA: 19s
 1712128/11490434 [===>..........................] - ETA: 17s
 2129920/11490434 [====>.........................] - ETA: 13s
 2162688/11490434 [====>.........................] - ETA: 13s
 2179072/11490434 [====>.........................] - ETA: 13s
 2383872/11490434 [=====>........................] - ETA: 12s
 2695168/11490434 [======>.......................] - ETA: 10s
 3088384/11490434 [=======>......................] - ETA: 9s 
 3137536/11490434 [=======>......................] - ETA: 9s
 3334144/11490434 [=======>......................] - ETA: 8s
 3563520/11490434 [========>.....................] - ETA: 7s
 3940352/11490434 [=========>....................] - ETA: 7s
 5201920/11490434 [============>.................] - ETA: 4s
 6201344/11490434 [===============>..............] - ETA: 3s
 6512640/11490434 [================>.............] - ETA: 3s
 6774784/11490434 [================>.............] - ETA: 2s
 7135232/11490434 [=================>............] - ETA: 2s
 7430144/11490434 [==================>...........] - ETA: 2s
 7692288/11490434 [===================>..........] - ETA: 2s
 7823360/11490434 [===================>..........] - ETA: 2s
 8151040/11490434 [====================>.........] - ETA: 1s
 8544256/11490434 [=====================>........] - ETA: 1s
 8937472/11490434 [======================>.......] - ETA: 1s
 9199616/11490434 [=======================>......] - ETA: 1s
 9510912/11490434 [=======================>......] - ETA: 0s
 9854976/11490434 [========================>.....] - ETA: 0s
10199040/11490434 [=========================>....] - ETA: 0s
10559488/11490434 [==========================>...] - ETA: 0s
10805248/11490434 [===========================>..] - ETA: 0s
11051008/11490434 [===========================>..] - ETA: 0s
11460608/11490434 [============================>.] - ETA: 0sx_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
2017-08-26 06:07:03.646670: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-26 06:07:03.647080: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-26 06:07:03.647105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
474s - loss: 1.9245 - acc: 0.3689 - val_loss: 0.9515 - val_acc: 0.7369
Epoch 2/12
469s - loss: 1.3751 - acc: 0.5627 - val_loss: 0.9359 - val_acc: 0.8066
Epoch 3/12
474s - loss: 1.3306 - acc: 0.5828 - val_loss: 0.8005 - val_acc: 0.8271
Epoch 4/12
483s - loss: 1.3302 - acc: 0.5826 - val_loss: 0.8961 - val_acc: 0.8273
Epoch 5/12
527s - loss: 1.3291 - acc: 0.5813 - val_loss: 0.8284 - val_acc: 0.8321
Epoch 6/12
475s - loss: 1.3124 - acc: 0.5967 - val_loss: 0.7779 - val_acc: 0.8301
Epoch 7/12
494s - loss: 1.3154 - acc: 0.6016 - val_loss: 0.8783 - val_acc: 0.7565
Epoch 8/12
567s - loss: 1.3306 - acc: 0.6149 - val_loss: 0.9024 - val_acc: 0.8350
Epoch 9/12
[✓ ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
[ ✓] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).