franciscoaraposo commented on 29 Jun 2017
Hi,
I was wondering whether it is possible to have RNNs ignore specific timesteps, e.g. padding steps, in the context of an Encoder-Decoder model that translates sequences in a source space to sequences in a target space. I know you can mask the output target sequence steps whose length is the same as the input target sequence (in fact, the sequence is the same but shifted 1 step). However, the input source sequence (of a different length) still needs to be padded and I don't know of any way to have the network ignore that padding.
I've read that the networks are supposed to ignore the padding but when the 1/6 of the sequences is padding (in average) I doubt the network can learn to ignore it, especially if the dataset is not that big.
One alternative is to feed batches of size 1 to the network and not specify a fixed number of steps (i.e., stop unrolling the recurrent cells) but I found that takes so much longer to train.
Can anyone point to an alternative?