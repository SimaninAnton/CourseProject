einareinarsson commented on 28 Feb 2017
I find the concept of seq2seq Neural Machine Translation fascinating, and albeit I understand the high level logic of the RNN based word2vec-encoder-decoder schema, but I struggle to implement it in Keras. My main problem is how to combine the encoder and the decoder (what is the output of the former and the input of the latter), and how could you tune/refine your encoder when the desired output (translated sentence) is only the product of the decoder. So the actual implementation is a bit too complex for me.
Moreover, I don't understand the concept of "1d convolution" for NLP what seems to be a usual part of the pipeline.