liyi193328 commented on 27 Dec 2015
I'm a theano and keras fresher, and want to learn them , which I think very interesting and helpful.
The following question confuses me about for one week. But I can't work it out after try some ways mentioned before.
I want to do sentiment analysis for texts to three classes. And I train word2vec(dim = 600) with gensim.
My train data is 10475 sequences in different length. label shape is [10475,3] After setting maxlen of sequence 200, every sequence are converted to 200*600 2D array.If some sequence's length is less than 200, then the remaining values is filled with 0(padding), resulting some rows are all zeroes. And then I feed them into LSTM,
LSTM code as following:
    sgd = SGD(lr=0.001, decay = 1e-6, momentum=0.9, nesterov=True, clipnorm=0.3)
    rmsprop = RMSprop(clipnorm=0.1,epsilon=5e-04)
    adam = Adam(epsilon=1e-03,clipnorm=0.1)
    model = Sequential()

    model.add(LSTM(output_dim=300,input_length=200,input_dim=600)) 
#     model.add(Dropout(0.5))
    model.add(BatchNormalization(epsilon=1e-04))
    model.add(Dense(nb_classes))
    model.add(Activation('softmax'))
    model.compile(loss='mean_squared_error',
                  optimizer='adam', class_mode="categorical")
model.fit(train,label,batch_size=100,nb_epoch=4,verbose=1,shuffle=True,validation_split=0.1,show_accuracy=True)
But Getting:
loss: nan
Train on 9430 samples, validate on 1048 samples
Epoch 1/4
9430/9430 [==============================] - 99s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 2/4
9430/9430 [==============================] - 96s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 3/4
9430/9430 [==============================] - 96s - loss: nan - acc: 0.2992 - val_loss: nan - val_acc: 0.1355
Epoch 4/4
1600/9430 [====>.........................] - ETA: 75s - loss: nan - acc: 0.3038
I test different optimizer,also improve epsilon value, set clipnorm(in optimizer above) and different loss functions('mean_squared_error', 'categorical_crossentropy') and so on, but failed.
Also in cpu or gpu mode, loss value is also nan.
##Even I switch to Convolution2D:
    nb_feature_maps = 120
    n_gram = 10
    model.add(Convolution2D(nb_filter = nb_feature_maps, nb_row=n_gram, nb_col=600,input_shape=(1,200,600)))
    model.add(Activation('relu'))

    model.add(MaxPooling2D(pool_size=(maxlen - n_gram + 1, 1)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(128))
    model.add(Activation('tanh'))
    model.add(Dropout(0.5))
    model.add(Dense(3))
    model.add(Activation('softmax'))

    model.compile(loss='mean_squared_error',
              optimizer='sgd', class_mode="categorical")
The loss values remain nan
Ways to solve?
So I'm wondering what's the real reason for the NaN loss value? How to solve or debug it?
Is the word2vec data wrong , padding method wrong or other?
If keras can't solve, I have to choose another deep learning package, or the reason is theano?
what can I do then? please help.