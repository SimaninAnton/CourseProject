anishsharma commented on 9 Oct 2018
I am developing a python prediction script using spark(Pyspark) streaming and keras. The prediction is happening on the executor where I am calling model.predict().
Modules that I have imported are
from keras.layers.core import Dense, Activation, Dropout from keras.layers.recurrent import LSTM from keras.models import Sequential
I have checked and these imports are taking 2.5 seconds on spark driver(2 core+ 2gb) to load. What is surprising for me is that each time executor gets the job, it automatically do these imports again. The raeason I am sure that these imports are happening each time a job is submitted to the executor is because I see below statements in executor logs per job which are only comes when I do imports of above said modules.
/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from floattonp.floatingis deprecated. In future, it will be treated asnp.float64 == np.dtype(float).type. from ._conv import register_converters as _register_converters Using TensorFlow backend.
My target is to make the prediction in 1 sec but imports itself are taking 2.5 seconds (Each time imports happens on the spark executor). Is this is the intended behavior? Is their anything I can do to minimize this ti say in milliseconds ?