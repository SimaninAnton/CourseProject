nanoix9 commented on 12 Nov 2015
I am a new user of RNN and Keras for language mode. I found Keras accepts 3D tensor as input of RNN, which means word sequences have to be encoded into sequences of word vectors. The simplest is one hot encoding, but that's a heavy waste of memory because most elements in the 3D tensor is zero.
I only find a Embedding layer which accepts index represented word sequence (no need for one hot encoding and thus memory efficient), but such layer generates a DENSE word vector and then feed this vector to the recurrent layer, which forces me to use dense representation instead of one hot encoding.
Is there any efficient way for one hot encoding? Or did I missed something?
Besides, I got "g++ not detected" error while data set goes large, but the same code works for small data set. I asked a question on SO http://stackoverflow.com/questions/33671453/g-not-detected-while-data-set-goes-larger-is-there-any-limit-to-matrix-size I thought larger data set might be supported if there was a memory-saving way for one hot representation.
1