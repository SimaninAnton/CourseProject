kootenpv commented on 1 Apr 2017
I was thinking the idea is that validation set does not get trained on, it is just there to give
Removed progress bar:
Epoch 48/300
239/239  - 9s - loss: 92863872.9707 - val_loss: 91774.0405
Epoch 49/300
239/239  - 8s - loss: 69643150.1151 - val_loss: 90764.2309
Epoch 50/300
239/239  - 7s - loss: 36798639.9874 - val_loss: 99837.9892
Epoch 51/300
239/239  - 7s - loss: 55373148.4351 - val_loss: 108689.3981
Epoch 52/300
239/239  - 9s - loss: 67886569.9079 - val_loss: 91504.1659
Epoch 53/300
239/239  - 10s - loss: 114660141.1883 - val_loss: 79516.2390
Epoch 54/300
239/239  - 10s - loss: 108228406.6946 - val_loss: 90147.0484
Epoch 55/300
239/239  - 8s - loss: 33950466.8619 - val_loss: 21559016.9042
Epoch 56/300
239/239  - 7s - loss: 65377748.9707 - val_loss: 16157222.2647
Epoch 57/300
239/239  - 7s - loss: 76082470.7782 - val_loss: 72440.8443
Epoch 58/300
239/239  - 7s - loss: 62150449.4310 - val_loss: 66478.5902
Epoch 59/300
239/239  - 7s - loss: 43239989.0879 - val_loss: 71589.3255
Epoch 60/300
239/239  - 8s - loss: 69309560.2678 - val_loss: 47086.6804
Epoch 61/300
239/239  - 9s - loss: 79306104.2343 - val_loss: 40792.5986
Epoch 62/300
239/239  - 10s - loss: 56083777.2720 - val_loss: 34290.9820
Epoch 63/300
239/239  - 10s - loss: 110019399.9665 - val_loss: 35704.8646
Epoch 64/300
239/239  - 8s - loss: 56800238.6192 - val_loss: 39407.7763
Epoch 65/300
239/239  - 7s - loss: 53952192.6360 - val_loss: 40326.1467
Epoch 66/300
239/239  - 8s - loss: 50015875.9331 - val_loss: 36026.2778
Epoch 67/300
239/239  - 7s - loss: 71438447.3808 - val_loss: 38214.0559
Epoch 68/300
239/239  - 7s - loss: 34660396.1172 - val_loss: 37944.6187
Epoch 69/300
239/239  - 8s - loss: 76797849.4226 - val_loss: 35968.8731
Epoch 70/300
239/239  - 7s - loss: 74659424.5690 - val_loss: 38454.2130
Epoch 71/300
239/239  - 6s - loss: 76083454.7280 - val_loss: 36816.5687
Epoch 72/300
239/239  - 7s - loss: 80730080.5690 - val_loss: 38050.1621
Epoch 73/300
239/239  - 9s - loss: 42861497.0753 - val_loss: 45427.8735
Epoch 74/300
239/239  - 9s - loss: 10022639.7448 - val_loss: 35731.4088
Epoch 75/300
239/239  - 8s - loss: 49303329.7071 - val_loss: 31094.2602
Epoch 76/300
239/239  - 9s - loss: 90730459.0795 - val_loss: 32623.6102
Epoch 77/300
239/239  - 10s - loss: 76086045.6402 - val_loss: 33247.5883
Epoch 78/300
239/239  - 10s - loss: 40014210.2092 - val_loss: 31222.1148
Epoch 79/300
239/239  - 9s - loss: 50014337.9247 - val_loss: 28382.3470
Epoch 80/300
239/239  - 9s - loss: 91440755.0795 - val_loss: 27024.7929
Epoch 81/300
239/239  - 8s - loss: 45373807.6151 - val_loss: 4670039.1653
Epoch 82/300
239/239  - 8s - loss: 66083655.1297 - val_loss: 26732.1618
Epoch 83/300
239/239  - 7s - loss: 100729685.1548 - val_loss: 27093.4615
Epoch 84/300
239/239  - 7s - loss: 61441098.2594 - val_loss: 25305.6464
Epoch 85/300
239/239  - 8s - loss: 40729278.8117 - val_loss: 4664932.3923
Epoch 86/300
239/239  - 9s - loss: 83951131.6820 - val_loss: 4669581.0251
Epoch 87/300
239/239  - 10s - loss: 116084378.7782 - val_loss: 4672233.4728
Epoch 88/300
239/239  - 9s - loss: 60018646.9289 - val_loss: 30915.8426
Epoch 89/300
239/239  - 7s - loss: 55374982.3598 - val_loss: 25944.5567
Epoch 90/300
239/239  - 7s - loss: 53239646.8285 - val_loss: 28732.6381
Epoch 91/300
239/239  - 7s - loss: 69306036.3180 - val_loss: 24406.3158
Epoch 92/300
239/239  - 7s - loss: 76084591.1464 - val_loss: 21972.2609
Epoch 93/300
239/239  - 7s - loss: 45372532.9372 - val_loss: 23626.9392
Epoch 94/300
239/239  - 8s - loss: 104661363.5816 - val_loss: 64292577.9144
Epoch 95/300
239/239  - 8s - loss: 104661896.0669 - val_loss: 23006.7219
Epoch 96/300
239/239  - 9s - loss: 51439832.6444 - val_loss: 24030.9212
Epoch 97/300
239/239  - 8s - loss: 28595586.0586 - val_loss: 28061.7102
Epoch 98/300
239/239  - 8s - loss: 70015147.2134 - val_loss: 28030.2847
Epoch 99/300
239/239  - 9s - loss: 62861227.8326 - val_loss: 21438.0281
Epoch 100/300
239/239  - 9s - loss: 34658402.1423 - val_loss: 32152287.1674
Epoch 101/300
239/239  - 10s - loss: 100016926.2594 - val_loss: 15848.2394
Epoch 102/300
239/239  - 10s - loss: 102149725.7238 - val_loss: 19861.6413
Epoch 103/300
239/239  - 8s - loss: 56793936.4937 - val_loss: 23655.1463
Epoch 104/300
239/239  - 10s - loss: 40727979.3473 - val_loss: 16092784.1883
Epoch 105/300
239/239  - 11s - loss: 60727879.0962 - val_loss: 69646054.8966
Epoch 106/300
239/239  - 12s - loss: 86083797.2887 - val_loss: 16767.6044
Epoch 107/300
239/239  - 10s - loss: 51438941.0795 - val_loss: 16087166.6556
Epoch 108/300
239/239  - 10s - loss: 94663518.6611 - val_loss: 69648134.3549
Epoch 109/300
239/239  - 8s - loss: 74659516.1172 - val_loss: 23075.7449
Epoch 110/300
239/239  - 7s - loss: 16084011.1531 - val_loss: 19175.1709
Epoch 111/300
239/239  - 8s - loss: 40015631.4310 - val_loss: 10015003.7531
Epoch 112/300
239/239  - 8s - loss: 69305231.1967 - val_loss: 36793160.7615
Epoch 113/300
239/239  - 8s - loss: 110726592.2343 - val_loss: 15371667.0962
Epoch 114/300
239/239  - 9s - loss: 56795408.7699 - val_loss: 4660255.1098
Epoch 115/300
239/239  - 8s - loss: 50017378.1423 - val_loss: 4656181.2306
Epoch 116/300
239/239  - 10s - loss: 79304331.7155 - val_loss: 12080.4031
Epoch 117/300
239/239  - 10s - loss: 59304649.1715 - val_loss: 14993.5516
Epoch 118/300
239/239  - 8s - loss: 51438745.4979 - val_loss: 18234.4949
Epoch 119/300
239/239  - 10s - loss: 25373999.5397 - val_loss: 5370519.4360
Epoch 120/300
239/239  - 10s - loss: 60728602.9121 - val_loss: 13896.3375
Epoch 121/300
239/239  - 12s - loss: 85373677.4561 - val_loss: 5370902.9291
Epoch 122/300
239/239  - 10s - loss: 56080379.4142 - val_loss: 15876.0079
Epoch 123/300
239/239  - 8s - loss: 55370079.6485 - val_loss: 15684.9076
Epoch 124/300
239/239  - 7s - loss: 70013858.8117 - val_loss: 19107.9356
Epoch 125/300
Am I misunderstanding that keras validation set should leak information?