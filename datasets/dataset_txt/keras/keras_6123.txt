vosybac commented on 14 Jan 2016
By following this discussion, #233, I can train the text classification on small dataset on a machine with 4GB GPU.
However when training on large dataset, some problems occurs:
Large number of training samples.
Large vocab_size in Embedding layer.
For the first problem, "Large number of training samples. ", I can solve using train_on_batch and test_on_batch and train it on GPU.
But for the second problem, "Large vocab_size in Embedding layer.", I don't know how to solve it.
My vocab_size is about 500000.
Here is the error:
"Error allocating 511161600 bytes of device memory (out of memory). Driver report 110804992 bytes free and 4 294770688 bytes total".