aseker00 commented on 4 Dec 2017 â€¢
edited
When using LSTM with return_sequences=True, if I understand it correctly the difference between using a TimeDistributed(Dense) layer and simply using a Dense layer, is that TimeDistributed will apply the Dense layer to each time step in the output sequence while simply using Dense (without TimeDistributed) will apply the Dense to the entire output sequence.
I am asking because in the seq2seq example (https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py)
the decoder is applying an LSTM with return_sequences=True and then using Dense (without TimeDistributed) which got me confused:
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)
If the decoder_outputs represent the entire output sequence shouldn't the decoder_outputs be passed to a TimeDistributed(Dense) layer?