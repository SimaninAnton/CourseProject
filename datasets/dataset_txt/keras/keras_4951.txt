pxlong commented on 17 Jun 2016
I have checked the #2271, but still can not fix my problems.
If I want to merge two tensors (like codes showed blow) at each timestep, how to write a lambda layer? Since the lambda layer take one argument, how to concatenate two tensors? Is there any other methods to solve this problem?
lstm_2 = LSTM(128, return_sequences=True)(dropout_1)
lstm_goal = LSTM(128, return_sequences=True)(masking_goal)
merge_direction = TimeDistributed(Merge([lstm_2, lstm_goal], mode='concat'))
If I run the code, will get error "'TensorVariable' object has no attribute 'get_output_shape_at'"