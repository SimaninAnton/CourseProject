Contributor
allanzelener commented on 10 May 2017
Currently in the BatchNormalization layer on line 179 the updates for moving_mean and moving_variance are added unconditionally of the training phase. These are correctly not updated at test time because the BN layer does not have the stateful attribute, but this is confusing from a readability standpoint if you don't check how updates work in the engine.
Additionally, if you set a BN's layer trainable attribute to False it will freeze the gamma and beta weights but not the updates for moving_mean and moving_variance. This matters if you want to do something like fine-tuning. E.g., take a model, freeze all its layers except the final one and train just on the final layer. If you compare the fine-tuned model with the original then you can't isolate if performance differences are due to fine-tuning the final layer or if all the model's BN layer update ops running during additional training also have an effect.
I think that preventing these updates when trainable=false is the behavior users would expect. The proposed fix is to change the updates to K.switch(self.trainable, K.moving_average_update(self.moving_x, ...), self.moving_x). This also improves readability by implying that the update only has effect during training. This change may break code that assumes the current behavior but as far as I know there's no work that explicitly relies on this behavior.
1