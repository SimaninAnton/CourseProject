Contributor
lewisacidic commented on 3 Feb 2016
I'm trying to train a multi-task regression model, but my outputs are not complete (in fact I only have on average <1% of the values per training instance). I expected mean squared error for the non-null outputs to be a reasonable objective, however, obviously using keras' mean squared error objective, the cost comes out as nan, as the nans will propagate.
Is there any plans for supporting this sort of thing (or is it already supported somehow and I missed it?)
If not, anyone have an idea of a hack? I tried writing a new cost function, like:
def mean_squared_error(y_true, y_pred):
    return K.mean(K.square(y_pred - y_true)[(1 - T.isnan(y_true)).nonzero()], axis=-1)
(sorry for mixing keras and theano!)
This evaluates correctly on 1D vectors with nans in the y_true, but this doesn't work with keras, even with batch size set to 1. My next plan was to set the NaNs in y_true to equal y_pred, but I'm not experienced with theano.