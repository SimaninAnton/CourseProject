pechersky commented on 8 Nov 2016
There are other issues which seem to be based on the same sort of phenomenon -- expecting the metrics at the end of an epoch to be weighted similarly to how the samples are weighted for loss (e.g., #3855, #4137). Desired behavior would be for the samples to be weighted in exactly the same way when they get assessed by the metrics.
After taking a look at the engine code, it seems that applying weighted_objective on the metric_fn would be appropriate, if we could figure out the right way of passing a weighted=True flag to the metrics kwarg. A possible approach would be to have a dict like {"name": "metric_name", "weighted": "True"}, but that messes with how metrics.get works in terms out how get_from_module passes kwargs to metrics.get. When a dict with only a name-key is passed to metrics.get, it complains with a TypeError: binary_accuracy() takes exactly 2 arguments (0 given) error. Unfortunately, these functions do not allow themselves to be returned in partial-application format.
I propose modifying keras/engine/training.compile to wrap metric appending in weighted_objective similar to how it is done for weighted_losses. Metrics will get passed in either as strings, functions, or dicts (with at least a name key). If the dict has a {"weights": "True"} key-value pair, then that metric will be wrapped with weighted_objective, and the sample_weight and mask will be applied in its calculation.
Another option is to wrap all metrics with weighted_objective exactly how it is done for weighted_losses. The behavior would then be to pass sample_weight and mask to every metric. This is likely a change that will result in different accuracy and other metric values for a lot of users, so perhaps is not appropriate. However, in the case that users are providing sample_weight, this seems like a common problem.
I will begin working on the first option (clunky dict checking). Any thoughts, @fchollet?
2