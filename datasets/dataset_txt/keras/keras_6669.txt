Imorton-zd commented on 10 Sep 2015
I used skipgram to product the word embeddings of texts, then input to the convolution1D(), next as same as the example in keras. Finally, the accuracy of my two classes classification task is 0.5. I think it must have some problems in my code. However, I have not found the errors. I guess if the input embedding have some problems. Are the input embeddings of one text 2D matrix? All the texts form the 3D matrix? My codes fragment as following:
#get word embeddings from the saved model
new_model = gensim.models.Word2Vec.load(r'Model\mymodel_question')   
X = []
j = 0
fe = open(r'best_standard\errors.txt','w')
for index,sentence in enumerate(sentences):
    sen = []    
    for word in sentence:
        if words[word] >= 3 :
            if word in new_model:
                sen.append(new_model[word])
                j += 1
                print (j)
    t = len(sen)
    if t == 0:
        fe.write('%d\n'%index) 
        continue
    if t < maxlen:
        i = 0
        while t < maxlen:
            sen.append(sen[i])
            i += 1
            t = len(sen)
    else:
        sen = sen[:maxlen]
    X.append(sen)

np.random.seed(seed)
np.random.shuffle(X)
np.random.seed(seed)
np.random.shuffle(labels)

X  = np.array(X)
X_train = X[:int(len(X)*(1-test_split))]
Y_train = labels[:int(len(X)*(1-test_split))]

X_test = X[int(len(X)*(1-test_split)):]
Y_test = labels[int(len(X)*(1-test_split)):]

# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(Y_train, nb_classes)
Y_test = np_utils.to_categorical(Y_test, nb_classes)


print(len(X_train), 'train sequences')
print(len(X_test), 'test sequences')
print('X_train shape:', X_train.shape)
print('X_test shape:', X_test.shape)

print('Build model...')
model = Sequential()

# we add a Convolution1D, which will learn nb_filters
# word group filters of size filter_length:
model.add(Convolution1D(input_dim=embedding_dims,
                        nb_filter=nb_filters,
                        filter_length=filter_length,
                        border_mode="valid",
                        activation="relu",
                        subsample_length=1))

# we use standard max pooling (halving the output of the previous layer):
model.add(MaxPooling1D(pool_length=2))

# We flatten the output of the conv layer, so that we can add a vanilla dense layer:
model.add(Flatten())

# Computing the output shape of a conv layer can be tricky;
output_size = nb_filters * (((maxlen - filter_length) / 1) + 1) / 2 #previous

# We add a vanilla hidden layer:
model.add(Dense(output_size, hidden_dims))
# model.add(Dropout(0.25))
model.add(Activation('relu'))

# We project onto a single unit output layer, and squash it with a sigmoid:
model.add(Dense(hidden_dims, nb_classes))
model.add(Activation('softmax'))
Opinions/Views would be highly appreciated!