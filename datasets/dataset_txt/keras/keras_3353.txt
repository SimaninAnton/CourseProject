dhrushilbadani commented on 27 Jan 2017 â€¢
edited
In the code examples here, in the section titled "Sequence-to-sequence autoencoder," it reads:
[...] first use a LSTM encoder to turn your input sequences into a single vector that contains information about the entire sequence, then repeat this vector n times (where n is the number of timesteps in the output sequence), and run a LSTM decoder to turn this constant sequence into the target sequence.
The code is:
from keras.layers import Input, LSTM, RepeatVector
from keras.models import Model
inputs = Input(shape=(timesteps, input_dim))
encoded = LSTM(latent_dim)(inputs)
decoded = RepeatVector(timesteps)(encoded)
decoded = LSTM(input_dim, return_sequences=True)(decoded)
sequence_autoencoder = Model(inputs, decoded)
encoder = Model(inputs, encoded)
My question is, why are we doing the RepeatVector operation? In the literature regarding sequence to sequence autoencoders (for example in this often cited paper by Dai & Le), there's no repetition as such. Instead, they have the following diagram:
What am I missing here? What exactly is the input sequence to the Decoder portion of the autoencoder?
Thanks!
50
3