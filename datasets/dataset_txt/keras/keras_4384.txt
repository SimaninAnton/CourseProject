asdavis commented on 15 Sep 2016
When applying activity regularizations to a Dense layer wrapped in a TimeDistributed layer, the self.layer.inbound_nodes list is empty. As a consequence, activity regularization is not actually applied. I am currently using the deprecated TimeDistributedDense layer as a workaround. Below is some example code that replicates the issue -- we expect to see a populated inbound_nodes list when call is called to compile the model, but we do not, so no regularization is applied.
import keras.backend as K
from keras.regularizers import Regularizer
from keras.layers import TimeDistributed, Dense, Flatten, Input
from keras.models import Model
from keras.optimizers import Adam`

import numpy as np

class ActivityRegularizer(Regularizer):
    def __init__(self, l1=0., l2=0.):
        self.l1 = K.cast_to_floatx(l1)
        self.l2 = K.cast_to_floatx(l2)
        self.uses_learning_phase = True

    def set_layer(self, layer):
        self.layer = layer

    def __call__(self, loss):
        if not hasattr(self, 'layer'):
            raise Exception('Need to call `set_layer` on '
                            'ActivityRegularizer instance '
                            'before calling the instance.')
        regularized_loss = loss

        print 'inbound_nodes list is empty:', self.layer.inbound_nodes
        for i in range(len(self.layer.inbound_nodes)):
            print 'We never loop because self.layer.inbound_nodes is empty'

            output = self.layer.get_output_at(i)
            if self.l1:
                regularized_loss += K.sum(self.l1 * K.abs(output))
            if self.l2:
                regularized_loss += K.sum(self.l2 * K.square(output))
        return K.in_train_phase(regularized_loss, loss)

    def get_config(self):
        return {'name': self.__class__.__name__,
                'l1': float(self.l1),
                'l2': float(self.l2)}

input_layer = Input(shape=(10,100))
tdd_layer = TimeDistributed(Dense(100, activation='relu', activity_regularizer=ActivityRegularizer(l1=0.1, l2=0.1)))(input_layer)
flat_layer = Flatten()(tdd_layer)
out_layer = Dense(1, activation='sigmoid')(flat_layer)

model = Model(input=input_layer, output=out_layer)

opt = Adam()

model.compile(loss='binary_crossentropy', optimizer=opt)

x = np.random.rand(10000, 10, 100).astype(np.float32)
y = np.random.rand(10000, 1).astype(np.float32)

model.fit(x, y)