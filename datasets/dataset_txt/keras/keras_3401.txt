ghost commented on 22 Jan 2017 â€¢
edited by ghost
i know that clipnorm fix this issue and i know that clipnorm clip the big number of the gradients but i want to know why the nan is produced?
why do i see loss=nan when i don't use clipping of the gradient?
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).