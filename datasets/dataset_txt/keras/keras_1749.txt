Contributor
wanasit commented on 12 Aug 2017 â€¢
edited
I have found an error when saving/loading Recurrent layers with initial_state
This is the code to reproduce the bug:
from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense
from keras.models import Model, load_model

VEC_SIZE = 10
INPUT_LENTGH = 20

input_initial_state = Input(shape=(VEC_SIZE,))
input_x = Input(shape=(INPUT_LENTGH,VEC_SIZE))

# Create an RNN with initial_state
lstm = LSTM(VEC_SIZE, return_sequences=True)(input_x, initial_state=[input_initial_state, input_initial_state])
model = Model(inputs=[input_x, input_initial_state], outputs=[lstm])

# Save/Load the model
model.save('test.h5')
loaded_model = load_model('test.h5') 
Loading model back in the last line will throw following error.
Layer lstm_8 expects 1 inputs, but it received 3 input tensors. Input received: [<tf.Tensor 'input_16_1:0' shape=(?, 20, 10) dtype=float32>, <tf.Tensor 'input_15_1:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'input_15_1:0' shape=(?, 10) dtype=float32>]
I have tracked the cause into __call__ in Recurrent class. When initial_state is assignd the model treat will eventually treat it as an input and add them into graph.
See. recurrent.py line 283
So, when saving, the model will see Recurrent layer with initial_state as having 3 inputs. And then, when loading, the model will be initialized with 3 inputs (but no initial_state), the function will throw an error at line 259.
One way to fix this is having Recurrent layer check and treat additional inputs as initial_state. Adding this code before line 259 fixed the problem for me.
if isinstance(inputs, (list, tuple)) and len(inputs) > 1 and initial_state is None:
    initial_state = inputs[1:]
    inputs = inputs[0]
2