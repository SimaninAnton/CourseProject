jinfengfeng commented on 2 Dec 2015
I implement a char language model similar to Andrej Karpathy's char-rnn. In the sampling step to generate text using a trained model, if I input char one by one, I got meaningless text; but if I input a sequence of chars to predict a next char, I got similar result with Andrej. It seems the LSTM by default reset its hidden state between different input sequences. This issue causes inconvenience. And we have to input an entire sequence to get a correct output in the rnn language model text generating case. Maybe we should let rnn keep its hidden state between different input sequences, and provide a function to reset hidden state if we don't want it to keep.
Is my expression clear?