neilthefrobot commented on 6 Dec 2017 â€¢
edited
Adding dropout to RNNs gives unexpected validation loss.
This issue can easily be seen and reproduced by setting your validation inputs/targets to be the same as your training inputs/targets. In this case you would expect to get the same results for training and validation error, and you do unless you use dropout. It may be expected when using dropout that the two losses may differ if dropout is only applied for training but not for validation, but in this case the validation loss should be lower than the training loss, but what you find is that the validation loss is significantly worse than training loss. I can get the training accuracy to 100% but it is impossible to get the validation that high, despite them being the exact same data. This is the exact opposite behavior I get in past experiences doing this same test.
What makes this even stranger and more convincingly a bug is that it only happens specifically when setting dropout for an RNN's inputs -
model.add(LSTM(neurons, dropout=0.5, input_shape=(inputs.shape[1], inputs.shape[2])))
You can add dropout anywhere else you want, as much as you want, (to fully connected layers after the LSTM for example) and even add "recurrent_dropout" to the LSTM and not get this issue at all. The test and validation loss will be equal as expected. But as soon as you add dropout to the RNN's inputs the validation loss doesn't behave as expected which is a big issue for me being able to tell if dropout is actually being applied correctly and if the validation loss I see is correct.
I have changed virtually every factor around (which type of RNN I use, amount of dropout, loss function, optimizer, network architecture, inputs/targets etc) and get the same odd behavior.
Another thing to note is that the more dropout the worse the validation is. And even stranger, a dropout of 0.8 or higher actually makes the validation loss get worse over time while the training loss gets better, despite being identical data. I can't think of any way to explain this, and it doesn't happen in my custom implementation of dropout on LSTMs.
Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on StackOverflow or join the Keras Slack channel and ask there instead of filing a GitHub issue.
Thank you!
[x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
[x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
[x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
[x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).