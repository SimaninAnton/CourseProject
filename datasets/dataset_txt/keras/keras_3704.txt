aurora1625 commented on 11 Dec 2016
I want to compute the attention between a sentence with a word.
I represent them in two sequential model:
sent_enc = Sequential()
    sent_enc.add(Embedding(input_dim=MAX_NB_WORDS+1,
                           output_dim=EMBEDDING_DIM,
                           weights=[embedding_matrix],
                           trainable=False,
                           input_length=MAX_SEQUENCE_LENGTH,
                           dropout=0.2))


    entity1_enc = Sequential()
    entity1_enc.add(Embedding(input_dim=MAX_NB_WORDS+1,
                             output_dim=EMBEDDING_DIM,
                             weights=[embedding_matrix],
                             trainable=False,
                             input_length=1))
Here the sent_enc is a matrix with dimention MAX_SEQUENCE_LENGTH * EMBEDDING_DIM, but the entity1_enc is just one word with dimention EMBEDDING_DIM
How can I compute the dot product between each word in the sentence with the entity using merge?
attn1 = Sequential()
    attn1.add(Merge([sent_enc, entity1_enc], mode="mul"))
I am expecting to get an array with length of MAX_SEQUENCE_LENGTH, each element is a scaler.
Exception: Only layers of same output shape can be merged using mul mode. Layer shapes: [(None, 28, 300), (None, 1, 300)]
I got the error above, could anyone help with the dimension and how to use merge to calculate the element-wise dot product between a matrix and an array.
Thanks.