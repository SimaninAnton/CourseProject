chunyangx commented on 6 Nov 2015
Hello,
I think it would be cool to be able to implement some attention model using Keras.
As the "attention vector" is a weighted vector from some intermediate layers of input embeddings. I think the solution goes to using graph. So the structure of the program should look like this:
self.model = Graph()
self.model.add_input()
'''add all the inputs'''
...
Then add layers such as Dense, LSTM to these inputs and 'wire them correctly' to generate correctly attention vector.
However, even without generating the attention vector, I have difficulties in using Graph model as I show in issue #933 . Could somebody explain to me what is happening bechind the scene please ?
Thank you