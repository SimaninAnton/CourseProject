dk1013 commented on 12 Oct 2016 â€¢
edited
I have a regression CNN model for object detection tasks.
When I use one output and one loss function, the result is very good.
model = Model(inp,out1)
model.compile( loss="mean_squared_error",optimizer=SGD() )
Now I add another output out2 with ZERO loss weight on top of out1:
out2 = SomeLayer(params)(out1)
model = Model(inp,[out1,out2])
model.compile( loss="mean_squared_error",optimizer=SGD(),loss_weights=[1.0, 0.0] )
The performance is much worse. Note that I tried different loss weight values for out2, which all gave bad results.
My thought is that if loss weight of out2 is zero, it should has no contribution in the back-propagation updates. And because out2 does not alter the sub-network from input to out1, the learning shouldn't change either.
Any idea where I was doing wrong?
Thanks a lot!