shyamupa commented on 17 Jul 2016
Sometime ago I implemented an attention model. I am moving it to new functional api, but having problems with a part of the code,
I generate attention weights alpha of size (None,20) from my LSTM output M of size (None, 20, 300),
alpha_=TimeDistributed(Dense(1,activation='linear'),name="alpha_")(M)
flat_alpha=Flatten(name="flat_alpha")(alpha_)
alpha=Dense(L,activation='softmax',name="alpha")(flat_alpha)
Now I merge alpha of shape (None,20) to Y a matrix of size (None, 20, 300),
r_= merge([alpha,Y],output_shape=(k,1),name="r_",mode=get_R)
where get_R is,
def get_R(X):
    alpha, Y = X[0],X[1] 
    ans=K.dot(alpha,Y)
But the shape is not the desired shape of (300,1). Instead I get output shape of ((None, 20), 300, 1).
Can anyone help? This worked before moving to functional api but now it does not.
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short). # #
1