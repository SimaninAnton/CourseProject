sungyongs commented on 7 Sep 2016
Hi,
For Embedding layer, the first weights (i.e., W[0]) are corresponding to 0 index of inputs.
Usually, if we use zero padding, the 0 index is not desire input but only needed for fixed lengths.
So, if possible, I'd like to initialize Embedding weights W[0] = 0 and keep it 0 but other weights are trainable.
Then, I may be possible to mask out the zero inputs (which are caused by W[0]) of subsequent layers.
Could you help me how to initialize it?
2