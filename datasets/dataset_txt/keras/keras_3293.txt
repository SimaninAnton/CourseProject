sebinjude commented on 6 Feb 2017
I am quite new to Keras. What I am trying to accomplish is to train a word level text generation module.
To overcome the problem of varying length of sentences in my training data, I set my batch_size to 1 and trained the model with statefull = true.
Now it will predict next word given a word. Then I tried to concatenate the generated word to the input to the predict function but in that case I get output shape as a array of predictions for each word in the input ( What I wished for was a single prediction for the next word, considering all the previous words)
I want to design something like if i give
START_TOKEN
Generate say : What
then i concatenate it to START_TOKEN to get "START_TOKEN What" (ie add a new row to input vector)
Then the next word generated should consider both START_TOKEN and word "What" to generate the next word and so on....
My current code:
model.add(LSTM(100, return_sequences=False, unroll=True, stateful=True, input_shape=(1,len(itw)) , batch_input_shape=(1,1,len(itw)) )) model.add(Activation('tanh')) model.add(Dense(len(itw))) model.add(Activation('softmax')) optimizer = RMSprop(lr=0.01) model.compile(loss="categorical_crossentropy",optimizer=optimizer,metrics=["accuracy"]) a,b =batch1() model.fit(a,b,batch_size=1,nb_epoch=10,verbose=1)
Here the batch function generates a x and y arrays representing with shape (len_corpus, 1, vocabulary)
y is one word ahead of x
like x= "how are you...."
then y ="are you.....
Thanks in advance