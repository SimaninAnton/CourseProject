Kyubyong commented on 24 Aug 2016
There seems to be a few keras sources that use the attention mechanism, but I still don't know how to modify them to apply to image captioning. What I referred to includes these:
https://github.com/farizrahman4u/seq2seq
https://github.com/codekansas/keras-language-modeling
What I want to do is reproduce the experiment in the seminal paper Show, Attend, and Tell. https://arxiv.org/pdf/1502.03044.pdf. I extracted annotation vectors from MS-COCO training images, and prepared their caption vectors (X_train) and the next word vectors (y_train). What am I supposed to do next?
1