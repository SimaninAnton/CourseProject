v1nc3nt27 commented on 25 Jul 2017
Hey,
I'm currently updating my code from Keras 1.0.8 to the latest version 2.0.6. I switched a merge-layer for the new concatenate-layer, but I'm getting an error:
The first layer in a Sequential model must get an input_shape or batch_input_shape argument.
Simplified, my code looks like this:
LSTM_1 = Sequential()
LSTM_1.add(Embedding(2000, 100, weights=[emb_1], input_length=100, mask_zero=True))
LSTM_1.add(LSTM(100, input_shape=(1000, 100)))

LSTM_2 = Sequential()
LSTM_2.add(Embedding(5000, 100, weights=[emb_2], input_length=2000, mask_zero=True))
LSTM_2.add(LSTM(100, input_shape=(2000, 100)))

LSTM_3 = Sequential()
LSTM_3.add(Embedding(3000, 100, weights=[emb_3], input_length=500, mask_zero=True))
LSTM_3.add(LSTM(100, input_shape=(500, 100)))

merged_model = Sequential()
merged_model.add(Concatenate([LSTM_1, LSTM_2, LSTM_3]))
merged_model.add(Dense(2, activation='softmax'))
merged_model.compile('adam', 'categorical_crossentropy')

merged_model.fit([X_1, X_2, X_3], y, batch_size=200, epochs=10, verbose=1)
Instead of the Concatenate layer I had the following line:
merged_model.add(Merge([LSTM_1, LSTM_2, LSTM_3], mode='concat'))
The problem is, that merged_model.summary() gives me the following with the old Merge layer and the latest Keras version:
Layer (type)                 Output Shape              Param #   
=================================================================
merge_1 (Merge)              (None, 300)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 602       
=================================================================
Total params: 10,943,302
Trainable params: 241,802
Non-trainable params: 10,701,500
Before I updated to the latest version, it was building the model correctly with the LSTM layers inside.
Can someone explain me what's going wrong here?
Thanks!