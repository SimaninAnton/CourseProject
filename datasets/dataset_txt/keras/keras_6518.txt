Contributor
NickShahML commented on 25 Oct 2015
I saw that @EderSantana (my hero) was working on changing the learning rates during training. I believe that commit was made here: #536
My question is that I normally have my model train on one epoch, do some predictions, and then train the next epoch. Can I simply modify the learning rate after say epoch 5?
for example:
for iteration in (0, 100):
    if iteration>5:
        optimizer.lr.set_value(0.02)
        optimizer.momentum.set_value(0.04) 
    model.fit(x_embed, y, nb_epoch=1, batch_size = batch_size, show_accuracy=True)
    preds = model.predict(x_embed_sample, verbose=0, batch_size = batch_size)[0]
Its kinda hard to test to see if this actually works or not.
6