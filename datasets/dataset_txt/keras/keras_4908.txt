aravindr93 commented on 24 Jun 2016 â€¢
edited
I would like to compute the gradients wrt weights of the neural network for studying a custom optimization procedure. This issue will also be useful for people trying to compute non-standard gradients (e.g. actor-critic algorithms in deep RL).
The gist of the scenario is described below in tensor-flow, but is easily translatable to Theano as well:
model = Sequential()
model.add(....) # add a couple of layers here
x = tf.placeholder(tf.float32, shape=(None, input_dim))
y = model(x)
loss = K.sum(K.square(y-target))  # just think of any standard loss fn
I need the gradient of loss wrt each parameter in the neural network and don't want to use the standard model.fit() function. Roughly, what i want to accomplish is:
param_grad_sym = tf.gradients(loss, model_params)  # symbolic
param_grad = sess.run(param_grad_sym, \
                       feed_dict={x: input_vals, model_params: model.get_weights()})
new_weights = custom_optimization_routine(param_grad, model.get_weights(), other_args)
model.set_weight(new_weights)
The problem is, I am unable to get the symbolic model_params required for tf.gradients or K.gradient. model.get_weights() returns the numerical values, but I need the symbolic variables. Thanks!