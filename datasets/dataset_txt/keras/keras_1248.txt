rcasiodu commented on 12 Feb 2018 â€¢
edited
I want to change the activation of the last layer 'softmax' to 'relu', the following is the code:
from keras.applications.vgg16 import VGG16
from keras.activations import softmax, relu, sigmoid
import numpy as np
import cv2
model_vgg16 = VGG16(weights='imagenet', include_top= True)
img_path = './vision_project/filter-visualization/doberman.png'
img = cv2.imread(img_path)
img = cv2.resize(img, (224, 224))
img = img.astype('float32') - 127.5
output = model_vgg16.predict(img[np.newaxis, :])
print(output[0, np.argmax(output)],np.argmax(output))
OUT:0.822323 236
model_vgg16.layers[-1].activation = relu
output = model_vgg16.predict(img[np.newaxis, :])
print(output[0, np.argmax(output)],np.argmax(output))
OUT:0.822323 236
The result is the same, but when i check the activation layer in the model.layers[-1].activation, it has changed with success.
model_vgg16.layers[-1].activation
OUT: <function keras.activations.relu>
Could anyone tell me how to solve this problem?