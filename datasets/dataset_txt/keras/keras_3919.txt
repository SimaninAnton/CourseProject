phdowling commented on 16 Nov 2016
From what I understand, Keras loads each minibatch onto the GPU seperately, in order to avoid running out of memory on the GPU. If this data transfer is a bottleneck, the standard solution is to increase the batch size.
However, I am facing the case where I have plenty of memory available on the GPU, but upping my batch size significantly lowers my networks accuracy (32 seems to make gains much faster than 128). Is there any way, or could any way be implemented, of pre-loading some larger subset of data onto the GPU, but still effectively maintaining small batch sizes during training?
4