volvador commented on 15 Feb 2017
I am confused by the explanation of distributed training here :
https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html
Let us say I have this simple model
x_train, y_train = .... # my daya
model = Sequential()
model.add(Dense(output_dim = 10, input_dim = x_train.shape[1]))
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(output_dim = y_train.shape[1], input_dim = 10))
model.compile(loss='mse', optimizer='rmsprop')
I have a machine with 8 CPU (not GPU) cores and I want to distribute my training on the 8 cores. I want data parallelism and not model parallelism.
According to the explanation in the keras blog, I have only to do this:
server = tf.train.Server.create_local_server()
sess = tf.Session(server.target)
from keras import backend as K
K.set_session(sess)
and then call
model.fit(X_train, y_train, nb_epoch=20, batch_size=200)
But I do not see how tf will understand that I want to distribute my computations on the 8 CPU cores, doing minibatches of 25 in each of them and averaging the SGD step.
Also, @fchollet gives a more elaborate manner to do data parallelism here
https://www.quora.com/What-is-the-state-of-distributed-learning-multi-GPU-and-across-multiple-hosts-in-Keras-and-what-are-the-future-plans
which seems much more complicated than what is say in the keras blog, adding to my confusion.
TL;DR for the model defined above, and having 8 CPU cores on my machine, how can I do distributed training (data parallelism) using keras/Tensorflow?