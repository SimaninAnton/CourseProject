droid666 commented on 30 Jan 2017 â€¢
edited
If I use models in other threads I get errors. If I run them first (call train_on_batch and predict_on_batch) it works. But I cannot do that for my real models because I only get train data after I eval a few times (simulation stuff).
I tried "tfGraph.as_default()" and "global/local_variables_initializer()", that things just give different errors for all combinations, see 4 examples:
Just spawning threads:
ValueError: Tensor("Const:0", shape=(), dtype=float32) must be from the same graph as Tensor("sub_3:0", shape=(), dtype=float32).
Using: with tfGraph.as_default()
IndexError: pop from empty list
Using: tfGraph.as_default() and global/local_variables_initializer()
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'keras_learning_phase' with dtype bool
Using: tfGraph.as_default() and global/local_variables_initializer() and K.manual_variable_initialization(True) [not shown in 2nd Gist]:
FailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_1_W
Gist: test.py (run without arguments, could also test sequential or processes, commented out atm to show the bug. If you remove the comment on line 209 it works (calls warmup(), which calls train_on_batch and predict_on_batch, before starting the threads):
https://gist.github.com/droid666/eebf14dc8e92f3c4bccb92a1c0fd4279
Same with "tfGraph.as_default()" and "global/local_variables_initializer()" at lines 134-143:
https://gist.github.com/droid666/3353c8f1f225a1245fab59f4d1570e89
Sorry if I just do it wrong (global/local_variables_initializer() is used wrong, I am rather sure).
Edit: also I cannot just do the following because I have complex custom loss functions, so y_predict and y_train are very different in size and content:
for m in myModels:
        x = np.zeros((1,m.inSize))
        y = m.model.predict_on_batch(x)
        m.model.train_on_batch(x, y)
1