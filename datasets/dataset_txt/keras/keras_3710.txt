KlaymenGC commented on 10 Dec 2016
Hello,
I'm writing a Maxout layer or Feature Pooling layer for both backends. The batch size is not fixed.
In Theano it's pretty straightforward:
import keras.backend as K
class FeaturePool(Layer):
        ...
        def call(self, x, mask=None):
                # in Theano, self.axis=1, pool_size=2 then the channel of the output will be halved
                # input shape: (None, n_channel, col, row)
                input_shape = tuple(x.shape)
                num_feature_maps = input_shape[self.axis]
                num_feature_maps_out = num_feature_maps // self.pool_size

                # pool shape: (None, num_feature_maps_out, pool_size, col, row)
                pool_shape = (input_shape[:self.axis] +
                              (num_feature_maps_out, self.pool_size) +
                              input_shape[self.axis+1:])

               input_reshaped = K.reshape(x, pool_shape)
               # and the final output will be a max function over the pool_size dimension
               return K.max(input_reshaped, axis=self.axis + 1)
But when implementing it for tensorflow backend, the input shape will be defined as
input_shape = x.get_shape()
which is a TensorShape and when I'm doing the reshape, an error occurs saying that the reshape is not possible because the shape contains None.
I'd definitely need this layer in tensorflow but currently haven't found a solution, any suggestion will be greatly appreciated.