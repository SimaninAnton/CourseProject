12gjang commented on 26 May 2016 â€¢
edited
from keras.layers.core import Dropout, Dense
from keras.layers.normalization import BatchNormalization
from keras.layers.recurrent import LSTM
from keras.models import Sequential
import numpy as np
import random
time_window_size = 50
nb_training_data = 100
data_dim = 25
nb_label = 2
train_x = np.zeros((nb_training_data, time_window_size+1, data_dim), dtype=bool)
for i in range(len(train_x)):
for j in range(len(train_x[i])):
for k in range(nb_label):
train_x[i][j][random.randint(0, data_dim - 1)] = 1
train_x = np.array(train_x, dtype=np.bool)
model = Sequential()
dropout = 0.25
model.add(LSTM(100, return_sequences=False, input_shape=(time_window_size, data_dim)))
model.add(BatchNormalization())
model.add(Dropout(dropout))
model.add(Dense(data_dim, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(train_x[:, 0:time_window_size, :], train_x[:,time_window_size, :], batch_size=50, nb_epoch=1000, shuffle=True)
That is my test code for multi-label classification.
I have searched method to solve multi label classification.
Many people recommended to use sigmoid + binary_crossentropy or relu (tanh) + mse
But, both of them doesn't work well on multi label classification.
Always stuck in around 0.6 accuracy. (in case of two label classification)
Is there some thing wrong in my approach ?
Please help me, I have too much struggled in this problem..