Suncicie commented on 22 Nov 2018 •
edited
I'm trying to write a new metric of f1-macro for the softmax, I have tried two methods, I can't understand why the result of them is different.
The first method, I defined a f1-macro function like that,
def f1_macro(y_true,y_pred):
    y_pred_tmp=K.argmax(y_pred,axis=-1)
    y_pred_tmp=K.one_hot(y_pred_tmp,4)  (MULTI CLASS OF 4)
    tp=y_pred_tmp*y_true
    tp=tf.reduce_sum(tp,axis=0)

    fp=K.greater(y_pred_tmp,y_true)
    fp=tf.reduce_sum(tf.cast(fp,tf.float32),axis=0)

    fn = K.greater(y_true,y_pred_tmp)
    fn = tf.reduce_sum(tf.cast(fn, tf.float32), axis=0)

    prec_list=[]
    recal_list=[]
    f1_list=[]
    for i in range(0,4):
        prec=tp[i]/(tp[i]+fp[i]+K.epsilon())
        rec=tp[i]/(tp[i]+fn[i]+K.epsilon())
        f1=2*prec*rec/(prec+rec+K.epsilon())
        prec_list.append(prec)
        recal_list.append(rec)
        f1_list.append(f1)
    return tf.reduce_mean(f1_list)
then use it like that model.compile(..., metric=[f1-macro])
The second method, I used the Callback to calculate the f1-macro by sklearn f1-score, the f1 calculated by f1_macro function is always less than the Callback. the Callback function is defined below:
class Metrics(Callback):
    def on_train_begin(self, logs={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_precisions = []

    def on_epoch_end(self, epoch, logs={}):
     
        pred=np.asarray(self.model.predict(self.validation_data[0]))
        val_predict = np.argmax(pred, axis=-1)
        val_targ = np.argmax(self.validation_data[1], axis=-1)
        _val_recall = recall_score(val_targ, val_predict, average='macro')
        _val_precision = precision_score(val_targ, val_predict, average='macro')
        _val_f1 = f1_score(val_targ, val_predict, average='macro')
        self.val_recalls.append(_val_recall, )
        self.val_precisions.append(_val_precision)
        self.val_f1s.append(_val_f1)
        print('— val_f1: %f — val_precision: %f — val_recall %f' % (_val_f1, _val_precision, _val_recall))
        return
and I use it by
 metrics = Metrics()
 model.fit(X_train, y_train,
                      epochs=10,
                      batch_size=batch_size,
                      validation_data=(X_valid, y_valid),
                      callbacks=[early_stopping, plateau, checkpoint, metrics],
                      verbose=2
                      )
In the two methods, is the validation data for keras to calculate (X_valid, y_valid)?
Is the calculation principle of sklearn f1 score the same with f1_macro ?
How do keras calculate the metrics, is it after each batch size, or after each epoch?