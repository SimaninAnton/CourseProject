Contributor
wxs commented on 26 Aug 2015
I'm getting concerned that we should not be applying a mean to the objective function in a mini-batch, because it means that mini-batches with more labels effectively have their labels downweighted.
Here is the relevant line weighted_objective
return weighted.sum() / (filtered_mask * filtered_weights).sum()
and also:
return weighted.sum() / filtered_weights.sum()
Originally we always applied a mean() to objective functions, so when I created the weighted_objective I kept using a mean, just now a weighted mean. However here's why I think this is wrong:
say 99% of my data is labeled A, and 1% is labeled B. I want to train with class-counts renormalized, so I apply a class_weight of 1/99 to class A. However say my mini-batches are of size 10: 90% of these will consist only of class A, so every single loss in the batch will be weighted by 1/99, so when I take the weighted mean (by dividing by the sum) I get the same as if they were all weighted 1. This will obviously not have the desired effect of treating class A and class B as though they had an equal number of training examples.
I think the correct answer is to eschew the normalization completely and replace both of these with:
return weighted.sum()
I'm a bit nervous about the interaction with masking though, because there was some discussion on another issue (I can't find it at the moment) that was arguing that we need to scale the outputs in the event that there are fewer due to masking.