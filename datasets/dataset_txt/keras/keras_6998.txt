Contributor
jfsantos commented on 27 May 2015
I made a simple model with a 1 cell LSTM to predict the next number in a sequence given the current number (not for any practical reasons, this is just to demonstrate the bug). Here's the code:
from keras.models import Sequential
from keras.layers.recurrent import LSTM

import numpy

# Dataset has a single sample just to make things simpler
x1 = numpy.random.randn(100)
X_train = numpy.empty((1,len(x1)-1, 1)) #  (# samples, # timesteps, # features) 
Y_train = numpy.empty((1,len(x1)-1, 1))

X_train[0,:] = x1[:-1]
Y_train[0,:] = x1[1:]

model = Sequential()
model.add(LSTM(1, 1, activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True))

model.compile(loss='mean_squared_error', optimizer='sgd')
model.fit(X_train, Y_train, batch_size=1, nb_epoch=10)
But running this fails during the compile step, with the following error message:
TypeError: Cannot convert Type TensorType(float64, (False, True, True)) (of Variable 
IncSubtensor{Set;:int64:}.0) into Type TensorType(float64, (False, False, True)). You can try to manually
 convert IncSubtensor{Set;:int64:}.0 into a TensorType(float64, (False, False, True)).
I remember that a while ago we had to pass a parameter to compile to say how many dimensions the targets have, but it was removed and now compile uses T.zeros_like(self.y_train) to set the output dimension.
Am I doing something wrong or is this a bug?