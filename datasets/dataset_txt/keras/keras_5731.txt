tanxchong commented on 18 Mar 2016
Sometime, we may update the graph using the activations of parts of the graph and output the results after some further manipulation.
for example:
model = Graph()
model.add_input(input_shape=(3, img_height,img_width),name='inputs')
model.add_output(name='res',input='inputs')
model.add_output(name='control',input='inputs')
opter = optimizers.Adadelta()
model.compile(opter,loss={'contro'l:'mse'})
this code segment does not contain further manipulation for simplicity, but it will not pass compiling. I checked the code in models.py (line:1234):
" for output_name in self.output_order:
loss_fn = loss[output_name] "
We are required to give every output a teaching signal. I guess it may be better to loop in "loss" dict rather than in output_order.