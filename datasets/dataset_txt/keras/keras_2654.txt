sun-peach commented on 15 Apr 2017 â€¢
edited
Hi, dear all,
I am building up a end-to-end RNN system for binary classification, The architecture is pasted here.
The architecture parameters is formed in a dictionary format:
{"E1": {1:{"num": 32, "act": "relu", "type": "timeconv2d","ks":[5,32],"stride":[1,4]},
2:{"num": 32, "act": "relu", "type": "timeconv2d","ks":[3,16],"stride":[1,2]},
3:{"num": 32, "act": "relu", "type": "timeconv2d","ks":[1,4]},
4:{"num": 64, "act": "tanh", "type": "lstm","bidi":False,"returnseq":True}},
"E2": {1:{"num": 64, "act": "relu", "type": "timeconv2d","ks":[5,5],"stride":[2,2]},
2:{"num": 64, "act": "relu", "type": "timeconv2d","ks":[5,5],"stride":[2,2]},
3:{"num": 64, "act": "relu", "type": "timeconv2d","ks":[5,5]},
4:{"num": 128, "act": "tanh", "type": "lstm","bidi":False,"returnseq":True}},
"C": {1:{"num": 256, "act": para_dict["activation"], "type": "lstm","bidi":False,"returnseq":True},
2:{"num": 256, "act": "relu","type": "timemaxout"},
3:{"num": 2, "act": "softmax", "type": "timedense"}}}
The "E1","E2" are the networks taking 2 inputs. they are merged and fed to the network "C".
For "E1","E2", I make reshape firstly. Then I use Masking. After masking, it is passed to the convolutional layers. After convolutional layers, I use flatten and connect them to LSTM.
My problem is the training loss very easily goes to NaN. I have tried batchnormalization and different optimizer. After several batches, the loss is always NaN. My batch size is 4 (my data is pretty large, so large batch size cannot fit my GPU memory).
Could anyone tell me how should I fix it?
Thank you very much!