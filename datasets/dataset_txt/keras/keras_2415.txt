godissaw commented on 11 May 2017
As we all know, the softmax activation layer output the same filters with the input.
I am doing the segmentation tasks.
For example, use the voc FCN, the second-last is (none, nb_classes=21, w, h)
after the softmax, it output the same (none, nb_classes=21, w, h),
So the y_pred =(21,w,h),but the y_ture =(1,w,h) ,if I don't want to one-hot every dot in my label, I think the "sparse_categorical_crossentrop" works.
However, when I check the loss function, I found that it bases on this "tf.nn.sparse_softmax_cross_entropy_with_logits" (tf backend), it has the softmax function.
So, should I delete the softmax before I using it?