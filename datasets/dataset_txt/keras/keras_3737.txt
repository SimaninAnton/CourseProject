drscotthawley commented on 8 Dec 2016 â€¢
edited
After familiarizing myself with keras' LSTM examples and tutorials (e.g. https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py), and discussions on the "stateful" flag (e.g. http://philipperemy.github.io/keras-stateful-lstm/, http://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/) , I modified the keras/stateful_lstm.py example so that the network is trained to learn an 'echo' (or delay) instead of fitting a decaying cosine.
The keras stateful_lstm.py example uses batch_size=25. Other examples & tutorials seem to prefer using batch_size=1 for stateful LSTM networks. I find that for batch_size=1, such a network does not learn the periodicity (echo) requested. What happens with batch_size=1 is simply that the output mirrors the input, with no echo. And with batch_size=25, only delays which are integer multiples of 25 can be learned. I'd like to be able to use batch_size=1 so that in principle any delay can be learned.
It seems only way the network seems to learn is if batch_size(>1) is a factor of the delay length in samples (variable "delay_num_samples" in gist), i.e. only if the delay length is an integer multiple of batch_size. Since delay length wouldn't be known a priori, this is a problem.
There doesn't seem to be a clear reason why batch_size=1 shouldn't work. We are simply evaluating the loss function every time instead of averaging over batches of 25 or 50, and still going through the whole dataset each epoch.
Link to code gist, modeled after examples:
https://gist.github.com/drscotthawley/cd4b21c110eefd53c553dc6283843094
Checklist:
Latest version of keras
Tried both (latest) Tensorflow and Theano backends.
Had posted to keras_users, long time no response, discovered batch_size sensitivity (i.e. that it can work for certain batch sizes but not for 1) and have escalated here to Issues.