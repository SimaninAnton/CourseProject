Xiang-Gu commented on 1 Jun 2019 â€¢
edited by jvishnuvardhan
System information
Have I written custom code (as opposed to using example directory): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04
TensorFlow backend (yes / no): yes
TensorFlow version: 1.12.0
Keras version: 2.2.4
Python version: 3.6.7
CUDA/cuDNN version: N//A
GPU model and memory: N/A
Describe the current behavior
I designed a custom ANN architecture and I want to implement it with Keras. One thing about this network is that I used shared layers for multiple models. That is, some layers appear (are shared) in more than one models. I want only one specific model (among all those models) to be able to update the weights of these (shared) layers while other models cannot when training. So, I defined and compiled that specific model first, add the freezing weights code, and then define and compile the remaining models.
We know that model.summary() will first list each and every layer of the model as well as the number of parameters in each layer. Then at the end, it will print out some summary message about the weights -- total # of parameters, # of trainable parameters, and # of non-trainable parameters.
So I tried to use model.summary() to print out the summary of those models to see whether coding like this achieves my purpose. I tried on those models that need freezing, the result is as expected. Yay! But when I tried it on that specific model (the one that does not need freezing), the output is weird. Namely, the number of trainable parameters is correct because it is equal to the sum of the number of parameters in each layer as shown above. After all, we expect this model to not freeze any weights of any layer and coded so. But the number of non-trainable weights is NOT 0. The "# of non-trainable parameters" section says it is equal to the number of parameters, no matter what number it is, of those frozen layers. The # of total parameters hence is the sum of the # of trainable and non-trainable parameters, which is not equal to the sum of the parameters of each individual layers as shown above.
Describe the expected behavior
"number of non-trainable parameters" = 0
"total number of parameters" = whatever "number of trainable parameters" is
Code to reproduce the issue
from keras.layers import Input, Dense
from keras.models import Model
'''Define the network architecture. Layer 1 and 2 are to be shared.'''
inputs = Input(shape=(10,))
layer_1 = Dense(10, name='layer_1')(inputs)
layer_2 = Dense(10, name='layer_2')(layer_1)
layer_3 = Dense(10)(layer_2)
layer_4 = Dense(10)(layer_2)
'''Define model 1'''
model_1 = Model(inputs=inputs, outputs=layer_3)
model_1.compile(loss='mse',optimizer='sgd')
'''Freeze layer 1 and 2 for model 2'''
model_1.get_layer('layer_1').trainable = False
model_1.get_layer('layer_2').trainable = False
'''Define model 2'''
model_2 = Model(inputs=inputs, outputs=layer_4)
model_2.compile(loss='mse',optimizer='sgd')
'''Now you can see the error in model_1.summary()'''
model_1.summary()
'''Correct summary for model_2'''
model_2.summary()