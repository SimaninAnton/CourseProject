cesarsalgado commented on 10 Jan 2016
In a discussion in https://groups.google.com/a/tensorflow.org/d/msg/discuss/V6aeBw4nlaE/VUAgE-nXEwAJ Mark Daoust suggested an impl for leaky relu as:
tf.maximum(alpha*x,x)
Wouldn't be better to change the current impl from:
def relu(x, alpha=0., max_value=None):
    negative_part = tf.nn.relu(-x)
    x = tf.nn.relu(x)
    if max_value is not None:
        x = tf.clip_by_value(x, tf.cast(0., dtype=_FLOATX),
                             tf.cast(max_value, dtype=_FLOATX))
    x -= tf.constant(alpha, dtype=_FLOATX) * negative_part
    return x
to something like:
def relu(x, alpha=0., max_value=None):
    x = tf.maximum(tf.cast(alpha, dtype=_FLOATX)*x, x)
    if max_value is not None:
        x = tf.minimum(tf.cast(max_value, dtype=_FLOATX), x)
    return x
I think the two are identical at least for alpha <= 1.
Is it preferable to use tf.nn.relu because it will be faster?
In one place you use tf.constant to cast alpha to the correct type and in other place you use tf.cast to cast max_value. Which one is better or are they equivalent?
Thanks