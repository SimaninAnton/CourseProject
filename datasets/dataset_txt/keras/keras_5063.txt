cdicle commented on 31 May 2016 â€¢
edited
Warning: I am new to Keras, python and whole collaborative coding scene, and falling in love with all.
I have written a simple transfer learning example fairly similar to examples/mnist_transfer_cnn.py. The difference in my code is I change the structure of the network. In mnist_transfer_cnn.py the structure does not change. In my example, I train the original network for 0-5 (6 classes) and then do the transfer learning for 6-9 (4 classes). In order to achieve that I remove last 5 layers and add new ones, freeze conv layers, compile and train.
I also surprisingly realized that compilation does not change the weights for already existing layers. That might be obvious for some of you but it was not for me.
This example might illustrate quick and easy manipulation of the trained network. Hopefully, in near future, one can load a Caffe model (eg AlexNet) remove last couple layers and get the FC7 features, instead of writing custom code to load the weights.
What do you think? Am I too naive?
Cha