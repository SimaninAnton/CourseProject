tagucci commented on 8 Jul 2016
Hi,
Now I'm implementing CNN + LSTM for a sequential labeling task.
However, I have a trouble of implementing.
My implementation outline is similar to imdb_cnn_lstm.py, but a little different in detail. In my setting, I have a set of documents(each document has set of sentences as input data), and target data has a correct label(general sentence/specific sentence) like below.
 docs = [[['Nobody', 'knows', 'why', 'our', 'early', 'ancestors', 'decided', 'to', 'get', 'off', 'their', 'knuckles', 'and', 'stand', 'upright.'], 
           ['Maybe', 'they', 'just', 'wanted', 'a    ', 'better', 'view', 'of', 'the', 'stars.'], 
           ['And', 'when', 'sky', 'gazers', 'finally', 'realized', 'that', 'the', 'heavenly', 'lights', 'were', 'not', 'the', 'footprints', 'of', 'the', 'gods,', 'but', 'rather', 'millions', 'of', 'blazing', 'stars', 'like', 'our', 'Sun', 'writ', 'far,', 'they', 'began', 'to', 'wonder,', 'How', 'do', 'we', 'get', 'there?']], 
           [['Among', 'the', 'possible', 'food', 'groups', 'are', 'wheat,', 'rice,', 'sweet', 'potatoes,', 'beans,', 'soy,', 'corn,', 'herbs', 'and', 'spices.'], 
           ['In', 'addition,',     'space-minded', 'agronomists', 'are', 'exploring', 'the', 'marvels', 'of', 'microbes.Plants', 'take', 'weeks', 'to', 'grow,', 'but', 'yeastlike', 'micro-organisms', 'replicating', 'in', 'vats', 'can', 'be', 'used', 'to', 'churn', 'out', 'significant', 'quantities', 'of', 'carbohydrates,', 'sugars,', 'proteins', 'and', 'fats', 'in', 'a', 'matter', 'of', 'hours.'],
           ['Of', 'benefit', 'to', 'a', 'community', 'in', 'which', 'recycling', 'is', 'not', 'just', 'a', 'personal', 'virtue', 'but', 'a', 'public', 'necessity,', 'micro-org    anisms', 'can', 'live', 'on', 'the', 'carboniferous', 'waste', 'products', 'of', 'plants', 'and', 'people.']],
           [['AstraZeneca', 'hired', 'Dr.', 'DelBello', 'and', 'Dr.', 'Kowatch', 'to', 'give', 'sponsored', 'talks.They', 'later', 'undertook', 'another', 'study', 'comparing', 'Seroquel', 'and', 'Depakote', 'in', 'bipolar', 'children', 'and', 'found', 'no', 'difference.'],
           ['Dr.DelBello,', 'who', 'earns', '$183,500', 'annually', 'from', 'the', 'University', 'of', 'Cincinnati,', 'would', 'not', 'discuss', 'how', 'much', 'she', 'is', 'paid', 'by', 'AstraZeneca.']]]
label = [[0, 0, 1], [0, 1, 1], [1, 1]] 
To make a sequence same length, padding the doc and sentences.
In above dataset, maximum number of sentences in each document are 3, and maximum number of words in each sentence are 39. Converting words into wordID and padding results are below. In my setting, I want to do binary classification. However, I assign 2 to PAD target label .
input_data =[array([[ 95,  85,   9,  98,  53,  29,   4, 138, 122,  18,  92,  57, 146,
         62,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [ 22, 143, 131,  75,  79,  38, 119,  33,  94, 105,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [ 51,   6,  70, 147,  31,  28,  37,  94, 107, 128,  86, 117,  94,
         42,  33,  94, 123,  84,  19, 129,  33,  58,  13,  52,  98,  83,
         66,  81, 143,  64, 138,  69,  93, 148,  30, 122,  27,   0,   0]], dtype=int32), array([[ 35,  94,  41,  10,  20,  71, 110,  16,  24, 112,  46, 125,  26,
        108, 146,  99,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [135,  73,  44,  55,  71,  40,  94, 104,  33,  97,  34,  80, 138,
        113,  84, 100,  12, 118, 132,   5,  88,  43, 127, 138, 134, 114,
          1,  56,  33,  87,  65, 137, 146,  74, 132,  79,  32,  33,  82],
       [130,  54, 138,  79,  76, 132,  15, 149,  96, 117, 131,  79,  78,
         25,  84,  79,  90,  23, 121,  88, 109, 101,  94, 140, 144,  17,
         33,  11, 146,  14,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32), array([[141, 133,  89, 102, 146,  89,  72, 138, 136,  59,  45,   7, 139,
          2,  61, 126,  91, 146,  39, 132, 142, 124, 146, 111, 115, 120,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [ 67,  68,  63,  50,  36,  60,  94, 106,  33,  77,  49, 117,  47,
        103,  48, 116,  96,   8, 145,  21,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32)]

target_label = [array([[ 1.,  0.,  0.],
       [ 1.,  0.,  0.],
       [ 0.,  1.,  0.]]), array([[ 1.,  0.,  0.],
       [ 0.,  1.,  0.],
       [ 0.,  1.,  0.]]), array([[ 0.,  1.,  0.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.]])]
After converting dataset, I construct below architecture
from keras.layers import Input, Embedding, LSTM, Dense, merge, Flatten
from keras.layers.core import Reshape
from keras.models import Model
from keras.layers import Convolution1D, MaxPooling1D
word_dim = 50
maxlen_sents = 3
maxlen_words = 39

inputs = Input(shape=(maxlen_sents, maxlen_words), dtype='int32')
x = Flatten()(inputs)
x = Embedding(output_dim=word_dim, input_dim=len(words)+1, input_length=maxlen_words*maxlen_sents)(x)
filters, filter_num = [1, 2, 3], 100
cnn_list  = []
for f in filters:
    cnn = Convolution1D(nb_filter=filter_num, filter_length=f, activation='relu', border_mode='valid')(x)
    cnn = MaxPooling1D(pool_length=maxlen_words-f+1)(cnn)
    cnn = Flatten()(cnn)
    cnn_list.append(cnn)
cnn_concat =  merge(cnn_list, mode='concat')
model = Model(input=inputs, output=cnn_concat)

# after that I want to add LSTM() and TimeDistributed(Dense(3)) and Activation('softmax')
I want to feed sentence embedding by CNN to LSTM, but CNN made document embedding not sentence embedding...

In this case, I want to get (None, 3, 900) after CNN layer(900dim embedding in each 3 sentences). However, I have no idea how to solve this problem and complete my implementation.
Any advice or comment would be appreciated. Thanks!