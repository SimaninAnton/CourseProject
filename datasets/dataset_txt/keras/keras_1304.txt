agamemnonc commented on 22 Jan 2018
I think it would be very useful to provide ReLU as an advanced activation layer (similar to PReLU, LeakyReLU, ELU etc.) so that it can be used with a combination of non-default parameters, i.e. alpha and max_value.