speedcell4 commented on 5 Dec 2016 â€¢
edited
In Tensorflow, I can do something like this
x = tf.placeholder(tf.float32, shape=[None, input_dim], name='x')
y_true = tf.placeholder(tf.int64, shape=[None], name='y_true')

y_true_one_hot = tf.one_hot(y_true, output_dim, on_value=1.0, off_value=0.0)

h1 = fully_connected(x, input_dim, activation_fn=tf.nn.sigmoid)
h2 = fully_connected(h1, input_dim, activation_fn=tf.nn.sigmoid)
y_pred_logits = fully_connected(h2, output_dim, activation_fn=tf.nn.softmax)

y_pred = tf.argmax(y_pred_logits, axis=1)

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_pred_logits, y_true_one_hot))
optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)
Please notice the loss is computed by y_pred_logits, instead of just using y_pred, while I still use y_pred, as the output of the model, to predicate.
Can Keras do the same thing? I mean, seems the Keras Model.compile's parameter optimizer automatically just use the output of Model...