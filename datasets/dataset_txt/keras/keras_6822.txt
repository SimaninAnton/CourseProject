w0nk0 commented on 28 Jul 2015
I have worked on models for text generation (cough..Karpathy..cough) as my getting-back-into NNs project for a while now. Slowly getting to where they make sense, I would like to preserve the trained network's states. I'm assuming that model.to_yaml() -> model_from_yaml() and model.get_weights() -> model.set_weights() should do that, but either I'm missing something or there's a bug in one of those two functions.
Here's my example code (with a keras version cloned from git yesterday afternoon):
newmodel = model_from_yaml(model.to_yaml()) 
# copy the model (model_from_yaml() should compile() according to source code)
wts=model.get_weights()
newmodel.set_weights(wts)
This should copy over the weights and thus the trained state
So let's compare the errors of both on a subset of my training data:
print(model.evaluate(trainX[:1000],trainy[:1000]))
print(newmodel.evaluate(trainX[:1000],trainy[:1000]))
.. results in this output:
1000/1000 [==============================] - 2s     
0.46311536622

1000/1000 [==============================] - 2s     
4.9628265152
So the errors are clearly not the same. The generated text also coincides with what seems to be an untrained new_network, just like the evaluate error level indicates.
So - am I missing a piece of the puzzle to save/load a trained model or did I find a bug?