phdowling commented on 6 Sep 2016
I'm building my model with optimizer="rmsporp". Since my dataset is quite large, training on the entire dataset for an epoch takes around one day, so I'd like to be able to stop training at any point but still keep whatever "progress" the model has made in the last few hours. For this, I manually split the data into smaller chunks that I train on for one "epoch" at a time via model.fit(), so that validation testing (via ModelCheckpoint) is done and model weights stored more frequently.
While this seems to work fine in practice, I am now wondering whether the optimizer will do its job correctly if I train the model in this fashion. Will the optimizer be reset on every call to model.fit() ? In this case, gradient updates magnitudes would probably be suboptimal.
The alternative would be to write a model callback that does validation testing in the style of ModelCheckpoint, but does not wait for the whole epoch to finish. Is this the only "right" way to do it, or will my method work fine?