Contributor
rpinsler commented on 23 Nov 2015
Hi all,
I try to build a MDN-like model with keras. This basically adds a GMM on top of a network, which can be modeled using appropriate activation functions, i.e. for a D-dimensional output and M mixture components, we would apply:
linear activation for the means (M*D outputs)
exponential activation for the variances (M outputs, assuming radial covariance matrix)
softmax activation for the weights (M outputs)
I tried to model this using Graph but the only thing I could come up with is to split the (intermediate) output of the network into three dense layers with M*D, M and M outputs, respectively, and then apply the activations mentioned above. However, I don't want to learn separate weights, but rather apply the three activations to different parts of the output (since this is typically done if I understand it correctly).
I had a look into the Lambda layer too, but then again it seems that indexing in Theano isn't straightforward as well.
Any ideas?