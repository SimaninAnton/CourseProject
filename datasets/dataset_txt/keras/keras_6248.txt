c19 commented on 17 Dec 2015
from keras.models import Sequential
from keras.layers.core import Activation, Dense, Dropout
from keras.optimizers import SGD
import numpy as np
import theano
from theano import pp

X_test = X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
Y_test = Y_train = np.array([0, 1, 1, 0])

model = Sequential()
model.add(Dense(2, input_shape=(2,)))
model.add(Activation('sigmoid'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', class_mode='binary', optimizer='sgd')

history = model.fit(X_train, Y_train, batch_size=4, verbose=1, shuffle=False, show_accuracy=True, nb_epoch=500)
score = model.evaluate(X_test, Y_test, show_accuracy=True)
print(score)
loss is dropping, but never get it right
model.predict(X_train)
array([[ 0.55641359],
       [ 0.51271898],
       [ 0.57324523],
       [ 0.52622819]])
I tried manually set the Ws and bs for each layer. and it got complete different result.
I checked the resulted network structure which is the same. I guess misunderstood something or something is really wrong. but I checked and experimented with different loss function. optimizers. had no clue :(