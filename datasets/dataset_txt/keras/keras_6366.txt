mhamilton723 commented on 1 Dec 2015
I'm attempting to implement a RNN for time series prediction. I have found a blog post:
http://danielhnyk.cz/predicting-sequences-vectors-keras-using-rnn-lstm/
which discusses the problem but I do not believe it is correct (because it only predicts one value into the future). I am attempting to fix this by using masking for my input sequence.
my data looks roughly like
original time series [1,2,3,4,5,6,7,8,....]
X = [[1,2,3,0,0],[[2,3,4,0,0]...], Y=[[1,2,3,4,5],[2,3,4,5,6]...]
Is this implementation correct, or should I use a different architecture?
Thanks for your help!
def masked_dataset(data, n_prev=3, n_masked = 2):
    """
    data should be pd.DataFrame()
    """
    docX, docY = [], []
    for i in range(len(data) - n_prev - n_masked):
        x = data.iloc[i:i + n_prev].as_matrix()
        x_mask = np.zeros((n_masked, x.shape[1]))
        docX.append(np.concatenate((x, x_mask)))

        y = data.iloc[i:i + n_prev + n_masked].as_matrix()
        docY.append(y)
    alsX = np.array(docX)
    alsY = np.array(docY)
    return alsX, alsY

in_out_neurons = len(data.columns)
model = Sequential()
model.add(Masking())
model.add(GRU(in_out_neurons, 300, return_sequences=True))
model.add(GRU(300, 500, return_sequences=True))
model.add(Dropout(0.2))
model.add(GRU(500, 200, return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributedDense(200, in_out_neurons))
model.compile(loss="mean_squared_error", optimizer="rmsprop")