insectatorious commented on 23 Aug 2016
Summary
Initializing keras.preprocessing.text.Tokenizer without any arguments results in a MemoryError. This appears to be because the variable nb_words is not set. Setting this to 1000 in the constructor fixes this issue. Suggested fix: if nb_words is not set using an argument it should have a sensible default.
See the script below for an example.
bug.py - script to reproduce error
Run this script after downloading and unzipping the attached data file (17MB). Change the value of the SOURCE_FILE variable to point to the unzipped file.
import io
import json
import keras
from keras.preprocessing import text

SOURCE_FILE = "/tmp/reddit_titles"
with open(SOURCE_FILE, "r") as f:
    data = json.load(f)

data = [d.encode('ascii') for d in data]

tk = text.Tokenizer() # Change this to fix: tk = text.Tokenizer(nb_words=1000)
tk.fit_on_texts(data)
tfidf = tk.texts_to_matrix(data, mode="tfidf")
Traceback (MemoryError)
Traceback (most recent call last):
  File "bug.py", line 13, in <module>
    tfidf = tk.texts_to_matrix(data, mode="tfidf")
  File "/home/sumanas/Documents/python/keras/local/lib/python2.7/site-packages/Keras-1.0.7-py2.7.egg/keras/preprocessing/text.py", line 167, in texts_to_matrix
    return self.sequences_to_matrix(sequences, mode=mode)
  File "/home/sumanas/Documents/python/keras/local/lib/python2.7/site-packages/Keras-1.0.7-py2.7.egg/keras/preprocessing/text.py", line 191, in sequences_to_matrix
    X = np.zeros((len(sequences), nb_words))
MemoryError
Using Keras version:
$ git describe --abbrev=4 HEAD
1.0.7-40-g090b
Desc Version
OS Ubuntu 14.04.5 64-bit
Python 2.7.6
Numpy 1.11.1
DATA FILE: reddit_titles.zip (17MB)