AlexVaith commented on 28 Nov 2019 â€¢
edited
System information
Have I written custom code (as opposed to using example directory): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac OS 10.15.1
TensorFlow backend (yes / no): yes
TensorFlow version: 2.0.0
Keras version: 2.3.0
Python version: 3.7
CUDA/cuDNN version:
GPU model and memory:
I want to build a data generator based on the keras.utils.Sequence class to avoid memory issued when dealing with very big datasets. The generator looks like the following
class DataGenerator(Sequence):
    def __init__(self, batch_size, id_list, dim, shuffle=True):
        # batch_size: batch_size at each iteration
        self.batch_size = batch_size
        self.id_list = id_list
        self.dim = dim
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        # Denotes the number of batches per epoch
        return int(np.floor(len(self.id_list)/self.batch_size))

    def __getitem__(self, index):
        '''''
        MAIN function:
        - index tells the number of batches that have already been processed
        :return: X and y with shape [batch_size, window_size, features]        
        '''''
        # check if the cache holds enough values to feed the model with shape [batch_size, *dims]
        X, y = self.__data_generation(self.id_list[index])
        return X, y

    def on_epoch_end(self):
        if self.shuffle:
            random.shuffle(self.id_list)

    def __data_generation(self, fname):
        # 1. load data from specified path
        data = self.__load_data(fname)
        return np.squeeze(data[0, 0]), np.squeeze(data[1, 0])

        # --------------------------------------- utility functions data loading
    def __load_data(self, filename):
        # load data from the filename
        return np.load(filename, allow_pickle=True)
It is called with the fit_generator like this:
train_gen = DataGenerator(batch_size=1,
                              id_list=train_files,
                              dim=(256, 6),
                              shuffle=True)

model.fit_generator(generator=train_gen,
                        validation_data=val_gen,
                        epochs=20,
                        verbose=1,
                        use_multiprocessing=False,
                        workers=4,
                        max_queue_size=10)
every batch has the shape of [64, 256, 6] and is a float array
With every batch the memory rises. In total I have 298 training batches. Why does the memory rise to 18gb after 4 epochs. A single batch has the a size of 791kb.
It still happens with multi_processing=False, workers=1 and max_q_size=1.
I also tried it with keras 2.2.4 and tensorflow 1.14.0. The result is the same
Thanks for help.