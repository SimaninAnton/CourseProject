Contributor
EderSantana commented on 16 Aug 2015
I'd like to ask if we could make the optimizer's learning rate (lr) and momentum (momentum) into shared scalars. This way we could change their values during training with .set_value using custom rules.
I could work on the PR as long as you guys don't have a reason for not to.