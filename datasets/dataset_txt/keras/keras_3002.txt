enavarrocomes commented on 14 Mar 2017
Is there a way I can symbolically initialize the state of a LSTM model with a previous state, while training?
This is, let's suppose my sequences have 10 timesteps, and my batch size is 5. I would like to use the last state of the 1st sample to initialize the first state of the 11th sample (use the last state of the first sample of the 1st batch to initialize the state for the first sample of the 3rd batch).
I know I can do something similar by fixing batch_size = timesteps, and setting stateful = True. But for my data, those hyperparameters being way different have proven to yield the best results. I am trying to get better results by adding more memory to the model using the method depicted above.
Any idea on how can I do this?