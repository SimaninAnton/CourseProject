NilsWinter commented on 1 Mar 2017
Hi there!
I noticed a very strange behaviour in a simple classification task depending on the keras syntax I use.
In my classification task I have two groups and therefore put a Dense(2) Keras layer plus a softmax activation at the end of my model. If I compile the model, there are two different syntaxes from which one can choose. Strangely, I noticed that for one version the model does learn something (I don't care if the model is just fitting noise at this point) and for one it doesn't. Down below you can see the two versions. They are almost identical except for the way the optimizer is passed to the compile() method.
This only happens with my data, the Adam optimizer and the softmax layer at the end of the model. Very strange.
Does anyone have even the slightest idea what's going on there? I don't have any explanation for that behaviour but now have developed some mistrust in the analyses I did before.
Thanks a lot for your help!
You can download my data from: https://drive.google.com/open?id=0B_3vtX0VzrOMVEIwdFo2U2FsU0E
import numpy as np
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from keras.layers.core import Activation
import matplotlib.pyplot as plt
%matplotlib inline


# Load data
data = np.load('/YOUR-PATH-TO-DATA/data_keras_bug.npy')
labels = np.load('/YOUR-PATH-TO-DATA/labels_keras_bug.npy')
input_dim = data.shape[1]


# Create first model
model1 = Sequential()
model1.add(Dense(1500, input_dim=input_dim))
model1.add(Activation('relu'))
model1.add(Dense(2))
model1.add(Activation('softmax'))
# declare the optimizer within the compile() method 
model1.compile(loss='categorical_crossentropy', optimizer='adam', lr=0.01, metrics=['accuracy'])


# Create second equivalent model
model2 = Sequential()
model2.add(Dense(1500, input_dim=input_dim))
model2.add(Activation('relu'))
model2.add(Dense(2))
model2.add(Activation('softmax'))
# declare the optimizer explicitly before passing it to the compile() method
optimizer = Adam(lr=0.01)
model2.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])


# fit both models the same way
results1 = model1.fit(data, labels, batch_size=32, nb_epoch=500, verbose=0)
results2 = model2.fit(data, labels, batch_size=32, nb_epoch=500, verbose=0)

# plot the training accuracies
plt.plot(results1.history['acc'])
plt.plot(results2.history['acc'],'k')
plt.show()