cottrell commented on 27 Apr 2015
I am just starting to explore kera and if I understand the layout, it seems like penalty/constraints are not really abstracted to the extent that other concepts are. Is there some obvious reason this would not work or be dangerous?
For example, I could imagine applying generic penalties to either weights or the activations. Like a sparsity inducing KL penalty that I typically want to apply to activations. If it was fully abstracted, I could try to apply it to the weights of some layer. This would be strange but it seems like it would maximize modularity and separation of concepts.
It seems like PR77 #77 is moving toward a kind of specialized penalty and there is already an L2/L1 penalty in the optimizers.