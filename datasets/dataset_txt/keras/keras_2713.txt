Contributor
nigeljyng commented on 6 Apr 2017 â€¢
edited
I'm trying to element-wise multiply a matrix tensor with a vector. I used K.tile() and the code below works:
>>> a           # shape=(?, 100)
>>> hidden      # shape=(?, 100, 256)
>>> tiled_a = K.tile(K.expand_dims(a, axis=-1), (1, 1, 256))  # shape=(?, 100, 256)
>>> K.sum(Multiply()([tiled_a, hidden]), axis=1)
<tf.Tensor 'Sum_6:0' shape=(?, 256) dtype=float32>
But when I try wrapping the exact same operations in a custom Lambda layer, it fails:
def attention_weighted_average(inputs):
    a = inputs[0]
    hidden = inputs[1]
    tiled_a = K.tile(K.expand_dims(a, axis=-1) , (1, 1, 256))
    return K.sum(Multiply()([tiled_a, hidden]), axis=1)

>>> Lambda(attention_weighted_average)([a, hidden])
ValueError: Operands could not be broadcast together with shapes (256, 256) (100, 256)
It seems like tiled_a receives a different shape of (256, 256) instead of (100, 256) as in the first example. Any thoughts? I'm using the latest Keras (2.0.2) and TensorFlow backend.