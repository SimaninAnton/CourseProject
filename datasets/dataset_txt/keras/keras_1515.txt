xn8812 commented on 7 Nov 2017
Hi, I am training a recurrent neural network (convlstm2d). The problem is that the ground truth is only given at sparse time steps, such as the 1st time step, 10th time step. I want to only do back propagation at those steps but not penalize at the other time steps. However I still want to keep the inputs at the other time steps (i.e., 2nd, 3rd ...). Is there a way to give a loss weight vector to the network? The loss weight vector will look like this: [1,0,0,0,0,0,0,0,0,1...]. So the weight is 1 only at the time steps which have ground truth.
I think the masking is not the solution, right? Please help!!! Thanks!