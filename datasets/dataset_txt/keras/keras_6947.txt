ghost commented on 19 Jun 2015
hi fchollet,
First thanks for this wonderful code.
I'm having some difficulty producing a CNN+LSTM model with embedding matrices for both the CNN features and the word encodings (1 of k).
For the keras example image caption generation model the word encodings were not learned, however my group believes learning the word encodings to be beneficial to the performance of the network.
We also want to feed the image in for only the initial timestep. Thus for each training instance, the input sequence would be:
t(-1): Wi(CNN)
t(0): We(sent_start_symbol)
t(1): We(word_1)
...
t(x):We(sent_end_symbol)
where Wi and We map the CNN features and the 1 of k word encodings to the same dimensional space.
Does keras have this functionality? I attempted to use the merge layer with separate dense layers for the CNN features and words, but the merge layer cannot produce the 3D tensor the lstm layer needs.
Thanks again!