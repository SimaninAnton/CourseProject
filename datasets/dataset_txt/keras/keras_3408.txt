sun-peach commented on 21 Jan 2017
Hi, dear all,
Keras has the merge function for parallel network and I have seen several examples. However, I have a special case that I did not find any example can tell if it is right or wrong. I have several sub network in my whole system. The first part has two parallel networks and their output will be concatenated and sent to the next part. The second part will take in the concatenated input and send them to two parallel network, the resulting output will be concatenated again and sent out. Could anyone tell me if the following code is right?
E1 = Sequential()
E2 = Sequential()
E1.add(Dropout(dropout))
E2.add(Dropout(dropout))
E1.add(LSTM(hidden_dim, batch_input_shape=(shape[1], shape[2][:I1_dim]))
E2.add(LSTM(hidden_dim, batch_input_shape=(shape[1], shape[2][I1_dim:]))
if bidirectional:
E1 = Bidirectional(E1, merge_mode='sum')
E2 = Bidirectional(E2, merge_mode='sum')
encoder = Merge([E1, E2], mode="concat")
input1 = Input(batch_shape=(shape[0],shape[1],shape[2][:I1_dim]))
input2 = Input(batch_shape=(shape[0],shape[1],shape[2][I1_dim:]))
input = Merge([input1,input2],mode="concat")
encoded = encoder(input)
Here, I1_dim represent the dimension of the input stream 1. The shape is the shape of the output from the previous sub network whose format is (sample_num,length,feature_dim).
I am not quite sure if this is right? Can someone tell me? If it is not right, how can I correct it?
Thank you very much!