ceteke commented on 8 Jun 2017
Hi,
I was looking at Nietzsche text generation example (https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py). I have seen that after predicting, sample(preds, temperature=1.0) function is used which is a softmax. But the network has softmax as an activation function. So after prediction a redundant softmax is applied. Is this on purpose or is it a mistake? If it is not a mistake can you explain the reasoning behind it?
Thanks