Contributor
udibr commented on 2 Apr 2016
In the example of good/bad answer https://github.com/fchollet/keras/blob/keras-1/docs/templates/getting-started/complete_guide_to_the_keras_functional_api.md#multi-input-and-multi-output-models
the answer_lstm is shared handling both the good answer input and the bad answer input.
In this example you would expect that if the lstm was made stateful then it would allow you to handle answers that are span over several batches. Each batch handle sequences of 100 but breaking a long answer over several batches (and making sure the continuation appears in the same index in the batch) will allow you to handle longer answers. Once the answers were feed you need to reset the state.
The shared lstm share their weights but in this case each usage of the lstm needs its own copy of the state.
In other cases you may want to share the same lstm in a sequence to sequence model. In this case you may want the end state of the encoder to be the start state of the decoder. It could be nice if this could be implemented by sharing the same lstm on the encoder and decoder (if there are several layers then each will require a separate shared lstm).
In this case the lstms need to share the state which happens now in the code if you set it to be stateful.
For every batch you first want to reset the state then handle the encoder and then handle the decoder.
Currently the reset is missing from the code and you need to somehow add it. The order in which the same state is re-used across shared locations in the graph is obvious in this case from the graph structure but you can think of graphs in which this is not obvious.
and what if someone wants both statefulness across batches and statefulness inside shared usage in a graph?
2