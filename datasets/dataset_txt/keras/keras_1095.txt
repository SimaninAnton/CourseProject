seanxh commented on 15 Apr 2018 •
edited
In some other frameworks, always learning rate decay every epoch. Like this formula
lr = lr_0 * 1/(1+decay_rate * epoch_num)
But i see in keras get_updates，the updates variable looks will be called every batch_fit_loop.
Does that means keras update learning rate decay between every batch??
3