franneck94 commented on 17 Nov 2018
I want to use the TensorBoard callback to visualize my conv layer kernels. But i can only see the first conv layer kernel in TensorBoard and my Dense layers at the end. For the other conv layers i can just see the bias values and not the kernels.
Here is my sample code for the Keras model.
# Imports
import tensorflow as tf
import numpy as np
import os
from os import makedirs
from os.path import exists, join
from keras.datasets import mnist
import time

from keras.layers import *
from keras.activations import *
from keras.models import *
from keras.optimizers import *
from keras.initializers import *
from keras.callbacks import TensorBoard
from keras.callbacks import ModelCheckpoint
from keras.utils.np_utils import to_categorical

from plotting import *

log_dir = '"./"

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

batch_size = 128
epochs = 10
width = 28
height = 28
depth = 1
num_classes = 10
train_size = x_train.shape[0]
test_size = x_test.shape[0]

x_train = x_train.reshape(train_size, width, height, depth)
y_train = to_categorical(y_train, num_classes=num_classes)
x_test = x_test.reshape(test_size, width, height, depth)
y_test = to_categorical(y_test, num_classes=num_classes)

tb = TensorBoard(
    log_dir=log_dir, 
    histogram_freq=1, 
    write_graph=True, 
    write_images=True)

# Define the DNN
model = Sequential()
model.add(Conv2D(filters=16, kernel_size=3, input_shape=(width, height, depth), name="conv1"))
model.add(Activation("relu"))
model.add(Conv2D(filters=20, kernel_size=3, name="conv2"))
model.add(Activation("relu"))
model.add(MaxPool2D())

model.add(Conv2D(filters=24, kernel_size=3, name="conv3"))
model.add(Activation("relu"))
model.add(Conv2D(filters=28, kernel_size=3, name="conv4"))
model.add(Activation("relu"))
model.add(MaxPool2D())

model.add(Flatten())
model.add(Dense(128))
model.add(Activation("relu"))
model.add(Dense(num_classes, name="features"))
model.add(Activation("softmax"))

# Print the DNN layers
model.summary()

# Train the DNN
lr = 1e-3
optimizer = Adam(lr=lr)
model.compile(loss="categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
model.fit(x_train, y_train, verbose=1, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=[tb])

# Test the DNN
score = model.evaluate(x_test, y_test, batch_size=batch_size)
print("Test performance: ", score)
Here is the resulting screenshot from TensorBoard.