alpapado commented on 12 Oct 2017
Consider the following scenario
from keras.layers import Input, Dense
from keras.models import Model
from keras.optimizers import Adam

x = Input(shape=(100,))
y1 = Dense(units=32)(x)
model1 = Model(inputs=x, outputs=y1)
print("MODEL 1")
model1.summary()

model1.trainable = False
x = Input(shape=(100,))
y1 = model1(x)
y2 = Dense(units=64)(y1)
model2 = Model(inputs=x, outputs=y2)
model2.compile(optimizer=Adam(), loss='categorical_crossentropy')

print("MODEL 2")
model2.summary()

model1.trainable = True
print("MODEL 2 after")
model2.summary()
I would expect that, since model2 has been compiled, the output of the two model2.summary() calls would be the same. However, after running the above code the actual output is
_________________________________________________________________
MODEL 2
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 100)               0         
_________________________________________________________________
model_1 (Model)              (None, 32)                3232      
_________________________________________________________________
dense_2 (Dense)              (None, 64)                2112      
=================================================================
Total params: 5,344
Trainable params: 2,112
Non-trainable params: 3,232
_________________________________________________________________
MODEL 2 after
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 100)               0         
_________________________________________________________________
model_1 (Model)              (None, 32)                3232      
_________________________________________________________________
dense_2 (Dense)              (None, 64)                2112      
=================================================================
Total params: 5,344
Trainable params: 5,344
Non-trainable params: 0
Is this the expected behavior?
In addition, from other experiments I get the feeling that although the number of trainable weights reported by summary() differs in the two cases, the actual number of trainable weights is the expected one, that is, in the example above, even after setting model1.trainable=True, training will really update only 2,112 parameters and not 5,344. So, maybe this is just a reporting issue.