nbansal90 commented on 21 Nov 2017
Hello All,
I am looking to create a new regularization function, which would be added to LOSS Function using keras. I know there is already a code for l1 and l2 regularization loss. But the new regularization term which I am Looking to add is |WtW-W|. Where Wt is transpose of W matrix. and norm |.| is a forbenius Norm. Now My confusion is If I simply implement a function such as this:
def l1_reg(w):
w = K.variable(np.random.random((2,3)))
inp_shape = (K.int_shape(w))
Max_elem = np.max(inp_shape)
W1 = K.transpose(w)
W_new = K.dot(W1,w)
Norm = W_new - w
return K.sum(K.square(Norm))
and add it to kernel_initializer . Would it work ? As the Diffrentiation Here would be wrt WtW which is a Square term.