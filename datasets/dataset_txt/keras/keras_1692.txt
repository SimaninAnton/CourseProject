Contributor
sachinruk commented on 4 Sep 2017 â€¢
edited
This is a crosspost from https://stackoverflow.com/questions/46032700/concatenate-embedding-layers. Would appreciate it if you could answer it there. The question is repeated below:
I'm trying to create a model which has words as inputs. Most of those words are in the glove word vector set (~50000). However, some of the frequent words are not (~1000). The question is, how do I concatenate the following two embedding layers to create one giant Embedding lookup table?
trained_em = Embedding(len_glove_words, 50, 
                       weights=np.array([word2glove[w] for w in words_in_glove]), 
                       trainable=False)
untrained_em = Embedding(len(word2num)-len_glove_words, 50)
As far as I understand these are simply two lookup tables with same number of dimensions. So I'm hoping that there is a way to stack these two lookup tables.
Edit 1: This is what I would expect to do in a numpy array representing the Embedding:
np.random.seed(42) # Set seed for reproducibility
pretrained = np.random.randn(15,3)
untrained = np.random.randn(5,3)
final_embedding = np.vstack([pretrained, untrained])

word_idx = [2, 5, 19]
np.take(final_embedding, word_idx, axis=0)
I believe the last bit can be done with something to do with keras.backend.gather but not sure how to put it all together.