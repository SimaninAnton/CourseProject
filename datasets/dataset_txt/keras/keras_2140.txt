Maxfashko commented on 14 Jun 2017 â€¢
edited
Hello. I had a problem with using a large dataset for semantic segmentation (more than 500k images 256 * 256). How do I load large amounts of data?
Is it possible to use this approach?
def get_image(count):
x_train = np.load('imgs_train{part}.npy'.format(part=count))
y_train = np.load('mask_train{part}.npy'.format(part=count))
return x_train, y_train
for e in range(10):
print("epoch %d" % e)
x_train, x_train = get_image(1) # these are chunks of ~10k pictures
model.fit(x_train, x_train, batch_size=32, nb_epoch=1)
x_train, x_train = get_image(2) # these are chunks of ~10k pictures
model.fit(x_train, x_train, batch_size=32, nb_epoch=1, validation_data=(x_test, y_test))