ghost commented on 30 Jun 2015
I'm experiencing a strange phenomenon with the below model. If I use a batch size of 1, it learns sequences in a comparable number of epochs as the cpu model we are basing it off of. However, if we use a batch size greater than 1, it does not learn any sequences. As far as we can tell the models are equivalent in architecture. Below are examples of the differences for a dataset of two images (this occurs for any number of images):
training on two images with batch size of 2:
GT: a black dog is running after a white dog in the snow
Pr: black black baby plays croquet white white white white snow snow snow snow snow snow snow snow snow snow snow
GT: a little baby plays croquet
Pr: black black baby plays croquet white white white white snow snow snow snow snow snow snow snow snow snow snow
training on two images with batch size of 1:
GT: a black dog is running after a white dog in the snow
Pr: a black dog is running after a white dog in the snow .
GT: a little baby plays croquet
Pr: a little baby plays croquet .
cpu model training on two images with batch size of 2:
GT: a black dog is running after a white dog in the snow
Pr: a black dog is running after a white dog in the snow .
GT: a little baby plays croquet
Pr: a little baby plays croquet .
We've been looking through the backprop process and comparing it with the cpu model, but so far we haven't found any differences. Perhaps you have some insights?
Thanks
image = Sequential()
image.add(TimeDistributedDense(cnn_feature_size, 512))

text = Sequential()
text.add(TimeDistributedDense(token_feature_size, 512))

model = Sequential()
model.add(Merge([image, text], mode='sum'))

model.add(LSTM(512, 512, return_sequences=True))
model.add(TimeDistributedDense(512, token_feature_size))
model.add(Activation('time_distributed_softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')