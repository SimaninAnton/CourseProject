dohmatob commented on 18 Jul 2017 â€¢
edited
Hi,
The following code snippet
from keras.layers import Input, Dense
from keras.models import Model

input_dims = [4, 4]
output_dim = 2
hidden = 3
inputs = []
outputs = []
shared = Dense(hidden, input_shape=(hidden,))
for idx, input_dim in enumerate(input_dims):
    # input layer
    inp = Input(shape=(input_dim,))
    inputs.append(inp)

    # encoding layer
    encoder = Dense(hidden, input_shape=inp.shape)
    z_ = encoder(inp)
    z = shared(z_)

    # prediction layer
    predictor = Dense(output_dim, input_shape=(hidden,))
    y = predictor(z)
    outputs.append(y)
net = Model(inputs, outputs)
net.to_json()
produces the error
TypeError: ('Not JSON Serializable:', Subtensor{int64}.0)
> /home/elvis/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py(2652)get_json_type()
   2650                 return obj.__name__
   2651 
-> 2652             raise TypeError('Not JSON Serializable:', obj)
   2653 
   2654         model_config = self._updated_config()

ipdb>
I looked at the keras-generated dictionary being stored and it contains stuff like 'batch_input_shape': (None, Subtensor{int64}.0, Subtensor{int64}.0), 'use_bias': True, 'activity_regularizer': None}.
The Subtensor token (type ?) is probably the trouble-maker.
Thanks in advance for any insights on this!