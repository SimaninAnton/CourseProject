teaglin commented on 11 Dec 2018
Hi,
I am attempting to train a MobileNetV2 classification model using fp16. I am using the latest GitHub version of Keras that contains the fp16 batch normalization fix. The model compiles and builds and trains, but the accuracy is always stuck at 25% while the lost constantly decreases.
When I disable K.set_floatx('float16') and stop using fp16 the model works fine and converges. Any idea why fp16 isn't working?