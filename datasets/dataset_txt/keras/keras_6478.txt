Contributor
NickShahML commented on 6 Nov 2015
Hey Guys,
I know @EderSantana asked this question a few months ago #395, and I have read several Keras threads that deal with masking.
The question was: We can mask the input of zeros, but won't it bias the cost function as we are not masking the zeros on the output?
Later, the Masking layer was added, and I do believe that this output problem was resolved. However, in order to use masking without an embedding layer, my model structure is:
model = Sequential()
M = Masking(mask_value=0.)
M._input_shape = (x_maxlen, word2vec_dimension)
model.add(M)
model.add(LSTM(hidden_variables_encoding, return_sequences = False)) 
model.add(Dense(hidden_variables_encoding)) 
model.add(Activation('relu'))
model.add(RepeatVector(y_maxlen))
for z in range(0,number_of_decoding_layers):
    model.add(LSTM(hidden_variables_decoding, return_sequences=True))
    model.add(Dropout(dropout))
model.add(TimeDistributedDense(y_matrix_axis, activation="softmax")) 
model.compile(loss='categorical_crossentropy', optimizer='adam')
I have two questions:
My input is in vector form, so I have '0.' (notice the float) for masking my input. My output, however is in integer numpy matrix. So, does the '0.' cover the number '0' in an integer numpy matrix? Or maybe I should use mask_value = 0 (no floating)
Given the model I've presented, does the masking layer mask the zeros that come out of the TimeDistributedDense layer? If not, this would lead to the cost function being biased by zeros!
I apologize for the redundancy of the issue, but it is a very important one. Thank you!
1