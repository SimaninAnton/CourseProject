marioem commented on 29 Aug 2018
Hi,
I have a data set sufficiently large to make RStudio run out of memory when trying to encode my features (one by one) using to_categorical. Is there a way to make this encoding in batches so that the consistency of the encoding is preserved across batches? The even distribution of categories across batches cannot be assumed, i.e. it may happen that when I split my data into, say, two equal parts, first part will contain a subset of all available categories, and the second part - different subset. I'd like to either subsequently combine the encoded batches into one data frame fed into the network or to use a data generator to feed it.
Any help much appreciated.
If the above is not possible, then new feature would be: add a new parameter to to_categorical, a dictionary of categories, so that running encoding on batches of data would yeld always the same number of columns, covering all categories from dictionary vector, and that the encoding across batches would be consistent allowing for posterior rbind or usage of a data generator.
Thanks,
Mariusz