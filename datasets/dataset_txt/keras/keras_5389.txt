sjayakum commented on 21 Apr 2016
model = Sequential()
model.add(LSTM(100, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(LSTM(100, return_sequences=False))
here, does the above way of stacking represent an "Encoder-Decoder" architecture?
Here by return_sequences=True are we only passing the O/P of the 1st LSTM to 2nd or we are passing the context [context vector/hidden activation] also?