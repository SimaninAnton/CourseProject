Imorton-zd commented on 22 Apr 2016
The input feeding to stateful LSTM layer has to be the pre-trained embedding?
If I do not misunderstand, the Embedding layer is to transform a word frequency index to a vector randomly. (Is it according to random data or some other rules?)
But stateful LSTM wants one batch input every time. Then every time, the same word in different batches will be represented by the different vectors. Therefore, I can't use Input and Embedding layer in keras before stateful LSTM layer?
If possible, what is the input shape in Input layer?
Any suggestions would be appreciated!
sequence = Input(shape=(batch_size, maxlen,),dtype='int32')
tweet = Embedding(max_features, embedding_dims, input_length=maxlen)(sequence)
encoder = LSTM(hidden_dims, stateful=True)(tweet)