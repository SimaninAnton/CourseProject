rishabh279 commented on 22 Mar 2019
Hi,
I have build a model using keras framework (with tensorflow backend). I have trained it on four T4 gpus using multi_gpu_model of keras. When I load the same model on CPU oriented gcp instance and use it for prediction it is giving me an error as "Floating point exception (core dumped)" but when I load the same model on instance with single gpu it is able to do the required prediction. I am not able to understand why the model is not able to do the prediction when loaded on CPU oritented instance.
For further investigation, I built another model using one T4 GPU. In this case, it is able to do prediction when loaded on CPU oriented instance.
So, it looks like there is some issue in using keras- "multi-gpu-model" to infer on CPU only machines?
Link to gist:
https://gist.github.com/rishabh279/8e8324bd9aeadf7763375d27451aa669
prediction.py is used for loading the model in which LstmModel class is called whose init method loads the saved model. selected_model. lstm_model.predict defined in predict_text function is used for predicting the sentence, but is throwing "Floating point exception (core dumped)" error when trained with multiple gpus.
train_model() function defined in LstmModel class is used to train the model.