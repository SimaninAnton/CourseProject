jacobsnoi commented on 14 Jul 2017
I have an existing network I would like to call with new inputs. If I do this several times with different inputs it seems to keep adding the regularization to the total loss. Here is a simple script to demonstrate the issue:
import numpy as np
from keras.models import Model
from keras.regularizers import l1
from keras.layers import Input, Dense
from keras.optimizers import SGD

x1 = Input(shape=(5,), name='x1')
x2 = Input(shape=(5,), name='x2')
x3 = Input(shape=(5,), name='x3')

# create a linear regression model
# if you set the regularizer weight to zero you can see the model 
# makes perfect predictions for the selected input_vals
y = Dense(1, kernel_initializer='ones', kernel_regularizer=l1(1))(x1)

lin1 = Model(inputs=[x1],outputs=[y],name='lin1')
lin1.compile(loss='mse',optimizer=SGD())

lin2 = Model(inputs=[x1],outputs=[lin1(x1)],name='lin2')
lin2.compile(loss='mse',optimizer=SGD())

lin3 = Model(inputs=[x2],outputs=[lin1(x2)],name='lin3')
lin3.compile(loss='mse',optimizer=SGD())

lin4 = Model(inputs=[x3],outputs=[lin1(x3)],name='lin4')
lin4.compile(loss='mse',optimizer=SGD())

input_vals = np.ones((100,5))
target_vals = 5*np.ones((100,1))
print "lin1 = ", lin1.evaluate(input_vals,target_vals,verbose=0)
print "lin2 = ", lin2.evaluate(input_vals,target_vals,verbose=0)
print "lin3 = ", lin3.evaluate(input_vals,target_vals,verbose=0)
print "lin4 = ", lin4.evaluate(input_vals,target_vals,verbose=0)
I would expect each of these to output 5 (because the prediction is perfect and the L1 norm of kernel is 5). Here is the output I get:
lin1 =  5.0
lin2 =  5.0
lin3 =  10.0
lin4 =  15.0
Is this expected behavior? Am I just misusing Keras or is it a bug?
This seems somewhat related to issue #5318. I am using the latest Keras and the bug happens with the Theano and Tensorflow backends.