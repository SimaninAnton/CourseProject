sanjeevmk commented on 16 Sep 2016 â€¢
edited
As the title says, consider the following scenario:
Layer 1 (trainable) ---> Layer 2 (untrainable) ---> Layer 3 (trainable)
Layer 3 is towards the output , so the weight update flows from Layer 3 towards Layer 1.
How does back propagation happen in this scenario , in Keras?
Does Layer 2 "block" the flow of updates to Layer 1, that is Layer 1 weights also become untrainable by extension?
Or is Layer 1 updated, but by using the error in Layer 2 weights?
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
1