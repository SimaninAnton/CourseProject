sneakers-the-rat commented on 14 Apr 2017
A model that is trained down to ~.8 categorical crossentropy loss and ~100% accuracy will return back to >2 loss and chance accuracy upon reloading. The weights are all the same as in the .h5 file, and when I run model.predict() on the same dataset as was used for training all the outputs for the first class are near 1.
I don't know how much detail you need on the model, but it's a simple convolutional model w/ dropout, batch normalization & max pooling terminating in a dense layer with softmax output, trained with Adam and categorical crossentropy loss. I can make a gist of the model code if needed.
On Keras 2.3, Theano 0.9.0
I've noticed that if I change the "from_logit" flag to true in the categorical_crossentropy backend function is changes the predictions (lowers it to be ~.75 and ~.25, but doesn't fix them) - should it? Seems like I should just be getting the output from the last layer. This happens with both categorical and binary crossentropy.
Let me know if you need more information.