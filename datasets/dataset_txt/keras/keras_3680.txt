Queequeg92 commented on 14 Dec 2016
When I set L2 to some typical values(e.g. 0.005) in some state-of-the-art papers(usually use deep networks with lots of parameters), I got a much higher training loss compared with caffe. So I cannot repeat the results of same state-of-the-art papers(implemented in caffe) using the same hyperparameters in keras. Does anyone can explain the L2 implementation difference or other differences between keras and caffe?