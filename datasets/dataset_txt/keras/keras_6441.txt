MilesQLi commented on 13 Nov 2015
I implement according to the original paper as
def adam(cost, params, learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999, eta = 1e-8):
grads = T.grad(cost, params)
ms = [theano.shared(np.array(np.zeros(p.get_value().shape), dtype=theano.config.floatX)) for p in params] # @UndefinedVariable
vs = [theano.shared(np.array(np.zeros(p.get_value().shape), dtype=theano.config.floatX)) for p in params] # @UndefinedVariable
iteration = theano.shared(np.array(0.)) # @UndefinedVariable
updates = []
updates.append((iteration,iteration+1))
for p, g, m, v in zip(params, grads, ms, vs):
m_new = beta1_m + (1-beta1) * g
v_new = beta2_v + (1-beta2)g__2
m_ka = m_new / (1-beta1__iteration)
v_ka = v_new / (1-beta2*iteration)
delta_theta = learning_rate *m_ka/(T.sqr(v_ka)+eta)
            updates.append((m, m_new))
            updates.append((v, v_new))
            updates.append((p, p - delta_theta))
        return updates
but it doesn't work. Your version is not exactly as what the algorithm describes but it works. Do you know why?