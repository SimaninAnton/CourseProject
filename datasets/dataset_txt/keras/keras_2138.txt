Contributor
ziky90 commented on 14 Jun 2017
I'm using keras==2.04 and tensorflow==1.1.0 and I'm experiencing following problem.
When I am plying with custom ResNet50 for semantic segmentation that ends by the softmax activation that goes straight to the losses.categorical_crossentropy() function, then following things are happening to me (multiple times on different types of datasets):


I have managed to fix this thing by 2 different approaches:
This seems to me rather as a hack, use clipvalue for gradient => but the training does not goes well (loss and accuracy are still pretty much the same)
Have "raw" logits as the output from the ResNet50 and use basically else condition from losses. categorical_crossentropy(), so tf.nn.softmax_cross_entropy_with_logits(), here everything seems to be working as expected
eg.
experiment on dataset1:


experiment on dataset2:


I assume that there is some bug somewhere here https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2745 I'd expect that there might be performed division by 0 here or loss becomes negative at some time?