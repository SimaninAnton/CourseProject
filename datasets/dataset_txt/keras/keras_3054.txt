duguyue100 commented on 8 Mar 2017
Hi,
I'm trying to implement batch normalisation in recurrent networks.
Instead of writing another batch normalisation in the class, I would like to find a way of using BatchNormalization layer. But it doesn't seem to work.
Is there anyway of doing something like this?
Here is what I did, I init one BatchNormalization layer in the __init__ function and build it in my build function. Since it's in the model, I added the trainable_weights and non_trainable_weights in the layer's weight's list. And finally I call it in the call function but then the model couldn't compile.
The conclusion is I think I did something wrong and nasty, so can someone give a small example that works?
Thanks in advance!