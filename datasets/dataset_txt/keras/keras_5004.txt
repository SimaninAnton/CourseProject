o0windseed0o commented on 8 Jun 2016 â€¢
edited
Hi everyone,
I have worked on theano for a period but a newbie to keras. Currently I am going to implement a sequence labeling task using keras. I have read some codes in the example dir but still feel confused about the architecture of keras models on sequence labeling.
Task:
Tagging each sentence in a dialogue with 1 of the pre-defined categories (say 0,1,2,3), and we might call the task as dialogue act tagging (classification).
i.e.
Batch 1:
Q: sentence1 -> 1
A: sentence2 ->2
Q: sentence3 ->0
A: sentence4 ->3
...
Each dialogue (of variable length) is compromised by several sentences, and the sentences thus form a sequence (a sequence of sentences). (Am I clear?)
My preliminary idea is to use a pipeline analogous to [https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py], with a CNN and an RNN (i.e. LSTM) to encode each sentence (i.e. sentence2) and the sequence (i.e. sentence1~4) respectively.
The current plan is just like this:
model = Sequential()
model.add(Embedding(weights=[embedding_weights])) #the pretrained embeddings
model.add(Convolution1D) #for each sentence
model.add(MaxPooling1D) #for each sentence
model.add(TimeDistributed(LSTM)) #to wrap all the sentences?
model.add(Activation)
I notice that TimeDistributed is the function to wrap multiple steps but it seems that I couldn't learn much about it from the given example usage.
Question1: Anyone could provide some guidelines?
Question2: How should my input data look like (just like below)?
X_train = [[1,2,3], [4,5,6], [7,8,9], [8,7,6]]
y_train = [1,2,0,3]
Question3: There is no need to padding the sequence (not the sentence), right? I remember there is a None parameter.
Thanks in advance.