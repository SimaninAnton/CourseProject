liueryun commented on 25 May 2016
My model and generator are attached.
My question is on the output of training process.
The first loss is small (3.0912), and the second is big (357.17), and then decrease. Why?
It seems the training process gets several batches of data at first without learning from them. See the printed numbers. I'm getting confused by the output.
My model:
def GetModel():
    model = Sequential()

    model.add(Convolution2D(32, 3, 3,
                            border_mode='valid',
                            input_shape=(3,40,40)))
    model.add(Activation('relu'))
    model.add(Convolution2D(32, 3, 3))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(128))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(5))

    model.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['mean_squared_error'])
    return  model
The generator:
def generate_fea_patch_from_file(listpath, basedir):
    X = []
    Y = []
    W = []
    batch_size = 128
    block_size = 4
    patch_size = 40
    while 1:
        k = 0
        f = open(list path
        for line in f:
            patch, inbox, label = extract_patches(basedir, line, block_size, patch_size)
            X = X[k:] + patch
            Y = Y[k:] + label
            W = W[k:] + inbox
            for i in xrange(0, len(X)-batch_size, batch_size):
                Input = X[i:i+batch_size]
                Output = Y[i:i+batch_size]
                Input = np.asarray(Input, dtype=np.float)
                Output = np.asarray(Output)

                # mean zero
                Input[:,0,:,:] -= 99.607
                Input[:,1,:,:] -= 126.817
                Input[:,2,:,:] -= 138.892
                # scale to [-1, 1]
                Input /= 255.

                # change channel order
                Input = np.swapaxes(Input, 1, 3)
                Input = np.swapaxes(Input, 2, 3)

                # make output
                Output[:, 0] /= 512

                k = i + batch_size
                print '\n',k
                yield (Input, Output)
        f.close()
output:
Epoch 1/3
Load feature from:  ../data/NIST4_Fea//F0001.bmp
128
256
384
512
640
768
896
1024
1152
1280
1408
  128/20000 [..............................] - ETA: 375s - loss: 3.0912 - mean_squared_error: 3.0912
1536
  256/20000 [..............................] - ETA: 271s - loss: 357.1761 - mean_squared_error: 357.1761
1664
  384/20000 [..............................] - ETA: 232s - loss: 238.5826 - mean_squared_error: 238.5826
1792