superhans commented on 21 Apr 2015
Hi Guys,
First and foremost, I think Keras is quite amazing !!
So far, I see that the largest dataset has about 50000 images. I was wondering if it is possible to work on Imagenet scale datasets (around 1,000,000 images, which are too big to fit in memory), by pre-processing the data (i.e., splitting it into say : 1000 containers of 1000 images each), and feeding one container at a time to the model.fit() function. Or, do I have to save_weights() and load_weights() after each container ?
Thanks for reading.
37