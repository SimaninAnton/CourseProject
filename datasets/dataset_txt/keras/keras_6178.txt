Contributor
berleon commented on 31 Dec 2015
As mentioned by @Sebubu in #1275 the following code results in a DisconnectedInputError:
He notes removing the BatchNormalization layer fixes the code.
from keras.models import Graph,  Sequential
from keras.layers.core import Dense
from keras.layers.normalization import BatchNormalization
from keras.layers.advanced_activations import PReLU


g = Graph()
g.add_input("input", input_shape=[20])
g.add_node(Dense(10), "dense", "input")
g.add_node(BatchNormalization(),"bn", "dense")
g.add_node(PReLU(),"activ", "bn")
g.add_output("output", "activ")
print "g output " + str(g.output_shape)

g2 = Graph()
g2.add_input("input", input_shape=[10])
g2.add_node(Dense(15), "dense", "input")
g2.add_node(BatchNormalization(),"bn", "dense")
g2.add_node(PReLU(),"activ", "bn")
g2.add_output("output", "activ")

model = Sequential()

model.add(g)
model.add(g2)

model.compile(loss="mse",optimizer="adadelta")
The problem here is that in the BatchNormalization layer the update rules of the running_mean and running_std members are set in the build method. When the input of the batch norm layer changes, the update rules are not updated accordingly.