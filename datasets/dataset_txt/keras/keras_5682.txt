Contributor
StephanHeijl commented on 24 Mar 2016
I'm using Word2Vec to train an initial embedding for my dataset. This is done for a labeling task, which necessitates that I pad the input values. In my Word2Vec model, the 0 (standard masking value) maps to the word "the", which I would rather not mask out. Due to the optimization methods Word2Vec uses to store its indices, it is nontrivial to change this value.
In accordance with the CONTRIBUTING.md document, I would like to propose using arbitrary values as the masking value. This is already possible with the standard Masking layer, but the Embedding layer can only be used as the first layer (per the documentation), making this route non-viable.
I have prepared a commit that would illustrate this functionality, whilst retaining the mask_zero parameter to maintain backwards compatibility.
StephanHeijl@1ebb646
Any thoughts on this?