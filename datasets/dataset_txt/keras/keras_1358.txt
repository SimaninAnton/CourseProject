fargiolas commented on 2 Jan 2018
Looking for the source of a memory leak I'm seeing after several hours of training with augmented data from fit_generator I stumbled into the process list and saw a way too long list of python processes.
Here's a sample from ps auxwww | grep python, but there's more... 109 processes at the moment after a couple of hours (epoch 277/700), 200 training and 200 validation samples.
user    26114  0.0 31.0 30467924 5113692 pts/1 S+  17:41   0:00 python ../../train.py --fold 0
user    26115  0.0 31.0 30467924 5113692 pts/1 S+  17:41   0:00 python ../../train.py --fold 0
user    26116  0.0 31.0 30467924 5113692 pts/1 S+  17:41   0:00 python ../../train.py --fold 0
user    28127  0.0 31.8 30689136 5230836 pts/1 S+  17:51   0:00 python ../../train.py --fold 0
user    28128  0.0 31.8 30689136 5230836 pts/1 S+  17:51   0:00 python ../../train.py --fold 0
user    28129  0.0 31.8 30689136 5230860 pts/1 S+  17:51   0:00 python ../../train.py --fold 0
user    28130  0.0 31.8 30689136 5230860 pts/1 S+  17:51   0:00 python ../../train.py --fold 0
user    28131  0.0 31.8 30689136 5230860 pts/1 S+  17:51   0:00 python ../../train.py --fold 0
user    28132  0.0 31.8 30689136 5230860 pts/1 S+  17:51   0:00 python ../../train.py --fold 0
user    28407  0.0 31.8 30910348 5239204 pts/1 S+  17:52   0:00 python ../../train.py --fold 0
user    28408  0.0 31.8 30910348 5239204 pts/1 S+  17:52   0:00 python ../../train.py --fold 0
user    28409  0.0 31.8 30910348 5239204 pts/1 S+  17:52   0:00 python ../../train.py --fold 0
user    28410  0.0 31.8 30910348 5239204 pts/1 S+  17:52   0:00 python ../../train.py --fold 0
user    28411  0.0 31.8 30910348 5239204 pts/1 S+  17:52   0:00 python ../../train.py --fold 0
user    28412  0.0 31.8 30910348 5239204 pts/1 S+  17:52   0:00 python ../../train.py --fold 0
user    30987  0.0 31.6 31131560 5211364 pts/1 S+  18:03   0:00 python ../../train.py --fold 0
user    30988  0.0 31.6 31131560 5211364 pts/1 S+  18:03   0:00 python ../../train.py --fold 0
user    30989  0.0 31.6 31131560 5211364 pts/1 S+  18:03   0:00 python ../../train.py --fold 0
user    30991  0.0 31.6 31131560 5211364 pts/1 S+  18:03   0:00 python ../../train.py --fold 0
user    30992  0.0 31.6 31131560 5211364 pts/1 S+  18:03   0:00 python ../../train.py --fold 0
user    30993  0.0 31.6 31131560 5211364 pts/1 S+  18:03   0:00 python ../../train.py --fold 0
user    31759  0.0 32.3 31352772 5326248 pts/1 S+  18:06   0:00 python ../../train.py --fold 0
user    31760  0.0 32.3 31352772 5326248 pts/1 S+  18:06   0:00 python ../../train.py --fold 0
user    31761  0.0 32.3 31352772 5326248 pts/1 S+  18:06   0:00 python ../../train.py --fold 0
user    31762  0.0 32.3 31352772 5326248 pts/1 S+  18:06   0:00 python ../../train.py --fold 0
user    31763  0.0 32.3 31352772 5326248 pts/1 S+  18:06   0:00 python ../../train.py --fold 0
user    31764  0.0 32.3 31352772 5326248 pts/1 S+  18:06   0:00 python ../../train.py --fold 0
user    32020  0.0 32.5 31573984 5355112 pts/1 S+  18:07   0:00 python ../../train.py --fold 0
user    32021  0.0 32.5 31573984 5355112 pts/1 S+  18:07   0:00 python ../../train.py --fold 0
user    32022  0.0 32.5 31573984 5355112 pts/1 S+  18:07   0:00 python ../../train.py --fold 0
user    32023  0.0 32.5 31573984 5355112 pts/1 S+  18:07   0:00 python ../../train.py --fold 0
user    32024  0.0 32.5 31573984 5355112 pts/1 S+  18:07   0:00 python ../../train.py --fold 0
user    32025  0.0 32.5 31573984 5355088 pts/1 S+  18:07   0:00 python ../../train.py --fold 0
user    32419  0.0 32.3 31795452 5327032 pts/1 S+  18:09   0:00 python ../../train.py --fold 0
user    32420  0.0 32.3 31795452 5327032 pts/1 S+  18:09   0:00 python ../../train.py --fold 0
user    32421  0.0 32.3 31795452 5327008 pts/1 S+  18:09   0:00 python ../../train.py --fold 0
user    32422  0.0 32.3 31795452 5327008 pts/1 S+  18:09   0:00 python ../../train.py --fold 0
user    32423  0.0 32.3 31795452 5327008 pts/1 S+  18:09   0:00 python ../../train.py --fold 0
My fit_generator looks like this:
        model.fit_generator(train_generator,
                            steps_per_epoch=len(train_generator),
                            validation_data=val_generator,
                            validation_steps=len(val_generator),
                            use_multiprocessing=True,
                            epochs=700, verbose=1, workers=6,
                            callbacks=[model_checkpoint, csv_logger, reduce_lr])
...where generators are actually keras Sequences, and callbacks are the usual ones, checkpoint, csv logger, and reduce lr on plateau.
Is there any known issue about multiprocessing with fit_generator that might cause something like this?
I will try to come up with a minimal example that reproduces the issue.