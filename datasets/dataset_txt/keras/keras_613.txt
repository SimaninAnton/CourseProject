marcoatgh1119 commented on 20 Nov 2018
Hi everyone. I would like to ask if there is anyone around who can help me with this problem. I would like to create a custom loss to implement the mutual information of y_true and y_pred, i.e.
I(y_true; y_pred) = H(y_true) - H(y_true|y_pred) = I(y_pred; y_true).
H is the Shannon entropy and H( | ) is the conditional Shannon entropy. y_true is a one hot tensor and y_pred is a tensor output by a softmax layer (4 classes classification problem).
Now this is pretty straightforward with python but, when it comes to using the functions for tensors provided by tensorflow, I always get stuck.
In particular if I compute the entropies I don't know how to represent the conditional probabilities (not even if I try to switch to the joint probability or better frequency since I am working with classes and we could count the occurrences).
I have also tried to switch to the Kullbackâ€“Leibler divergence formulation of the mutual information https://en.wikipedia.org/wiki/Mutual_information#Relation_to_Kullback%E2%80%93Leibler_divergence. But once again I got stuck. I wish there was a more straightforward way to understand how to use the tensorflow functions. Sometimes the man pages are not enough for me to fully understand things. I have been stuck on this for a few days and I really don't see any real way to overcome the obstacle. Any hint is very much appreciated.
Don't hesitate to ask if you need further details.