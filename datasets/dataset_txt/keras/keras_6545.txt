avisingh599 commented on 18 Oct 2015
Hello,
I am interested in implementing a system that is similar to the current image caption generation example of Keras, but with one difference: Instead of feeding the image feature vector at every time step of the RNN, I only want to feed it once (i.e. the first time step), and the rest of input vectors to the RNN will be word vectors. This was done in Vinyals at al1, and gives better results than feeding identical image features at every time step. Note that the dimension of the word vectors and the image vector will have to be the same in this case.
Is there an elegant way to do this in Keras such that the system is still end-to-end trainable? Could it be done via changing the concatenation axis in Merge?
Thank you.
[1]Show and Tell: A Neural Image Caption Generator
http://arxiv.org/abs/1411.4555
http://arxiv.org/pdf/1411.4555v2.pdf