wailoktam commented on 20 Jun 2016
Hi, I am implementing two merged CNN, one for Questions and one for Answers, based on the description of the following paper.
http://arxiv.org/abs/1508.01585
The merge is done with calculating cosine similarity between the output of the Q stacks and the A stacks.
Initially, I tried to use Word2Vec and build my own input layers, 30x200 for Q and 200x200 for A.
Following the instruction of the paper, which mention a first layer of size 200, I have to flatten the matrix to a dense of size 200. So when I applying cosine similarity during merge of the Q stack output (vector) and V stack output (vector), word order matters now, unlike the original application of cosine similarity to sentences as vectors of word frequencies, which care only about word frequencies.
Now I have given that up and use embedding layers of size 200 to accept sequences of indices of words in Q and A. The output of the embedding layer is a vector, rather than a matrix. Can I assume that it means it flattens the Q matrix and A matrix as before such that when applying cosine similarity to the output vectors of the embedding layers taking Q and A, word order also matters?
I know my word description can be confusion. What I mean is:
Assume each word is a vector of size 2 and a sentence has 3 words:
I love you is:
[1,4,3]
[2,5,8]
[3,6,9]
"you love me"(assume lemmatization which changes me to I) is
[3,6,9]
[2,5,8]
[1,4,3]
The traditional cosine similarity formula would yield 1 for these two sentences when each word is represented by an index. "I love you" is represented by the vector [1,1,1] where the first 1 corresponding to the freq of I, the 2nd 1 the freq of love, the 3rd I the freq of you. Likewise, "you love me" is represented by the vector [1,1,1] where the first 1 corresponding to the freq of I, the 2nd 1 the freq of love, the 3rd I the freq of you
Now applying cosine similarity to flattened matrices formed with multiple word vectors, it would mean calculating cosine similarity between the vector [1,4,3,2,5,8,3,6,9] and the vector [3,6,9,2,5,8,1,4,3], which is 0.82.
In short, my question is:
Is using the embedding layer means flattening the representation of a sentence by a matrix formed with word vectors to a vector?
Does applying cosine similarity to two such vectors now care about word order like I illustrate?
Many thanks in advance for reading this long and confusion questions.
Please make sure that the boxes below are checked before you submit your issue. Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).