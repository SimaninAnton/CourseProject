amiasato-zz commented on 30 Aug 2017 â€¢
edited
Code example of the issue here.
tldr: temporal weighting seems to break with fancy custom loss functions, even when I just want to apply weights to non-custom losses.
Now, for more context:
I have a multi-objective LSTM network of which one of the objectives is updated by a custom loss function. No issues here so far: the network works as expected and both objectives are learned jointly.
However, on some instances, I wish to weight examples from the regular hinge loss function, which is being used for simple binary prediction (unbalanced classes).
The attached code reproduces the working scenario. However, you just need to uncomment line 59 (and fix syntax), to provoke a tensor dimension mismatch error.
So, what I would like to do at first is to ignore the custom loss function when applying weights. But this doesn't seem possible, since all I need is to change the weighting type in the model compilation to break my code. I do hope for something like passing a 'None' in the weight dict from the data generator, which would make the compiler ignore dimension mismatches in the custom loss output.
In the best case, it would be really nice if sample weights worked in this custom loss function, and i guess it would involve dealing with the dimension permutation operations in the custom loss.
Thanks for your attention,