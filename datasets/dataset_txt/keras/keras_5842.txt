ghost commented on 2 Mar 2016
Hi developers,
I run over the stateful_lstm.py example and see some problems:
The data doesn't arrange as it suppose to be in stateful mode - the batch_size are 25 and "cos" doesn't arrange in like it say Keras FAQ - " If X1 and X2 are successive batches of samples, then X2[i] is the follow-up sequence to X1[i], for every i".
When i turn off stateful mode (stateful=False) i get the good results:
stateful=True >>> loss = ±0.28
stateful=False >>> loss = ±0.3
Beside that, I just built a model using stateful lstm layers, and have some issue, i saw that when i turn on the stateful mode, the train loss getting better but the test loss are getting very bad.
After debugging the data preprocessing step (and see that it has no bugs), i really think that it has some bug in the lstm stateful implementation. (I order the data as it write in Keras FAQ for stateful mode).
When i turn off the stateful mode the model archive better results in the test set.
Has anyone been able to train stateful network that achieve good result compare to unstateful network?
(I use the latest version of Keras)
Thanks!