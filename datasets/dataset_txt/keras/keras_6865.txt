ghost commented on 10 Jul 2015
So I am getting nans when fitting the model with binary cross-entropy as loss function, and it seems it is because the epsilon used to clip the predictions is too small.
Not sure what happens with float64, but when working with floatx=float32:
import theano.tensor as T
import theano

y = T.vector()
cl = theano.function([y], [T.clip(y, 1.0e-9, 1.0 - 1.0e-9)])

p = np.array([0,1]).astype(np.float32)
print cl(p)

>>> [array([  9.99999994e-09,   1.00000000e+00], dtype=float32)]
So the "1" doesn't get clipped and therefore the binary cross entropy function returns nan.
Solution could be:
cl = theano.function([yy], [T.clip(y, 1.0e-7, 1.0 - 1.0e-7)])
print cl(p)

>>> [array([  1.00000001e-07,   9.99999881e-01], dtype=float32)]
But I am not sure if this could cause a significant precision loss when working with float64... Indeed in my experiments with float64 I also got nans returned in this situation, but I didn't expect to have that result.