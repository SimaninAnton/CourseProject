zzjin13 commented on 17 Jan 2016
I want to implement the skip connection.

model1 = Sequential()
model1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, init='normal'))
model2 = Sequential()
model2.add(model1)
model2.add(LSTM(hidden_layer_size, go_backwards=is_reverse, return_sequences=True))

model3 = Sequential()
model3.add(Merge([model1,model2], mode='concat'))
model3.add(LSTM(hidden_layer_size, return_sequences=False))
model3.add(Dropout(dropout_rate))

model3.add(Dense(output_dim=entity_size, init="normal"))
model3.add(Activation("softmax"))

model3.compile(loss='categorical_crossentropy', optimizer=SGD(lr=init_learning_rate, momentum=0.9, decay=0.95, nesterov=True))
And an error occurs.(Note that embedding_dim=1000,hidden_layer_size=256) ValueError: dimension mismatch in args to gemm (128,1000)x(1256,256)->(128,256) How can I solve this problem?