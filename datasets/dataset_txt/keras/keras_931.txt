Tube-jr commented on 27 Jun 2018 â€¢
edited
Hello,
there is some strange behavior in keras, if I train my model in 3 * 5 epochs, I get a much higher validation accuracy than training directly 15 epochs. Below is some example output.
Training of 15 epochs
Epoch 1/15
9350/9350 [==============================] - 8s 836us/step - loss: 1.9323 - acc: 0.2848 - val_loss: 1.8086 - val_acc: 0.3964
Epoch 2/15
9350/9350 [==============================] - 7s 786us/step - loss: 1.1929 - acc: 0.5540 - val_loss: 1.3848 - val_acc: 0.6109
Epoch 3/15
9350/9350 [==============================] - 8s 839us/step - loss: 0.8032 - acc: 0.6734 - val_loss: 1.1329 - val_acc: 0.6721
Epoch 4/15
9350/9350 [==============================] - 7s 788us/step - loss: 0.5716 - acc: 0.7772 - val_loss: 0.9861 - val_acc: 0.7073
Epoch 5/15
9350/9350 [==============================] - 7s 767us/step - loss: 0.4152 - acc: 0.8457 - val_loss: 0.9026 - val_acc: 0.7145
Epoch 6/15
9350/9350 [==============================] - 7s 785us/step - loss: 0.3110 - acc: 0.8913 - val_loss: 0.8726 - val_acc: 0.7206
Epoch 7/15
9350/9350 [==============================] - 7s 787us/step - loss: 0.2319 - acc: 0.9231 - val_loss: 0.8683 - val_acc: 0.7212
Epoch 8/15
9350/9350 [==============================] - 7s 771us/step - loss: 0.1975 - acc: 0.9404 - val_loss: 0.8698 - val_acc: 0.7261
Epoch 9/15
9350/9350 [==============================] - 8s 830us/step - loss: 0.1516 - acc: 0.9538 - val_loss: 0.8482 - val_acc: 0.7242
Epoch 10/15
9350/9350 [==============================] - 8s 867us/step - loss: 0.1287 - acc: 0.9615 - val_loss: 0.9034 - val_acc: 0.7127
Epoch 11/15
9350/9350 [==============================] - 7s 762us/step - loss: 0.1182 - acc: 0.9659 - val_loss: 0.9304 - val_acc: 0.7255
Epoch 12/15
9350/9350 [==============================] - 7s 768us/step - loss: 0.1138 - acc: 0.9684 - val_loss: 0.9267 - val_acc: 0.7152
Epoch 13/15
9350/9350 [==============================] - 7s 768us/step - loss: 0.1070 - acc: 0.9699 - val_loss: 0.9332 - val_acc: 0.7121
Epoch 14/15
9350/9350 [==============================] - 7s 761us/step - loss: 0.1145 - acc: 0.9704 - val_loss: 0.9027 - val_acc: 0.7206
Epoch 15/15
9350/9350 [==============================] - 7s 785us/step - loss: 0.0946 - acc: 0.9743 - val_loss: 0.9050 - val_acc: 0.7376
Training in 3 * 5 epochs
Training on new shuffled set 1 of 3
Train on 9350 samples, validate on 1650 samples
Epoch 1/5
9350/9350 [==============================] - 8s 818us/step - loss: 1.9296 - acc: 0.2874 - val_loss: 1.7998 - val_acc: 0.4485
Epoch 2/5
9350/9350 [==============================] - 7s 786us/step - loss: 1.0975 - acc: 0.5866 - val_loss: 1.2432 - val_acc: 0.6145
Epoch 3/5
9350/9350 [==============================] - 7s 800us/step - loss: 0.6959 - acc: 0.7317 - val_loss: 1.0633 - val_acc: 0.6927
Epoch 4/5
9350/9350 [==============================] - 7s 780us/step - loss: 0.4518 - acc: 0.8364 - val_loss: 0.9601 - val_acc: 0.6897
Epoch 5/5
9350/9350 [==============================] - 8s 803us/step - loss: 0.3191 - acc: 0.8954 - val_loss: 0.8622 - val_acc: 0.7103

Training on new shuffled set 2 of 3
Train on 9350 samples, validate on 1650 samples
Epoch 1/5
9350/9350 [==============================] - 7s 777us/step - loss: 0.3561 - acc: 0.8957 - val_loss: 0.6045 - val_acc: 0.8721
Epoch 2/5
9350/9350 [==============================] - 7s 789us/step - loss: 0.2411 - acc: 0.9273 - val_loss: 0.4767 - val_acc: 0.9018
Epoch 3/5
9350/9350 [==============================] - 7s 792us/step - loss: 0.1757 - acc: 0.9486 - val_loss: 0.4585 - val_acc: 0.8818
Epoch 4/5
9350/9350 [==============================] - 7s 773us/step - loss: 0.1419 - acc: 0.9576 - val_loss: 0.4396 - val_acc: 0.8800
Epoch 5/5
9350/9350 [==============================] - 7s 782us/step - loss: 0.1119 - acc: 0.9693 - val_loss: 0.4333 - val_acc: 0.8739

Training on new shuffled set 3 of 3
Train on 9350 samples, validate on 1650 samples
Epoch 1/5
9350/9350 [==============================] - 7s 779us/step - loss: 0.1802 - acc: 0.9514 - val_loss: 0.2755 - val_acc: 0.9576
Epoch 2/5
9350/9350 [==============================] - 7s 778us/step - loss: 0.1328 - acc: 0.9619 - val_loss: 0.2275 - val_acc: 0.9545
Epoch 3/5
9350/9350 [==============================] - 7s 774us/step - loss: 0.1044 - acc: 0.9697 - val_loss: 0.2060 - val_acc: 0.9594
Epoch 4/5
9350/9350 [==============================] - 7s 781us/step - loss: 0.0936 - acc: 0.9741 - val_loss: 0.2067 - val_acc: 0.9539
Epoch 5/5
9350/9350 [==============================] - 7s 773us/step - loss: 0.0845 - acc: 0.9796 - val_loss: 0.1877 - val_acc: 0.9564
My model.fit for the first one looks like this:
model.fit(x_train, y_train, validation_split=0.15, epochs=15, batch_size=128, shuffle=True)
For the second output it looks mostly the same (epochs=5), with the small difference, that there is a "for loop" around it, in which x_train and y_train get setted new in every loop (but with the same data).
As you see, the val_acc in the first output gets stuck around 71% and in the second it reaches around 95%. Do you have some explanation for this behavior?
Keras: 2.2.0
Tensorflow (CPU only): 1.8.0