jmlipman commented on 12 Aug 2017
I modified the ReLU activation function to print the value of the learning phase, expecting that during the training (fit) and testing (evaluate, predict) it will be different. In fact, according to the documentation, the value during the training should be 1 whereas during the testing 0 is expected.
def relu(x, alpha=0., max_value=None): print(K.learning_phase()) return K.relu(x, alpha=alpha, max_value=max_value)
During the training I get:
Tensor("activation_1/keras_learning_phase:0", dtype=bool)
and during testing I don't get anything at all.
How does it make sense?
JM.