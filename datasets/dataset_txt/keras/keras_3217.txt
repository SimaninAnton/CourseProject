lxafly commented on 14 Feb 2017
Hello!
I have some confusion about the weight updates for embedding layer, and wondering if someone could shed some light. (It could be a mechanism that I don't understand or could be something missing in my setup)
My original code is lengthy, but here is a summary of the major setup.
I have a vocabulary of V, where the first token gets mapped to index 1.
Therefore the input_dim to the Embedding layer is (V + 1). Because the 0-th index is for sequence padding. For example three sentences of various length, and say max length is 5.
[40,20] [19,17,15] [5] Would be padded as [40,20,0,0,0] [19,17,15,0,0] [5,0,0,0,0]
Let's say with the 0-th index the embedding matrix is called my_embed. and has (V+1) rows. With the 0-th row being all zeros, the rest initialized from glove word vectors. The Embedding layer is defined as below.
embed_layer = Embedding(input_dim=V+1, output_dim=200, weights=[my_embed],mask_zero=True)
In brief, an input sequence goes through the Embedding layer, and then a MeanPooling(or MaxPooling) layer, and then normal dense layer etc. (Parts after that are not important for the discussion here)
And say after the training is done, and loaded the saved model. I got a embedding matrix called my_adapted_embed which should be different from my_embed, due to back propagation. What I expected is that my_adapted_embed[0] should stay intact to be all zeros. Because they corresponding to the padding 0-th index, where masking should have kept it from updating. But what I found is my_adapted_embed[0] actually changed.
Could someone shed light here, what is it I don't understand about masking?
Thanks a lot for your help!
1