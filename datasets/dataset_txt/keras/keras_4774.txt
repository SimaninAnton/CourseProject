saicoco commented on 15 Jul 2016
I just want to rewrite the code as a keras layer, the code come from this paperA Deep Semi-NMF Model for Learning Hidden Representations
from __future__ import print_function
from collections import OrderedDict

import numpy as np
import theano
import theano.tensor as T

from scipy.sparse.linalg import svds

relu = lambda x: 0.5 * (x + abs(x))

def floatX(x):
    return np.asarray(x, dtype=theano.config.floatX)

def appr_seminmf(M, r):
    """
        Approximate Semi-NMF factorisation. 

        Parameters
        ----------
        M: array-like, shape=(n_features, n_samples)
        r: number of components to keep during factorisation
    """

    if r < 2:
        raise ValueError("The number of components (r) has to be >=2.")

    A, S, B = svds(M, r-1)
    S = np.diag(S)
    A = np.dot(A, S)

    m, n = M.shape

    for i in range(r-1):
        if B[i, :].min() < (-B[i, :]).min():
            B[i, :] = -B[i, :]
            A[:, i] = -A[:, i]


    if r == 2:
        U = np.concatenate([A, -A], axis=1)
    else:
        An = -np.sum(A, 1).reshape(A.shape[0], 1)
        U = np.concatenate([A, An], 1)

    V = np.concatenate([B, np.zeros((1, n))], 0)

    if r>=3:
        V -= np.minimum(0, B.min(0))
    else:
        V -= np.minimum(0, B)

    return U, V

def adam(loss, params, learning_rate=0.001, beta1=0.9,
         beta2=0.999, epsilon=1e-8):
    """Adam updates

    Adam updates implemented as in [1]_.

    Parameters
    ----------
    loss_or_grads : symbolic expression or list of expressions
        A scalar loss expression, or a list of gradient expressions
    params : list of shared variables
        The variables to generate update expressions for
    learning_rate : float
        Learning rate
    beta_1 : float
        Exponential decay rate for the first moment estimates.
    beta_2 : float
        Exponential decay rate for the second moment estimates.
    epsilon : float
        Constant for numerical stability.

    Returns
    -------
    OrderedDict
        A dictionary mapping each parameter to its update expression

    Notes
    -----
    The paper [1]_ includes an additional hyperparameter lambda. This is only
    needed to prove convergence of the algorithm and has no practical use
    (personal communication with the authors), it is therefore omitted here.

    References
    ----------
    .. [1] Kingma, Diederik, and Jimmy Ba (2014):
           Adam: A Method for Stochastic Optimization.
           arXiv preprint arXiv:1412.6980.
    """

    all_grads = theano.grad(loss, params)
    t_prev = theano.shared(floatX(0.))
    updates = OrderedDict()

    for param, g_t in zip(params, all_grads):
        m_prev = theano.shared(param.get_value() * 0.)
        v_prev = theano.shared(param.get_value() * 0.)
        t = t_prev + 1
        m_t = beta1*m_prev + (1-beta1)*g_t
        v_t = beta2*v_prev + (1-beta2)*g_t**2
        a_t = learning_rate*T.sqrt(1-beta2**t)/(1-beta1**t)
        step = a_t*m_t/(T.sqrt(v_t) + epsilon)

        updates[m_prev] = m_t
        updates[v_prev] = v_t
        updates[param] = param - step

    updates[t_prev] = t
    return updates


def init_weights(X, num_components, svd_init=True):
    if svd_init:
        return appr_seminmf(X, num_components)

    Z = 0.08 * np.random.rand(X.shape[0], num_components)
    H = 0.08 * np.random.rand(num_components, X.shape[1])

    return Z, H


from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams
rng = RandomStreams()

def dropout(x, p=0):
    if p == 0:
        return x
    else:
        p = 1 - p
        x /= p

        return x * rng.binomial(x.shape, p=p, dtype=theano.config.floatX)


class DSNMF(object):

    def __init__(self, data, layers, verbose=False, l1_norms=[], pretrain=True, learning_rate=1e-3):
        """
        Parameters
        ----------
        :param data: array-like, shape=(n_samples, n_features)
        :param layers: list, shape=(n_layers) containing the size of each of the layers
        :param verbose: boolean
        :param l1_norms: list, shape=(n_layers) the l1-weighting of each of the layers
        :param pretrain: pretrain layers using svd
        """
        H = data.T

        assert len(layers) > 0, "You have to provide a positive number of layers."

        params = []

        for i, l in enumerate(layers, start=1):
            print('Pretraining {}th layer [{}]'.format(i, l), end='\r')

            Z, H = init_weights(H, l, svd_init=pretrain)

            params.append(theano.shared(floatX(Z), name='Z_%d' % (i)))

        params.append(theano.shared(floatX(H), name='H_%d' % len(layers)))

        self.params = params
        self.layers = layers

        cost = ((data.T - self.get_h(-1))**2).sum()

        for norm, param in zip(l1_norms, params):
            cost += ((abs(param)) * norm).sum()

        H = relu(self.params[-1])

        updates = adam(cost, params, learning_rate=learning_rate)

        self.cost = cost
        self.train_fun = theano.function([], cost, updates=updates)
        self.get_features = theano.function([], H)

        self.get_reconstruction = theano.function([], self.get_h(-1))

    def finetune_features(self):

        updates = adam(self.cost, self.params[-1:])
        self.train_fun = theano.function([], self.cost, updates=updates)

    def get_param_values(self):
        return [p.get_value() for p in self.params]

    def set_param_values(self, values):
        params = self.params

        if len(params) != len(values):
            raise ValueError("mismatch: got %d values to set %d parameters" %
                            (len(values), len(params)))

        for p, v in zip(params, values):
            if p.get_value().shape[0] != v.shape[0]:
                raise ValueError("mismatch: parameter has shape %r but value to "
                             "set has shape %r" %
                             (p.get_value().shape, v.shape))
            else:
                p.set_value(v)

    def get_h(self, layer_num, have_dropout=False):
        h = relu(self.params[-1])

        if have_dropout:
            h = dropout(h, p=.1)

        for z in reversed(self.params[1:-1][:]):
            h = relu(z.dot(h))

        if layer_num == -1:
            h = self.params[0].dot(h)

        return h
and following code are my code:
# -*- coding: utf-8 -*-
# author = sai
from keras import backend as K
from keras.engine.topology import Layer
import numpy as np

def init_weights(X, num_components, i, svd_init=True):
    # if svd_init:
    #     return appr_seminmf(X, num_components)

    Z = K.variable(0.08 * np.random.rand(X.shape[0], num_components), name='z_%d'%(i))
    H = K.variable(0.08 * np.random.rand(num_components, X.shape[1]), name='h_%d'%(i))
    return Z, H

class NMFLayer(Layer):
    def __init__(self, layers, input_dim=None, input_length=None, **kwargs):
        self.output_dim = layers[-1]
        self.layers = layers
        self.input_dim = input_dim
        self.input_length = input_length
        if self.input_dim:
            kwargs['input_shape'] = (self.input_length, self.input_dim)
        super(NMFLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.trainable = []
        H = K.variable(value=np.random.rand(input_shape[::-1]))
        for i, l in enumerate(self.layers):
            Z, H = init_weights(H, l, i)
            self.trainable.append(Z)
        self.trainable.append(H)

    def call(self, x, mask=None):
        h = K.relu(self.trainable[-1])
        for z in reversed(self.trainable[1:-1][:]):
            h = K.relu(K.dot(z, h))
        h = K.dot(self.trainable[0], h)
        return h

    def get_output_shape_for(self, input_shape):
        return (input_shape[1], input_shape[0])

def cost(y_true, y_pred):
    return K.sum((y_true.T - y_pred)**2)

def DSNMF(shape):
    inputs = InputLayer(input_shape=shape)
    nmf_layer = NMFLayer(layers=[400, 100])(inputs)
    model = Model(input=inputs, output=nmf_layer)
    model.compile(optimizer=Adam(lr=1e-5), loss=cost, metrics=[cost])
    return model

if __name__=='__main__':
    shape = data.T.shape
    dsnmf = DSNMF(shape)
When DSNMF is created, there will be a error:
Traceback (most recent call last):
  File "/home/sai/code/papers_code/Deep-Semi-NMF/deep_smi_nmf.py", line 37, in <module>
    dsnmf = DSNMF(shape)
  File "/home/sai/code/papers_code/Deep-Semi-NMF/deep_smi_nmf.py", line 31, in DSNMF
    nmf_layer = NMF_layer.NMFLayer(layers=[400, 100])(inputs)
  File "/home/sai/anaconda2/lib/python2.7/site-packages/Keras-1.0.5-py2.7.egg/keras/engine/topology.py", line 452, in __call__
    '". This layer has no information'
Exception: You tried to call layer "nmflayer_1". This layer has no information about its expected input shape, and thus cannot be built. You can build it manually via: `layer.build(batch_input_shape)`
I do not know how to process intput_shape