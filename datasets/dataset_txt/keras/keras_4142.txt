shwetgarg commented on 17 Oct 2016 â€¢
edited
I am trying to define custom loss function from multiple outputs.
In the simplest case, i took output from forward and backward biLSTM layer using following code:
  l1 = recurrent.LSTM(output_dim=self.hidden_size, init='uniform', inner_init='uniform',
                            forget_bias_init='one', return_sequences=False, 
                            activation='tanh', inner_activation='sigmoid')(x)
  r1 = recurrent.LSTM(output_dim=self.hidden_size, init='uniform', inner_init='uniform',
                            forget_bias_init='one', return_sequences=False, 
                            activation='tanh', inner_activation='sigmoid',
                            go_backwards=True)(x)

  print l1.ndim           # prints correctly "2"
  print r1.ndim          # prints correctly "2"

  model = Model(input=sentence, output=[l1, r1])
Then compile it with following code:
   model.compile(optimizer='adam', loss=cosine)

   def cosine(y_true, y_pred):
        print type(y_pred)             # print <class 'theano.tensor.var.TensorVariable'> inspite of "list"
        q_array = y_pred[0]           # prints "1" inspite of "2"
        a_array = y_pred[1]            # prints "1" inspite of "2"
The custom loss function is getting some TensorVariable as input inspite of list, because of which i am unable to calculate cosine similarity. Please help me with this problem.
PS: This is not the actual problem which I am trying to solve, it is just a simpler version of it.
4