jonilaserson commented on 7 Jun 2017
I have a generator that covers a dataset of unkown size (although I can upper-bound it).
I'd like to predict over the entire dataset. predict_generator requires declaring the number of steps in advance. If your generator was depleted in the middle the prediction is aborted and you lose all the predictions and also the data the generator already covered.
On the other hand if you get the batches one by one and just use "predict" your GPU will stand idle while the next batch is being pre-processed.
I guess you have to create your own multi-process queue, or is there a work-around?