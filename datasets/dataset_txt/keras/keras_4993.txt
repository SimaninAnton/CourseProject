rishy commented on 10 Jun 2016 â€¢
edited
I am getting a "MemoryError" while using a vocab size of 1 million in "Embedding" layer.
I am using the latest development versions of both Theano and Keras. Here's the code snippet where I am getting the error:
    self.model = Sequential()

    self.model.add(Embedding(input_dim = self.vocab_size, output_dim = self.word_dim,
                             weights = [self.word_vectors.T]))

    self.model.add(LSTM(output_dim=self.hidden_dim,
                        activation='tanh', inner_activation='tanh',
                        dropout_W = 0.2, dropout_U = 0.2, return_sequences = False))

    self.model.add(Dense(self.output_dim, activation="softmax"))

    adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
    self.model.compile(loss='categorical_crossentropy',
                  optimizer= adam,
                  metrics=['accuracy'])`
Error:
MemoryError                               Traceback (most recent call last)
<ipython-input-13-83465b841fe7> in <module>()
----> 1 model.fit(train_X[:1], train_y[:1], batch_size = 1, nb_epoch=1, verbose = 1)

/home/monster/anaconda2/lib/python2.7/site-packages/keras/models.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)
    406                               shuffle=shuffle,
    407                               class_weight=class_weight,
--> 408                               sample_weight=sample_weight)
    409 
    410     def evaluate(self, x, y, batch_size=32, verbose=1,

/home/monster/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc in fit(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)
   1040         else:
   1041             ins = x + y + sample_weights
-> 1042         self._make_train_function()
   1043         f = self.train_function
   1044 

/home/monster/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc in _make_train_function(self)
    662                 trainable_weights += collect_trainable_weights(layer)
    663 
--> 664             training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)
    665             updates = self.updates + training_updates
    666 

/home/monster/anaconda2/lib/python2.7/site-packages/keras/optimizers.pyc in get_updates(self, params, constraints, loss)
    326 
    327         ms = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]
--> 328         vs = [K.variable(np.zeros(K.get_value(p).shape)) for p in params]
    329         self.weights = ms + vs
    330 

/home/monster/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc in variable(value, dtype, name)
     29     '''
     30     value = np.asarray(value, dtype=dtype)
---> 31     return theano.shared(value=value, name=name, strict=False)
     32 
     33 

/home/monster/anaconda2/lib/python2.7/site-packages/theano/compile/sharedvalue.pyc in shared(value, name, strict, allow_downcast, **kwargs)
    245             try:
    246                 var = ctor(value, name=name, strict=strict,
--> 247                            allow_downcast=allow_downcast, **kwargs)
    248                 utils.add_tag_trace(var)
    249                 return var

/home/monster/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/var.pyc in float32_shared_constructor(value, name, strict, allow_downcast, borrow, broadcastable, target)
    240         # type.broadcastable is guaranteed to be a tuple, which this next
    241         # function requires
--> 242         deviceval = type_support_filter(value, type.broadcastable, False, None)
    243 
    244     try:

MemoryError: ('Error allocating 673444864 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).', "you might consider using 'theano.shared(..., borrow=True)'")`
I have developed a similar network with same number of parameters in "Theano", and it works just fine. I am not sure why Keras is throwing the error here.
Any help is much appreciated.