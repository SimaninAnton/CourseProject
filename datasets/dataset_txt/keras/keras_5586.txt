3rduncle commented on 5 Apr 2016
I am working with a Chinese word segment task and training a network to predict the the sequential labels({B,M,E,S}) from a sentence.
I build a LSTM network to resolve this task below.
print('Build Model ... ')
_, xmax_timestamp, x_feature = x_train.shape
_, ymax_timestamp, y_feature = y_train.shape
model = Sequential()
model.add(Masking(input_shape=(xmax_timestamp, x_feature)))
model.add(UNIT(HIDDEN_SIZE, activation='relu'))
model.add(Dropout(0.2))
model.add(RepeatVector(ymax_timestamp))
model.add(TimeDistributedDense(y_feature))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode="temporal")
model.fit(...sample_weight=m_train)
Embedding layer is not needed, because chars are decoded to 64d vector by Font Image Mapping before input.
the labels {B,M,E,S} are presented as 4d one-hot vectors, for example 'B' -> [1,0,0,0]
zeros vectors are padded at the end of input, because the input sentences have difference length.
the padding vectors are masked by Masking layer and the configure of the loss function when fitting.
some sentences are very long. The longest sentence has 1019 time steps. Can it make a LSTM node overflow float32 after 1019-times operator?
When I fit it to x and y I get a loss of nan for all epochs. It doesn't seem to learn anything.
Epoch 1/1
400/900 [============>.................] - ETA: 51s - loss: nan - acc: 0.3015
As I know, the output of 'relu' activation is in a half-open range [0, +INF), and some +INF value can make loss function 'categorical_crossentropy' get an nan value. What is I don't understand is why this risk not be avoided by the sequentially 'softmax' layer after which the output will be transformed in a closed range (0,1)?
What am I doing wrong?
It is a one to one mapping task. If the reason of this issue is such long sentences, make it bad affect that the long sentences are split into some shorter sentences, for example a 1000-length sentence with 1000-length labels to 100 sentences with 10-length labels. extremely, if the hidden state not reset when passing a new sample, what will happen after that all sentences are split into one-length sentences(char)?
1