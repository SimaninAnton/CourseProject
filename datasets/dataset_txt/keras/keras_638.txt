edumucelli commented on 8 Nov 2018 â€¢
edited
I am using Keras 2.2.4 (it also happens on 2.2.0). Training is not starting at all. Even after waiting for 12 hours. I am using Xception (but even a simple model as shown below results the same thing). The following fit generates the following log
model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size,
    # class_weight=class_weights,
    callbacks=[
        reduce_lr,
        checkpoint #,
        # tensorboard
    ],
    verbose=1)
For the following configuration parameters:
nb_train_samples = 142785
nb_validation_samples = 10000
epochs = 10
patience = 20
batch_size = 16
number_of_classes = 2
With the following simple model or Xception or anything else problem is the same:
def simple_model():
    model = Sequential(name='simple_model')
    model.add(Conv2D(32, (3, 3), input_shape=input_shape))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(32, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(64, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(128, (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())
    model.add(Dense(64))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(number_of_classes, activation='softmax'))

    return model
Using TensorFlow backend.
2018-11-07 23:17:33.919972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-07 23:17:33.920367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.721
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.83GiB
2018-11-07 23:17:33.920382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2018-11-07 23:17:34.135313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-07 23:17:34.135508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0
2018-11-07 23:17:34.135574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N
2018-11-07 23:17:34.135772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7568 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
Found 142785 images belonging to 2 classes.
Found 10000 images belonging to 2 classes.
Epoch 1/10
The GPU is not being used at all, as it shows 0% usage. I have tried to force the CPU usage to see if that was a problem related to GPU communication or whatnot, but the it also hangs with the "Epoch 1/10" when using CPU forever.
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.87                 Driver Version: 390.87                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   53C    P8    12W / 151W |   7721MiB /  8117MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     26075      C   python                                      7711MiB |
+-----------------------------------------------------------------------------+
I am wondering what are the possible causes of such problem? I am having a hard time to debug it properly as not log is out and no apparent problems seems to exist. Any suggestion is welcome. Thanks in advance.