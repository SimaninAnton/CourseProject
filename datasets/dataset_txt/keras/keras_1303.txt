monktastic commented on 23 Jan 2018
If I create a model and set trainable=False, then use that model as a layer in another model, that second model reports that there are no trainable params. Yet it trains them. This is true whether or not the first model is compiled.
import numpy as np
from keras.layers import Dense
from keras.models import Model, Sequential
from keras.optimizers import Adam

D = Sequential(name="D")
D.add(Dense(1, input_shape=(1,)))
D.trainable = False
# D.compile() here makes no difference.
D.compile(loss='mse', optimizer=Adam(lr=1e-2))

A = Sequential(name="D-untrainable")
A.add(D)
A.compile(loss='mse', optimizer=Adam(lr=1e-2))
A.summary()  # Shows no trainable params
assert not A.trainable_weights


x = np.array([[1]])
y = np.array([[5]])

weights_d_before = D.get_weights()[0]
weights_a_before = A.get_weights()[0]
predict_a_before = A.predict(x)
A.fit(x, y, epochs=1000, verbose=False)
weights_d_after = D.get_weights()[0]
weights_a_after = A.get_weights()[0]
predict_a_after = A.predict(x)

# All three asserts fail.
assert np.array_equal(predict_a_before, predict_a_after)
assert np.array_equal(weights_a_before, weights_a_after)
assert np.array_equal(weights_d_before, weights_d_after)
Am I misunderstanding something?