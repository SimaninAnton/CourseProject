FrenkT commented on 9 May 2016 â€¢
edited
Hi, I'm trying to train this Siamese architecture (referenced here #242 (comment) )
leg = Sequential()
leg.add(Dense(128, input_dim=400, init='uniform', activation='relu'))

model = Sequential()
model.add(Merge([leg, leg], mode='cos'))

rms = RMSprop()
model.compile(loss='binary_crossentropy', optimizer=rms)

train_inps, train_true_idx, train_false_idx, train_labels, L = utils.load_train_data()
norm_train_inps = utils.mean_normalization(train_inps)

for e in range(n_epoch):
    batch_epoch = int((len(train_true_idx[0]) + len(train_false_idx[1])) / (batch_size*2))
    for i in range(batch_epoch):
        X_batch, Y_batch = utils.sample_batch_siamese(batch_size, norm_train_inps, train_true_idx, train_false_idx, L)
        model.train_on_batch(X_batch, Y_batch)
The error i get is
ValueError: ('You cannot drop a non-broadcastable dimension.', ((False, False, False), (0, 2)))
Can this be caused by a not correct shape of the training data? In my case X_batch is (N, 800) where N is the number of samples and 800 should be taken half by one "leg" and half by the other one. Y_batch is a vector of ones and zeros of shape (N).
Am i getting this right? Thanks.