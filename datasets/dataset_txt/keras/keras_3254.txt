Contributor
jpeg729 commented on 9 Feb 2017
I would like to experiment with meta-RL, but the central idea involves adding the previous prediction and its quality to the input vector at each timestep.
This means I'll be writing my own train loop and in the middle I will have something like this.
loss = model.train_on_batch(X[t],Y[t])
predicted = model.predict_on_batch(X[t])
X[t+1]["previous"] = predicted
X[t+1]["loss"] = loss
But there are two problems with that.
It runs the prediction step twice, once internally for the training step, and once to give me the prediction
It gives me a scalar loss, not a vector representing the individual loss for each prediction.
So basically, I would like to have a single function that does everything in one go.
vector_loss, prediction = model.train_and_predict_on_batch(X,Y)