Contributor
braingineer commented on 9 Apr 2016
Hi all,
TimeDistributedDense is becoming deprecated, but TimeDistributed is throwing the "does not support masking" exception.
Is it as simple as putting the "supports_masking" flag to True inside TimeDistributed's constructor and then passing the mask into the K.rnn?
i.e.:
# inside def __init__
self.supports_masking = True
and
# inside def call
last_output, outputs, states = K.rnn(step, X,
                                                 initial_states=[],
                                                 mask=mask)
I've been over the code and it seems like this is it, but I don't quite get how the mask flows through the code.
For my stuff, I'm just using a mask on an RNN that has a classification layer at every time step. Also, the mask should be used on the loss prior to taking the mean.