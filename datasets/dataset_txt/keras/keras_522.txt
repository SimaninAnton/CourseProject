sidgairo18 commented on 18 Dec 2018
With reference to https://stackoverflow.com/questions/53400472/keras-model-weights-for-some-layers-become-all-nans , what I have noticed that the "NaNs" are present only in the "batch_norm" layers. Why must this be happening?
Is there some problem with the training - like Vanishing Gradient or Gradients exploding? or are the NaNs only placeholders for biases?
(Just a prior - I am trying to train a Siamese Network with Triplet Loss as mentioned here https://stackoverflow.com/questions/53400472/keras-model-weights-for-some-layers-become-all-nans (architecture and loss) )
Sample Weights listed here-
https://stackoverflow.com/questions/53830547/batch-norm-layer-weights-parameters-turning-nans-during-training-of-a-siamese-ne
Link to the Gist for the code I am using for training - https://gist.github.com/sidgairo18/dca347edd4588484237a231d7dab9a63
Please make sure that the boxes below are checked before you submit your issue.
If your issue is an implementation question, please ask your question on StackOverflow or on the Keras Slack channel instead of opening a GitHub issue.
Thank you!
[Yes] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps
[Yes] Check that your version of TensorFlow is up-to-date. The installation instructions can be found here.
[Yes] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).