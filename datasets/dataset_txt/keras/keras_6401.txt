vict0rsch commented on 23 Nov 2015
Hello, I am training a single LSTM layer with these parameters :
number of examples : 27,000
size of sample : (2500, 1 , dtype=float64)
size of target : (250, 1 , dtype=float64)
batch size : 64
activation : linear
I use a GeForce GTX 970 GPU and the training is really slow : a single epoch takes 678 seconds, which seems really slow. Any idea of what could be happening/how I can speed things up?
thanks!