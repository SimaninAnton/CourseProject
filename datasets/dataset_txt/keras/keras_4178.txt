ishank26 commented on 13 Oct 2016 â€¢
edited
Hi,
I am using my pretrained embedding layer on top of a stateful LSTM. My word2vec embedding is trained on a larger corpus(word2vec corpus) than the lstm training corpus(model corpus). i'm mapping word vectors to embedding weights using the word2vec model. Truncated model corpus to be divided into uniform sequences
For preparing the labels of train data i'm using the LSTM model vocab(not word2vec vocab) with one-hot encoding and no 0 masking.
While training the model, I'm getting the following error-
IndexError: One of the index value is out of bound. Error code: 65535.\n
Apply node that caused the error: GpuAdvancedSubtensor1(embedding_1_W, Elemwise{Cast{int64}}.0)
Toposort index: 142
Inputs types: [CudaNdarrayType(float32, matrix), TensorType(int64, vector)]
Inputs shapes: [(12288, 280), (1024,)]
Inputs strides: [(280, 1), (8,)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[GpuReshape{2}(GpuAdvancedSubtensor1.0, TensorConstant{[ -1 280]})]]
Model-
batch= 32
seq_length= 32
w2v_dim= 280
Corpus size: 12288 and vocab size: 7625 #truncated model corpus for equal sequence length 
X_train.shape= (384, 32) #words mapped to indices using word2vec dict  
y_train.shape= (384, 32, 7625)# words mapped to one hot using model corpus dict
embedding.shape= (12288, 280)# embedding layer weights using word2vec model and word2vec corpus.
memory_units=512

model= Sequential()
model.add(Embedding(corpus_size, w2v_dim,batch_input_shape=(batch,seq_length), mask_zero=False, weights=[embedding], input_length=seq_length))
model.add(LSTM(memory_units, return_sequences=True, stateful= True, init= "orthogonal"))
model.add(Dropout(0.5))
model.add(LSTM(memory_units, return_sequences=True, stateful= True, init= "orthogonal"))
model.add(Dropout(0.5))
model.add(TimeDistributed(Dense(vocab_size, activation='softmax', init= "orthogonal")))
Am I missing something? Thanks.