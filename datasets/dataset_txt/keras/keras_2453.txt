fmfn commented on 7 May 2017 â€¢
edited
Edit: The problem seems to be with tensorboard callback with histogram_freq=1, I will post a script to reproduce soon.
I'm encountering a weird error that I haven't been able to pinpoint yet. When fitting a model with fit_generator, at validation time it tries to allocate the entire validation set at once, leading to a memory error.
Here is the model specification:
Training data shape: (38478, 128, 128, 3) (38478, 17)
Validation data shape: (2000, 128, 128, 3) (2000, 17)
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
image (InputLayer)           (None, 128, 128, 3)       0
_________________________________________________________________
conv01 (Conv2D)              (None, 128, 128, 64)      1792
_________________________________________________________________
conv02 (Conv2D)              (None, 128, 128, 64)      36928
_________________________________________________________________
maxpool0 (MaxPooling2D)      (None, 32, 32, 64)        0
_________________________________________________________________
dropout0 (Dropout)           (None, 32, 32, 64)        0
_________________________________________________________________
conv11 (Conv2D)              (None, 32, 32, 128)       73856
_________________________________________________________________
conv12 (Conv2D)              (None, 32, 32, 128)       147584
_________________________________________________________________
maxpool1 (MaxPooling2D)      (None, 8, 8, 128)         0
_________________________________________________________________
dropout1 (Dropout)           (None, 8, 8, 128)         0
_________________________________________________________________
conv21 (Conv2D)              (None, 8, 8, 256)         295168
_________________________________________________________________
conv22 (Conv2D)              (None, 8, 8, 256)         590080
_________________________________________________________________
maxpool2 (MaxPooling2D)      (None, 2, 2, 256)         0
_________________________________________________________________
dropout2 (Dropout)           (None, 2, 2, 256)         0
_________________________________________________________________
flat-representation (Flatten (None, 1024)              0
_________________________________________________________________
sigmoid (Dense)              (None, 17)                17425
=================================================================
Total params: 1,162,833
Trainable params: 1,162,833
Non-trainable params: 0
_________________________________________________________________
The model is generated by:
def conv_conv_pool(img_size, classes, filters, pools=2, dropout=False):
    """Define the double convolution model."""
    with tf.name_scope("Input-image"):
        img_input = Input(shape=(img_size, img_size, 3), name="image")

    x = img_input
    for i, filter in enumerate(list(filters)):
        with tf.name_scope("Double-Conv-{}".format(i)):
            x = Conv2D(
                filters=filter,
                kernel_size=(3, 3),
                activation='relu',
                padding="same",
                name='conv{}1'.format(i)
            )(x)
            x = Conv2D(
                filters=filter,
                kernel_size=(3, 3),
                activation='relu',
                padding="same",
                name='conv{}2'.format(i)
            )(x)
            x = MaxPooling2D((pools, pools), name="maxpool{}".format(i))(x)
            
            if dropout:
                x = Dropout(0.5, name="dropout{}".format(i))(x)

    with tf.name_scope("Flat-Output"):
        x = Flatten(name="flat-representation")(x)
        x = Dense(classes, activation="sigmoid", name="sigmoid")(x)

    return Model(
        inputs=img_input,
        outputs=x,
        name="dconv-{}".format(len(filters))
    )
And training done by:
with tf.name_scope("Train"):
    h = model.fit_generator(
        gen,
        steps_per_epoch=max(len(xtr) // 64, 1),
        epochs=500,
        validation_data=(xva, yva),
        verbose=2,
        callbacks=cbs,
    )
Leads to:
2017-05-07 11:19:24.751289: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.81GiB. Current allocation summary follows.
and finally:
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2000,128,128,64] [[Node: Double-Conv-0/conv01/convolution = Conv2D[T=DT_FLOAT, data_format="NHWC", padding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/gpu:0"](_recv_Input-image/image_0/_159, Double-Conv-0/conv01/kernel/read)]] [[Node: Double-Conv-1/conv11/Relu/_169 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_105_Double-Conv-1/conv11/Relu", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]()]]
The setup is Ubuntu 16.04, gtx1080 (x2), and the latest tf and keras. Unfortunately I haven't been able to reproduce this is a simple script as the behaviour is quite unpredictable. I've had the same architecture running just fine, only to see it failing when I removed dropout layers. Any help is appreciated, thanks.
ps: If I try to fix the batch size with batch_input I get:
Traceback (most recent call last): File "src/run.py", line 258, in <module> main(**vars(args)) File "src/run.py", line 236, in main train(train_path, model_config, kwargs["name"], run_name) File "src/run.py", line 154, in train callbacks=cbs, File "/home/fernando/venvs/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py", line 88, in wrapper return func(*args, **kwargs) File "/home/fernando/venvs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py", line 1927, in fit_generator callbacks.on_epoch_end(epoch, epoch_logs) File "/home/fernando/venvs/tensorflow/lib/python3.5/site-packages/keras/callbacks.py", line 77, in on_epoch_end callback.on_epoch_end(epoch, logs) File "/home/fernando/venvs/tensorflow/lib/python3.5/site-packages/keras/callbacks.py", line 705, in on_epoch_end result = self.sess.run([self.merged], feed_dict=feed_dict) File "/home/fernando/venvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 778, in run run_metadata_ptr) File "/home/fernando/venvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 961, in _run % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape()))) ValueError: Cannot feed value of shape (1984, 128, 128, 3) for Tensor 'Input-image/image:0', which has shape '(32, 128, 128, 3)'
It seems there is an issue with batch size.