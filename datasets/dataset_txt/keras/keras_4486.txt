JosephDavidsonKSWH commented on 30 Aug 2016
Hi,
Is there any way to modify the input data as the model executes? So if we have some number of timesteps of input of the form (constant_input, mutable_input), is it possible for the model at timestep t to make changes to the mutable_input vector at time t+1?
I've dummied up a few layers which almost support this in the attached code. The mutable part of the input is a shared variable which is updated by the last layer of the model, however this only works if .predict() is called with each timestep individually. If called on all of them, each timestep is paired with
the initial state of the mutable_input, rather than the updated states which are the result of the models operation on the previous timestep.
I suspect this is because the layer computations are applied to all inputs/timesteps in a batch simultaneously, so changes to the mutable_input are not retroactively propagated through the merge
for each timestep. This kind of temporal dependency of the inputs necessitates stepwise application of the network to the inputs, but I believe that .fit() will then become an issue, especially if there are recurrent layers, as they will only be able to train for a single timestep per .fit() call.
Has anyone tried something similar? A solution for this may be to unroll the structure like recursive AE's (or presumably Tree LSTMs #352) for the number of timesteps and synchronise the environment layers. But it may quickly become unmanageable for many timesteps and/or larger nets.
Gist is https://gist.github.com/JosephDavidsonKSWH/61ff87939eb7ac8ef2c37b36b4de6250