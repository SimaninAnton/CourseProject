Jihadik commented on 10 Oct 2016
Suppose we have some trained RNN and some sample with T time-steps. I want to get hidden states of the network after each time-step. What is the best way of doing that?
Details
Architecture of my network is the following:
[Masking, LSTM, LSTM, TimeDistributed, Dense]
I want to get hidden states from the last LSTM. I'm just popping last two layers (Dense and TimeDistributed) and making predict_on_batch with the intention that predictions will be hidden states.
Unfortunately, something is wrong since all hidden states (obtained as was said earlier) looks like some fixed vector is multiplied by different constants. I looked at the weight matrices and they don't look like identity matrices.