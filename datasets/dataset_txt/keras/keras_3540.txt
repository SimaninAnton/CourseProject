vinayakumarr commented on 5 Jan 2017
snippet for tokenizer for text
tk = keras.preprocessing.text.Tokenizer(nb_words=500, filters=keras.preprocessing.text.base_filter(), lower=True, split=" ")
tk.fit_on_texts(x)
x = tk.texts_to_sequences(x)
What exactly the difference between the above code and the bag-of-words