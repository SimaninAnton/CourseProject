icelighter commented on 7 Jan 2017
If I have a time-series X with dimensions (steps, features) = (10000, 10) and a corresponding time-series Y with dimensions (steps, states) = (10000, 2) and I feed them into an LSTM layer with a TimeDistributedDense layer do the outputs respect causality? I'm trying to learn the time-series Y by feeding only the time-series X into the network; there is a functional mapping between the two.
As a concrete example, say I break up X into samples of 100 timesteps:
feautures = 10
timesteps = 100
output = 2
model = Sequential()
model.add(LSTM(50, return_sequences=True,input_shape=(timesteps, features)))
model.add(LSTM(50, return_sequences=True))
model.add(TimeDistributed(Dense(output,init='uniform',activation='linear')))
model.compile(loss='mse',optimizer='rmsprop',)
For each sample, this network will give output_vector = (100, 2), with the loss calculated against the corresponding timesteps in Y.
Would the RNN be able to 'cheat' by looking ahead in each sample of 100 timesteps and thus infer what the 100th timestep for the output_vector should be, or does the network respect causality, in the sense that each input timestep is processed one at a time and the output_vector is built up sequentially? Put another way, do the LSTM layers have access to all 100 timesteps simultaneously when calculating each layer's output?