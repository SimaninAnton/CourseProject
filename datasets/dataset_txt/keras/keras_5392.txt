tstandley commented on 21 Apr 2016 â€¢
edited
I have a situation where I want to split a sentence into three different chunks...
consider the following sentence, where the word 'sat' is called the pivot:
The cat [sat] on the mat
The words on the left of the pivot and the words on the right of the pivot are fed into GRU units after they get embedded.
However, I want to embed the pivot too, but I can't figure out how to share the embedding layer among the 3 pieces.
Using the functional API I can share the embedding layers for the pre-pivot and post-pivot pieces like so:
pre_pivot_input = Input(shape=(max_sequence_length,), dtype='int32')
post_pivot_input = Input(shape=(max_sequence_length,), dtype='int32')
shared_embedding = Embedding(vocab_size, embedding_dimension, weights=embedding_weights, input_length=max_sequence_length, mask_zero=True)
pre_pivot_vectors = shared_embedding(pre_pivot_input)
post_pivot_vectors = shared_embedding(post_pivot_input)
pre_pivot_embedding = getRecurrantFunctional(pre_pivot_vectors)
post_pivot_embedding = getRecurrantFunctional(post_pivot_vectors)
pivot_input = Input(shape=(1,), dtype='int32', name='pivot_input')
second_embedding = Embedding(vocab_size, embedding_dimension, weights=embedding_weights, input_length=1)
pivot_embedding = Reshape((embedding_dimension,))(second_embedding(pivot_input))
concat = merge([pre_pivot_embedding, pivot_embedding, post_pivot_embedding], mode='concat')
Is there any way to get these two embeddings to be the same? If not, do you have any suggestions whereby I might achieve something similar?
Thanks!