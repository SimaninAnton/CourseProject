slavakung commented on 6 Apr 2017 â€¢
edited
Hello,
I am a researcher in optimization and I trying to write a custom optimizer. I have come across a problem.
Take any optimizer code, say just copy SGD. In the beginning of get_updates, you see
    grads = self.get_gradients(loss, params)
now add the following line right after this one:
    gradsb = self.get_gradients(loss, [tf.Variable(a) for a in params])
this should compute the gradients at a new tensor, with all the values the same as before
now try to see what you get:
    for a in gradsb:
       print(a)
you get a list of Nones (but if you print the list grads you see that they are still Tensors)
Why?
And how to circumvent this problem? This is important as I'd like to compute the gradients at another point for my algorithm.
On a perhaps related level, when is batch_size used? get_gradients just goes to the gradients backend. So in some sense does the keras wrapper modify the loss function to just include the batch at the current iteration? Where is this done? I would be interested in adaptively modifying the batch_size in the optimizer, eventually.