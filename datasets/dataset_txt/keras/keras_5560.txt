indra215 commented on 7 Apr 2016
I have a dataset of size 84000x121 (image patches) and I want to train a autoencoder with single hidden layer. My architecture is 121 -> 100 -> 121 and use the hidden layer as features to train a regression model (say SVR).
import h5py
import scipy.io as sio
import numpy as np

from keras.models import Sequential
from keras.layers import containers
from keras.layers.core import Dense, AutoEncoder
from keras.activations import sigmoid
from keras.utils import np_utils
from keras.optimizers import SGD, RMSprop

# layer dimension
h1_dim = 100
input_dim = 121

# training data
data = sio.loadmat('../data/GBLUR_Patches.mat')
X_train = data['all_patches']
X_train = np.transpose(X_train)
# X_train = X_train.reshape(84000,h1_dim)

print np.shape(X_train)

# define model
model = Sequential()
encoder = containers.Sequential([Dense(output_dim=h1_dim, input_dim=input_dim)])
decoder = containers.Sequential([Dense(output_dim=input_dim, input_dim=h1_dim)])
model.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False))

# optimizer 
rms = RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)
model.compile(loss='mean_squared_error', optimizer=rms)

# fit model
model.fit(X_train, X_train, nb_epoch=10, batch_size=64, validation_data=None, verbose = 2, shuffle=True, show_accuracy = `False)
But when i train this model i'm getting the following error
Epoch 1/10
Traceback (most recent call last):
File "autoencoder_gblur.py", line 38, in
model.fit(X_train, X_train, nb_epoch=10, batch_size=64, validation_data=None, verbose = 2, shuffle=True, show_accuracy = False)
File "/usr/local/lib/python2.7/dist-packages/Keras-0.3.3-py2.7.egg/keras/models.py", line 701, in fit
shuffle=shuffle, metrics=metrics)
File "/usr/local/lib/python2.7/dist-packages/Keras-0.3.3-py2.7.egg/keras/models.py", line 317, in _fit
outs = f(ins_batch)
File "/usr/local/lib/python2.7/dist-packages/Keras-0.3.3-py2.7.egg/keras/backend/theano_backend.py", line 446, in call
return self.function(*inputs)
File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 871, in call
storage_map=getattr(self.fn, 'storage_map', None))
File "/usr/local/lib/python2.7/dist-packages/theano/gof/link.py", line 314, in raise_with_op
reraise(exc_type, exc_value, exc_trace)
File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 859, in call
outputs = self.fn()
ValueError: dimension mismatch in args to gemm (64,121)x(121,100)->(64,121)
Apply node that caused the error: GpuGemm{inplace}(GpuFromHost.0, TensorConstant{1.0}, GpuFromHost.0, dense_W, TensorConstant{-1.0})
Toposort index: 15
Inputs types: [CudaNdarrayType(float32, matrix), TensorType(float32, scalar), CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix), TensorType(float32, scalar)]
Inputs shapes: [(64, 121), (), (64, 121), (121, 100), ()]
Inputs strides: [(121, 1), (), (121, 1), (100, 1), ()]
Inputs values: ['not shown', array(1.0, dtype=float32), 'not shown', 'not shown', array(-1.0, dtype=float32)]
Outputs clients: [[GpuElemwise{Add}[(0, 1)](GpuDimShuffle{x,0}.0, GpuGemm{inplace}.0)]]
HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
Please anyone can help me with this..