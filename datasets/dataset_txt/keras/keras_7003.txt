juwlee commented on 26 May 2015
First of all, thank you for sharing this framework! Its nicely-written clean code makes it easier to do deep learning.
I wanted to report an issue that I face. Please see the following error:
  File "/Users/juwlee/workspace/personal/fstrat/model/model.py", line 22, in build
    self.model.compile(loss='mse', optimizer='sgd')
  File "build/bdist.macosx-10.5-x86_64/egg/keras/models.py", line 71, in compile
  File "build/bdist.macosx-10.5-x86_64/egg/keras/models.py", line 152, in get_output
  File "build/bdist.macosx-10.5-x86_64/egg/keras/layers/core.py", line 161, in get_output
  File "build/bdist.macosx-10.5-x86_64/egg/keras/layers/core.py", line 27, in get_input
  File "build/bdist.macosx-10.5-x86_64/egg/keras/layers/core.py", line 233, in get_output
  File "build/bdist.macosx-10.5-x86_64/egg/keras/layers/core.py", line 27, in get_input
  File "build/bdist.macosx-10.5-x86_64/egg/keras/layers/core.py", line 115, in get_output
  File "build/bdist.macosx-10.5-x86_64/egg/keras/layers/core.py", line 27, in get_input
  File "build/bdist.macosx-10.5-x86_64/egg/keras/layers/recurrent.py", line 338, in get_output
  File "/Users/juwlee/anaconda/lib/python2.7/site-packages/theano/tensor/var.py", line 341, in dimshuffle
    pattern)
  File "/Users/juwlee/anaconda/lib/python2.7/site-packages/theano/tensor/elemwise.py", line 141, in __init__
    (i, j, len(input_broadcastable)))
ValueError: new_order[2] is 2, but the input only has 2 axes.
where model.py looks like
  def build(self):
    self.model = Sequential()
    self.model.add(Dense(self.input_dim, self.input_dim / 2, W_regularizer = l1(.01)))
    self.model.add(LSTM(self.input_dim / 2))
    self.model.add(Dropout(0.5))
    self.model.add(Dense(self.input_dim / 2, self.output_dim, activation='softmax'))
    self.model.add(Reshape(self.output_dim, self.output_dim))
    self.model.compile(loss='mse', optimizer='sgd')
It seems the error is thrown from this line. The issue looks similar to #18 but the difference is that here I have LSTM layer stack up on top of Dense layer, whereas recurrent layer was at the bottom in #18 .