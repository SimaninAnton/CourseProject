RaffEdwardBAH commented on 3 Oct 2016
I've had a number of issues recently with the batch-norm statistics being "dirty" for some very large networks we are training. It seems the exponential average used has too much history from earlier iterations and the weights have drifted so far away that they are no longer useful. We see this issue manifest itself as a huge difference in training accuracy / loss, and predictiction accuracy /loss on the training data. While this issue "resolves" itself if we let the network continue to train for several more epochs, these models are huge and time consuming - it would be faster to just do an epoch of predictions and get the correct statistics as used in the original paper.
Is there a way to get keras to do this? I'm not seeing an obvious place to try and modify the keras code to add this functionality.