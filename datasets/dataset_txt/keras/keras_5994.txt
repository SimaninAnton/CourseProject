WeidiXie commented on 6 Feb 2016
Sorry for bothering you guys, I'm really new to python,
I want to write my own class for a general RNN, say, I want to i_t = h(t) + h(t-10)
here is the code, it just doens't work.
import theano
import theano.tensor as T
from keras import activations, initializations
from keras.layers.core import Layer
import numpy as np
class General_LSTM(Layer):
def init(self, input_dim, output_dim=128,
init='glorot_uniform', inner_init='orthogonal',
activation='tanh', inner_activation='hard_sigmoid',
truncate_gradient=-1, return_sequences=False,
output_mode = 'sum', rowLen = 1, seqLen = 1, **kwargs):
    self.input_dim = input_dim
    self.output_dim = output_dim
    self.truncate_gradient = truncate_gradient
    self.return_sequences = return_sequences
    self.output_mode = output_mode
    self.rowLen = rowLen
    self.seqLen = seqLen

    if self.input_dim:
        kwargs['input_shape'] = (self.seqLen, self.input_dim)

    super(General_LSTM, self).__init__(**kwargs)

    self.init = initializations.get(init)
    self.inner_init = initializations.get(inner_init)
    self.activation = activations.get(activation)
    self.inner_activation = activations.get(inner_activation)
    self.input = T.tensor3()

    # Weights to the input gates
    self.W_xi = self.init((self.input_dim, self.output_dim))

    self.W_hi_1 = self.inner_init((self.output_dim, self.output_dim))

    self.W_hi_k = self.inner_init((self.output_dim, self.output_dim))

    self.W_ci_1 = self.inner_init((self.output_dim, self.output_dim))

    self.W_ci_k = self.inner_init((self.output_dim, self.output_dim))

    self.b_i = theano.shared(value = np.zeros((self.output_dim,),dtype = theano.config.floatX))

    # Weights to the forget gates
    self.W_xf = self.init((self.input_dim, self.output_dim))

    self.W_hf_1 = self.inner_init((self.output_dim, self.output_dim))

    self.W_hf_k = self.inner_init((self.output_dim, self.output_dim))

    self.W_cf_1 = self.inner_init((self.output_dim, self.output_dim))

    self.W_cf_k = self.inner_init((self.output_dim, self.output_dim))

    self.b_f = theano.shared(value = np.zeros((self.output_dim,),dtype = theano.config.floatX))

    # Weights to cell units
    self.W_xc = self.init((self.input_dim, self.output_dim))

    self.W_hc_1 = self.inner_init((self.output_dim, self.output_dim))

    self.W_hc_k = self.inner_init((self.output_dim, self.output_dim))

    self.b_c = theano.shared(value = np.zeros((self.output_dim,),dtype = theano.config.floatX))

    # Weights to output gates
    self.W_xo = self.init((self.input_dim, self.output_dim))

    self.W_ho_1 = self.inner_init((self.output_dim, self.output_dim))

    self.W_ho_k = self.inner_init((self.output_dim, self.output_dim))

    self.W_co_1 = self.inner_init((self.output_dim, self.output_dim))

    self.W_co_k = self.inner_init((self.output_dim, self.output_dim))

    self.b_o = theano.shared(value = np.zeros((self.output_dim,),dtype = theano.config.floatX))

    self.h_0 = theano.shared(value = np.zeros((self.seqLen + self.rowLen, self.output_dim),dtype = theano.config.floatX))
    self.c_0 = theano.shared(value = np.zeros((self.seqLen + self.rowLen, self.output_dim),dtype = theano.config.floatX))
==============================================================================
                        # Backward #

    self.Wb_xi = self.init((self.input_dim, self.output_dim))

    self.Wb_hi_1 = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_hi_k = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_ci_1 = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_ci_k = self.inner_init((self.output_dim, self.output_dim))

    self.bb_i = theano.shared(value = np.zeros((self.output_dim,),dtype = theano.config.floatX))

    # Weights to the forget gates
    self.Wb_xf = self.init((self.input_dim, self.output_dim))

    self.Wb_hf_1 = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_hf_k = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_cf_1 = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_cf_k = self.inner_init((self.output_dim, self.output_dim))

    self.bb_f = theano.shared(value = np.zeros((self.output_dim,),dtype = theano.config.floatX))

    # Weights to cell units
    self.Wb_xc = self.init((self.input_dim, self.output_dim))

    self.Wb_hc_1 = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_hc_k = self.inner_init((self.output_dim, self.output_dim))

    self.bb_c = theano.shared(value = np.zeros((self.output_dim,),dtype = theano.config.floatX))

    # Weights to output gates
    self.Wb_xo = self.init((self.input_dim, self.output_dim))

    self.Wb_ho_1 = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_ho_k = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_co_1 = self.inner_init((self.output_dim, self.output_dim))

    self.Wb_co_k = self.inner_init((self.output_dim, self.output_dim))

    self.bb_o = theano.shared(value = np.zeros((self.output_dim,),dtype = theano.config.floatX))

    self.bh_0 = theano.shared(value = np.zeros((self.seqLen + self.rowLen, self.output_dim),dtype = theano.config.floatX))
    self.bc_0 = theano.shared(value = np.zeros((self.seqLen + self.rowLen, self.output_dim),dtype = theano.config.floatX))

    self.params = [self.W_xi, self.W_hi_1, self.W_hi_k, self.W_ci_1, 
                   self.W_ci_k, self.W_xf, self.W_hf_1, self.W_hf_k,
                   self.W_cf_1, self.W_cf_k, self.W_xc, self.W_hc_1,
                   self.W_hc_k, self.W_xo, self.W_ho_1, self.W_ho_k,
                   self.W_co_1, self.W_co_k, 
                   self.b_i, self.b_f, self.b_c, self.b_o, self.h_0,

                   self.Wb_xi, self.Wb_hi_1, self.Wb_hi_k, self.Wb_ci_1, 
                   self.Wb_ci_k, self.Wb_xf, self.Wb_hf_1, self.Wb_hf_k,
                   self.Wb_cf_1, self.Wb_cf_k, self.Wb_xc, self.Wb_hc_1,
                   self.Wb_hc_k, self.Wb_xo, self.Wb_ho_1, self.Wb_ho_k,
                   self.Wb_co_1, self.Wb_co_k, 
                   self.bb_i, self.bb_f, self.bb_c, self.bb_o, self.bh_0]

def _forward_step (self, x_t,
                   h_tm1, h_tmk, 
                   c_tm1 = None , c_tmk = None ):

    xi_t = T.dot(x_t, self.W_xi) + self.b_i
    xf_t = T.dot(x_t, self.W_xf) + self.b_f
    xc_t = T.dot(x_t, self.W_xc) + self.b_c
    xo_t = T.dot(x_t, self.W_xo) + self.b_o

    i_t = self.inner_activation(xi_t + \
                         T.dot(self.W_hi_1, h_tm1) + \
                         T.dot(self.W_hi_k, h_tmk) + \
                         T.dot(self.W_ci_1, c_tm1) + \
                         T.dot(self.W_ci_k, c_tmk) )
    f_t1 = self.inner_activation(xf_t + \
                         T.dot(self.W_hf_1, h_tm1) + \
                         T.dot(self.W_cf_1, c_tm1) )
    f_tk = self.inner_activation(xf_t + \
                         T.dot(self.W_hf_k, h_tmk) + \
                         T.dot(self.W_cf_k, c_tmk) )
    c_t = f_t1 * c_tm1 + f_tk * c_tmk + i_t * \
                        self.activation(xc_t + \
                         T.dot(self.W_hc_1, h_tm1) + \
                         T.dot(self.W_hc_k, h_tmk) )
    o_t = self.inner_activation(xo_t + \
                         T.dot(self.W_ho_1, h_tm1) + \
                         T.dot(self.W_ho_k, h_tmk) + \
                         T.dot(self.W_co_1, c_tm1) + \
                         T.dot(self.W_co_k, c_tmk) )
    h_t = o_t * self.activation(c_t)

    return [h_t, c_t]


def get_forward_output(self, train):
    X = self.get_input(train)
    X = X.dimshuffle((1,0,2))

    [outputs, memories],_ = theano.scan(
            self._forward_step, 
            sequences = X,
            outputs_info = [dict(initial = self.h_0, taps = [-self.rowLen, -1]),
                            dict(initial = self.c_0, taps = [-self.rowLen, -1])],
            go_backwards = False,
            truncate_gradient=self.truncate_gradient)
    return outputs.dimshuffle((1,0,2))

def get_backward_output(self, train):
    X = self.get_input(train)
    X = X.dimshuffle((1,0,2))


    [outputs, memories],_ = theano.scan(
            self._forward_step, 
            sequences = X,
            outputs_info = [dict(initial = self.bh_0, taps = [-self.rowLen, -1]),
                            dict(initial = self.bc_0, taps = [-self.rowLen, -1])],
            go_backwards = True,
            truncate_gradient=self.truncate_gradient)
    return outputs.dimshuffle((1,0,2))

def get_output(self, train):
    forward = self.get_forward_output(train)
    backward = self.get_backward_output(train)
    if self.output_mode is 'sum':
        output = T.squeeze(forward + backward)
    elif self.output_mode is 'concat':
        output = T.concatenate([forward, backward], axis = 2)
    else:
        raise Exception('output mode is not sum or concat')

    if self.return_sequences == False:
        return output[:,-1,:]
    elif self.return_sequences == True:
        return output[:,self.rowLen:,:]
    else:
        raise Exception('Unexpected output shape for return_sequences')

def get_config(self):
    return {"name":self.__class__.__name__,
        "input_dim":self.input_dim,
        "output_dim":self.output_dim,
        "init":self.init.__name__,
        "inner_init":self.inner_init.__name__,
        "activation":self.activation.__name__,
        "inner_activation":self.inner_activation.__name__,
        "truncate_gradient":self.truncate_gradient}
The error is about the scan function,
ValueError: Please provide None as outputs_info for any output that does not feed back into scan (i.e. it behaves like a map)