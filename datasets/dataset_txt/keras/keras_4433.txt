Contributor
gw0 commented on 6 Sep 2016
Would it make sense to modify the GRU layer to apply the reset gate after the recurrent multiply contributes to get a ~10% speedup without loss in accuracy (analyzed by Baidu)? "While this changes the function being computed, it has similar functionality, and produces extremely similar training / accuracy performance in many of our experiments. It also makes all the recurrent multiplies independent of each other."
If I understand correctly, they are suggesting the change at line L562 of:
-            hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))
+            hh = self.activation(x_h + r * K.dot(h_tm1 * B_U[2], self.U_h))
This then enables to group and optimize GEMM computations as recommended and will hopefully produce speedups up to ~40%.