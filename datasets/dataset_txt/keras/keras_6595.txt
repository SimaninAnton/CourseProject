Contributor
jeffzhengye commented on 5 Oct 2015
Hi. all.
I have a problem to make different inputs go through the same/shared embedding layer. Is there any good workaround?
'''
shared_embedding_layer = Embedding(max_features, embedding_size, weights=[W2V], W_regularizer=l2(50.), mask_zero=False)
graph = Graph()
graph.add_input(name='input1_story', ndim=3, dtype='int64') # story_input: nb_story * nb_sentence * nb_words
graph.add_input(name='input2_question', ndim=2, dtype='int64') # question_input: nb_story*4 * nb_words
graph.add_node(shared_embedding_layer, name='story_word_embedding',
input='input1_story') # nb_batch * nb_sentence * nb_words * embedding_size
graph.add_node(shared_embedding_layer, name='question_word_embedding', input='input2_question')
'''
The problem is if you use a shared_embedding_layer, shared_embedding_layer.previous will change.
But I also don't want to multiple embedding with the same weights.
Thanks