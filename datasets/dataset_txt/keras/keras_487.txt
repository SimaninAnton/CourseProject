ZeroCool11000 commented on 3 Jan 2019
I trained two net structures one without batch normalization and one with batch normalization, the only difference is just that I added batch normalization for one net. The backend was in both cases CNTK. The problem is just that the net with batch normalization takes a factor of 10 more inference time during c++ inference. I know that batch normalization takes more operations but not a factor of 10 ? The used net without batch normalization looks similar to this:
model = Sequential()
input: 100x100 images with 3 channels -> (3, 100, 100) tensors.
this applies 32 convolution filters of size 3x3 each.
model.add(Convolution2D(32, 3, 3, border_mode='full', input_shape=(3, 100, 100)))
model.add(Activation('relu'))
model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Convolution2D(64, 3, 3, border_mode='valid'))
model.add(Activation('relu'))
model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
Note: Keras does automatic shape inference.
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(10))
I only added
model.add(BatchNormalization())
after each Convolution
I also read that BatchNormalization() should not be used if DropOut is used like her https://towardsdatascience.com/dont-use-dropout-in-convolutional-networks-81486c823c16, is it also possible to use BatchNormalization() only before the classification output like for example
.....
model.add(BatchNormalization())
model.add(Dense(10))
...
Are there any examples for batch normalization with CNTK and how is this layer used right ? Maybe BatchNormalization has the same influence like adding an additional convolution layer for the inference time? But batch normalization has mainly linear order not quadratic like for the convolutional layer?
Best regards