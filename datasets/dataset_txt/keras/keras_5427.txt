imayukh commented on 19 Apr 2016 â€¢
edited
Hi,
I have been trying to freeze a layer before training. But it does not seem to be working.
The code below illustrates this.
from keras.layers import Dense
from keras.models import  Sequential

model = Sequential()
model.add(Dense(32, input_shape=(500,)))
model.add(Dense(10, activation='softmax',trainable = False))
model.compile(optimizer='rmsprop',
      loss='categorical_crossentropy',
      metrics=['accuracy'])
model.get_config() produces the following output
[{'class_name': 'Dense',
  'config': {'W_constraint': None,
   'W_regularizer': None,
   'activation': 'linear',
   'activity_regularizer': None,
   'b_constraint': None,
   'b_regularizer': None,
   'batch_input_shape': (None, 500),
   'init': 'glorot_uniform',
   'input_dim': None,
   'input_dtype': 'float32',
   'name': 'dense_1',
   'output_dim': 32,
   'trainable': True}},
 {'class_name': 'Dense',
  'config': {'W_constraint': None,
   'W_regularizer': None,
   'activation': 'softmax',
   'activity_regularizer': None,
   'b_constraint': None,
   'b_regularizer': None,
   'init': 'glorot_uniform',
   'input_dim': None,
   'name': 'dense_2',
   'output_dim': 10,
   'trainable': False}}]
However, model.trainable_weights is [dense_1_W, dense_1_b, dense_2_W, dense_2_b] while model.non_trainable_weights is [].
Looking through layers/core.py, I noticed that in the build method of the Dense class, the trainable flag is not checked before self.trainable_weights is set.
Changing the build function to:
        if self.trainable:
            self.trainable_weights = [self.W, self.b]
        else:
            self.non_trainable_weights = [self.W, self.b]
appears to have fixed the problem. However, this change would need to be made to the build function of each layer individually. Is there a more elegant solution to this problem?