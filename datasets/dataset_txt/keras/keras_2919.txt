zehzhang commented on 20 Mar 2017 â€¢
edited
I am trying to implement Grad-CAM and need to compute the gradients of the output of the last softmax layer w.r.t. a certain former layer.
This is my model:
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3)) model = Sequential() model.add(base_model) model.add(Flatten()) model.add(Dense(4096, activation='relu')) model.add(Dense(4096, activation='relu')) model.add(Dense(24, activation='softmax'))
If I compute the gradients like this:
grads = K.gradients(model.layers[-1].output[0, 0], model.layers[-5].layers[-2].output)[0]
I will get an array all of zeros.
However, if I compute the gradients of the output of the last but two layer w.r.t. the same former layer:
grads = K.gradients(model.layers[-2].output[0, 0], model.layers[-5].layers[-2].output)[0]
I will get reasonable gradients.
So how can I solve this?