unrealwill commented on 30 Sep 2016
I have a gradient which is noisy, so it needs to be accumulated across many examples, without moving.
It converges better when I increase batch_size (~1000) but it consumes more memory.
When using theano without keras, I used to accumulate the gradient (in an extra accumulated gradient variable) from multiple passes over the examples.
Memory usage could be reduced (a lot) by performing multiple passes. It could also allow some multi-gpu parallelism.
Is there a mechanism in Keras for doing so ?
(The only problem I see is with ops like Batch Normalization which can't be calculated easily with reduced memory).
Thank you