SourKream commented on 24 Apr 2016 â€¢
edited
I am implementing a word by word attention model as proposed in this paper. I had come up with 2 ways of implementing such a model.
The first method I though was to implement an unrolled version of the attention model. This unrolled version has multiple Dense layers that are actually supposed to have the same weigths, but fitting directly may result in different weigths. So I use a callback which, on every batch end, sets the weights of all these Dense layers, that are supposed to be constraint to have the same weights, to the average value of the learnt weights of these layers. (As suggested by @andychisholm in issue #362) This method has been working decently well for me and it takes about 1hr per epoch to train.
The second method I thought was to create a custom recurrent node, which extends the Recurrent class. I made this class by looking at the class definition for SimpleRNN node and LSTM node. I have included this code below. Since time was my only concern here and not memory, I wrote the code (to the best that i could think of) only for consume_less=='cpu', although i later also tried (again to the best that i could think of) for consume_less=='mem' and it did not really give me a time boost. I am running the code on a GPU. This node takes around 1 day per epoch to fit.
This is a very drastic difference in terms of time taken by the 2 methods, considering that both the methods are essentially performing the same thing, even though method 2 is what i understand to be the correct way to implement it. Any suggestions for improving the running time for method 2 or any comments on making method 1 work better are welcome.
attentionModel.txt
Gist for the above code.