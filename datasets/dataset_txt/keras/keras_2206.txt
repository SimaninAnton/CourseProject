jinfengfeng commented on 7 Jun 2017
Hello!
I'm using Keras to learn models to automatically score essays. When I use sequence.pad_sequences(data, maxlen=maxlen, padding='pre', truncating='pre') to pad the word index sequences of each essay, I got much worse performance using Keras 2.0.3 than using Keras 1.0.3. But if I pad the sequences post using pad_sequences(data, maxlen=maxlen, padding='post', truncating='post'), I got similar performance. So is there any bug in the RNN (LSTM) layer(s) of Keras 2.0.3 when handling pre-padded input?