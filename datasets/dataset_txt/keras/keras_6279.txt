Contributor
ozancaglayan commented on 10 Dec 2015
Hi,
Theano's T.nnet.categorical_crossentropy is a dual-behaviour function which accepts either a 2D tensor or an integer vector as the true_dist parameter.
I have a graph model with an output node that concatenates the output of N different softmax activations. I also have an integer vector representing the true labels.
By manually calling T.nnet.categorical_crossentropy() on my predictions and the true labels, I can obtain the loss. But this isn't working through Keras probably because of the standardize_y() function which forces the labels to be a 2D tensor. So we can't benefit of the second functionality of theano.
I implemented the following loss (which first flattens the true labels and then cast it to integer) to make things work:
def categorical_crossentropy_int(y_true, y_pred):
    return K.mean(K.categorical_crossentropy(y_pred, K.cast(y_true.flatten(), dtype='int32')), axis=-1)
What do you suggest as a fix for this? I saw that there's a utility function which converts integer label vector to one-hot encoded 2D tensor but I think with a huge class size (like 20k, 30k) this method is less efficient than the integer vector approach.
Thanks.