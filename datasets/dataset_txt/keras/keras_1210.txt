tchaton commented on 1 Mar 2018 â€¢
edited
Hello there,
I am trying to calculate the distance between a tensor (inputs: batch of previous activations, (batch_size, dim)) to an another tensor of different dimension (nb_class, dim), and in order reduce overhead memory, I wanted to iterate over the first tensor column (range(batch)), tile the vector for each activation; get of the size (nb_class, dim). Then calculate the distance iteratively between the tiled input and the tensor. But batch_size is not defined until the graph is compiled and feed with data. So the compile step doesn t work. I tried to hack it by passing the batch_size to the custom layer. But still didn t work.
Could you please give me an idea how to solve this issue.
Best Regards, T.C
Here is the code I am trying to make work.
def get_distance(activations, tensor, batch):
dists = []
batch_size = activations.get_shape().as_list()[0]
for i in range(batch_size):
tile = tf.tile (tf.expand_dims (activations[i], axis=0), (nb_class, 1))
dist = distance(tile, tensor)
dists.append(dist)
dists = tf.expand_dims(dists, axis=0)
return tf.concat(dists, axis=0)