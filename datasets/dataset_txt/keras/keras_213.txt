Raukk commented on 24 May 2019
Note: this is different than re-using the same layer in the network.
For a specific example; (Code below)
If multiple Convolutions with the only difference being dilation_rate (Note: padding='same', strides=1) are given the same input, then they will have the same weight shapes, because the dilation only changes what values are inputted, not the inner workings of the network or it's weights.
Workaround:
A 'simple' workaround is to fully training one layer, then transfer the weights.
This is tangentially related to this old issue Filter Rotation in Convolution
System information
Have I written custom code (as opposed to using example directory): See end for code
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
TensorFlow backend (yes / no): YES
TensorFlow version: 1.13.1 - Google Colab
Keras version: 2.2.4 - Google Colab
Python version: 3.6.7 - Google Colab
CUDA/cuDNN version: /NA (CPU)
GPU model and memory: /NA (CPU)
Describe the current behavior
The Weights are trained separately.
Describe the expected behavior
The Weights are Shared during Training and Inference.
Code to reproduce the issue
Here is the Colab
import tensorflow as tf
import keras
import numpy as np
import sys
print(sys.version_info)
print(tf.VERSION)
print(keras.__version__)

input = keras.layers.Input(shape=(10,10,1))

layer_1 = keras.layers.Conv2D(4, (3, 3), dilation_rate=1, padding='same', strides=1, activation='relu')
layer_2 = keras.layers.Conv2D(4, (3, 3), dilation_rate=2, padding='same', strides=1, activation='relu')
layer_3 = keras.layers.Conv2D(4, (3, 3), dilation_rate=3, padding='same', strides=1, activation='relu')

concat = keras.layers.Concatenate()([layer_1(input), layer_2(input), layer_3(input)])

last_layer = keras.layers.Dense(10, activation='softmax')(concat)

model = keras.models.Model(inputs=input, outputs=last_layer)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

print(layer_1.get_weights()[0].shape)
print(layer_1.get_weights()[1].shape)

print(layer_2.get_weights()[0].shape)
print(layer_2.get_weights()[1].shape)

print(layer_3.get_weights()[0].shape)
print(layer_3.get_weights()[1].shape)
(3, 3, 1, 4)
(4,)
(3, 3, 1, 4)
(4,)
(3, 3, 1, 4)
(4,)