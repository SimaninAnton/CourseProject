ShunyuanZ commented on 11 Jan 2017
I have a data set of 250,000 images, about 130GB, in hdf5 format. The problem is that when I do the following to read data, it will exceed the memory limit.
datapath='mydataset.h5' # data set
h5f=h5py.File(datapath,'r')
X_train=h5f['images'][:] # X is image data
y_train=h5f['labels'][:] # y is label/target
Now what I've tested is that if I only load 1/3 of data (~80,000 images), that is fine. So I'm thinking of splitting data into 3 parts then read each of them in a sequence when training. Ideally, I'd like to let the model trained on the first part, then it automatically reads the 2nd part when finished training on the first batch, and then moves to the 3rd part when done with the 2nd part. Hence, one epoch consists of training on 3 parts in a sequence.
How should I I implement this type of training (i.e., read part data from local drive then move to next part without interrupting training)? Thank you very much!