Contributor
braingineer commented on 28 Apr 2016 â€¢
edited
related: #1941 #1703 and a ton of others, but they don't really capture this issue.
It came up in #2413 and #2393 when discussing masks for merge layers, but was consigned to another PR. This issue post is basically to discuss the best route.
basically: (0 is mask)
abcd000 
# and its reversed version
000dcba
The desired output should be aligned
abcd000
dcba000
To have a proper aligned bi-directional RNN. And actually.. being able to set the offset would also be useful.. If one wanted.
Tensorflow does not have this problem. They have a custom implementation which addresses it.
I've **scoured theano, lasagne, blocks, pylearn issues and code, but I can't find any implementations that would work.
At the moment, I have the following possible solution:
https://gist.github.com/braingineer/4837baee2f2782f77cddc0dce852064d
**footnote. though, the internets are vast. I probably missed things.