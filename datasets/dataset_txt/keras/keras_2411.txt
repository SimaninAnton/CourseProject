gauss-clb commented on 12 May 2017 â€¢
edited
When I write the code, I find something weird about lambda layer.
So I look the source code:
https://github.com/fchollet/keras/blob/master/keras/layers/core.py#L623
I provide [tf.variable, tf.placeholder] to lambda layer, but it change inputs to [tf.placeholder, tf.placeholder],
and my tf.variable links to previous network, it is used to bp algorithm. But tf.placeholder is just a input we should feed, so I think the it break the link of previous network, and bp can't arrive the previous network.
So I can't understand why lambda will change tf.variable to tf.placeholder?
And my tf.variable.dtype is 'int32', but lambda layer change it to tf.placeholder.dtype 'float32', and I pass it to K.one_hot, which only need 'int' paramters, so I get a error.
1