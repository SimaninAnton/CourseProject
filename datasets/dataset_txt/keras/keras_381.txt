LuCeHe commented on 14 Feb 2019 â€¢
edited
I'm not sure if the gradients are passing through the embedding layer properly:
import tensorflow as tf

import keras.backend as K
from keras.models import Sequential, Model
from keras.layers import Dense, concatenate, Input, Conv2D, Embedding, \
                         Bidirectional, LSTM, Lambda, TimeDistributed, \
                         RepeatVector, Activation


vocabSize = 3
embDim = 5
rnn = LSTM(vocabSize, return_sequences=True)
embedding = Embedding(vocabSize, embDim)
      
      
input_questions = Input(shape=(None,), name='question')
embed = embedding(input_questions)
rnn_output = rnn(embed)    
softmax = TimeDistributed(Activation('softmax'))(rnn_output)

# these gradients don't work, FIXME!
grad = tf.gradients(xs=input_questions, ys=rnn_output)
print('grad (xs=input_questions, ys=rnn_output):   ', grad)            
grad = tf.gradients(xs=input_questions, ys=embed)
print('grad (xs=input_questions, ys=embed):        ', grad)            
grad = tf.gradients(xs=input_questions, ys=softmax)
print('grad (xs=input_questions, ys=softmax):      ', grad)    
grad = tf.gradients(xs=embed, ys=softmax)
print('grad (xs=embed, ys=softmax):                ', grad)    

model = Model(inputs=input_questions, outputs=softmax)
The output of this code is
grad (xs=input_questions, ys=rnn_output):    [None]
grad (xs=input_questions, ys=embed):         [None]
grad (xs=input_questions, ys=softmax):       [None]
grad (xs=embed, ys=softmax):                 [<tf.Tensor 'gradients_10/lstm_3/transpose_grad/transpose:0' shape=(?, ?, 5) dtype=float32>]