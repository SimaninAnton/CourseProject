penguinshin commented on 14 Aug 2016
Hi guys, I'm trying to see if Keras linear neural network (one dense layer but no non-linear activation functions) can replicate the results of an equivalent sklearn linear regression. So far, I'm having issues getting equal performance. My inputs are of dimension 625, and I used the following code:
model = Sequential() model.add(Dense(1, input_shape = (x_train.shape[1],))) model.compile(loss='mse', optimizer='adagrad') model.fit(x_train, y_train, batch_size=128, nb_epoch=1000, verbose=1)
Whereas the linear regression in sklearn gets an r^2 of .26 in roughly 2 minutes, this NN in keras doesn't get beyond .2 after thousands of epochs. I haven't been able to try SGD yet because it spits out NAN's. My y values are very large, so that might the issue for why SGD is failing, but not sure.
Furthermore, if I take the weights learned from sklearn linear regression and initialize the NN with them, i do in fact get an r^2 of .26. Thus, this NN has the ability to represent the optimal linear regression, but it can't seem to converge to that solution.
Any ideas?
Thanks!