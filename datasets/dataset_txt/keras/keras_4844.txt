Contributor
jmhessel commented on 5 Jul 2016 â€¢
edited
What is hierarchical softmax used for?
Hierarchical softmax is an approximate version of softmax that is much faster to compute when the number of potential classes is high. In general, hierarchical softmax is used in language modeling tasks, where vocabulary sizes exceed ~10K. For very large vocabulary scenarios (think 1M word vocab) the normal softmax is computationally expensive. Two popular methods for approximating the softmax in cases of large vocabularies are negative sampling (a simplification of noise-constrastive estimation) and hierarchical softmax. The version I have implemented is the two-layer version, originally given here. This version is thought to be efficient for GPUs, in particular, because it decomposes the softmax into two reasonably sized matrix multiplications.
How does hierarchical softmax work?
Hierarchical softmax can be thought of as a tree -- each leaf node of the tree is a class of softmax. Instead of computing a probability distribution over all possible classes given an input vector, hierarchical softmax computes the probability of a single given class (generally the correct class) given an input vector. Each node on the path from the root to the correct leaf node is tasked with making a decision about which of its children it should go to. Each node learns a linear classifier to pick a child node given an input, and the total probability of the correct class is modeled as the product of the probabilities of the correct decision at each node from the root to the correct leaf node.
For a much better explanation, check out Hugo Larochelle's video
What's the status of the implementation?
I have forked keras, and believe that I have a somewhat functional version of two-layer hierarchical softmax. However, it's pretty messy (see below).
I have tested it on a simple MNIST MLP classification model, and in a "large number of classes" case. In some cases, it runs 10x faster than normal softmax. In other cases, it's comparable or even slower than normal softmax. I wrote a dummy task script, which also demonstrates how to use the layer..
Remaining questions
Is this a feature keras wants to have in the main repo? Should I keep working on this?
I added an additional objective function: "mean identity error," which basically ignores the targets, and simply takes a mean over the input tensor. This causes Theano to raise a warning about unused portions of the computation graph. Mean identity error is good for hierarchical softmax, because the layer outputs the negative log probability of the correct class -- this is something that should probably be minimized. The "right" solution might be to have models that require no labels (e.g. it could become legal to pass "None" as the target when using mean identity error; this could be useful for a variety of models outside of hierarchical softmax).
Is my implementation too messy? It was a bit of a pain to write, and seems a bit inefficient, and I had to do some things in a relatively round-about way (e.g. the indexing operations are strange and messy). The messiness potentially derives from both my own coding skills, and from the limited set of functions supported by the backend (see below).
Theano has an implementation of hierarchical softmax, which would be ideal to use. Theano's HSM is made efficient by a specialized call to "sparse_block_dot," which is not in the keras backend because, to my knowledge, this operation doesn't exist in tensorflow. Many of the messy operations I had to write would be eliminated if there were access to this operation through keras. Would it be possible to support an operation only for one backend (this would allow for the use of theano's optimized implementation)? Or is that not worth it?
9
2