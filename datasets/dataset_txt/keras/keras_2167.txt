szcom commented on 11 Jun 2017 â€¢
edited
When sharing stateful rnn between models computational graph asks for inputs across these models. So I wonder if this is a good idea at all?
import keras
import numpy as np
from keras import layers
from keras.layers import merge


'''Large model - weights container'''
a_input = keras.layers.Input(batch_shape=(1,1,8))
a_gru = keras.layers.GRU(8, return_sequences=True, stateful=True)
a_top = a_gru(a_input)

b_input = keras.layers.Input(batch_shape=(1,1,8))
b_add = layers.add([b_input, a_top])
b_gru = keras.layers.GRU(8, return_sequences=True, stateful=True)
b_top = b_gru(b_add)

''' Small predictor - sharing b_gru from large '''
c_input = keras.layers.Input(batch_shape=(1, 1, 8))
c_gru = b_gru 
c_top = c_gru(c_input)


ab = keras.models.Model(inputs=[a_input, b_input], outputs=b_top)
c = keras.models.Model(inputs=[c_input], outputs=c_top)

x = np.ones((1,1,8))
'''Here ab model wants input from model "c" due to statefulness of b_gru'''
ab.predict_on_batch([x,x])