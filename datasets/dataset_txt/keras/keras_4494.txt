ttthhh commented on 29 Aug 2016
The mnist_hierarchical_rnn.py example mentions that
HRNNs can learn across multiple levels of temporal hiearchy over a complex sequence.
Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors)
into a sentence vector. The second recurrent layer then encodes a sequence of
such vectors (encoded by the first layer) into a document vector. This
document vector is considered to preserve both the word-level and
sentence-level structure of the context.
Is there a code example demonstrating HRNN on text, such as the IMDB data?