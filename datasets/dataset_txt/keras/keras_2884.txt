wolfshow commented on 23 Mar 2017 â€¢
edited
I found the code has the problem as the title shows, but if I add the dropout layer model.add(Dropout(dropout)), it can work. Anyone knows why? The back-end is tensorflow 1.0, Keras 2.0.2
def prep_model1(embedding_layer1, embedding_layer2, dropout=0.5):

    model0 = Sequential()  
    model0.add(embedding_layer1)
    model0.add(Bidirectional(LSTM(128, return_sequences=False, dropout=dropout)))

    model1 = Sequential() 
    model1.add(embedding_layer2)
    model1.add(Bidirectional(LSTM(128, return_sequences=False, dropout=dropout)))

    model = Sequential()
    model.add(Merge([model0, model1], mode='concat', concat_axis=1))
    #model.add(Dropout(dropout))
    model.add(Dense(1, activation='sigmoid'))

    return model
11