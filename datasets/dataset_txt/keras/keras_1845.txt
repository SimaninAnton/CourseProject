stulacy commented on 27 Jul 2017 â€¢
edited
I've got an MLP with an input layer connecting into a Dense layer. I currently have 3 inputs, the last of which must only have non-negative weightts, but for the other 2 covariates I don't want any restrictions.
Currently, my Dense layer is defined as:
Dense(2, activation='relu',
      kernel_constraint=non_neg(), 
      kernel_initializer=RandomUniform(minval=0, maxval=2))
However, I'd ideally only apply this initialiser and constraint to the last input, i.e. the last row of the weight matrix. Is this possible?
1