Contributor
icyblade commented on 24 Mar 2017 â€¢
edited
There are some new neural networks which need to freeze some specific connections(weights) between layers. For example: PathNet: Evolution Channels Gradient Descent in Super Neural Networks. But I can't find related methods after digging the source code.
Is it possible to implement something like this?
model.freeze_weight(lambda x: np.abs(x) < 1e-3)
I'd like to contribute if possible (because I'm doing some related works).