twangnh commented on 7 Dec 2016
Hi! as proposed in #4594 ,keras mask layer can now deal with variable-length sequence training of RNN, but I still get lower accuracy with mask layer than single batch training, I suspected that if I'm using mask layer correctly, below is details of my implementation, my goal is like training LSTM to learn how to spell words, the sequences(which are like different words composed of English letters) are encoded with one-hot representation, below is code of data encoding part: ( details: chars are the set of all letters that make up the sequences, mylist is list of the sequences , MAXLEN is the max length of sequences )
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

X = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)
y = np.zeros((len(mylist), MAXLEN, len(chars)), dtype=np.bool)

for i, sentence in enumerate(mylist):
    for t in range(len(sentence)-Data_end):
        X[i, t, char_indices[sentence[t]]] = 1
        y[i, t, char_indices[sentence[t+1]]] = 1
then network part is:
model = Sequential()
model.add(Masking(mask_value=0., input_shape=(None, len(chars))))
model.add(LSTM(2000, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(2000, return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributed(Dense(len(chars))))
model.add(Activation('softmax'))

sgd = SGD(lr=lr_init, decay=decay_init, momentum=momentum_init, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd)
early_stopping = EarlyStopping(patience=2,verbose=1)
then the training part is:
model.fit(X, y, callbacks=[early_stopping],batch_size=32, nb_epoch=1)
I'm wondering if I'm using mask correctly?