Holy-Shine commented on 19 Jun 2017 â€¢
edited
When I use Keras to write my own Layer as follows:
def call(self, inputs):
out = K.variable(np.zeros(shape=(1,self.output_dim),
                  dtype='float32'))
for i in range(K.int_shape(inputs)[-1]):
    dui_jiao_c = np.zeros(shape=(K.int_shape(inputs)[-1],K.int_shape(inputs)[-1]),
                         dtype='float32')
    dui_jiao_o = np.eye(N=K.int_shape(inputs)[-1],
                         dtype='float32')
    dui_jiao_c[i,i] = 1
    dui_jiao_o[i,i] = 0

    out += T.mul(K.dot(K.dot(inputs,dui_jiao_c),self.kernel),
                 K.dot(K.dot(inputs,dui_jiao_o),self.kernel))


return out
After compiling model,I try to fit data: model.fit(train,label,batch_size=10,epochs=20,verbose=1,shuffle=True)
and it raises error:
ValueError: Input dimension mis-match. (input[0].shape[0] = 10,input[18].shape[0] = 1)
Apply node that caused the error: Elemwise{Composite{((i0 * i1) + (i2 * i3) + (i4 * i5) + (i6 * i7) + (i8 * i9) + (i10 * i11) + (i12 * i13) + (i14* i15) + (i16 * i17) + i18 + (i19 * i20))}}(Dot22.0, Dot22.0, Dot22.0, Dot22.0,Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, Dot22.0, pair_wise_layer_1/variable, Dot22.0, Dot22.0)
Toposort index: 75
Inputs types: [TensorType(float32, matrix), TensorType(float32, matrix),TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix), TensorType(float32, matrix)]
Inputs shapes: [(10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (10L, 4L), (1L, 4L), (10L, 4L), (10L, 4L)]
Inputs strides: [(16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L), (16L, 4L)]
Inputs values: ['not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', 'not shown', array([[ 0.,  0.,  0.,  0.]], dtype=float32), 'not shown', 'not shown']
Outputs clients: [[InplaceDimShuffle{1,0}(Elemwise{Composite{((i0 * i1) + (i2 * i3) + (i4 * i5) + (i6 * i7) + (i8 * i9) + (i10 * i11) + (i12 * i13) + (i14 * i15) + (i16 * i17) + i18 + (i19 * i20))}}.0), Gemm{no_inplace}(/dense_1_target, TensorConstant{1.0}, Elemwise{Composite{((i0 * i1) + (i2 * i3) + (i4 * i5) + (i6 * i7) + (i8 * i9) + (i10 * i11) + (i12 * i13) + (i14 * i15) + (i16 * i17) + i18 + (i19 * i20))}}.0, dense_1/kernel, TensorConstant{-1.0})]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
If the pramater batch_size=1 it could work.
I am confused,what should I do