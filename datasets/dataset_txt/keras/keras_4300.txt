ijmarshall commented on 27 Sep 2016
My custom metric functions are not producing expected results in keras. I've narrowed down the issue down to the y_true tensor passed to the scoring function.
The y_true tensors appear to contain floats inbetween 0 and 1, even in binary classification tasks where the true y's should only be 0's and 1's.
The can be reproduced in the keras IMDB CNN example, by adding the following simple functions to demonstrate:
def y_sum(y_true, y_pred):
    """sum of 0/1s should be int"""
    return K.sum(y_true)

def y_ones(y_true, y_pred):
    """count the ones"""
    return T.eq(y_true, 1).nonzero()[0].shape[0]

def y_zeros(y_true, y_pred):
    """count the zeros"""
    return T.eq(y_true, 0).nonzero()[0].shape[0]

def y_element(y_true, y_pred):
    """return a single element from the y_trues"""
    return y_true[0][0]
and using these by changing the model.compile line to:
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=[y_sum, y_ones, y_zeros, y_element])
Given the y_train numpy array of true labels contains solely 0/1s, I would expect the following metrics to return integers, however, on fitting, all of them return non-integer values, indicating there are non 0/1 values in the y_trues. E.g.:
Epoch 1/2
 1856/25000 [=>............................] - ETA: 67s - loss: 0.6872 - y_sum: 8.1293 - y_ones: 8.1293 - y_zeros: 7.8707 - y_element: 0.5259
Thank you for any help, and my apologies if I have misunderstood what should be happening here!