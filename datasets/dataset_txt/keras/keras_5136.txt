allentran commented on 21 May 2016 â€¢
edited
I have two layers with output shape (b, t_1, p) and (b, t_2, p) .
If I setup a TimeDistributedDense layer, it is not possible to use this on the first layer and the second (or vice versa) unless t_1=t_2. Is there a way around this?
Exception: Input 0 is incompatible with layer timedistributed_1: expected shape=(None, <TensorType(int32, scalar)>, 40), found shape=(None, <TensorType(int32, scalar)>, 40)
If not, I can probably come up with a custom layer to do this.