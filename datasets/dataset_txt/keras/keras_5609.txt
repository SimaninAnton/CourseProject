rfeinman commented on 1 Apr 2016
I am using Keras for sequence classification, and I would like to add temporal pooling to the classification task. Thus, rather than handing off only the last hidden layer output from the rnn layer, I would like to hand off the sequence of hidden outputs and pool over the time steps to produce a single temporal component. The input to the pooling layer would have dimensionality (samples, steps, features) and the output would have (samples, features). It should be able to handle masked input. It would look like the following:
X = [[0,0,0,1,2,5], [0,2,7,3,5,1], [0,0,5,5,1,3]] # this is returned from pad_sequences
y = [1, 0, 1]
model = Sequential()
model.add(Embedding(input_dim=10, output_dim=5, mask_zero=True))
model.add(LSTM(output_dim=5, return_sequences=True))
model.add(TemporalPooling())
model.add(Dense(output_dim=1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop')
model.fit(X, y)
Does anything like this exist? I have written what I think it would look like (this is the max pooling case):
from keras.layers.core import MaskedLayer

class TemporalPooling(MaskedLayer):
    def __init__(self):
        super(MaskedLayer, self).__init__()
        self.input = T.tensor3()

    @property
    def output_shape(self):
        # remove temporal dimension
        return (self.input_shape[0], self.input_shape[2])

    def get_output_mask(self, train=False):
        return None

    def get_output(self, train=False):
        data = self.get_input(train)
        mask = self.get_input_mask(train)
        if mask is None:
            mask = T.sum(T.ones_like(data), axis=-1)
        mask = mask.dimshuffle(0, 1, "x")
        masked_data = T.switch(T.eq(mask, 0), -np.inf, data)

        return masked_data.max(axis=1)
1