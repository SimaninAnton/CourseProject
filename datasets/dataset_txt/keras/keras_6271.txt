nicholaslocascio commented on 13 Dec 2015
Currently RNN's with return_sequences=True and TimeDistributedDense take the greedy decision for each step in the sequence path. During prediction, this is usually not optimal as the best path may not be the greedy path. Beam search is often used to improve the accuracy of sequence generating recurrent neural networks. See http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf for reference.
I'm interested in implementing a Beam Search mechanism/option for the predict method (or to make a new method) for these sequence-generating models. Checking in to see if there's been work around this and if anyone can suggest good general starting places.
A rough outline is as follows (if we're predicting character sequences): Predict the probability distribution for the first predicted character slot, predict the probabilities of the 2nd character slot for each of all of the first character possibilities, calculate the total path probabilities by summing each's log probabilities, then prune down to the top K total paths + probabilities. If an end character is predicted, add the solution + probability to an answers table. Then predict the probabilities of the 3rd character slot using the paths of length 2 as starting points. And so on. Continue this process till the max possible length is reached. Take the argmax of the answers table and return it.
The api could be model.predict(x, 'beam_search'=True) or model.predict_beam_search(x). The one uncertainty I have is how to know which layer to grab the probabilities from. Maybe the user has to specify which layer?
Would love some thoughts + discussion!
13