pratyush-kumar-sinha commented on 9 Mar 2016
`from nltk.corpus import brown
from future import print_function
from nltk.tokenize import word_tokenize
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout, Flatten
from keras.layers.recurrent import LSTM
from keras.datasets.data_utils import get_file
import numpy as np
import random
import sys
import os
import h5py
directory ='/home/sinah/'
def create_dic():
vocab={}
for item in brown.words(categories='news'):
try:
vocab[str(item)] = vocab[str(item)] +1
except:
vocab[str(item)] = 1
key_list= sorted(vocab, key=vocab.get,reverse=True)
print('Number of words in text..', len(key_list))
vocab_object=open(directory + 'vocab.txt','w')
for things in key_list:
vocab_object.write(things.lower())
vocab_object.write('\n')
vocab_object.close()
print('Dictionary created for all the words in news category of brown corpus and stored in vocab.txt..')
Create a dictionary and store it
create_dic()
Create the words to index and vice versa indixes
vocabsize=10
maxlen = 100
print('Reading dictionary to create word to index and index to word indices...')
words=[]
text_open=open(directory + 'vocab.txt','r')
for item in text_open:
words.append(item.strip('\n'))
word_indices = dict((w, i) for i, w in enumerate(words))
indices_word = dict((i, w) for i, w in enumerate(words))
build the model: 2 stacked LSTM
print('Building model...')
model = Sequential()
model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, vocabsize)))
model.add(Activation('sigmoid'))
model.add(Dropout(0.2))
model.add(LSTM(512, return_sequences=True))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(vocabsize))
model.add(Activation('softmax'))
Create the sentences from the corpus
sentences = []
for sentence in brown.sents(categories = 'news'):
sentences.append(sentence)
Train
print('Vectorization of the news category in Brown corpus...')
X = np.zeros((len(sentences), maxlen, vocabsize+1), dtype=np.bool)
y = np.zeros((len(sentences), maxlen, vocabsize+1), dtype=np.bool)
for sentence in brown.sents(categories = 'news'):
for i, sentence in enumerate(sentences):
for t, wor in enumerate(sentence):
if t < min(len(sentence) -1,maxlen-1):
try:
X[i, t, word_indices[wor.lower()]] = 1
y[i, t+1, word_indices[wor.lower()]] = 1
except:# for words those are outside the vocabsize
X[i, t, vocabsize] = 1
y[i, t+1, vocabsize] = 1
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(X, y, batch_size=128, nb_epoch=15)
score, acc = model.evaluate(X, y,batch_size=32,show_accuracy=True)
print('Test score:', score)
print('Test accuracy:', acc)
model.save_weights(directory + 'model_weight.hdf5',overwrite=True)`