Contributor
jpeg729 commented on 19 Jun 2017
The standard implementation of LSTM and GRU initialises the biases to zero.
The referenced literature Supervised sequence labeling with recurrent neural networks states that forget gate biases should be set to something positive and input gate biases should be set to something negative. However Keras doesn't provide an easy way to do it.
The rationale is that the network shouldn't forget past information until it has learnt to forget it, and that it shouldn't bring in new info unless it has learnt that it is good to add new info to its internal state. While that seems sensible I have little idea whether it is useful in practice.
The simplest solution would be to add default bias initializers to LSTM and GRU that set the forget biases to 1, the other gate biases to -1, and any other biases to 0?
If the idea seems good to you, I'll prepare a pull request.
5