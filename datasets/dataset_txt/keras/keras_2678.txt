liujxing commented on 11 Apr 2017
I used the default value of epsilon = 0.001 for my BatchNormalization layers, but the epsilon of all such layers become 0.00001 after training. Is epsilon a trainable parameter here?