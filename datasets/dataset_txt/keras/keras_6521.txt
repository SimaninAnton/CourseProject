Contributor
EderSantana commented on 23 Oct 2015
As we approach have passed the 3000 stars and attract several users more and more edge cases are being added to the main repo. As this is awesome to make the library grow fast and help as much people as possible, it makes me wonder if we can continue to scale at that speed.
To make sure Keras can continue to grow fast and helping as much as possible without breaking things, I'd like to propose some functional options, the first two things I thought about was a LambdaLayer and LambdaDataset. I'm already using LambdaLayer in Seya pretty successfully and it looks like this:
class Lambda(MaskedLayer):
    def __init__(self, func, output_shape, ndim=2):
        super(Lambda, self).__init__()
        self.input = ndim_tensor(ndim)
        self.func = func
        self._output_shape = output_shape

    def get_output(self, train=False):
        X = self.get_input(train)
        return self.func(X)

    @property
    def output_shape(self):
        return self._output_shape
As an example of how powerful this is, we can bootstrap all Merge modes with a combination of merge_mode="join" and a LambdaLayer. For example, say we want to merge things with a kernel projection:
def kproj(x):
    # x[0] is layer1
    # x[1] is layer2
    return (T.dot(x[0], x[1].T) + 1)**3

model.add_node(Lambda(kproj, output_shape=(None, out_dim)), inputs=['layer1', 'layer2'], merge_mode='join')
We did it pretty quickly and nothing new had to be added to the main repo, although now we are merging layer1 and layer2 in a huge dimensional space and making Jordan happy.
The same flexibility could be added to datasets and I'm sure you guys have ideas of how to do that elsewhere. This is the same philosophy as the one behind our beloved Callbacks. Let me know what you think.