harewei commented on 13 Jul 2018 â€¢
edited
I'm trying to create a stacked LSTM network by using recursive methods. E.g., the first LSTM computes the result, I modify this output and assign it as the input to the second LSTM, and the output of it will feedback to the first LSTM, and repeat. However, by assigning the input again, it seems to wipe away the previous model information. Here's a much simpler model which displays the same behavior as what I've described.
x = 5
input = Input(shape=(1, 10))
for i in range (x):
    layer = Dense(10)(input)
    input = Input(tensor=layer)
out = Dense(10)(input)
model = Model(input=input, output=out)
model.summary()
return model
The output model summary is
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         (None, 1, 10)             0         
_________________________________________________________________
dense_6 (Dense)              (None, 1, 10)             110       
=================================================================
Total params: 110
Trainable params: 110
Non-trainable params: 0
The network has indeed looped, which you can tell by the layer name is input_6 instead of input_1, but what happened to all the layers 1~5? Have they just disappeared completely?