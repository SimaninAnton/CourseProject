Wenbo93 commented on 29 Mar 2017 â€¢
edited
I met some strange things these days. I use Keras2.0 and tensorflow backend in win10 to train a CNN+LSTM network for video analysis. I use cuda and cudnn with TitanX. I choose the build-in InceptionV3 for CNN and set it to untrainable. Then I add some stacks of LSTM layers and a Dense layer on top of that. I use the default initializer.
After 50 epochs, the train_acc only reached around 0.10 and the worse thing was it collapsed when trying to save the model. The error is Exceeding Python Recursion Limit. I raised sys.setrecursionlimit to 10k and reran the training from scratch because I didn't use checkpoints to save model during the training and used only model.save() after model.fit_generator().
But the next morning, I could not waken the monitor. The computer was on but did not respond. I had to reboot the computer. I did not know what happened. In order to avoid not being able to save the model after a day's training, I tried model.save() immediately after compiling the model and found that even with sys.setrecursionlimit raised to 10k, it still collapsed and threw the same error. So I switched to model.save_weights() and it turned out to work. So I placed model.save_weights() after model.fit_generator() and restarted the training from scratch again. I know I should add checkpoints to save during training but I was stupid enough to forget about that.
It seemed all normal at the beginning: the train_loss started from about 4.6150 and the train_acc started from about 0.0098, which was almost the same as my first training. But here came the strange things. The train_loss decreased a lot more quickly than my first training and the train_acc improved more quickly as well. So did the val_loss and the val_acc, which stayed around the same with train_loss and train_acc, respectively. The train_acc and val_acc reached about 0.20 after the first 15 epochs.
I was very happy yet curious about this. I did not know why. What I changed was only the saving options and I did not touch the network itself. What caused this acceleration? It didn't seem like Keras resumed the training from where it collapsed because the train_loss and acc started from very initial values. And between these two training processes, I initialized the model several times to test how to save it. So I simply ascribed it to lucky initialization of the second training.
Just when I felt very lucky to have this quick convergence, the misfortune fell on me again: I could not waken the monitor again this morning! I was very unwilling to reboot the computer because the 50 epochs' training was about to finish this noon. I didn't use checkpoints, so I would lose the weights and might never again meet such quick convergence if I reboot the computer. Tried a lot though, I had to reboot the computer. And of course there was no weight file saved. I added checkpoint after every 5 epochs this time and restarted the training from scratch the third time.
The strange things happened again. The only difference is that the loss of both train and val decreased even more quickly than my second training! After only 9 epochs, the acc exceeded 0.35 for both train and val.
I am astonished on this. I did not change anything of the model but observed very different training. Each time the loss and val started from very initial values. Is there anybody else who has met this before? Are there any mechanisms that can explain this?