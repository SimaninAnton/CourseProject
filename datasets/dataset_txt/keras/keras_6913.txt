simonhughes22 commented on 30 Jun 2015
I have the following network configuration:
nb_feature_maps = 32
embedding_size = 64

ngram_filters = [3, 5, 7, 9]
conv_filters = []

for n_gram in ngram_filters:
    sequential = Sequential()
    conv_filters.append(sequential)

    sequential.add(Embedding(max_features, embedding_size))
    sequential.add(Reshape(1, maxlen, embedding_size))
    sequential.add(Convolution2D(nb_feature_maps, 1, n_gram, embedding_size))
    sequential.add(Activation("relu"))
    sequential.add(MaxPooling2D(poolsize=(maxlen - n_gram + 1, 1))) #collapses to nb_feature_maps * 1
    sequential.add(Flatten())
    sequential.add(Dense(nb_feature_maps, 32))

model = Sequential()
model.add(Merge(conv_filters, mode='concat'))
model.add(Dense(128, 1)) # len(ngram_filters) * 32
model.add(Activation("sigmoid"))

model.compile(loss='binary_crossentropy', optimizer='adam', class_mode="binary")
According to your documentation and tests, the output dimensionality of the merge + concat layer should be 128, 4 * 32, but this causes the following error when compiling:
File "/Users/simon.hughes/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/theano/compile/function_module.py", line 595, in call
outputs = self.fn()
ValueError: dimension mismatch in args to gemm (16,32)x(128,1)->(16,1)
Apply node that caused the error: GpuDot22(GpuJoin.0, <CudaNdarrayType(float32, matrix)>)
Inputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]
Inputs shapes: [(16, 32), (128, 1)]
Inputs strides: [(32, 1), (1, 0)]
Inputs values: ['not shown', 'not shown']
Please help! I can get this working with mode='sum', but that doesn't seem to do very well on my problem. I think concat would do much better.