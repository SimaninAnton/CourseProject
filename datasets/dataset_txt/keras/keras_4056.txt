slegall56 commented on 27 Oct 2016
I'm creating a custom loss function with an additionnal parameter as below :
#My custom loss function
def customLoss_decorator(addParam):
    def customLoss(y_true, y_pred):
        loss =  K.mean(K.square(y_pred - C), axis=-1)
        return loss

    customLoss.C = addParam
    return customLoss

#Creation of a simple network
input = Input(shape=(100, 100, 3))
....
f = Flatten()(#last layer)
fc1 = Dense(200)(f) 
net = Model(input=input, output=f c1)

sgd = SGD(lr=0.01, decay=0.0001, momentum=0.9, nesterov=True)
custom_loss = customLoss_decorator(add_param)

self.net.compile(optimizer=sgd, loss=custom_loss , metrics=['accuracy'])
But after backpropagation, i want to update my additionnal parameters according to y_true.
How can I modify the code of keras to implement this?