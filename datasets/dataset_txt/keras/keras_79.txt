kristofgiber commented on 28 Aug 2019
I cannot find any documentation or example code on how to do dynamic file loading with distribute strategies in tensorflow 2.0 when doing distributed training.
I see many examples using keras flow_from_directory which seems to be very nice because it relies on generator and does scaling etc on the fly so doesn't require loading hundreds of thousands of images at once. But the distribute strategy examples (mirrored strategy etc) all show pre-loading the entire dataset and then using dataset.from_tensor_slices() and loading that to strategy.experimental_distribute_dataset(). This is not feasable with large data that exceeds the memory. I've tried to combine the above dynamic loading method from keras with distributed batching but it looks like there is no trivial way to convert some flow_from_directory output to be compatible with what strategy.experimental_distribute_dataset() expects so these two features are probably not compatible with one another.
Is there some other way to do this with distributed training?
I could probably hand-code dynamic file loading during training but it would be pretty slow and basic compared to what generators and flow offer. I would be very surprised if TF2 didn't include this functionality for distributed training yet.