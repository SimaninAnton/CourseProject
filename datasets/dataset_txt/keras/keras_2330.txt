anthdm commented on 21 May 2017 •
edited
Hi,
I want to implement this blog post in Keras.
It roughly works as follows:
Both the context and the response text are split by words, and each word is embedded into a vector. The word embeddings are initialized with Stanford’s GloVe vectors and are fine-tuned during training (Side note: This is optional and not shown in the picture. I found that initializing the word embeddings with GloVe did not make a big difference in terms of model performance).
Both the embedded context and response are fed into the same Recurrent Neural Network word-by-word. The RNN generates a vector representation that, loosely speaking, captures the “meaning” of the context and response (c and r in the picture). We can choose how large these vectors should be, but let’s say we pick 256 dimensions.
We multiply c with a matrix M to “predict” a response r'. If c is a 256-dimensional vector, then M is a 256×256 dimensional matrix, and the result is another 256-dimensional vector, which we can interpret as a generated response. The matrix M is learned during training.
We measure the similarity of the predicted response r' and the actual response r by taking the dot product of these two vectors. A large dot product means the vectors are similar and that the response should receive a high score. We then apply a sigmoid function to convert that score into a probability. Note that steps 3 and 4 are combined in the figure.
You can read the details further here
any tips to getting started?
This is what is got so far but I'm not sure how to implement the matrix multiply and dot product steps.
question_embedding = Sequential()
question_embedding.add(Embedding(input_dim=len(vocab) + 1, output_dim=64))

answer_embedding = Sequential()
answer_embedding.add(Embedding(input_dim=len(vocab) + 1, output_dim=64))

model = Sequential()
model.add(Merge([question_embedding, answer_embedding], 'concat', concat_axis=-1))
model.add(LSTM(64))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam')
Thanks for the help.