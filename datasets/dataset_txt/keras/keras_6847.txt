Cognitively commented on 17 Jul 2015
I'm running the text-generation LSTM example on an iCore7 machine with 64GB of RAM on Ubuntu, with Python 3.4. No changes to the code except I'm using my own text sample of ~850K characters. One virtual CPU is running the script at 100%, and I have tons of free RAM.
Anyhow, it looks like it'll take ~60K seconds just for Iteration 1 of Epoch 0. Does it sound reasonable or is there a problem on my side?