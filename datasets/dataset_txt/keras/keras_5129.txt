wailoktam commented on 22 May 2016
Hi, I badly need help on this. I suppose it is a problem of the network structure. Below is the output of model.summary:
Layer (type) Output Shape Param # Connected to
dense_1 (Dense) (None, 200) 2000200 dense_input_1[0][0]
reshape_1 (Reshape) (None, 1, 200) 0 dense_1[0][0]
convolution1d_1 (Convolution1D) (None, 1, 10) 6010 reshape_1[0][0]
activation_1 (Activation) (None, 1, 10) 0 convolution1d_1[0][0]
dense_2 (Dense) (None, 200) 2000200 dense_input_2[0][0]
reshape_2 (Reshape) (None, 1, 200) 0 dense_2[0][0]
convolution1d_2 (Convolution1D) (None, 1, 10) 6010 reshape_2[0][0]
activation_2 (Activation) (None, 1, 10) 0 convolution1d_2[0][0]
lambda_1 (Lambda) (None, 1) 0 merge_1[0][0]
Initially, I have a maxpooling 1d layer after each of the two relu layer but now I comment them out. If I put them back, I get a floating point exception after the warning in the title. Now I can complete execution without the maxpooling but the loss is always NAN. I actually need the Maxpooling layer to be there because it is said to be there by some paper that I need to follow during implementation.
A billion thanks in advance.
Please make sure that the boxes below are checked before you submit your issue. Thank you!
[ O ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
[ O ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
[ O ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).