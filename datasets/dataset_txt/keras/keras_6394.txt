stes commented on 24 Nov 2015
I know that there is the AutoEncoder layer, however important concepts such as DBNs + RBMs are not implemented. Is keras intended to be a framework for purely supervised learning methods? If not, I could propose working on integrating RBMs, Denoising Autoencoders with tied weights etc. into keras.
I would propose to implement 'transpose' operations for each layer, such that a layer can be used "in both directions", i.e. in a manner similar to
Dense with (W, bias) -> Dense.T with (W.T, bias)
or
Conv2d with (W, bias) -> Conv2D.T with (flipped(W), bias)
Another option would be to define special layers such as a TiedDense (that's how it is currently called in my code), subclassing the Dense layer class like
class TiedDense(Dense):
    def __init__(self, transposed_layer=None, **kwargs):
        self.transposed_layer = transposed_layer
        # ...

    def get_output(self, train=False):
        X = self.get_input(train)
        output = self.activation(T.dot(X, self.transposed_layer.W.T) + self.b)
        return output
The obvious downside of this solution is that one would have to implement such a class separately for every layer type.
Any opinions on this?