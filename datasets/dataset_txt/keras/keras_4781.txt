ago109 commented on 13 Jul 2016 â€¢
edited
I am using a very recent version of Keras (as of the start of last week). Very simply put, how do I freeze a specific weight in a given (recurrent) layer? Note that I do NOT want to freeze the entire layer, just a specific parameter (such as the input-to-hidden parameter matrix).
I wrote this manually and it works during training (Note that ellipsis "..." implies "stuff goes here" that is being omitted for clarity):
...
model.compile(...)
# move recurrent layer's input-to-hidden parameters to non-trainable
rnn1_W = model.layers[1].W
model.layers[1].trainable_weights = [model.layers[1].U,model.layers[1].b]
model.layers[1].non_trainable_weights = [rnn1_W]
...
Saving the model to disk with the usual save_weights(...) also works. However, when I load this model with the load_weights(...) routine, I get a Theano error:
Traceback (most recent call last):
  .../site-packages/keras/engine/topology.py", line 2402, in load_weights
    K.batch_set_value(weight_value_tuples)
  File ".../site-packages/keras/backend/theano_backend.py", line 515, in batch_set_value
    x.set_value(np.asarray(value, dtype=x.dtype))
  File ".../site-packages/theano/compile/sharedvalue.py", line 127, in set_value
    self.container.value = copy.deepcopy(new_value)
  File ".../site-packages/theano/gof/link.py", line 471, in __set__
    self.storage[0] = self.type.filter(value, **kwargs)
  File ".../site-packages/theano/tensor/type.py", line 178, in filter
    data.shape))
TypeError: ('Wrong number of dimensions: expected 2, got 1 with shape (20,).', 'Container name "simplernn_1_U"')
How do I move a specific matrix (such as "W" in the SimpleRNN layer) into non-trainable so that way it isn't updated during training (and any computation normally spent on computing its gradient isn't performed) but the other parameters are? "W" itself should still be used in the inference process (i.e., to gather statistics normally done in the feedforward operations).
Thank you very much.