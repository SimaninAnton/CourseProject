beamandrew commented on 5 Apr 2016
I have a some what unique structure to the problem I'm trying to solve. Suppose I have the following structure for my labels:
     root
     /   \
    A     B
   / \   / \
  C   D E   F
where the interpretation is {C,D} are members of class A, and {E,F} are members of class B. Given my input matrix X and labels y, I want to predict splits in this tree, so I need to predict Pr(y = A | X) and Pr(y = C | X) and Pr(y=E | X). In reality the situation is more complicated (i.e. sometimes a label is just A or B and information on membership to the child nodes is not available and the actual tree is much bigger), but this setup will be enough to convey the main idea.
I am modeling this using 3 softmax classifiers in Keras. I feed my X matrix through the net to the three softmax outputs, then multiply by the relevant conditional probabilities to recover the probability for each path through the tree. For example, if I want Pr(y = C | X), I multiply Pr(y = C | y = A)*Pr(y = A | X). This is a little bit of abuse of notation, but you should get the drift.
The main issue I'm having is how to ensure the gradient gets back-propogated correctly. For instance, if y = {A,C}, then I do not want to backprop through split for {E,F}. One hack is to multiply by a mask matrix that is all 0s if the split is not involved with the label and thus zeroing out the gradient on the backprop, but that seems like a lot of wasted computation for larger trees.
Is there functionality in Keras that can help with this problem?