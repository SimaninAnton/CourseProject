nikkey2x2 commented on 10 Mar 2016
Hello!
I'm trying to make some sort of lstm autoencoder:
words = [0, '<!END!>']
X_train = np.zeros((len(db), 1, 100), dtype=int)
for i, ln in enumerate(db):
    g = ln.lower().split(' ')
    for iw, word in enumerate(g[:99]):
        try:
            X_train[i][0][iw] = words.index(word)
        except:
            words.append(word)
            X_train[i][0][iw] = words.index(word)
    X_train[i][0][iw+1] = 1
model = Sequential()
model.add(LSTM(256, input_dim=100, return_sequences=True))
model.add(LSTM(100, return_sequences=True, activation='linear'))
model.compile(optimizer='rms', loss='mse')
model.fit(X_train, X_train, nb_epoch=3000, verbose=1, batch_size=128)
My db is just a list of sentences, I take maximum of 99 words, then append 1 to the end of the sequence.
I thought this will do the trick, but according to loss values - network is pretty much confused.
Epoch 1/3000
12149/12149 [==============================] - 4s - loss: 189790438.0818     
Epoch 2/3000
12149/12149 [==============================] - 4s - loss: 189585845.3180     
Epoch 3/3000
12149/12149 [==============================] - 4s - loss: 189437667.9760     
...
Epoch 101/3000
12149/12149 [==============================] - 5s - loss: 177903567.4785     
Epoch 102/3000
12149/12149 [==============================] - 5s - loss: 177798999.0755     
Epoch 103/3000
12149/12149 [==============================] - 5s - loss: 177687385.3571     
So... should I change something? More layers? Apply embedding?
(len(words) = 81281)
P.S.: print(X_train[:5])
[[[ 2  3  4  5  6  7  8  9 10 11 12  1  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0]]

 [[13 14 15  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0]]

 [[16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  1  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0]]

 [[16 36 37 38 39 40  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0]]

 [[41 42 43 44 18 45 46 33 47 48 49 16 17 50 51 52 53 54 33 55 19 56 57 37
   58 59 60 61 62 63 64 65 66 61 67 68 69 64 70 71 72 17 73 71 74 75 76 77
   78 79 80  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
    0  0  0  0]]]