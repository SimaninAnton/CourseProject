bzhong2 commented on 30 May 2017
Hello,
I rent a AWS EC2 to run inception and vgg16 fine tuning code. When I used a 4g memory GPU that both of them can run individuall and if I want to run both, I faced with a Resource Exhausted Error. I guess it is because the 4g GPU memory was not large enough. However, after I turned to a 12g GPU, I still faced with the problem when I try to run two programs at the same time but I start them sequentially.
Before running the second program, it says : name: Tesla K80 major: 3 minor: 7 memoryClockRate (GHz) 0.8235 pciBusID 0000:00:1e.0 Total memory: 11.17GiB Free memory: 437.38MiB. Since I can run any of these two program on a 4g GPU, I assume that there probably exist ways to limit the memory use of the first program. But I am not sure how to do it in keras and whether it is not the case at all. Any suggestion is appreciated! I am using an Ubuntu OS with python 3.5, keras 2.0, tensorflow 1.1.
The error print looks like this:
"ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,3,224,224]
[[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format="NHWC", padding="VALID", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/gpu:0"(_recv_input_1_0/_1707, conv2d_1/kernel/read)]][[Node: mul_2/_2103 = _Recvclient_terminated=false, recv_device="/job:localhost/replica:0/task:0/cpu:0", send_device="/job:localhost/replica:0/task:0/gpu:0", send_device_incarnation=1, tensor_name="edge_4130_mul_2", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/cpu:0"]]"