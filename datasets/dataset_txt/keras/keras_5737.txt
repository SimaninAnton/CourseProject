adityamj commented on 17 Mar 2016
Hi,
The traditional LSTM network has input_dim = output_dim as shown in Cris Olah's blog.
According to keras documentation, the input accepted by recurrent networks is a 3D Tensor (numsamples, timesteps, inputdim) and the output is a 2D Tensor (numsamples, outputdim) when return_sequences is false.
Keras does not require the number of input dimensions to be equal to the number of output dimensions. From what I understand, please correct me if I am wrong, the parameter output_dim dictates the number of hidden nodes that are created in a layer.
Consider an example where the LSTM is used to predict next word in an incomplete sentence.
Suppose N is the number of words that are present and I have to predict N+1th word. The words are encoded using some form of word-vectors with D dimensions.
In this case, would N be the number of time steps and D the input dimension?
If I use output_dim = D. I will get the desired output, the N+1th word.
But if there are H hidden LSTM units, shouldn't the output from these H units be HxD as each hidden unit will be outputting one vector of D dimensions.
How does keras handle this internally?
Thanks