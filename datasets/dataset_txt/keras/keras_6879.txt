Contributor
mmmikael commented on 7 Jul 2015
I am trying to convert the mnist cnn example to a "fully convolutional" net in order to apply it efficiently across an image (like a sliding window). I did the following changes:
    # model.add(Flatten())
    # model.add(Dense(32*196, 128))
    ## replaced by:
    model.add(Convolution2D(128, 32, 14, 14))
and
    # model.add(Dense(128, nb_classes))
    ## replaced by:
    model.add(Convolution2D(nb_classes, 128, 1, 1))
    model.add(Flatten())
The training step goes well but when testing on input images different than 28x28 I don't get the expected results. For example, here I created an image slightly bigger to have two classifications:
data = np.zeros([1,1,28, 30])
data[0,0,:,:28] = X_train[0,0,:,:]

print('label: %d' % y_train[0])
print(model.predict(data[:,:,:,:28]))
print(model.predict(data))            
The output is something like:
label: 5
[[ 0.     0.     0.     0.046  0.     0.954  0.     0.     0.     0.   ]]
[[ 0.     0.     0.     0.     0.     0.     0.036  0.001  0.     0.     0.756
   0.207  0.     0.     0.     0.     0.     0.     0.     0.   ]]
I would expect to see the numbers of the first output in the second one as well since they should correspond to the same data.
Any idea what I am doing wrong?
Here is a gist of the complete script:
https://gist.github.com/mmmikael/cc895249e7941c2f56b3