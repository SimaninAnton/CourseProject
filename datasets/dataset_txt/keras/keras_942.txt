chatrapathik commented on 22 Jun 2018
I have a large dataset. I am not able to load whole data into the RAM. So I started using fit_generator to train model.
So I wrote a custom generator function, then I started training the model. I trained model for 100 epochs.
The loss values are very strange here.
Sometimes the loss value is not yet all changed or else for the first few epochs the loss value is reducing after some point loss values keep on increases.
When I use the same model architecture and same dataset with the model.fit() the loss values are reducing.
Can someone help me to find out what's the issue here? Thanks in advance.
Here is my entire code
This is my generator function:
    def generator( max_len, nchars, nwords, words, charmap, b_size):
        while 1:
            char_none = to_categorical(charmap[self.CHAR_NONE], num_classes=nchars)
            num_batches = (nwords//b_size) + 1

            for i in range(num_batches):
                start = b_size * i
                end = b_size * (i + 1)

                split_words = words[start:end]
                n = len(split_words)

                data = np.zeros(shape=(n, max_len, nchars), dtype=np.float32)
                labels = np.zeros(shape=(n, nchars), dtype=np.float32)

                for i in range(n):
                    w = split_words[i][:-1]
                    last_char = split_words[i][-1]
                    w = '%s%s%s' % (self.CHAR_START, w, self.CHAR_END)
                    w = [to_categorical(charmap[x], num_classes=nchars) for x in w]
                    w = w + ([char_none] * (max_len - len(w)))
                    data[i] = w
                    labels[i] = to_categorical(charmap[last_char], num_classes=nchars

                yield data, labels
Here is my model
def create_model(num_units, word_len, num_unique_chars):
    input_shape = (word_len, num_unique_chars)

    model = Sequential()
    model.add(LSTM(num_units, input_shape=input_shape, unroll=True))
    model.add(Dense(num_unique_chars, activation='sigmoid'))

    model.compile(optimizer=optimizers.Adam(lr=0.1),
                    loss='categorical_crossentropy',
                    metrics=['mse', 'acc'])
    return model
How I am training the model
generator = generator(max_len, nchars, nwords, words, charmap, 2048)
model.fit_generator(generator, steps_per_epoch= nwords/2048, epochs=epochs, shuffle=False)
The bellow logs are example for constant loss value.
423/423 [==============================] - 137s 325ms/step - loss: 1.8152 - mean_squared_error: 0.0049 - acc: 0.6175
Epoch 2/100
423/423 [==============================] - 129s 304ms/step - loss: 1.9417 - mean_squared_error: 0.0051 - acc: 0.5940
Epoch 3/100
423/423 [==============================] - 128s 303ms/step - loss: 1.9391 - mean_squared_error: 0.0051 - acc: 0.5968
Epoch 4/100
423/423 [==============================] - 128s 303ms/step - loss: 1.9169 - mean_squared_error: 0.0051 - acc: 0.5965
Epoch 5/100
423/423 [==============================] - 128s 303ms/step - loss: 1.9513 - mean_squared_error: 0.0051 - acc: 0.5956
Epoch 6/100
423/423 [==============================] - 128s 303ms/step - loss: 1.9201 - mean_squared_error: 0.0051 - acc: 0.6005
Epoch 7/100
423/423 [==============================] - 128s 303ms/step - loss: 1.9341 - mean_squared_error: 0.0051 - acc: 0.5987
Epoch 8/100
423/423 [==============================] - 128s 303ms/step - loss: 1.9139 - mean_squared_error: 0.0051 - acc: 0.5964
Epoch 9/100
423/423 [==============================] - 128s 304ms/step - loss: 1.9607 - mean_squared_error: 0.0051 - acc: 0.5974
Epoch 10/100
423/423 [==============================] - 128s 304ms/step - loss: 1.9373 - mean_squared_error: 0.0051 - acc: 0.6010
The below logs are examples for the increasing of loss value.
Epoch 1/100
423/423 [==============================] - 192s 454ms/step - loss: 1.8386 - mean_squared_error: 0.0049 - acc: 0.6136
Epoch 2/100
423/423 [==============================] - 186s 439ms/step - loss: 1.8087 - mean_squared_error: 0.0043 - acc: 0.6201
Epoch 3/100
423/423 [==============================] - 184s 436ms/step - loss: 1.3863 - mean_squared_error: 0.0037 - acc: 0.6445
Epoch 4/100
423/423 [==============================] - 185s 438ms/step - loss: 1.1163 - mean_squared_error: 0.0032 - acc: 0.6856
Epoch 5/100
423/423 [==============================] - 186s 439ms/step - loss: 1.0246 - mean_squared_error: 0.0030 - acc: 0.7058
Epoch 6/100
423/423 [==============================] - 166s 392ms/step - loss: 1.0277 - mean_squared_error: 0.0030 - acc: 0.7130
Epoch 7/100
423/423 [==============================] - 186s 441ms/step - loss: 0.9387 - mean_squared_error: 0.0028 - acc: 0.7244
Epoch 8/100
423/423 [==============================] - 187s 443ms/step - loss: 0.9164 - mean_squared_error: 0.0028 - acc: 0.7305
Epoch 9/100
423/423 [==============================] - 186s 439ms/step - loss: 0.9023 - mean_squared_error: 0.0027 - acc: 0.7345
Epoch 10/100
423/423 [==============================] - 188s 444ms/step - loss: 0.9081 - mean_squared_error: 0.0027 - acc: 0.7349
Epoch 11/100
423/423 [==============================] - 188s 444ms/step - loss: 0.9617 - mean_squared_error: 0.0028 - acc: 0.7297
Epoch 12/100
423/423 [==============================] - 188s 443ms/step - loss: 0.8798 - mean_squared_error: 0.0027 - acc: 0.7395
Epoch 13/100
423/423 [==============================] - 188s 444ms/step - loss: 0.8804 - mean_squared_error: 0.0027 - acc: 0.7400
Epoch 14/100
423/423 [==============================] - 188s 444ms/step - loss: 3.0268 - mean_squared_error: 0.0048 - acc: 0.5968
Epoch 15/100
423/423 [==============================] - 188s 444ms/step - loss: 2.1736 - mean_squared_error: 0.0043 - acc: 0.6356
Epoch 16/100
423/423 [==============================] - 188s 444ms/step - loss: 0.9362 - mean_squared_error: 0.0029 - acc: 0.7257
Epoch 17/100
423/423 [==============================] - 188s 444ms/step - loss: 0.8729 - mean_squared_error: 0.0027 - acc: 0.7413
Epoch 18/100
423/423 [==============================] - 188s 444ms/step - loss: 0.8695 - mean_squared_error: 0.0027 - acc: 0.7420
Epoch 19/100
423/423 [==============================] - 188s 444ms/step - loss: 0.9623 - mean_squared_error: 0.0028 - acc: 0.7312
Epoch 20/100
423/423 [==============================] - 187s 442ms/step - loss: 0.8934 - mean_squared_error: 0.0027 - acc: 0.7398
Epoch 21/100
423/423 [==============================] - 188s 444ms/step - loss: 0.8649 - mean_squared_error: 0.0027 - acc: 0.7429
Epoch 26/100
423/423 [==============================] - 188s 444ms/step - loss: 1.1365 - mean_squared_error: 0.0032 - acc: 0.7036
Epoch 27/100
423/423 [==============================] - 188s 444ms/step - loss: 3.8524 - mean_squared_error: 0.0065 - acc: 0.4976
Epoch 28/100
423/423 [==============================] - 188s 444ms/step - loss: 2.6476 - mean_squared_error: 0.0057 - acc: 0.5406
Epoch 29/100
423/423 [==============================] - 186s 440ms/step - loss: 1.5818 - mean_squared_error: 0.0044 - acc: 0.6141
Epoch 30/100
423/423 [==============================] - 187s 443ms/step - loss: 2.4326 - mean_squared_error: 0.0056 - acc: 0.5285
Epoch 31/100
423/423 [==============================] - 188s 444ms/step - loss: 3.3618 - mean_squared_error: 0.0065 - acc: 0.4919
Epoch 32/100
423/423 [==============================] - 188s 444ms/step - loss: 4.7409 - mean_squared_error: 0.0072 - acc: 0.4452
Epoch 33/100
423/423 [==============================] - 186s 439ms/step - loss: 5.4348 - mean_squared_error: 0.0080 - acc: 0.4243  
Epoch 34/100
423/423 [==============================] - 188s 444ms/step - loss: 6.1750 - mean_squared_error: 0.0084 - acc: 0.4448
Epoch 35/100
423/423 [==============================] - 204s 482ms/step - loss: 6.2113 - mean_squared_error: 0.0084 - acc: 0.4343
Epoch 36/100
423/423 [==============================] - 188s 444ms/step - loss: 7.6153 - mean_squared_error: 0.0092 - acc: 0.4181