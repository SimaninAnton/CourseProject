nidleo commented on 18 Feb 2016
The following Theano tutorial recommends storing the dataset as shared variables and indexing into it to form minibatches.
http://deeplearning.net/tutorial/gettingstarted.html (see shared_dataset function)
This avoids the overhead of copying a each minibatch into GPU.
From my experience, this overhead can cause x2 slowdown even for reasonable batch size (e.g. 128).
It appears Keras does not follow this recommendation. Is there a reason not to do so?