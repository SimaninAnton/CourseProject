StuartFarmer commented on 8 Nov 2016
I'm attempting to write a binary tree evaluation layer based on genetic programming where each node on an internal tree structure stores weights which describe the probability that an operator will be selected to create an entire operation to be run on the input tensor.
However, although these weights are directly linked to the layer and can be explored, there's no way for me to hook them into my custom layer without getting an error that the trainable weights cannot have gradient decent performed on them.
I notice that in other custom layers, the weights are directly applied in some way to the input tensor. However, in others, there is some preprocessing that occurs with the weights and the layer still builds.
What are the requirements for trainable weights in custom layers? What are the best practices to developing new and novel layers to explore new ML possibilities?