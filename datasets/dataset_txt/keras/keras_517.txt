hardik124 commented on 20 Dec 2018 â€¢
edited
I am trying to train facenet model for face matching, I used lfw for validation and VGG face for training, I saw my training loss converge while validation loss does not, I tried predicting embeddings and I get same embedding for all Anchor,positive and negative . Then I made my training and validation data same, used same generator and still same issue.
Epoch 1/1500
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: RuntimeWarning: invalid value encountered in less
500/500 [==============================] - 213s 427ms/step - loss: 270.2950 - val_loss: 0.2000
Epoch 2/1500
500/500 [==============================] - 152s 304ms/step - loss: 3.5313 - val_loss: 0.2000
Epoch 3/1500
500/500 [==============================] - 152s 305ms/step - loss: 2519.9942 - val_loss: 0.2000
Epoch 4/1500
500/500 [==============================] - 151s 301ms/step - loss: 24.8725 - val_loss: 0.2000
Epoch 5/1500
500/500 [==============================] - 151s 301ms/step - loss: 104.2208 - val_loss: 0.2000
Epoch 6/1500
500/500 [==============================] - 150s 301ms/step - loss: 10.0355 - val_loss: 0.2000
Epoch 7/1500
500/500 [==============================] - 150s 301ms/step - loss: 24.4225 - val_loss: 0.2000
Epoch 8/1500
500/500 [==============================] - 150s 301ms/step - loss: 238.7915 - val_loss: 0.2000
.
.
.
.
Epoch 367/1500
500/500 [==============================] - 151s 302ms/step - loss: 3.3887e-04 - val_loss: 0.2000
Epoch 368/1500
500/500 [==============================] - 151s 302ms/step - loss: 4.2150e-04 - val_loss: 0.2000
Epoch 369/1500
65/500 [==>...........................] - ETA: 2:09 - loss: 5.9903e-04
EDIT :
I was inspecting each layer.
M2 = Model(base_network.inputs[0],base_network.layers[2].output)
y = M2.predict(a[0].reshape((1,256,256,3)))
x = M2.predict(p[0].reshape((1,256,256,3)))
print(base_network.layers[2].name)
print(x-y)
batch_normalization_1
[[[[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
...
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]]
[[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
...
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]]
[[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
...
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]
[nan nan nan ... nan nan nan]]
...
This is what I get.