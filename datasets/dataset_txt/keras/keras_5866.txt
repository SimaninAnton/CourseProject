bkj commented on 28 Feb 2016
I'm training a text classification model (bidirectional LSTM) and it seems like there is a reasonable large amount of overfitting going on in the first epoch. Does anyone know why this may be the case? I would understand overfitting after multiple epochs, but am more confused when it happens during the first.
# Model definition
model = Graph()

model.add_input(name='input', input_shape=(maxlen,), dtype=int)
model.add_node(Embedding(max_features, 256, input_length=maxlen),
    name='embedding', input='input')
model.add_node(LSTM(64, dropout_W = 0.5, dropout_U = 0.1), 
    name='forward', input='embedding')
model.add_node(LSTM(64, dropout_W = 0.5, dropout_U = 0.1, go_backwards=True),
    name='backward', input='embedding')
model.add_node(Dropout(0.5), 
    name='dropout', inputs=['forward', 'backward'])

model.add_node(Dense(1, activation='sigmoid'), 
    name   = 'sigmoid', 
    input    = 'dropout'
)

model.add_output(name='output', input='sigmoid')
model.compile('adam', {'output': 'binary_crossentropy'})

# Training loss: 0.53
# Training accuracy: 0.72
# Test loss: 0.70
# Test accuracy : 0.60
I should mention that each input observation is a social media post, and I'm trying to predict something about the author -- so there are multiple inputs per author in the training dataset, which may have something to do with this. (Any ideas on how to learn on multiple posts from the same author simultaneously rather than feeding them in separately would also be much appreciated...)
Thanks
Ben