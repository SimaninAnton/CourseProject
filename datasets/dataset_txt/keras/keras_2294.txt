rjpg commented on 25 May 2017 â€¢
edited
hello,
I have been using sklearn but I want to build a classifier using stacked autoencoders to compare the results with my already implemented "classical" deep classifier (3 deep layers with Relu and output layer with sofhtmax...). sklearn does not have much support with autoencoders ...
I saw this https://blog.keras.io/building-autoencoders-in-keras.html link and is well explained.
But something critical is missing: the classification part. They explain how to encode/decode but how to introduce classification into that methodology is not presented.
As far as I know, to use classification with autoencoders we must :
1- pre-train the autoencoder NN - unsupervised (input is the output)
2- slice the autoencoder NN in half on the last encoder layer (before the decode starts - higher abstraction layer )
3- freeze the the weights of the encoders (so pos train does not mess them )
4- add/concat a new NN in front of the last encoding layer (I think a pipeline would be applied here..)
5- (pos) train this concat NN with a softmax output layer for classification ...
How could we do this with keras ? (using "high code" so it can be readable :-) )
One example with MNIST digits (real classification not just encode/decode) would be TOP.
I believe using high abstraction info , extracted after encoding raw inputs, to do classification will improve results (mainly in problems with lots of inputs).
Thanks in advance.