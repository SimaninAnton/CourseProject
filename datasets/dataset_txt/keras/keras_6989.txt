Contributor
tdhd commented on 31 May 2015
Recently I work with a relatively strongly imbalanced classification problem and was wondering if you had any plans on addressing the problem of imbalanced classes during model fitting.
As a motivation: Wouldn't it be great to have, for example, a parameter class_weights for the fit method, similar to some fit methods of scikit-learn models which can be auto determined on class frequency or manually specified? :)
Two methods I can think of at the moment:
Resampling with replacement given some class-wise probabilities during gradient descent.
Implementing SMOTE http://arxiv.org/abs/1106.1813 and adding that as an option to the optimizers. The approach consists of calculating the k nearest neighbours for each example and creating synthetic ones based on a linear combination of the original example and its k nearest neighbours.
There is a lot of literature on the topic of imbalanced classes so these two approaches might not be the best candidates. The resampling method should be relatively simple to implement though.
I would like to contribute to this problem if you can imagine to have that for keras, although I am not too familiar with theano (but that can change :)).
What do you think?