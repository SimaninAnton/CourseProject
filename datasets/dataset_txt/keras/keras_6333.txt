lireagan commented on 3 Dec 2015
model = Sequential()
model.add(Embedding(max_features, 128, input_length=maxlen))
model.add(LSTM(128)) # try using a GRU instead, for fun
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
optimizer='adam',
class_mode="binary")
Hey, guys. I've try the code examples/imdb_lstm.py mentioned above. But I don't understand the layer structures between the LSTM layer and activation layer.
We know for a text sequential input we will get a variable length output in LSTM layer, here we get a 128 * nb_words matrix as I see, and the nb_words must be variable because different length texts.
Then the model add a full connection layer with output size of one. HERE is the question confuses me: IF (1) there are nb_words full-connection neurons with 128 input and 1 output, (2) OR one neuron with 128*nb_words input and 1 output ??? IF(2), I think it's impossifble to learn a full-connection layer with variable weight size. IF (1), the same question will happend for activation layer, then how to get a final 1 dim output of the total network?
So, I'm confused about how to connect a length variable layer(LSTM) with length-fixed layer(full-connection )? Thank you !