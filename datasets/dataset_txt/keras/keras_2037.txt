loztop commented on 29 Jun 2017 â€¢
edited
Hi,
Thanks for the great Keras project. I just wanted to report a small problem I have come across.
When training using multiple gpus, setup using https://github.com/kuza55/keras-extras/blob/master/utils/multi_gpu.py , the weight decay part (for example l2 weight decay) of the total loss calculation is added multiple times for each gpu batch split.
I.e. The more gpus used to split the batch -> the higher the loss ! I'm guessing somewhere each gpu is separately adding the same regularization loss to the model's total loss.
The summing of all extra (e.g. regularization) losses happens in https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L846-L847
For now my fix is to just divide the weight_decay by the number of gpus used (e.g. 0.0001/16), to ensure that the loss calculation does not depend on the number of gpus.
Is there a better/cleaner solution ?
Thanks