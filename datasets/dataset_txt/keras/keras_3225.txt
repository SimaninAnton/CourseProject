dupsys commented on 13 Feb 2017 â€¢
edited
I want to use seq2seq prediction where the input sequence is "cu 2morr, gf" and the output sequence is "see you tomorrow,
girl friend". In this case the input and output seq lengths are different as shown below:
import matplotlib.pyplot as plt
plt.ion()
abbreviation, meaning = list(zip(*[(key, value) for key, value in {
'ao': 'adult only',
'u': 'you',
'gf': 'girl friend',
'sul': 'see you later',
'tmi': 'too much information',
'amz': 'amazing',
'beg': 'big evil grim',
'bff4l': 'best friend forever for life',
'2mor': 'tomorrow',
'2night': 'tonight',
'2oo': 'too',
'4eva': 'forever',
'abt': 'about',
'': '',
}.iteritems()]))
Define the symbol alphabet, including the padding symbol '#'
letters = '#' +''.join(sorted({c for s in abbreviation+meaning for c in s}))
A map from letters to indexes
letter_index = {c:i for i,c in enumerate(letters)}
Compute the size of our alphabet
n_letters = len(letters)
Compute maximum string length
max_length = max(map(len, abbreviation+meaning))
n_letters, max_length, letters
model construction
word = Input((max_length,))
embedded_word = Embedding(32, n_letters, mask_zero=False)(word)
encoding = LSTM(128)(embedded_word)
repeated_encoding = RepeatVector(max_length)(encoding)
We concatenate the repeated encoding with the embedded input string (in the last dimension).
merged_encoding = merge([repeated_encoding, embedded_word], mode='concat')
Our LSTM decoder will produce a sequence of 128-dimensional vectors.
decoded = LSTM(128, return_sequences=True)(merged_encoding)
output_word = TimeDistributed(Dense(n_letters, activation='softmax'))(decoded)
output_word = TimeDistributed(Dense(n_letters, activation='softmax'))(decoded)
I am interested in making two models, one for abbreviation and the other for meaning. Do i need to add attention LSTM and how will i merge the two together, so that my output could be "see you tomorrow,
girl friend".