khushmeeet commented on 3 Jul 2017
I am running a 1D convolution on text, using the following model in keras
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=256, input_length=100))
model.add(Conv1D(filters=128, kernel_size=3, padding='SAME', activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)))
model.add(Conv1D(filters=128, kernel_size=7, padding='SAME', activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)))
model.add(Conv1D(filters=128, kernel_size=7, padding='SAME', activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)))
model.add(MaxPool1D(pool_size=3))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
My issue is that, the model is not training whatever I do. But the surprising part is that when i try to run the same model, with exact same configuration in tflearn, the models gets accuracy upto 95%.
Why is that so?
Here are my training results.
Running on macOS sierra, python 3.6.0, tensorflow - 1.2.1, keras - 2.0.4 (latest)