sergeyf commented on 26 Oct 2015 â€¢
edited
Howdy,
In published papers I often see that the learning rates are decreased after some hundreds of epochs when learning stalls. What is the best way to do this in Keras? Thus far, I have been recompiling, but (not knowing if there is a better way), that seems foolish.
An example:
First, I build some model and train it.
model = Sequential()
# insert model here
optimizer = adagrad(lr=0.01)
model.compile(optimizer=optimizer)
model.fit(X,y,nb_epoch=50)
UPDATE -- the following works without having to recompile
K.set_value(model.optimizer.lr, 0.001)
model(X,y,nb_epoch=50)
Thank you to @EderSantana for the quick reply.
21
1
4