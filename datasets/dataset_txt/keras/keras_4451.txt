Arsey commented on 3 Sep 2016 â€¢
edited
Following by the example (taken here https://keras.io/applications/#examples) about fine-tune InceptionV3 on a new set of classes, I'm getting NotImplementedError on the second model fitting.
The first step finishes normally. Here's the code to fit:
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=16,
    class_mode='categorical'
)

validation_generator = datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=16,
    class_mode='categorical'
)

# train the model on the new data for a few epochs
history = model.fit_generator(
    train_generator,
    nb_epoch=nb_epoch,
    samples_per_epoch=128,
    validation_data=validation_generator,
    nb_val_samples=1020)
But the second fails. Here's the code:
model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])

train_generator = datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=16,
    class_mode='categorical'
)

validation_generator = datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=16,
    class_mode='categorical'
)

# we train our model again (this time fine-tuning the top 2 inception blocks
# alongside the top Dense layers
history = model.fit_generator(
    train_generator,
    nb_epoch=nb_epoch,
    samples_per_epoch=128,
    validation_data=validation_generator,
    nb_val_samples=1020)
So on the second step as a result I'm getting NotImplementedError. Here's the console output
Found 6149 images belonging to 102 classes.
Found 1020 images belonging to 102 classes.
Epoch 1/1
Traceback (most recent call last):
  File "/Volumes/storage/projects/keras-oxford102/inception-v3.py", line 107, in <module>
    nb_val_samples=1020)
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/keras/engine/training.py", line 1441, in fit_generator
    class_weight=class_weight)
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/keras/engine/training.py", line 1219, in train_on_batch
    outputs = self.train_function(ins)
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/keras/backend/theano_backend.py", line 675, in __call__
    return self.function(*inputs)
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/compile/function_module.py", line 879, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gof/link.py", line 325, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/compile/function_module.py", line 866, in __call__
    self.fn() if output_subset is None else\
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gof/op.py", line 908, in rval
    r = p(n, [x[0] for x in i], o)
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/tensor/signal/pool.py", line 977, in perform
    raise NotImplementedError()
NotImplementedError: 
Apply node that caused the error: AveragePoolGrad{ignore_border=True, mode='average_exc_pad'}(Join.0, IncSubtensor{InplaceInc;::, ::, :int64:, :int64:}.0, TensorConstant{(2,) of 3}, TensorConstant{(2,) of 1}, TensorConstant{(2,) of 1})
Toposort index: 2850
Inputs types: [TensorType(float32, 4D), TensorType(float32, 4D), TensorType(int64, vector), TensorType(int64, vector), TensorType(int64, vector)]
Inputs shapes: [(16, 2048, 8, 8), (16, 2048, 8, 8), (2,), (2,), (2,)]
Inputs strides: [(524288, 256, 32, 4), (524288, 256, 32, 4), (8,), (8,), (8,)]
Inputs values: ['not shown', 'not shown', array([3, 3]), array([1, 1]), array([1, 1])]
Inputs type_num: [11, 11, 7, 7, 7]
Outputs clients: [[Elemwise{Add}[(0, 1)](AveragePoolGrad{ignore_border=True, mode='average_exc_pad'}.0, CorrMM_gradInputs{half, (1, 1), (1, 1)}.0, CorrMM_gradInputs{half, (1, 1), (1, 1)}.0, CorrMM_gradInputs{half, (1, 1), (1, 1)}.0)]]

Backtrace when the node is created(use Theano flag traceback.limit=N to make it longer):
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gradient.py", line 1270, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gradient.py", line 964, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gradient.py", line 1270, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gradient.py", line 964, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gradient.py", line 1270, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gradient.py", line 964, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gradient.py", line 1270, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "/Users/alexanderlazarev/anaconda/envs/vgg-16-fine-tuning-102-flowers/lib/python2.7/site-packages/theano/gradient.py", line 1104, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)
I was trying to skip the first fitting and have got the same error. Full console output see in this gist - https://gist.github.com/Arsey/d7b8061e74cb8db971939fd03f6f8685
1