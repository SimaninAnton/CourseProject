ddofer commented on 11 Oct 2016
I'm trying a character level model, and unless i'm missing something, the text tokenizer doesn't seem to be filtering for only the nb_words most frequent characters. I set it to 21, but the output seems to be higher in dimension. (Ideally, I would use the raw OHE, but I've been having trouble getting keras to work with classification on character level text sequences if I don't use an embedding layer).
`tk = text.Tokenizer(nb_words=21, lower=True,char_level=True)
tk.fit_on_texts(all_seqs)
X = tk.texts_to_sequences(all_seqs)
X = sequence.pad_sequences(X, maxlen=maxlen)
print(X.shape)`
(67982L, 21L)
print(tk.word_counts)
{'A': 90956,
'B': 11,
'C': 38222,
'D': 75005,
'E': 87134,
'F': 59260,
'G': 96993,
'H': 34411,
'I': 70190,
'K': 72485,
'L': 129784,
'M': 26433,
'N': 66470,
'P': 81383,
'Q': 59701,
'R': 72479,
'S': 112304,
'T': 90323,
'U': 12,
'V': 94493,
'W': 21867,
'X': 19,
'Y': 47682,
'Z': 5}
len(tk.word_counts)
24