kaushalshetty commented on 17 Aug 2017
Hi all,
I have built a LSTM model that spits out the next word given a sequence of words . But the network is spitting out only the word 'the' as the next predicted word. After that i checked out frequency distribution of the labels where I found out that stopwords were constituting a large number. I can remove the stopwords but if i do so then the final generated results would not make any sense with the absence of stopwords. This is like a classical skewed data problem and how do i get about this ? Would random sampling from the highest probabilities make any sense ? Please help.