ajwootto commented on 1 Dec 2016
Hi, I'm having trouble understanding what format I need for my training data and why, using the following network architecture:
model = Sequential()
num_vals = 1617
input_length = 5
model.add(Embedding(num_vals + 1, 50, mask_zero=True, batch_input_shape=(1, input_length)))
model.add(BatchNormalization())
model.add(LSTM(100, stateful=True, return_sequences=True, dropout_W=0.2, dropout_U=0.2))
model.add(LSTM(50, return_sequences=True,  stateful=True, dropout_W=0.2, dropout_U=0.2))
model.add(TimeDistributed(Dense(num_vals + 1, activation='softmax')))

adam = Adam(lr=0.0001)
model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
I'm feeding the network X data of the form [[5, 10, 12, 3, 5]], so basically a single batch of 5 datapoints in a series. My target data is the above shifted one time step, so [[10, 12, 3, 5, 8]] for example. However, the network complains that the output shape must be (1, 5, 1) and not (1,5), so apparently it wants it in the form [[[10], [12], [3], [5], [8]]]. What I don't understand is, why? The output of the network when making predictions has shape (1, 5, 1618) but the training data has to be (1, 5, 1)? Is there some kind of internal conversion happening here due to the embedding layer?
My ultimate goal here is just to learn the time-shifted sequence of 5 datapoints, but I can't figure out what the network is actually doing here.