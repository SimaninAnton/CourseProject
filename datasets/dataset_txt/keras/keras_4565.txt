Member
farizrahman4u commented on 18 Aug 2016 â€¢
edited
Script for reproducing the bug:
from keras.layers import LSTM
from keras.models import Sequential
import numpy as np

model = Sequential()
model.add(LSTM(1, input_shape=(3, 2)))
model.compile(loss='mse', optimizer='sgd')

x = np.random.random((4, 3, 2))
y = np.random.random((4, 1))

model.fit(x, y)
Error message :
PS C:\Users\Fariz\Desktop> python keras_bug.py
Using Theano backend.
Using gpu device 0: GeForce GTX 980M (CNMeM is enabled with initial size: 50.0% of memory, cuDNN 5103)
Traceback (most recent call last):
  File "keras_bug.py", line 12, in <module>
    model.fit(x, y)
  File "C:\Python27\lib\site-packages\keras-1.0.7-py2.7.egg\keras\models.py", line 613, in fit
    sample_weight=sample_weight)
  File "C:\Python27\lib\site-packages\keras-1.0.7-py2.7.egg\keras\engine\training.py", line 1080, in fit
    self._make_train_function()
  File "C:\Python27\lib\site-packages\keras-1.0.7-py2.7.egg\keras\engine\training.py", line 697, in _make_train_functio

    training_updates = self.optimizer.get_updates(trainable_weights, self.constraints, self.total_loss)
  File "C:\Python27\lib\site-packages\keras-1.0.7-py2.7.egg\keras\optimizers.py", line 140, in get_updates
    grads = self.get_gradients(loss, params)
  File "C:\Python27\lib\site-packages\keras-1.0.7-py2.7.egg\keras\optimizers.py", line 68, in get_gradients
    grads = K.gradients(loss, params)
  File "C:\Python27\lib\site-packages\keras-1.0.7-py2.7.egg\keras\backend\theano_backend.py", line 677, in gradients
    return T.grad(loss, variables)
  File "C:\Python27\lib\site-packages\theano\gradient.py", line 549, in grad
    grad_dict, wrt, cost_name)
  File "C:\Python27\lib\site-packages\theano\gradient.py", line 1312, in _populate_grad_dict
    rval = [access_grad_cache(elem) for elem in wrt]
  File "C:\Python27\lib\site-packages\theano\gradient.py", line 1267, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "C:\Python27\lib\site-packages\theano\gradient.py", line 961, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]
  File "C:\Python27\lib\site-packages\theano\gradient.py", line 1267, in access_grad_cache
    term = access_term_cache(node)[idx]
  File "C:\Python27\lib\site-packages\theano\gradient.py", line 1101, in access_term_cache
    input_grads = node.op.grad(inputs, new_output_grads)
  File "C:\Python27\lib\site-packages\theano\scan_module\scan_op.py", line 2523, in grad
    outputs = local_op(*outer_inputs)
  File "C:\Python27\lib\site-packages\theano\gof\op.py", line 602, in __call__
    node = self.make_node(*inputs, **kwargs)
  File "C:\Python27\lib\site-packages\theano\scan_module\scan_op.py", line 430, in make_node
    new_inputs.append(format(outer_seq, as_var=inner_seq))
  File "C:\Python27\lib\site-packages\theano\scan_module\scan_op.py", line 422, in format
    rval = tmp.filter_variable(rval)
  File "C:\Python27\lib\site-packages\theano\tensor\type.py", line 234, in filter_variable
    self=self))
TypeError: Cannot convert Type TensorType(float32, 3D) (of Variable Subtensor{:int64:}.0) into Type TensorType(float32,
(False, False, True)). You can try to manually convert Subtensor{:int64:}.0 into a TensorType(float32, (False, False, T
ue)).
PS C:\Users\Fariz\Desktop>