semitom commented on 11 Jan 2016
I am trying to make a temporal context predictor. For this I implemented a BLSTM, I want it to predict data of timesteps which are unknown based on data of neighbouring timesteps. I need the resulting model to be able to handle sequences of undefined length, so I want the model to be fully sequential. Currently my code boils down to:
model = Graph()
model.add_input(name='input', input_shape=(None,200))
model.add_node(LSTM(input_dim=200, output_dim=200, return_sequences=True), name='forward', input='input')
model.add_node(LSTM(input_dim=200, output_dim=200, return_sequences=True), name='backward', input='input')
model.add_output(name='output', inputs=['forward','backward'], merge_mode='ave')
model.compile(optimizer='sgd', loss={'output': 'mse'})

train_X = mask_data(data) # a percentage of the timesteps is masked with 0's
train_Y = data

model.fit(train_X, train_Y)
To give you an idea how my data looks like, you can think about it like this:
train_X = [[[0.1],[0.2],[0.3],[0.0],[0.5],[0.6],[0.0],[0.8],[0.9]]]
train_Y = [[[0.1],[0.2],[0.3],[0.4],[0.5],[0.6],[0.7],[0.8],[0.9]]]
except that my data is 200 dimensional instead of 1 dimensional and has much longer sequences.
Because the input data is the same as the output data (except that the input is 'masked'), I do not really care about the outputs at which the input is known. I just need to predict the output at timesteps for which the input is unknown. My idea was to base the calculation of the loss function purely on the timesteps at which all inputs are 0 (the masked inputs). However, if I implement a loss function myself this can only take y_true and y_pred as parameters and there is no way to check the inputs (or mask) during the loss calculation.
Is there currently a way to implement what I want in Keras and if so, how?