Contributor
pdasigi commented on 1 Sep 2016
from keras.layers import Input, Embedding, TimeDistributed

input_layer1 = Input(shape=(2,), dtype='int32')
embedding = Embedding(input_dim=10, output_dim=20, mask_zero=True)
output1 = embedding(input_layer1)
print embedding.get_output_mask_at(0)

input_layer2 = Input(shape=(2,5), dtype='int32')
td_embedding = TimeDistributed(embedding)
output2 = td_embedding(input_layer2)
print td_embedding.get_output_mask_at(0)
I am using TimeDistributed(Embedding) as shown above only to operate on higher order inputs, and propagate the shape information accordingly. However, when I run the code above, I see that the first print statement shows a (symbolic) mask (Elemwise{neq,no_inplace}.0) and the second one just gives a None. I'm not sure why TimeDistributed does not propagate the mask.
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).