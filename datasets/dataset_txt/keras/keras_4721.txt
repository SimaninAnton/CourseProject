mklawonn commented on 25 Jul 2016
The Keras implementation of Dropout references this paper: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
The following excerpt is from that paper.
"The idea is to use a single neural net at test time without dropout. The weights
of this network are scaled-down versions of the trained weights. If a unit is retained with
probability p during training, the outgoing weights of that unit are multiplied by p at test
time as shown in Figure 2."
The Keras documentation mentions that dropout is only used at train time, and the following line from the Dropout implementation
x = K.in_train_phase(K.dropout(x, level=self.p), x)
seems to indicate that indeed outputs from layers are simply passed along during test time. Further, I cannot find code which scales down the weights after training is complete as the paper suggests. I'm hoping somebody can show me said code in the repo and put my mind at ease :-).