yushuinanrong commented on 14 Feb 2017 â€¢
edited
Hi all,
I'm working on a image SR problem and my dataset has ~300K 32x32 pixel images. I'm using fit_generator to fetch images with a batch size of 128 in training.
I just find when I only use a small portion of the dataset (~30K), the training for every batch is fairly fast. However, if I use the whole dataset instead, the training for every batch becomes much slower (~10 times slower).
I'm confused because fit_generator only fetch a batch of images every iteration, it should not be sensitive to the number of overall training samples....
I'm using tensorflow backend on Ubuntu 16.04 with a Titan X GPU.
Any idea will be greatly appreciated.