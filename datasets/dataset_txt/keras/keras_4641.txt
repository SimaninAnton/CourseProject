brunoalano commented on 5 Aug 2016
I'm trying to get the thought vector described in some papers using an autoencoder, to be able to get more meaningful vectors then Doc2Vec (since it doesn't work well in small sentences).
But when I train my model, I'm getting an incredible small loss (loss: 9.5479e-05 - val_loss: 9.6238e-05) on the first epoch and so on. I don't know if that's correct or I have some problem with my implementation.
To represent the text, I'm using one-hot encoding (as you see in my code) and using Mean-Squared Error as loss function.
# Settings
number_of_samples   = 1000
max_length          = 20
nb_words            = 10000
latent_dim          = 100

# Create the Dataset
X_train, X_test, (index_to_word, word_to_index) = create_dataset(number_of_samples=number_of_samples, max_length=max_length)

# Create the Autoencoder Model
inputs = Input(shape=(max_length, nb_words), name='main_input')

# Codification
encoded = LSTM(latent_dim)(inputs)

decoded = RepeatVector(max_length)(encoded)
decoded = LSTM(latent_dim, return_sequences=True)(decoded)
decoded = TimeDistributed(Dense(nb_words))(decoded)

# Models
autoencoder = Model(inputs, decoded)
encoder     = Model(inputs, encoded)

# Compile
autoencoder.compile(optimizer='rmsprop', loss='mse')
I'm adding a Gist with all the code, and how the data is generated (this can be useful for someone later): https://gist.github.com/brunoalano/ddb041338a486648b7730ed567840818