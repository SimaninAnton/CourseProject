redraven984 commented on 24 Oct 2018 â€¢
edited
Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on StackOverflow or join the Keras Slack channel and ask there instead of filing a GitHub issue.
Thank you!
[x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps
[x ] If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
[x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
[x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
I'm training a simple Keras model using the sklearn api. The idea is to use gridsearchcv to learn a relatively simple architecture. However, what I am noticing is each iteration of gridsearch is getting slower. Normally, I would try to do some garbage collection between runs, or look to make sure that the model isnt getting bigger with each run. However, I'm not sure how to check this in this library. Please let me know if you have any suggestions.
Here is some toy code to replicate the problem:
# import libraries
import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasRegressor

# load the data
def data(test_train_split = 0.9):

    # create a random data frame
    df      = pd.DataFrame(np.random.randint(0,100,size=(1000, 12)),columns=list('ABCDEFGHIJKL'))

    #  pull off the data len
    dataLen = len(df)

    # take a target series
    y = df['L'].values
    df.drop(columns=['L'], axis=1, inplace=True)

    # extract the values from the dataframe
    x = df.values
    data_to_be_partitioned = x

    # split the data into train and test
    cutoffSample = int(dataLen*test_train_split)
    x_train      = data_to_be_partitioned[:cutoffSample]
    y_train      = y[:cutoffSample]
    x_test       = data_to_be_partitioned[cutoffSample:]
    y_test       = y[cutoffSample:]

    return x_train, y_train, x_test, y_test

X_train,y_train,X_test,y_test = data()

# create a function that returns a model, taking as parameters things you
# want to verify using cross-valdiation and model selection
def create_model(optimizer='adagrad',activation='relu',units=64,dropout=0.2,numLayers =1):
    from keras.layers import Dense, Dropout
    from keras.models import Model, Sequential

    # Create the model
    model = Sequential()

    # add the layers
    model.add(Dense(units, activation=activation))
    model.add(Dropout(dropout))

    if numLayers == 2 :
        model.add(Dense(units, activation=activation))
        model.add(Dropout(dropout))

    model.add(Dense(1, activation='linear'))

    # Compile the model
    model.compile(loss='mean_squared_error',optimizer=optimizer, metrics=['mse'])

    return model

# wrap the model using the function you created
clf = KerasRegressor(build_fn=create_model,verbose=0)

# instantiate a scaler
scaler = StandardScaler()

# create parameter grid, as usual, but note that you can
# vary other model parameters such as 'epochs' (and others
# such as 'batch_size' too)
param_grid = {
     'clf__optimizer':['rmsprop','adam','sgd'],
     'clf__epochs':[4,12,24,50],
     'clf__dropout':[0.1,0.2],
     'clf__activation':['relu'], #, 'sigmoid', 'tanh', 'elu'],
     'clf__units':[4,8,12,24,32],
     'clf__numLayers':[1,2]
}

pipeline = Pipeline([
    ('preprocess',scaler),
    ('clf',clf)
])

# if you're not using a GPU, you can set n_jobs to something other than 1
grid = GridSearchCV(pipeline, cv=3, param_grid=param_grid, 
scoring='neg_mean_squared_error',verbose = 11
print(X_train.shape)
print(y_train.shape)
  grid.fit(X_train, y_train)

# summarize results
print("Best: %f using %s" % (grid.best_score_, grid.best_params_))
means = grid.cv_results_['mean_test_score']
stds = grid.cv_results_['std_test_score']
params = grid.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))