log0 commented on 24 Mar 2016
Python3, used latest keras.git and Theano.git with update prior to posting. Consistent repro.
Previously I saved a model after running the lstm_text_generation.py for 500 epochs. The model spits our reasonable output. I saved the weights and reloaded the model using the weights file and it completely becomes gibberish. I saw previous bugs like #655 that indicates the save/load may not be perfect that it misses some information during save/load, could this be a case of that?
I modified the lstm_text_generation and I get a repro as well. When the model is trained, it gives "reasonable output" like this, but not after load weights. I even hardcoded the generating seed to minimize variance. For the example to repro, I used fewer epochs (5) only and a small piece of text. However, I used a larger text corpus with more epochs and it's still the same (trained for 18 hours).
Train + Save: https://gist.github.com/log0/d54ba54fb73837c2e3bf
Load: https://gist.github.com/log0/50c43671dd1426cf1cea
// This is the model that is trained.
Iteration 5                                                                                                                                                                  [37/220]
Epoch 1/1
11644/11644 [==============================] - 11s - loss: 3.0284     

----- diversity: 0.2
----- Generating with seed: "ilosophers, in so fa"
ilosophers, in so fa   oaae ta    oaoea toat    est oa e o  e otoo eao to o     ae ao ooetta  t  t ooto ioo t      oteasaet o t  ttt tot te   o tttot  otot t t ott a t   oto to te t
t  teto  so ooaot  tt t attt r  tttt   t t tr e oseoa totttt    teae aoaott to    h eeaeeootttt     et ttoets oto  tttttt   t    eaottoot et      eeoteot o     eeteaoeostt     oao a
t   oooaaa t e   taeooo eeo tt e et hortr h   oeteo   teot

----- diversity: 0.5
----- Generating with seed: "ilosophers, in so fa"
ilosophers, in so faso et   u ats sarosst aooo a rnaseut aio oa a heestertatnenouptttlhtt t    tiaoteootetst  oetu uo  tdae  eittoethettt to  ehor mrso tea  ead  autlttwotnea  nh  t
aiett leoas
 isno ttettds l ot t o atrar
t t o s tattarh t oah o taltae ow  sto tr oyeotooarotesottsh rn  e ehtr oo ottteiee e ste   esoertpaan  it s atstit ta thoa p ee aee e aaeheo ttaoate  ee t lruhe ht eao e utst earrt
 to t  rfr oantha
// This model never trains, just use the previously supplied weights.
----- diversity: 0.2
----- Generating with seed: "ilosophers, in so fa"
ilosophers, in so fa((((((e):l9(::(9e9(9)(d(:::((9e(e99(e((e)))(9(((e:((e9)(:eee9e9(((:99:((99:9(:ee((9999:9)(9:d9(((9(e9:9e9(:(ee(()((9:9::(9((e(le:9(9)(e9999(e)((e((e9:e:(9)(e19($e9(((e(:)e(:ee9le(9(9(9(((::(:e)e9):9((()99((((:9e(e:e99(e(9:::(((999ee99)((99(((((eeeee:((e()ee9(:((e(((9:ee9e9:((d99(((u:(9(::)9e9e((99(((:9)(ee::999((((:e9e:():999(e(99:(:(9(:(9$e:9(9((99(9(:(emeeeeel(999(l(9()9(:(ee(e(e9e()9((()(9::9)(

----- diversity: 0.5
----- Generating with seed: "ilosophers, in so fa"
ilosophers, in so fae(9(9(m(u99:(9(nd19(9(l(dm(e(eues9(::e:ulm(lu:feo(e((:(m((u:e(ds(:(ee9e::(:1(e(ue 1(e:)(dnd o(eel(:))()(:9ee1((9d9e(s(9u:((e e:9eeeo(de9(()99:::9(991:ee9e :1(99$((:s)):,()1ee9((n(((e9:)de((d):lee9m),))9(9(9:() 19 ((9lmee9eeeb9(11(:99(l:(e9(u1((e):d(e()(9((9:()l)9d:l(edl:9999nu)nd(e9l(e)((d():1,)(9(9((e (f):le)s(be((((19le99(,:(9()(,:e):d9e$9()((b((:9f11)lee9(e(((e(le)9d)9()()(1(9(b)():)des:9e::e9)
As you can see above, the output looks much more gibberish. Also, I have tested with a text corpus with no english characters but chinese characters, the output is similar and only outputs english characters even though there is not a single english character.
Appreciate any help on this. Thank you.