singlasahil14 commented on 6 Feb 2017 â€¢
edited
f = h5py.File(path+'results/precomp.h5', 'r')
conv_trn_feat = f['train_features'][:]
trn_labels = np.tile(f['train_labels'][:], reps=(2,1))

def myGenerator():
    while True:
        for i in range(0, len(conv_trn_feat), batch_size):
            yield conv_trn_feat[i:i+batch_size], trn_labels[i:i+batch_size]

train_datagen = myGenerator()

bn_model = Sequential(get_bn_layers(0, num=num_hidden))
bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
bn_model.optimizer.lr = 1e-3
bn_model.fit_generator(train_datagen, samples_per_epoch=len(conv_trn_feat), nb_epoch=4)
bn_model = Sequential(get_bn_layers(0, num=num_hidden))
bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
bn_model.optimizer.lr = 1e-3
bn_model.fit(conv_trn_feat, trn_labels, nb_epoch=4)`
I am precomputing the features of a convolutional layer and training fully connected layers using them. I have to use hdf5 file and a custom generator because the size of dataset is quite large.
Following is the output:
The loss when using fit decreases much more quickly then using fit_generator with a custom generator.
Am I doing something wrong? To me it looks like some bug in fit_generator.