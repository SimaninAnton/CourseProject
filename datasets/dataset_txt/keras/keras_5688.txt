viksit commented on 24 Mar 2016
I have a couple of questions on serving models in production.
In the case of a complex model, loading from json can end up taking a while (100s of seconds), given there's a model.compile() happening in there. Now, if you have one model, but different versions of it - say, trained on datasets d1, d2 and .. dn - what would be the best way to maintain these in memory? A deep copy doesn't work (recursion depth exceeded, for Theano backed models) - so perhaps a copy.copy() is the best option for now.
Are there any thoughts on utilizing Tensorflow serving for something like this with Keras models, and has any thought been given to integration/examples of how one might do that, given they support this kind of stuff?
Thanks for any inputs.