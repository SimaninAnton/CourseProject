discobot commented on 21 Jan 2016
Whenever I try to fit network with wide recurrent layers (dim > 1000) on GPU (doesnt matter SimpleRNN or LSTM) I get 'RuntimeError: Error copying memory'
  File "train.py", line 49, in <module>
    rslt = model.fit(x, y, nb_epoch=60, batch_size=batch_size)
  File "/usr/local/lib/python2.7/dist-packages/keras/models.py", line 581, in fit
    shuffle=shuffle, metrics=metrics)
  File "/usr/local/lib/python2.7/dist-packages/keras/models.py", line 239, in _fit
    outs = f(ins_batch)
  File "/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py", line 365, in __call__
    return self.function(*inputs)
  File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 606, in __call__
    storage_map=self.fn.storage_map)
  File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 595, in __call__
    outputs = self.fn()
  File "/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.py", line 672, in rval
    r = p(n, [x[0] for x in i], o)
  File "/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.py", line 661, in <lambda>
    self, node)
  File "scan_perform.pyx", line 251, in theano.scan_module.scan_perform.perform (/home/phill/.theano/compiledir_Linux-3.10--x86_64-with-Ubuntu-12.04-precise-x86_64-2.7.3-64/scan_perform/mod.cpp:2609)
RuntimeError: Error copying memory
Apply node that caused the error: for{gpu,grad_of_scan_fn}(Elemwise{sub,no_inplace}.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}
.0, GpuSubtensor{:int64:}.0, GpuSubtensor{:int64:}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, GpuAlloc{memset_0=True}.0, Elemwise{sub,no_inplace}.0, <CudaNdarrayType(float32, matrix)>, <CudaNdarrayType(float32, vector)>, <CudaNdarrayType(float32, matrix)>, <CudaNdarrayType(float32, matrix)>, <CudaNdarrayType(float32, vector)>, <CudaNdarrayType(float32, matrix)>, <CudaNdarrayType(float32, matrix)>, <CudaNdarrayType(float32, vector)>, <CudaNdarrayType(float32, matrix)>, <CudaNdarrayType(float32, matrix)>, <CudaNdarrayType(float32, vector)>, <CudaNdarrayType(float32, matrix)>)
I'm sure that I have enough memory because everything works perfectly OK with dim < 900, and network only takes about 1GB of my 25GB VRAM.