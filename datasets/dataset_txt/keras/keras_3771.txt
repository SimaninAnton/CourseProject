jaredsagendorf commented on 5 Dec 2016 â€¢
edited
I'm trying to do some sequence to sequence learning using an Embedding layer and LSTM layer. My model looks something like this:
model = Sequential()
model.add(Embedding(num_classes_in,  embed_size, mask_zero=True))
model.add(Bidirectional(LSTM(lstm_size, activation="tanh", return_sequences=True), merge_mode='sum'))
model.add(TimeDistributed(Dense(num_classes_out, activation='softmax')))
My X and Y data consist of sequences of variable length, and the objective is to learn a mapping from the sequence X to the sequence Y, where each xi in X is a label of 20 possible values, and each yi in Y is a label of 3 possible values. When I try fitting the model without padding the input sequences, using an iteration such as
for i in range(num_epochs):
   indices = np.random.permutation(N)
   for index in indices:
        model.fit(Xtr[index], Ytr[index], nb_epoch=1, batch_size=1, verbose=0)
and mask_zero=False (since the sequences are not padded), I get good accuracy, but very slow training time, especially when trying to use GPUs. My belief is that if I pad the sequences and call model.fit() once, with a larger batch size, I should be able to get better runtime performance.
However, when I do so, the model accuracy seems to decrease vastly.
Is there a reason for this? When you pad the sequences, and turn on mask_zero=True in the embedding layer, does this work properly when return_sequences=True is used? Is there some way to check whether masking is being applied in the loss and metric evaluation?
Thanks for any help you can provide.