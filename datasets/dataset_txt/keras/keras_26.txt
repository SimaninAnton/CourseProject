TMaysGGS commented on 12 Nov 2019 â€¢
edited
System information
Have I written custom code (as opposed to using example directory): No. https://github.com/keras-team/keras/blob/master/keras/layers/advanced_activations.py
OS Platform and Distribution : Windows 10
TensorFlow backend : yes
TensorFlow version: 1.15.0
Keras version: 2.2.4-tf
Python version: 3.7.4
CUDA/cuDNN version:
GPU model and memory:
Describe the current behavior
Hi everyone,
Recently I checked the paper and the tensorflow definition of PReLU (Paramatic ReLU) function as well as some other DL frameworks. I think when using PReLU, the parameter "alpha" should be the of the shape (channel, ) where the channel means the number of channels of the PReLU layer input feature map. However, the keras.layers.PReLU API defines the alpha has the same shape as the input (and it does implement this way), which introduces a lot more paramters to this layer.
In Keras API, it says "where alpha is a learned array with the same shape as x.", while in the original paper, it says "The subscript i in ai indicates that we allow the nonlinear activation to vary on different channels." (ai means alpha here). So I am wondering if there is any difference between the Keras PReLU & the original PReLU.
The most important things is, the reason to introduce PReLU is to use a very small number of extra parameters to get a better classification performance, while using Keras API PReLU, it will add a lot of parameters that are even far more than paramters in convolutional layers.
PS: I just checked the TensorFlow 2.0 tf.keras.layers.PReLU class, it has the same implementation as Keras API does.
Describe the expected behavior
If I happen to be right, I wonder if there will be some corrections on this; if I am wrong, I wonder if someone can help me with this confusion.
Thanks.