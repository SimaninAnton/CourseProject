apple2373 commented on 12 Jun 2016 â€¢
edited
Hi,
I want to train a language model with more flexibilities. I am using Keras with tensorflow so that I can train easily on multiple GPUs and use the nice visualization with tensorboad. But I got several problems.
I looked into two examples.
[Stateless LSTM] https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py
[Statefull LSTM] https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py
In Stateless example, I did not want to fix the input length. Especially in terms of prediction, I want to designate arbitrary length of seed. The example use a seed of the same length as input length (i.e. give the first 40 characters), and predict the characters after that. But I just want to give only one or two characters as a seed. Only way I can think is, pad zeros before the seed, and make it to input length, but it's a waste of resource. So I turned to statefull RNN.
In statefull example, it seems to solve the issue above. BUT, now I need to fix the batch size in advance. That will end up another resource waste. For example, if I need only one batch prediction, I will waste (batch size - 1) predictions by padding zeros.
In short, I want to request stateless RNN with arbitrary input length or statefull RNN with arbitrary batch size.
Best,
Satoshi