Rampeul commented on 11 Jun 2018
Hi there,
First of all, thanks a lot for keras and all the great work, it's an amazing and a very useful tool to me.
I'm quite new to deep learning and GPU as well but I have some knowledge in CPU and FPGA.
But here come my thoughs :
When I run a model for a prediction loop on my GPU, I see that the GPU utilization (in nvidia-smi command) is changing between 0% and 40%. As GPU is a parallel architecture, it should be possible to instantiate the model twice for processing two input at a time without raising prediction time.
I know that it's possible to set a batch size of two or more, but in my understanding, it's the same network instance which is used as a pipeline resulting in more memory utilization and then prediction optimization.
Am I right ? Because all the following start from these statement ...
For testing these, I used a keras model on top of my previous model this way :
# the model importation is related to it's definition but I don't think it's relevant to my issue
model_A = load_model('model.h5') 
model_A.load_weights('weights.h5')
for layer in model_A.layers:
    layer.name = layer.name + str("_A")

model_B = load_model('model.h5')
model_B.load_weights('weights.h5')
for layer in model_B.layers:
    layer.name = layer.name + str("_B")

model = Model(input = [model_A.input, model_B.input], output = [model_A.output, model_B.output])
This code run and I get some tensorflow log for model_A instantiation. Then I move to prediction as follow :
predictions = model.predict([sample_A, sample_B])
I was hoping to get a prediction time near from a single model prediction and more GPU utilization, but I get my prediction time doubled and nothing significative from GPU utilization instead. It looks like the models are run one by one ...
Am I missing something or my idea is just not coherent ?
Could this behavior comes from using two different input layer (one in model_A, one in model_B) ?
Any help or thoughts is welcome ! :)