Akababa commented on 9 Jan 2018 â€¢
edited
Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on StackOverflow or join the Keras Slack channel and ask there instead of filing a GitHub issue.
Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps
If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
https://www.kaggle.com/akababa/rnn-gru?scriptVersionId=2109295
I'm attempting to train an RNN to predict the next element of variable-length sequences (so it can be used in an online fashion). In order to do that I want to calculate the loss gradients of L(f(x_1,...x_i), x_{i+1}) for i from 1 to n-1, not only for i=n-1 (which is what I suspect it's doing). Is there a way to do this other than by copying all n-1 prefixes of the sequence and running train_on_batch once for each of them?