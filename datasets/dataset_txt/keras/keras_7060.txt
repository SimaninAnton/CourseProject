Contributor
pranv commented on 19 Apr 2015
I think it would be much more clear and easy to have a "batch size" parameter separately for Batch Normalization layer. We can just directly pass the outputs of our convolution or pooling layers to it. The layers as a whole will be more coherent.
(As an aside, did anyone have any luck with batch normalization? I tried many times, but actually got worse results most of the time.)