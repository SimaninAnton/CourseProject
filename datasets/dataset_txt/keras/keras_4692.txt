Contributor
dolaameng commented on 28 Jul 2016
I notice there is a difference in how images are pre-processed in the neural_style_transfer example and the recommended way of using VGG16 model. Specially,
The color channels in neural_style_transfer example is RGB (as read by scipy.misc.imread), whereas the order in original VGG is expected to be BGR.
The mean-centering step is not performed in the example, i.e.,
im[:,:,0] -= 103.939
im[:,:,1] -= 116.779
im[:,:,2] -= 123.68
Some of my experiments showed that these differences could result in sub-optimal results in image classification, e.g., red fox misclassified to grey fox due to the wrong order of color channel.
However, when it comes to generating "artistic" images as in the neural_style_transfer example, it seems that the results caused by the difference are not so different. I can understand there are probably some reasons behind this,
The "content loss" and "total variation loss" defined in the example don't depend on the order of color channels.
The "style loss" might depend on the color orders, because it depends on the difference of "styles" in the original and the generated image. And this style is measured by the inner-product distribution of different vgg filters (Gram matrix). However, it should be expected that most of vgg filters will be based on a mix of colors (e.g., edges, corners) rather than a single color alone (e.g, blue color for sea).
Those could be the reasons why I don't see many differences when experimenting with the different color channel encoding in the neural_style_transfer example. But I really want to know whether we can come up with some contrived cases where the different color channel order will result in a difference for the neural_style_transfer example.
I post this question here because it might lead to a potential bug report to the above example code.