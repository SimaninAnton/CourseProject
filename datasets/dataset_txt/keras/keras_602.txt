stillwaterman commented on 23 Nov 2018 â€¢
edited
Hello, I am trying to build DenseNet in Keras, but this model requires huge memory for original version. Thus, I found an implementation (tensorflow Version) which uses tf.contrib.layers.recompute_grad to reduce the memory cost. I tried to use this code in my Keras model, unfortunately, it caused some errors.
Here is my experiments.
For the first time, I simply add tf.contrib.layers.recompute_grad to my _conv_block which give rise to an error.
Code:
def __conv_block(ip):
def _x(inner_ip):
made somthing operation
return x
if efficient:
_x = tf.contrib.layers.recompute_grad(_x)
return _x(ip)
Error Message: TypeError: All variables used by a function wrapped with @custom_gradient must be ResourceVariables. Ensure that no variable_scope is created with use_resource=False.
Then I tried to fix this problem by adding the code:with tf.variable_scope('denseblock_{}'.format(block_idx), use_resource=True), but I got another different error.
Error Message: AttributeError: NoneType object has no attribute _inbound_nodes.
At that time, I thought maybe it is a problem about tensorflow function. So I tried to use Lambda to wrap it. recompute_grad_cp = Lambda(lambda dx: tf.contrib.layers.recompute_grad(dx)), _x = recompute_grad_cp(_x), but the error said Lambda layer only accepts Keras tensor as input. tf.contrib.layers.recompute_grad requires a function as input, so it cannot be wrapped by Lambda.
I would like to ask if there is a solution to use this function tf.contrib.layers.recompute_grad in Keras.