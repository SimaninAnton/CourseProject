renyajie commented on 14 Apr 2019
I have warp all the function into lambda, why still error??
def call(self, inputs, mask=None):
'''
#original is below
memory, query = inputs
hidden = K.dot(memory, self.Wh) + K.expand_dims(K.dot(query, self.Wq), 1) + self.bias
hidden = K.tanh(hidden)
# remove the dimension whose shape is 1
s = K.squeeze(K.dot(hidden, self.v), -1)
    # compute the weight use soft_max
    s = K.softmax(s)

    return K.sum(memory * K.expand_dims(s), axis=1)
    '''
    # score(previous, target) = Vt * tanh(Wh * memory + target * Wq)
    memory, query = inputs
    print(1, memory.shape, query.shape, self.Wh.shape, self.Wq.shape)

    # K.expend_dims(input, num): add 1 dimension at num in input
    one = Lambda(lambda x: K.dot(x, self.Wh))(memory)
    two = Lambda(lambda x: K.dot(x, self.Wh))(query)
    two = Lambda(lambda x: K.expand_dims(x, 1))(two)
    hidden = Lambda(lambda x: x + two)(one)
    print(2, one.shape, two.shape, hidden.shape)

    hidden = Lambda(lambda x: K.tanh(x))(hidden)
    # remove the dimension whose shape is 1
    print(3, hidden.shape)
    #s = K.squeeze(K.dot(hidden, self.v), -1)
    s = Lambda(lambda x: K.dot(x, self.v))(hidden)
    print(4, s.shape)
    s = Lambda(lambda x: K.squeeze(x, -1))(s)
    print(4, s.shape)
    #s = Lambda(lambda x: K.squeeze(x, -1))(merge.Dot(axes=-1)([hidden, self.v]))

    # compute the weight use soft_max
    # s = K.softmax(s)
    s = Lambda(lambda x: K.softmax(x))(s)
    print(5, s.shape)
    p = Lambda(lambda x: K.expand_dims(x))(s)
    p = Lambda(lambda x: memory * x)(p)
    #return K.sum(memory * K.expand_dims(s), axis=1)
    print(6, p.shape)
    m = Lambda(lambda x: K.sum(x, axis=1))(p)
    print(7, m.shape)
    return m
the original is this