Contributor
pdasigi commented on 4 May 2016
It looks like the callback causes training to stop if the validation loss increases, which means the parameters from the last epoch are the ones that are kept. Can someone confirm this? Should it not be the case that we store the parameters from the epoch before the last one instead, which result in the lowest validation loss?