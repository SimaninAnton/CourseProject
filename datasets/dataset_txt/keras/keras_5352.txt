mininaNik commented on 25 Apr 2016 â€¢
edited
I wanted to implement the forth architecture from the left (first many to many):
I my case the length of input and outputs aren't equal (I mean that number of blue and red units aren't equal)! I have n samples to train the NN then the shape of input is nn_prev1 and the out shape is nn_nxt1. I want to have a stateful NN. I understood how to do it by help of @carlthome and other guys in issue #2403. It is my final code:
 hidden_neurons = 50
 model = Sequential()
 ##Encoder
 model.add(LSTM(hidden_neurons,
           batch_input_shape=(batch_size, n_prev, 1),
           forget_bias_init='one',
           return_sequences=False,
           stateful=True))
model.add(Dense(hidden_neurons))
model.add(RepeatVector(n_nxt))
##Decoder
model.add(LSTM(hidden_neurons,
           batch_input_shape=(batch_size, n_prev, 1),
           forget_bias_init='one',
           return_sequences=True,
           stateful=True))
 model.add(TimeDistributed(Dense(1)))
 model.compile(loss='mse', optimizer='rmsprop')
Now I am skeptical whether my encoder is working like fig(1) or fig(2)?

fig(1)

fig(2)
5