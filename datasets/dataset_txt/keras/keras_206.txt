Marcosuff commented on 27 May 2019
Hello,
I'm trying to train a multi-task neural network for classification. Each sample is labeled as 1 or 0, while others have missing labels (e.g. -1).
I found an approach that gives different weights to the samples. In this approach a 2d array stores the weights for each sample, where samples with label = -1 have weights = 0. Using this approach it was possible to train the neural net using softmax_crossentropy.
On the other hand I also found custom implementations of the loss functions which ignores samples with missing labels without using precalculated weights.
I'm quite new to Keras and deep learning and this is the first time I get a problem with missing labels. So I'm confused about the right way to go here. Should I create an array of samples weights and assign 0 to samples with missing labels? Or should I implement a custom loss function that masks these samples? So far I experimented with a custom mean squared error and a custom binary_crossentropy functions that masks samples with labels -1.
Here's a sample code:
Custom binary_crossentropy
def binary_crossentropy(y_true, y_pred):
return K.mean(K.binary_crossentropy(tf.multiply(y_pred, tf.cast(tf.not_equal(y_true, -1), tf.float32)),
                                    tf.multiply(y_true, tf.cast(tf.not_equal(y_true, -1), tf.float32))), axis=-1)
Custom Mean Squared Error
def custom_mse(y_true, y_pred):
    mask = K.cast(K.not_equal(y_true, MASK_VALUE), K.floatx())
    return K.mean(K.square(y_pred * mask - y_true * mask), axis=-1)