KeironO commented on 2 Mar 2016
When training Neural Networks I tend to find that the validation loss starts low, increases and then drops down after a few epochs. However, in some cases initial validation loss can be lower than validation loss after training.
Is there a way to ignore the first x epochs on callbacks?