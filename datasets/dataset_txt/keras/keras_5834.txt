rudraptpsingh commented on 3 Mar 2016
I am trying to implement a language model using LSTMs. My network runs fine and I also see that the training loss decreases but the testing accuracy is always above 99% even if I don not train my network for long.
I have used word2vec vectors and embedded the weights in the Embedding layer. My network looks like:
model = Graph()
model.add_input(name='input', input_shape=(n_train,), dtype=int)
model.add_node(Embedding(output_dim=rnn_dim, input_dim=n_symbols, weights=[embedding_weights]),name = 'embedding',input='input')
model.add_node(LSTM(output_dim=dense_dim,input_dim=rnn_dim), name='forward', input='embedding')
model.add_node(Dropout(0.5), name='dropout', input='forward')
model.add_node(Dense(output_size, activation='softmax'), name='softmax', input='dropout')
model.add_output(name='output', input='softmax')
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.99, nesterov=True)
model.compile(sgd, loss={'output': 'categorical_crossentropy'})
print("Train...")
model.fit({'input': X_train,'output': y_train},
batch_size=128,
nb_epoch=1,verbose=1)
My training and testing array shapes are:
X_train shape: (100000, 18)
X_test shape: (10000, 18)
y_train shape: (100000, 998)
y_test shape: (10000, 998)
where there are 100000 training and 10000 testing sentences and each sentence contains 18 words. The number of classes for output is 998.
Can anyone suggest why I might not be getting a genuine testing error?