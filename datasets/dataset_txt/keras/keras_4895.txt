AkatsukiTse commented on 27 Jun 2016 â€¢
edited
I have a model using batch normalization layer.what confuse me is:
print(model.predict(x)) #return [0.9, 0.8, 0.7]
print(model.predict(x[1:])) #return [0.83, 0.73]
I know the layer normalize the input and with increasing the batch_size, the difference wil be smaller.
But according to Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift.
In test, the batch normalization use running_mean and running_variance which are caculated in train.
So why batch normalization layer normalize the input depending on the batch_size and other inputs in the batch?
Does it use the batch_mean and batch_variance to normalize the input?