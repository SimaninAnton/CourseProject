ronzillia commented on 12 Jan 2017 â€¢
edited
I want to see the result for LSTM on the 20 Newsgroup data.
Everything is the same as https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py
I just replace the Conv part by LSTM
`
model = Sequential()
model.add(embedding_layer)
model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.add(Dense(len(labels_index), activation='softmax'))
model.compile(loss='binary_crossentropy',
optimizer='adam',
metrics=['accuracy'])
`
Unfortunately, for every batch processed, I got the same acc as follow:
Epoch 1/1
4384/15998 [=======>......................] - ETA: 1953s - loss: 0.1985 - acc: 0.9500
Epoch 1/1
4480/15998 [=======>......................] - ETA: 1937s - loss: 0.1985 - acc: 0.9500
Epoch 1/1
4544/15998 [=======>......................] - ETA: 1928s - loss: 0.1985 - acc: 0.9500
the acc is always 0.95.
I have no idea where went wrong.
Thank you all