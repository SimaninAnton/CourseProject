kris-singh commented on 1 Apr 2018 â€¢
edited
I want to do sampling based on the loss of the data points. For this i need to compute the loss for each data point. categorical_cross_entropy(y_true, y_pred) requires y_true to be tensor. So i just used tf.nn.softmax_cross_entropy_with_logits(Here is the gist https://gist.github.com/kris-singh/53b1c6c687ce2b44c0140f87628ea370.) but this creates a node in the graph again and again(i guess) which is the reason the program is very slow. How should i convert the code. If i define this in the init function and use sess.run(res, feed_dict)(https://gist.github.com/kris-singh/364867d41d4c65fe356ac3d454fba596) which sess variable should i use.
Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on StackOverflow or join the Keras Slack channel and ask there instead of filing a GitHub issue.
Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps
If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).