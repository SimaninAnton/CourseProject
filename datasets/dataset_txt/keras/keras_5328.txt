naure commented on 27 Apr 2016
When building softmax classifiers with the Tensorflow backend, multiple conversions between the softmax and log scales are performed, resulting in sub-optimal performance and numerical stability.
Example:
# Computes softmax with exponentials
model.add(Activation('softmax'))

# Reverses softmax with clip() and log(), then calls
# the tensorflow crossentropy op that reapplies softmax in the logit scale
model.compile(
    loss=sparse_categorical_crossentropy,
)
Proposing an objective that merges them:
# No softmax
model.add(Dense(12))

# Computes the softmax cross-entropy:
model.compile(
    loss='sparse_softmax_categorical_crossentropy',
)
[ x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
[ x ] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
[ x ] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).