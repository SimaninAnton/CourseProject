Contributor
MartinThoma commented on 27 Apr 2017 â€¢
edited
I have Keras 2.0.2 and a CNN which I train on CIFAR-100 with a NVIDIA GTX 970, the tensorflow backend with CUDA 8 and cuDNN 5.1.
1 epoch of training needs 23 seconds in average with ReLU
1 epoch of training needs 32 seconds in average with the 'linear' activation function
Both networks are the same, except for the activation function. Why is applying the identity so slow? Especially: Why is it slower than ReLU?
Related code:
https://github.com/fchollet/keras/blob/master/keras/activations.py#L61-L62