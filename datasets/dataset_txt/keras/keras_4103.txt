gbezerra commented on 23 Oct 2016
I'm trying to build an autoencoder that converts a sentence into a fixed-length vector. Below is the code:
        self.sentence_len = 20
        self.input_dim = 100000
        self.output_dim = 100

        inputs = Input(shape=(self.sentence_len, self.input_dim))
        encoded = LSTM(output_dim=self.output_dim)(inputs)

        decoded = RepeatVector(self.sentence_len)(encoded)
        decoded = LSTM(output_dim=self.input_dim,
                       return_sequences=True)(decoded)

        autoencoder = Model(inputs, decoded)
        encoder = Model(inputs, encoded)

        autoencoder.compile(optimizer='rmsprop',
                            loss='categorical_crossentropy')

        autoencoder.fit(X, X,
                        nb_epoch=100,
                        batch_size=128,
                        shuffle=True)
Assuming the above code is correct, I have a question about the loss function. Is categorical_crossentropy the right loss function? Is there a way in Keras to do a sampled crossentropy, where you compare the prediction against a sample of the vocabulary instead of all the words? (i.e., similar to the sample_softmax_loss in Tensorflow)
1