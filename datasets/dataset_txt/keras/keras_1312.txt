DHKLeung commented on 20 Jan 2018 â€¢
edited
just asked in stackoverflow
input0 = keras.layers.Input((32, 32, 3), name='Input0')
flatten = keras.layers.Flatten(name='Flatten')(input0)
relu1 = keras.layers.Dense(256, activation='relu', name='ReLU1')(flatten)
dropout = keras.layers.Dropout(1., name='Dropout')(relu1)
softmax2 = keras.layers.Dense(10, activation='softmax', name='Softmax2')(dropout)
model = keras.models.Model(inputs=input0, outputs=softmax2, name='cifar')
I found that dropout works wrongly so i write a simple NN to test it.
when i set the dropout rate = 1.0
it should drop all the nodes from the previous layer and thus the output layer should remain the same from the initialization.
That is, the performance throughout epochs should be the same
however it still work although i drop all the nodes