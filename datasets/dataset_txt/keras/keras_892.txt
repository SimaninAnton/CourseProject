lminer commented on 13 Jul 2018
I'm training a u-net model using batch normalization on all the convolution and deconvolution layers except for the last. For some reason the validation loss is going up and down by 50% or more from one epoch to the next. If I disable batch normalization, however, I don't see this phenomena. I'm having this problem with keras 2.1.4 - 2.2.0 and tensorflow 1.8 - 1.9. I don't see this issue in Chainer though training an identical network with identical data.