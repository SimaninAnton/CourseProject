beomjoonkim commented on 9 Aug 2016
Hi,
I believe there is an error in coordinating weights for BatchNormalization with load_weights function. I was training an architecture with BatchNormalization layer in it, for which I saved the weights via checkpoint function. When I ran the predict() function after loading the weights on test data, I observed that the prediction varied when I changed the number of the test data points. I suspected that this has to do with BatchNormalization layer calculating the normalization parameters on the fly with the test data points instead of the one saved from the training which is what it should be doing.
When I took out BatchNormalization layer from training and repeat the same procedure, I was no longer having the same issue.
1