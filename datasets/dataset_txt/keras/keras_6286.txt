lemuriandezapada commented on 10 Dec 2015
It took me 2 whole days to figure out why my RNN was fucked. Finally I reached the step function.
h = K.dot(x, self.W) + self.b
output = self.activation(h * K.dot(prev_output, self.U))
If I'm correct it should actually be
output = self.activation(h + K.dot(prev_output, self.U))
Which actually lets you use something other than sigmoid for your transfer function