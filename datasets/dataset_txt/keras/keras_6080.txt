bwkchee commented on 21 Jan 2016
I'm trying to follow the Image Captioning example and walk through the code.
1: The code doesn't seem to compile (minus loading the h5 and running the lstm at the end). Because there's no imports included I'm assuming: keras.layers.core.Merge and I get the following error: AttributeError: 'Merge' object has no attribute 'add'
Merge is a layer, but is being treated as having a return type of model?
2: The comments before training the lstm are somewhat unclear:
"images" is a numpy float array of shape (nb_samples, nb_channels=3, width, height). "captions" is a numpy integer array of shape (nb_samples, max_caption_len) containing word index sequences representing partial captions. "next_words" is a numpy float array of shape (nb_samples, vocab_size) containing a categorical encoding (0s and 1s) of the next word in the corresponding partial caption.
model.fit([images, partial_captions], next_words, batch_size=16, nb_epoch=100)
There is no "captions" input is it referring to "partial captions"?
Is only the initial state represented by this matrix for example: "A cat on the bed". The index for "a" would be 1 and in the next words the index for "cat" would be 1?
Do we need to encode the entire sequence such that the caption "A cat on the bed" would include 4 examples with the next_state for the 4th word (bed) would be a 1 one for a stop or null feature?
Perhaps a concrete example with input data specified (not just models) as well as examples using one hot to create the vocabulary would be helpful. I'm willing to contribute these after I get my example up and running.
Thanks!