lazydroid commented on 12 May 2016 â€¢
edited
I have a data set (X, y), that's a bit large, so I have to use batch_size = 20 for training, however when I try to use fit_generator() this way:
history = model.fit_generator( datagen.flow( X, y, batch_size = BATCH_SIZE, shuffle = True),
        samples_per_epoch = len(X), nb_epoch = 15, callbacks = callbacks,
        validation_data = ( X, y ), verbose = 1, show_accuracy = True )
it seems like the whole data set is being loaded into the GPU memory for the validation, resulting in memory overflow. When I try to specify validation_data = ( X[:20], y[:20] ) it works, but I would like to validate over the whole dataset, not only the first 20 items.
Is there anything like batch_size for validation data or any other way to use it?