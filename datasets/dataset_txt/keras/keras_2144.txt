freundzi commented on 13 Jun 2017
I am trying to use batch normalization, but for some reason, even for the simplest network, when I run model.fit even for one epoch,the loss is nan and naturally no learning is performed.
For example - I use a simple model like this:
model = Sequential()
model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(16,16,3)))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(BatchNormalization())
model.add(Flatten())
model.add(Dense(2,activation='softmax'))
model.compile (loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
If I remove the batch normalization, everything works great.
I am using keras 2.0.4 and theano 0.9.0, cuda 7. I tried removing cudnn, got the same results.
I tried a diffrent axis (axis=1) when calling BN, (although this should not be right) and got the same result.
What am I doing wrong ?
Thank YOU!