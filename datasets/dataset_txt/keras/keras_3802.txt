mustgoplay commented on 30 Nov 2016
I want weight changes in one of my layers to be dependent on the current weight and the outputs. For example, let's say we're looking at xor and the input is 0 1 and the output is 1. Let's say there are no hidden layers and no bias and no sigmoid/relu/etc. This is unrealistic and very simple but is being used to explain the problem. Let's say the weights are w00=0.25 and w10=0.75 but let's only look at the 0.75 weight. I want the update to that weight to not only be dependent on the error (1-0.75) but also on w10. How do I access the 0.75 value specifically during the training with respect to the loss?
The nearest I can figure is that I would need to use an optimizer and that the value is in the params variable but I can't tell what's happening when a whole batch is being presented (i.e. how do isolate the single input) and I can't tell how to access the param to use in the update calculation.
Please make sure that the boxes below are checked before you submit your issue. Thank you!
[x ] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
[ x] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
[ x] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).