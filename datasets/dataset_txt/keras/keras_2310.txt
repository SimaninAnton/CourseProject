inspiralpatterns commented on 23 May 2017
I've been trying to insert batch normalisation inside my convnet.
Doing so, I split between Conv2D and relu. I therefore deployed something similar:
x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.01), 
           kernel_initializer='he_normal', name='enc')(in)
x = BatchNormalization(axis=-1)(x)
x = Activation('relu')(x)
and I always get nan values for training and validation loss.
It happens to be the same for the advanced activation layers, such as LeakyReLU or PReLU.
I'd like to get values instead, especially if I do get values for the metric.
Anyone knows why such a behaviour?