Contributor
sunil-at-gh commented on 21 Feb 2016
Streamlined RNN
At least in Theano, reducing the amount of computation done during the scan operation can significantly reduce the run-time. The RNN operations can be decomposed so that some of the work is done before the scan step. For example in SimpleRNN the scan step is:
h = K.dot(x, self.W) + self.b
output = self.activation(h + K.dot(prev_output, self.U))
Here h does not depend on the previous state, so it can be computed before the scan step. So you can compute h for the entire batch before the scan as:
Z = K.dot(X, self.W) + self.b
Using Z as the input to scan instead of X, the scan step reduces to:
    def step(self, z_t, prev_state_list):
        assert len(prev_state_list) == 1
        h_tm1 = prev_state_list[0]
        h_t = self.activation(z_t + K.dot(h_tm1, self.U))
        return h_t, [h_t]
Here is a Gist https://gist.github.com/sunil-at-gh/0513c1d30cb378c66b0d
showing an implementation of a streamlined version of the SimpleRNN, including code for testing. The amount of speedup depends on the sizes of the input and hidden state, and the number of time-steps. For the configuration in the test file, the training time is reduced by ~45% (cpu, iMac).
Streamlining LSTM takes a little more work, but the reward is greater: more than 50% speedup.
I have not tested these with TensorFlow.
Checklist
Please make sure that the boxes below are checked before you submit your issue. Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).