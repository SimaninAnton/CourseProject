AdityaGudimella commented on 24 Jun 2016 â€¢
edited
As far as I understand, when I write a layer, and add some weights to self.non_trainable_weights, it should add these weights to the gradient function as T.grad(loss, wrt, consider_constant=non_trainable_weights). Keras is not doing that for my layer, and I don't understand why. Please help me out.
I'm adding the non_trainable_weights in the call function of the layer
Please make sure that the boxes below are checked before you submit your issue. Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).