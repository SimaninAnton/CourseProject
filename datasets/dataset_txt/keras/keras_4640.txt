enmce commented on 6 Aug 2016 â€¢
edited
Hello!
Keras FAQ states that i can load saved model using load_model function. But i cannot perform this action due to import error as log states that:
Using Theano backend.

Using gpu device 0: GeForce GTX 660 (CNMeM is enabled with initial size: 86.4% of memory, cuDNN 5005)

Traceback (most recent call last):
  File "/home/{some_user}/{some_project}/{some_script}.py", line 5, in <module>
    from keras.models import load_model

ImportError: cannot import name 'load_model'`
I tried to save my model using ModelCheckpoint callback into hdf5 file using save_weights_only argument of this function, to load only weights for my constructed model, but it is also unsupported.
        model_save_callback = ModelCheckpoint(filepath='saved_models/{some}_model.{epoch:02d}-{val_loss:.2f}.hdf5',
                                              verbose=1, save_best_only=True, save_weights_only=True)

        self.model.fit_generator(self.data_permutator(self.dataset,
                                                      self.labels,
                                                      batch_size=16,
                                                      max_letters_to_permute=3),
                                 nb_epoch=20,
                                 samples_per_epoch=self.dataset.shape[0],
                                 validation_data=(self.validation_dataset,
                                                  self.validation_labels),
                                 callbacks=[model_save_callback]
                                 )
Is this behavior my error, e.g. i use inadequate settings or docs does not cover recent changes? I can write callback to perform desired action, which will save model on epoch end, when loss is lower than previous, but i do prefer to use error-free library-specific solutions.
I use Keras 1.0.4 version (installed using pip3), with CUDA and cuDNN on Ubuntu 16.04.
Thank you for your answer!
P.S.: Excuse me for my pretty rusty english. And this library is the best.