Contributor
arodiss commented on 28 Jun 2017
Currently all layers have following default value: kernel_initializer=glorot_uniform
According to paper the main goal of this initializer is to reduce gradient vanishing with tanh activation.
This is a paper from 2010, and now in 2017 tanh is no longer the most common activation. I have no data to prove this, but it feels like now the default activation is relu (BTW in Keras the default is linear, which is also somewhat weird IMO)
relu does not suffer from vanishing gradient the same way as tanh does, so it cannot make the most out of Xavier init. However ReLU has it's own difficulties and another famous paper suggests activation that deals precisely with this difficulties. Keras already has option for kernel_initializer=he_uniform, but I see no reason why it is not default.
One of the reasons to love Keras are "reasonable defaults", and in this case the default seems to be not reasonable.
Or I am missing something?