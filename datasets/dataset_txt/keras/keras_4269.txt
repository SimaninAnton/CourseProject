flybass commented on 30 Sep 2016 â€¢
edited
Hi,
I'm trying to get a keras model into a production setup for MLP where only traditional dense layers and a limited set of nonlinearities are included. I've looked through the batch normalization code and believe that my code below should convert a batch-normalization layer (mode 0) into a dense layer. When I make the substitution in keras, I get values that are highly correlated, but not that close in absolute terms. Have I misunderstood anything in the batch norm call? If it helps, I always use the TF backend.
import numpy as np

def batchNormToDense( bnLayer):
    gamma,beta,mean,std = bnlayer.get_weights()
    #from what i can tell in the code std is actually variance 
    epsilon = bnlayer.epsilon
    featureDimension = gamma.shape[0]
    W = np.zeros(  (featureDimension,featureDimension)   )
    B = np.zeros(  (featureDimension,)  )
    for i in range( featureDimension ):
        denom_i = np.sqrt(std[i]) + epsilon
        W[i,i] = float( gamma[i]/ denom_i )
        b[i]   = -1*(mean[i]*gamma[i]/float(denom_i)) + beta[i]
    return Dense(featureDimension, weights = [ W, b ] )
The correlation of the output from my dense layer and the batch norm layer is 99%, but I don't understand why there should be differences in the 1e^-5 absolute range.
Thanks,
Daniel