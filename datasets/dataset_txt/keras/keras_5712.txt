bryandeng commented on 21 Mar 2016
In the ICLR 2016 paper Regularizing RNNs by Stabilizing Activations, the authors propose a simple yet effective regularizer for RNNs, called "norm-stabilizer", which outperforms weight noise and dropout.
Do you think it's a good idea to have this in keras?