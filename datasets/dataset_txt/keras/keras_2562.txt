EllieElliegogo commented on 24 Apr 2017 â€¢
edited
I am a freshman on Keras,and these days I get this problem.I don' t know what cause this and how to fix it.
Please help me ,thanks a looooot!!!!!!!
anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 967, in access_term_cache
output_grads = [access_grad_cache(var) for var in node.outputs]
  /home/ymd/dl/416/polardec16copy2D.py:160: UserWarning: You are creating a TensorVariable with float64 dtype. You requested an action via the Theano flag warn_float64={ignore,warn,raise,pdb}.
  return x + w
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
modulator (Lambda)           (None, 1, 16, 16)         0         
_________________________________________________________________
noise (Lambda)               (None, 1, 16, 16)         0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 64, 16, 16)        128       
_________________________________________________________________
batch_normalization_16 (Batc (None, 64, 16, 16)        256       
_________________________________________________________________
activation_13 (Activation)   (None, 64, 16, 16)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 64, 14, 14)        36928     
_________________________________________________________________
batch_normalization_17 (Batc (None, 64, 14, 14)        256       
_________________________________________________________________
activation_14 (Activation)   (None, 64, 14, 14)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 64, 14, 14)        0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 12544)             0         
_________________________________________________________________
batch_normalization_18 (Batc (None, 12544)             50176     
_________________________________________________________________
dense_10 (Dense)             (None, 512)               6423040   
_________________________________________________________________
batch_normalization_19 (Batc (None, 512)               2048      
_________________________________________________________________
activation_15 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_11 (Dense)             (None, 256)               131328    
_________________________________________________________________
batch_normalization_20 (Batc (None, 256)               1024      
_________________________________________________________________
activation_16 (Activation)   (None, 256)               0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 256)               0         
_________________________________________________________________
dense_12 (Dense)             (None, 8)                 2056      
=================================================================
Total params: 6,647,240.0
Trainable params: 6,620,360.0
Non-trainable params: 26,880.0
_________________________________________________________________
train....
Traceback (most recent call last):

  File "<ipython-input-6-1dfb42fb5a67>", line 1, in <module>
    runfile('/home/ymd/dl/416/polardec16copy2D.py', wdir='/home/ymd/dl/416')

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/spyder/utils/site/sitecustomize.py", line 866, in runfile
    execfile(filename, namespace)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/spyder/utils/site/sitecustomize.py", line 94, in execfile
    builtins.execfile(filename, *where)

  File "/home/ymd/dl/416/polardec16copy2D.py", line 352, in <module>
    history = model.fit(dtrain_in, d, batch_size=batch_size, epochs=nb_epoch, verbose=1, shuffle=True)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/keras/models.py", line 845, in fit
    initial_epoch=initial_epoch)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/keras/engine/training.py", line 1457, in fit
    self._make_train_function()

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/keras/engine/training.py", line 1001, in _make_train_function
    self.total_loss)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/keras/optimizers.py", line 381, in get_updates
    grads = self.get_gradients(loss, params)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/keras/optimizers.py", line 47, in get_gradients
    grads = K.gradients(loss, params)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py", line 1108, in gradients
    return T.grad(loss, variables)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 555, in grad
    grad_dict, wrt, cost_name)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1317, in _populate_grad_dict
    rval = [access_grad_cache(elem) for elem in wrt]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 967, in access_term_cache
    output_grads = [access_grad_cache(var) for var in node.outputs]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1272, in access_grad_cache
    term = access_term_cache(node)[idx]

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gradient.py", line 1108, in access_term_cache
    new_output_grads)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/gof/op.py", line 711, in L_op
    return self.grad(inputs, output_grads)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/tensor/nnet/abstract_conv.py", line 1704, in grad
    d_bottom = bottom.type.filter_variable(d_bottom)

  File "/home/ymd/anaconda2/lib/python2.7/site-packages/theano/tensor/type.py", line 235, in filter_variable
    self=self))

TypeError: Cannot convert Type TensorType(float32, 4D) (of Variable AbstractConv2d_gradInputs{convdim=2, border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, 64, 16, 16), kshp=(64, 64, 3, 3), filter_dilation=(1, 1)}.0) into Type TensorType(float64, 4D). You can try to manually convert AbstractConv2d_gradInputs{convdim=2, border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=(None, 64, 16, 16), kshp=(64, 64, 3, 3), filter_dilation=(1, 1)}.0 into a TensorType(float64, 4D).
my code is:
def modulateBPSK(x):
    return -2*x +1

def addNoise(x, sigma):
    #w = K.random_normal(K.shape(x), mean=0.0, stddev=sigma)
    #w = sigma*np.random.standard_normal([1,16])
 
    w0=np.zeros([1,16,16])                      #,dtype = 'float64'
    #for i in range (train_allnum):
    w = sigma*np.float64(np.random.standard_normal([1,N]))
    for j in range (N):
        w0[0,j,:]=w

    return x + w
x_train=x
x_train=x_train.reshape(train_allnum,1,N)

gtrain=np.zeros([train_allnum,N,N])

for i in range (train_allnum):
    for j in range (N):
        gtrain[i,j,:]=  x_train[i,:,:]
dtrain_in=gtrain.reshape(256,1,N,N)

in_shape=dtrain_in.shape[1:]
# Define modulator
modulator_layers = [Lambda(modulateBPSK, 
                          input_shape=in_shape, output_shape=return_output_shape, name="modulator")]
modulator = compose_model(modulator_layers)
modulator.compile(optimizer=optimizer, loss=loss)

# Define noise
noise_layers = [Lambda(addNoise, arguments={'sigma':train_sigma}, 
                       input_shape=in_shape, output_shape=return_output_shape, name="noise")]
noise = compose_model(noise_layers)
noise.compile(optimizer=optimizer, loss=loss)
decoder_layers = [Conv2D(64, (1, 1), input_shape=in_shape,data_format='channels_first')]#, activation='relu'
decoder_layers.append(BatchNormalization(axis=1))
decoder_layers.append(Activation('tanh'))
#decoder_layers.append(Dropout(0.3))

decoder_layers.append(Conv2D(64, (3, 3), data_format='channels_first'))#,kernel_initializer='glorot_normal',kernel_regularizer=regularizers.l2(0.001)
decoder_layers.append(BatchNormalization(axis=1))
decoder_layers.append(Activation('tanh'))
decoder_layers.append(Dropout(0.3))
model.summary()
print('train....')
history = model.fit(dtrain_in, d, batch_size=batch_size, epochs=nb_epoch, verbose=1, shuffle=True)