jingweimo commented on 26 Jun 2017 â€¢
edited
In a keras example on LSTM for modeling IMDB sequence data (https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py), there is an embedding layer before input into a LSTM layer:
model.add(Embedding(max_features,128)) #max_features=20000
model.add(LSTM(128))
What does the embedding layer really do? In this case, does that means the length of the input sequence into the LSTM layer is 128?
If so, can I write the LSTM layer as:
model.add(LSTM(128,input_shape=(128,1))
But it is also noted the input X_train has subjected to pad_sequences processing:
print('Pad sequences (samples x time)')
X_train = sequence.pad_sequences(X_train, maxlen=maxlen) #maxlen=80
X_test = sequence.pad_sequences(X_test, maxlen=maxlen) #maxlen=80
It seems the input sequence length is 80?