bhawmik commented on 9 Nov 2018
I am trying to create an Seq2Seq translation model with attention in Keras.
This is what I am trying to implement.
encoder_input -> ENCODER -> encoder_output, state_h
decoder_input, state_h, context_vector - > DECODER -> decoder_output
encoder_output, decoder_output -> ATTENTION -> context_vector
As you can see, the context_vector is not defined until I define the ATTENTION block.
How do I define the model and train?
(Trying teacher enforcing training)