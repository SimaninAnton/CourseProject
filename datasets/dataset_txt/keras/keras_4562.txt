rylanchiu commented on 19 Aug 2016
As far as I know, keras has batch normalization feature. But I want the feature of layer normalization, which is suggested by this paper. In general, it is to normalize the inputs to neurons in every layers. As a result, the neurons in the same layer share equal mean and variance, which is helpful to the covariate shift problem. Now I am going to implement this feature myself. If possible, I can also PR to do some contribution for keras. I wonder how and where should I add this feature. Should I write one more layer class like Batch Normalization ? Or I can just normalize the input during feedforward? Thank you in advance!
6