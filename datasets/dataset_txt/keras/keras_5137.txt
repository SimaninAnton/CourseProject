RaffEdwardBAH commented on 21 May 2016
I'm trying to implement a custom regularization over the activation of a hidden layer, so I copied the code from ActivityRegularization as a starting point. However, I'm not sure ActivityRegularization does anything at all. It seems like the __call__ method is never being called (I added an infinite loop yet training progressed)
If I add a simple layer into a network, like this x = ActivityRegularization(l2=1000.0)(x) , the model learns exactly as it did without any regularization added to the network at all. I'm using the head version of Keras, and have tried it with such absurdity like ActivityRegularization(l2=100000.0, l1=100000.0) and observe no impact. Surely this is enough regularization that just learning to output zero is the best strategy?