wangtjwork commented on 21 Nov 2016
I'm now having something around 500000 data points. They are all texts so they could be fit into the memory together. But if I fit all of those, when I try to compile the model, it gave me killed: 9 because there's no way to fit the model into the memory.
I've tried the first 1000 points of the data and it could be ft and trained well using batch-size 150.
So I'm now thinking maybe to take in the data points 1000 at a time or maybe bigger, and fit the model one by one. However, I'm not sure at this moment how to do that. Do I call model.fit multiple times? Also if I want to train more than one epoch, will it give me the same results to train the model with n_epoch = 1 and iterate for 10 times? Is that somehow like this?
for _ in range(10):
#somehow cut the data into slices and fit them one by one
    model.fit(data_slice, label_slice ......)
19