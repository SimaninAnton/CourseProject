alansplaza commented on 26 Oct 2016 â€¢
edited
When I use keras LSTM for part-of-speech tagging, I used mask_value=0 to mask the 0 value timesteps in the input, and it worked because the output prob are all same in the input end, for example, input is [1,2,3,4,0,0,0,0], and output is [0.2,0.4,0.5,0.7,0.7,0.7,0.7,0.7], this is just for example...
But the train accuracy and train loss are calculated containing the masking timesteps, leading that the verbose log is not useful and correct.
Furthermore, will this incorrect loss influence the backpropagation algorithm?
How can I fixed this?
Thanks!