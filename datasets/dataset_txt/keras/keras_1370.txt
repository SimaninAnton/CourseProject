cjnolet commented on 29 Dec 2017 â€¢
edited
I am running an LSTM encoder/decoder framework as specified in the paper "Deep and confident prediction for time series at Uber". I'm having trouble getting the Monte Carlo dropout to work. I have reached out to @yaringal (author of the paper "A theoretically grounded application of Dropout in Recurrent Neural networks") to make sure I'm doing this properly and he has informed me that he helped implement recurrent dropout in Keras (I believe both for Theano and for Tensorflow).
So, my code looks like this (hand typing from a proprietary network so please forgive my typos, the code executes):
dropout_monte_carlo = K.function([model.layers[0].input, K.learning_phase()], [model.layers[3].output])
layer_output = dropout_monte_carlo([[X], 1])[0]
My model looks like this:
model = Sequential()
model.add(LSTM(128, activation = 'tanh', input_shape = (1, 28), implementation = 2, dropout = 0.25, recurrent_dropout = 0.25, return_sequences = True))
model.add(LSTM(32, activation = 'tanh', return_sequences = True, implementation = 2, recurrent_dropout = 0.25, dropout = 0.25))
model.add(LSTM(128, activation = 'tanh', return_sequences = True, implementation = 2, recurrent_dropout = 0.25, dropout = 0.25))
model.add(TimeDistributed(Dense(28))

model.compile(loss = 'mse', optimizer = 'adam')
The problem is that invoking dropout_monte_carlo() never changes the output. I would expect that the dropout would have given me slightly varying output such that I can do my uncertainty calculation.
I have tried both Theano and Tensorflow backends (though my understanding is that the recurrent_dropout has been dropped in the current Theano backend).