enavarrocomes commented on 18 Jan 2017
While digging deeper into how Keras implements the LSTM, I have a couple questions.
First, some background:
I use LSTM for time series regression, in the fashion of sequences (training) to integer (target). The sequences are obtained rolling a moving window over the time series.
I use stateful mode to share the cell parametres across batches. Should I though? Even tho future and past inputs should be correlated, the fact of sliding a window might destroy that relationship.
Let's say I have 1000 sequences of length 4 (4 timesteps?).
I use the following toy model:
model = Sequential() model.add(LSTM(32, batch_input_shape=(1, 4, 1), stateful=True, return_sequences=True)) model.add(LSTM(32, batch_input_shape=(1, 4, 1), stateful=True, return_sequences=False)) model.add(Dense(1,activation='linear')) model.compile(loss='mean_squared_error', optimizer='adam')
My first sequence is x = [50,43,61,12] and first target is 14.
Is the data fed into the network using this fashion?:

Where the sequence is fed integer-by-integer to a total of 4 LSTM blocks.
I understand that if my batch_size = 1, the weight matrices will be updated and then carry on with the next example.
If my batch_size is bigger than that, the loss of every training example is summed and then the weights updated.
Should I use a TimeDistributed(Dense) layer here? That would only be if I was doing something like:
train 50, target 14
train 43, target 14... and so on, right?
I am just trying to see if I understand it right. Any comment is highly appreciated!
1