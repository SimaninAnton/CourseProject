brunoalano commented on 14 May 2016 â€¢
edited
Hello,
I understand that LSTM layers expect a 3-dimensional input with the following shape: (batch size, sequence length, features).
But for example, I have a sample text: "I love dogs and love cats". Then I can tokenize and transform it into numerical features, for example: [5, 81, 192, 32, 81, 431]. The sequence length is 6. But suppose that the biggest sentence length is 20, then I use the pad_sequences method to pad with the maximum length, and that become: [0, 0, ..., 5, 81, 192, ..., 431].
So, what I'm doing now is something like that:
# Generate Sample Dataset (like a text input)
X_train = np.random.randint(55, size=(32, 50)) # (batch size, max sequence length), with `55` words
y_train = np.random.randint(100, size=(32, 70)) # (batch size, max sequence length), with `100` words

# Input Embedding
# ---------------
# Build a Sequential Model
input_embedding_model = Sequential()

# input_dim: maximum index integer on text
# output_dim: dimension of dense output
# input_length: since we are padding the sequence, it's constant
input_embedding_model.add(
    Embedding(input_dim=55, input_length=50, output_dim=128)
)
input_embedding_model.compile('rmsprop', 'mse')

# Output Embedding
# ----------------
# Build a Sequential Model
output_embedding_model = Sequential()

# input_dim: maximum index integer on text
# output_dim: dimension of dense output
# input_length: since we are padding the sequence, it's constant
output_embedding_model.add(
    Embedding(input_dim=100, input_length=70, output_dim=128)
)
output_embedding_model.compile('rmsprop', 'mse')
So, after that, I do this to transform into a 3D matrix:
# Convert X_train and y_train into 3d
X_train_fix = input_embedding_model.predict(X_train)
y_train_fix = output_embedding_model.predict(y_train)
Ok, but, how can I transform this into a matrix of shape (sequence length, features)? Should I copy this vector sequence_length times? Have some function to do that on numpy or Keras? What I'm doing is right?