Blockost commented on 6 Jun 2017
Hi,
My dataset is basically a number of pairs of title-synopis from movies. I'm trying to find a mapping between synopses and titles so that I can summarize a synopsis into a coherent and creative title.
lstm_units = 1024
dropout_value = 0.2
model_depth = 2
model = Sequential()
    
embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LENGTH, , mask_zero=True)
activation_layer = Activation(activation='softmax')
                              
model.add(embedding_layer)
for i in range(model_depth):
    model.add(LSTM(lstm_units, return_sequences=True)) #, recurrent_dropout=dropout_value, dropout=dropout_value
    #model.add(Dropout(dropout_value))
model.add(TimeDistributed(Dense(vocab_size)))
model.add(activation_layer)

model.compile(loss='sparse_categorical_crossentropy', optimizer=(RMSprop()), metrics=['accuracy'], sample_weight_mode='temporal')
I didn't use any regularization mechanism as I want to overfit a small subset (100 pairs while the original dataset is ~200k) to see if my model can actually learn something useful.
I used mask_zero=True on my embedding layer to mask zero in input sequence and sample_weight to mask zero from the loss function, i.e sparse_categorical_crossentropy.
sample_weight is basically telling the loss function to ignore every token after the 3rd one (as my target sequence is much smaller than my input sequence) during loss calculation.
I don't understand how the loss can keep improving while the accuracy is that unstable:
I took a look at my prediction and the vast majority of words that the model predicted is the, <eos> (the end-of-sequence token) and <0> (the pad token). Thus, if the target sequence contains those words (which are common) the accuracy grows, if not the accuracy goes down, is that right ? But what about the loss then ?
Thanks for any advice/tips on how to improve my model