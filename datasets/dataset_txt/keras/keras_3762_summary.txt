Training keras LSTM model with variable-length-sequence: mask or pading or batch_size = 1 or group ?