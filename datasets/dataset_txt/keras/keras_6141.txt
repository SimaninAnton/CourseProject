haoqi commented on 10 Jan 2016
Hi guys,
I am trying to implement a DNN with shared hidden layer but with different output layer on the top towards different task related to different training data. Thus, I shuffle training data to get all different training data in one mini-bach.
Also, I need to modify the back propagation a little bit, when a training data is presented to the DNN trainer, only the shared hidden layers and corresponding top softmax layer are updated. How can I do this in Keras? It is kind of hard for me to handle this, hopefully you can give me some help.
For example, we have two language: English(lang1) and French(lang2), and want to obtain shared information of language and get different output towards different language.
Thanks a lot!!