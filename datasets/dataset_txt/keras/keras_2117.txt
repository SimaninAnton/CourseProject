fotisj commented on 17 Jun 2017
Please add an attribute to Tokenizer which allows you to define your own function how to tokenize the text (like the tokenizer attribute of sklearn.feature_extraction.text.CountVectorizer).
Rationale: In NLP tokenization is a complex, language-dependent task[1] often solved by creating complex rule based functions or training models on large corpora, see for example in Python the nltk.tokenize module[2]. It would be great to be able to use the existing tools for all these different languages in Keras' preprocessing.
[1] Stanford IR book on tokenization
[2] NLTK tokenization module