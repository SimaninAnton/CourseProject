Rachnog commented on 15 Mar 2016
I have a sentence tagging problem and I want to use Embedding layer to use vector word representation during learning.
The problem is that I have only around 5000 training examples with tagged words, but actually my dataset is much larger (~100M examples) and I want to create embedding according to this larger dataset, using vocabulary from it.
Is it possible to pretrain Embedding first on this 100M dataset and then use these embeddings to train on my 5K tagged dataset?
Thank you for answer!
Please make sure that the boxes below are checked before you submit your issue. Thank you!
[+] Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
[+] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
[?] Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short). --> My model is pretty simple and the question is about general possibility of pretraining Embedding layer :)