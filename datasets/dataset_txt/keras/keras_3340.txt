stepjam commented on 29 Jan 2017 â€¢
edited
Hi,
There seems to be a problem when you combine 2 models (sub-models) that use batch normalization into another model (master-model), and then try and train one of the sub-models. When removing the batch normalization, it works as expected.
Below is a code snipped to reproduce.
When you run the example, you should see:
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'combined_input' with dtype float
Thank you in advanced.
from keras.models import Model
from keras.layers import Dense, Input
from keras.layers.core import Activation
from keras.layers.normalization import BatchNormalization
from keras.optimizers import Adam
import numpy as np

LR = 0.0002
BATCH_SIZE = 128

def generator_model():
    input = x = Input(shape=(100,), name='generator_input')
    x = Dense(40)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dense(10)(x)
    x = Activation('tanh')(x)
    return Model(input, x, name='generator')


def discriminator_model():
    input = x = Input(shape=(10,), name='discriminator_input')
    x = Dense(60)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dense(1, activation='sigmoid')(x)
    return Model(input, x, name='discriminator')


def combined_model(generator, discriminator):
    input = Input(shape=(100,), name='combined_input')
    x = generator(input)
    dcganOutput = discriminator(x)
    return Model(input=input, output=dcganOutput)


adam = Adam(lr=LR, beta_1=0.5)

generator = generator_model()
discriminator = discriminator_model()
generator.summary()
discriminator.summary()
generator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])
discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

dcgan = combined_model(generator, discriminator)
dcgan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

train_data = np.zeros((BATCH_SIZE, 10), dtype=np.float32)
train_labels = np.ones(BATCH_SIZE)

# HERE is when the error occurs
discriminator_loss = discriminator.train_on_batch(x=train_data, y=train_labels)