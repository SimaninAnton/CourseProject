Imorton-zd commented on 10 Sep 2015
Opinions/Views would be highly appreciated!
model = Sequential()
model.add(Embedding(max_features, embedding_dims)) # embed into dense 3D float tensor (samples, maxlen, 256)
model.add(Reshape(1, maxlen, embedding_dims)) # reshape into 4D tensor (samples, 1, maxlen, 256)
model.add(Convolution2D(nb_filters, 1, filter_length, filter_length, border_mode='full')) 
model.add(Activation('relu'))
model.add(MaxPooling2D(poolsize=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
output_size = nb_filters * (((maxlen - filter_length) / 1) + 1) / 2*(((embedding_dims - filter_length) / 1) + 1) / 2
 # We add a vanilla hidden layer:
model.add(Dense(output_size, hidden_dims))
model.add(Dropout(0.25))
model.add(Activation('relu'))

 # We project onto a single unit output layer, and squash it with a sigmoid:
model.add(Dense(hidden_dims, nb_classes))
model.add(Activation('softmax'))


Traceback (most recent call last):
  File "D:\workspace\search\src\test\cnn2D.py", line 284, in <module>
    loss,accuracy = model.train_on_batch(X_train[i*batch_size:(i+1)*batch_size], Y_train[i*batch_size:(i+1)*batch_size],accuracy=True)
  File "D:\Python27\lib\site-packages\keras\models.py", line 429, in train_on_batch
    return self._train_with_acc(*ins)
  File "D:\Python27\lib\site-packages\theano\compile\function_module.py", line 606, in __call__
    storage_map=self.fn.storage_map)
  File "D:\Python27\lib\site-packages\theano\compile\function_module.py", line 595, in __call__
    outputs = self.fn()
ValueError: dimension mismatch in args to gemm (32,83232)x(76832,250)->(32,250)
Apply node that caused the error: GpuDot22(GpuReshape{2}.0, <CudaNdarrayType(float32, matrix)>)
Inputs types: [CudaNdarrayType(float32, matrix), CudaNdarrayType(float32, matrix)]
Inputs shapes: [(32, 83232), (76832, 250)]
Inputs strides: [(83232, 1), (250, 1)]
Inputs values: ['not shown', 'not shown']

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.