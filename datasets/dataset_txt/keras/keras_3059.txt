Contributor
pasky commented on 7 Mar 2017
The image_ocr code contains the typical mistake of trying to construct a bidirectional RNN network manually and failing to notice that the go_backwards sequence output is unfortunately reversed. Simplifying the model code to
    # Two layers of bidirecitonal GRUs
    # GRU seems to work as well, if not better than LSTM:
    gru_1 = Bidirectional(GRU(rnn_size, return_sequences=True, init='he_normal', name='gru1'), merge_mode='sum')(inner)
    gru_2 = Bidirectional(GRU(rnn_size, return_sequences=True, init='he_normal', name='gru2'), merge_mode='concat')(gru_1)

    # transforms RNN output to character activations:
    inner = Dense(img_gen.get_output_size(), init='he_normal', name='dense2')(gru_2)
therefore enables the model of actually taking advantage of the backwards GRU. Norm. ED (TF):
Epoch old fixed
10 0.082 0.039
15 0.074 0.022
20 0.039 0.021
25 0.034 0.008
(Not creating a PR myself due to keras-1 PR freeze.)