se210 commented on 25 Apr 2017 â€¢
edited
I have a sequence of 2-dimensional vector which is fed into a simple model which has TimeDistributed(Dense) layers. The 2D vectors are sliced in chunks of sequence_length.
My model:
x = Input(shape=(sequence_length, 2,), name='input')

y = TimeDistributed(Dense(128, activation='relu', name='fc1'))(x)
y = TimeDistributed(Dense(128, activation='relu', name='fc2'))(y)
y = TimeDistributed(Dense(6, activation='softmax', name='predictions'))(y)

Model = Model(input=x, output=y)
The model does not learn anything (constant output regardless of input) when the sequence_length is set to 1, but the model is able to learn something when sequence_length is set to something bigger.
My understanding of TimeDistributed was that it applies the underlying layer independently to dimension one (in my case sequence_length dimension), so it should not matter how long the sequence is. In other words, my understanding of TimeDistributed was that it would effectively apply the following model to 2D vector at each timestep along sequence_length:
x = Input(shape=(2,), name='input')

y = Dense(128, activation='relu', name='fc1')(x)
y = Dense(128, activation='relu', name='fc2')(y)
y = Dense(6, activation='softmax', name='predictions')(y)

Model = Model(input=x, output=y)
Is this a bug? Thank you for your help.
I am using the latest version of Keras 2.0.3 with Tensorflow-GPU 1.0.1.
1