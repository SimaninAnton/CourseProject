shuvrobiswas commented on 7 Nov 2015
Hi,
I just wanted to confirm that the implementation of Nesterov based gradient descent: https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L74 was correct.
Based on my understanding, Nesterov based gradient descent should work as follows:
Let's say you start off a point p1 in weight space. Take a step in the direction of accumulated gradients. Now you are at point p2.
At point p2, compute the gradient and take a step in the direction of that computed gradient, to end up at point p3.
Source: https://class.coursera.org/neuralnets-2012-001/lecture/63 (Hinton lecture, 5:30 onward is the relevant bit)
So from my understanding of the code I linked to:
new_p = p + self.momentum * v - lr * g
new_p = p + self.momentum * (self.momentum * m - lr * g) - lr * g
new_p = p + self.momentum**2 * m - (1 + self.momentum) * lr * g
which is essentially just normal momentum with a slightly different constant structure?
I think the formula is on the right track:
new_p = p + self.momentum * v - lr * g
but g needs to be computed at new_p: new_p = p + self.momentum * v, and then there needs to be another update: newer_p = new_p - lr * g
EDIT: Perhaps we can think of:
new_p = p + self.momentum * v - lr * g
In the following way:
- lr * g 
is for the previous weight update, and
self.momentum * v
is for the current weight update.
Under that interpretation I think all is good. Could you please confirm?
Brilliant package btw, very well put together!
Best Regards,
Shuvro Biswas