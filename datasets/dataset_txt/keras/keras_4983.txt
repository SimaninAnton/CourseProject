Contributor
githubnemo commented on 12 Jun 2016
Since K.std(x) is just sqrt(var(x)) under the hood the gradient will be inf if the input vector x is 0.
This is problematic when using the BatchNormalization layer and having a model that sometimes produces zero vectors.
Wouldn't it be better to expose K.var in the backends and use that instead of K.std so that
the code can do K.sqrt(K.var(x) + eps) and does not have to add eps to x?
1
1
1
1