jerrypaytm commented on 2 Aug 2017
I'm trying to train a LSTM model using Tensorflow in distributed mode.
The model definition is quite simple right now:
model = Sequential()
model.add(Embedding(N, embedding_vector_length, input_length=max_sequence_length))
model.add(LSTM(512))
model.add(Dense(1024))
model.add(Dense(label.size, activation='softmax'))
optimizer = RMSprop(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
When I run the above model using Tensorflow, I got the following error:
InvalidArgumentError: Cannot assign a device for operation 'lstm/bias': Operation was explicitly assigned to /job:ps/task:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.
  [[Node: lstm/bias = VariableV2[_class=["loc:@lstm/bias"], container="", dtype=DT_FLOAT, shape=[2048], shared_name="", _device="/job:ps/task:0"]()]]
But if I replace the model definition to:
model = Sequential()
model.add(Dense(100, input_shape(max_sequence_length,)))
model.add(Dense(1024))
model.add(Dense(label.size, activation='softmax'))
optimizer = RMSprop(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
It works fine. That is without the LSTM layer (and the embedding layer), the error goes away.
Is it a bug in the LSTM layer or there is something I miss to configure?
Thank you!