siddheshk commented on 17 Jan 2017
Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on StackOverflow or join the Keras Slack channel and ask there instead of filing a GitHub issue.
Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
I need to define layers in a loop for generating class specific attention models. (Loop as number of classes can vary)
I have tried something similar to the following:
models = [{}]*num_classes

for i in range(num_classes):
    model[i]['input_layer'] = Input(shape=(max_sentences, img_h,), dtype='int32', name='input_layer')
    model[i]['attention'] = AttentionLayer(name='attention_'+str(i))(model[i]['input_layer'])
    .......
    model[i]['model'] = Model(input=[model[i]['input_layer']], output=[model[i]['softmax_layer']], name="model_"+str(i))
Here AttentionLayer is a custom layer.
Assume that there are 6 classes.
When I print the summary for model[0], it shows the name of the AttentionLayer to be 'attention_5' instead of 'attention_0' (as the value of i when the loop finishes will be 5). However the memory allocations are different for each AttentionLayer.
Is this a bug?
Please let me know.
Thanks