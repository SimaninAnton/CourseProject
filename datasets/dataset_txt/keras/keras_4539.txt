glouppe commented on 24 Aug 2016
Hi!
In <=0.3.3, I used to be able to define something like
D = Sequential()
D.add(...)
...

G = Sequential()
G.add(...)
...
G.compile(...) # necessary but irrelevant

DG = Sequential()
DG.add(G)  
D.trainable = False
DG.add(D) # layers of D are frozen for GD

D.compile(some_loss)
DG.compile(some_other_loss)
And then be able to train jointly D and DG (writing some small custom loop for alternating updates), such that updates on DG only change the weights of the inner model G.
In master, the same code has different behaviour -- namely, D.trainable = False seems to no longer be taken into account, resulting in DG.trainable_weights to include both G and D weights. Is this new behaviour expected or this a bug?