mbollmann commented on 31 May 2016
I am training a sequence-to-sequence model with TimeDistributed(Dense()) as the final layer. As far as I can see, if I compile my model with metrics=['accuracy'], the accuracy is calculated as the average across all timesteps.
What I would like to have is an accuracy metric that looks at full sequences instead of individual timesteps, i.e., the accuracy for an output sequence should be 1 iff all timesteps have been predicted correctly, and 0 otherwise.
Is it possible to write a function to do this which can be used with metrics=...?
I have a really hard time figuring out how to write a custom function for use with metrics=..., since I've never worked with tensor functions directly, and I'm also unsure what exactly gets passed to that function. Any help would be appreciated.