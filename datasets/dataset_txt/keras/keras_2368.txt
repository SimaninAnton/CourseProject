hkmztrk commented on 16 May 2017
Hello,
I'm trying to monitor loss function for a grid search with the following code,
 for batchind in range(len(batch_size)):
            batchsz= batch_size[batchind]
            for epochind in range(len(epochs)):
                gridmodel = baseline_model(hidden_neurons, train_pair.shape[1])
                epoch = epochs[epochind]
                gridres = gridmodel.fit(np.array(train_pair),np.array(train_Y),batch_size=batchsz, nb_epoch=epoch, validation_data=(np.array(val_pair), np.array(val_Y)))
                predicted_labels = gridmodel.predict(np.array(val_pair))
                rperf = get_cindex(val_Y, predicted_labels)
                f.write(str(epochind) + "\t" + str(batchind) + "\t" + str( foldind) + "\t" + str(rperf) + "\n")
                plotLoss(gridres, epochind, batchind, foldind)
plotLoss function is like this:
def plotLoss(history, epochind, batchind, foldind):
 figname = str(epochind) + "_" + str(batchind) + "_" + str( foldind) + "_" + str(time.time()) +".png"
 plt.plot(history.history['loss'])
 plt.plot(history.history['val_loss'])
 plt.title('model loss')
 plt.ylabel('loss')
 plt.xlabel('epoch')
 plt.legend(['train', 'test'], loc='upper left')
 
 plt.savefig("figures/"+figname )
My problem is even though I have new figures for each batchsize and epoch setting, each figure displays previous loss values on top of each other. How can I possibly prevent that?