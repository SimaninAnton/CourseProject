Contributor
EderSantana commented on 22 Aug 2015
I've been doing some local experiments and there is no problem to have a layer returning a list at get_output as long as the next layer knows how to handle it. Also, I created a Lambda layer that is simply:
class Lambda(MaskedLayer):
    def __init__(self, func, params=None):
        super(Lambda, self).__init__()
        self.func = func
        if params is not None:
             self.params=params
    def get_output(self, train=False):
        X = self.get_input(train)
        return self.func(X)
That function can be used to slice the list and communicate with other regular layers that expect theano.tensors as input for example.
I believe that Lambda layers together with a new Graph merge_mode='list' option would make Keras even more expressive. By merge_mode='list' I simply mean creating a list with all the inputs, letting the next layer handle its pieces.
--- Example ---
I'm writing a Spatial Transformer Network using Keras. Right now there is an example at my edersantana/seya/examples repo. That layer does the following:
gets a localization network as parameter
gets an input
pass input to localization network and gets its output
return the input processed with the localization network's output
If that Spatial transformer layer could receive a list as its merge_mode, the localization part could be an independent node in the graph and its output could be used somewhere else. It is not always easy to use merge_mode='concat' and slice the resulting tensor since the proper shapes would have to be passed to the next layer, cluttering the code.
What do you guys think?