jerryli1981 commented on 6 Nov 2015
Hi All,
I would like to know how to write code to conduct gradient back propagation. Like Lua does below,
local sim_grad = self.criterion:backward(output, targets[j])
local rep_grad = self.MLP:backward(rep, sim_grad)
Keras's example teach me how to construct sequential model like below,
model = Sequential()
model.add(Dense(128, input_shape=(784,)))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(10))
model.add(Activation('softmax'))
However, it is not enough for me. I need generate gradient for this model. How can write code to control sequential model backward propagation?
Thanks