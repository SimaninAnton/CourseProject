lhk commented on 3 Nov 2016
I've written a custom Layer for Keras: It combines a 2D convolution with a GRU and calculates the gates for the GRU with a convolution.
The new layer seems to work nicely. The code still needs to be cleaned up, but it is rather straightforward, the code is here:
http://pastebin.com/P7ZeiVjz
Here's how this would be used in a model:
inputs = Input((1, 40, 40, 40))

# now reshape to a sequence
reshaped = Reshape((40, 1, 40, 40))(inputs)

# now create a convnet which is fed the time-distributed data
# this model will perform a segmentation on a 2D slice of data.

conv_inputs = Input((1, 40, 40))
base_filters_size = 64
conv1 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv_inputs)
#conv1 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv1)
conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)
#conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same')(conv1)
conv1 = Convolution2D(3, 3, 3, activation='relu', border_mode='same')(conv1)
convmodel = Model(input=conv_inputs, output=conv1)

convmodel.summary()

#apply the segmentation to each layer
time_dist=TimeDistributed(convmodel)(reshaped)
temp_model=Model(input=inputs, output=time_dist)
temp_model.summary()

# now a model with the two GRUs which move in different directions

up=CGRU(go_backwards=False, return_sequences=True, name="up", input_shape=[40,3,40,40])(time_dist)
down=CGRU(go_backwards=True, return_sequences=True, name="down", input_shape=[40,3,40,40])(time_dist)

merged_updown=merge([up,down], mode="concat", concat_axis=2)
reshaped=Reshape((40,2,40,40))(merged_updown)

mergeconv_inputs=Input((2,40,40))
mergeconv=Convolution2D(1,3,3, activation="sigmoid", border_mode="same")(mergeconv_inputs)
merge_model=Model(input=mergeconv_inputs, output=mergeconv)

final_time_dist=TimeDistributed(merge_model)(reshaped)

output=Reshape((1,40,40,40))(final_time_dist)



model=Model(input=inputs, output=output)
print(model.summary())
This code runs perfectly fine on Theano.
The output of the final model.summary() is
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_4 (InputLayer)             (None, 1, 40, 40, 40) 0                                            
____________________________________________________________________________________________________
reshape_4 (Reshape)              (None, 40, 1, 40, 40) 0           input_4[0][0]                    
____________________________________________________________________________________________________
timedistributed_3 (TimeDistribute(None, 40, 3, 40, 40) 19971       reshape_4[0][0]                  
____________________________________________________________________________________________________
up (CGRU)                        (None, 40, 1, 40, 40) 0           timedistributed_3[0][0]          
____________________________________________________________________________________________________
down (CGRU)                      (None, 40, 1, 40, 40) 0           timedistributed_3[0][0]          
____________________________________________________________________________________________________
merge_2 (Merge)                  (None, 40, 2, 40, 40) 0           up[0][0]                         
                                                                   down[0][0]                       
____________________________________________________________________________________________________
reshape_5 (Reshape)              (None, 40, 2, 40, 40) 0           merge_2[0][0]                    
____________________________________________________________________________________________________
timedistributed_4 (TimeDistribute(None, 40, 1, 40, 40) 19          reshape_5[0][0]                  
____________________________________________________________________________________________________
reshape_6 (Reshape)              (None, 1, 40, 40, 40) 0           timedistributed_4[0][0]          
====================================================================================================
Total params: 19990
But with Tensorflow as backend, somehow the shape inference is messed up:
There is this error message:
ValueError: Shapes (?, ?, 40, 40) and (40, ?, 40) are not compatible
This is really strange. It's not just a matter of reshaping, the number of dimensions is wrong, too.
I think I'm missing some important configuration.
Could you point me in the right direction ?
Here is the whole error message:
ValueError Traceback (most recent call last)
in ()
28 # now a model with the two GRUs which move in different directions
29
---> 30 up=CGRU(go_backwards=False, return_sequences=True, name="up", input_shape=[40,3,40,40])(time_dist)
31 down=CGRU(go_backwards=True, return_sequences=True, name="down", input_shape=[40,3,40,40])(time_dist)
32
/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in call(self, x, mask)
513 if inbound_layers:
514 # this will call layer.build() if necessary
--> 515 self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
516 input_added = True
517
/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in add_inbound_node(self, inbound_layers, node_indices, tensor_indices)
571 # creating the node automatically updates self.inbound_nodes
572 # as well as outbound_nodes on inbound layers.
--> 573 Node.create_node(self, inbound_layers, node_indices, tensor_indices)
574
575 def get_output_shape_for(self, input_shape):
/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in create_node(cls, outbound_layer, inbound_layers, node_indices, tensor_indices)
148
149 if len(input_tensors) == 1:
--> 150 output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
151 output_masks = to_list(outbound_layer.compute_mask(input_tensors[0], input_masks[0]))
152 # TODO: try to auto-infer shape if exception is raised by get_output_shape_for
/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc in call(self, x, mask)
211 constants=constants,
212 unroll=self.unroll,
--> 213 input_length=input_shape[1])
214 if self.stateful:
215 self.updates = []
/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in rnn(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length)
1193 parallel_iterations=32,
1194 swap_memory=True,
-> 1195 sequence_length=None)
1196
1197 if nb_states > 1:
/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)
1023 shape = _state_size_with_prefix(
1024 output_size, prefix=[const_time_steps, const_batch_size])
-> 1025 output.set_shape(shape)
1026
1027 final_outputs = nest.pack_sequence_as(
/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shape(self, shape)
406 this tensor.
407 """
--> 408 self._shape = self._shape.merge_with(shape)
409
410 @Property
/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)
568 except ValueError:
569 raise ValueError("Shapes %s and %s are not compatible" %
--> 570 (self, other))
571
572 def concatenate(self, other):
ValueError: Shapes (?, ?, 40, 40) and (40, ?, 40) are not compatible