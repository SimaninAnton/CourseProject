rmalouf commented on 24 Nov 2015
The documentation says: "Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers" so that
from keras.layers.core import Activation, Dense

model.add(Dense(64))
model.add(Activation('tanh'))
is equivalent to:
model.add(Dense(64, activation='tanh'))
However, this doesn't seem to be the case. I'm getting very different results using the two methods (namely, the second converges and the first doesn't), and as far as I can tell, the problem is that the first version applies both the default activation for the previous layer and the activation specified in the Activation layer. Only in the case where the previous layer's activation is linear (which happens to be the case for Dense) do the two forms end up being equivalent. Or am I missing something?