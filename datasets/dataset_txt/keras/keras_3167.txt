fortuin commented on 20 Feb 2017
I built a small LSTM model with multiple outputs using the Theano backend and functional API. The dimensions of the inputs are not in the right order for the LSTM, which is why the inputs have to be permuted before being passed on to the LSTM layer. The model construction looks like this:
from keras.models import Model, Sequential
from keras.layers import  Dense, LSTM, Permute, Input
from keras import backend as K

main_input = Input(shape=(3,100))

lstm = Sequential()
lstm.add(Permute(dims=(1,2), input_shape=(3,100)))
lstm.add(LSTM(output_dim=512, consume_less="gpu"))

lstm_out = lstm(main_input)

out1 = Dense(5, activation="softmax")(lstm_out)
out2 = Dense(5, activation="softmax")(lstm_out)

model = Model(input=main_input, output=[out1, out2])

model.compile(loss="categorical_crossentropy", optimizer="sgd")
The model compiles without errors and also works in training and prediction. However, when I wanted to assess the hidden activations of the LSTM layer with
get_lstm_activations = K.function([model.input, K.learning_phase()], [model.layers[1].layers[1].output])
I get the following error:
MissingInputError: ("An input of the graph, used to compute DimShuffle{0,1,2}(permute_input_2), was not provided and not given a value.Use the Theano flag exception_verbosity='high',for more information on this error.", permute_input_2)
If I try to get the same tensor not as the output of the LSTM layer, but as the input of the next layer, like
get_lstm_activations = K.function([model.input, K.learning_phase()], [model.layers[2].input])
I don't get any errors and it indeed seems to work. That seems really counterintuitive to me and I came up with this alternative approach mostly by chance. Can anyone explain why the intermediate output tensor cannot be obtained and whether this is a bug in the functional API or the consequence of an intended behavior?