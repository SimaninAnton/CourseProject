Contributor
patyork commented on 18 Apr 2015
Given the discussion of weight initializations, any opinions on how an autoencoder architecture should be added?
-Stopping conditions for each in a pre-created set of layers [vs.] manual saving -> layer addition -> compile -> start next level of training?
-Noise addition (perhaps as a layer) over a distribution?
-Used only as a pretraining device [vs.] allow backproagation to create and encoder + decoder?