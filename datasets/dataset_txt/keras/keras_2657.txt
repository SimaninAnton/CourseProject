kzk2000 commented on 14 Apr 2017
Question
Paper A Robust Adaptive Stochastic Gradient Method for Deep Learning proposes the AdaSecant optimizer. Its code is available in https://github.com/sotelo/scribe/blob/master/algorithms.py but the code is based on blocks + theano.
Is there an easy way to convert this code into a Keras optimizer?