phquang commented on 27 Aug 2016 â€¢
edited
I am working with a video labeling problem where given a sequence of frames, I have to output a set of labels, one for each frame. I am using a custom loss function for each frame:
def custom_loss(y_true, y_pred):
  #y have shape (1,1000) for example
  loss = 0.0
  update_loss
  return loss
I build a GRU model to read the sequence and return the sequence of output and the above custom_loss works fine for the case there is only one timestep (sequence length is 1).
For the general case, the loss of the sequence would be
def sequence_loss(y_true, y_pred):
  #y have shape (1, #timesteps, 1000)
  loss = 0.0
  num_time_steps = y_pred.shape[1]
  for i in range(num_time_steps):
    loss += custom_loss(y_true[0,i,:], y_pred[0,i,:])
  loss /= num_time_steps
  return loss
The problem is that the length of the sequence varies, therefore num_time_steps is not fixed and the sequence_loss can't be used for optimization.
I tried a solution that pads each sequence so that they have length of MAX_LEN, however, it is not practical because the sequence's length varies alot, the maximum length is about 5000 where the minimum length is only 5. It also takes about 5 minutes to calculate the sequence_loss for one sequence of MAX_LEN so training with this way is not efficient.
Another solution I am trying is that randomly sample a fixed number of frames for each video, e.g. 128, and do training on the sample but I think it may not work well when testing because of the difference in length of the data.
So, is there any workaround for this case?
Thank you