aurora1625 commented on 8 Dec 2016 •
edited
Hello,
I am trying to implement an attention model based on LSTM to do relation classification based on SemEval 2010 task 8 dataset. An example of the sample looks like:
28 "This <e1>article</e1> gives details on 2004 in <e2>music</e2> in the United Kingdom, including the official charts from that year."

Message-Topic(e1,e2)
The number is just index, which is not important. Given this sentences, the classifier is supposed to classify this sentence to Message-Topic(e1,e2) class. The dataset contains 19 classes.
For this sentence:
Poor care led to her <e1>death</e1> from <e2>pneumonia</e2>.
My design is to feed this sentence into a LSTM model and with the output of the LSTM, attend to the entity and the preposition, to select the important words with high importance to the entities and the preposition. (The preposition matters in relation classification, which is my own observation.)
I designed this model based on the famous paper
Rocktäschel, Tim, et al. "Reasoning about entailment with neural attention." arXiv preprint arXiv:1509.06664 (2015).
I try to modify this implementation to fit my own model.
The attention code is:
main_input = Input(shape=(N,), dtype='int32', name='main_input')
x = Embedding(output_dim=opts.emb, input_dim=opts.max_features, input_length=N, name='x')(main_input)
drop_out = Dropout(0.1, name='dropout')(x)
lstm_fwd = LSTM(opts.lstm_units, return_sequences=True, name='lstm_fwd')(drop_out)
lstm_bwd = LSTM(opts.lstm_units, return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)
bilstm = merge([lstm_fwd, lstm_bwd], name='bilstm', mode='concat')
drop_out = Dropout(0.1)(bilstm)
h_n = Lambda(get_H_n, output_shape=(k,), name="h_n")(drop_out)
Y = Lambda(get_Y, arguments={"xmaxlen": L}, name="Y", output_shape=(L, k))(drop_out)
Whn = Dense(k, W_regularizer=l2(0.01), name="Wh_n")(h_n)
Whn_x_e = RepeatVector(L, name="Wh_n_x_e")(Whn)
WY = TimeDistributed(Dense(k, W_regularizer=l2(0.01)), name="WY")(Y)
merged = merge([Whn_x_e, WY], name="merged", mode='sum')
M = Activation('tanh', name="M")(merged)
alpha_ = TimeDistributed(Dense(1, activation='linear'), name="alpha_")(M)
flat_alpha = Flatten(name="flat_alpha")(alpha_)
alpha = Dense(L, activation='softmax', name="alpha")(flat_alpha)

Y_trans = Permute((2, 1), name="y_trans")(Y)  # of shape (None,300,20)

r_ = merge([Y_trans, alpha], output_shape=(k, 1), name="r_", mode=get_R)

r = Reshape((k,), name="r")(r_)

Wr = Dense(k, W_regularizer=l2(0.01))(r)
Wh = Dense(k, W_regularizer=l2(0.01))(h_n)
merged = merge([Wr, Wh], mode='sum')
h_star = Activation('tanh')(merged)
out = Dense(3, activation='softmax')(h_star)
My problems are:
If I want to convert this code to Theano backend Keras code,
h_n = Lambda(get_H_n, output_shape=(k,), name="h_n")(drop_out)
Y = Lambda(get_Y, arguments={"xmaxlen": L}, name="Y", output_shape=(L, k))(drop_out)
Both h_n and Y take in the output of drop_out, and then later merged together. Should I make two Sequential() layer? Then how to take in the same input from the drop_out?
I am quite new to Keras, I think it provides great convenience for newbies like me, but sometimes, I don't know whether I should go deeper to write my own layer or learn new tricks to make the program work.
Thank you for your help.