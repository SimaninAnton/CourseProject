around1991 commented on 15 Dec 2015
Hi, I've got another bug report, sorry :(
The following sample code causes an error when compiling:
from keras.models import Sequential
encoder = Dense(input_dim=10, output_dim=2)
decoder = Dense(input_dim=2, output_dim=10)
model = Sequential()
model.add(Dense(input_dim=20, output_dim=10))
model.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False))
model.compile(loss='mse', optimizer='sgd')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/auto/homes/kc391/event_rnn/env/bin/keras/keras/models.py", line 432, in compile
    train_loss)
  File "/auto/homes/kc391/event_rnn/env/bin/keras/keras/optimizers.py", line 79, in get_updates
    grads = self.get_gradients(loss, params)
  File "/auto/homes/kc391/event_rnn/env/bin/keras/keras/optimizers.py", line 47, in get_gradients
    grads = K.gradients(loss, params)
  File "/auto/homes/kc391/event_rnn/env/bin/keras/keras/backend/theano_backend.py", line 365, in gradients
    return T.grad(loss, variables)
  File "/auto/homes/kc391/event_rnn/env/bin/Theano/theano/gradient.py", line 545, in grad
    handle_disconnected(elem)
  File "/auto/homes/kc391/event_rnn/env/bin/Theano/theano/gradient.py", line 532, in handle_disconnected
    raise DisconnectedInputError(message)
theano.gradient.DisconnectedInputError: grad method was asked to compute the gradient with respect to a variable that is not part of the computational graph of the cost, or is used only by a non-differentiable operator: <TensorType(float32, matrix)>
Backtrace when the node is created:
  File "/auto/homes/kc391/event_rnn/env/bin/keras/keras/backend/theano_backend.py", line 34, in variable
    return theano.shared(value=value, name=name, strict=False)
The error is caused by:
when AutoEncoder is inited, self.params is set by scraping the weights and biases from the encoder and decoder layers.
when model.add is called with an AutoEncoder layer as the argument, this calls AutoEncoder.set_previous(), which calls AutoEncoder.encoder.set_previous().
this reinitialises the weights and biases on the encoder layer, so that the pointers contained in AutoEncoder.params no longer points to the correct weights and biases on the encoder layer.
Possible solutions:
update AutoEncoder.params when AutoEncoder.set_previous() is called
add an ability to call set_previous() without overwriting existing weights (similar to #1266)
Thoughts?
Kris