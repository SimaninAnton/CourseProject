ivodopyanov commented on 28 Oct 2016
Hello. I intend to use system of two hierarchical seq2seq models for unsupervised training. It should look something like that:
model = Sequential()
model.add(Layer(batch_input_shape=(PARAGRAPH_LEN, SENTENCE_LEN, WORD_LEN, CHAR_COUNT)))
model.add(TimeDistributed(LSTM(HIDDEN_LAYER_1_SIZE)))
model.add(LSTM(HIDDEN_LAYER_2_SIZE))
model.add(RepeatVector(SENTENCE_LEN))
model.add(LSTM(HIDDEN_LAYER_2_SIZE, return_sequences=True))
model.add(TimeDistributed(Dense(HIDDEN_LAYER_1_SIZE)))
model.add(TimeDistributed(RepeatVector(WORD_LEN)))
model.add(TimeDistributed(LSTM(HIDDEN_LAYER_1_SIZE, return_sequences=True)))
model.add(TimeDistributed(TimeDistributed(Dense(CHAR_COUNT, activation='softmax'))))
What I'm trying to achieve is to get char-based word embedding from 1st LSTM layer and then feed them into 2nd LSTM layer to generate sentence embedding. For that I use seq2seq as autoencoders (so, X == Y).
Can I use sample weights for such data to mask out paddings in Y? Wiki says it supports only 1D and 2D sample weights but here they should be 3D.