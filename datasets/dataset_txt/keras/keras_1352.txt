Contributor
ozabluda commented on 6 Jan 2018 â€¢
edited
Using same exact code and output as in #8982, we can see that validation loss is small: 2.4932038522607058e-11 before training starts and 0.048399086749553677 after training is complete. However, training loss is large 0.6948 (and it's not the effect of averaging batches in an epoch).
I have no good hypothesis why it is happening but 0.6948 looks close to (but not quite) 1/sqrt(2)=0.7071.