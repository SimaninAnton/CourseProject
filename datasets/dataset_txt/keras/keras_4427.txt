Contributor
strin commented on 7 Sep 2016 â€¢
edited
Currently, Kers uses a RNN based implementation for TimeDistributed.call.
def call(self, X, mask=None):
        # no batch size specified, therefore the layer will be able
        # to process batches of any size
        # we can go with reshape-based implementation for performance
        if input_shape[0]:
            # batch size matters, use rnn-based implementation
            def step(x, states):
                output = self.layer.call(x)
                return output, []

            last_output, outputs, states = K.rnn(step, X,
                                                 initial_states=[])
            y = outputs
        else:
        input_length = input_shape[1]
        if not input_length:
            input_length = K.shape(X)[1]
        X = K.reshape(X, (-1, ) + input_shape[2:])  # (nb_samples * timesteps, ...)
        y = self.layer.call(X)  # (nb_samples * timesteps, ...)
        # (nb_samples, timesteps, ...)
        output_shape = self.get_output_shape_for(input_shape)
        y = K.reshape(y, (-1, input_length) + output_shape[2:])
        return y
Notice, in calling the rnn, the initial states are set to be []. However, in the new K.rnn implementation with dynamic rnn, an exception is raised for initial_states = [].
if nb_states == 0:
            raise Exception('No initial states provided.')