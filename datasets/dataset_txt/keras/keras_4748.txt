yurkor commented on 20 Jul 2016 â€¢
edited
I have feel that current keras examples LSTM of seq to seq training is suboptimal, correct me if I wrong.
It works for simple tasks like https://github.com/fchollet/keras/blob/master/examples/addition_rnn.py.
But not for Neural conversation model https://arxiv.org/pdf/1506.05869.pdf and machine translation.
Most of the results of seq-seq model returns common repetitive(like "the the the a" in dialog generation) words and reason I believe in that LSTM accumulate errors when using generated output of previous state as input for next state. It would be nice to have ability to use target output as input for next step.
Tensor-flow does this in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py with feed_previous.
Code in keras is more general but I would like to get some help on ideas how could this be implemented in keras.
I think may be special LSTM version which combines softmax output and uses K.use_in_train(target_embedding, prev_state)
Or may be have additional input with shifted target (y_in[0:] = y[1:]) to LSTM concatenated with other inputs but then its unclear how to test this model.