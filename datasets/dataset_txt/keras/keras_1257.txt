armada11 commented on 8 Feb 2018
I am new to Keras and I am trying to build a text classification of 10k sequences. The length of my sequences varies up to 100 words for each sequence. My vocabulary size is 30. So, when I am trying to create one hot encoding I got an array of shape (10000, 100, 30) where 10000 is the number of sequences, 100 is the maximum length of a sequence and 30 is the size of my vocabulary. I have tried to feed that to my model but it doesn't accept shape of 3-dims, however, it accepts shape of 2-dim. Do I need to reshape my one hot array to become (10000, 3000)? Here is the code I have!
embedding_layer = Embedding(30, 50, trainable=False)
sequence_input = Input(shape=(100,30), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
x = Convolution1D(128, 5, activation='relu')(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Convolution1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Convolution1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x) # global max pooling
x = Dropout(0.5)(x)
x = Flatten()(x)
x = Dense(8, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(4, activation ='relu')(x)
sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
preds = Dense(1, activation='sigmoid')(x)
model = Model(sequence_input, preds)
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])
print(model.summary())