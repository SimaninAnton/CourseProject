Contributor
mjdietzx commented on 7 Oct 2016
In this case:
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=4, cooldown=0)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
After 4 epochs of val_loss not improving, we reduce the learning rate as expected. However, since self.cooldown == 0 we don't take this branch and therefore never reset self.wait. Which causes:
Learning rate reduced after 4 epochs of no improvement.
Learning rate reduced again the next epoch since we don't reset self.wait.
Every single epoch we are reducing the learning rate after the first time we reduce it.
Once val_loss improves, we reset self.wait and now wait for self.patience epochs before reducing LR again.
In the case where self.cooldown > 0 this code should work as expected.
Gist: https://gist.github.com/mjdietzx/3aaf9c58486f6e6ff310a0d960d8bb4e
@basveeling