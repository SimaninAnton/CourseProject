Contributor
happygds commented on 9 Nov 2016
After reading the ICLR 2017 submitted paper unrolled generative adversarial networks in http://104.155.136.4:3000/pdf?id=BydrOIcle, I'm curious about this technique for addressing the problem of mode collapse when training GAN models.
The key idea is that we could update the generator after several simulative updating of discriminator, while the discriminator uses the original parameter before these simulations to complete updates.
At first sight, I think of the recurrent process using theano.scan(). However, since the parameters are theano.shared, how could we simulate one subset of them recurrently to update the others while keep the original subset being kept for another use?
Any ideas? Thanks !
4