Contributor
ducha-aiki commented on 1 Jul 2016
I`d like to put here question from keras-users https://groups.google.com/forum/#!topic/keras-users/QWe3xAkuBOg
I'm wondering if and how the gradient reversal layer from the paper "Domain-Adversarial Training of Neural Networks" (http://arxiv.org/abs/1505.07818) can be implemented in Keras.
The "gradient reversal layer" as described there is a layer which passes input through unchanged, but during backpropagation multiplies all gradients by a constant (negative) factor.*
According to this answer https://groups.google.com/forum/#!topic/keras-users/FcbrlLY94ms
If you want to explicitly define your gradients you would probably be better off using Torch than using Keras/Theano.
it is not possible in Keras. Am I right?