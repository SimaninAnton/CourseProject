aaronpolhamus commented on 2 Apr 2016
I'm training a CNN in Keras to differentiate between 196 different classes of automobile's with subtle differences distinguishing them (we're using the Stanford Cars Dataset). It's a tricky problem, since the number of training examples is small relative to the number of classes (~40 each), so I don't expect great performance. Even if the model is wrong a lot, though, I still expect it to make different guesses based on the data. What is puzzling here is that after training on the following gist script, both the methods model.predict and model.proba yield exactly the same thing--a 196-element vector of all zeroes, with the exception of a "1" in the 189th position, e.g.:
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
To me this looks like the model is all zeroes, and no updates are being made. This would also explain why the training history from the SGD looks like this:
{'loss': [311015.37542976422, 311094.98858055013, 311095.03131139488, 311094.99815815326, 311094.97765225935, 311094.98489685659, 311094.95284872298, 311094.97243369353, 311094.96813605109, 311095.01657662081], 'batch': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'size': [1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967, 1967]}
I've seen this behavior across a range of learning rates. Is this simply a case of failing to randomly initialize the model weights? Would we still expect backprop to create changes in weight values?
I've included 20 post-processed training data examples here
Any pointers very much appreciated.