konstantinberlin commented on 3 Aug 2015
When using BatchNormalization with Dropout the output is not deterministic during prediction, and the variance could be very high.
Here is sample code
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.normalization import BatchNormalization
from keras.layers.advanced_activations import PReLU
import numpy as np

if __name__=='__main__':

    units = 1024
    feats = 1024
    feat_dropout = 0.2
    dropout = 0.5

    model = Sequential()
    model.add(Dropout(feat_dropout))

    model.add(Dense(feats,units, init='glorot_uniform'))
    model.add(PReLU((units,)))
    model.add(BatchNormalization((units,)))
    model.add(Dropout(dropout))

    model.add(Dense(units, 1, init='glorot_uniform'))
    model.add(Activation('sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer='adam', class_mode = 'binary')

    X = np.random.rand(4, units)
    y = np.array([0.0, 0.0, 1.0, 1.0])

    model.fit(X, y, batch_size=128, nb_epoch = 10, show_accuracy=False, verbose=1)

    for iter in range(5):
        nn_pred = model.predict_proba(X, batch_size = 128, verbose=1)

        print nn_pred
here is the output
Epoch 0
4/4 [==============================] - 0s - loss: 0.7074
Epoch 1
4/4 [==============================] - 0s - loss: 0.6859
Epoch 2
4/4 [==============================] - 0s - loss: 0.6461
Epoch 3
4/4 [==============================] - 0s - loss: 0.6204
Epoch 4
4/4 [==============================] - 0s - loss: 0.5937
Epoch 5
4/4 [==============================] - 0s - loss: 0.5653
Epoch 6
4/4 [==============================] - 0s - loss: 0.5440
Epoch 7
4/4 [==============================] - 0s - loss: 0.5318
Epoch 8
4/4 [==============================] - 0s - loss: 0.5122
Epoch 9
4/4 [==============================] - 0s - loss: 0.4948
4/4 [==============================] - 0s
[[ 0.28348112]
 [ 0.3500709 ]
 [ 0.61124349]
 [ 0.6202431 ]]
4/4 [==============================] - 0s
[[ 0.33657578]
 [ 0.36096211]
 [ 0.63513098]
 [ 0.65311897]]
4/4 [==============================] - 0s
[[ 0.32223953]
 [ 0.3655569 ]
 [ 0.64139681]
 [ 0.65343767]]
4/4 [==============================] - 0s
[[ 0.37226692]
 [ 0.36298559]
 [ 0.62061622]
 [ 0.65021041]]
4/4 [==============================] - 0s
[[ 0.35410142]
 [ 0.26217727]
 [ 0.67513401]
 [ 0.65493008]]