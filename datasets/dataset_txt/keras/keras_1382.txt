Enumaris commented on 23 Dec 2017 â€¢
edited
Hi, I am running a regression using the mean squared logarithmic error as my loss function. My evaluation metric is simply the root mean squared logarithmnic loss. I found the source code for Keras implementation of the mean squared logarithmic here: https://github.com/keras-team/keras/blob/master/keras/losses.py which gives:
    def mean_squared_logarithmic_error(y_true, y_pred):
        first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)
        second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)
        return K.mean(K.square(first_log - second_log), axis=-1)
And for convenience (so I don't have to keep taking square roots myself), I simply created my own custom metric where I just take the square root of the output above:
    def root_mean_squared_logarithmic_error(y_true, y_pred):
        first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)
        second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)
        return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))
I compiled my model with the msle as the loss and the rmsle as the metric. When I run the model, though, the rmsle is nowhere near the square-root of the msle and I don't understand why. Is this expected behavior? Does keras do something different to calculate the loss and the metric? E.g. is the loss a full average over an epoch while the metric is some running average or something of that kind?