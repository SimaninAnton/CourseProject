shaotongkai commented on 22 Apr 2017
I am working on the pretrained_word_embeddings.py,
here is the link: https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py
for this part:
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
x = Conv1D(128, 5, activation='relu')(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(35)(x)
x = Flatten()(x)
x = Dense(128, activation='relu')(x)
I think the parameter for last MaxPooling1D (line 136) should be 5 not 35.