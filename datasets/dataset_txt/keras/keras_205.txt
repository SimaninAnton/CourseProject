SyedShaQutub commented on 27 May 2019 â€¢
edited
Please make sure that this is a Bug or a Feature Request and provide all applicable information asked by the template.
If your issue is an implementation question, please ask your question on StackOverflow or on the Keras Slack channel instead of opening a GitHub issue.
System information
Have I written custom code (as opposed to using example directory): yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
TensorFlow backend (yes / no): yes
TensorFlow version: tf 1.13.1
Keras version: 2.2.4
Python version: 3.7
CUDA/cuDNN version: 10.1/7.5 [ nvidia-smi says CUDA Version: 10.2 ]
GPU model and memory: RTX2070, 8 gb
You can obtain the TensorFlow version with:
python -c "import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"
You can obtain the Keras version with:
python -c 'import keras as k; print(k.version)'
Describe the current behavior
Error : ValueError: Output tensors to a Model must be the output of a Keras Layer (thus holding past layer metadata). Found: Tensor("truediv:0", shape=(?, 2048, 1024, 128), dtype=float32)
I see that after adding activation layers at various segments of my model, layer drops 2 attributes _keras_history and _keras_shape.
Currently, the model isn't compiling as my architecture has an activation layer at the end applied after computing logits.
Note: The same code works perfectly fine with tf.keras and tf 2.0 alpha
I'm compiling the model with with the output layer activated by softmax.
Describe the expected behavior
model should compile.
Code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
x = keras.activations.relu(x)
ff_layer2 = keras.layers.DepthwiseConv2D(128, strides=(1, 1), depth_multiplier=1, padding='same')(x)
classifier = keras.activations.softmax(ff_layer2)
Other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.