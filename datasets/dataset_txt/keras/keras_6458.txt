chunyangx commented on 9 Nov 2015
Hello,
The attention model that I tried to implement is as http://arxiv.org/abs/1412.7449
The code for the model was the following:
self.model = Graph()
self.model.add_input(name='utterance', input_shape=(uttr1_max_length, uttr1_dict_length)))
self.model.add_input(name='utterance2', input_shape=(uttr2_max_length, uttr2_dict_length)))
self.model.add_node(TimeDistributedDense(dim), name='Embed_uttr', input='utterance')
self.model.add_node(TimeDistributedDense(dim), name='Embed_uttr2', input='utterance2')
self.model.add_node(LSTM(dim), name='Hidden_uttr', input='Embed_uttr')
self.model.add_node(LSTM(dim), name='Hidden_uttr2', input='Embed_uttr2')
# Build attention vector
self.model.add_node(TimeDistributedDense(dim), name='Above_embedding', input='Embed_uttr')
self.model.add_node(Dense(dim), name='Above_hidden', input='Hidden_uttr2')
self.model.add_node(RepeatVector(self.preprocessor.uttr_max_length), name='repeat_above_hidden', input='Above_hidden')
self.model.add_node(Activation('tanh'), name='intermediate',inputs=['Above_embedding', 'repeat_above_hidden'], merge_mode='sum')
self.model.add_node(TimeDistributedDense(1), name='inter_att', input='intermediate')
self.model.add_node(Flatten(), name='flatten_att', input='inter_att')
self.model.add_node(Activation('softmax'), name='att_vector', input='flatten_att')
self.model.add_node(Activation('linear'), name='attention', inputs=['Embed_uttr','att_vector'], merge_mode='dot', dot_axes=[[1],[1]])
# Final prediction
self.model.add_node(Dense(uttr2_dict_length)), name='merged', inputs=['Hidden_uttr','Hidden_uttr2', 'attention'], merge_mode='concat')
self.model.add_node(Activation('softmax'), name='activation', input='merged')
self.model.add_output(name='output', input='activation')
self.model.compile('rmsprop', {'output':'categorical_crossentropy'})
And I receive the error message: ValueError: ('You cannot drop a non-broadcastable dimension.', ((False, False), (0, 'x'))). Could somebody tell me where is the problem please ?