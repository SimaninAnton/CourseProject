sanjeevmk commented on 6 Jul 2016 â€¢
edited
I'm using Deep CNNs for 3D object recognition, so instead of images I have voxel data stored in files. I have about 10k such voxel data files in some format, each voxel grid of size 60x60x60 (float 32) . I load these voxel data in a large 5D numpy array which has a shape like this : (num_samples, 60,60,60,1) . num_samples = 10k approx. Lets call this input shape X.
If I load entire X in one go, and the call fit() , then loading itself takes too much RAM and I don't even reach the training phase. I have 32G RAM and 8G GPU. The loading part uses up all of 32G RAM .
Instead, what I'd like to do is keep loading small subsets of data from files and call fit() on each of them. So, something like this in pseudo code:
for i in range(0,len(training_data), batch_size):
     X = loadNextTrainingData(i , i+batch_size)
     model.fit(X)
Let say batch_size is 100 , so I load 0-100, train on that, then load 100-200 and so on. But when I load and train subsequent samples after the first one, the network should not re-initialize, it should resume training from its previous point (I also don't want to keep dumping the network weights and them reloading them, that's too untidy) .
Will calling fit() multiple times like above for different sub-samples , re-initialize the weights every time it is called?
If fit() does re-initialize weights every time, then i'll have to write my own batch generator. What is the format for a batch generator - input arguments , return values etc ? I saw the CIFAR 10 batch generator example, but that seems tailor-made for images. Is there a much generic batch generator? If I choose to implement my own, what should be the format?