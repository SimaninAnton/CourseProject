Ejhfast commented on 24 Aug 2016 â€¢
edited
I am trying to build a simple sequence labeler. The training data X is of shape (num_docs, max_len) (to pass through an embedding layer) and y is of shape (num_docs, max_len, num_classes), a one-hot encoding of sequence label classes. Since both X and y are padded to max_len, I am using an embedding layer with zero-masking and would like to prevent the padding from being incorporated into the cost function. It seems the correct way to do this in keras is by passing a sample_weight parameter.
The model code is:
model = Sequential()
model.add(Embedding(vocab_len+1, embed_dim, input_length=max_len, weights=[embedding_matrix], mask_zero=True))
model.add(LSTM(hidden_size, return_sequences=True))
model.add(TimeDistributed(Dense(n_classes, activation="softmax")))
model.compile(sample_weight_mode="temporal",loss="categorical_crossentropy",optimizer="adam")
For this task, I would like sample_weights to be matrix that corresponds to the input samples over time. For each training example I fill sample_weights[i,:] with 1s until the start of the padding, after which it is filled with 0s. So the shape of sample_weights is (num_docs, max_len), which corresponds to the shape of X. Now I would like to run:
model.fit(X,y,sample_weight=sample_weights)
Edit: I fixed the problem, and it wasn't with sample_weight so closing the issue. I'm leaving the above text as there isn't great documentation available for sample_weight and other people might find the description of the model useful.