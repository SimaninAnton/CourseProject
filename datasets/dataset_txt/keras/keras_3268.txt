enavarrocomes commented on 8 Feb 2017
Is there any difference between using a Dropout layer before an LSTM and using the argument dropout_W of that LSTM layer?
Like this:
model.add(Dropout(0.5)) model.add(LSTM(100))
model.add(LSTM(100, dropout_W=0.5)
Also, in Zaremba et al paper of RNN regularization, they say dropout should never be applied to recurrent connections (dropout_U = 0). Why do I see many examples with dropout_U > 0? Am I missing something?
Thank you.
1