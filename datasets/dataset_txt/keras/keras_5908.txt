qxcv commented on 20 Feb 2016
When training or validating a Graph model with two objectives, it's frustrating to be unable to see both losses at once. Right now, for instance, I'm trying to train a network to regress and classify at the same time, but it's impossible to balance the regression and classification losses because I can't see how large they are. For one illustration of how the balance parameter in a multitask loss can impact accuracy, look at Table 9 in Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (loss terms and relative importance of Î» explained below Eqn. 1 on p5).
#580 presents a workaround, but that workaround is slow (especially if you want to see each loss on every minibatch!) and error-prone. Ideally train_on_batch, validate_on_batch, etc. should give a dictionary of losses like
{
  'regressor_loss': 23.5185,
  'classifier_loss': 3.9618
}
The return value and debugging output of .fit() and other convenience methods would have to be similarly modified. This probably breaks a lot of old code, so it may be necessary to introduce a flag analogous to accuracy={True,False} which indicates whether all losses should be returned or just one.
Kind-of related: #1510.