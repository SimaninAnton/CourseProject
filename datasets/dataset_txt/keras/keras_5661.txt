mydearbob commented on 26 Mar 2016
I tested the example idmb_lstm.py with keras 0.3.2 and theano 0.8, every epoch takes 110 second.
But when I degrade keras to 0.2.0 (pip install keras==0.2.0 --no-deps), every epoch just takes 63 seoncd.
The output is:
Using gpu device 0: Tesla K40m (CNMeM is enabled with initial size: 98.0% of memory, CuDNN 4007)
/home/chris/anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.
"downsample module has been moved to the theano.tensor.signal.pool module.")
Loading data...
20000 train sequences
5000 test sequences
Pad sequences (samples x time)
X_train shape: (20000, 100)
X_test shape: (5000, 100)
Build model...
Train...
Train on 18000 samples, validate on 2000 samples
Epoch 1/5
18000/18000 [==============================] - 63s - loss: 0.5750 - acc: 0.6929 - val_loss: 0.5219 - val_acc: 0.7220
Epoch 2/5
18000/18000 [==============================] - 63s - loss: 0.3800 - acc: 0.8421 - val_loss: 0.4200 - val_acc: 0.8155
Epoch 3/5
18000/18000 [==============================] - 63s - loss: 0.2553 - acc: 0.9014 - val_loss: 0.4463 - val_acc: 0.8180
Epoch 4/5
18000/18000 [==============================] - 63s - loss: 0.1593 - acc: 0.9431 - val_loss: 0.5047 - val_acc: 0.8235
Epoch 5/5
18000/18000 [==============================] - 63s - loss: 0.0942 - acc: 0.9679 - val_loss: 0.6666 - val_acc: 0.8155
5000/5000 [==============================] - 6s
Test score: 0.598775035354
5000/5000 [==============================] - 6s
Test accuracy: 0.8242
Why the 0.3.2 performance in training LSTM is worse than the old version 0.2.0?