LiuAndYang commented on 21 Mar 2016
In this paper , http://arxiv.org/pdf/1412.7449v3.pdf, They designed a attention model, I implement the first one, the single attenion model. But how can i implement word to word attention using Keras? They need the attend vector from last step, I got no idea now about how to do it? Can anyone help me with this thank you very much!
I refered to the code posted in https://github.com/fchollet/keras/issues/981, but it seems that they are different.
and the codes posted in issue #2067 seems like single word attention, not a word-by-word attention.
@fchollet @farizrahman4u @tboquet @wxs