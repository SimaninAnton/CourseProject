Rithmax commented on 4 Nov 2017
I have run the same code on Keras 2.0.9 and 2.0.8 as follows.
print('Build model...')
main_input = Input(shape=(100, 256), name='main_input')
x = LSTM(1024, return_sequences=True,trainable=False,name='fc2')(main_input)
x = GlobalAveragePooling1D()(x)
main_output = Dense(10, activation='softmax', trainable=False,name='main_output')(x)
model = Model(inputs=[main_input], outputs=[main_output])
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
model.summary()
With keras 2.0.9:
Using TensorFlow backend.
Build model...
Layer (type) Output Shape Param #
main_input (InputLayer) (None, 100, 256) 0
fc2 (LSTM) (None, 100, 1024) 5246976
global_average_pooling1d_1 ( (None, 1024) 0
main_output (Dense) (None, 10) 10250
Total params: 5,257,226
Trainable params: 5,246,976
Non-trainable params: 10,250
With keras 2.0.8:
Using TensorFlow backend.
Build model...
Layer (type) Output Shape Param #
main_input (InputLayer) (None, 100, 256) 0
fc2 (LSTM) (None, 100, 1024) 5246976
global_average_pooling1d_1 ( (None, 1024) 0
main_output (Dense) (None, 10) 10250
Total params: 5,257,226
Trainable params: 0
Non-trainable params: 5,257,226
The number of trainable parameters is different in both cases. Aperantly LSTM 'trainable=False' not working properly. Wondering whether do I need to change anything in the model when migrating keras 2.0.8 to 2.0.9?