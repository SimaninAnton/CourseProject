code-ball commented on 13 Jul 2016
Hi,
I have two inputs for which I learn the embeddings and then I want to dot product the embedded vectors to get a scaler value. I tried to use Merge layer with 'dot' mode but it still output a tensor. I am not able to understand how to get the output as I want.
Following is the input and embedding details:
inp1 = np.random.randint(1000, size=(10000,1))
inp2 = np.random.randint(1000, size=(10000,1))

a = Input(shape=(1,),dtype='int32',name='a')
b = Input(shape=(1,),dtype='int32',name='b')

em_ab = Embedding(output_dim=512,input_dim=1000,input_length=1)
em_a = em_ab(a)
em_b = em_ab(b)
As you can see my training input is a pair of integers. Each value range between 1 and 1000, and there are 10000 such pairs.
As each input has only one value, input_lenght=1 for embedding layer. Once the embedding completes,
I want to extract the embedded vectors for each training pair i.e. currently both em_a and em_b will have shape (None, 1, 512). But I want to extract the last dim for both embeddings which will give me a vector of size 512 for each of them.
How to do this? I tried reshape and squeeze options in addition to above but I think I am confused when this should be very simple. Can you help to see where am I going wrong?
Thanks