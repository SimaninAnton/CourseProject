pavitrakumar78 commented on 25 Dec 2016 â€¢
edited
Hi,
I am constructing a DQN model in keras (with theano backend) based on a lasagne implementation of the same.
The lasagne implementation is:
  from lasagne.layers import dnn

  l_in = lasagne.layers.InputLayer(
   shape=(batch_size, num_frames, input_width, input_height)
  )


  l_conv1 = dnn.Conv2DDNNLayer(
   l_in,
   num_filters=16,
   filter_size=(8, 8),
   stride=(4, 4),
   nonlinearity=lasagne.nonlinearities.rectify,
   W=lasagne.init.Normal(.01),
   b=lasagne.init.Constant(.1)
  )

  l_conv2 = dnn.Conv2DDNNLayer(
   l_conv1,
   num_filters=32,
   filter_size=(4, 4),
   stride=(2, 2),
   nonlinearity=lasagne.nonlinearities.rectify,
   W=lasagne.init.Normal(.01),
   b=lasagne.init.Constant(.1)
  )

  l_hidden1 = lasagne.layers.DenseLayer(
   l_conv2,
   num_units=256,
   nonlinearity=lasagne.nonlinearities.rectify,
   W=lasagne.init.Normal(.01),
   b=lasagne.init.Constant(.1)
  )

  l_out = lasagne.layers.DenseLayer(
   l_hidden1,
   num_units=output_dim,
   nonlinearity=None,
   W=lasagne.init.Normal(.01),
   b=lasagne.init.Constant(.1)
  )
                #return l_out
Keras implementation:
  inputs = Input(shape=(4, 84, 84,))
  model = Convolution2D(nb_filter=16, nb_row=8, nb_col=8, subsample=(4,4), activation='relu', border_mode='same',weights = [np.random.normal(scale = 0.01,size = (8,8,84,16)),np.full((16,),0.1)])(inputs)
  model = Convolution2D(nb_filter=32, nb_row=4, nb_col=4, subsample=(2,2), activation='relu', border_mode='same',weights = [np.random.normal(scale = 0.01,size = (4,4,16,32)),np.full((32,),0.1)])(model)
  model = Flatten()(model)
  model = Dense(output_dim=256, activation='relu',weights = [np.random.normal(scale = 0.01,size = (352,256)),np.full((256,),0.1)])(model)
  q_values = Dense(output_dim=self.num_actions, activation='linear',weights = [np.random.normal(scale = 0.01,size = (256, self.num_actions)),np.full((self.num_actions,),0.1)])(model)
  m = Model(input=inputs, output=q_values)

                #return m
I use RMSProp for both the model (rmsprop impl. of the respective libraries), all the hyperparameters are exactly the same - the loss calculation, updates are all the same for both models. But, the performance I get from lasagne's model (loss and agent's score in environment) is much better than Keras model. The keras model's loss for each training is also much higher than lasagne model's loss.
Is the performance difference because of the differing rms prop implementations? or maybe the weights and bias initializations? because other than that, every other price of code is exactly the same for both the models.
OS: Ubuntu 14.04
Python 2.7
Keras 1.1.2
Theano 0.8.2 (all tests run on GPU with same seed)