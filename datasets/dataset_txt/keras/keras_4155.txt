vyraun commented on 16 Oct 2016 â€¢
edited
Hi
In the babi_memnn example, test data is used as the validation data.
answer.fit([inputs_train, queries_train, inputs_train], answers_train,
batch_size=32,
nb_epoch=120,
validation_data=([inputs_test, queries_test, inputs_test], answers_test))
I guess this would be better :
answer.fit([inputs_train, queries_train, inputs_train], answers_train,
batch_size=32,
nb_epoch=120,
validation_split=0.05)
and then, evaluation on the test set to get the final accuracy and loss metrics.
loss, acc = answer.evaluate([inputs_test, queries_test, inputs_test], answers_test, batch_size=32).
So the last lines would be:
answer.fit([inputs_train, queries_train, inputs_train], answers_train,
batch_size=32,
nb_epoch=120,
validation_split=0.05)
loss, acc = answer.evaluate([inputs_test, queries_test, inputs_test], answers_test, batch_size=32)
print("Test Evaluation: Loss= {0}, Acc= {1}".format(loss,acc))
I created a pull request "Training/Validation/Test Split in babi_memnn #4078" to include test evaluation as well.