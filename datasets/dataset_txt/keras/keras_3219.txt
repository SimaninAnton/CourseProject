hamelsmu commented on 14 Feb 2017 â€¢
edited
I am having trouble wrapping my head around certain aspects of the Keras implementations of RNN. This is a description of my problem:
The observations each have 200 features, and varying time steps from 1 - 730 days.
I have labels for each day for each of these observations. So its like a sequence to sequence time series regression problem. The data is daily customer activity and the label is customer spend (regression).
I want to make predictions for the next X days, basically for any arbitrarily length sequence I want into the future.
Here are my questions:
Preparing training data:
I know the input dimension to RNN is (nb_samples, timesteps, input_dim) which in this case for me will be (# of customers, something between 1 - 730, 200)
should I zero-pad all of my sequences such that they all have time dimension of 730?
Any thoughts on left padding vs right padding, for this example?
I should pad my labels in the exact same way I pad my input, correct?
If I do the padding, I understand I should have a mask layer at the input of the network. Is that the only mask layer I need? Are there any disadvantages to using zero padding + masking vs. using batch size = 1 below?
I understand that an alternate approach would be to have batch size = 1 Timestep and set Stateful = True and reset state at the end of each variable length sequence. Is there any reason this is better than zero padding? Can someone confirm that I have the right understanding that you would reset the state at the end of the sequence? For example if customer #1 has 300 time steps and customer #2 has 400 time steps, I would reset the state of the model after 300 batches and then after 400 batches, assuming that each batch = 1 time step only.
Model Architecture:
Would turning on stateful offer any advantages in this situation? It doesn't look like I need it if I just use approach #1 above, and would just add unnecessary complexity to my problem.
Just to confirm if I zero pad all my sequences such that each sample contains the same number of time steps, I can actually still let Keras shuffle my data as it will only shuffle the first dimension (the customers) and not the time dimension within each of the customers, right?
Since I have labels available at every time step I will have return sequences = True
Making Predictions:
If I want to generate predictions for arbitrary length sequences, what is the best way to accomplish this in Keras? Lets say I want to make a prediction for the next time step t+1, I feed in the features for time step t. But what about carrying the hidden state forward and making prediction for time step t+2? Does my model need to predict the features for the next time step in addition to the label so that I can feed the features back into the model to predict the next time step?
Is it important to have stateful = True if I am trying to use the approach where I'm predicting the features for the next time step?
7