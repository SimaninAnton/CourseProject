CodingMojo22 commented on 2 Mar 2017
I'm feeding 2000 JPEG images with size 100 by 100 for Keras' Convolution2D but the accuracy doesn't seem to improve beyond 40% or acc- 0.40.
I've tried changing the optimizer, or increasing the number of layers but it improvements are not evident.
How can I improve the accuracy further?
'
#KERAS
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD,RMSprop,adam
from keras.utils import np_utils
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import os
import tensorflow
from PIL import Image
from numpy import *
SKLEARN
from sklearn.utils import shuffle
from sklearn.cross_validation import train_test_split
input image dimensions
img_rows, img_cols = 200, 200
number of channels
img_channels = 1
#%%
data
path1 = '/home/Keras-1.1.2/dataset/train' #path of folder of images
path2 = '/home/Keras-1.1.2/dataset/train_Resized' #path of folder to save images
listing = os.listdir(path1)
num_samples=size(listing)
#print (num_samples)
for name in listing:
if (name <> '.DS_store' and name <> '.DS_Store'):
im = Image.open(path1 + '/' + name)
img = im.resize((img_rows,img_cols))
gray = img.convert('L') #need to do some more processing here
gray.save(path2 + '/' + name, "JPEG")
imlist = os.listdir(path2)
im1 = array(Image.open(path2 + '/'+ imlist[0])) # open one image to get size
m,n = im1.shape[0:2] # get the size of the images
imnbr = len(imlist) # get the number of images
create matrix to store all flattened images
immatrix = array([array(Image.open(path2+ '/' + im2)).flatten()
for im2 in imlist],'f')
label=np.ones(((num_samples-1),),dtype = int)
label[0:905]=0 # jeep
label[906:1981]=1 # sedan
label[1982:]=2 #SUV
data,Label = shuffle(immatrix,label, random_state=2)
train_data = [data,Label]
img=immatrix[167].reshape(img_rows,img_cols)
#%%
#batch_size to train
batch_size = 2000
#batch_size = 2
number of output classes
nb_classes = 3
number of epochs to train
nb_epoch = 20
#nb_epoch = 1
number of convolutional filters to use
nb_filters = 32
#nb_filters = 16
size of pooling area for max pooling
nb_pool = 2
convolution kernel size
nb_conv = 9
#%%
(X, y) = (train_data[0],train_data[1])
STEP 1: split X and y into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
TENSORFLOW:
X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')
convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
i = 100
#%%
model = Sequential()
TENSORFLOW:
model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
border_mode='valid',
input_shape=(img_rows, img_cols, 1)))
THEANO:
#model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
border_mode='valid',
input_shape=(1, img_rows, img_cols)))
convout1 = Activation('relu')
model.add(convout1)
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
convout2 = Activation('relu')
model.add(convout2)
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
model.add(Convolution2D(50, 9, 9, border_mode='valid', input_shape=(img_rows, img_cols, 1)))
convout1 = Activation('relu')
model.add(convout1)
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
convout2 = Activation('relu')
model.add(convout2)
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
sgd=SGD(lr=0.003, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=["accuracy"])
'
`
`