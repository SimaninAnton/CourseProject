v1nc3nt27 commented on 13 Aug 2017
I want to use an implementation of an attention mechanism by Yang et al.. I found a working implementation of a custom layer that uses this attention machanism here. Instead of using the output values of my LSTM:
my_lstm = LSTM(128, input_shape=(a, b), return_sequences=True)
my_lstm = AttentionWithContext()(my_lstm)
out = Dense(2, activation='softmax')(my_lstm)
I would like to use the hidden states of the LSTM:
my_lstm = LSTM(128, input_shape=(a, b), return_state=True)
my_lstm = AttentionWithContext()(my_lstm)
out = Dense(2, activation='softmax')(my_lstm)
But I get the error:
TypeError: can only concatenate tuple (not "int") to tuple
I think I interprete the returning value of return_state incorrectly but I can't find any examples on how to do this. Is this working? Do I have to use it in combination with return_sequences? I also don't understand whether it returns all of the hidden states or only the last hidden state.
Thanks!