v-v commented on 7 Mar 2016
Hello,
I'm having troubles using a stateful LSTM after an Embedding layer. If I use it directly or have a dense layer before (but no Embedding layer), it works fine. I wrote a simple demo to illustrate the problem:
vocab_size = 20
max_len = 10
n_labels = 5

n_samples = 500

embedding_size = 100
batch_size = 16

# generate fake data
X_train = np.random.randint(0, vocab_size, size=[n_samples, max_len])
y_train = np.random.randint(0, 2, size=[n_samples, n_labels]) #multiclass, multilabel

model = Sequential()

#have to give full batch size to first layer for using stateful LSTMs later on
model.add(Embedding(vocab_size, embedding_size, input_length=max_len, batch_input_shape=(batch_size, max_len))) 
#stateful LSTM (everything works great with normal LSTM)
model.add(LSTM(output_dim=300, stateful=True))
model.add(Dropout(0.5))
model.add(Dense(n_labels))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', class_mode="binary")
model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10)
the model runs successfully until the end of the epoch, when it fails:
Epoch 1/10
496/500 [============================>.] - ETA: 0s - loss: 0.6959
Traceback (most recent call last):
  File "./stateful_lstm_embedding_test.py", line 34, in <module>
    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=10)
  File "build/bdist.macosx-10.10-x86_64/egg/keras/models.py", line 678, in fit
  File "build/bdist.macosx-10.10-x86_64/egg/keras/models.py", line 314, in _fit
ValueError: Shape mismatch: x has 16 rows but z has 4 rows
Apply node that caused the error: Gemm{no_inplace}(Subtensor{::, int64::}.0, TensorConstant{0.20000000298}, <TensorType(float32, matrix)>, lstm_U_o_copy0, TensorConstant{0.20000000298})
The same happens with both tensorflow and theano backends. Maybe I'm confused on how to give batch_input_shape to an Embedding layer, but I'm providing it batches of sizes batch_size x max_len so I cannot think of what else to provide.