ivodopyanov commented on 25 Feb 2016
Hello.
I have following model:
pack_card_count = 432
picked_card_count = 432

model = Graph()
model.add_input(name='pack', input_shape=(pack_card_count,))
model.add_input(name='picked', input_shape=(picked_card_count,))

model.add_node(Dense(5000), name='picked1', input='picked')
model.add_node(PReLU(), name='picked1Act', input='picked1')
model.add_node(BatchNormalization(), name='picked1Norm', input='picked1Act')
model.add_node(Dropout(0.5), name='picked1Dropout', input='picked1Norm')

model.add_node(Dense(2000), name='picked2', input='picked1Dropout')
model.add_node(PReLU(), name='picked2Act', input='picked2')
model.add_node(BatchNormalization(), name='picked2Norm', input='picked2Act')
model.add_node(Dropout(0.5), name='picked2Dropout', input='picked2Norm')

model.add_node(Dense(500), name='picked4', input='picked2Dropout')
model.add_node(PReLU(), name='picked4Act', input='picked4')
model.add_node(BatchNormalization(), name='picked4Norm', input='picked4Act')
model.add_node(Dropout(0.5), name='picked4Dropout', input='picked4Norm')

model.add_node(Dense(pack_card_count), name='sum1', input='picked4Dropout')
model.add_node(Activation('softmax'), name='res', inputs=['sum1','pack'], merge_mode='mul')
model.add_output(name='output', input='res')
optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.99, epsilon=1e-08)
model.compile(optimizer, {'output':'categorical_crossentropy'})
It occupies about 220M of RAM (got the value by loading model from saved json file). Production web-server (where I use that model for prediction) has limited amount for RAM and several such models exceed that limitation. Maybe there is a way to reduce model memory size? For example, there is no need for training capabilities in production so could their exclusion lower memory consumption?