dpappas commented on 20 Nov 2015
I want to train an lstm with samples of different size of timesteps
import numpy as np
train_x = [ [ [1,1], # timest1 [2,2] # timest2 ], # sample1 [ [1,1], # timest1 [2,2], # timest2 [3,3], # timest3 [4,4] # timest4 ], # sample2 [ [1,1], # timest1 [2,2], # timest2 [3,3] # timest3 ] # sample3 ]
train_y = np.array([[ 1, 1, 1, 1, 1],
[ 2, 1, 1, 1, 2],
[ 0.5, 1, 1, 1, 3]
])
train_x = np.array(train_x)
train_y = np.array(train_y)
from keras.models import Sequential
from keras.layers.core import Dense, Dropout
from keras.layers.recurrent import LSTM
from keras.regularizers import l2, activity_l2
in_dim = 2
model = Sequential()
model.add(LSTM(50, input_dim = in_dim, activation='tanh'))
model.add(Dense(5, activation='relu', W_regularizer=l2(0.01) , activity_regularizer=activity_l2(0.01) ))
model.compile(loss='mse', optimizer='sgd')
t = model.fit(train_x, train_y, batch_size=2, nb_epoch=5)
it yelds an error
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "/usr/lib/python2.7/site-packages/keras/models.py", line 489, in fit
shuffle=shuffle, metrics=metrics)
File "/usr/lib/python2.7/site-packages/keras/models.py", line 210, in _fit
outs = f(*ins_batch)
File "/usr/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/compile/function_module.py", line 786, in __call__
allow_downcast=s.allow_downcast)
File "/usr/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/tensor/type.py", line 116, in filter
data = theano._asarray(data, dtype=self.dtype)
File "/usr/lib/python2.7/site-packages/Theano-0.7.0-py2.7.egg/theano/misc/safe_asarray.py", line 33, in _asarray
rval = numpy.asarray(a, dtype=dtype, order=order)
File "/usr/lib64/python2.7/site-packages/numpy/core/numeric.py", line 474, in asarray
return array(a, dtype, copy=False, order=order)
ValueError: ('Bad input argument to theano function with name "/usr/lib/python2.7/site-packages/keras/models.py:399" at index 0(0-based)', 'setting an array element with a sequence.')
I know that i could use padding but since i use
word embeddings i do not want to use zero matrices to pad train_x
because it might get a meaning of another word.
The feature vectors are of the same length so the lstm should train regardless of number of timesteps.
Is there a way to do so in keras?
Thank you all in advance for your support