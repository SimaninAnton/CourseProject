jacobzweig commented on 28 Jun 2016 â€¢
edited
I'm experimenting with the convolutional autoencoder demo from the blog post but I'm getting some odd shape errors. My input data are (nSamples, 1, 512, 1).
The model structure is:
input_img = Input(shape=(1, 512, 1))

x = Convolution2D(32, 3, 1, activation='relu', border_mode='same')(input_img)
x = MaxPooling2D((2,1), border_mode='same')(x)
x = Convolution2D(32, 3, 1, activation='relu', border_mode='same')(x)
encoded = MaxPooling2D((2,1), border_mode='same')(x)


x = Convolution2D(32, 3, 1, activation='relu', border_mode='same')(encoded)
x = UpSampling2D((2,1))(x)
x = Convolution2D(32,3,1, activation='relu', border_mode='same')(x)
x = UpSampling2D((2,1))(x)
decoded = Convolution2D(1, 3,1, activation='sigmoid', border_mode='same')(x)

autoencoder = Model(input_img, decoded)
autoencoder.summary()
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_14 (InputLayer)            (None, 1, 512, 1)     0                                            
____________________________________________________________________________________________________
convolution2d_50 (Convolution2D) (None, 32, 512, 1)    128         input_14[0][0]                   
____________________________________________________________________________________________________
maxpooling2d_19 (MaxPooling2D)   (None, 32, 256, 1)    0           convolution2d_50[0][0]           
____________________________________________________________________________________________________
convolution2d_51 (Convolution2D) (None, 32, 256, 1)    3104        maxpooling2d_19[0][0]            
____________________________________________________________________________________________________
maxpooling2d_20 (MaxPooling2D)   (None, 32, 128, 1)    0           convolution2d_51[0][0]           
____________________________________________________________________________________________________
convolution2d_52 (Convolution2D) (None, 32, 128, 1)    3104        maxpooling2d_20[0][0]            
____________________________________________________________________________________________________
upsampling2d_19 (UpSampling2D)   (None, 32, 256, 1)    0           convolution2d_52[0][0]           
____________________________________________________________________________________________________
convolution2d_53 (Convolution2D) (None, 32, 256, 1)    3104        upsampling2d_19[0][0]            
____________________________________________________________________________________________________
upsampling2d_20 (UpSampling2D)   (None, 32, 512, 1)    0           convolution2d_53[0][0]           
____________________________________________________________________________________________________
convolution2d_54 (Convolution2D) (None, 1, 512, 1)     97          upsampling2d_20[0][0]            
====================================================================================================
Total params: 9537
____________________________________________________________________________________________________
However, I'm getting the following error, which suggests that the nb_col dimension is somehow becoming -1 in the network. I'm not sure how that's happening, as the structure above looks correct to me. Anyone have any ideas?
AssertionError: Can't store in size_t for the bytes requested 18446744073709551615 * 4
Apply node that caused the error: GpuAlloc{memset_0=True}(CudaNdarrayConstant{[[[[ 0.]]]]}, Shape_i{0}.0, TensorConstant{32}, Elemwise{Composite{((i0 + i1) // i0)}}[(0, 1)].0, Elemwise{add,no_inplace}.0)
Toposort index: 94
Inputs types: [CudaNdarrayType(float32, (True, True, True, True)), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]
Inputs shapes: [(1, 1, 1, 1), (), (), (), ()]
Inputs strides: [(0, 0, 0, 0), (), (), (), ()]
Inputs values: [CudaNdarray([[[[ 0.]]]]), array(64L, dtype=int64), array(32L, dtype=int64), array(257L, dtype=int64), array(-1L, dtype=int64)]
Outputs clients: [[GpuIncSubtensor{InplaceInc;::, ::, :int64:, :int64:}(GpuAlloc{memset_0=True}.0, GpuDnnConvGradI{algo='time_once', inplace=True}.0, ScalarFromTensor.0, ScalarFromTensor.0)]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node. 
Please make sure that the boxes below are checked before you submit your issue. Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
1