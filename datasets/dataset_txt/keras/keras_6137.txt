KlaymenGC commented on 12 Jan 2016
I'm building an end-to-end image segmentation network whose output has the same size as the input.
When I add BatchNormalization layer to the model, the prediction of any image in the train/test set is strangely always the same, when I remove the BN layer, everything goes normal.
I've looked into the code of BatchNormalization many times but I still can't figure out what's wrong, any ideas?