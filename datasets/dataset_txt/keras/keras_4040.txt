hadikazemi commented on 31 Oct 2016
I needed a layer to just do element-wise multiplication. I first tried to use locally connected like this:
model.add(LocallyConnected2D(3, 1, 1, input_shape=(1, 224, 224)))
But later I found locally connected is so slow which makes it useless for my problem. Then I started to write a custom layer but it doesn't work and gives me the following error:
File "/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py", line 866, in call
self.fn() if output_subset is None else
ValueError: GpuElemwise. Output dimension mis-match. Output 0 (indices start at 0), working inplace on input 0, has shape[0] == 1, but the output's size on that axis is 3.
Apply node that caused the error: GpuElemwise{Mul}[(0, 0)](GpuFromHost.0, mylayer_1_W)
Toposort index: 1
Inputs types: [CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, 4D)]
Inputs shapes: [(1, 3, 224, 224), (3, 3, 224, 224)]
Inputs strides: [(0, 50176, 224, 1), (150528, 50176, 224, 1)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[HostFromGpu(GpuElemwise{Mul}[(0, 0)].0)]]
and here is my code (theano backend):
class MyLayer(Layer):
    def __init__(self, nb_filter, nb_row, nb_col,
                 init='glorot_uniform', activation='linear', weights=None,
                 border_mode='valid', subsample=(1, 1), dim_ordering='default',
                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):
        if dim_ordering == 'default':
            dim_ordering = K.image_dim_ordering()
        self.init = initializations.get(init, dim_ordering=dim_ordering)
        self.activation = activations.get(activation)
        self.nb_filter = nb_filter
        self.nb_row = nb_row
        self.nb_col = nb_col
        self.bias = False
        self.initial_weights = weights
        self.dim_ordering = dim_ordering
        super(MyLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        input_dim = input_shape[0]
        if self.dim_ordering == 'th':
            stack_size = input_shape[1]
            self.W_shape = (self.nb_filter, stack_size, self.nb_row, self.nb_col)

        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))

        if self.bias:
            self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))
            self.trainable_weights = [self.W, self.b]
        else:
            self.trainable_weights = [self.W]

        if self.initial_weights is not None:
            self.set_weights(self.initial_weights)
            del self.initial_weights


    def call(self, x, mask=None):
        return x*self.W

    def get_output_shape_for(self, input_shape):
        rows = input_shape[2]
        cols = input_shape[3]
        return (input_shape[0], self.nb_filter, rows, cols)

model = Sequential()
model.add(MyLayer(3,224, 224, input_shape=(3, 224, 224)))

get_2rd_layer_output = K.function([model.layers[0].input],
                                  [model.layers[0].output])
img = np.ones((1,3,224,224))
k = get_2rd_layer_output([img])[0]
anybody knows where is the problem?