Kyubyong commented on 1 Aug 2016
I know there have been quite intensive discussions about applying bidirectional recurrent layer. So, I guess this code is a potential solution to the masking issue.
https://github.com/farizrahman4u/seq2seq/blob/master/seq2seq/layers/bidirectional.py
I tried the following:
model = Sequential()
model.add(Embedding(30, 10, mask_zero=True, input_length=10))
model.add(Bidirectional(LSTM(10, return_sequences=True)))
But an error occurred. Its message is as follows:
'LSTM' object has no attribute 'batch_input_shape'
Any thoughts?