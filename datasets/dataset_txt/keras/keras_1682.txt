suissemaxx commented on 8 Sep 2017 •
edited
The Tokenizer still doesn´t take into account the nb_words (nor num_words) argument.
from keras.preprocessing.text import Tokenizer

MAX_NB_WORDS = 20000

tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
Results for my texts (newsgroup example from Keras blog) in this:
Found 174074 unique tokens.
How can I fix this?