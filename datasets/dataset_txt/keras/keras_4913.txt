zero76114 commented on 24 Jun 2016
Dear All,
Please help me explain the result training. Why val_acc seems abnormal fluctuations. Is there overfitting? How I can avoid and tunning hyperparameter. Please help me.
Loading data...
47808 train sequences
11952 test sequences
Pad sequences (samples x time)
X_train shape: (47808, 100)
X_test shape: (11952, 100)
Build model...
Train...
(47808, 100)
(47808,)
Train on 47808 samples, validate on 11952 samples
Epoch 1/15
47808/47808 [==============================] - 300s - loss: 0.6807 - acc: 0.5745 - val_loss: 0.6770 - val_acc: 0.5802
Epoch 2/15
47808/47808 [==============================] - 324s - loss: 0.6693 - acc: 0.5843 - val_loss: 0.6765 - val_acc: 0.5796
Epoch 3/15
47808/47808 [==============================] - 349s - loss: 0.6678 - acc: 0.5857 - val_loss: 0.6789 - val_acc: 0.5801
Epoch 4/15
47808/47808 [==============================] - 345s - loss: 0.6485 - acc: 0.6128 - val_loss: 0.6913 - val_acc: 0.5599
Epoch 5/15
47808/47808 [==============================] - 362s - loss: 0.6175 - acc: 0.6478 - val_loss: 0.7101 - val_acc: 0.5591
Epoch 6/15
47808/47808 [==============================] - 344s - loss: 0.5860 - acc: 0.6778 - val_loss: 0.7268 - val_acc: 0.5556
Epoch 7/15
47808/47808 [==============================] - 347s - loss: 0.5521 - acc: 0.7081 - val_loss: 0.7586 - val_acc: 0.5456
Epoch 8/15
47808/47808 [==============================] - 353s - loss: 0.5202 - acc: 0.7324 - val_loss: 0.8535 - val_acc: 0.5518
Epoch 9/15
47808/47808 [==============================] - 354s - loss: 0.4908 - acc: 0.7519 - val_loss: 0.8264 - val_acc: 0.5393
Epoch 10/15
47808/47808 [==============================] - 352s - loss: 0.4640 - acc: 0.7673 - val_loss: 0.9265 - val_acc: 0.5376
Epoch 11/15
47808/47808 [==============================] - 340s - loss: 0.4427 - acc: 0.7817 - val_loss: 0.9088 - val_acc: 0.5418
Epoch 12/15
47808/47808 [==============================] - 358s - loss: 0.4148 - acc: 0.7972 - val_loss: 0.9248 - val_acc: 0.5501
Epoch 13/15
47808/47808 [==============================] - 355s - loss: 0.3903 - acc: 0.8125 - val_loss: 1.0106 - val_acc: 0.5452
Epoch 14/15
47808/47808 [==============================] - 360s - loss: 0.3669 - acc: 0.8290 - val_loss: 1.0123 - val_acc: 0.5361
Epoch 15/15
47808/47808 [==============================] - 437s - loss: 0.3514 - acc: 0.8371 - val_loss: 1.1003 - val_acc: 0.5429
11952/11952 [==============================] - 25s
Test score: 1.10034437192
Test accuracy: 0.542921686747
This is my model:
max_features = 20000
maxlen = 100 # cut texts after this number of words (among top max_features most common words)
batch_size = 32
print('Loading data...')
(X_train, y_train), (X_test, y_test) = reuter.load_data(path='/home/huy/Create_data/aclImdb/reuter23.pkl',nb_words=max_features,
test_split=0.2)
print(len(X_train), 'train sequences')
print(len(X_test), 'test sequences')
print('Pad sequences (samples x time)')
X_train = sequence.pad_sequences(X_train, maxlen=maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=maxlen)
print('X_train shape:', X_train.shape)
print('X_test shape:', X_test.shape)
print('Build model...')
model = Sequential()
model.add(Embedding(max_features, 128, input_length=maxlen, dropout=0.2))
model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) # try using a GRU instead, for fun
model.add(Dense(1))
model.add(Activation('sigmoid'))