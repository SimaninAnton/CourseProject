fantauzzi commented on 31 Mar 2017
Feeding input to the pre-trained InceptionV3 model, with Model.predict(), I get an error. Here below a short program to reproduce the issue. Note that if you replace
network='inception'
with
network='vgg'
the program completes successfully. The program runs the dataset through the model, collects the predictions and save them in a pickled file (to do transfer learning).
from keras.applications.inception_v3 import InceptionV3
from keras.applications.vgg16 import VGG16
from keras.layers import Input, AveragePooling2D
from keras.models import Model
from keras.datasets import cifar10
from scipy.misc import imresize
import pickle
import tensorflow as tf
import keras.backend as K
import numpy as np

network='inception'  # Must be 'inception' or 'vgg'
dataset='cifar10'
batch_size=64

if network == 'vgg':
    size = (224, 224)
elif network == 'inception':
    size = (299, 299)
else:
    assert False, "network must be either 'inception' or 'vgg'"

def create_model():
    input_tensor = Input(shape=(size[0], size[1], 3))
    if network == 'inception':
        model = InceptionV3(input_tensor=input_tensor, include_top=False)
        x = model.output
        x = AveragePooling2D((8, 8), strides=(8, 8))(x)
        model = Model(model.input, x)
    elif network == 'vgg':
        model = VGG16(input_tensor=input_tensor, include_top=False)
        x = model.output
        x = AveragePooling2D((7, 7))(x)
        model = Model(model.input, x)
    else:
        assert False
    return model

def main():

    # Download and load cifar10 dataset
    (X_train, y_train), (_, _) = cifar10.load_data()

    # Reduce the dataset to the first 1000 entries, to save memory and computation time
    X_train = X_train[0:1000]
    y_train = y_train[0:1000]

    # Resize dataset images to comply with expected input image size
    X_train = [imresize(image, size) for image in X_train]
    X_train = np.array(X_train)

    # File name where to save bottlenecked features
    train_output_file = "{}_{}_{}.p".format(network, dataset, 'bottleneck_features_train')
    print("Saving to", train_output_file)

    with tf.Session() as sess:
        K.set_session(sess)
        K.set_learning_phase(1)
        model = create_model()
        # We skip pre-processing and bottleneck the features
        bottleneck_features_train = model.predict(X_train, batch_size=batch_size, verbose=1)
        data = {'features': bottleneck_features_train, 'labels': y_train}
        pickle.dump(data, open(train_output_file, 'wb'))

if __name__ == '__main__':
    main()
When I run it, I get this output (error at the bottom):
/usr/bin/python3.5 /home/fanta/workspace/CarND-Transfer-Learning-Lab/issue2.py
Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Saving to inception_cifar10_bottleneck_features_train.p
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.797
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.29GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
Traceback (most recent call last):
  File "/home/fanta/workspace/CarND-Transfer-Learning-Lab/issue2.py", line 66, in <module>
    main()
  File "/home/fanta/workspace/CarND-Transfer-Learning-Lab/issue2.py", line 61, in main
    bottleneck_features_train = model.predict(X_train, batch_size=batch_size, verbose=1)
  File "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py", line 1554, in predict
    check_batch_axis=False)
  File "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py", line 100, in _standardize_input_data
    'Found: array with shape ' + str(data.shape))
ValueError: The model expects 0 input arrays, but only received one array. Found: array with shape (1000, 299, 299, 3)

Process finished with exit code 1
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
[N/A] If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).