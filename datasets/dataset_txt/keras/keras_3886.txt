wangtjwork commented on 19 Nov 2016
So I'm now using Keras for a information summarization problem, and I want to summarize news articles to their titles.
I have very many articles of different lengths, and padded them into the same length by adding 0s before them. I also padded the titles to have the same length to the longest of themselves.
The model I use is an embedding layer, an encoder-decoder approach with 1 LSTM each, and a TimeDistributed Dense to convert embeddings back to word index, with a softmax at the end. But this approach is requiring the output length to have the same length as the input length.
So now I have this problem: Is there a layer or something that could control the number of timestep? Or do I have to extend the titles to be the same length as articles, and have 0s before them?
Also, the output I get after embedding layer changed from 2D to 3D, but the titles themselves were still 2D, is there a way to collapse the 3D output of the whole model into 2D again, or do I have to manually expend the 2D titles into 3D?
Any helps or suggestions?