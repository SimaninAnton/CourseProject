mininaNik commented on 3 Mar 2016
I have a conceptual question about LSTM and sequences. Consider we have only one input sequence (a sequence of numbers with some patterns (not random) ) and we want to predict the future according to past. The first architecture that comes into my mind is as follows: (borrowed from: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
For training we can have two approaches:
• First approach:
n = length of the input sequence
X_train = input sequence (length = n_1)
Y_train = shifted input sequence (only one) (length = n_1)
So, for each value in X_train we have one corresponding value in Y_train (next value in the sequence). Then for training I fedd values in the X_train one by one (batch_size = 1) and update weights by using an optimization method. Here I assume the whole input as a continuous sequence. And the memory of LSTM should be able to extract long and short time dependencies in input sequence. After training the network we have a RNN that can predict one step ahead for each input. For predicting multiple steps ahead (for example 3 steps ahead) we should do as follows: feed x1 to have y1, then feed y1 to have y2, then feed y2 to have y3.
• Second approach:
The proposed approach in:
https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py
Here the approach is making some subsequences. Here we want to predict the future of the sequence based on the history. I considered the length of the history length_history and we want to predict future for the length length_future. I transformed the data to following format:
X_train is n x length_history numpy array and y_train is n x length_future numpy array (for details check the above link) . Then we can train model for predicting multiple steps ahead in future.
I implemented both of these approaches in Keras (LSTM with stateful mode)
My questions:
Which approach is correct? I myself think that the first approach is better and more logical. The reason is that the whole input is a sequence and making subsequences might be wrong!
Is the second approach sequence to sequence training?
How should I manage model internal states. I mean in prediction step when I should reset the state for both approaches?
I fed my input to a NN without any hidden layer (just a dense layer) and for both approaches this neural network (without any hidden layer) worked better than RNN with LSTM hidden layers! How is it possible?
3