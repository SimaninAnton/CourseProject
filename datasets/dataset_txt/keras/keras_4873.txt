dongzhuoyao commented on 29 Jun 2016 â€¢
edited
from http://keras.io/getting-started/functional-api-guide/#shared-layers
when we use keras implement some weight sharing model,we often use the following snippets:
merged_vector = merge([encoded_a, encoded_b,encoded_c,encoded_d,encoded_e,encoded_f,encoded_g,encoded_h], mode='concat', concat_axis=-1)
it has seven weight sharing blocks,since they are concatenated,based on the following fact:
1,they are the same network structure.
2,they are just concatinated.
so,when it BP,the seven blocks' weight change simultaneously,and their loss is equal.
finally,we can treat this total network as a weight sharing model.
however,since they are weight sharing,why they still takes seven blocks of weight numbers rather than one block of weight numbers?
so is there a mechanism in keras that we could only occupy one block of weight numbers?(so that we can save eight blocks of weight memory usage.)