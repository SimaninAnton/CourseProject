OverLordGoldDragon commented on 5 Oct 2019 â€¢
edited
Calling K.get_value(self.param) inside optimizer.get_update() yields below error:
NotImplementedError: numpy() is only available when eager execution is enabled.
Debugging, I learn that context.executing_eagerly() evaluates to False in given scope, but to True in optimizer.get_config(). I note the latter is always called in a declared scope - e.g. with strategy.scope() here.
However - the buggy aspect here is that K.get_value() works fine post-error - i.e. if I run K.get_value(model.optimizer.param), it evaluates successfully. This is accomplished via the K.symbolic wrapper, which calls tensorflow_backend.symbolic() post-error - which, when finishes executing, changes the parameter as follows:
<tf.Variable 'Adam/param:0' shape=() dtype=int64>
# into
<tf.Variable 'Adam/param:0' shape=() dtype=int64, numpy=100>
Also, calling param.numpy() directly yields the same error. I'm using Keras 2.3.0 w/ TensorFlow 2.0.0. -- Bug or feature? And how do I now retrieve K.variable values during optimizer compilation? (usage is boolean - e.g. if param == 1)
UPDATE:
print(self.param); print(tf.executing_eagerly) under each of below yields, respectively:
# def __init__(self, ...)
<tf.Variable 'Adam/param:0' shape=() dtype=int64, numpy=100>
True

# def get_updates(self)
<tf.Variable 'Adam/param:0' shape=() dtype=int64>
False
The wrappers appear to be involved - from K.symbolic.__doc__:
"Decorator used in TensorFlow 2.0 to enter the Keras graph"
So this is at the expense of class attribute values getting lost? Should __init__ also be wrapped then?