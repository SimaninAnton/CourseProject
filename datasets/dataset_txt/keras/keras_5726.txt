fgadaleta commented on 20 Mar 2016
I am training a network to predict the next word from a context window of maxlen words.
I feed the network with a pair (x,y) where
x is a list of maxlen word indices and
y is the index of the next word
I need to learn the embedding of all vocsize words
Here is the model:
 model = Sequential()
layers = [maxlen,256,256,1]
model.add(Embedding(vocsize, 128))

model.add(LSTM(input_dim=layers[0], output_dim=layers[1], return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(layers[2], return_sequences=False))
model.add(Dropout(0.2))

model.add(Dense(output_dim = layers[3]))
model.add(Activation("linear"))
#model.add(Activation('sigmoid'))
model.compile(loss="mse", optimizer="rmsprop")
#model.compile(loss='binary_crossentropy', optimizer='rmsprop')
When I fit it to x and y I get a loss of -5444.4293 steady for all epochs. It doesn't seem to learn anything.
What am I doing wrong?