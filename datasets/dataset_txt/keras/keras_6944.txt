dhdaines commented on 20 Jun 2015
The MNIST example seems to be running about 2x slower for me on a GPU (Tesla K10.G1.8GB) than on CPU.
When I run it with profiling it seems that it is spending a large amount of time in GpuFromHost. This would indicate that the data is not being loaded onto the GPU with shared variables (and in fact I can't really tell where in the Keras code this would be taking place) and thus each minibatch must be transferred to the GPU at each iteration of training.
Yet all the other examples give instructions for running on GPU, and thus Keras is presumably optimized for GPU. Am I missing something here?