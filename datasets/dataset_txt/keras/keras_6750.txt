superhans commented on 20 Aug 2015
Let's say I have code to merge two layers, like this :
left = Sequential()
left.add(Dense(784, 50))
left.add(Dense(50, 50))
left.add(Activation('relu'))

right = Sequential()
right.add(Dense(784, 50))
right.add(Dense(50, 50))
right.add(Activation('relu'))

model = Sequential()
model.add(Merge([left, right], mode='concat'))

model.add(Dense(100, 10))
model.add(Activation('softmax'))
Now, it's apparent that concat(left,right) is used to generate the softmax output. But while training, I want to network to treat "left" as a constant , and not backprop through it, while "right" can be "backproped" through. Is this possible ?