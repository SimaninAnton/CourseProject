bfelbo commented on 1 Nov 2016
Changing layer.trainable attribute for a Bidirectional wrapper does not affect the ability to train the layers that it wraps. I've reproduced this issue below, where I've made a quick fix (i.e. bidir_fix=True) to show how this can be resolved from an end-user perspective. Obviously, this should be fixed in the core code as this behavior is highly unintuitive. This is tested with the newest Keras and Theano version.
from __future__ import print_function
import numpy as np
np.random.seed(1337)  # for reproducibility

from keras.preprocessing import sequence
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Embedding, Input, merge
from keras.layers import LSTM, Bidirectional
from keras.datasets import imdb

# data
max_features = 20000
maxlen = 80
batch_size = 32

(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)
X_train = sequence.pad_sequences(X_train, maxlen=maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=maxlen)

# define model architecture
model_input = Input(shape=(maxlen,), dtype='int32')
embed = Embedding(max_features, 128, dropout=0.2)(model_input)
x = Bidirectional(LSTM(128, dropout_W=0.2, dropout_U=0.2))(embed)
x = Dense(1, activation='sigmoid')(x)
model = Model(input=[model_input], output=[x])
model.summary()

def layer_trainable(l, freeze, verbose=False, bidir_fix=False):
    l.trainable = freeze

    if bidir_fix:
        if type(l) == Bidirectional:
            l.backward_layer.trainable = freeze
            l.forward_layer.trainable = freeze

    if verbose:
        action = 'Froze' if freeze else 'Unfroze'
        print("{} {}".format(action, l.name))


# attempt to freeze all layers except last
unfrozen_types = [Dense]
for l in model.layers:
    if len(l.trainable_weights):
        if type(l) in unfrozen_types:
            layer_trainable(l, True, verbose=True)
        else:
            layer_trainable(l, False, verbose=True)

# compile and show that weights are still trainable
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
print(model.trainable_weights)

# training also shows that the model learns
model.fit(X_train, y_train,
          batch_size=100,
          validation_data=(X_test, y_test),
          nb_epoch=1)