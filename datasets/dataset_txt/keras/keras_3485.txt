CCXD commented on 12 Jan 2017 â€¢
edited
I'm trying to define a custom loss function for one-hot encoding in Keras. Using the Theano backend I've had it working using:
def custom_sqe(y_true, y_pred):
     ind = y_true.nonzero()
     return ((y_true[ind] - y_pred[ind])) ** 2
This gives train/valid loss values of around 0.1-6.0
Using the Tensorflow backend some rewriting is required, as these methods are not available.
return (y_true - K.cast(K.not_equal(y_true, 0), "float32") * y_pred)**2
Should be the equivalent in Tensorflow. However, this gives much lower loss values, starting at 0.065 on epoch 1, going down to 0.013. The resulting performance is much lower in the Tensorflow implementation too, as measured with RMSE and MAE.
Does anyone have an idea what is going on? The only difference I see is that the Theano implementation returns a single value, while the Tensorflow implementation returns an array with all but one element on zero. Calculating the mean from this should thus be exactly the same, and it does not alter the results.
Any help would be greatly appreciated