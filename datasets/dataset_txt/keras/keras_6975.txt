markberger commented on 7 Jun 2015
Hi, first I just wanted to say that Keras it really great! Thanks for putting so much work into it.
As for this issue, I was using the GRU implementation, and I noticed that the W matrices are multiplied with X before the recurrence is processed:
https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L226
While this is very fast, I was experiencing memory limitations when training a sequence model with 200 dimensional word vectors and a hidden layer size of 250 on a 4GB gpu. I eventually solved this by coding a similar GRU in theano, without precomputing the W * X operations. In the future, it would be nice to have some sort of "low memory" option that allows people to sacrifice speed in the event that they are constrained for memory