simonhughes22 commented on 1 Jul 2015
Over the weekend I wrote this model which was working:
embedding_size = 32
hidden_size = 512

print('Build model...')
model = Sequential()
model.add(Embedding(max_features, embedding_size))
model.add(JZS1(embedding_size, hidden_size)) # try using a GRU instead, for fun
model.add(Dense(hidden_size, hidden_size))
model.add(Activation('relu'))
model.add(RepeatVector(MAX_LEN))
model.add(JZS1(hidden_size, hidden_size, return_sequences=True))
model.add(TimeDistributedDense(hidden_size, max_features, activation="softmax"))

# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy', optimizer='adam', class_mode="binary")
This is an attempt at a sequence to sequence model as used in machine translation and in that recent neural conversation model by Quoc Le for google, and originated here (http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). At one point the above code was working, but at some point I updated Theano to bleeding edge and keras to the latest and it fails when calling model.train.
File "/Users/simon.hughes/GitHub/Theano/theano/compile/function_module.py", line 607, in __call__
    outputs = self.fn()
ValueError: Input dimension mis-match. (input[0].shape[0] = 1392, input[1].shape[0] = 952128)
Apply node that caused the error: Elemwise{mul,no_inplace}(Flatten{1}.0, Flatten{1}.0)
Toposort index: 439
Inputs types: [TensorType(float32, vector), TensorType(float32, vector)]
Inputs shapes: [(1392,), (952128,)]
Inputs strides: [(4,), (4,)]
Inputs values: ['not shown', 'not shown']
Outputs clients: [[Sum{acc_dtype=float64}(Elemwise{mul,no_inplace}.0)]]
Any ideas? The input is 2D padded (num_samples, max_tokens) which gets converted to 3D by the embedding layer. The output is 3D (num_samples, max_tokens, max_features) which does a softmax over a one hot vector representing the target words. The input and output are padded to the same length, the output being padded with one-hot vectors instead.