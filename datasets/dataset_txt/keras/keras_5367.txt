Contributor
carlthome commented on 23 Apr 2016 â€¢
edited
I think with my current dataset models can converge to a lot of silly solutions so I was thinking one way of finding deeper minima could be train a bunch of models in parallel and iteratively adjust the random weight initialization by Bayesian optimization as a function of validation loss, and restart training. Has anyone attempted something like this before with Keras and perhaps could share an example? Is it worthwhile?