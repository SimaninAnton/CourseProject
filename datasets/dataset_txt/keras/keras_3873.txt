singlasahil14 commented on 21 Nov 2016 â€¢
edited
from keras.models import Sequential
from keras.layers.advanced_activations import PReLU
from keras.layers import Convolution2D, MaxPooling2D

model = Sequential()
model.add(Convolution2D(32, 5, 5, input_shape=(28,28,1)))
model.add(PReLU())

model.summary()
This script produces the PReLU layer with 18432 parameters. If the PReLU was implemented according to the paper (https://arxiv.org/abs/1502.01852), number of parameters would be 32. Shouldn't this not be implemented that way? It adds an unnecessarily large number of parameters to the model.