KlaymenGC commented on 16 Jul 2017
Hello,
I'm doing semantic segmentation using Keras and I've notice that the GPU utilization is always fluctuating between 0% and 100% but never under full load (or fluctuating within a tighter range like 80% to 100%). And sometimes for each epoch the training time varies (for example, normally it takes 1000s but sometimes it would take 3000s).
I know that for semantic segmentation you have to calculate for every pixel and it's expensive, I'm using multiprocessing in fit_generator so that the GPU shouldn't theoretically always wait for the data. My questions are:
Is the loss calculated on the CPU then the gradients are transferred back to the GPU? What improvements can be done in Keras to achieve better GPU utilization?
Why is the training time for each epoch sometimes varies?
Thanks in advance!