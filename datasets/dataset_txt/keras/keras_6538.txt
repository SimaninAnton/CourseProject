alejandrok commented on 20 Oct 2015
Hi, I understand that when using LSTM layers if I set the return_sequences = false , that layer will output the last vector in the input sequence, being the input sequence a matrix of the form [timestep x input_dim] and if I set it to true, it will output the whole sequence (timestep x input_dim matrix). I've been doing some tests and it seems that when return_sequences = false, the layer actually outputs the whole sequence but as a vector, meaning it outputs a flattened version of the sequence instead of the last vector in the sequence.
Can someone point out if this is ok? Based on the documentation this should not be the output right?
Thanks