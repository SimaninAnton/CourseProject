hugman commented on 5 Oct 2015
I've checked Keras GRU implementation line by line to extend it.
In Keras GRU code, the update function is looks like
h_t = z * h_mask_tm1 + (1 - z) * hh_t
( from https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L272 )
But, from the referenced paper, the h_t formula is slightly different.
( from http://arxiv.org/pdf/1412.3555v1.pdf)
The difference is (1-z)_h~ or (1-z)_h .
Am I missing something?
I'm very sure Keras implementation is alright. (It works all the time)