mharradon commented on 5 Nov 2018 â€¢
edited
I'm training a model using fit_generator with multiprocessing. I've been working on optimizing training performance, and I've found via some profiling that roughly half of the time training is spent in various multiprocessing _recv methods (the other half waiting on tensorflow for GPU callbacks). It appears that significant time is spent transferring bytes and pickling/unpickling (I just benchmarked, and the pickling/unpickling is sufficient to explain the time).
My workload is moderately extreme in terms of batch size and number of GPUs, but I would imagine many people are unknowingly getting limited by this. I was curious if anyone had suggestions or if there was interest in contributions here - I'm considering implementing something to improve this situation, though I'm not sure what yet.