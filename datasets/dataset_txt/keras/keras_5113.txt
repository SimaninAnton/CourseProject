bernardab commented on 24 May 2016
Is there a way to share weights between two models in keras 1, where model1 is trained with single gradient update over one batch of samples (train_on_batch) and model2 is updated with model1 weights.
In keras 0.3, this is possible by using a single model and setting the trainable attributes of the layers to false for model2 compilation.
I tried creating identical models and then use model2.set_weights(model1.get_weights()) on each iteration, but this leads to out of memory error after a few iterations with tensorflow.