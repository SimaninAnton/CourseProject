amityaffliction commented on 30 Jun 2016
I'm curious about how embedding layers work
Embedding layer is trained for objective function that user can specify. for example 'mse' can be passed as argument
Q1:
but when using 'mse' which is (0.5) * (target - output)^2 what is target and output when training embedding layer? and what is embedding vector that model generates?
here is what I understand. is it right?
for example input takes sentences in each word as index (ex [0,12, 4, 4, 1]) and embedding model(RNN) is trained to predict next word for example
input 0 -> ouput 12
input 12 -> output 4
.. and so on...
and error metric is 'mse'
is it right? where does embedding vector come from?
I thought it as RNN hidden state activation but hidden state activation changes through context...
Q2:
is there Keras example that can be used as Mikolov Word2Vec model?
Q3:
sentence can be variable length sequence that consists of different number of words.
how can we train embedding layer with variable length sequence??? do we have to pad it with special token?
I think it seems inefficient
7