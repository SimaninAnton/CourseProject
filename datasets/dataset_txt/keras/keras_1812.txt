joelkuiper commented on 3 Aug 2017
I'm trying to build a sequence to sequence classifier for natural language tagging. One common problem is the variable length sequences of sentences (both in input and in output). Now I'm trying to use the Masking feature, but I'm a bit unsure how this works without and Embedding Layer. Basically I feed the input the tensor of (samples, timestep, embedding) directly, so the input is not of (1, vocab_dim) but rather (1, embedding_dim) (in this case 128). How does this work with masking, does it work if I set all the dimensions to zero, or is there something else I need to do?