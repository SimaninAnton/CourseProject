ThomasAger commented on 23 Mar 2018 â€¢
edited
My understanding was that that the cell state would have the activation function applied to it before being returned. However, it can exceed 1, or be far into the negative ranges, which should be impossible if the activation function is being applied.
Here's some example code:
from numpy import *
from keras.layers import Input,  LSTM
from keras.models import Model
main_input = Input(shape = (3,1), dtype = 'float32', name = 'main_input')
lstm_out = LSTM(1,use_bias = False, return_state = True)
sequence_out, state_h, state_c = lstm_out(main_input)
model = Model(inputs = main_input, outputs = [sequence_out, state_h, state_c])
model.layers[1].set_weights([array([[1,1,1,1]], dtype=float32), array([[1,1,1,1]],
                            dtype=float32)])
predict = model.predict(array([1,1,1]).reshape(1,3,1))

print(predict)
And output:
[array([[0.7629155]], dtype=float32), array([[0.7629155]], dtype=float32), array([[1.6458433]], dtype=float32)]
If you examine the predictions, it can be seen that the cell state does not have the activation function applied. It is 1.6, when it should be in the range of the activation function. Is this intended behaviour? Does the same thing apply during training, or is this just when predicting?
Thanks for any help on this issue.