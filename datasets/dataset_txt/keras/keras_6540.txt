Contributor
dbonadiman commented on 19 Oct 2015
Hello everyone, in the past days i've looked into many different issues posted about the possibility of implementing Bidirectional RNN in Keras. After digging a bit into it i came out with a simple solution that seems to converge but i need to understand to what extent it is theoretically sound and then i can push an update.
From what i have understood we need to look at the input forward and backward, then the output should be merged in some way before feeding it in some way to the following layers.
Well, using the graph model with a little modification to the actual recurrent layers we are one step away.
the Theano scan function has a go_backwards function we can wrap this api into the init of our Recurrent layer. And produce a bidirectional recurrent neural network in this way.
Example BidirectionalLSTM.
model = Graph()
model.add_input(name='input', input_shape=(1,), dtype=int)
model.add_node(LSTM(256,  return_sequences=True), name='forward', input='input')
model.add_node(LSTM(256,  return_sequences=True, go_backwards=True), name='backward', input='input')
model.add_node(The rest of the network, name='output_layers', inputs=['forward', 'backward'])
Am i missing something or we arrived at a point with that?
By the way i tried it locally and it converges.