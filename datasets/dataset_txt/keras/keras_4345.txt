iheo commented on 20 Sep 2016 â€¢
edited
Hi all,
I just wonder someone can help to find a problem I am having now,
I tried to make the network as simple as possible:
one hidden RNN layer with linear activation function and output dimension is two.
I compared the prection by keras and my calculation but they are different.
from keras.models import Sequential
from keras.layers import Activation, SimpleRNN
Du = 3; Dy = 2
model = Sequential()
model.add(SimpleRNN(Dy, return_sequences=True, input_shape=(None, Du)))
model.add(Activation("linear"))

# Input data (2 time steps)
xx = np.random.random((1, 2, 3))

# prediction using model.predict
Xpred1 = model.predict(xx)

# prediction using actual calculation
W = model.get_weights()
Xpred2 = np.empty((1, 2, 2))
h1 = np.zeros((2,))
h1 = np.dot(W[0].T, xx[0][0]) + np.dot(W[1].T, h1) # I didnt include the bias since they are zeros
Xpred2[0][0] = h1
h1 = np.dot(W[0].T, xx[0][1]) + np.dot(W[1].T, h1)
Xpred2[0][1] = h1


# print values
print Xpred1
print Xpred2
[[[ 0.70926803 -0.122134  ]
  [-0.39438242  0.55906796]]]
[[[ 0.8857093  -0.12274676]
  [-0.51366907  0.79976374]]]
I did similar approach for 'Dense' network and Xpred1 and Xpred2 were the same.
But in SimpleRNN, Xpred1 and Xpred2 are different.
The actual calculation involves the initialized hidden (recurrence) nodes h1 whose dimension is the same as the output dimension--2.
Can anyone help where I made mistake in the calculation?
Thanks.