Joerg99 commented on 21 Dec 2018
I'm using Keras with Tensorflow backend and I use the following code to evaluate my model on a test and a dev set with perplexity. The code below is called every epoch of training and memory sums up until it finally crashes.
I tried gc.collect() or K.clear_session() but it didn't work. Would be great to get some help.
def compute_perplexity(self, modelName, sentences):
    all_labels, all_predictions = self.predictLabels_for_perplexity_evaluation(self.models[modelName], sentences)
    # add an axis to fit tensor shape
    for i in range(len(all_labels)):
        all_labels[i] = all_labels[i][:,:, np.newaxis]
    
    
    #calculate perplexity for each sentence length and each datapoint and append to list
    perplexity = []
    for i in range(10,15): #range(len(all_labels)):
        start = time.time()
        xentropy = K.sparse_categorical_crossentropy(tf.convert_to_tensor(all_labels[i]), tf.convert_to_tensor(all_predictions[i]))
        perplexity.append(K.eval(K.pow(2.0, xentropy)))
        print('time for one set of sentences. ', time.time()- start)
    #average for each datapoint
    for i in range(len(perplexity)):
        perplexity[i] = np.average(perplexity[i], axis=1)
        perplexity[i] = np.average(perplexity[i])
    
    return np.mean(perplexity)