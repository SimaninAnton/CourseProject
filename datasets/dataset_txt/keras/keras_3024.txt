akshayk0406 commented on 10 Mar 2017
I am facing the issue where validation error is less than training error. I came across different different describing the issue but that thread was closed. Hence, i opened new thread for the issue. My problem is 3 class classification(with 29 columns) and classes are highly imbalance.
Architecture of network:-
4 Hidden layers with number of nodes in each layer be [columns, 1.25 * columns, 0.75 * columns, columns] where columns = 29.
I am using relu activation for all hidden layers and for output layer i am using softmax.
Adagrad Optimizer.
I am optimizing for sparse_categorical_crossentropy loss.
Dropout 0.1
I observed few things which doesn't seem intuitive:-
i) To account for class imbalance, I am oversampling from minority classes. And I observe that more I oversample there is decrease in training and validation accuracy.
ii) validation accuracy is always greater than training accuracy irrespective of using oversampling or not.
Below is the code for the reference:-
def create_model(columns, num_class, optimizer='Adagrad', init_mode='normal', activation='relu',
   dropout_rate=0.1, weight_constraint=5):
  
  model = Sequential()
  num_nodes = [columns, int(1.25 * columns), int(0.75 * columns), columns]

  for i in range(len(num_nodes)):
   model.add(Dense(columns, input_dim = columns, init = init_mode, activation = activation, W_constraint = maxnorm(weight_constraint)))
   model.add(Dropout(dropout_rate))
  
  model.add(Dense(output_dim = num_class, activation = 'softmax')) #Output layer
  model.compile(loss='sparse_categorical_crossentropy', optimizer = optimizer, metrics=['fbeta_score'])
  return model

model = create_model(X.shape[1], len(labels))
model.fit(X_train, y_train, nb_epoch=num_epoch, batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[history], verbose = 0)