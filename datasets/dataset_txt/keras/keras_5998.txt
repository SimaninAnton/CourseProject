danijel3 commented on 4 Feb 2016
Judging by the docs, this will probably get added one day, but I wanted to add some code suggestions, to possibly speed things up.
The loss function seems to take sample weights into account everywhere, by using the "weighted_loss" function in the code, but accuracy is the same, regardless if you use sample weights or not.
I made this simple method to compute the accuracy with sampled weights using numpy, but I guess it shouldn't be too hard to rewrite it into Keras/Theano:
def accuracy(model,inp,out,weigths):
    p=model.predict(inp)
    cnt=np.sum(weigths)    
    err=np.sum(np.not_equal(np.argmax(p,axis=-1)*weigths,np.argmax(out,axis=-1)*weigths))
    acc=1.0-(err/cnt)
    return acc
It actually works by computing the error rate instead of accuracy and then subtracting that from 1. It's easier to count errors, because then you don't have to consider the zeroed out samples in any special way.
Oh and BTW, for categorical classification, this only works with weights that are 0 or 1. Modifying classes by other weight values obviously don't make any sense.
Finally, can I have one question: what's the difference between Masking/Sample weights? Can you use them both? Can you use only one of them? Is the reason for using Masking only to save time (cause I don't notice much difference with it)? Can Masking be used in the first layer now?
2