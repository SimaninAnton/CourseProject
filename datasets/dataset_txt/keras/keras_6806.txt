Contributor
the-moliver commented on 4 Aug 2015
The way the UnitNorm and MaxNorm constraints work is currently somewhat different, and problematic on certain layer types. For example if one wanted unit-norm convolutional filters weights, these layers would do something, but it wouldn't be quite right since they compute norms over hard-coded dimensions. I'm not sure what the best general solution is to this problem so I wanted to open it up for feedback. The two main options I can think of are:
standardize weight matrices so that the norms one would aggregate over would all be in the same place so the constraints are applied properly during updates, but have them reshaped them to proper form for computing the layer output.
have the constraints.get function call in each layer also take an argument that determines which dimension(s) are aggregated over to compute the norm (which is obviously ignored if it's not needed i.e. NonNeg).
Thinking this through while writing it, I'm pretty sure 2 is the best option. What so you all think?