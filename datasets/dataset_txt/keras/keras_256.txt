erikfiit commented on 27 Apr 2019
Hello.
I would like to ask, how can I concate word emebedding with word feature (which is vector of size 63) in model for classify one class ( from 3 classes)?
I have sequences of words (sentences) and each word has token, which I want to pull to embedding layer and each word has feature vector (one-hot encoding of POS, size 64) and concate them to LSTM. Each word has label ("1" or "2" or "3" in one-hot encoding). How to do that?
So my word token for one sentence looks like: [[34, 44, 76, 89, 122, 41, 0,0,0 .. to max_len]]
feature_vector: [ [[0,1,0,0,0,0,1,0,0.. size 63],[0,0,1,0,0,0,1,0,0.. size 63], [0,0,1,0,0,0,0,1,0.. size 63], [0,1,0,0,0,0,1,0,0.. size 63], [0,0,0,1,0,0,1,0,0.. size 63], [0,0,0,0,10,0,1,0,0.. size 63]...]]
labels : [[[1,0,0], [0,1,0], [0,0,1],[1,0,0] ... to max_len]]
word_tokens = InputLayer(input_shape=(max_len,)))
embed = Embedding(vocab_size, 63, input_length=max_len, trainable=False)(word_tokens)
lstm = LSTM(300, dropout=0.3, recurrent_dropout=0.3)(embed)
word_features = Input(shape=(???))
conc = Concatenate()(lstm, word_features )
drop = Dropout(0.6)(conc)
dens = Dense(3)(drop)
acti = Activation('softmax')(dens)
model = Model([embed, word_features ], acti)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics['accuracy'])
model.fit([word_tokens, word_features], labels, batch_size=100, epochs=20,
validation_split=0.05)
Thank's a lot.