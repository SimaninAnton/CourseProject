Erstwild commented on 21 Sep 2016
I have created feature vectors from a irregular multivariate time series that additionally contains static metadata features. I have then converted all values into a one hot encoded representation (continuous time series values via binning to established thresholds and then one hot encoding them). I then time order them (irregular intervals) and concatenate them together like the following simplified example:
Each Measurement Always Contains 5 Elements
2 static_values 3 dynamic_values (one of which is time related itself) 
XX              XXX
01110 for example

id  feature_vector(time-ordered)    indicator
1   01101 01001 01101 01100 01000       0
2   11001 11001 11101                   1 
3   01010                               0
I then pad them to be the same length with zeros:
id   feature_vector(time-ordered)   indicator
 1   01101 01001 01101 01100 01000       0
 2   11001 11001 11101 00000 00000       1 
 3   01010 00000 00000 00000 00000       0
My fundamental question is if this actually an appropriate data representation? I have read about approach to creating "stationary" time series representations that usually involve creating standardized distance metrics. I have not run into lit similar to this simple an approach. I have tried LSTM models more appropriate for time series (are very slow and low accuracy in my case), but I find treating this as a traditional binary classification problem gives me really exceptional accuracy on my task (even with dropout, l2 regularization applied). Any advice or insight would be appreciated!