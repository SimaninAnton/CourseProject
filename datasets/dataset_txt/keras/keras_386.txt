Joerg99 commented on 11 Feb 2019
Currently my model concatenates two input embedding which is working fine:
`
tokens_input = Input(shape=(None,), dtype='int32', name='words_input')
tokens = Embedding(input_dim=self.embeddings.shape[0], output_dim=self.embeddings.shape[1], weights=[self.embeddings], trainable=False, name='word_embeddings')(tokens_input)
feature_input = Input(shape=(None,), dtype='int32', name=featureName+'_input')
feature_embedding = Embedding(input_dim=len(self.mappings[featureName]), output_dim=self.params['addFeatureDimensions'], name=featureName+'_emebddings')(feature_input)
inputNodes.append(feature_input)
mergeInputLayers.append(feature_embedding)
chars_input = Input(shape=(None, maxCharLen), dtype='int32', name='char_input')
chars = TimeDistributed(
Embedding(input_dim=charEmbeddings.shape[0], output_dim=charEmbeddings.shape[1],
weights=[charEmbeddings],
trainable=True, mask_zero=mask_zero), name='char_emd')(chars_input)
inputNodes.append(chars_input)
mergeInputLayers.append(chars)
merged_input = concatenate(mergeInputLayers)
model = Model(inputs=inputNodes, outputs=[output])
But instead of adding the feature_input as an Embedding I'd like to add it as a single float value to the input. What's the best way to do this? I tried for example this (just don't use the embedding) but it didn't work:
inputNodes.append(feature_input)
mergeInputLayers.append(feature_Input)
The error is a shape problem I tried some reshaping but it didn't help: ' ValueError: AConcatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, None, 300), (None, 1), (None, None, 25)]
'
How can I reshape (None, 1) to (None, None, 1) and would it be good idea in general?