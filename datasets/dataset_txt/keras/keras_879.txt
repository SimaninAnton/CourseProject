zgbkdlm commented on 18 Jul 2018 â€¢
edited
Hi,
Are there any ways to manually control the update of the weights of customized layer in Keras?
for example, if I have a custom layer:
class MyLayer(Layer):

    def __init__(self, output_dim, **kwargs):
        self.output_dim = output_dim
        super(MyLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(input_shape[1], self.output_dim),
                                      initializer='uniform',
                                      trainable=True)
        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end

    def call(self, x):
        return K.dot(x, self.kernel)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)
My question is:
How exactly are the gradient calculated? If we have a custom layer, the derivation of backpropogation on this layer will be different from usual MLP or CNN layer. How exactly gradients of weights are calculated in custom layer? Are there any ways to customize the calculation of gradients of weights in custom layer?
If I understand correctly, for example, Tensorflow backend, it will automatically calculate the gradient according to the computation graph that defined. But for some iterative algorithm in the layer (filtering etc. ), I doubt it can automatically derive the gradient.
5