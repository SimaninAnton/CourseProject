yxchng commented on 28 Aug 2016 â€¢
edited
The following is a snippet of my code that specifies the model. It compiles but it is stucked after that, meaning that it doesn't start training. There is no error shown. I leave it for about 2 hours and it still doesn't start training. Does anyone have any idea why this happens? Batch size is 16.
def _conv_bn_relu(nb_filter, nb_row=1, nb_col=1, subsample=(1, 1)):
    def f(input):
        conv = Convolution2D(nb_filter=nb_filter, nb_row=nb_row, nb_col=nb_col, subsample=subsample,
                             init="he_normal", border_mode="same")(input)
        norm = BatchNormalization(mode=0,axis=1)(conv)
        return Activation("relu")(norm)

    return f

def _bn_relu_conv(nb_filter, nb_row, nb_col, subsample=(1, 1)):
    def f(input):
        norm = BatchNormalization(mode=0,axis=1)(input)
        activation = Activation("relu")(norm)
        return Convolution2D(nb_filter=nb_filter, nb_row=nb_row, nb_col=nb_col, subsample=subsample,
                             init="he_normal", border_mode="same")(activation)

    return f

def _shortcut(input, residual):
    # Expand channels of shortcut to match residual.
    # Stride appropriately to match residual (width, height)
    # Should be int if network architecture is correctly configured.
    stride_width = input._keras_shape[2] / residual._keras_shape[2]
    stride_height = input._keras_shape[3] / residual._keras_shape[3]
    equal_channels = residual._keras_shape[1] == input._keras_shape[1]

    shortcut = input
    #1 X 1 conv if shape is different. Else identity.
    if stride_width > 1 or stride_height > 1 or not equal_channels:
        shortcut = Convolution2D(nb_filter=residual._keras_shape[1], nb_row=1, nb_col=1,
                                 subsample=(stride_width, stride_height),
                                 init="he_normal", border_mode="valid")(input)

    return merge([shortcut, residual], mode="sum")


def _residual_block(nb_filter_in, nb_filter_out):
    def f(input):
        conv_1_1 = _bn_relu_conv(nb_filter_in, 1, 1)(input)
        conv_3_3 = _bn_relu_conv(nb_filter_out/2, 3, 3)(conv_1_1)
        residual = _bn_relu_conv(nb_filter_out, 1, 1)(conv_3_3)
        return _shortcut(input, residual)

    return f

def _hourglass(n, nb_filter_in, nb_filter_out):
    def f(input):
        # Upper branch
        up1 = _residual_block(nb_filter_in, 128)(input)
        up2 = _residual_block(128, 128)(up1)
        up4 = _residual_block(128, nb_filter_out)(up2)

        # Lower branch
        pool = MaxPooling2D(pool_size=(2,2), strides=(2,2))(input)
        low1 = _residual_block(nb_filter_in, 128)(pool)
        low2 = _residual_block(128, 128)(low1)
        low5 = _residual_block(128, 128)(low2)

        if n > 1:
            low6 = _hourglass(n-1, 128, nb_filter_out)(low5)
        else:
            low6 = _residual_block(128, nb_filter_out)(low5)

        low7 = _residual_block(nb_filter_out, nb_filter_out)(low6)
        up5 = UpSampling2D((2,2))(low7)

        return merge([up4, up5], mode='sum')

    return f

def build_model():
    inputs = Input((COLOR, IMG_ROWS, IMG_COLS))
    conv1 = ZeroPadding2D((3,3))(inputs)
    conv1 = Convolution2D(32, 7, 7, subsample=(2,2))(conv1)
    conv1 = BatchNormalization(mode=0, axis=1)(conv1)
    conv1 = Activation('relu')(conv1)
    r1 = _residual_block(32, 64)(conv1)
    pool = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(r1)
    r4 = _residual_block(64, 64)(pool)
    r5 = _residual_block(64, 64)(r4)
    r6 = _residual_block(64, 128)(r5)

    hg1 = _hourglass(4, 128, 256)(r6)

    l1 = _conv_bn_relu(256)(hg1)
    l2 = _conv_bn_relu(128)(l1)

    f = Flatten()(l2)
    d1 = Dense(256, activation='relu')(f)
    drop = Dropout(0.5)(d1)
    d2 = Dense(28)(drop)

    model = Model(input=inputs, output=d2)

    return model
1