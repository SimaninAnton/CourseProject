chengzhang1 commented on 13 Nov 2015
I've been using keras to build convolution neural networks for binary classification. Using the same input data, I've tried to vary the model structure (i.e. filter size, number of filters, number of hidden layer neurons) for better performance. I noticed that for certain models, the training accuracy remains unchanged at a low value through all 50 training epochs. One of the model structure is as follows:
  n_filters=32
  n_conv_row=n_conv_col=400
  weight_init='he_normal'
  img_rows=img_cols=400
  nn_dense=25
  n_classes=2
  loss_func='categorical_crossentropy'
  model = Sequential()
  model.add(Convolution2D(n_filters, n_conv_row, n_conv_col,
                          border_mode='valid',init=weight_init,
                          input_shape=(1, img_rows, img_cols)))
  model.add(Activation('relu'))
  model.add(Flatten())
  model.add(Dense(nn_dense, init=weight_init))
  model.add(Activation('relu'))
  model.add(Dense(n_classes, init=weight_init))
  model.add(Activation('softmax'))
  model.compile(loss=loss_func, optimizer='sgd')
I used the whole training set as validation so I can get the training accuracy at the end of each epoch. The output after each epoch looks like:
259s - loss: 10.4525 - acc: 0.4903 - val_loss: 11.4543 - val_acc: 0.4473
269s - loss: 11.4543 - acc: 0.4473 - val_loss: 11.4543 - val_acc: 0.4473
251s - loss: 11.4543 - acc: 0.4473 - val_loss: 11.4543 - val_acc: 0.4473
252s - loss: 11.4543 - acc: 0.4473 - val_loss: 11.4543 - val_acc: 0.4473
...
I plotted the filter weights of the convolution layer at each epoch and they stayed the same for most of the epochs and had minor changes rarely.
at the beginning of epoch 1:

at the beginning of epoch 3:

at the beginning of epoch 5:
I also tried rmsprop as optimizer, and the accuracy at each epoch was exactly the same 0.4473 . However the filter weights formed different pattern at the end of epoch 1 but remained unchanged after that.
What may I have missed? I tried this model structure directly with Theano and the problem persisted, so I guess this problem was not from keras.
To identify the cause of this problem, I think it would be helpful to track the activation and gradient values of each mini-batch. How do I do these with Keras or Theano? Or what would be the right direction to look into?
Thanks a lot.
Cheng