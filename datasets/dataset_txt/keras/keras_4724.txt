Lzc6996 commented on 25 Jul 2016 â€¢
edited
i have a CNN run well ,but when i replace the relu by prelu.the loss go to Nan.what happened?
`
def build_model(batch_size,sent_size,vec_size,embedding_weights,nb_class):
mask_value = 0
input_sentence = Input(shape=(sent_size,), dtype='int32')
embedding = keras.layers.embeddings.Embedding(input_dim = embedding_weights.shape[0],output_dim = vec_size,weights=[embedding_weights])
x = embedding(input_sentence)
x3 = Convolution1D(200, 3,input_shape=(sent_size,vec_size), border_mode='same')(x)
x3=PReLU()(x3)
#x3 = MaxPooling1D((sent_size-2))(x3)
#x3 = Flatten()(x3)
x4 = Convolution1D(200, 4,input_shape=(sent_size,vec_size), border_mode='same')(x)
x4 =PReLU()(x4)
#x4 = MaxPooling1D((sent_size-3))(x4)
#x4 = Flatten()(x4)
x5 = Convolution1D(200, 5,input_shape=(sent_size,vec_size), border_mode='same')(x)
x5 =PReLU()(x5 )
#x5 = MaxPooling1D((sent_size-4))(x5)
#x5 = Flatten()(x5)
hidden1 = merge([x3,x4,x5],mode='concat', concat_axis=2)
xx3 = Convolution1D(100, 3, input_shape=(sent_size,600), activation='relu', border_mode='same')(hidden1)
xx3 = MaxPooling1D(sent_size)(xx3)
xx3 = Flatten()(xx3)
xx4 = Convolution1D(100, 4, input_shape=(sent_size,600), activation='relu', border_mode='same')(hidden1)
xx4 = MaxPooling1D(sent_size)(xx4)
xx4 = Flatten()(xx4)
xx5 = Convolution1D(100, 5, input_shape=(sent_size,600), activation='relu', border_mode='same')(hidden1)
xx5 = MaxPooling1D(sent_size)(xx5)
xx5 = Flatten()(xx5)
hidden2 = merge([xx3,xx4,xx5],mode='concat', concat_axis=1)
out = Dense(100,activation='tanh')(hidden2)
#out = Dropout(0.5)(out)
out = Dense(nb_class,activation='softmax')(out)
#out = Dense(1, activation="sigmoid")(out)
model = Model(input_sentence, out)
return model
`
and the results:
46850/46890 [============================>.] - ETA: 0s - loss: nan - acc: 7.2572e-04Epoch 00000: val_acc improved from -inf to 0.00077, saving model to mouth46890/46890 [==============================] - 70s - loss: nan - acc: 7.2510e-04 - val_loss: nan - val_acc: 7.6775e-04
Epoch 2/10
46890/46890 [==============================] - 70s - loss: nan - acc: 7.0377e-04 - val_loss: nan - val_acc: 7.6775e-04ve
Epoch 3/10
46890/46890 [==============================] - 69s - loss: nan - acc: 7.0377e-04 - val_loss: nan - val_acc: 7.6775e-04ve
Epoch 4/10
46890/46890 [==============================] - 69s - loss: nan - acc: 7.0377e-04 - val_loss: nan - val_acc: 7.6775e-04ve
Epoch 5/10
46890/46890 [==============================] - 69s - loss: nan - acc: 7.0377e-04 - val_loss: nan - val_acc: 7.6775e-04ve
Epoch 6/10
46890/46890 [==============================] - 69s - loss: nan - acc: 7.0377e-04 - val_loss: nan - val_acc: 7.6775e-04ve
Epoch 7/10
46890/46890 [==============================] - 69s - loss: nan - acc: 7.0377e-04 - val_loss: nan - val_acc: 7.6775e-04ve