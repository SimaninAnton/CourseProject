rfeinman commented on 14 Jan 2016
Hello,
I am trying to build an LSTM for sequence classification. I would like the network to be able to accept variable length inputs. Here is an example of what the inputs would look like:
inputs after applying pad_sequences:
X = [[0, 0, 0, 1, 2], [0, 1, 4, 3, 5], [0, 0, 1, 1, 5]]
binary class labels:
y = [1, 0, 1]
I am looking for something as defined below. This model uses a 10-word vocabulary, an embedding that converts words into 5-element vectors, and a single 5-node hidden LSTM layer that returns sequences. A pooling layer accepts the temporal sequence output by the LSTM layer and performs temporal max-pooling, looking at only the non-masked portion of the sequence. The pooling layer converts the entire variable-length hidden vector sequence into a single hidden vector, and then feeds its output to the Dense layer.
model = Sequential()
model.add(Embedding(input_dim=10, output_dim=5, mask_zero=True))
model.add(LSTM(output_dim=5, activation='sigmoid', return_sequences=True))
model.add(MaxPool()) ## <---- This is what I need
model.add(Dense(output_dim=1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop', class_mode='binary')
model.fit(X, y)
Any thoughts on how to best facilitate this?
Thanks,
Reuben