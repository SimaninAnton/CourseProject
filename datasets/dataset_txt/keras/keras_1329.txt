mpariente commented on 13 Jan 2018
I want to write a new constraint, something like ZeroMean but before I want to make sure I completely understand how they work.
Let me first summarize what I understood so far.
First part : Instantiation
When we instantiate a layer, we can provide a kernel_constraint argument, here for example
This constraint will only be used in the build method when calling add_weights which is defined here in the Layer class.
In add_weights, the weights are created using a Keras variable which includes the constraint as an attribute : here
Second part : during training
When calling fit on the model, the training function is created using _make_train_function here, which calls self.optimizer.get_updates which is defined here, for example.
And finally, we apply the constraint (here), to the trainable_weights(if i'm not mistaken). But the constraint is applied after the optimizer step (new_p = p + v, 3 lines above).
So it seems to me that the constraint is applied to the weights posterior to the optimization step and is not really included in the optimization process.
Is there a proof of convergence for DL models trained with constraints on weights?
Because I feel weird about making an optimization step and then canceling out a part of that step by changing the norm, or zeroing negative weights. Would you have any references about that I could read?
Could anyone tell me if I'm wrong and explain me what I missed please?
7