Contributor
stephenroller commented on 5 Nov 2015
Forgive me if this has come up before.
It's common in other NN libraries to unroll recurrent layers whenever possible, replacing a theano.scan with bunch of non-scan computations. This can significantly speed up the network, though often at the cost of memory. In general, Theano advises against using scans whenever possible, as performance is greatly hindered on the GPU.
Should we unroll scans in the recurrent layers when we can? One option would to borrow Lasagne's unroll_scan utility function, which conveniently has the same options as the theano.scan option.
Pros:
Potentially much faster recurrent computations (my preliminary experiment saw the training time of one epoch go from ~1300s to ~1050s, which adds up quickly)
Cons
No more truncate_gradient option (does anyone use this?)
Only possible when input_length is provided
More confusing code base
1