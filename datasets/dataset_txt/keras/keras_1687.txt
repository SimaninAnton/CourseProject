muximus3 commented on 5 Sep 2017 â€¢
edited
I am trying to implements multi-task with functional api. my model is below
    sequence_input = Input(shape=(None, max_sequence_length), dtype='int32')
    embedding_layer = Embedding(embedding_matrix.shape[0],
                                embedding_matrix.shape[1],
                                weights=[embedding_matrix],
                                input_length=max_sequence_length,
                                mask_zero=True,
                                trainable=False)(sequence_input)
    print(K.ndim(embedding_layer))
    lstm = Bidirectional(LSTM(lstm_unit, return_sequences=False, dropout=drop_out))(embedding_layer)
    bn1 = BatchNormalization()(lstm)
    dense = Dense(hidden_unit, use_bias=True)(bn1)
    bn2 = BatchNormalization()(dense)
    out_puts = []
    for name, num in labels_nums.items():
        out = Dense(num, name=name, activation='softmax')(bn2)
        out_puts.append(out)
    model = Model(inputs=sequence_input, outputs=out_puts)
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['acc'])
    model.fit(x, y, epochs=epochs_num, validation_split=0.3, verbose=2, batch_size=batch_size)
and i get error:
ValueError: Shape must be rank 3 but is rank 2 for 'bidirectional_1/Tile' (op: 'Tile') with input shapes: [?,22,1], [2].
if I remove 'Bidirectional' , which means Bidirectional(LSTM(lstm_unit, return_sequences=False, dropout=drop_out)) -> LSTM(lstm_unit, return_sequences=False, dropout=drop_out))
I get another error:
ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4
when I use K.ndim(embedding_layer), it shows l embedding output is 4 ndims, and if i remove the embedding layer and change input data shape to fit this, everything is ok, this really confused me, anybody meet the same error ? or know why?