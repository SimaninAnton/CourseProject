gprabaha commented on 22 May 2017 â€¢
edited
I want to make a custom layer through Keras such that the neurons with similar feature preference are connected linearly. For example, consider the output of each neuron is,


where yk-s are neurons with "similar" preferences, i.e. in a layer with shape 224x224x92 each 224x224 neurons are connected by 'alpha' weights but there are no connections between any of the layers corresponding to the 94 filters. How can I implement this? All the weights, 'alpha' and 'w' are going to be trained by back propagation. It is effectively like implementing a Hopfield network in the output of each of the kernels at a particular layer.
I do not want to use 'recurrent' connections, since my data is not time variable. There are 'n' mages of 'm' classes, and I want the network to have connections like the one I mentioned above.