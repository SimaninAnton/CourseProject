Contributor
jfsantos commented on 24 Apr 2015
This discussion was done in #79 but since that issue is closed I figured it would make sense to open another issue. We need to fix the batch normalization layer such that:
It can measure the mean and variance of the batch activations of each batch it sees and store it, and
Use that information instead of the mean and variance of the current batch during testing.
Regarding 1, I think maybe it's not good to measure the activation statistics during training because they will be changing over time. Maybe a safer way is to wait until training is over, and then measure these over a single epoch, with all network parameters static.