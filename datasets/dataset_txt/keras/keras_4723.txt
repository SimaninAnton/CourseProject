gxlzj commented on 25 Jul 2016
Dear developers and friends,
I am a new user to keras. My question is as fellows:
I see for "train_on_batch", the optimizer only do one single gradient update. But how is the learning rate/ momentum used in this setting?
For example, for SGD, we have a learning rate and momentum. When I use "train_on_batch" manually, will optimizer SGD automatically store the gradient update and use it as momentum when I use "train_on_batch" again next time?