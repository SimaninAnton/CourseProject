raddy commented on 27 May 2015
I've done zero work in RL so this might be REALLY stupid...
Is the cleanest method generally just to add an "Action" column to each matrix in your inputs and create a new objective function for your rewards?
After mucking with this for a day, I found that I couldn't really get better performing models than just fitting an estimated reward straight to MAE from my state matrix. My predictions just approached that fantasy 'decision boundary' and started having ugly gradients.
Or am I massively misunderstanding how this is normally stacked. The other thought was to just "encourage" the model to predict values partitioned at 0 via the loss function.
In the case where the action state space is discrete and just 0 or 1, where you either leave the lever alone or pull it I was working with a loss function along the lines:
def slot_reward(y_true,y_pred):
    y_pred = (T.sgn(y_pred)+1)/2
    return ((y_pred * y_true)*-1).sum()
This seems to blow up the gradients pretty badly too.
Any advice?