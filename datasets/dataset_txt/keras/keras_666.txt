EelcoHoogendoorn commented on 25 Oct 2018
Please make sure that the boxes below are checked before you submit your issue.
If your issue is an implementation question, please ask your question on StackOverflow or on the Keras Slack channel instead of opening a GitHub issue.
Thank you!
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps
Check that your version of TensorFlow is up-to-date. The installation instructions can be found here.
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
I know the description above urges the use of latest versions, but I cant use those sadly; but I doubt it has any bearing on this bug anyway. These are the packages I am using:
TF 1.8.0, conda-forge
Keras 2.2.4, conda-forge
I have an input tensor with strictly positive values, that I would like to have normalised in scale; without losing the positivity.
From reading the docstring and underlying code, it seems clear that this should be accomplished with the center=False kwarg. However, in practice it seem that using scale=False gives me the expected high level behavior.
Digging in a little deeper, I can indeed verify that I pass on only positive values; yet that right after the BN with center=False, I may end up with negative values (even though they ought only to have been scaled). And this does not happen with scale=False.
It seems to me that somewhere in the (very long) calling chain, these flags are getting swapped in meaning. I havnt been able to pin down where this happens; but as for a stylistic suggestion, that may be related to this... There is a lot of calling of functions with lots of arguments going on, without using kwarg names explicitly at the calling site. Having a long list of positional arguments without an obvious implied ordering makes for code that is very hard to check for correctness!