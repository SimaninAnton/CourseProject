jamesmf commented on 16 Feb 2016
I'm hoping to train a network that Merges two RNNs. I would like those RNNs to be stateful.
charModel = Sequential()
charModel.add(GRU(128, return_sequences=True,stateful=True, batch_input_shape=(batchSize,maxlen, len(chars))))
charModel.add(TimeDistributedDense(128))

wordModel = Sequential()
wordModel.add(Embedding(vocSize+1, 128, input_length=maxlen))
wordModel.add(GRU(128, return_sequences=False,stateful=True,batch_input_shape=(batchSize,maxlen,128)))
wordModel.add(RepeatVector(maxlen))

model = Sequential()
model.add(Merge([charModel, wordModel], mode='concat', concat_axis=-1))
model.add(GRU(256, return_sequences=False))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
When the recurrent layer in the first model is built, I get no errors. However when the second model's recurrent layer is built, I get the 'a complete input_shape must be provided' error. Is the input_shape being defined by the previous layer instead of batch_input_shape?