lechuzo commented on 5 Jul 2016
hi,
I use LSTM for text classification (return_sequences=False).
Each input instance is n size words. Now instead of feeding the whole input sequence, I'd like to feed the first m (m<n) words into LSTM and feed a second LSTM with m+1..n words (the rest of the sequence) and concatenate the vectors of each LSTM output (and use a dense layer after/before concatenation)
It that possible with Keras? if not, should I wrap the LSTM class with my own? how?
thanks a lot
1