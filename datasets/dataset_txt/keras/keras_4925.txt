RaffEdwardBAH commented on 22 Jun 2016
If I have a model defined like this, no issue
main_input = Input(shape=(100,), name='main_input')
emb = Reshape((10, 10))(main_input)
emb = TimeDistributed(BatchNormalization())(emb)

loss_out = Dense(1, activation='sigmoid', name='loss_out')(Flatten()(emb))

model = Model(input=[main_input], output=[loss_out])
optimizer = Adam(lr=0.001, clipnorm=grad_clip)
model.compile(optimizer, loss='binary_crossentropy')
But if the BatchNormalization is a component of another sub model, like so:
main_in_bn = Input(shape=(100,), name='main_input')
x = Dense(10)(main_in_bn)
x = BatchNormalization()(x)
sub_model = Model(input=[main_in_bn], output=[x])

main_input = Input(shape=(100,), name='main_input')

emb= Reshape((10, 10))(main_input)
emb = TimeDistributed(sub_model)(emb)

loss_out = Dense(1, activation='sigmoid', name='loss_out')(Flatten()(emb))

model = Model(input=[main_input], output=[loss_out])
optimizer = Adam(lr=0.001, clipnorm=grad_clip)
model.compile(optimizer, loss='binary_crossentropy')
I get the following error:
Exception: You are attempting to share a same `BatchNormalization` layer across different data flows. This is not possible. You should use `mode=2` in `BatchNormalization`, which has a similar behavior but is shareable (see docs for a description of the behavior).
If I remove the BatchNormalization object from the sub model it works just fine.
Obviously the intended case for this, the sub model is more complicated.
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).