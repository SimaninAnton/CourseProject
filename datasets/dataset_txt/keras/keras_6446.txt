Wainberg commented on 12 Nov 2015
Sequential.set_weights() contains the line:
nb_param = len(self.layers[i].params)
This assumes that len(self.layers[i].params) == len(self.layers[i].get_weights()). This is not true for BatchNormalization, where len(self.layers[i].params) == 2 but len(self.layers[i].get_weights()) == 4 because get_weights() includes the running mean and standard deviation as parameters, but params doesn't:
def get_weights(self):
    return super(BatchNormalization, self).get_weights() + [self.running_mean.get_value(), self.running_std.get_value()]
The simplest fix is to change the problematic line to:
nb_param = len(self.layers[i].get_weights())
However, it might be preferable to rework BatchNormalization so that len(self.layers[i].params) == len(self.layers[i].get_weights()), because it makes sense for this condition to always be true.
P.S. Thanks for creating a fantastic deep learning library!