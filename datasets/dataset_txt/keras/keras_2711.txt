schmolze commented on 7 Apr 2017
I'm training a network on survival curves. The curves cover 20 years, and I'm generating data at 6 month intervals, so my targets are 40 element vectors with each value representing the probability of survival at that time period.
I'd like to minimize the Kullbackâ€“Leibler divergence between the final output layer (40 units) and the target survival curve. My data is gene expression data for 87 genes, and I have 1088 cases, so my input shape is (1088, 87) and my target shape is (1088, 40). Here is my model:
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=input_dim))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(40, activation='sigmoid'))
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='kld', optimizer=sgd, metrics=['accuracy'])
model.fit(X, Y, validation_split=0.33, epochs=200, [batch_size=64)]
And here's what the accuracy and loss look like:
Val accuracy approaching 100%, but train accuracy less?
Negative loss?
If I try predicting from new input data with
model.predict(X_new)
I get all 1's, or values like 0.99999988.
Any suggestions would be appreciated.