Vijetha1 commented on 6 Feb 2016
I am trying to train a model built with Keras's Sequential class. The training is very slow with 8000 secs per epoch, i.e 9.25 days for 100 epochs which I can't afford to wait.
Details about the model and data are as follows.
The model has 2 channels and each one has 4 CNN layers, 2 max-pooling layers and 3 dense layers.
Both the channels are independent in the initial layers and are combined only before the final dense layer.
The input size is (150000, 3, 224, 224) with 32 samples per gradient update. I have 2 different data for the 2 channels and therefore have a total of 2*150,000 images sent to the network in pairs, one for each channel.
I am using all relu activations but a softmax one in the last but one layer; optimizer is rmsprop and loss is categorical_crossentropy
Details about the setup is as follows.
Ubuntu - 14.04.3
GPU - Nvidia Tesla K40c(GK110BGL)/33Mhz/12GB
Cuda - 7.5
Python - 2.7.6
Keras backend - Theano
The code-piece where I train the model is given here.
h5fGlobal = h5py.File( 'global_data.h5', 'r')
h5fLocal = h5py.File( 'local_data.h5', 'r')
h5fLabel = h5py.File( 'label_data.h5', 'r')
label_data = h5fLabel['label_data'][:]
h5fLabel.close()
label_data = np_utils.to_categorical(label_data,2)
model.fit(X=[h5fGlobal['global_data'], h5fLocal['local_data']],
                  y=label_data, batch_size=32, nb_epoch=1,
                  callbacks=[], validation_split=0.0, validation_data=None,  shuffle="batch",show_accuracy=False,
                  class_weight=None, sample_weight=None)
I put the entire data into 8 global and 8 local files(the size of each file is 22GB) and run the fit function on each of these. The program loops through them for 100 epochs.
My ~.theanorc file is given below in case you wish to look at it.
[global]
floatX = float32
device=gpu0
exception_verbosity=high
optimizer_including=cudnn
mode=FAST_RUN
openmp=True

[blas]
ldflags = -L/usr/lib -lblas -llapack

[nvcc]
nvcc.fastmath=True
allow_gc=True

[cuda]
root = /usr/local/cuda-7.5/

[lib]
cnmem=0.85

[gcc]
cxxflags=-I /usr/local/cuda/include -L /usr/local/cuda/lib64

[scan]
allow_gc=True
allow_output_prealloc=True
Could you give me a clue why my training is so slow?
1