Moondra commented on 18 Jan 2018
Let's say I'm transfer learning via Inception. I add a few layers and train it for a while.
Here is what my model topology looks like:
base_model = InceptionV3(weights='imagenet', include_top=False)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu', name = 'Dense_1')(x)
predictions = Dense(12, activation='softmax', name = 'Predictions')(x)
model = Model(input=base_model.input, output=predictions)
I train this model for a while, save it and load it again for retraining; this time I want to add l2-regularizer to the 'Dense_1' without resetting the weights? Is this possible?
path = .\model.hdf5
from keras.models import load_model
model = load_model(path)
The docs show only show the that regularizer can be added as parameter when you initialize a layer:
from keras import regularizers
model.add(Dense(64, input_dim=64,
                kernel_regularizer=regularizers.l2(0.01),
                activity_regularizer=regularizers.l1(0.01)))