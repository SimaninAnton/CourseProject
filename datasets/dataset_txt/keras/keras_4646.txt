sun-peach commented on 4 Aug 2016
Hi, everyone, I am using keras to build up a regular DNN with dropout and RELU. I find my prediction performance vary among experiments, e.g. with same setting and data, I ran couple of experiments, but the performance from each one is different and the difference sometimes is very large.
I understand this is because random number is used in initialization. I just want to know if there is anyway I can reduce variance in performances of different experiments.
Thank you.