franciscoaraposo commented on 2 Dec 2016 â€¢
edited
Hi all,
I have a problem where I need to map time-sequences to other time-sequences. However, since each sequence is too big and the number of sequences (samples) is also large, I need to split them and use stateful LSTM in order to split the data into smaller chunks that fit into the GPU's memory.
Suppose that each sample consists of 61 timesteps. Also suppose the training dataset is 619 samples long and the test dataset is 125 samples long.
This problem can be easily (and naively) fixed by feeding the network 1 timestep of 1 sample at a time. However, if I do this, I'm only using around 900MB of memory and I have GPUs with 12GB of memory available! This means that I can train the network much faster if I take full advantage of the GPU's memory by changing, for instance, the number of samples in a batch.
So what is the problem in doing such a thing?
If I want to change the number of samples, I will have to change the batch_size of the network to something other than 1, which means that I will eventually have a batch of the dataset whose size is smaller than the batch_size, i.e., the remainder of the integer division of 619 by batch_size. This, in turn, results in an error while training because the network is expecting a larger batch. Furthermore, I suspect the same thing will happen when evaluating the network - it will expect a batch of the same size specified when building the network.
Is there any way of circumventing this problem (like padding the input and output, and having the network ignore that padding)?