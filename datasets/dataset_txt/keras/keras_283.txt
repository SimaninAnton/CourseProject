renyajie commented on 14 Apr 2019
from keras.layers import Layer
import keras.backend as K
from keras.layers import Lambda
import numpy as np
class Attention(Layer):
def __init__(self, **kwargs):
    super(Attention, self).__init__(**kwargs)

def build(self, input_shape):
    # three weight
    # Wh: att_size, att_size -->  previous hidden state
    # Wq: query_dim, att_size --> target hidden state
    # V:  att_size, 1 --> tanh
    # score(previous, target) = Vt * tanh(Wh * previous + target * Wq + b???) --> (1, 1)

    # the dimension of previous hidden state
    self.att_size = input_shape[0][-1]
    # the dimension of target hidden state
    self.query_dim = input_shape[1][-1]

    
    self.Wq = self.add_weight(name='kernal_query_features', shape=(self.query_dim, self.att_size),
                              initializer='glorot_normal', trainable=True)

    self.Wh = self.add_weight(name='kernal_hidden_features', shape=(self.att_size, self.att_size),
                              initializer='glorot_normal', trainable=True)

    self.v = self.add_weight(name='query_vector', shape=(self.att_size, 1),
                             initializer='zeros', trainable=True)

    super(Attention, self).build(input_shape)


def call(self, inputs, mask=None):


    # score(previous, target) = Vt * tanh(Wh * memory + target * Wq)

    memory, query = inputs
    hidden = K.dot(memory, self.Wh) + K.expand_dims(K.dot(query, self.Wq), 1)
    hidden = K.tanh(hidden)
    # remove the dimension whose shape is 1
    #s = K.squeeze(K.dot(hidden, self.v), -1)

    s= K.reshape(K.dot(hidden, self.v), (-1, self.att_size))
    # compute the weight use soft_max
    s = K.softmax(s)

    return K.sum(memory * K.expand_dims(s), axis=1)


def compute_output_shape(self, input_shape):
    att_size = input_shape[0][-1]
    batch = input_shape[0][0]
    return batch, att_size
if name == 'main':
Attention()
Log:
Traceback (most recent call last):
File "/home/ryj/renyajie/exp/GLST_Net/model/GLSTModel.py", line 769, in
model = glst_net_lstm_double_attention(args)
File "/home/ryj/renyajie/exp/GLST_Net/model/GLSTModel.py", line 764, in glst_net_lstm_double_attention
model = Model(inputs=main_input, outputs=main_output, name='GLST-Net-LSTM-DA')
File "/home/ryj/anaconda3/envs/ryj_keras/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/legacy/interfaces.py", line 91, in wrapper
return func(*args, **kwargs)
File "/home/ryj/anaconda3/envs/ryj_keras/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py", line 93, in init
self._init_graph_network(*args, **kwargs)
File "/home/ryj/anaconda3/envs/ryj_keras/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py", line 231, in _init_graph_network
self.inputs, self.outputs)
File "/home/ryj/anaconda3/envs/ryj_keras/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py", line 1411, in _map_graph_network
tensor_index=tensor_index)
File "/home/ryj/anaconda3/envs/ryj_keras/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py", line 1398, in build_map
node_index, tensor_index)
File "/home/ryj/anaconda3/envs/ryj_keras/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py", line 1398, in build_map
node_index, tensor_index)
File "/home/ryj/anaconda3/envs/ryj_keras/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py", line 1398, in build_map
node_index, tensor_index)
[Previous line repeated 4 more times]
File "/home/ryj/anaconda3/envs/ryj_keras/lib/python3.6/site-packages/Keras-2.2.4-py3.6.egg/keras/engine/network.py", line 1370, in build_map
node = layer._inbound_nodes[node_index]
AttributeError: 'NoneType' object has no attribute '_inbound_nodes'
Process finished with exit code 1