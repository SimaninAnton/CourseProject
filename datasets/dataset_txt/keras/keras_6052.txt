haoqi commented on 27 Jan 2016
I build a basic multi-task NN by Graph(). On the top layer, there are 5 softmax output layers, referring 5 classification problems, the training labels are encoded as 1 of k(k =3 in each task).
However, when training the system, the val_loss will increase from the beginning, any idea?
I was think if it is the unbalanced training data issue, since for each 3 classed, the data percentage is 25%, 25%,50%, but not sure, because even if i change the validation_split to a larger number, it still happens. The loss is using 'categorical_crossentropy'
Using Theano backend.
105.0
Training...
Train on 61148 samples, validate on 15288 samples
Epoch 1/50
61148/61148 [==============================] - 1s - loss: 4.7502 - val_loss: 5.5295
Epoch 2/50
61148/61148 [==============================] - 1s - loss: 4.1619 - val_loss: 5.8611
Epoch 3/50
61148/61148 [==============================] - 1s - loss: 3.8825 - val_loss: 6.2142
Epoch 4/50
61148/61148 [==============================] - 1s - loss: 3.6600 - val_loss: 6.3189
Epoch 5/50
61148/61148 [==============================] - 1s - loss: 3.4832 - val_loss: 6.5152