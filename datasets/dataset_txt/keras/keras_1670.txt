Cerno-b commented on 12 Sep 2017
I am trying to train an LSTM with variable length inputs, so I am using a Masking Layer to deal with the length differences.
When I classify a padded sample, I get classifier output for the unpadded as well as the padded time steps (the classifier seems to repeat the output of the last non-padded time step onto the padded ones). I can understand the rationale behind this and I can manually chop off the dimensions that are masked. So far so good.
I was wondering what exactly happens during minibatch training, especially in the backprop step.
I would imagine that Keras/TF ignores all masked time steps when computing the loss, since the label data is zero padded but the forward pass output is not (as seen above), correct?
Now we backpropagate the loss through time. And here is the part I don't understand: What exactly does Keras do with samples that have masked time steps during backprop?
Are the samples removed for that time step? Are they used normally, completely ignoring the masking? If so, doesn't that cause problems, e.g. when a batch only has few long samples and a lot of short ones? Would it mess up the gradient for the few long samples?
Thanks for any clarification.