LeZhengThu commented on 10 Mar 2017
Hi I'm working on a deep survival model that has three input: X_train whose size is (n,d), Y_train whose size is (n,1), and E_train whose size is (n,1). X and Y are feed into a keras NN, whereas E is used to customize the loss function.
def negative_log_likelihood(E):
def loss(y_true,y_pred):
hazard_ratio = T.exp(y_pred)
log_risk = T.log(T.extra_ops.cumsum(hazard_ratio))
uncensored_likelihood = y_pred.T - log_risk
censored_likelihood = uncensored_likelihood * E
neg_likelihood = -T.sum(censored_likelihood)
return neg_likelihood
return loss
Then in the model compile part I wrote.
model.compile(loss=negative_log_likelihood(E_train), optimizer='rmsprop')
Then I found that I couldn't train the model in a mini-batch manner because the loss took a parameter E_train that had the same dimension with the whole input X_train. Can anyone help with this? How to make the loss function take identical batch value of E_train? Thanks.