dalstonChen commented on 23 Jan 2016
Hi,
I am confused by the MaxPooling1D() function, why the output of it is the same as ave-pooling? I pass the parameter just as the document says.
graph.add_node(MaxPooling1D(2), name = 'pooling', input = 'conv')
I have tried to read the source code of keras, but it's an implementation using TensorFlow which I am not quiet familiar with.
plus, this code will get error if I use keras 0.3.1, while 0.3.0 seems to be OK.
Could anyone who is familiar with max-pooling can help me ?
Here's my code: (sorry for the messy)
from keras.preprocessing import sequence
from keras.utils import np_utils
from keras.models import Sequential, Graph
from keras.layers.core import Dense, Dropout, Activation, Reshape, TimeDistributedDense, Merge, Lambda
from keras.layers.embeddings import Embedding
from embeddings3D import Embedding3D
from keras.layers.recurrent import LSTM, SimpleRNN
from keras.layers.convolutional import Convolution2D, Convolution1D, UpSampling1D, MaxPooling1D
from numpy import random
import numpy as np
import theano
from keras import backend as K
from theano.tensor.signal import downsample
np.random.seed(4321)  # for reproducibility


def fun(X):
    X = K.expand_dims(X, -1)
    X = K.permute_dimensions(X, (0, 2, 1, 3))
def pool(X):
    X = K.expand_dims(X, -1)
    maxpool_shape = (2,1)
    return theano.tensor.signal.downsample.max_pool_2d(X,maxpool_shape, ignore_border=True)

graph = Graph()
graph.add_input('input', input_shape = (10,8))
#graph.add_node(Lambda(fun),name = 'reform', input = 'input')
graph.add_node(Convolution1D(nb_filter=4,filter_length=3,border_mode='same'), name = 'conv', input = 'input')
graph.add_output(name='conv_out',input = 'conv')
graph.add_node(MaxPooling1D(2), name = 'pooling', input = 'conv')
#graph.add_node(Convolution2D(nb_filter=1,nb_row=3,nb_col=1), name = 'conv', input = 'reform')
graph.add_output(name = 'pooling_out', input = 'pooling')
graph.add_node(UpSampling1D(2), name = 'upsampling', input = 'pooling')
graph.add_output(name = 'upsampling_out', input = 'upsampling')
graph.compile('adam', {'conv_out': 'binary_crossentropy','pooling_out': 'binary_crossentropy', 'upsampling_out': 'binary_crossentropy'})

X = random.randint(0,2,size = (1,10,8))
out = graph.predict({'input': X})
print out['conv_out']
print out['pooling_out']
print out['upsampling_out']

'''
print (X)
X = np.expand_dims(X, -1)
print (X.shape)
print (X)
X = np.transpose(X, (0, 2, 1, 3))
print (X.shape)
print (X)

print (graph.predict({'input': X})['output'].shape)
print (X)
print (graph.predict({'input': X})['output'])
print (graph.nodes['conv'].params[0].get_value(),graph.nodes['conv'].params[1].get_value())
'''

#>>> graph.outputs['output'].input.get_value()
#>>> graph.nodes['conv'].params[0].get_value()
and it's the result(according to the code above):
it shows that the pooling_out tensor is the same as the output of average-pooling.
Thanks!