Contributor
jmhessel commented on 13 Dec 2016 â€¢
edited
When trying to share batch normalization layers, it looks like you can't use the sequential container as a function. For example, when crash is False in the following script, it runs fine. Otherwise, you get a theano/tensorflow error about variable naming.
import numpy as np
from keras.layers import Input, Dropout
from keras.layers.normalization import BatchNormalization
from keras.models import Sequential, Model

crash = True

x = np.random.normal(loc=5.0, scale=10.0, size=(2, 10))

x1 = Input(shape=(10,))
x2 = Input(shape=(10,))

# Test within sequential model
seq = Sequential()
if crash:
    seq.add(BatchNormalization(input_shape = (10,)))
if not crash:
    seq.add(Dropout(.5, input_shape = (10,)))
y3 = seq(x1)
y4 = seq(x2)

model = Model([x1,x2], [y3,y4])
model.compile('sgd','mse')
model.train_on_batch([x,x],[x,x])
For tensorflow the error is:
ValueError: Variable batchnormalization_1_running_mean/biased already exists, disallowed. Did you mean to set reuse=True in VarScope?
For theano the error is:
ValueError: ('this shared variable already has an update expression', (batchnormalization_1_running_mean, Elemwise{add,no_inplace}.0))
Check that you are up-to-date with the master branch of Keras. You can update with:
pip install git+git://github.com/fchollet/keras.git --upgrade --no-deps
If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).