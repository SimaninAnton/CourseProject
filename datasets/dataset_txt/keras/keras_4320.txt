Vladimir-Yashin commented on 23 Sep 2016
I'm getting a Traceback every time "clipnorm" is used in NN with Embedding layer.
Here is a simple script where the problem is obvious:
import numpy as np
from keras.layers import Input, Embedding
from keras.optimizers import Adam
from keras.models import Model

input_layer = Input(shape = (1,) )

embedding = Embedding(input_dim = 1,
                      output_dim = 1)(input_layer)

model = Model(input = input_layer, output = embedding)

model.compile(optimizer = Adam(clipnorm = 1.0), loss = 'mse')

X = np.array([[1]])
Y = np.array([[[0.5]]])
model.fit(X, Y, nb_epoch = 1)
Failure:
I tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)
Traceback (most recent call last):
  File "./clipnorm-bug.py", line 20, in <module>
    model.fit(X, Y, nb_epoch = 1)
  File "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py", line 1079, in fit
    self._make_train_function()
  File "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py", line 696, in _make_train_function
    self.total_loss)
  File "/usr/local/lib/python3.5/dist-packages/keras/optimizers.py", line 379, in get_updates
    grads = self.get_gradients(loss, params)
  File "/usr/local/lib/python3.5/dist-packages/keras/optimizers.py", line 71, in get_gradients
    grads = [clip_norm(g, self.clipnorm, norm) for g in grads]
  File "/usr/local/lib/python3.5/dist-packages/keras/optimizers.py", line 71, in <listcomp>
    grads = [clip_norm(g, self.clipnorm, norm) for g in grads]
  File "/usr/local/lib/python3.5/dist-packages/keras/optimizers.py", line 9, in clip_norm
    g = K.switch(n >= c, g * c / n, g)
TypeError: unsupported operand type(s) for *: 'IndexedSlices' and 'float'
Keras version is 1.1.0, TensorFlow is 0.10rc
clipvalue on the other hand works fine.
9