blairhan commented on 14 Mar 2018 â€¢
edited
I'm using a custom function that updates learning rates for each epoch.
My callback function is similar to this:
class lrModify():
       on_epoch_end: 
             lr = self.model.optimizer.lr
             decay = self.model.optimizer.decay
             lr_with_decay = lr * decay
             K.set_value(model.optimizer.lr, float(K.eval(lr_with_decay)))
And I have another code that defines the learning configuration and runs the model as follow:
optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001, nesterov=False)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(..., callbacks=[lrmodifiy])
So my question is in that case Keras has two updated learning rates for each epoch:
(1) one from my callback function
(2) the other from the optimizer function: it calculates lr for every iteration if decay is defined (https://github.com/keras-team/keras/blob/master/keras/optimizers.py)
then does Keras compute and use both learning rates? or does it know that my intention was to use my callback function so use only the first one?
In addition, there is "LearningRateScheduler" which takes schedule function as an input. Then what is the difference between the above implementation and the one using my callback function as an input of the scheduler method?
Thank you!!