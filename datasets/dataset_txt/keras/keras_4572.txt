SkidanovAlex commented on 18 Aug 2016
Take a look at this stackoverflow question:
http://stackoverflow.com/questions/38987680/in-kerasdeep-learning-library-repeatvector-timedistributed-error/39005626#39005626
Note how the end error is a confusing shape mismatch deep in execution. There are at least two places that should have asserted way before it happened:
Lambda should assert that the produced tensor actually has the shape it reports in get_output_shape_for. Since get_output_shape_for in Lambda dangerously resorts to input_shape as the default, it is very easy to end up with a graph that has the reported output_shape different from the actual shape.
RepeatVector certainly needs to assert that the input shape length is 2, since it blindly ignores the remaining dimensions when it computes the output_shape.
Also, in general, is there any reason Keras doesn't assert that the output of a layer has the same shape as the one produced by get_output_shape_for?