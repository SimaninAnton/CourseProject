Kevinpsk commented on 13 Jun 2016
Hi All,
I was wondering how to change the learning rate in SGD based on the validation loss at each epoch end using the LearningRateScheduler callback function?
I have seen in many papers that people reduce the learning rate by a factor of 2 if the validation loss does not decrease by a certain threshold and stop training when validation loss converges by some standard. Can I do this using the LearningRateScheduler function, or maybe I need to write a new callback function? I know Keras is computing the validation loss at the end of each epoch, but how can I get these loss values while the training is in progress?
Also in the SGD method provided in Keras, there is an option for learning rate decay, which is implemented as this,
lr = self.lr * (1. / (1. + self.decay * self.iterations))
Does this 'iteration' count the number of epochs or mini-batches in one training task?
Thanks for your help in advance.
Cheers,
Shuokai