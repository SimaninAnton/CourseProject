samuelBB commented on 4 Nov 2016
I am attempting to perform binary classification with a custom objective that averages the "cost" of the "mistakes" over all pairs (y_true, y_pred). More specifically, suppose my two classes are A and B. I define a cost function f(y_true, y_pred) that returns a cost C1 if y_true is A but the prediction y_pred is B, and it returns a cost C2 if a true B-instance is mistaken for an A-instance. A correct prediction has no cost.
From what I understand, the inputs y_true and y_pred to a Keras objective function represent the (encoded) true and predicted class labels, respectively, over a batch. I would like to be able to compare individual instances y_true[i], y_pred[i] and calculate the cost of this pair (based on the idea above). I would then like to sum these costs up and return the average. However, I am having a very difficult time translating this idea into the correct syntax (i.e., utilizing the symbolic operations).
Any help is greatly appreciated. FYI, I am using the Tensorflow backend.
(Additionally, I was wondering if this objective would run into gradient problems, or maybe it would be fine...)