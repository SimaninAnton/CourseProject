HenryJia commented on 11 Sep 2016
In commit 25874ce line 122 of layers/wrappers.py breaks it:
Exception: When specifying unroll=True, an input_length must be provided to rnn.
I am specifying the batch_input_shape in the first layer which is TD because afterwards there are some LSTM layers (stateful ones).