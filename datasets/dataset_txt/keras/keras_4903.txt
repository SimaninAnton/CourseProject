aravindr93 commented on 25 Jun 2016 â€¢
edited
I am writing a custom optimization routine which requires me to access the values of different weights, corresponding gradients, and finally set the new weights. See below for a simplified version of issue:
Getting the model
model = Sequential()
model.add (...) # add a few layers
x = tf.placeholder(tf.float32, size=(None,2))
y = model(x)
Updating the model
curr_weights = unpack_weights(model)  # produces flat numpy array (fn defn below)
new_weights = 2*curr_weights  # doesn't matter how this is calculated
new_weights_shaped = pack_weights(model, new_weights) # convert to appropriate shape
model.set_weights(new_weights_shaped)
If we do this, there is a difference between:
case1 = model.get_weights()   # this shows the changed weights correctly
case2 = unpack_weights(model)  # the change is not reflected here and it is wrong
The issue is that sess.run(param) doesn't seem to return the updated value of param. The update occured during the model.set_weights() command above.
Functions I used:
def unpack_weights(model):
    trainable_weights = []
    for layer in model.layers:
        trainable_weights += layer.trainable_weights

    x = np.empty(0)
    for weight in trainable_weights
        value = sess.run(weight)
        x = np.concatenate([x, value.reshape(-1)])
    return x
Note: It's essential that the shared tensor variables return the correct numerical values when called since the gradient computation is dependent on the same. Hence, a work-around is not an option, but what I seek is a solution.
1