kemingzeng commented on 6 Jun 2018 â€¢
edited
#10359
@fchollet Thanks, but your previous answer is not to the point.
This is the re-edited description.
I feed the same data to train_on_batch() and test_on_batch(), but I found the return values are different, why?
data_x = np.random.randint(0, 255, size=(100, 299, 299, 3))
data_y = np.random.randint(10, size=(100,))
data_y = to_categorical(data_y, 10)

model_base = xception.Xception(input_shape=(299, 299, 3), include_top=False, weights='imagenet')
x = model_base.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dense(10, activation='softmax')(x)
model = Model(inputs=model_base.input, outputs=x)
op = Adam(amsgrad=False, lr=0.001)
model.compile(optimizer=op, metrics=['accuracy'], loss='categorical_crossentropy')

for i in range(epoch):
    for j in range(len(data_y) // batchsize):
        batch_x = data_x[i * batchsize: (i + 1) * batchsize]
        batch_y = data_y[i * batchsize: (i + 1) * batchsize]
        c1 = model.train_on_batch(batch_x, batch_y)
        c2 = model.test_on_batch(batch_x, batch_y)
        print('train', c1, '///', 'test', c2)
the result:
train [2.242869, 0.0] /// test [1.353638, 0.875]
train [1.1516197, 0.875] /// test [0.57030946, 0.875]
train [0.6326684, 0.875] /// test [0.12945667, 1.0]
train [0.096219786, 1.0] /// test [0.024434712, 1.0]
train [0.0048230793, 1.0] /// test [0.009723314, 1.0]
train [0.002285802, 1.0] /// test [0.0029675527, 1.0]
train [0.001326285, 1.0] /// test [0.0011125191, 1.0]
train [0.00088387507, 1.0] /// test [0.00050206465, 1.0]
train [0.0005565068, 1.0] /// test [0.00024553234, 1.0]
train [0.00033587546, 1.0] /// test [0.0001305549, 1.0]
train [0.00020628066, 1.0] /// test [7.6089185e-05, 1.0]
train [0.00013133757, 1.0] /// test [4.7047954e-05, 1.0]
train [8.107312, 0.125] /// test [5.148449, 0.25]
train [2.9016776, 0.625] /// test [3.2328603, 0.625]
train [2.4415042, 0.625] /// test [1.9716715, 0.625]
train [1.5458783, 0.625] /// test [1.0773551, 0.625]
train [0.9205065, 0.625] /// test [0.9961054, 0.625]
train [0.86996764, 0.625] /// test [0.9912855, 0.625]
train [0.85850644, 0.625] /// test [0.9998646, 0.625]
train [0.8432696, 0.75] /// test [0.98874205, 0.625]
train [0.809088, 0.75] /// test [0.9463253, 0.625]
train [0.73538023, 0.875] /// test [0.86658263, 0.625]
train [0.59028786, 0.875] /// test [0.7360598, 0.75]
train [0.38827628, 0.875] /// test [0.59089166, 0.875]
train [7.699051, 0.0] /// test [6.839504, 0.0]
train [4.082408, 0.125] /// test [5.8524218, 0.0]
train [3.8092349, 0.375] /// test [5.0478907, 0.25]
train [3.663819, 0.375] /// test [4.8433256, 0.375]
train [3.4256167, 0.375] /// test [5.064186, 0.375]
train [3.2714806, 0.375] /// test [5.4150677, 0.375]
train [3.2075856, 0.375] /// test [5.746809, 0.375]
train [3.1563709, 0.375] /// test [5.9714627, 0.375]
train [3.1146622, 0.375] /// test [6.1083155, 0.375]
train [3.083407, 0.375] /// test [6.1930876, 0.375]
train [3.040458, 0.375] /// test [6.240694, 0.375]
train [2.973553, 0.375] /// test [6.2188063, 0.375]
train [3.5465846, 0.125] /// test [7.974852, 0.125]
train [2.094486, 0.25] /// test [4.177498, 0.125]
train [2.0146909, 0.25] /// test [2.5001984, 0.125]
train [1.9010043, 0.25] /// test [2.0694466, 0.125]
train [1.7921565, 0.25] /// test [1.889832, 0.25]
train [1.7151368, 0.25] /// test [1.7522879, 0.5]
train [1.6373961, 0.25] /// test [1.6356146, 0.5]
train [1.5506047, 0.375] /// test [1.5393696, 0.5]