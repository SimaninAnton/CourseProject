yxchng commented on 25 Sep 2016 â€¢
edited
Currently I am facing memory problems when the training reaches the 2nd epoch. Apparently that means that I have sufficient memory but the training is not waiting for garbage collection to complete before the start of the next iteration. Therefore, I am wondering if it is possible to make the training wait before the start of a new epoch. If so, how can I do it? Thanks
On top of that, should samples_per_epoch affect memory usage? When I lower it, training can last more than 1 epoch. However, I think if garbage collection is done properly, it shouldn't affect memory usage, should it?