ceroxlol commented on 14 Feb 2019 â€¢
edited
Hi,
I am using Keras 2.2.4, tensorflow 1.12.0.
I am experiencing a problem, where I use the sklearn.metrics log_loss function on my training and validation data to calculate the loss. Upon comparing it to the result that model.fit prints in the last step, I realized that the validation losses of the model.fit and log_loss function are identical (which they should be), but for the training data, they aren't.
I reproduced the issue on a simple iris network.
The model is set up via
train_x, test_x, train_y, test_y = model_selection.train_test_split(X,Y,test_size = 0.1, random_state = 0)
input_dim = len(data.columns) - 1
model = Sequential()
model.add(Dense(8, input_dim = input_dim , activation = 'relu'))
model.add(Dense(10, activation = 'relu'))
model.add(Dense(10, activation = 'relu'))
model.add(Dense(10, activation = 'relu'))
model.add(Dense(3, activation = 'softmax'))
model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' )
validation_data = (test_x, test_y)
model.fit(train_x, train_y, epochs = 1, batch_size = 2, validation_data=validation_data)
and afterwards the losses are calculated like this:
p = model.predict_proba(train_x, batch_size=2)
ll = log_loss(train_y, p)
p = model.predict_proba(test_x, batch_size=2)
ll = log_loss(test_y, p)
The generated output then is:
2/127 [..............................] - ETA: 30s - loss: 0.0097
66/127 [==============>...............] - ETA: 0s - loss: 2.4860
127/127 [==============================] - 1s 5ms/step - loss: 1.8052 - val_loss: 0.9724
# training | log loss: 0.98358043, AUC: 69.53%, accuracy: 0.00%
# testing | log loss: 0.97241303, AUC: 73.33%, accuracy: 0.00%
Any ideas on why the testing/validation loss are equal, but test loss isn't?