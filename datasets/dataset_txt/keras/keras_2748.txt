Contributor
yhenon commented on 3 Apr 2017 â€¢
edited
Keras throws an error when the first layer is an Activation layer with mode='linear' (this is a pretty unusual use case, but it should work). This seems to be because the definition of that layer is simply:
def linear(x):
    return x
So the topology is not properly created.
Here's a simple script to reproduce the error:
from keras.models import Sequential
from keras.layers import Dense, Activation
import numpy as np

def build_net_bug(activation):
 model = Sequential()
 model.add(Activation(activation,input_shape=(12,)))
 model.add(Dense(2))
 model.compile(loss='categorical_crossentropy', optimizer='sgd')
 return model

model = build_net_bug('linear')
model.train_on_batch(np.zeros((32,12)),np.zeros((32,2)))
print('Ok')
Producted the following error:
/path/experimental/keras/keras/engine/topology.py:1516: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "sequential_1_model" was not an Input tensor, it was generated by layer activation_1.
Note that input tensors are instantiated via `tensor = Input(shape)`.
The tensor that caused the issue was: activation_1_input:0
  str(x.name))
Traceback (most recent call last):
  File "test_keras_bug_2.py", line 12, in <module>
    model = build_net_bug('linear')
  File "test_keras_bug_2.py", line 9, in build_net_bug
    model.compile(loss='categorical_crossentropy', optimizer='sgd')
  File "/path/experimental/keras/keras/models.py", line 761, in compile
    self.build()
  File "/path/experimental/keras/keras/models.py", line 520, in build
    name=self.name + '_model')
  File "/path/experimental/keras/keras/legacy/interfaces.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/path/experimental/keras/keras/engine/topology.py", line 1569, in __init__
    if layer.is_placeholder:
AttributeError: 'Activation' object has no attribute 'is_placeholder'
Replacting 'linear' with any other activation fixes the issue.
Any suggestions on the best way to fix this (preferably without impacting the performance of Activation('linear') ?
1