sjhddh commented on 24 Jun 2015
Hello,
I am quite new to Keras, I am wondering how keras could extend my pre-processed inputs.
My case is similar to a "sentiment analysis". what I have is large english corpus labeled with their "sentiments", either 0(negative) or 1(positive).
Firstly, I tried to use word2vec to give each labeled sentences a fixed dimension of vector representations.
Hence, my x_train is N samples, each with dimension 100. the shape will be (nb_samples, sample_dimension) ==> (5000, 100).
and I also scaled them into between 0 and 1.
Then, of course, the y_train is just one-dimension labels, 0 or 1. shape (nb_labels, label_dimension) ==> (5000, 1)
I hope this input format is "well structured", then I try to write with Keras for an LSTM classification model:
def fit(x_train, x_test, y_train, y_test):
    # build model
    model = Sequential()
    model.add(Embedding(100, 256))
    model.add(LSTM(256, 128, activation='tanh', inner_activation='hard_sigmoid', return_sequences=False))
    model.add(Dropout(0.5))
    model.add(Dense(128, 1))
    model.add(Activation('tanh'))
    model.compile(loss='binary_crossentropy', optimizer='rmsprop')

    # train model
    model.fit(x_train, y_train, batch_size=30, nb_epoch=10, show_accuracy=True)

    # print test results
    print 'run on test set:'
    score = model.evaluate(x_test, y_test, batch_size=30, show_accuracy=True)
    print 'LSTM test accuracy:', score[1]
    print model.predict(x_test, batch_size=30)
    print y_test
there occurs a few puzzles:
Firstly, I actually feed pre-processed vectors into the model, so should I remove the Embedding layer at the beginning? However, if do so, LSTM layers' input will be wrong. My sample is 2D, seems like LSTM need a 3D input. what can I do? or is it necessary to keep the Embedding layer?
Secondly, if I just run this code, I will receive training outputs like this:
Epoch 0
671/671 [==============================] - 7s - loss: 1.7508 - acc.: 1.0000     
Epoch 1
671/671 [==============================] - 8s - loss: 0.7121 - acc.: 1.0000     
Epoch 2
671/671 [==============================] - 8s - loss: 0.7165 - acc.: 1.0000     
Epoch 3
671/671 [==============================] - 8s - loss: 0.7133 - acc.: 1.0000     
Epoch 4
671/671 [==============================] - 8s - loss: 0.7077 - acc.: 1.0000     
Epoch 5
671/671 [==============================] - 8s - loss: 0.7175 - acc.: 1.0000     
Epoch 6
671/671 [==============================] - 8s - loss: 0.6922 - acc.: 1.0000     
Epoch 7
671/671 [==============================] - 8s - loss: 0.7076 - acc.: 1.0000     
Epoch 8
671/671 [==============================] - 7s - loss: 0.6990 - acc.: 1.0000     
Epoch 9
671/671 [==============================] - 8s - loss: 0.7036 - acc.: 1.0000     
run on test set:
119/119 [==============================] - 0s - loss: 0.6914 - acc.: 1.0000     
LSTM test accuracy: 1.0
119/119 [==============================] - 0s
The accuracy is always 1.000, and when I print out the predictions, seems like the model never predicts a right classification, they are all numbers between 0 and 1
[[ 0.53357949]
 [ 0.52476386]
 [ 0.52066977]
 [ 0.55957317]
 [ 0.50465528]
 [ 0.52868442]
....
while, instead, the labels should be:
[ 0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.
  0.  0.  1.  0.  1.  0.  1.  1.  1.  0.  0.  1.  1.  0.  1.  0.  0.  1.
  0.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.
  0.  0.  0.  0.  0.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.  1.  1.
  0.  1.  1.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  1.  1.  0.  1.
  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.
  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.]
I tried to change several things, like activation = 'sigmoid', or optimizer='sgd', it still gives some unacceptable results.
I believe I have made some huge mistakes here, but I really cannot figure it out.
Wish to get some help, thx