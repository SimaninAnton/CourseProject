smgutstein commented on 30 Jan 2019 â€¢
edited
When I ran cifar10_resnet.py from keras/examples, I found Tensorflow ran about 10x faster than Theano. Not only is this a suspiciously large difference, but most people seem to find Theano to be faster than Tensorflow as a Keras backend.
What I saw was:
With Tensorflow:
smgutstein@gimli:~/Downloads/Packages/keras/examples$ python cifar10_resnet.py 
/home/smgutstein/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
x_train shape: (50000, 32, 32, 3)
50000 train samples
10000 test samples
y_train shape: (50000, 1)
2019-01-29 11:22:52.267342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 11:22:52.267798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1080 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.366
pciBusID: 0000:01:00.0
totalMemory: 7.93GiB freeMemory: 7.52GiB
2019-01-29 11:22:52.267814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0
2019-01-29 11:22:52.715976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-01-29 11:22:52.716002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 
2019-01-29 11:22:52.716008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N 
2019-01-29 11:22:52.716446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7265 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-01-29 11:22:52.788181: I tensorflow/core/common_runtime/process_util.cc:63] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Learning rate:  0.001
....
ResNet20v1
Using real-time data augmentation.
Epoch 1/200
Learning rate:  0.001
1563/1563 [==============================] - **36s 23ms/step**
Note: 36 sec/epoch is consistent with the 35 sec/epoch for ResNet 20 given in the code's comments/documentation
But, with Theano:
smgutstein@gimli:~/Downloads/Packages/keras/examples$ python cifar10_resnet.py 
/home/smgutstein/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using Theano backend.
Using cuDNN version 7104 on context None
Mapped name None to device cuda: GeForce GTX 1080 with Max-Q Design (0000:01:00.0)
x_train shape: (50000, 3, 32, 32)
50000 train samples
10000 test samples
y_train shape: (50000, 1)
Learning rate:  0.001
...
ResNet20v1
Using real-time data augmentation.
Epoch 1/200
Learning rate:  0.001
1563/1563 [==============================] - **348s 222ms/step** 
I installed everything with Anaconda, so have the following Keras, Tensorflow and Theano versions:
Keras: 2.2.4
Theano: 1.0.3
Tensorflow: 1.8.0
Also,
Ubuntu: 18.04
CUDA: 9.2.88
My keras.json file is:
{
    "epsilon": 1e-07, 
    "floatx": "float32", 
    "image_data_format": "channels_first",  #When using tensorflow as backend, this was "channels_last"
    "backend": "theano" #Note: Or "tensorflow",
}
My .theanorc file is as follows:
[global]
floatX=float32
device=cuda
optimizer=fast_run

[nvcc]
fastmath=True
the relevant (I believe) portions of my theano.config are:
floatX (('float64', 'float32', 'float16')) 
    Doc:  Default floating-point precision for python casts.

Note: float16 support is experimental, use at your own risk.
    Value:  float32

device (cpu, opencl*, cuda*) 
    Doc:  Default device for computations. If cuda* or opencl*, change thedefault to try to move computation to the GPU. Do not use upper caseletters, only lower case even if NVIDIA uses capital letters.
    Value:  cuda

gpuarray.preallocate (<type 'float'>) 
    Doc:  If negative it disables the allocation cache. If
             between 0 and 1 it enables the allocation cache and
             preallocates that fraction of the total GPU memory.  If 1
             or greater it will preallocate that amount of memory (in
             megabytes).
    Value:  0.0

dnn.conv.algo_fwd (('small', 'none', 'large', 'fft', 'fft_tiling', 'winograd', 'winograd_non_fused', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN forward convolution.
    Value:  small

dnn.conv.algo_bwd_data (('none', 'deterministic', 'fft', 'fft_tiling', 'winograd', 'winograd_non_fused', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the inputs.
    Value:  none

dnn.conv.algo_bwd_filter (('none', 'deterministic', 'fft', 'small', 'winograd_non_fused', 'fft_tiling', 'guess_once', 'guess_on_shape_change', 'time_once', 'time_on_shape_change')) 
    Doc:  Default implementation to use for cuDNN backward convolution to get the gradients of the convolution with regard to the filters.
    Value:  none

dnn.conv.precision (('as_input_f32', 'as_input', 'float16', 'float32', 'float64')) 
    Doc:  Default data precision to use for the computation in cuDNN convolutions (defaults to the same dtype as the inputs of the convolutions, or float32 if inputs are float16).
    Value:  as_input_f32


optimizer (('o4', 'o3', 'o2', 'o1', 'unsafe', 'fast_run', 'fast_compile', 'merge', 'None')) 
    Doc:  Default optimizer. If not None, will use this optimizer with the Mode
    Value:  fast_run

blas.ldflags (<type 'str'>) 
    Doc:  lib[s] to include for [Fortran] level-3 blas implementation
    Value:  -L/home/smgutstein/anaconda2/lib -lmkl_core -lmkl_gnu_thread -lmkl_rt -Wl,-rpath,/home/smgutstein/anaconda2/lib
Do my results make sense? Is the time difference a sign that Keras and Tensorflow are evolving, but Theano is standing still?