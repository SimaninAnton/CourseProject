guirighetto commented on 4 Aug 2016
Hello, I have a large dataset, and I wonder if I use ImageDataGenerator and fit_generator, the framework does not overload the memory of the GPU? Below is an example code that I did:
    model.compile(loss="categorical_crossentropy", optimizer="Adadelta",metrics=['accuracy'])
train_datagen = ImageDataGenerator(rescale=1./255)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    trainPath,
    target_size=(blockWidth, blockHeight),
    color_mode='grayscale',
    batch_size=batchSize,
    class_mode='categorical',
    shuffle=True)

validation_generator = test_datagen.flow_from_directory(
    testPath,
    target_size=(blockWidth, blockHeight),
    color_mode='grayscale',
    batch_size=batchSize,
    class_mode='categorical',
    shuffle=True)

model.fit_generator(
    train_generator,
    samples_per_epoch=630,
    nb_epoch=epoch,
    validation_data=validation_generator,
    nb_val_samples=315)

predict = model.predict_generator(validation_generator,315)