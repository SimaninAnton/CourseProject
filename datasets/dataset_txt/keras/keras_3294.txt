hhjjxx1989 commented on 6 Feb 2017
Hi,
Recently , I write a nlp program about text generation.In this program, I use seq2seq . When I use "categorical_crossentropy" as my loss, it need the target must be one-hot.Beacause my vocabulary size is big (20w), so OOM always come. Then I change loss to sparse_categorical_crossentropy, but it also have this problem . Also, I make a vector for each word,but it seems didn't work well as the loss were not descend.
If I need one-hot to be my loss, is there any way to avoid this problem?
Is there any way to compute the loss besides one -hot?
Thanks!