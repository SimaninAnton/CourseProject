Contributor
keunwoochoi commented on 2 Feb 2016
My multi-task network turns out to be massive!
I had a network of conv + dense layers with a output dimension of 50 and it was okay with my 4GB gtx 970.
As it is a multi-label network, I converted it into a multi-task (50 tasks) networks to make each dimension independent. However I couldn't run it and even guess how big it is.
I feel like it became massive as it has 50 objective functions. Would there some workarounds to resolve this issue, e.g. reduce memory usage for the price of computation time?
I also considered to force the layers to be sparse by not using Maxout, but using relu + dropout and/or l1 regularisation, but was not very successful.