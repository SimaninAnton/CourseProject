Contributor
MoyanZitto commented on 29 Nov 2015
We can see in the Convolution2D:
self.params = [self.W, self.b]
if weights is not None:
    self.set_weights(weights)
and in the set_weights function, we have:
def set_weights(self, weights):
    for p, w in zip(self.params, weights):
        if p.eval().shape != w.shape:
            raise Exception("Layer shape %s not compatible with weight shape %s." % (p.eval().shape, w.shape))
        p.set_value(floatX(w))
Ok, now we have a weight matrix whose shape is (64,3,3,3), means that there are 64 filters in the layer, for each filter there are 3 channels and the size of each filter is 3*3
And we have bias stored in another file, its a (64,1) np array, according to set_weights showed above, kera will check if the weights matrix given matches the initial one, since the params contains two part: weight and bias, we should merge our weight matrix and bias together, for example:
merge_weight = (weight, bias)
I am very sure that the shape of weight here is (64,3,3,3), but when I was trying to pass the merge_matrix to Convolution2D, it turns out in the conlution2D, merge_weight[0].shape is (64,)
and thus the set_weights raise the exception:
Layer shape (64, 3, 3, 3) not compatible with weight shape (64,)
If you trying to access weights[0][0], it's ok although the shape here is showed to be (64,)
Hope to fix this problem as soon as possible....
Thank you very much