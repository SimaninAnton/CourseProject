tstandley commented on 14 Dec 2016
I'm evaluating a model on a dataset I have and I've noticed that the batch size is changing the results!
#load model from weights
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

output = model.evaluate(val_data[0],val_data[1], batch_size=1, verbose=1)
print(output) # [0.077739617644647999, 0.95640591050569823]
output = model.evaluate(val_data[0],val_data[1], batch_size=24, verbose=1)
print(output) # [0.059797465635033756, 0.95971895979000976]
output = model.evaluate(val_data[0],val_data[1], batch_size=2, verbose=1)
print(output) #[0.067645869500075395, 0.95771058495228112]

#check again just to be sure
output = model.evaluate(val_data[0],val_data[1], batch_size=1, verbose=1)
print(output) # [0.077739617644647999, 0.95640591050569823]
output = model.evaluate(val_data[0],val_data[1], batch_size=24, verbose=1)
print(output) # [0.059797465635033756, 0.95971895979000976]
output = model.evaluate(val_data[0],val_data[1], batch_size=2, verbose=1)
print(output) #[0.067645869500075395, 0.95771058495228112]
Any idea what's going on? It may have something to do with the fact that my output is 4 dimensional (batch_size, 28,28,28). All outputs are sigmoid activations. I should also note that this difference seems to happen with even custom metrics (even though I've taken pains to make sure they are vectorized correctly over the batch size).
Finally, I believe that the correct output is the output with batch_size=1.