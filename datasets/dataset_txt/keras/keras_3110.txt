MaratZakirov commented on 27 Feb 2017 â€¢
edited
Please pay attention to this because user experience here is completely unusual.
I am asking because model.fit(nb_epoch=1) and model.fit(nb_epoch=10) is my case gives dramatically different results (after just one iteration) when model.fit(nb_epoch=10) is much better. I also interested in model.fit_generator, it also behaves different from model.fit (fit_generator is much worse and it is also creates FPE very often). So the problem is seems to be in learning rate adjustments. Do anybody have explanation to that?
model.fit(x=Q, y=L, validation_split=0.1, nb_epoch=1)
Train on 450000 samples, validate on 50000 samples
Epoch 1/1
450000/450000 [==============================] - 40s - loss: 0.0432 - val_loss: 0.0389

model.fit(x=Q, y=L, validation_split=0.1)#, nb_epoch=1)
Train on 450000 samples, validate on 50000 samples
Epoch 1/10
450000/450000 [==============================] - 40s - loss: 0.0219 - val_loss: 0.0141
Epoch 2/10
119584/450000 [======>.......................] - ETA: 28s - loss: 0.0148
I also consider for now fit_generator as completely unusable thing.