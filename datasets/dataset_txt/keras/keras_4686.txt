NMRobert commented on 29 Jul 2016 â€¢
edited
Hi there,
I am experiencing a bug where the addition of an regularizer causes keras to throw a ValueError.
My code is as follows:
`input_lc = Input(shape=(X.shape[1],))
encoded = Dense(100, activation='relu', input_shape=(X.shape[1],))(input_lc)
encoded = Dense(50, activation='relu')(encoded)
encoded = Dense(output_dim=encoding_dim, activation='linear')(encoded)
decoded = Dense(50, activation='relu', input_shape=(encoding_dim,))(encoded)
decoded = Dense(100, activation='relu')(decoded)
decoded = Dense(output_dim=X.shape[1], activation='linear')(decoded)
autoencoder = Model(input=input_lc, output=decoded)
encoder = Model(input=input_lc, output=encoded)
autoencoder.compile(optimizer='adam', loss='mse')`
This code runs fine. However, if I alter the last encoder layer such that it is
encoded = Dense(output_dim=encoding_dim, activation='linear', activity_regularizer=regularizers.activity_l1(0.1))(encoded)
then I get this error:
Traceback (most recent call last): File "/usr/local/lib/python3.5/dist-packages/keras/optimizers.py", line 321, in get_updates grads = self.get_gradients(loss, params) File "/usr/local/lib/python3.5/dist-packages/keras/optimizers.py", line 53, in get_gradients grads = K.gradients(loss, params) File "/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py", line 770, in gradients return tf.gradients(loss, variables) File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients.py", line 505, in gradients in_grad.set_shape(t_in.get_shape()) File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 404, in set_shape self._shape = self._shape.merge_with(shape) File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py", line 570, in merge_with (self, other)) **ValueError: Shapes (24,) and () are not compatible**
Any ideas on why this might be happening?
(Sorry for the edit spam, I accidentally submitted before having anything written...)