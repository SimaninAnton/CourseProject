Gtesk commented on 25 May 2017 â€¢
edited
Hello , i'v been lately facing some issues with fine-tuning GoogleNet in Keras ,I'v already done that with VGG16 , but the difference between those two models its vgg is sequential Google I'snt . in vgg 16 all what i had to do is pop the last layer and replace it with my own Dense layer .
i wonder if any one of you had faced this issue , and how he managed to solve it .
i'v got this implementation from https://gist.github.com/joelouismarino/a2ede9ab3928f999575423b9887abd14
I tried first to change the numbers of classes in each classifier of googleNet , hoping that keras will solve weights exception issues , but he didn't .
loss1_classifier = Dense(10,name='loss1/classifier',W_regularizer=l2(0.0002))(loss1_drop_fc loss2_classifier = Dense(10,name='loss2/classifier',W_regularizer=l2(0.0002))(loss2_drop_fc) loss1_classifier = Dense(10,name='loss1/classifier',W_regularizer=l2(0.0002))(loss1_drop_fc)
and thats how i train my model /

the error that pop up ::
ValueError: Error when checking model target: expected activation_3 to have shape (None, 10) but got array with shape (64, 1)
i know that something is wrong with my code , but since im not expert in keras don't know where exactly .
here are my intuitions .
since every layer has a name can i change the FC layers to my own with my classes number after i load my weights and then i retrain my model
thanks in advance