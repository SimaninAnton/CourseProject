maxbry commented on 22 Nov 2016
I want to use a trained keras model inside of threads:
def do_something(params):
    model, input = params
    [...]
    output = model.predict(input)
    [...]

def __name__ == '__main__:
    [...]
    pool = Pool(12)
    pool.map(do_something, params)
When I use my script like this, CUDA throws an outofmemory exception. Next, I tried to use a global lock:
    [...]
    lock.acquire()
    output = model.predict(input)
    lock.release()
    [...]
Same problem. Next, I set to pool size to 1 to have only one running thread at the same time. But I'm still getting the same error. When I don't use threading and call the model in the main thread, the script runs smoothly.
Has anyone ever used keras inside of threads?
2