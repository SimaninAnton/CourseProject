Contributor
dbonadiman commented on 13 Apr 2016
In these days i looked into the MaxPooling1D function and in particular its usage in the imdb_cnn.py example. This is not an actual issue of the code but a things that may be confusing for many people.
I think that it is a bit unrealistic useless for actual sentiment analysis models for various reasons:
It results into 3M parameters in the Dense layer after the convolution.
It requires to cut the input document at size 100.
It assumes that the position of the sentiment rich contexts into the sentence matter.
In many works the used max pooling assumes you take the maximum value along the second axis (the time axis) after the convolution.
This can be done in two ways:
(i) by assigning pool_lenght=previous_layer.output_shape[1] instead of pool_lenght=2.
(ii) or using a lambda function as follows:
    def max_1d(X):
        return K.max(X, axis=1)

    model.add(Lambda(max_1d, output_shape=(nb_filter,)))
    # No Flatten Layer
The major benefit of using this kind of pooling operation is that the number of parameters is independent on the length of the document. Only with this change it is possible to obtain more or less the same results in 30s per epoch (probably you require an epoch or two more) on CPU (previously it was >100s.
For the fact that the performance on both memory and time one can set maxlen=400 resulting in a boost of performance of ~7 points of accuracy after some epochs (with other small modifications it is possible to easily obtain 90 accuracy, i.e. early stopping after some epochs).
My point is asking to the community if this is worth a pull request modifying the example or another example is needed.
3